
1
00:00:00.830 --> 00:00:02.180
Hello and welcome.

2
00:00:02.180 --> 00:00:07.180
My name is Rene and I'll be sharing with
you, how to create a report using Pivot.

3
00:00:07.180 --> 00:00:11.920
To access the Pivot interface,
click Pivot on the navigation menu.

4
00:00:13.630 --> 00:00:17.370
The first step is to select
a prebuilt data model.

5
00:00:17.370 --> 00:00:21.260
Now the data model allows you
to create compelling reports and

6
00:00:21.260 --> 00:00:26.090
dashboards without having to know
how to write complex search queries.

7
00:00:26.090 --> 00:00:31.000
As a side note data models are typically
created by a knowledge manager that

8
00:00:31.000 --> 00:00:34.200
understands their
organization's index data.

9
00:00:34.200 --> 00:00:38.760
Grasps the search language and
are familiar with lookups,

10
00:00:38.760 --> 00:00:42.640
transactions, field extractions and
calculated fields.

11
00:00:43.750 --> 00:00:48.260
For our example, we're going to
use Buttercup Games Online Sales.

12
00:00:50.200 --> 00:00:54.960
Now, this data model includes
the online sales activities.

13
00:00:54.960 --> 00:00:58.100
It's made up of nine objects.

14
00:00:58.100 --> 00:01:03.660
Each object represents a specific set
of events in a hierarchical structure.

15
00:01:05.290 --> 00:01:12.390
The http request would include the largest
number of events in this data model.

16
00:01:12.390 --> 00:01:15.540
The next object, successful request,

17
00:01:15.540 --> 00:01:20.950
would be a subset of the http
request events and so on.

18
00:01:20.950 --> 00:01:25.710
Let's say that we want to create a report
for purchases over the last seven days.

19
00:01:25.710 --> 00:01:28.560
We would select the most
appropriate object.

20
00:01:28.560 --> 00:01:32.200
In this case we're going to
select successful purchase.

21
00:01:33.910 --> 00:01:37.940
After you have selected the object
you will notice that a new

22
00:01:37.940 --> 00:01:39.460
Pivot view will display.

23
00:01:40.720 --> 00:01:45.950
If you notice, the count of
successful purchase is the total

24
00:01:45.950 --> 00:01:50.580
number of events for this specific object.

25
00:01:50.580 --> 00:01:52.780
Now, it is based off of all time.

26
00:01:54.470 --> 00:01:59.840
Before you begin, you should have an idea
of what type of report you want to create.

27
00:01:59.840 --> 00:02:02.720
So let's say that we
want to create a bar chart.

28
00:02:02.720 --> 00:02:08.120
We're going to select that
from the left navigation pane.

29
00:02:08.120 --> 00:02:12.375
So the third icon will allow me
to click and select Bar Chart.

30
00:02:14.278 --> 00:02:18.130
So at this point we are ready
to start building our report.

31
00:02:19.560 --> 00:02:21.310
The first selection is Time Range.

32
00:02:21.310 --> 00:02:24.480
Let's set that to the last seven days.

33
00:02:26.030 --> 00:02:30.780
The next one is Filter, now we don't need
to define Filter, because we've already

34
00:02:30.780 --> 00:02:36.210
selected an object of successful purchase
and that's what we're reporting against.

35
00:02:37.720 --> 00:02:42.570
Now, for my x axis,
I'm going to select product name.

36
00:02:43.620 --> 00:02:46.040
You'll notice that I can set the label.

37
00:02:46.040 --> 00:02:48.170
I can set the sort order.

38
00:02:48.170 --> 00:02:51.091
I can even set how many
maximum bars that I want.

39
00:02:52.728 --> 00:02:56.988
The next thing to define is the y axis.

40
00:02:56.988 --> 00:03:01.970
At this time it's set to
count of successful purchase.

41
00:03:01.970 --> 00:03:05.325
But I want the total sales by product.

42
00:03:05.325 --> 00:03:07.785
So I'm going to select price and

43
00:03:07.785 --> 00:03:12.655
then you'll notice that my
value is already set to sum.

44
00:03:12.655 --> 00:03:14.475
So I'm going to keep it as sum.

45
00:03:16.340 --> 00:03:18.030
My report is starting to look really good.

46
00:03:18.030 --> 00:03:25.370
But after reviewing it I might decide that
I want to see values segmented by host.

47
00:03:25.370 --> 00:03:30.670
So let's do that,
in order to set the segmentation

48
00:03:30.670 --> 00:03:38.290
of a specific product name I could go and
set the color to the field that I want.

49
00:03:38.290 --> 00:03:39.824
So I'm going to select Host.

50
00:03:42.936 --> 00:03:49.490
Now, you'll see that my total
sales are segmented by host.

51
00:03:50.966 --> 00:03:52.566
So we really like this report.

52
00:03:52.566 --> 00:03:54.190
Let's go ahead and save it.

53
00:03:55.240 --> 00:03:57.860
So we're going to click on Save As.

54
00:03:59.050 --> 00:04:01.560
Select report and give it a title.

55
00:04:01.560 --> 00:04:06.810
Let's call it Product Sales by Host.

56
00:04:06.810 --> 00:04:13.684
We're going to click Save and
then we're going to view our report.

57
00:04:16.563 --> 00:04:19.880
It was that easy to create
a report through Pivot.

58
00:04:21.850 --> 00:04:26.030
Now, let's just say that we want to go and
work with another Pivot.

59
00:04:26.030 --> 00:04:28.810
This time we're going to just go and

60
00:04:28.810 --> 00:04:32.070
create a statistic table
to view some of that data.

61
00:04:32.070 --> 00:04:34.917
Let's go ahead and click Pivot again.

62
00:04:36.700 --> 00:04:39.090
We'll select the same data model.

63
00:04:39.090 --> 00:04:43.637
Buttercup games online sales and
let's keep to the object that

64
00:04:43.637 --> 00:04:47.855
we're familiar with right now,
successful purchases.

65
00:04:54.902 --> 00:04:59.920
As I mentioned, I want to just
simply create a statistic table.

66
00:05:00.990 --> 00:05:03.400
I want to show you how
we'd go about doing that.

67
00:05:04.620 --> 00:05:11.700
So the first step is to think about what
are some of the row data that you want?

68
00:05:11.700 --> 00:05:18.790
Let's just assume that we want to view
the product names within their categories.

69
00:05:18.790 --> 00:05:24.500
So, let's go ahead, under Split Rows,
we're going to select category first.

70
00:05:25.530 --> 00:05:28.060
Notice that I can set a label if I want.

71
00:05:28.060 --> 00:05:32.500
Let's go ahead and
set it capitalized category.

72
00:05:32.500 --> 00:05:33.897
I'm going to add that to my table.

73
00:05:33.897 --> 00:05:38.403
You'll see that it
becomes the first column.

74
00:05:38.403 --> 00:05:42.240
Now I want to add another column.

75
00:05:42.240 --> 00:05:46.535
Let's go ahead and
click the plus right next to categories.

76
00:05:46.535 --> 00:05:49.995
Again we're still working
with the split rows.

77
00:05:51.065 --> 00:05:56.375
I'm going to select product name, again if
you want to set a label you can do that.

78
00:05:56.375 --> 00:05:58.955
Now this is the label of our column.

79
00:06:00.175 --> 00:06:00.675
Add to table.

80
00:06:04.367 --> 00:06:09.282
And I want you to notice that
now we are viewing each product

81
00:06:09.282 --> 00:06:11.400
within their category.

82
00:06:12.580 --> 00:06:16.130
Now at this point if you look at
the statistic that is being defined,

83
00:06:16.130 --> 00:06:19.850
it's the count of successful purchases.

84
00:06:19.850 --> 00:06:22.570
That is defined as column values.

85
00:06:23.940 --> 00:06:28.960
Now if I want to add to this,
I can append and have another column.

86
00:06:28.960 --> 00:06:34.050
Let's go ahead and click the plus next
to the Count of successful purchase.

87
00:06:35.480 --> 00:06:37.300
This time I want to add the price.

88
00:06:38.520 --> 00:06:41.216
And you'll notice that I have
different values available.

89
00:06:41.216 --> 00:06:44.662
I could do a Sum, a Count,
an Average, Max, Min.

90
00:06:44.662 --> 00:06:51.274
Let's keep it as a Sum and
let's add a label of Total Sales.

91
00:06:51.274 --> 00:06:53.795
And let's Add To Table.

92
00:06:55.955 --> 00:06:59.675
So this is just to show you
how quick it is to go and

93
00:06:59.675 --> 00:07:03.955
build a table with the statistics
that you would need.

94
00:07:03.955 --> 00:07:07.977
Now at this point, if ever you decide,
I only want to view the data for

95
00:07:07.977 --> 00:07:09.260
the last seven days.

96
00:07:09.260 --> 00:07:13.590
Just note that you could go
back up to your filter and

97
00:07:13.590 --> 00:07:16.630
your first filter is
based off of all time.

98
00:07:16.630 --> 00:07:20.870
So we could set that to
the last seven days.

99
00:07:20.870 --> 00:07:23.610
So of course that the data
in our table will change.

100
00:07:24.750 --> 00:07:29.030
Now, the last thing I want to show you
here is if ever you did want to see

101
00:07:30.120 --> 00:07:37.075
the count and the total sales per host,
as we've defined in our previous example.

102
00:07:37.075 --> 00:07:41.615
That is where you could go
to the split columns and

103
00:07:41.615 --> 00:07:44.325
define how you wish to
split that information.

104
00:07:44.325 --> 00:07:47.825
So let's go ahead and select host again.

105
00:07:47.825 --> 00:07:50.731
And I'm just going to
click on add to table.

106
00:07:55.074 --> 00:08:02.820
Now I could see that the information
has been split by host.

107
00:08:02.820 --> 00:08:08.460
I have www1 count of successful
purchases as well as total sales.

108
00:08:08.460 --> 00:08:15.810
And then I have the split of www2 I have
the count and the total sales and etc.

109
00:08:15.810 --> 00:08:20.950
So this concludes our short
video hope you enjoy it.

110
00:08:20.950 --> 00:08:21.450
Thank you.

1
00:00:01.860 --> 00:00:03.350
Welcome.

2
00:00:03.350 --> 00:00:08.570
In this short module we'll talk about
information integration which refers

3
00:00:08.570 --> 00:00:13.470
to the problem of using many different
information sources to accomplish a task.

4
00:00:14.780 --> 00:00:19.939
In this module, we'll look at the problems
and solutions through a few use cases.

5
00:00:22.330 --> 00:00:27.830
So after this video, you'll be able to
explain the data integration problem,

6
00:00:29.250 --> 00:00:34.930
define integrated views and schema
mapping, describe the impact of increasing

7
00:00:34.930 --> 00:00:40.306
the number of data sources, appreciate
the need to use data compression,

8
00:00:40.306 --> 00:00:46.910
And describe record linking,
data exchange, and data fusion tasks.

9
00:00:50.008 --> 00:00:54.230
Our first use case starts with
an example given at an IBM website for

10
00:00:54.230 --> 00:00:55.760
their information integration products.

11
00:00:57.150 --> 00:01:00.580
It represents a very common
scenario in today's business world.

12
00:01:02.100 --> 00:01:06.040
Due to the changing market dynamics,
companies

13
00:01:06.040 --> 00:01:09.620
are always selling off a part of their
company or acquiring another company.

14
00:01:11.330 --> 00:01:15.950
As these mergers and acquisitions happen,
databases which were developed and

15
00:01:15.950 --> 00:01:19.900
stored separately in different companies
would now need to be brought together.

16
00:01:21.680 --> 00:01:23.640
Now take a minute to read this case.

17
00:01:28.134 --> 00:01:32.644
This is the case of an expanding financial
services group that's growing its customer

18
00:01:32.644 --> 00:01:34.260
base in different countries.

19
00:01:36.420 --> 00:01:41.980
And all they want is a single view
of their entire customer base.

20
00:01:41.980 --> 00:01:46.590
In other words, it does not matter
which previous company originally had

21
00:01:46.590 --> 00:01:51.920
the customers, Suncorp-Metway
want to consolidate all customer

22
00:01:51.920 --> 00:01:58.130
information as if they were
in one single database.

23
00:01:58.130 --> 00:02:02.223
And in reality, of course, they may
not want to buy a huge machine and

24
00:02:02.223 --> 00:02:05.340
migrate every subsidiary
company's data into it.

25
00:02:06.680 --> 00:02:12.338
What they're looking to create is possibly
a software solution which would make all

26
00:02:12.338 --> 00:02:18.710
customer-related data to appear as though
they were together as a single database.

27
00:02:18.710 --> 00:02:22.290
This software solution is called
an information integration system.

28
00:02:23.590 --> 00:02:27.920
This will help them ensure that they have
a uniform set of marketing campaigns for

29
00:02:27.920 --> 00:02:29.040
all their customers.

30
00:02:31.780 --> 00:02:36.893
Let's try to see, hypothetically, of
course, what might be involved in creating

31
00:02:36.893 --> 00:02:41.511
this combined data and what kind of use
the integrated data might result in.

32
00:02:41.511 --> 00:02:44.330
So we first create
a hypothetical scenario.

33
00:02:45.552 --> 00:02:48.920
Although Suncorp have a large
number of data sources,

34
00:02:48.920 --> 00:02:51.290
we will take a much simpler situation and

35
00:02:51.290 --> 00:02:55.610
have only two data sources from two
different financial service companies.

36
00:02:57.010 --> 00:02:58.190
The first data source,

37
00:02:58.190 --> 00:03:03.090
which is an insurance company that manages
it's data with a relation of DBMS,

38
00:03:03.090 --> 00:03:08.660
this database has nine tables where the
primary object of information is a policy.

39
00:03:10.040 --> 00:03:13.090
The company offers many
different types of policies

40
00:03:13.090 --> 00:03:16.380
sold to individual people by their agents.

41
00:03:16.380 --> 00:03:20.740
Now as it's true for all insurance
companies, policyholders pay their monthly

42
00:03:20.740 --> 00:03:26.130
dues, and sometimes people make claims
against their insurance policies.

43
00:03:26.130 --> 00:03:30.080
When they do, the details of the claims
are maintained in the database.

44
00:03:31.170 --> 00:03:35.220
These claims can belong to different
categories, and when the claims have

45
00:03:35.220 --> 00:03:39.780
paid to the claimants, the transaction
is recorded in the transactions table.

46
00:03:41.350 --> 00:03:43.640
As we have done several times now,

47
00:03:43.640 --> 00:03:46.930
the primary keys of the table
are the underlined attributes here.

48
00:03:48.900 --> 00:03:51.000
The second company in
our example is a bank,

49
00:03:52.060 --> 00:03:53.850
which also uses a relational database.

50
00:03:55.240 --> 00:03:59.500
In this bank, both individuals and
businesses called corporations here,

51
00:03:59.500 --> 00:04:00.140
can have accounts.

52
00:04:02.900 --> 00:04:05.050
Now accounts can be of different types.

53
00:04:05.050 --> 00:04:07.900
For example, a money market account
is different from a savings account.

54
00:04:09.330 --> 00:04:13.910
A bank also maintains its transactions
in a table, which can be really large.

55
00:04:15.620 --> 00:04:20.050
But the dispute in a bank record
case happens when the bank is

56
00:04:20.050 --> 00:04:24.690
charged a customer, or the customer has
declined responsibility of the charge.

57
00:04:24.690 --> 00:04:29.030
This can happen, for example, if a
customer's Internet account was hacked or

58
00:04:29.030 --> 00:04:30.260
a debit card got stolen.

59
00:04:31.700 --> 00:04:33.800
The bank keeps a record
of these anomalies and

60
00:04:33.800 --> 00:04:38.260
fraudulent events in a disputes table,
all right.

61
00:04:38.260 --> 00:04:42.730
Let's see what happens after the data
from these two subsidiary companies

62
00:04:42.730 --> 00:04:43.640
are integrated.

63
00:04:46.410 --> 00:04:50.970
After the merger the company wants to
do a promotional goodwill activity.

64
00:04:51.970 --> 00:04:54.490
They would like to offer
a small discount to their

65
00:04:54.490 --> 00:04:59.130
insurance policyholders if they're also
customers of the newly acquired bank.

66
00:05:00.350 --> 00:05:02.650
How do you identify these customers?

67
00:05:02.650 --> 00:05:03.960
Let's see.

68
00:05:03.960 --> 00:05:08.882
In other words, we need to use the table
shown on the left to create the table

69
00:05:08.882 --> 00:05:12.103
shown on the right called
discount candidates.

70
00:05:12.103 --> 00:05:16.547
One is to create a yellow tables from
the insurance company database, and

71
00:05:16.547 --> 00:05:21.351
the blue table from the bank database,
and then join them to construct the table

72
00:05:21.351 --> 00:05:25.960
with a common customer ID, and both
the policyKey and bank account number.

73
00:05:27.290 --> 00:05:31.600
Now, this relation, which is derived that
is computed by querying two different

74
00:05:31.600 --> 00:05:36.260
data sources and combining their results,
is called an integrated view.

75
00:05:37.330 --> 00:05:42.400
It is integrated because the data is
retrieved from different data sources,

76
00:05:43.610 --> 00:05:47.640
and it's called a view because
in database terminology

77
00:05:47.640 --> 00:05:50.680
it is a relation computed
from other relations.

78
00:05:53.498 --> 00:05:56.780
To populate the integrated
view discount candidates,

79
00:05:56.780 --> 00:05:59.470
we need to go through a step
called schema mapping.

80
00:06:00.580 --> 00:06:04.550
The term mapping means to
establish correspondence between

81
00:06:04.550 --> 00:06:08.850
the attributes of the view, which is
also called a target relation, and

82
00:06:08.850 --> 00:06:10.120
that of the source relations.

83
00:06:11.200 --> 00:06:16.270
For example, we can map the full address
from individuals to the address attribute

84
00:06:16.270 --> 00:06:21.380
in discountCandidates, but
this would only be true for

85
00:06:21.380 --> 00:06:25.300
customers whose names and
addresses match in the two databases.

86
00:06:27.040 --> 00:06:32.120
As you can see, policyholders uses
the full name of a customer, whereas

87
00:06:32.120 --> 00:06:36.050
individuals has it broken down into first
name, middle initial, and last name.

88
00:06:37.520 --> 00:06:42.760
On the other hand, full address is
a single field in individuals, but

89
00:06:42.760 --> 00:06:46.397
represented in full attributes
in the policyholders relation.

90
00:06:48.090 --> 00:06:51.650
The mappings of account number and
policyKey are more straightforward.

91
00:06:52.680 --> 00:06:55.752
Well, what about customer ID which doesn't
correspond to anything in the four

92
00:06:55.752 --> 00:06:57.300
input relations?

93
00:06:57.300 --> 00:06:58.959
We'll come back to this later on.

94
00:07:01.019 --> 00:07:04.470
Okay, now we'll define
an integrated relation.

95
00:07:04.470 --> 00:07:06.020
How do we query?

96
00:07:06.020 --> 00:07:07.090
For example,

97
00:07:07.090 --> 00:07:11.170
how do you find the bank account number
of a person whose policyKey is known?

98
00:07:12.530 --> 00:07:15.020
You might think, what's the problem here?

99
00:07:15.020 --> 00:07:16.590
We have a table.

100
00:07:16.590 --> 00:07:22.020
Just say select account number from
discount candidates where policyKey

101
00:07:22.020 --> 00:07:26.680
is equal to 4-937528734, and we're done.

102
00:07:28.730 --> 00:07:35.310
Well, yes, you can write this query,
but how the query be evaluated?

103
00:07:35.310 --> 00:07:37.820
That depends on what's called the query

104
00:07:37.820 --> 00:07:40.610
architecture of the data
integration system.

105
00:07:40.610 --> 00:07:43.130
The figure on the left shows
the elements of this architecture.

106
00:07:44.290 --> 00:07:47.694
We'll discover it in more detail,
but on this slide,

107
00:07:47.694 --> 00:07:50.810
we'll just describe
the three axes of this cube.

108
00:07:51.830 --> 00:07:57.090
The vertical z axis specifies
whether we have one data source or

109
00:07:57.090 --> 00:07:58.320
multiple data sources.

110
00:07:59.320 --> 00:08:02.390
Our interest is in the case where
there are multiple data sources.

111
00:08:04.218 --> 00:08:10.110
The x axis asks whether the integrated
data is actually stored physically

112
00:08:10.110 --> 00:08:16.280
in some place or whether it is computed
on the fly, each time a query is asked.

113
00:08:16.280 --> 00:08:22.120
If it is all precomputed and stored,
we say that the data is materialized.

114
00:08:22.120 --> 00:08:24.940
And if it is computed on the fly,
we say it's virtual.

115
00:08:26.480 --> 00:08:30.174
The y axis asks whether
there is a single schema or

116
00:08:30.174 --> 00:08:34.319
global schema defined all
over the data integrated for

117
00:08:34.319 --> 00:08:39.364
an application or whether the data
stay in different computers and

118
00:08:39.364 --> 00:08:43.531
it is accessed in a peer-to-peer
manner at runtime.

119
00:08:43.531 --> 00:08:48.267
Thus, the seemingly simple select
project query will be evaluated

120
00:08:48.267 --> 00:08:53.870
depending on which part of the cube
our architecture implements.

121
00:08:53.870 --> 00:08:56.880
But for now,
let's return to our example use case.

122
00:08:59.900 --> 00:09:03.920
An obvious goal of an information
integration system

123
00:09:03.920 --> 00:09:05.260
is to be complete and accurate.

124
00:09:06.770 --> 00:09:10.790
Complete means no eligible record
from the source should be absent in

125
00:09:10.790 --> 00:09:11.670
the target relation.

126
00:09:12.970 --> 00:09:18.668
Accurate means all the entries in
the integrated relation should be correct.

127
00:09:18.668 --> 00:09:23.148
Now we said on the previous
slide that a matching

128
00:09:23.148 --> 00:09:27.740
customer is a person who
was in both databases and

129
00:09:27.740 --> 00:09:32.590
has the same name and
address in the two databases.

130
00:09:32.590 --> 00:09:34.310
Now let's look at some example records.

131
00:09:35.390 --> 00:09:39.110
Specifically, consider the records
marked by the three arrows.

132
00:09:40.510 --> 00:09:45.320
The two bank accounts and
the policy record do not match for name or

133
00:09:45.320 --> 00:09:47.050
for address.

134
00:09:47.050 --> 00:09:49.360
So our previous method would discard them.

135
00:09:50.510 --> 00:09:52.030
But look at the records closely.

136
00:09:53.380 --> 00:09:56.070
Do you think they might all
belong to the same customer?

137
00:09:57.560 --> 00:09:59.740
Maybe this lady has a maiden name and

138
00:09:59.740 --> 00:10:03.150
a married name, and
has moved from one address to another.

139
00:10:04.290 --> 00:10:07.289
Maybe she changed her Social Security
number somewhere along the way.

140
00:10:08.820 --> 00:10:11.640
So this is called
a record linkage problem.

141
00:10:12.730 --> 00:10:17.510
That means we would like to ensure that
the set of data records that belong to

142
00:10:17.510 --> 00:10:22.040
a single entity are recognized,
perhaps by clustering

143
00:10:22.040 --> 00:10:26.790
the values of different attributes or
by using a set of matching rules so

144
00:10:26.790 --> 00:10:31.130
that we know how to deal with it
during the integration process.

145
00:10:31.130 --> 00:10:33.600
For example, we need to determine

146
00:10:33.600 --> 00:10:36.250
which of the addresses should be
used in the integrated relation.

147
00:10:37.570 --> 00:10:38.650
Which of the two bank accounts?

148
00:10:40.070 --> 00:10:44.800
If the answer is both accounts 102 and
103, we will need to change

149
00:10:44.800 --> 00:10:50.380
the schema of the target's relation
to a list instead of an atomic number

150
00:10:50.380 --> 00:10:53.290
to avoid creating multiple tuples for
the same entity.

151
00:10:55.990 --> 00:11:00.380
As we saw, the schema of adding
process is a task of figuring out

152
00:11:00.380 --> 00:11:04.850
how elements of the schema from two
sources would relate to each other and

153
00:11:04.850 --> 00:11:07.940
determining how they would
map the target schema.

154
00:11:08.980 --> 00:11:13.180
You also saw that this is not really
a simple process, that we are trying to

155
00:11:13.180 --> 00:11:18.600
produce one integrated relation using
a couple of relations from each source.

156
00:11:20.130 --> 00:11:25.250
In a Big Data situation,
there are dozens of data sources, or

157
00:11:25.250 --> 00:11:31.880
more because the company's growing and
each source may have a few hundred tables.

158
00:11:31.880 --> 00:11:35.370
So it becomes very hard to actually solve

159
00:11:35.370 --> 00:11:40.419
this correspondence-making problem
completely and accurately just because

160
00:11:40.419 --> 00:11:44.920
the number of combinations one has to
go through is really, really high.

161
00:11:47.120 --> 00:11:52.250
One practical way to tackle this problem
is not to do a full-scale detail

162
00:11:52.250 --> 00:11:57.147
integration in the beginning but
adopt what's called a pay-as-you-go model.

163
00:11:57.147 --> 00:12:00.189
The pay-as-you-go data
management principle is simple.

164
00:12:01.380 --> 00:12:06.214
The system should provide some basic
integration services at the outset and

165
00:12:06.214 --> 00:12:11.123
then evolve the schema mappings between
the different sources on an as needed

166
00:12:11.123 --> 00:12:12.660
basis.

167
00:12:12.660 --> 00:12:17.190
So given a query, the system should
generate a best effort or approximate

168
00:12:17.190 --> 00:12:21.310
answers from the data sources for
a perfect schema mappings do not exist.

169
00:12:22.310 --> 00:12:26.015
When it discovers a large number
of sophisticated queries or

170
00:12:26.015 --> 00:12:30.375
data mining tasks over certain sources,
it will guide the users to make

171
00:12:30.375 --> 00:12:34.387
additional efforts to integrate
these sources more precisely.

172
00:12:36.401 --> 00:12:40.270
Okay, so how does the first
approximate schema mapping performed?

173
00:12:41.660 --> 00:12:46.920
One approach to do this is called
Probabilistic Schema Mapping.

174
00:12:46.920 --> 00:12:48.920
We'll describe it in more detail next.

175
00:12:51.120 --> 00:12:52.450
In the previous step,

176
00:12:52.450 --> 00:12:58.050
we just decided to create the disk count
candidates rrelation in an ad hoc way.

177
00:12:58.050 --> 00:12:59.990
And in a Big Data situation,

178
00:12:59.990 --> 00:13:03.920
we need to carefully determine
what the integrated schema,

179
00:13:03.920 --> 00:13:08.000
also called mediated schemas, should be,
and we should evaluate them properly.

180
00:13:09.640 --> 00:13:15.550
Since our toy company is trying
to create a single customer view,

181
00:13:15.550 --> 00:13:19.660
it's natural to create an integrated
table called customers.

182
00:13:19.660 --> 00:13:21.690
But how can we design this table?

183
00:13:21.690 --> 00:13:22.550
Here are some options.

184
00:13:23.820 --> 00:13:29.487
We can create the customer table to
include individuals and corporations and

185
00:13:29.487 --> 00:13:34.384
then use a flag called customer
type to distinguish between them.

186
00:13:34.384 --> 00:13:39.051
Now in the mediated schema then,
the individuals first name,

187
00:13:39.051 --> 00:13:43.191
middle initial, last name,
policyholder's name and

188
00:13:43.191 --> 00:13:48.140
corporation's name would all
map to Customer_Name similarly.

189
00:13:49.390 --> 00:13:53.978
The individual's full address,
the corporation's registered address,

190
00:13:53.978 --> 00:13:57.440
and the policyholder's address,
plus city, plus state,

191
00:13:57.440 --> 00:14:01.530
plus zip would all map
to customer address.

192
00:14:01.530 --> 00:14:06.010
Now we can enumerate all such choices of
which attributes to group together and

193
00:14:06.010 --> 00:14:09.230
map for each single attribute
in the target schema.

194
00:14:10.620 --> 00:14:15.630
But no matter how you'll do it,
it will never be a perfect fit because

195
00:14:15.630 --> 00:14:18.590
not all these combinations
would go well together.

196
00:14:18.590 --> 00:14:22.160
For example, should that date of
birth be included in this table?

197
00:14:23.550 --> 00:14:24.870
Would it make sense for corporations?

198
00:14:26.810 --> 00:14:32.310
In Probabilistic Mediated Schema Design,
we answer this question by

199
00:14:32.310 --> 00:14:35.410
associating probability values
with each of these options.

200
00:14:38.040 --> 00:14:42.410
To compute these values, we need to
quantify the relationships between

201
00:14:42.410 --> 00:14:46.202
attributes by figuring out which
attributes should be grouped or

202
00:14:46.202 --> 00:14:48.710
clustered together?

203
00:14:48.710 --> 00:14:52.140
Now two pieces of information
available in the source schemas

204
00:14:52.140 --> 00:14:55.497
can serve as evidence for
attribute clustering.

205
00:14:55.497 --> 00:14:59.740
One, the parallel similarity
of source attributes, and two,

206
00:14:59.740 --> 00:15:04.640
statistical properties
of service attributes.

207
00:15:05.800 --> 00:15:10.260
The first piece of information indicates
when two attributes are likely to be

208
00:15:10.260 --> 00:15:14.459
similar and is used for
creating multiple mediated schemas.

209
00:15:15.680 --> 00:15:19.050
One can apply a collection of
attribute matching modules

210
00:15:19.050 --> 00:15:21.650
to compute pairwise similarity.

211
00:15:21.650 --> 00:15:23.755
For example, individual names and

212
00:15:23.755 --> 00:15:26.700
policyholder names
are possibly quite similar.

213
00:15:27.930 --> 00:15:32.878
Are individual names versus
corporation names similar?

214
00:15:32.878 --> 00:15:37.283
Now, the similarity between two
source attributes, EIN and EZ,

215
00:15:37.283 --> 00:15:42.560
measure how closely the two attributes
represent the same real world concept.

216
00:15:44.520 --> 00:15:47.230
The second piece of information indicates

217
00:15:47.230 --> 00:15:51.650
when two attributes are likely
to be different, and is used for

218
00:15:51.650 --> 00:15:54.599
assigning probabilities to
each of the mediated schemas.

219
00:15:56.370 --> 00:15:59.770
For example, date of birth and

220
00:15:59.770 --> 00:16:02.410
corporation name possibly
will never co-occur together.

221
00:16:04.050 --> 00:16:07.250
But for
large schemas with large data volumes,

222
00:16:07.250 --> 00:16:11.980
one can estimate these measures by
taking samples from the actual database

223
00:16:11.980 --> 00:16:16.312
to come up with reasonable
similarity co-occurrence scores.

224
00:16:16.312 --> 00:16:22.658
To illustrate attribute regrouping, we
take a significant re-simplified example.

225
00:16:22.658 --> 00:16:27.543
Here we want to create customer
transactions as a mediated relation

226
00:16:27.543 --> 00:16:32.180
based upon the bank transactions and
insurance transactions.

227
00:16:33.530 --> 00:16:36.159
Each attribute is given
an abbreviation for simplicity.

228
00:16:38.030 --> 00:16:41.575
Below, you can see three
possible mediated schemas.

229
00:16:42.770 --> 00:16:47.140
In the first one,
the transaction begin and end times

230
00:16:47.140 --> 00:16:52.270
from bank transactions are grouped into
the same cluster as transaction date time

231
00:16:52.270 --> 00:16:56.040
from the insurance transactions,
because all of them are the same type.

232
00:16:57.280 --> 00:17:02.000
Similarly, transaction party, that
means who is giving or receiving money,

233
00:17:03.010 --> 00:17:06.610
and transaction description are grouped
together with transaction details.

234
00:17:08.490 --> 00:17:11.130
The second schema keeps
all of them separate.

235
00:17:12.320 --> 00:17:16.145
And the third candidate schema
groups some of them and not others.

236
00:17:19.998 --> 00:17:23.860
Now that we have multiple mediated
schemas, which one should we choose?

237
00:17:25.030 --> 00:17:27.990
Now I'm presenting here
a qualitative account of the method.

238
00:17:29.280 --> 00:17:32.730
The primary goal is to look for
what we can call consistency.

239
00:17:34.430 --> 00:17:38.260
A source schema is consistent
with a mediated schema if

240
00:17:38.260 --> 00:17:43.070
two different attributes of the source
schema do not occur in one cluster.

241
00:17:44.190 --> 00:17:48.990
And in the example, Med3,
that means related schema three,

242
00:17:48.990 --> 00:17:54.560
is more consistent with bank
transactions because, unlike Med1,

243
00:17:54.560 --> 00:17:58.700
it keeps TBT,
TET in two different clusters.

244
00:17:59.800 --> 00:18:06.090
Once this is done, we can count the number
of consistent sources for each candidate

245
00:18:06.090 --> 00:18:10.650
mediated schema and then use this count
to come up with a probability estimate.

246
00:18:11.890 --> 00:18:16.100
This estimate can then be used
to choose the k best schemas.

247
00:18:17.590 --> 00:18:22.030
Should one ever choose more
than one just best schema?

248
00:18:22.030 --> 00:18:23.860
Well, that's a hard question
to answer in general.

249
00:18:25.280 --> 00:18:29.932
It is done when the top capability
estimates are very close to each other.

1
00:00:00.025 --> 00:00:04.576
[SOUND] The Splunk platform for
operational intelligence is

2
00:00:04.576 --> 00:00:10.493
a revolutionary suite of products that
is unlocking unprecedented value for

3
00:00:10.493 --> 00:00:13.779
thousands of customers around the world.

4
00:00:13.779 --> 00:00:15.970
Why Splunk?

5
00:00:15.970 --> 00:00:18.420
It all starts with machine data.

6
00:00:18.420 --> 00:00:20.970
Machine data is the big data generated by

7
00:00:20.970 --> 00:00:23.710
all the technologies that
power our businesses.

8
00:00:23.710 --> 00:00:28.320
From the applications, servers, websites,
and network devices in the data center and

9
00:00:28.320 --> 00:00:31.300
the cloud, to the mobile device
in the palm of your hand.

10
00:00:32.340 --> 00:00:37.460
Thermostats, train sensors, electric cars,
and the Internet of things.

11
00:00:37.460 --> 00:00:39.412
Machine data is everywhere.

12
00:00:39.412 --> 00:00:41.710
It's fast-growing and complex.

13
00:00:41.710 --> 00:00:44.550
It's also incredibly valuable, why?

14
00:00:44.550 --> 00:00:48.630
Because it contains a definitive
record of all activity and behavior.

15
00:00:49.970 --> 00:00:54.380
Splunk software collects and
indexes this data at massive scale,

16
00:00:54.380 --> 00:00:58.340
from wherever it's generated,
regardless of format or source.

17
00:00:58.340 --> 00:01:02.290
Users can quickly and
easily monitor, search, analyze, and

18
00:01:02.290 --> 00:01:05.680
report on their data, all in real time.

19
00:01:05.680 --> 00:01:07.640
Machine data is different.

20
00:01:07.640 --> 00:01:11.450
It can't be processed and
analyzed using traditional methods.

21
00:01:11.450 --> 00:01:16.540
Splunk software does not rely on brittle
schemas and inflexible databases.

22
00:01:16.540 --> 00:01:19.660
Splunk is easy to deploy, easy to use, and

23
00:01:19.660 --> 00:01:25.290
easy to scale, whether on premises or
in a public, private, or hybrid cloud.

24
00:01:25.290 --> 00:01:27.960
Splunk is also available
as a cloud service.

25
00:01:27.960 --> 00:01:31.942
And for big data environments that
used Hadoop for cheap app storage,

26
00:01:31.942 --> 00:01:34.522
we have Hunk, Splunk analytics for Hadoop.

27
00:01:34.522 --> 00:01:36.950
[MUSIC]

28
00:01:36.950 --> 00:01:39.843
Our customers from around
the world illustrate why so

29
00:01:39.843 --> 00:01:42.070
many organizations use Splunk.

30
00:01:42.070 --> 00:01:46.710
Intuit has standardized on Splunk,
delivering operational visibility for

31
00:01:46.710 --> 00:01:52.060
their leading online products, including
QuickBooks, Quicken, and TurboTax.

32
00:01:52.060 --> 00:01:55.770
Intuit considers Splunk one of
their cornerstone technologies

33
00:01:55.770 --> 00:01:59.950
that is helping them innovate and
deliver better service to their customers.

34
00:01:59.950 --> 00:02:03.210
Cisco, one of the world's
largest technology providers,

35
00:02:03.210 --> 00:02:06.940
empowers their global security
team with Splunk enterprise

36
00:02:06.940 --> 00:02:11.220
to gain a centralized view into
end user and system activities.

37
00:02:11.220 --> 00:02:15.000
Splunk has dramatically helped
improve their incident detection and

38
00:02:15.000 --> 00:02:15.690
response rate.

39
00:02:17.000 --> 00:02:20.650
Splunk was key to Domino's Pizza's
success during the Super Bowl,

40
00:02:20.650 --> 00:02:24.020
by monitoring database uptime and
order response.

41
00:02:24.020 --> 00:02:26.710
Armed with new levels of
customer understanding,

42
00:02:26.710 --> 00:02:30.490
Domino's was able to strengthen their
online business, in addition to saving

43
00:02:30.490 --> 00:02:34.960
hundreds of thousands of dollars replacing
legacy technologies with Splunk.

44
00:02:34.960 --> 00:02:39.350
We are thrilled to hear Domino's
call Splunk their secret sauce.

45
00:02:39.350 --> 00:02:43.200
Another great Splunk story includes
cars and the Internet of things.

46
00:02:44.810 --> 00:02:48.813
Working together with the Ford Motor
Company and Ford's OpenXC platform,

47
00:02:48.813 --> 00:02:52.816
Splunk delivered connected car dashboards
to examine driving behavior and

48
00:02:52.816 --> 00:02:54.079
vehicle performance.

49
00:02:54.079 --> 00:02:58.529
UK-based Tesco is one of
the world's largest retailers,

50
00:02:58.529 --> 00:03:04.650
with nearly 1 million customers and
a half billion online orders per week.

51
00:03:04.650 --> 00:03:07.480
Customer satisfaction
is a critical metric.

52
00:03:07.480 --> 00:03:12.580
Tesco deployed Splunk to gain a unified
view across their websites, transactions,

53
00:03:12.580 --> 00:03:17.410
and business, gaining valuable digital
intelligence about their customers.

54
00:03:17.410 --> 00:03:22.820
Splunk was founded to pursue a disruptive
vision to make machine data accessible,

55
00:03:22.820 --> 00:03:25.490
usable, and valuable to everyone.

56
00:03:25.490 --> 00:03:27.280
Find out more about Splunk and

57
00:03:27.280 --> 00:03:30.820
operational intelligence by
downloading the executive summary.

58
00:03:30.820 --> 00:03:31.660
Or better yet,

59
00:03:31.660 --> 00:03:36.230
experience the value firsthand by
downloading Splunk software for free.

60
00:03:36.230 --> 00:03:39.565
Chances are someone in your
organization already has.

61
00:03:39.565 --> 00:03:45.649
[SOUND]

1
00:00:02.630 --> 00:00:04.630
Aggregations in Big Data Pipelines.

2
00:00:07.220 --> 00:00:10.680
After this video you will
be able to compare and

3
00:00:10.680 --> 00:00:15.740
select the aggregation operation that
you require to solve your problem.

4
00:00:15.740 --> 00:00:21.650
Explain how you can use aggregations to
compact your dataset and reduce volume.

5
00:00:21.650 --> 00:00:22.890
That is in many cases.

6
00:00:24.260 --> 00:00:29.586
And design complex operations in your
pipeline, using a series of aggregations.

7
00:00:32.313 --> 00:00:39.189
Aggregation is any operation on a data set
that performs a specific transformation,

8
00:00:39.189 --> 00:00:43.979
taking all the related data
elements into consideration.

9
00:00:45.060 --> 00:00:49.550
Let's say we have a bunch of stars
which are of different colors.

10
00:00:50.570 --> 00:00:54.550
Different colors denote diversity or
variety in the data.

11
00:00:56.260 --> 00:01:02.610
To keep things simple, we will use
letter 'f' to denote a transformation.

12
00:01:03.700 --> 00:01:04.720
In the following slides,

13
00:01:04.720 --> 00:01:10.360
we will see examples of how 'f' can take
the shape of different transformations.

14
00:01:13.550 --> 00:01:18.390
If we apply a transformation that does
something using the information of

15
00:01:18.390 --> 00:01:22.430
all the stars here,
we are performing an aggregation.

16
00:01:24.070 --> 00:01:29.850
Loosely speaking, we can say
that applying a transformation 'f'

17
00:01:29.850 --> 00:01:34.830
that takes all the elements of data
as input is called 'aggregation'.

18
00:01:38.490 --> 00:01:44.400
One of the simplest aggregations is
summation over all the data elements.

19
00:01:44.400 --> 00:01:48.750
In this case,
let's say every star counted as 1.

20
00:01:48.750 --> 00:01:52.586
Summing over all the stars gives 14,

21
00:01:52.586 --> 00:01:57.119
which is the summation of 3 stars for
yellow,

22
00:01:57.119 --> 00:02:01.897
5 stars for green, and
6 stars for color pink.

23
00:02:04.779 --> 00:02:10.513
Another aggregation that you could perform
is summation of individual star colors,

24
00:02:10.513 --> 00:02:13.070
that is, grouping the sums by color.

25
00:02:14.200 --> 00:02:20.360
So, If each star is a 1,
adding each group will result in 3 for

26
00:02:20.360 --> 00:02:25.530
yellow stars, 5 for green stars,
and 6 for pink stars.

27
00:02:26.860 --> 00:02:27.952
In this case,

28
00:02:27.952 --> 00:02:33.611
the aggregation function 'f' will output
3 tuples of star colors and counts.

29
00:02:36.065 --> 00:02:41.190
In a sales scenario, each color could
denote a different product type.

30
00:02:42.470 --> 00:02:47.320
And the number 1 could be replaced
by revenue generated by a product

31
00:02:47.320 --> 00:02:49.230
in each city the product is sold.

32
00:02:50.600 --> 00:02:54.250
In fact,
we will keep coming back to this analogy.

33
00:02:56.420 --> 00:03:01.188
You can also perform average
over items of similar kind,

34
00:03:01.188 --> 00:03:06.150
such as sums grouped by color.

35
00:03:08.060 --> 00:03:10.630
Continuing the example earlier,

36
00:03:10.630 --> 00:03:15.060
you can calculate average revenue per
product type using this aggregation.

37
00:03:17.530 --> 00:03:22.270
Other simple yet useful aggregational
operations to help you extract meaning

38
00:03:22.270 --> 00:03:29.010
from large data sets are maximum,
minimum, and standard deviation.

39
00:03:29.010 --> 00:03:34.090
Remember, you can always perform
aggregation as a series of operations,

40
00:03:34.090 --> 00:03:38.140
such as maximum of the sums per product.

41
00:03:38.140 --> 00:03:41.510
That is, summation followed by maximum.

42
00:03:42.725 --> 00:03:46.920
If you first sum sales for
each city, that is for

43
00:03:46.920 --> 00:03:51.640
each product,
then you can take the maximum of it

44
00:03:51.640 --> 00:03:57.050
by applying maximum function to
the result of the summation function.

45
00:03:57.050 --> 00:04:01.870
In this case, you get the product,
which has maximum sales in the country.

46
00:04:04.040 --> 00:04:08.084
Aggregation over Boolean data
sets that can have true-false or

47
00:04:08.084 --> 00:04:14.430
one-zero values could be a complex mixture
of AND, OR, and NOT logical operations.

48
00:04:16.030 --> 00:04:20.110
A lot of problems become easy
to manipulate using sets.

49
00:04:20.110 --> 00:04:24.008
Because sets don't allow duplicate values.

50
00:04:24.008 --> 00:04:27.710
Depending on your application,
this could be very useful.

51
00:04:28.800 --> 00:04:33.730
For example, to count the number
of products from a sales table,

52
00:04:33.730 --> 00:04:38.920
you can simply take all
the sales tables and create sets

53
00:04:38.920 --> 00:04:44.130
of these products in those tables,
and take a union of these sets.

54
00:04:46.090 --> 00:04:52.130
To summarize, by choosing the right
aggregation, you can generate compact and

55
00:04:52.130 --> 00:04:58.720
meaningful insights that enable faster and
effective decision making in business.

56
00:04:58.720 --> 00:05:00.960
You will find that in most cases,

57
00:05:00.960 --> 00:05:04.270
aggregation results in
smaller output data sets.

58
00:05:05.450 --> 00:05:10.140
Hence, aggregation is an important tool
set to keep in pocket when dealing with

59
00:05:10.140 --> 00:05:12.669
large data sets, and big data pipelines.

1
00:00:02.723 --> 00:00:05.960
Big Data Processing Pipelines:
A Dataflow Approach.

2
00:00:07.160 --> 00:00:11.420
Most big data applications
are composed of a set of operations

3
00:00:11.420 --> 00:00:13.980
executed one after another as a pipeline.

4
00:00:14.980 --> 00:00:17.520
Data flows through these operations,

5
00:00:17.520 --> 00:00:20.177
going through various
transformations along the way.

6
00:00:20.177 --> 00:00:24.080
We also call this dataflow graphs.

7
00:00:24.080 --> 00:00:27.625
So to understand big data
processing we should start by

8
00:00:27.625 --> 00:00:30.100
understanding what dataflow means.

9
00:00:31.820 --> 00:00:36.781
After this video you will be able to
summarize what dataflow means and

10
00:00:36.781 --> 00:00:38.844
it's role in data science.

11
00:00:38.844 --> 00:00:44.096
Explain split->do->merge as a big
data pipeline with examples,

12
00:00:44.096 --> 00:00:46.770
and define the term data parallel.

13
00:00:48.910 --> 00:00:53.484
Let's consider the hello
world MapReduce example for

14
00:00:53.484 --> 00:00:57.461
WordCount which reads one or
more text files and

15
00:00:57.461 --> 00:01:03.043
counts the number of occurrences
of each word in these text files.

16
00:01:03.043 --> 00:01:07.483
You are by now very familiar with
this example, but as a reminder,

17
00:01:07.483 --> 00:01:10.891
the output will be a text
file with a list of words and

18
00:01:10.891 --> 00:01:14.250
their occurrence frequencies
in the input data.

19
00:01:16.451 --> 00:01:20.985
In this application,
the files were first split into HDFS

20
00:01:20.985 --> 00:01:25.990
cluster nodes as partitions of
the same file or multiple files.

21
00:01:27.560 --> 00:01:30.680
Then a map operation, in this case,

22
00:01:30.680 --> 00:01:36.340
a user defined function to count words
was executed on each of these nodes.

23
00:01:36.340 --> 00:01:42.460
And all the key values that were output
from map were sorted based on the key.

24
00:01:42.460 --> 00:01:47.380
And the key values with the same word
were moved or shuffled to the same node.

25
00:01:48.920 --> 00:01:54.130
Finally, the reduce operation
was executed on these nodes

26
00:01:54.130 --> 00:01:57.900
to add the values for
key-value pairs with the same keys.

27
00:01:59.920 --> 00:02:05.880
If you look back at this example, we see
that there were four distinct steps,

28
00:02:05.880 --> 00:02:11.070
namely the data split step,
the map step, the shuffle and

29
00:02:11.070 --> 00:02:13.030
sort step, and the reduce step.

30
00:02:14.310 --> 00:02:18.417
Although, the word count
example is pretty simple it

31
00:02:18.417 --> 00:02:22.980
represents a large number of
applications that these three

32
00:02:22.980 --> 00:02:27.652
steps can be applied to achieve
data parallel scalability.

33
00:02:27.652 --> 00:02:33.290
We refer in general to this
pattern as "split-do-merge".

34
00:02:35.040 --> 00:02:41.490
In these applications, data flows
through a number of steps, going through

35
00:02:41.490 --> 00:02:46.990
transformations with various scalability
needs, leading to a final product.

36
00:02:48.180 --> 00:02:50.310
The data first gets partitioned.

37
00:02:51.430 --> 00:02:56.450
The split data goes through a set of
user-defined functions to do something,

38
00:02:57.840 --> 00:03:02.420
ranging from statistical operations to
data joins to machine learning functions.

39
00:03:03.870 --> 00:03:08.510
Depending on the application's
data processing needs,

40
00:03:08.510 --> 00:03:13.350
these "do something" operations can
differ and can be chained together.

41
00:03:14.620 --> 00:03:20.130
In the end results can be combined
using a merging algorithm or

42
00:03:20.130 --> 00:03:22.300
a higher-order function like reduce.

43
00:03:23.580 --> 00:03:27.520
We call the stitched-together
version of these sets of steps for

44
00:03:27.520 --> 00:03:30.970
big data processing "big data pipelines".

45
00:03:33.700 --> 00:03:38.180
The term pipe comes from
a UNIX separation that

46
00:03:38.180 --> 00:03:43.490
the output of one running program gets
piped into the next program as an input.

47
00:03:43.490 --> 00:03:47.870
As you might imagine,
one can string multiple programs together

48
00:03:47.870 --> 00:03:52.560
to make longer pipelines with various
scalability needs at each step.

49
00:03:53.740 --> 00:03:56.750
However, for big data processing,

50
00:03:56.750 --> 00:04:02.110
the parallelism of each step in
the pipeline is mainly data parallelism.

51
00:04:02.110 --> 00:04:07.284
We can simply define data parallelism
as running the same functions

52
00:04:07.284 --> 00:04:13.380
simultaneously for the elements or
partitions of a dataset on multiple cores.

53
00:04:13.380 --> 00:04:17.447
For example,
in our word count example, data

54
00:04:17.447 --> 00:04:21.720
parallelism occurs in every
step of the pipeline.

55
00:04:22.770 --> 00:04:26.530
There's definitely parallelization
during map over the input

56
00:04:26.530 --> 00:04:30.460
as each partition gets
processed as a line at a time.

57
00:04:30.460 --> 00:04:32.920
To achieve this type of data parallelism,

58
00:04:32.920 --> 00:04:37.560
we must decide on the data granularity
of each parallel computation.

59
00:04:37.560 --> 00:04:38.930
In this case, it is a line.

60
00:04:41.022 --> 00:04:46.560
We also see a parallel grouping of
data in the shuffle and sort phase.

61
00:04:46.560 --> 00:04:50.630
This time, the parallelization is
over the intermediate products,

62
00:04:50.630 --> 00:04:53.050
that is, the individual key-value pairs.

63
00:04:55.140 --> 00:04:58.810
And after the grouping of
the intermediate products

64
00:04:58.810 --> 00:05:03.270
the reduce step gets parallelized
to construct one output file.

65
00:05:04.360 --> 00:05:08.470
You have probably noticed that
the data gets reduced to a smaller set

66
00:05:08.470 --> 00:05:09.060
at each step.

67
00:05:11.370 --> 00:05:13.890
Although, the example we have given is for

68
00:05:13.890 --> 00:05:18.020
batch processing, similar techniques
apply to stream processing.

69
00:05:19.060 --> 00:05:20.740
Let's discuss this for

70
00:05:20.740 --> 00:05:24.690
our simplified advanced stream
data from an online game example.

71
00:05:25.910 --> 00:05:30.070
In this case, your event gets ingested

72
00:05:30.070 --> 00:05:34.960
through a real time big data ingestion
engine, like Kafka or Flume.

73
00:05:36.380 --> 00:05:40.400
Then they get passed into
a Streaming Data Platform for

74
00:05:40.400 --> 00:05:44.670
processing like Samza,
Storm or Spark streaming.

75
00:05:45.750 --> 00:05:51.910
This is a valid choice for processing
data one event at a time or chunking

76
00:05:51.910 --> 00:05:57.390
the data into Windows or Microbatches
of time or other features.

77
00:05:58.940 --> 00:06:04.470
Any pipeline processing of data can
be applied to the streaming data here

78
00:06:04.470 --> 00:06:07.410
as we wrote in a batch-
processing Big Data engine.

79
00:06:09.350 --> 00:06:15.034
The process stream data can then be
served through a real-time view or

80
00:06:15.034 --> 00:06:17.279
a batch-processing view.

81
00:06:17.279 --> 00:06:22.380
Real-time view is often subject to change
as potentially delayed new data comes in.

82
00:06:23.590 --> 00:06:28.490
The storage of the data can be
accomplished using H-Base, Cassandra,

83
00:06:28.490 --> 00:06:32.660
HDFS, or
many other persistent storage systems.

84
00:06:34.120 --> 00:06:39.110
To summarize, big data pipelines
get created to process data

85
00:06:39.110 --> 00:06:43.990
through an aggregated set of steps that
can be represented with the split-

86
00:06:43.990 --> 00:06:48.370
do-merge pattern with
data parallel scalability.

87
00:06:48.370 --> 00:06:51.040
This pattern can be
applied to many batch and

88
00:06:51.040 --> 00:06:54.100
streaming data processing applications.

89
00:06:54.100 --> 00:06:58.958
Next we will go through some processing
steps in a big data pipeline in

90
00:06:58.958 --> 00:07:03.589
more detail, first conceptually,
then practically in Spark.
