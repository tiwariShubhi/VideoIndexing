
1
00:00:02.370 --> 00:00:05.969
This is the second hands on exercises for
CSV data.

2
00:00:05.969 --> 00:00:10.155
In the first, we saw how to import
a CSV file into a spreadsheet and

3
00:00:10.155 --> 00:00:12.430
make a simple plot.

4
00:00:12.430 --> 00:00:15.240
In this one,
we will learn how to filter data and

5
00:00:15.240 --> 00:00:16.830
perform some aggregate operations.

6
00:00:18.390 --> 00:00:21.659
We will begin by opening a terminal
window and starting a spreadsheet.

7
00:00:22.800 --> 00:00:26.440
Next, we will load the CSV
data into the spreadsheet and

8
00:00:26.440 --> 00:00:29.490
perform a filter over several columns.

9
00:00:29.490 --> 00:00:32.398
Finally, we will calculate an average and
sum from the data.

10
00:00:32.398 --> 00:00:35.313
Let's begin.

11
00:00:35.313 --> 00:00:39.470
First, we open a terminal window by
clicking on the terminal icon in

12
00:00:39.470 --> 00:00:40.290
the top toolbar.

13
00:00:42.770 --> 00:00:46.296
We can start the spreadsheet
by writing oocalc.

14
00:00:51.338 --> 00:00:54.590
Next, let's load our CSV
data into the spreadsheet.

15
00:00:54.590 --> 00:01:02.142
We click on File > Open, CENSUS.CSV.

16
00:01:05.719 --> 00:01:08.426
And click OK on this
dialog to load the data.

17
00:01:12.387 --> 00:01:17.344
Column F in the spreadsheet is the state,
and

18
00:01:17.344 --> 00:01:22.704
column H is the census population for
2010.

19
00:01:25.110 --> 00:01:28.480
Let's create a filter that
just shows the data for

20
00:01:28.480 --> 00:01:32.920
California, for
counties larger than one million people.

21
00:01:34.100 --> 00:01:39.030
We can create this filter by first
selecting both the state name column and

22
00:01:39.030 --> 00:01:41.210
the census 2000 population column.

23
00:01:42.790 --> 00:01:48.054
Next we go to data,
filter, standard filter.

24
00:01:52.184 --> 00:01:56.025
Here we change the field
name to be the state name,

25
00:01:56.025 --> 00:02:01.061
the condition we leave at equals and
the value we use California.

26
00:02:06.318 --> 00:02:11.055
This filters all the rows,
unless the state is California.

27
00:02:15.143 --> 00:02:16.498
We then want to filter for

28
00:02:16.498 --> 00:02:20.590
all the counties whose population
is greater than one million people.

29
00:02:20.590 --> 00:02:24.400
To do that, in this second line here,
we change the operator to AND.

30
00:02:26.180 --> 00:02:31.710
The field name should be
census 2010 population and

31
00:02:31.710 --> 00:02:35.040
the condition should be greater than.

32
00:02:35.040 --> 00:02:36.431
Then set the value to be one million.

33
00:02:41.923 --> 00:02:44.893
And click OK.

34
00:02:44.893 --> 00:02:49.603
We can see that all the data from the
spreadsheet has disappeared except where

35
00:02:49.603 --> 00:02:51.559
the state name is California and

36
00:02:51.559 --> 00:02:54.762
the population is greater
than one million people.

37
00:02:56.310 --> 00:03:00.110
We can reset or remove this
filter to see all the data again

38
00:03:00.110 --> 00:03:04.240
by going to Data > Filter.

39
00:03:06.260 --> 00:03:07.020
Reset filter.

40
00:03:08.390 --> 00:03:11.550
You can perform aggregate operations
on the data in a spreadsheet.

41
00:03:13.160 --> 00:03:16.830
Next, let's perform some aggregate
operations over the data.

42
00:03:16.830 --> 00:03:19.220
We can compute the average and
the sum of the data.

43
00:03:20.460 --> 00:03:23.980
To do this, let's run these
calculations in a separate sheet.

44
00:03:25.470 --> 00:03:28.181
Create a new sheet by clicking
on the green plus button.

45
00:03:31.854 --> 00:03:37.656
To compute the average we select
a cell and enter =Average( and

46
00:03:37.656 --> 00:03:43.470
then we select the data that we
want to compute the average from.

47
00:03:44.870 --> 00:03:46.150
If we go back to sheet one.

48
00:03:46.150 --> 00:03:50.190
We can select some of
the data from the H column.

49
00:03:50.190 --> 00:03:54.634
So let's just choose several
counties in Alabama.

50
00:03:54.634 --> 00:03:58.267
When we hit Enter,
it takes us back to sheet one and

51
00:03:58.267 --> 00:04:01.306
we can see that the average is computing.

52
00:04:03.513 --> 00:04:12.220
Similarly, we can compute the sum
by entering =sum, open parentheses.

53
00:04:12.220 --> 00:04:16.120
Going back to sheet one and
selecting the columns we want to sum.

54
00:04:16.120 --> 00:04:21.360
When we're done hit Enter and
the sum is computed.

1
00:00:02.360 --> 00:00:05.130
In this hands on exercise,
we will be looking at JSON data.

2
00:00:07.040 --> 00:00:09.600
First, we will open a terminal window and

3
00:00:09.600 --> 00:00:13.300
then look at the contents of a JSON
file containing tweets from Twitter.

4
00:00:14.860 --> 00:00:17.800
Next, we will examine
the schema of this JSON file.

5
00:00:18.810 --> 00:00:21.920
Finally, we will extract different
fields from the JSON data.

6
00:00:24.340 --> 00:00:25.470
Let's begin.

7
00:00:25.470 --> 00:00:27.307
First, we'll open a terminal window,

8
00:00:27.307 --> 00:00:29.964
by clicking on the terminal
icon at the top of the toolbar.

9
00:00:37.240 --> 00:00:42.005
Next, we'll see cd into the directory
containing the json data,

10
00:00:42.005 --> 00:00:45.594
by running cddownload/big/data/2/json.

11
00:00:52.298 --> 00:00:54.520
We can run ls, to see the json file.

12
00:00:58.836 --> 00:01:04.030
The json file is called twitter.json.

13
00:01:04.030 --> 00:01:07.980
We can run more twitter.json to
view the contents of this file.

14
00:01:16.280 --> 00:01:20.990
The json data contains semi-structured
data, which is nested several levels.

15
00:01:20.990 --> 00:01:25.350
There are many tweets in this file, and
it's hard to read using the more command.

16
00:01:25.350 --> 00:01:29.690
You can use space to scroll down and
when we're done, hit q.

17
00:01:32.420 --> 00:01:37.358
We can run the jsonschema.pi command
to view the schema of this data.

18
00:01:37.358 --> 00:01:42.179
We run jsonschema.pitwitter.json

19
00:01:49.715 --> 00:01:52.670
And we'll add a pipe more at the end.

20
00:01:56.665 --> 00:02:00.572
This shows the nested
fields within this data,

21
00:02:00.572 --> 00:02:06.139
at the top level there are fields
like contributors text and id and so

22
00:02:06.139 --> 00:02:11.900
on, but there are also fields nested
within these top level fields for

23
00:02:11.900 --> 00:02:17.372
example entities also contains
called symbols media hashtags and

24
00:02:17.372 --> 00:02:20.691
so on If we scroll down by hitting space,

25
00:02:20.691 --> 00:02:25.270
we'll see that there's
several levels of nesting.

26
00:02:25.270 --> 00:02:32.320
For example, user also has
follow_request_sent, id, and so on.

27
00:02:36.105 --> 00:02:40.466
We can run the print json script to view
the contents of a particular tweet and

28
00:02:40.466 --> 00:02:42.746
a particular field within that tweet.

29
00:02:42.746 --> 00:02:47.018
Let's run print_json.py.

30
00:02:47.018 --> 00:02:50.812
It asks for the file name so

31
00:02:50.812 --> 00:02:55.300
we'll enter twitter.json.

32
00:02:58.407 --> 00:03:01.434
And we'll look at tweet 99.

33
00:03:03.645 --> 00:03:06.340
So let's look at the top
level field called text.

34
00:03:10.506 --> 00:03:14.884
So here we see the text for
note the 99th tweet in this file.

35
00:03:14.884 --> 00:03:19.110
We could also look at a nested field
within the file, by running print_json

36
00:03:19.110 --> 00:03:24.276
again The file name is twitter.json.

37
00:03:26.975 --> 00:03:28.833
We'll look at tweet 99 again.

38
00:03:32.097 --> 00:03:35.240
And we'll look at the field
entities hashtags.

39
00:03:35.240 --> 00:03:39.565
The hashtags that are embedded or
nested within the entities field.

1
00:00:02.390 --> 00:00:08.427
This is the first of two hands
on exercises involving CSV Data.

2
00:00:08.427 --> 00:00:12.568
In this exercise, we will import
a CSV file into a spreadsheet and

3
00:00:12.568 --> 00:00:13.860
make a simple plot.

4
00:00:15.290 --> 00:00:20.350
We will begin by opening a terminal window
and looking at a CSV file in the terminal.

5
00:00:20.350 --> 00:00:23.330
Next, we will start
the spreadsheet application and

6
00:00:23.330 --> 00:00:25.410
import the CSV data into the spreadsheet.

7
00:00:25.410 --> 00:00:30.760
We can then look at the rows and columns
of the CSV file and make a simple plot.

8
00:00:33.088 --> 00:00:35.837
Let's begin, first,
open a terminal shell by

9
00:00:35.837 --> 00:00:39.423
clicking on the black terminal
icon at the top of the toolbar.

10
00:00:47.759 --> 00:00:52.914
Next, let's cd into the directory
containing the CSV data.

11
00:00:52.914 --> 00:00:58.587
We'll run cd space
Download/big-data-2/csv.

12
00:00:58.587 --> 00:01:04.042
We can run ls to

13
00:01:04.042 --> 00:01:11.325
see the CSV files.

14
00:01:11.325 --> 00:01:16.190
The file census.csv contains
census data for the United States.

15
00:01:16.190 --> 00:01:21.367
We can run the command more census.csv
to see the contents of this file.

16
00:01:29.262 --> 00:01:32.925
The first line of this file is the header
with the columns separated by commas.

17
00:01:32.925 --> 00:01:37.705
You can go down in
the file by hitting space.

18
00:01:41.295 --> 00:01:42.735
Hit Q to quit more.

19
00:01:45.570 --> 00:01:48.210
Next, let's start
a spreadsheet application.

20
00:01:48.210 --> 00:01:52.446
We run oocalc to start this.

21
00:02:00.318 --> 00:02:04.453
We can import the census
data CSV file into

22
00:02:04.453 --> 00:02:08.598
the spreadsheet by going to File > Open.

23
00:02:12.568 --> 00:02:16.067
Clicking on downloads.

24
00:02:16.067 --> 00:02:22.468
Big Data 2, CSV, census.csv.

25
00:02:27.531 --> 00:02:29.109
In this dialog click OK.

26
00:02:44.945 --> 00:02:47.355
You can see into this spreadsheet,

27
00:02:47.355 --> 00:02:50.900
import of the CSV data to
a bunch of rows and columns.

28
00:02:52.880 --> 00:02:56.460
Each column that was separated
by a comma in the CSV file,

29
00:02:56.460 --> 00:02:58.580
is a column in the spreadsheet.

30
00:02:59.700 --> 00:03:02.590
We can see that our CSV file
was successfully imported into

31
00:03:02.590 --> 00:03:03.250
the spreadsheet.

32
00:03:05.650 --> 00:03:07.892
If we scroll down to
the bottom of the spreadsheet,

33
00:03:07.892 --> 00:03:10.094
we can see how many rows
there were in the CSV file.

34
00:03:19.939 --> 00:03:25.372
There are 3194 rows in the CSV file.

35
00:03:25.372 --> 00:03:29.149
If this file instead had millions or
10 millions of rows,

36
00:03:29.149 --> 00:03:33.237
then we would have to use a big
data system such as Hadoop or HDFS.

37
00:03:36.882 --> 00:03:38.089
Let's scroll back to the top.

38
00:03:43.592 --> 00:03:49.351
Next, let's make a simple plot of
some of the data in the CSV file.

39
00:03:49.351 --> 00:03:54.993
Let's plot the population estimates for
several years for the state of Alabama.

40
00:03:54.993 --> 00:04:00.558
The state of Alabama is
given in the second row and

41
00:04:00.558 --> 00:04:06.813
the population estimates
are given in these columns.

42
00:04:06.813 --> 00:04:12.225
Let's select J through O,
so you get the population

43
00:04:12.225 --> 00:04:16.657
estimate for 2010 through 2015.

44
00:04:20.086 --> 00:04:24.331
We can create a plot of these values
by clicking on the Chart button.

45
00:04:29.393 --> 00:04:32.806
And clicking finish.

46
00:04:32.806 --> 00:04:36.442
In the second hands on for CSV data,
we'll perform some filtering and

47
00:04:36.442 --> 00:04:38.669
some aggregate operations over the data.

1
00:00:02.170 --> 00:00:02.670
Welcome.

2
00:00:03.800 --> 00:00:06.200
In this module,
we'll talk about data models.

3
00:00:07.530 --> 00:00:10.770
If you completed the introductory
course of this specialization,

4
00:00:10.770 --> 00:00:12.940
you might recall our
video on data variety.

5
00:00:14.220 --> 00:00:16.840
One way to characterize data variety

6
00:00:16.840 --> 00:00:21.290
is to identify the different models of
data that are used in any application.

7
00:00:23.120 --> 00:00:24.480
So what is a data model?

8
00:00:24.480 --> 00:00:27.980
And why do we care about data
models in the context of big data?

9
00:00:27.980 --> 00:00:32.010
In this lesson, we'll introduce you to
three components of a data model and

10
00:00:32.010 --> 00:00:33.380
what they tell us about the data.

11
00:00:34.550 --> 00:00:39.620
So after this lesson, you'll be able
to distinguish between structured and

12
00:00:39.620 --> 00:00:41.270
unstructured data.

13
00:00:41.270 --> 00:00:46.880
Describe four basic data operations namely
selection, projection, union, and join.

14
00:00:46.880 --> 00:00:51.120
And enumerate different types of data
constraints like type, value and

15
00:00:51.120 --> 00:00:53.240
structural constraints.

16
00:00:53.240 --> 00:00:57.760
You'll also be able to explain why
constraints are useful to specify

17
00:00:57.760 --> 00:01:00.320
the semantics of data.

18
00:01:00.320 --> 00:01:04.980
Now, regardless of whether the data is big
or small, one needs to know or determine

19
00:01:04.980 --> 00:01:10.100
the characteristics of data before one can
manipulate or analyze them meaningfully.

20
00:01:10.100 --> 00:01:14.320
Let's use a simple example,
suppose you have data is

21
00:01:14.320 --> 00:01:19.330
a file of records with fields called first
name, last name and date of birth of

22
00:01:19.330 --> 00:01:24.840
the employees in the company that this
file consists of records with fields,

23
00:01:24.840 --> 00:01:29.290
and not for instance plain text,
gives us more insight

24
00:01:29.290 --> 00:01:34.770
into the organization of the data in the
file and hence is part of the data model.

25
00:01:34.770 --> 00:01:37.810
This aspect is called Structure.

26
00:01:37.810 --> 00:01:41.930
Similarly, the consideration
that we can perform

27
00:01:41.930 --> 00:01:44.980
data arithmetic with
the date of birth field, and

28
00:01:44.980 --> 00:01:50.340
not with the first name field, is also
part of our understanding of data model.

29
00:01:50.340 --> 00:01:52.620
These are called Operations.

30
00:01:52.620 --> 00:01:57.300
Finally, we may know that in
this company no one's age

31
00:01:57.300 --> 00:02:00.950
that is today's date minus the date
of birth, can't be less than 18.

32
00:02:00.950 --> 00:02:06.000
So it gives us a way to detect records
with blatantly erroneous dates of birth.

33
00:02:06.000 --> 00:02:07.940
In the following three videos,

34
00:02:07.940 --> 00:02:11.790
we'll look at these three aspects
of data models more carefully.

1
00:00:01.120 --> 00:00:05.140
We mentioned before that a data model
is characterized by the structure of

2
00:00:05.140 --> 00:00:09.430
the data that it admits,
the operations on that structure, and

3
00:00:09.430 --> 00:00:11.090
a way to specify constraints.

4
00:00:12.290 --> 00:00:13.750
In this lesson,

5
00:00:13.750 --> 00:00:18.950
we'll present a more detailed description
of a number of common data models.

6
00:00:18.950 --> 00:00:20.800
We'll start with relational data.

7
00:00:22.460 --> 00:00:26.780
It is one of the simplest and most
frequently used data models today, and

8
00:00:26.780 --> 00:00:30.320
forms the basis of many other
traditional database management systems,

9
00:00:30.320 --> 00:00:33.930
like MySQL, Oracle,
Teradata, and so forth.

10
00:00:35.480 --> 00:00:38.940
So after this video you'll be able to

11
00:00:38.940 --> 00:00:41.680
describe the structural components
of a relational data model.

12
00:00:43.320 --> 00:00:46.430
Demonstrate which components
become a data model's schema.

13
00:00:48.760 --> 00:00:51.760
Explain the purpose of primary and
foreign keys.

14
00:00:52.770 --> 00:00:55.370
And describe Join and other operations.

15
00:00:58.070 --> 00:00:59.610
The primary data structure for

16
00:00:59.610 --> 00:01:03.380
a relational model is a table, like
the one shown here for a toy application.

17
00:01:05.030 --> 00:01:09.740
But we need to be careful about relational
tables, which are also called relations.

18
00:01:11.110 --> 00:01:15.695
This table actually
represents a set of tuples.

19
00:01:17.550 --> 00:01:22.390
This is a relational tuple,
represented as a row in the table.

20
00:01:23.500 --> 00:01:26.140
We were informally calling
this a record before.

21
00:01:27.350 --> 00:01:33.110
But a relational tuple implies that unless
otherwise stated, the elements of it

22
00:01:33.110 --> 00:01:37.600
like 203 or 204, Mary and
so forth, are atomic.

23
00:01:38.950 --> 00:01:43.300
That is,
they represent one unit of information and

24
00:01:43.300 --> 00:01:44.740
cannot be decomposed further.

25
00:01:45.830 --> 00:01:48.150
We'll return to this issue
in the next few slides.

26
00:01:49.490 --> 00:01:53.935
Thus, this is a relation of six tuples.

27
00:01:55.468 --> 00:01:58.490
Remember, the definition of sets,

28
00:01:58.490 --> 00:02:03.270
it's a collection of distinct
elements of the same type.

29
00:02:04.420 --> 00:02:10.920
That means I cannot add
this tuple to the solution.

30
00:02:10.920 --> 00:02:14.350
Because if I do,
it will be introducing a duplicate.

31
00:02:15.820 --> 00:02:21.330
Now in practice, many systems will allow
duplicate tuples in a relation, but

32
00:02:21.330 --> 00:02:26.450
mechanisms are provided to prevent
duplicate entries if the user so chooses.

33
00:02:26.450 --> 00:02:28.170
So I cannot add it.

34
00:02:28.170 --> 00:02:31.170
Here is another tuple I cannot add.

35
00:02:31.170 --> 00:02:33.680
It has all the right
pieces of information, but

36
00:02:33.680 --> 00:02:35.410
it's all in the wrong order.

37
00:02:35.410 --> 00:02:39.300
So it is a tuple dissimilar with
the other six tuples in the relation.

38
00:02:40.300 --> 00:02:43.690
Okay, so how does the system know
that this tuple is different?

39
00:02:45.390 --> 00:02:49.700
This brings our attention to the very
first row that is the header of this table

40
00:02:49.700 --> 00:02:50.710
painted in black.

41
00:02:52.680 --> 00:02:55.910
This row is part of
the scheme of the table.

42
00:02:55.910 --> 00:02:56.420
Lets look at it.

43
00:02:57.890 --> 00:03:01.190
It tells us the name of the table,
in this case employee.

44
00:03:02.580 --> 00:03:06.560
This also tells us the names of the six
columns called attributes of the relation.

45
00:03:07.740 --> 00:03:09.090
And for each column,

46
00:03:09.090 --> 00:03:13.370
it tells us the allowed data type, that
is the type constraint for each column.

47
00:03:14.610 --> 00:03:16.340
Given this schema,

48
00:03:16.340 --> 00:03:20.890
it should now be clear why the last
red row does not belong to this table.

49
00:03:21.940 --> 00:03:26.180
The schema in a relational table
can also specify constraints,

50
00:03:27.330 --> 00:03:30.150
shown in yellow in the third
line of the schema row.

51
00:03:31.720 --> 00:03:36.050
It says that the minimum salary of
a person has to be greater than 25k.

52
00:03:38.150 --> 00:03:44.670
Further, it states that every employee
must have a first and last name.

53
00:03:44.670 --> 00:03:46.660
They cannot be left null,
that means without a value.

54
00:03:48.840 --> 00:03:51.900
Why doesn't department or
title column have this constraint?

55
00:03:53.500 --> 00:03:57.990
One answer can be that a newly
hired employee may not be assigned

56
00:03:57.990 --> 00:04:01.350
a department or a title yet but
can still be an entry in the table.

57
00:04:02.560 --> 00:04:06.040
However, the department column
has another constraint.

58
00:04:07.130 --> 00:04:11.390
It restricts the possible values
that is the domain of the attribute

59
00:04:11.390 --> 00:04:14.060
to only four possibilities.

60
00:04:14.060 --> 00:04:17.900
HR, IT, research, and business.

61
00:04:19.620 --> 00:04:24.840
Finally, the first says
that ID is a primary key.

62
00:04:26.180 --> 00:04:29.960
This means it is unique for each employee.

63
00:04:29.960 --> 00:04:33.200
And for
every employee knowing the primary key for

64
00:04:33.200 --> 00:04:38.720
the employee will also uniquely know the
other five attributes of that employee.

65
00:04:40.180 --> 00:04:43.240
You should now see that
a table with a primary key

66
00:04:43.240 --> 00:04:48.590
logically implies that the table cannot
have a duplicate record because if we do,

67
00:04:48.590 --> 00:04:51.870
it will violate the uniqueness constraint
associated with the primary key.

68
00:04:53.330 --> 00:04:57.140
Let us introduce a new table containing
the salary history of employees.

69
00:04:58.410 --> 00:05:02.010
The employees are identified
with the column EmpID, but

70
00:05:02.010 --> 00:05:04.760
these are not new values that
this table happens to have.

71
00:05:05.900 --> 00:05:10.460
They are the same IDs that are present in
the ID column of the employee's table,

72
00:05:10.460 --> 00:05:11.140
presented earlier.

73
00:05:13.420 --> 00:05:15.570
This is reflected in
the statement made on the right.

74
00:05:16.760 --> 00:05:23.530
The term References means,
the values in this column can exist

75
00:05:23.530 --> 00:05:28.760
only if the same values if you are in
employees the table being referenced,

76
00:05:28.760 --> 00:05:30.700
also called the parent table.

77
00:05:32.150 --> 00:05:38.920
So in the terminology of the relational
model, the EmpID column of EmpSalaries

78
00:05:38.920 --> 00:05:44.660
table is called a foreign key that refers
to the primary key of the Employees table.

79
00:05:46.560 --> 00:05:52.650
Note that EmpID is not a primary
key in this EmpSalaries table.

80
00:05:52.650 --> 00:05:56.420
Because it is multiple to
post with the same EmpID

81
00:05:56.420 --> 00:05:58.920
reflecting the salary of
the employee at different times.

82
00:06:00.700 --> 00:06:04.070
You will remember join is a common
operation that we discussed before.

83
00:06:05.150 --> 00:06:09.200
So here is an example of
a relational join performed

84
00:06:09.200 --> 00:06:13.810
on the first three columns of employee and
EmpSalaries table.

85
00:06:13.810 --> 00:06:20.630
Where employees.ID, and EmpSalaries.EmpID
columns are matched for equality.

86
00:06:21.770 --> 00:06:24.130
The output table shows
all the columns involved.

87
00:06:25.360 --> 00:06:27.360
The common column is represented once.

88
00:06:28.680 --> 00:06:31.210
This form of join is
called a Natural Join.

89
00:06:32.690 --> 00:06:37.770
It is important to understand that
join is one of the most expensive

90
00:06:37.770 --> 00:06:41.570
that means time consuming and
space consuming operations.

91
00:06:42.770 --> 00:06:48.270
As data becomes larger, and tables contain
hundreds of millions of tuples, the join

92
00:06:48.270 --> 00:06:52.480
operation can easily become a bottleneck
in a larger analytic application.

93
00:06:53.880 --> 00:06:59.280
So for analytical big data application
that needs joins, it's very important to

94
00:06:59.280 --> 00:07:04.890
choose a suitable data management platform
that makes this operation efficient.

95
00:07:04.890 --> 00:07:07.090
We will return to this
issue in module four.

96
00:07:08.760 --> 00:07:10.600
We end this video on a practical note.

97
00:07:11.830 --> 00:07:13.060
In many scientific and

98
00:07:13.060 --> 00:07:16.990
business applications,
people start with CSV files,

99
00:07:16.990 --> 00:07:21.650
manipulate them with the spreadsheet,
then migrate their relational system only

100
00:07:21.650 --> 00:07:25.790
as an afterthought where the data becomes
too large to handle the spreadsheet.

101
00:07:27.490 --> 00:07:30.500
While the spreadsheet offers
many useful features.

102
00:07:30.500 --> 00:07:36.037
It does not conform and enforce many
principles of relational data models.

103
00:07:37.700 --> 00:07:42.210
Consequently, a large amount of time
may be spent in cleaning up and

104
00:07:42.210 --> 00:07:45.480
correcting data errors after
the migration actually happens.

105
00:07:46.670 --> 00:07:51.860
Let me show a few examples from
a spreadsheet that has 125,000 rows and

106
00:07:51.860 --> 00:07:52.430
over 100 columns.

107
00:07:53.950 --> 00:07:59.280
The spreadsheet here lists terrorism
attacks gathered from news media.

108
00:07:59.280 --> 00:08:02.030
So each row represents one attack.

109
00:08:03.080 --> 00:08:07.210
This is a valuable piece of data for
people who study terrorism.

110
00:08:07.210 --> 00:08:10.420
But we are going to look at it from
a relational data modelling viewpoint.

111
00:08:12.200 --> 00:08:15.160
First, notice the column marked in green.

112
00:08:16.750 --> 00:08:20.900
It lists two weapons used in
the attack separated by a semicolon.

113
00:08:22.330 --> 00:08:24.390
Why is this really common?

114
00:08:24.390 --> 00:08:26.950
It makes this column non-atomic.

115
00:08:26.950 --> 00:08:31.450
It means that this column actually
has two different values.

116
00:08:32.570 --> 00:08:37.140
In a relational design, this information
will be moved to another table

117
00:08:37.140 --> 00:08:40.870
just like the multiple salaries of
employees were placed in a separate table.

118
00:08:42.100 --> 00:08:45.740
Next, notice the column outlined in red.

119
00:08:47.290 --> 00:08:52.370
It describes the amount of property
damaged by a possible terrorist attack.

120
00:08:53.890 --> 00:08:59.880
In this column, the intended
legitimate values are unknown,

121
00:08:59.880 --> 00:09:01.540
minor, major and catastrophic.

122
00:09:02.670 --> 00:09:07.350
However, the value in the highlighted
part of the spreadsheet is minor and

123
00:09:07.350 --> 00:09:12.012
then within bracket,
likely less than $1 million.

124
00:09:12.012 --> 00:09:16.700
Which means a query like
find all attacks for

125
00:09:16.700 --> 00:09:20.390
which the property damage is equal to
minor cannot be answered directly.

126
00:09:21.680 --> 00:09:25.150
Instead, we need to perform
a substring search for

127
00:09:25.150 --> 00:09:27.630
minor in the beginning of the description.

128
00:09:27.630 --> 00:09:30.420
Which is doable, but
it's a more expensive operation.

129
00:09:31.960 --> 00:09:35.730
This shows the columns of the spreadsheet.

130
00:09:35.730 --> 00:09:37.950
So this is part of the schema of the data.

131
00:09:39.120 --> 00:09:42.310
If you observe carefully you
will see a recurring pattern.

132
00:09:44.120 --> 00:09:49.290
The designer of the data table determined
that there can be at most three types of

133
00:09:49.290 --> 00:09:54.380
attacks within a single encounter and
represented with three separate columns.

134
00:09:55.790 --> 00:10:01.220
Now in proper relational modeling,
one would say that there is a one to many

135
00:10:01.220 --> 00:10:06.650
relationship between the attack and
the number of attack types.

136
00:10:08.210 --> 00:10:10.980
In such a case, it would be more prudent

137
00:10:10.980 --> 00:10:15.030
to place these attack type
columns in a separate table and

138
00:10:15.030 --> 00:10:19.480
connect with the parent using a primary
key, foreign key relationship.

139
00:10:20.920 --> 00:10:24.240
Here's another block,
with a similar pattern,

140
00:10:25.490 --> 00:10:29.420
this time this is about the types and
subtypes of weapons used.

141
00:10:30.620 --> 00:10:33.690
Now can you determine how you might
be able to reorganize this block?

142
00:10:35.130 --> 00:10:36.761
We'll leave this as an exercise.
