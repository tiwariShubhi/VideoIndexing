
1
00:00:01.220 --> 00:00:05.360
A very important part of data preparation
is to assess the quality of your data.

2
00:00:05.360 --> 00:00:09.120
We will look at some common data
quality issues in this lecture.

3
00:00:09.120 --> 00:00:14.660
After this video, you will be able to
describe three data quality issues,

4
00:00:14.660 --> 00:00:17.680
name three reasons for
poor data quality and

5
00:00:17.680 --> 00:00:21.060
explain why data quality
issues need to be addressed.

6
00:00:21.060 --> 00:00:23.820
Real world data is often very messy, so

7
00:00:23.820 --> 00:00:27.150
it's a given fact that you will need
to clean your data by identifying and

8
00:00:27.150 --> 00:00:30.660
addressing many issues that
affect the quality of your data.

9
00:00:30.660 --> 00:00:33.130
Let's take a closer look at what
these data quality issues are.

10
00:00:34.330 --> 00:00:37.770
A very common data quality
issue is missing data.

11
00:00:37.770 --> 00:00:41.410
Recall that a sample in your dataset
typically contains several variables or

12
00:00:41.410 --> 00:00:45.170
features like name, age and income.

13
00:00:45.170 --> 00:00:48.960
For some samples, some of these
variables may not have a value.

14
00:00:48.960 --> 00:00:53.420
These are referred to as
missing values in the data.

15
00:00:53.420 --> 00:00:57.650
Missing values are also referred
to as N/A for not available.

16
00:00:57.650 --> 00:01:00.810
So you will see N/A and
missing values used interchangeably.

17
00:01:01.910 --> 00:01:05.462
You may have missing values in your data
if you have an optional field in your

18
00:01:05.462 --> 00:01:06.500
data set.

19
00:01:06.500 --> 00:01:11.471
For example, the field age is often
an optional field on a survey.

20
00:01:11.471 --> 00:01:15.590
Also many people may choose not
to provide a response for income.

21
00:01:15.590 --> 00:01:20.470
And so you will end up with missing values
for the variable income in your data set.

22
00:01:20.470 --> 00:01:24.390
In some cases, a variable may
not be applicable to all cases.

23
00:01:24.390 --> 00:01:29.120
For example, income may not be
applicable to people who are retired or

24
00:01:29.120 --> 00:01:31.430
unemployed or to children.

25
00:01:31.430 --> 00:01:34.350
So you will not have an entry for
income in all of your samples.

26
00:01:35.490 --> 00:01:39.930
You can also have missing values, due to a
data collecting device that malfunctions,

27
00:01:39.930 --> 00:01:43.240
a network problem that affects
how the data was transmitted, or

28
00:01:43.240 --> 00:01:47.120
something else that goes wrong during
the data collection process itself, or

29
00:01:47.120 --> 00:01:49.930
the process of transmitting the data or
storing the data.

30
00:01:51.360 --> 00:01:55.680
Duplicate data occurs when your data set
has data objects that are duplicates or

31
00:01:55.680 --> 00:01:57.030
new duplicates of one another.

32
00:01:58.280 --> 00:02:01.090
An example of this is when there
are two different records for

33
00:02:01.090 --> 00:02:03.470
the same customer with
different addresses.

34
00:02:04.860 --> 00:02:06.590
This can come about, for example,

35
00:02:06.590 --> 00:02:10.510
if a customer's address has changed,
but the second address was simply

36
00:02:10.510 --> 00:02:15.160
added to this customer's records instead
of used to update the first address.

37
00:02:15.160 --> 00:02:18.220
Duplicate data can occur when
merging data from multiple sources.

38
00:02:19.470 --> 00:02:20.260
Invalid or

39
00:02:20.260 --> 00:02:24.720
inconsistent data occurs when you have
an impossible value for a variable.

40
00:02:24.720 --> 00:02:29.810
Some common examples are when you have
a six digit zip code, the letters AB for

41
00:02:29.810 --> 00:02:33.570
state abbreviations, or
a negative numbers for age.

42
00:02:33.570 --> 00:02:36.460
These invalid data values can
come about when there is a data

43
00:02:36.460 --> 00:02:38.860
entry error during data collection.

44
00:02:38.860 --> 00:02:41.460
For example, if you allow people
to type in their zip code and

45
00:02:41.460 --> 00:02:45.650
someone accidentally includes an extra
digit to their five digit zip code,

46
00:02:45.650 --> 00:02:47.915
then you will end up with
an invalid six digit zipcode.

47
00:02:49.240 --> 00:02:52.710
Noise refers to anything
that can distort your data.

48
00:02:52.710 --> 00:02:55.780
Noise can be introduced during
the data collection process or

49
00:02:55.780 --> 00:02:57.910
data transmission process.

50
00:02:57.910 --> 00:03:01.590
An example is buzzing in the background
when an audio message is recorded

51
00:03:01.590 --> 00:03:04.920
due to background noise or
a faulty microphone.

52
00:03:04.920 --> 00:03:08.470
Another example is an overly bright
image due to an incorrect light

53
00:03:08.470 --> 00:03:09.290
exposure setting.

54
00:03:10.690 --> 00:03:14.150
An Outlier is a data sample with
values that are considerably different

55
00:03:14.150 --> 00:03:16.550
than the rest of the other
data samples in a data set.

56
00:03:17.670 --> 00:03:21.110
An example scenario, that can create
Outliers is when there's a sense of

57
00:03:21.110 --> 00:03:26.650
failure that causes values being recorded
to be much higher or lower than normal.

58
00:03:26.650 --> 00:03:30.130
In this case, you want to remove
the Outliers from your data.

59
00:03:30.130 --> 00:03:33.580
In other applications however,
such as fraud detection,

60
00:03:33.580 --> 00:03:38.080
outliers are the important samples
that should be examined more closely.

61
00:03:38.080 --> 00:03:41.460
So depending on the application,
outliers may need to be removed or

62
00:03:41.460 --> 00:03:43.270
be kept for further analysis.

63
00:03:44.400 --> 00:03:47.000
If you simply ignore these
data quality issues,

64
00:03:47.000 --> 00:03:51.300
any analysis that is performed
will produce misleading results.

65
00:03:51.300 --> 00:03:54.220
In addition, some implementations
of analysis techniques

66
00:03:54.220 --> 00:03:57.030
cannot handle some of these
problems such as missing values.

67
00:03:58.160 --> 00:04:01.231
So, problems that we've discussed
in this lecture need to be

68
00:04:01.231 --> 00:04:04.312
addressed before any meaningful
analysis can be performed.

69
00:04:04.312 --> 00:04:05.813
We will discuss some techniques for

70
00:04:05.813 --> 00:04:07.970
handling data quality
issues in the next lecture.

1
00:00:00.790 --> 00:00:03.560
If you have been in a conversation
on machine learning,

2
00:00:03.560 --> 00:00:07.680
you have probably heard terms like
feature, sample, and variable.

3
00:00:07.680 --> 00:00:09.950
We will be defining some of
those terms in this lecture.

4
00:00:11.930 --> 00:00:16.580
After this video, you will be able
to describe what a feature is, and

5
00:00:16.580 --> 00:00:18.740
how it relates to a sample.

6
00:00:18.740 --> 00:00:21.480
Name some alternative terms for feature.

7
00:00:21.480 --> 00:00:25.100
Summarize how a categorical feature
differs from a numerical feature.

8
00:00:28.170 --> 00:00:30.300
Before we delve into the methods for
processing and

9
00:00:30.300 --> 00:00:34.690
analyzing data, let's first start with
defining some term used to describe data,

10
00:00:34.690 --> 00:00:36.610
starting with sample and variable.

11
00:00:38.100 --> 00:00:42.240
A sample is an instance or
example of an entity in your data.

12
00:00:42.240 --> 00:00:45.150
This is typically a row in your dataset.

13
00:00:45.150 --> 00:00:48.380
This figure shows part of a dataset
of values related to weather.

14
00:00:49.670 --> 00:00:54.950
Each row is a sample representing
weather data for particular day.

15
00:00:54.950 --> 00:00:59.370
The table in the figure shows four samples
of weather data, each for different day.

16
00:01:00.960 --> 00:01:05.410
In this table, each sample has
five values associated with it.

17
00:01:05.410 --> 00:01:09.430
These values are different
information pieces about the sample

18
00:01:09.430 --> 00:01:13.200
such as the sample ID, sample date,

19
00:01:13.200 --> 00:01:17.660
minimum temperature, maximum temperature,
and rainfall on that day.

20
00:01:18.820 --> 00:01:21.630
We call these different values
variables of the sample.

21
00:01:24.020 --> 00:01:26.920
There are many names for
sample and variable.

22
00:01:26.920 --> 00:01:31.600
Some other terms for sample that you
might hear in a machine learning

23
00:01:31.600 --> 00:01:37.470
context include record, example,
row, instance and observation.

24
00:01:37.470 --> 00:01:40.600
It is helpful to realize that all of
these terms mean the same thing in

25
00:01:40.600 --> 00:01:41.830
machine learning.

26
00:01:41.830 --> 00:01:46.360
That is, they all refer to a specific
example of an entity in your dataset.

27
00:01:47.970 --> 00:01:53.400
There are also many names for the term
variable, such as feature, column,

28
00:01:53.400 --> 00:01:55.450
dimension, attribute, and field.

29
00:01:55.450 --> 00:01:59.330
All of these terms refer to
specific characteristics for

30
00:01:59.330 --> 00:02:00.660
each sample in your dataset.

31
00:02:02.680 --> 00:02:05.540
An important point to emphasize
about variable is that,

32
00:02:05.540 --> 00:02:08.380
they are additional
values with a data type.

33
00:02:08.380 --> 00:02:11.550
Each variable has a data
type associated with it.

34
00:02:11.550 --> 00:02:14.630
The most common data types are numeric and
categorical.

35
00:02:15.770 --> 00:02:19.080
There are other data types as
well such as string and date but

36
00:02:19.080 --> 00:02:23.260
we will focus on two of the more common
data types, numeric and categorical.

37
00:02:25.380 --> 00:02:30.720
As the name implies, numeric variables
are variables that take on number values.

38
00:02:30.720 --> 00:02:35.320
Numeric variables can be measured, and
their values can be sorted in some way.

39
00:02:35.320 --> 00:02:39.120
Note that a numeric variable can
take on just integer values or

40
00:02:39.120 --> 00:02:40.950
be continuous valued.

41
00:02:40.950 --> 00:02:44.600
It can also have just positive numbers,
negative numbers or both.

42
00:02:45.990 --> 00:02:48.750
Let's go over some examples
of various numeric variables.

43
00:02:49.830 --> 00:02:54.570
A person's height is a positive,
continuous valued number.

44
00:02:54.570 --> 00:03:00.220
The score in an exam is a positive number
that range between zero and a 100%.

45
00:03:00.220 --> 00:03:04.280
The number of transactions per
hour is a positive integer,

46
00:03:04.280 --> 00:03:08.030
whereas the change in a stock price
can be either positive or negative.

47
00:03:10.170 --> 00:03:13.650
A variable with labels,
names, or categories for

48
00:03:13.650 --> 00:03:17.070
values instead of numbers
are called categorical variables.

49
00:03:18.120 --> 00:03:22.250
For example a variable that describes
the color of an item, such as the color of

50
00:03:22.250 --> 00:03:28.948
a car, can have values such as red,
silver, blue, white and black.

51
00:03:28.948 --> 00:03:32.140
These are non-numeric values
that describes some quality or

52
00:03:32.140 --> 00:03:33.650
characteristic of an entity.

53
00:03:34.830 --> 00:03:39.670
These values can be thought of as names or
labels that can be sorted into categories.

54
00:03:39.670 --> 00:03:44.450
Therefore, categorical variables are also
referred to as qualitative variables, or

55
00:03:44.450 --> 00:03:45.650
nominal variables.

56
00:03:47.360 --> 00:03:52.340
Some examples of categorical
variables are gender, marital status,

57
00:03:52.340 --> 00:03:55.640
type of customer, for example,
teenager, adult, senior.

58
00:03:56.650 --> 00:04:01.310
Product categories, for example,
electronics, kitchen, bathroom and

59
00:04:01.310 --> 00:04:02.200
color of an item.

60
00:04:04.110 --> 00:04:08.790
To summarize, a sample is an instance or
example of an entity in your data.

61
00:04:08.790 --> 00:04:13.190
A variable captures a specific
characteristic of each entity.

62
00:04:13.190 --> 00:04:16.800
So a sample has many
variables to describe it.

63
00:04:16.800 --> 00:04:19.786
Data from real applications
are often multidimensional,

64
00:04:19.786 --> 00:04:24.240
meaning that there are many dimensions or
variables describing each sample.

65
00:04:24.240 --> 00:04:27.430
Each variable has a data
type associated with it,

66
00:04:27.430 --> 00:04:31.280
the most common data types are numeric and
categorical.

67
00:04:31.280 --> 00:04:34.633
Note that there are many terms to
describe these data related concepts.

1
00:00:00.790 --> 00:00:05.070
The data used in machine learning
processes often have many variables.

2
00:00:05.070 --> 00:00:08.413
This is what we call
highly dimensional data.

3
00:00:08.413 --> 00:00:12.815
Most of these dimensions may or may not
matter in the context of our application

4
00:00:12.815 --> 00:00:14.760
with the questions we are asking.

5
00:00:14.760 --> 00:00:19.288
Reducing such high dimensions to
a more manageable set of related and

6
00:00:19.288 --> 00:00:24.222
useful variables improves the performance
and accuracy of our analysis.

7
00:00:24.222 --> 00:00:30.365
After this video, you will be able to
explain what dimensionality reduction is,

8
00:00:30.365 --> 00:00:34.639
discuss the benefits of
dimensionality reduction, and

9
00:00:34.639 --> 00:00:37.678
describe how PCA transforms your data.

10
00:00:37.678 --> 00:00:42.465
The number of features or variables
you have in your data set determines

11
00:00:42.465 --> 00:00:46.391
the number of dimensions or
dimensionality of your data.

12
00:00:46.391 --> 00:00:50.857
If your dataset has two features,
than it is two dimensional data.

13
00:00:50.857 --> 00:00:55.454
If it has three features than
it has three features and so on.

14
00:00:55.454 --> 00:01:00.090
You want to use as many features as
possible to capture the characteristics of

15
00:01:00.090 --> 00:01:01.016
your data, but

16
00:01:01.016 --> 00:01:05.105
you also don't want the dimension
audio of your data to be too high.

17
00:01:05.105 --> 00:01:10.844
As the dimensionality increases, the
problem spaces you're looking at increases

18
00:01:10.844 --> 00:01:16.197
requiring substantially more instances
to adequately sample of that space.

19
00:01:16.197 --> 00:01:18.805
So as the dimensionality increases,

20
00:01:18.805 --> 00:01:22.640
the space that you are looking
at grows exponentially.

21
00:01:22.640 --> 00:01:26.460
As the space grows data
becomes increasingly sparse.

22
00:01:26.460 --> 00:01:30.782
In this diagram we see how
the problem space grows as

23
00:01:30.782 --> 00:01:35.011
the dimensionality
increases from 1 to 2 to 3.

24
00:01:35.011 --> 00:01:39.915
In the left plot, we have a one
dimensional space partitioned into four

25
00:01:39.915 --> 00:01:43.150
regions each with size of 5 units.

26
00:01:43.150 --> 00:01:48.249
The middle plot shows a two
dimensional space with 5x5 regions.

27
00:01:48.249 --> 00:01:52.429
The number of regions has
now going from 4 to 16.

28
00:01:52.429 --> 00:01:58.911
In the third plot, the problem space is
three dimensional with 5x5x5 regions.

29
00:01:58.911 --> 00:02:03.171
The number of regions
increased even more to 64.

30
00:02:03.171 --> 00:02:08.364
We see that as the number of dimensions
increases, the number of regions

31
00:02:08.364 --> 00:02:13.403
increases exponentially and
the data becomes increasingly sparse.

32
00:02:13.403 --> 00:02:19.230
With a small dataset relative to the
problem space, analysis results degrade.

33
00:02:19.230 --> 00:02:23.927
In addition, certain calculations used in
analysis become much more difficult to

34
00:02:23.927 --> 00:02:26.018
define and calculate effectively.

35
00:02:26.018 --> 00:02:29.998
For example, distances between samples
are harder to compare since all samples

36
00:02:29.998 --> 00:02:32.330
are far away from each other.

37
00:02:32.330 --> 00:02:36.784
All of these challenges represent
the difficulty of dealing with high

38
00:02:36.784 --> 00:02:41.172
dimensional data and as referred
to as the curse of dimensionality.

39
00:02:41.172 --> 00:02:43.632
To avoid the curse of dimensionality,

40
00:02:43.632 --> 00:02:46.918
you want to reduce
the dimensionality of your data.

41
00:02:46.918 --> 00:02:51.095
This means finding a smaller subset of
features that can effectively capture

42
00:02:51.095 --> 00:02:53.030
the characteristics of your data.

43
00:02:54.050 --> 00:02:58.350
Recall from the lecture on feature
selection part of data preparation is to

44
00:02:58.350 --> 00:02:59.996
select the features to use.

45
00:02:59.996 --> 00:03:05.081
For example, you can a feature that is
very correlated with another feature.

46
00:03:05.081 --> 00:03:08.021
Using feature selection
techniques to select assessitive

47
00:03:08.021 --> 00:03:10.850
features is one approach to
dimensionality reduction.

48
00:03:12.220 --> 00:03:16.160
Another approach to dimensionality
reduction is to mathematically determine

49
00:03:16.160 --> 00:03:20.230
the most important dimension to keep and
ignore the rest.

50
00:03:20.230 --> 00:03:23.260
The idea is to find a smallest
subset of dimensions that

51
00:03:23.260 --> 00:03:25.670
capture the most variation in your data.

52
00:03:26.850 --> 00:03:31.734
This reduces the dimensions of the data
while eliminating the relevant features

53
00:03:31.734 --> 00:03:34.263
making the subsequent analysis simple.

54
00:03:34.263 --> 00:03:39.188
A technique commonly use to find
the subset of most important dimensions is

55
00:03:39.188 --> 00:03:43.028
called principal component analysis,
or PCA for short.

56
00:03:43.028 --> 00:03:47.997
The goal of PCA is to map the data from
the original high dimensional space

57
00:03:47.997 --> 00:03:52.722
to a lower dimensional space that
captures as much of the variation in

58
00:03:52.722 --> 00:03:54.282
the data as possible.

59
00:03:54.282 --> 00:03:55.444
In other words,

60
00:03:55.444 --> 00:04:00.760
PCA aims to find the most useful subset
of dimensions to summarize the data.

61
00:04:02.380 --> 00:04:04.330
This plot illustrates the idea behind PCA.

62
00:04:05.560 --> 00:04:10.200
Here, we have data samples in a two
dimensional space that is defined

63
00:04:10.200 --> 00:04:12.044
by the x axis and the y axis.

64
00:04:12.044 --> 00:04:17.168
You can see that most of the variation in
the data lies along the red diagonal line.

65
00:04:17.168 --> 00:04:21.527
This means that the dat samples are best
differentiated along this dimension

66
00:04:21.527 --> 00:04:26.730
because they're spread out,
not clumped together along this dimension.

67
00:04:26.730 --> 00:04:31.577
This dimension indicated by the red
line is the first principle component

68
00:04:31.577 --> 00:04:33.538
labelled as PC1 in the part.

69
00:04:33.538 --> 00:04:39.130
It captures the large amount of variance
along a single dimension in data.

70
00:04:39.130 --> 00:04:44.305
PC1, indicated by the red line does
not correspond to either axis.

71
00:04:44.305 --> 00:04:48.450
The next principle component is determined
by looking in the direction that is

72
00:04:48.450 --> 00:04:52.972
orthogonal, in other words perpendicular,
to the first principle component which

73
00:04:52.972 --> 00:04:56.800
captures the next largest
amount of variance in the data.

74
00:04:56.800 --> 00:05:00.042
This is the second
principal component PC2 and

75
00:05:00.042 --> 00:05:03.212
it's indicated by
the green line in the plot.

76
00:05:03.212 --> 00:05:08.198
This process can be repeated to find as
many principal components as desired.

77
00:05:08.198 --> 00:05:12.726
Note that the principal components do
not align with either the x-axis or

78
00:05:12.726 --> 00:05:13.610
the y-axis.

79
00:05:13.610 --> 00:05:17.720
And that they are orthogonal, in other
words, perpendicular to each other.

80
00:05:17.720 --> 00:05:19.489
This is what PCA does.

81
00:05:19.489 --> 00:05:22.693
It finds the underlined dimensions,
the principal

82
00:05:22.693 --> 00:05:27.223
components that capture as much of
the variation in the data as possible.

83
00:05:27.223 --> 00:05:31.853
These principal components form a new
coordinates system to transform

84
00:05:31.853 --> 00:05:36.567
the data to, instead of the conventional
dimensions like X, Y, and Z.

85
00:05:36.567 --> 00:05:40.320
So how does PCA help with
dimensionality reduction?

86
00:05:40.320 --> 00:05:43.817
Let's look again in this plot with
the first principle component.

87
00:05:43.817 --> 00:05:47.846
Since the first principle component
captures most of the variations in

88
00:05:47.846 --> 00:05:52.342
the data, the original data sample can
be mapped to this dimension indicated by

89
00:05:52.342 --> 00:05:55.045
the red line with minimum
loss of information.

90
00:05:55.045 --> 00:05:58.645
In this case then,
we map a two-dimensional dataset to

91
00:05:58.645 --> 00:06:03.487
a one-dimensional space while keeping
as much information as possible.

92
00:06:03.487 --> 00:06:06.990
Here are some main points about
principal components analysis.

93
00:06:06.990 --> 00:06:10.147
PCA finds a new coordinate system for
your data,

94
00:06:10.147 --> 00:06:14.151
such that the first coordinate
defined by the first principal

95
00:06:14.151 --> 00:06:17.855
component Captures the greatest
variance in your data.

96
00:06:17.855 --> 00:06:22.341
The second coordinate defined by
the second principal component captures

97
00:06:22.341 --> 00:06:25.271
the second greatest variance in a data,
etc..

98
00:06:25.271 --> 00:06:30.098
The first few principle components that
capture most of the variance in a data

99
00:06:30.098 --> 00:06:34.120
can be used to define
a lower-dimensional space for your data.

100
00:06:34.120 --> 00:06:38.513
PCA can be a very useful technique for
dimensionality reduction,

101
00:06:38.513 --> 00:06:42.284
especially when working
with high-dimensional data.

102
00:06:42.284 --> 00:06:46.032
While PCA is a useful technique for
reducing the dimensionality of your

103
00:06:46.032 --> 00:06:48.718
data which can help with
the downstream analysis,

104
00:06:48.718 --> 00:06:53.715
it can also make the resulting analysis
models more difficult to interpret.

105
00:06:53.715 --> 00:06:58.965
The original features in your data set
have specific meanings such as income,

106
00:06:58.965 --> 00:07:00.865
age and occupation.

107
00:07:00.865 --> 00:07:05.792
By mapping the data to a new coordinate
system defined by principal components,

108
00:07:05.792 --> 00:07:10.364
the dimensions in your transformed
data no longer have natural meanings.

109
00:07:10.364 --> 00:07:14.136
This should be kept in mind when using
PCA for dimensionality reduction.

1
00:00:01.740 --> 00:00:05.720
In this activity, we will use KNIME to
perform data exploration of weather data.

2
00:00:07.890 --> 00:00:10.000
First, we will create
a new KNIME workflow.

3
00:00:11.140 --> 00:00:12.560
We'll then input the weather data.

4
00:00:12.560 --> 00:00:17.740
Next, we will create
a histogram of air temperature.

5
00:00:17.740 --> 00:00:21.250
Create a scatter plot
between two variables.

6
00:00:21.250 --> 00:00:25.930
Create a bar chart to show the
distribution of a categorical variable.

7
00:00:25.930 --> 00:00:29.700
And finally, create a box plot to
compare two different distributions.

8
00:00:31.493 --> 00:00:34.350
Let's begin.

9
00:00:34.350 --> 00:00:36.660
First, we will create
a new workflow in KNIME.

10
00:00:38.100 --> 00:00:40.000
To do this we go to the File menu.

11
00:00:41.277 --> 00:00:48.890
Select new, select new KNIME workflow and
click on next.

12
00:00:50.410 --> 00:00:52.720
Then we type the name of
the workflow we want to create.

13
00:00:53.730 --> 00:00:55.380
We'll call this one plots.

14
00:00:56.850 --> 00:00:57.710
Click on finish.

15
00:00:59.969 --> 00:01:04.690
Next we want to load
weather data into KNIME.

16
00:01:04.690 --> 00:01:06.710
To do this we'll use a file reader node.

17
00:01:07.720 --> 00:01:11.890
To add this node,
go to the bottom left node repository.

18
00:01:11.890 --> 00:01:14.220
And in the box type in file reader.

19
00:01:15.298 --> 00:01:19.670
Drag File Reader to the canvas.

20
00:01:19.670 --> 00:01:23.890
Next, double click on File Reader to
configure it with the weather data.

21
00:01:24.930 --> 00:01:25.900
Click on the Browse button.

22
00:01:27.879 --> 00:01:31.360
We'll choose the weather file.

23
00:01:31.360 --> 00:01:34.084
That's daily_weather.csv.

24
00:01:35.794 --> 00:01:38.875
We can see preview of
the data at the bottom half

25
00:01:38.875 --> 00:01:41.410
of the file reader configure dialog.

26
00:01:41.410 --> 00:01:45.270
You can see values for
each column in the CSV file.

27
00:01:45.270 --> 00:01:48.440
Click OK to close the dialog.

28
00:01:49.520 --> 00:01:53.510
Next, we want to create
a histogram of air temperature.

29
00:01:53.510 --> 00:01:56.410
We'll add the histogram
node to the workflow.

30
00:01:56.410 --> 00:02:00.230
Again, we go to the node repository and
type in histogram.

31
00:02:01.640 --> 00:02:04.170
We'll drag and
drop histogram to the workflow and

32
00:02:06.640 --> 00:02:08.700
we'll connect File Reader to histogram.

33
00:02:09.700 --> 00:02:13.327
We'll click and
hold on output to File Reader and

34
00:02:13.327 --> 00:02:16.610
drag to the input of histogram and
release.

35
00:02:18.530 --> 00:02:22.520
Before we configure the histogram node,
we need to run the file reader node.

36
00:02:22.520 --> 00:02:26.662
We can do that by selecting
the file reader and

37
00:02:26.662 --> 00:02:30.913
either clicking on the green
arrow at the top or

38
00:02:30.913 --> 00:02:34.952
right clicking and choosing execute here.

39
00:02:37.466 --> 00:02:42.508
Once we've done that,
double-click on histogram.

40
00:02:42.508 --> 00:02:48.176
We'll select error_temp_9AM
as the biding column and

41
00:02:48.176 --> 00:02:54.520
also add error_temp_9AM to
the aggregation column.

42
00:02:54.520 --> 00:02:57.570
We'll leave the default
number of bins as ten.

43
00:02:57.570 --> 00:02:59.020
Click on OK.

44
00:03:01.110 --> 00:03:02.070
Now we'll run the workflow.

45
00:03:02.070 --> 00:03:07.060
You can click on the two arrow green
button at the top to run all the nodes.

46
00:03:08.339 --> 00:03:11.420
Now let's view the histogram.

47
00:03:11.420 --> 00:03:13.690
Right click on the histogram node.

48
00:03:13.690 --> 00:03:17.453
And choose View, Histogram View.

49
00:03:17.453 --> 00:03:21.530
The x-axis shows the value for
each bin in the histogram.

50
00:03:21.530 --> 00:03:24.850
And the y-axis is the count, or frequency.

51
00:03:26.080 --> 00:03:30.000
We can see the most frequent
values are between 60 and 73.

52
00:03:30.000 --> 00:03:35.119
On the right, we also see that
there's some with missing values.

53
00:03:36.710 --> 00:03:38.250
Next close the window.

54
00:03:39.580 --> 00:03:43.550
Now let's create a scatter plot to show
the relationship between two variables.

55
00:03:44.650 --> 00:03:47.430
First, we will add the scatter
plot node to the work flow.

56
00:03:50.128 --> 00:03:54.870
We'll connect the the output of
File Reader to the input of Scatter Plot.

57
00:03:56.244 --> 00:04:01.700
Execute the workflow, right-click on

58
00:04:01.700 --> 00:04:07.650
Scatter Plot and choose View Scatter Plot.

59
00:04:07.650 --> 00:04:14.560
We'll click on the Column Selection and
choose air_temp_9am for the X Column.

60
00:04:14.560 --> 00:04:17.990
And for the Y Column,
choose relative humidity 9 AM.

61
00:04:17.990 --> 00:04:20.630
In this plot,

62
00:04:20.630 --> 00:04:24.650
we can see a negative correlation
between temperature and humidity.

63
00:04:24.650 --> 00:04:27.650
As the temperature increases,
the humidity goes down.

64
00:04:29.520 --> 00:04:30.480
We'll close this window.

65
00:04:32.490 --> 00:04:37.890
Next, we'll create a bar chart to show the
distribution of a categorical variable.

66
00:04:37.890 --> 00:04:40.120
We'll visualize the wind
direction at 9 AM.

67
00:04:40.120 --> 00:04:45.290
We will begin by creating the categorical
variable by using the numeric binner node.

68
00:04:46.310 --> 00:04:48.400
Let's add numeric binner to the work flow.

69
00:04:51.011 --> 00:04:56.510
We'll connect the output of file
reader to the input of numeric binner.

70
00:04:57.675 --> 00:05:01.528
Double-click on Numeric Binner
to configure it.

71
00:05:01.528 --> 00:05:05.799
Select max_wind_direction_9am and

72
00:05:05.799 --> 00:05:09.950
click on add five times to add five bins.

73
00:05:11.220 --> 00:05:13.050
Now let's give a name for
each of these bins.

74
00:05:14.510 --> 00:05:17.180
Select a bin and now choose a name.

75
00:05:17.180 --> 00:05:19.920
The first will be for the North direction.

76
00:05:19.920 --> 00:05:22.938
So we'll call that 1-N.

77
00:05:22.938 --> 00:05:29.214
Next, 2-E for East, 3-S for

78
00:05:29.214 --> 00:05:34.444
South, 4-W for West and

79
00:05:34.444 --> 00:05:38.630
finally 1-N for north again.

80
00:05:39.650 --> 00:05:43.340
Now we need to specify the values for
each of these bins.

81
00:05:43.340 --> 00:05:48.010
We'll select the first one and
set the endpoint to be 45 degrees.

82
00:05:48.010 --> 00:05:53.820
Next, choose east and
set that maximum to be 135.

83
00:05:55.330 --> 00:05:59.360
Select the third one and
choose the max to be 225.

84
00:05:59.360 --> 00:06:05.110
Finally, select the fourth one and
select the max to be 315.

85
00:06:05.110 --> 00:06:08.510
Next, click on a pin new column.

86
00:06:08.510 --> 00:06:14.368
Set the name for categorical variable to
be categorical underscore max underscore.

87
00:06:14.368 --> 00:06:18.531
Wind_direction_9AM.

88
00:06:20.419 --> 00:06:23.740
Click OK.

89
00:06:23.740 --> 00:06:25.880
Now add a histogram note to the work flow.

90
00:06:30.111 --> 00:06:34.455
Connect the output of
Numeric Binner to histogram.

91
00:06:34.455 --> 00:06:38.630
Double-click on Histogram to configure it.

92
00:06:38.630 --> 00:06:44.908
Make sure they binning column is
categorical_max_wind_direction_9AM.

93
00:06:44.908 --> 00:06:47.160
And also there are no aggregation columns.

94
00:06:48.800 --> 00:06:52.140
Click OK and execute the work flow.

95
00:06:54.066 --> 00:06:55.840
Let's view the chart.

96
00:06:56.880 --> 00:07:01.170
Right click on Histogram and
choose View, Histogram View.

97
00:07:03.470 --> 00:07:07.720
This tells us that most of the wind
comes from the east and the south.

98
00:07:09.070 --> 00:07:10.620
And not many measurements from the north.

99
00:07:12.220 --> 00:07:13.650
Next, close this.

100
00:07:15.490 --> 00:07:19.252
Now lets create a box plot to
compare two different distributions.

101
00:07:19.252 --> 00:07:22.380
We w'll examine the distribution,
air pressure for

102
00:07:22.380 --> 00:07:26.610
low humidity days versus normal or
high humidity days.

103
00:07:26.610 --> 00:07:30.810
To do this, we'll first need to creat
a categorical variable for humidity.

104
00:07:30.810 --> 00:07:33.800
Lets add another numeric
binner actor to the workflow.

105
00:07:36.599 --> 00:07:40.190
Connect the output,
a file reader to this actor.

106
00:07:42.090 --> 00:07:44.500
Double click to configure.

107
00:07:44.500 --> 00:07:46.830
Select relative humidity, 9 AM.

108
00:07:46.830 --> 00:07:49.830
And click on add twice to add two bins.

109
00:07:51.820 --> 00:07:56.350
Select the first bin and
we'll call that humidity_low.

110
00:07:56.350 --> 00:08:03.750
Select the second bin and
we'll call that humidity_not_low.

111
00:08:03.750 --> 00:08:06.859
Select the first bin, and

112
00:08:06.859 --> 00:08:11.390
we'll set the maximum value to 25.

113
00:08:11.390 --> 00:08:12.900
Check append new column.

114
00:08:14.290 --> 00:08:21.130
And set the name to low_humidity_day,
click on OK.

115
00:08:21.130 --> 00:08:24.110
Next, add the conditional box
plot node to the workflow.

116
00:08:28.604 --> 00:08:33.310
Right click on conditional
box plot to configure it.

117
00:08:33.310 --> 00:08:37.891
Make sure the nominal column
is low_humidity_day and

118
00:08:37.891 --> 00:08:41.100
the numeric column is air_pressure_9am.

119
00:08:41.100 --> 00:08:42.010
Click OK.

120
00:08:42.010 --> 00:08:46.260
Run the workflow.

121
00:08:48.760 --> 00:08:53.410
Right click on conditional box plot and
choose few additional box plot.

122
00:08:55.210 --> 00:08:59.870
And the x axis, we see the two values or
a categorical variable, humidity low and

123
00:08:59.870 --> 00:09:01.650
humidity not low.

124
00:09:01.650 --> 00:09:04.000
The y axis shows the air pressure values.

125
00:09:05.370 --> 00:09:09.300
I can see that on average,
lower humidity means higher air pressure.

126
00:09:11.792 --> 00:09:13.750
Close this.

127
00:09:13.750 --> 00:09:17.330
Finally, save the workflow when
we're done by clicking on the disk

128
00:09:17.330 --> 00:09:18.630
at the top left of the toolbar.

1
00:00:01.190 --> 00:00:04.590
In addition to cleaning your data
to address data quality issues,

2
00:00:04.590 --> 00:00:09.470
data preparation also includes
selecting features to use for analysis.

3
00:00:09.470 --> 00:00:14.980
After this video, you will be able to
explain what feature selection involves,

4
00:00:14.980 --> 00:00:18.880
discuss the goal of feature selection,
and list three approaches for

5
00:00:18.880 --> 00:00:20.940
selecting features.

6
00:00:20.940 --> 00:00:23.720
Feature selection refers to
choosing the set of features to

7
00:00:23.720 --> 00:00:27.000
use that is appropriate for
the subsequent analysis.

8
00:00:27.000 --> 00:00:30.930
The goal of feature selection is to come
up with the smallest set of features

9
00:00:30.930 --> 00:00:35.030
that best captures the characteristics
of the problem being addressed.

10
00:00:35.030 --> 00:00:39.030
The smaller the number of features used,
the simpler the analysis will be.

11
00:00:39.030 --> 00:00:39.980
But of course,

12
00:00:39.980 --> 00:00:44.430
the set of features used must include
all features relevant to the problem.

13
00:00:44.430 --> 00:00:47.100
So, there must be a balance
between expressiveness and

14
00:00:47.100 --> 00:00:48.830
compactness of the feature set.

15
00:00:48.830 --> 00:00:53.560
There are several methods to
consider in selecting features.

16
00:00:53.560 --> 00:00:55.010
New features can be added,

17
00:00:55.010 --> 00:01:00.430
some features can be removed, features can
be re-coded or features can be combined.

18
00:01:00.430 --> 00:01:04.060
All these operations affect the final
set of features that will be used for

19
00:01:04.060 --> 00:01:05.270
analysis.

20
00:01:05.270 --> 00:01:07.790
Of course some features
can be kept as is as well.

21
00:01:09.210 --> 00:01:12.070
New features can be derived
from existing features.

22
00:01:12.070 --> 00:01:17.200
For example, a new feature to specify
whether a student is in state or

23
00:01:17.200 --> 00:01:21.700
out of state can be added based on
the student's state of residence.

24
00:01:21.700 --> 00:01:24.490
For an application such
as college admissions,

25
00:01:24.490 --> 00:01:28.300
this new feature represents an important
aspect of an application and so

26
00:01:28.300 --> 00:01:30.450
would be very helpful
as a separate feature.

27
00:01:31.480 --> 00:01:35.510
Another example is adding a feature
to indicate the color of a vehicle,

28
00:01:35.510 --> 00:01:38.870
which can play an important role
in an auto insurance application.

29
00:01:39.940 --> 00:01:43.000
Features can also be removed,
candidates for

30
00:01:43.000 --> 00:01:45.950
removal are features
that are very correlated.

31
00:01:45.950 --> 00:01:48.630
During data exploration,
you may have discovered that

32
00:01:48.630 --> 00:01:53.820
two features are very correlated,
that is they change in very similar ways.

33
00:01:53.820 --> 00:01:55.990
For example,
the purchase price of a product and

34
00:01:55.990 --> 00:02:00.480
the amount of sales tax paid
are likely to be very correlated.

35
00:02:00.480 --> 00:02:04.250
The higher the purchase price
the higher the sales tax.

36
00:02:04.250 --> 00:02:07.110
In this case, you might want
to drop one of these features,

37
00:02:07.110 --> 00:02:10.340
since these features have
essentially duplicate information.

38
00:02:10.340 --> 00:02:13.280
And keeping both features makes
the feature set larger and

39
00:02:13.280 --> 00:02:16.770
the analysis unnecessarily more complex.

40
00:02:16.770 --> 00:02:19.810
Features with a high percentage of
missing values may also be good

41
00:02:19.810 --> 00:02:21.640
candidates for removal.

42
00:02:21.640 --> 00:02:22.740
The validity and

43
00:02:22.740 --> 00:02:26.360
usefulness of features with a lot
of missing values are in question.

44
00:02:26.360 --> 00:02:29.810
So removing may not result
in any loss of information.

45
00:02:29.810 --> 00:02:32.350
Again these features would have
been discovered during the data

46
00:02:32.350 --> 00:02:33.090
exploration step.

47
00:02:34.240 --> 00:02:37.880
Irrelevant features should also
be removed from the data set.

48
00:02:37.880 --> 00:02:41.550
Irrelevant features are those that
contain no information that is useful for

49
00:02:41.550 --> 00:02:43.320
the analysis task.

50
00:02:43.320 --> 00:02:46.778
An example of this is employee
ID in predicting income.

51
00:02:46.778 --> 00:02:51.770
Other fields used simply for
identification such as row number,

52
00:02:51.770 --> 00:02:54.850
person's ID, etc.,
are good candidates for removal.

53
00:02:55.940 --> 00:02:59.990
Features can also be combined if the new
feature presents important information

54
00:02:59.990 --> 00:03:03.570
that is not represented by looking at
the original features individually.

55
00:03:05.160 --> 00:03:09.640
For example, BMI, which is body
mass index is an indicator of

56
00:03:09.640 --> 00:03:13.290
whether a person is underweight,
average weight, or overweight.

57
00:03:14.290 --> 00:03:17.890
This is an important feature to have for
a weight loss application.

58
00:03:17.890 --> 00:03:22.270
It represents information about how much
a person weighs relative to their height

59
00:03:22.270 --> 00:03:26.230
that is not available by looking at just
the person's height or weight alone.

60
00:03:27.840 --> 00:03:31.480
A feature can be re-coded as
appropriate for the application.

61
00:03:31.480 --> 00:03:35.550
A common example of this is when you
want to turn a continuous feature in to

62
00:03:35.550 --> 00:03:37.420
a categorical one.

63
00:03:37.420 --> 00:03:42.324
For example for a marketing application
you might want to re-code customer's age

64
00:03:42.324 --> 00:03:48.910
into customer categories such as teenager,
young adult, adult and senior citizen.

65
00:03:48.910 --> 00:03:55.310
So you would map ages 13 to 19 to
teenager, ages 20 to 25 to young adult,

66
00:03:55.310 --> 00:04:00.370
26 to 55 as adult and over 55 as senior.

67
00:04:01.590 --> 00:04:05.640
For some applications you may want
to make use of finery features.

68
00:04:05.640 --> 00:04:09.550
As an example, you might want a feature to
capture whether a customer tends to buy

69
00:04:09.550 --> 00:04:11.810
expensive items or not.

70
00:04:11.810 --> 00:04:16.100
In this case, you would want a feature
that maps to one for a customer with

71
00:04:16.100 --> 00:04:20.484
an average purchase price over a certain
amount and maps to zero otherwise.

72
00:04:20.484 --> 00:04:24.960
Re-coding features can also
result in breaking one feature

73
00:04:24.960 --> 00:04:26.920
into multiple features.

74
00:04:26.920 --> 00:04:30.730
A common example of this is to
separate an address feature

75
00:04:30.730 --> 00:04:36.580
into its constituent parts street address,
city, state and zip code.

76
00:04:36.580 --> 00:04:39.280
This way you can more easily
group records by state, for

77
00:04:39.280 --> 00:04:42.130
example, to provide
a state by state analysis.

78
00:04:43.550 --> 00:04:48.280
Future selection aims to select
the smallest set of features to best

79
00:04:48.280 --> 00:04:51.100
capture the characteristics of
the data for your application.

80
00:04:52.390 --> 00:04:56.310
Know from the examples represented
that domain knowledge once again place

81
00:04:56.310 --> 00:05:00.400
a key role in choosing
the appropriate features to use.

82
00:05:00.400 --> 00:05:04.934
Good understanding of the application is
essential in deciding which features to

83
00:05:04.934 --> 00:05:06.190
add, drop or modify.

84
00:05:07.350 --> 00:05:11.256
It should also be noted that feature
selection can be referred to as feature

85
00:05:11.256 --> 00:05:15.596
engineering, since what you're doing here
is to engineer the best feature set for

86
00:05:15.596 --> 00:05:16.661
your application.
