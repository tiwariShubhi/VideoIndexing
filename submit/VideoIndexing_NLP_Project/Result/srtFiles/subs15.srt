
1
00:00:03.030 --> 00:00:05.956
It can be said without a doubt,
and the Internet and

2
00:00:05.956 --> 00:00:08.960
the worldwide web changed
everything in our lives.

3
00:00:10.450 --> 00:00:15.100
The worldwide web is indeed the largest
information source there is today.

4
00:00:15.100 --> 00:00:16.630
But what's the data model behind the web?

5
00:00:17.800 --> 00:00:20.740
We will say that it is
the semi-structure data model.

6
00:00:22.310 --> 00:00:26.560
So after going through this video you
will be able to distinguish between

7
00:00:26.560 --> 00:00:29.870
the structured data model that we
talked about the last time and

8
00:00:29.870 --> 00:00:32.805
semi-structured data model.

9
00:00:32.805 --> 00:00:36.110
Further, you will recognize that the most

10
00:00:36.110 --> 00:00:40.680
times the semi-structured data
refers to tree structured data.

11
00:00:41.720 --> 00:00:47.160
And you can explain why tree
navigation operations are important for

12
00:00:47.160 --> 00:00:49.370
formats like XML and JSON.

13
00:00:51.050 --> 00:00:53.290
Let's a take a very simple web page.

14
00:00:55.090 --> 00:00:58.740
Now this page does not have
a lot of content or stylization.

15
00:00:58.740 --> 00:01:01.970
It doesn't even have links to other pages,
but

16
00:01:01.970 --> 00:01:04.390
let's look at the corresponding HTML code.

17
00:01:06.560 --> 00:01:11.340
This code is used by the browser so
that it can render the HTML, and

18
00:01:11.340 --> 00:01:13.580
notice a few things in this data.

19
00:01:13.580 --> 00:01:19.710
The entire data comes within the HTML and
slash HTML blocks.

20
00:01:21.760 --> 00:01:28.590
And we similarly have a body begin and
end, a header begin and

21
00:01:28.590 --> 00:01:33.950
end, a list begin and end and
a paragraph begin and end.

22
00:01:36.020 --> 00:01:40.330
Everywhere here a block is
nested within a larger block.

23
00:01:41.770 --> 00:01:46.350
The second item to notice is that
unlike a relational structure

24
00:01:46.350 --> 00:01:49.940
there are multiple list items and
multiple paragraphs.

25
00:01:49.940 --> 00:01:53.040
And any single document would
have a different number of them.

26
00:01:54.100 --> 00:01:59.250
This means while the date object has
some structure it is more flexible.

27
00:02:00.360 --> 00:02:05.930
So this is the hallmark office
semi structure date model.

28
00:02:05.930 --> 00:02:07.020
Now XML, or

29
00:02:07.020 --> 00:02:12.100
the extensible markup language, is another
well known standard to represent data.

30
00:02:12.100 --> 00:02:16.880
You can think of XML as a generalization
of HTML where the elements, that's

31
00:02:16.880 --> 00:02:21.270
the beginning and end markers within
the angular brackets, can be any string.

32
00:02:21.270 --> 00:02:24.240
And not like the ones
allowed by standard HTML.

33
00:02:25.530 --> 00:02:28.840
Let's see an example
from a biological case.

34
00:02:30.140 --> 00:02:34.770
As you can see, there are two
elements called sample attribute.

35
00:02:36.100 --> 00:02:39.130
They do structurally
different because they

36
00:02:39.130 --> 00:02:42.320
have different numbers of sub
elements called the value.

37
00:02:44.030 --> 00:02:46.960
Another interesting issue
about XML data processing

38
00:02:46.960 --> 00:02:50.530
is that you can actually credit for
the structure elements.

39
00:02:50.530 --> 00:02:55.560
For example, it is perfectly fine to ask,
what is the name of the element

40
00:02:55.560 --> 00:02:59.250
which contains a sub-element whose
textual content is cell type?

41
00:03:00.390 --> 00:03:06.990
As you can see, you'll get two results,
sample attribute.

42
00:03:06.990 --> 00:03:11.604
An experimental factor because sample
attribute has a sub-element called

43
00:03:11.604 --> 00:03:15.940
category and experimental factor
has a subelement called link and

44
00:03:15.940 --> 00:03:18.860
each of these subelements
have the value celltape.

45
00:03:20.090 --> 00:03:23.860
Now we cannot perform an operation
like this in a relational data model.

46
00:03:23.860 --> 00:03:28.090
For example, we cannot say which relation
has a column with a value, John.

47
00:03:30.730 --> 00:03:35.740
The same idea can also be seen in JSON or
the Java Script Object Notation, which

48
00:03:35.740 --> 00:03:41.610
is a very popular format used for many
different data like Twitter and Facebook.

49
00:03:41.610 --> 00:03:47.170
Consider the example here,
all of the format looks different.

50
00:03:47.170 --> 00:03:50.390
We have a similar nested
structure varies that is lists

51
00:03:50.390 --> 00:03:55.030
containing other lists which will contain
topples Which consists of p value ps.

52
00:03:57.080 --> 00:04:01.160
So the key value pairs at atomic
property names and their values.

53
00:04:02.660 --> 00:04:07.760
But one way to generalize about all these
different forms of semi structured data

54
00:04:07.760 --> 00:04:10.460
is to model them as trees.

55
00:04:10.460 --> 00:04:12.380
Let's go back to .xml.

56
00:04:12.380 --> 00:04:15.210
The left side shows an XML document, and

57
00:04:15.210 --> 00:04:17.440
the right side shows
the corresponding tree.

58
00:04:18.660 --> 00:04:22.030
Since the top object of
the root element is document,

59
00:04:22.030 --> 00:04:23.230
it is also the root of the tree.

60
00:04:24.550 --> 00:04:28.620
Now under document we have
a report element with author and

61
00:04:28.620 --> 00:04:34.450
date under it, and also a paper element
with title, author, and source under it.

62
00:04:34.450 --> 00:04:38.670
The actual values,
like is the textual content of an element.

63
00:04:40.480 --> 00:04:43.760
Since a text data item cannot
have any further components,

64
00:04:43.760 --> 00:04:46.590
these text values are always
the leaves of the tree.

65
00:04:48.760 --> 00:04:53.950
Now, modeling a document as a tree
has significant advantages.

66
00:04:53.950 --> 00:04:55.740
A tree is a well-known data structure,

67
00:04:55.740 --> 00:05:00.100
that allows what's called
a navigational access to data.

68
00:05:00.100 --> 00:05:02.150
Imagine you are standing
on the note paper.

69
00:05:03.460 --> 00:05:08.610
Now you can perform a getParent
operation and navigate the document.

70
00:05:08.610 --> 00:05:14.460
Or you can perform a getChildren operation
to get to the title, author and source.

71
00:05:14.460 --> 00:05:18.550
You can even perform a getSiblings
operation and get to the report.

72
00:05:19.810 --> 00:05:24.970
You can also ask a textual query like
which strings have the substring data and

73
00:05:24.970 --> 00:05:31.340
seek their root-to-node path to get to
the path from document to the text nodes.

74
00:05:31.340 --> 00:05:34.590
You can possibly see how queries
can be evaluated on the tree,

75
00:05:35.670 --> 00:05:37.250
now let us take the query.

76
00:05:37.250 --> 00:05:39.990
Who is the author of XML query data model.

77
00:05:41.900 --> 00:05:47.987
In one evaluation scheme we can navigate
up from the text note to title,

78
00:05:47.987 --> 00:05:53.580
to paper, and then navigate down
to author and then to Don Robie.

79
00:05:53.580 --> 00:05:56.990
Well how do we know that we have to get up
to paper before reversing the direction?

80
00:05:58.050 --> 00:06:01.470
Well, paper is the least,
that's the lowest in the tree,

81
00:06:01.470 --> 00:06:07.170
common ancestor of the author note,
and the XM query data model note.

82
00:06:07.170 --> 00:06:10.207
We will come back to semi
structure data in a later module.

1
00:00:01.851 --> 00:00:05.973
In this hands-on activity,
we will be looking at graph data in Gephi.

2
00:00:05.973 --> 00:00:10.510
First, we will import data into Gephi and
then examine the properties of the graph.

3
00:00:11.720 --> 00:00:15.160
Next, we will perform some statistical
operations on the graph data,

4
00:00:15.160 --> 00:00:17.750
and then run some different
layout algorithms.

5
00:00:20.090 --> 00:00:21.585
Let's begin.

6
00:00:21.585 --> 00:00:26.370
We're not running Gephi in the Cloudera
virtual machine, on the Coursera website,

7
00:00:26.370 --> 00:00:30.450
there will be a reading with instructions
on how to download, install, and

8
00:00:30.450 --> 00:00:34.050
run Gephi on your native hardware,
instead of in the virtual machine.

9
00:00:35.960 --> 00:00:39.693
Once you have Gephi started,
let's import the data into Gephi.

10
00:00:39.693 --> 00:00:46.908
We'll go to File Import spreadsheet,
and in the CSV dialog,

11
00:00:46.908 --> 00:00:52.338
we'll click the button with dot, dot, dot.

12
00:00:52.338 --> 00:00:57.679
We'll choose diseasegraph.csv and
click open.

13
00:01:01.336 --> 00:01:06.420
Make sure that as table says
edges table Click next.

14
00:01:09.570 --> 00:01:13.535
And make sure Create
missing nodes is checked.

15
00:01:13.535 --> 00:01:17.215
We'll click finish to import
the CSV file as a graph.

16
00:01:17.215 --> 00:01:19.630
Gephi now shows the graph
in the center pane.

17
00:01:21.270 --> 00:01:23.930
The little black circles
are the nodes of the graph and

18
00:01:23.930 --> 00:01:26.500
the lines between them are the edges.

19
00:01:26.500 --> 00:01:32.707
In the top right we can see that
there are 777 nodes and 998 edges.

20
00:01:32.707 --> 00:01:36.380
Next let's perform some statistical
operations on this graph.

21
00:01:37.510 --> 00:01:42.145
In the statistics pane we
can see average degree.

22
00:01:42.145 --> 00:01:44.980
Let's compute the average degree
of the graph by clicking on Run.

23
00:01:49.320 --> 00:01:53.260
This says that the average
degree is 2.569.

24
00:01:53.260 --> 00:01:58.693
Let's close this,
let's compute the connected

25
00:01:58.693 --> 00:02:02.583
components, we'll click on Run.

26
00:02:02.583 --> 00:02:05.400
We'll leave this as a directed,
since the graph is directed.

27
00:02:07.140 --> 00:02:09.930
Click OK.
It says that there

28
00:02:09.930 --> 00:02:14.940
are 5 weakly connected components,
and 761 strongly connected components.

29
00:02:16.440 --> 00:02:17.400
Let's close this.

30
00:02:18.470 --> 00:02:21.710
Next, let's run some different
layout algorithms over the graph.

31
00:02:23.240 --> 00:02:25.760
The bottom left,
we'll go to choose layout.

32
00:02:27.550 --> 00:02:32.019
We'll choose Force Atlas and click run.

33
00:02:50.345 --> 00:02:52.080
Click stop to stop the layout.

34
00:02:53.655 --> 00:02:58.200
We can see that Gephi has grouped
strongly connected components together

35
00:02:58.200 --> 00:02:59.320
in different clusters.

36
00:02:59.320 --> 00:03:01.920
We can also see that they're parts
of the graph that are not connected.

37
00:03:03.440 --> 00:03:04.830
Let's run a different layout algorithm.

38
00:03:06.080 --> 00:03:11.395
The combo box,
choose Fruchterman Reingold, click run.

39
00:03:18.234 --> 00:03:23.125
After it runs for a few seconds, click
stop, then click the magnifying glass,

40
00:03:23.125 --> 00:03:25.680
center on graph, to see the whole graph.

41
00:03:28.320 --> 00:03:31.700
In this layout,
all the nodes appear to be equally spaced.

42
00:03:31.700 --> 00:03:34.537
But we can also see
the nodes with many edges.

1
00:00:01.350 --> 00:00:04.350
In this hands on activity,
we will be working with Lucene,

2
00:00:04.350 --> 00:00:07.310
a search engine that uses a vector
space model to index data.

3
00:00:08.430 --> 00:00:10.580
First, we will open a terminal window and

4
00:00:10.580 --> 00:00:14.200
change into the directory
containing the data and scripts.

5
00:00:14.200 --> 00:00:18.700
Next, we will index some text
documents and query terms in Lucene.

6
00:00:19.960 --> 00:00:22.538
After that we'll query
using weighted terms or

7
00:00:22.538 --> 00:00:25.810
boosting to see how this
changes the rankings.

8
00:00:25.810 --> 00:00:30.045
Finally, we will show the term
frequency-inverse document frequency or

9
00:00:30.045 --> 00:00:32.988
TF-IDF in terms.

10
00:00:32.988 --> 00:00:35.300
Let's begin.

11
00:00:35.300 --> 00:00:39.139
First, let's open a terminal window by
clicking on the terminal icon at the top

12
00:00:39.139 --> 00:00:39.953
of the tool bar.

13
00:00:43.413 --> 00:00:47.090
Next, let's cd into the directory
containing the scripts and data.

14
00:00:47.090 --> 00:00:48.679
We'll run cd

15
00:00:48.679 --> 00:00:58.560
Downloads/big-data-2/vector/.

16
00:00:58.560 --> 00:01:02.201
We'll run ls to see the scripts.

17
00:01:06.543 --> 00:01:09.670
The data directory
contains three text files.

18
00:01:09.670 --> 00:01:13.310
Each of these text files contains
news data about elections.

19
00:01:15.730 --> 00:01:21.497
Let's index these files by
running runLuceneQuery.sh data.

20
00:01:28.974 --> 00:01:35.740
Next, let's query Lucene for some terms in
these text documents and see the rankings.

21
00:01:35.740 --> 00:01:37.970
Let's query for the term voters.

22
00:01:43.242 --> 00:01:47.973
You can see the rankings and
scores that news1.csv ranked first,

23
00:01:47.973 --> 00:01:52.900
news2.csv was the second ranking,
and the third was news3.csv.

24
00:01:52.900 --> 00:01:56.359
Let's query for delegates.

25
00:01:56.359 --> 00:02:01.324
For this term, we see that news2.csv

26
00:02:01.324 --> 00:02:06.288
was first, and news1 was second, and

27
00:02:06.288 --> 00:02:11.260
news3 did not contain the term at all.

28
00:02:13.710 --> 00:02:17.275
Now, let's query for both terms,
voters and delegates.

29
00:02:17.275 --> 00:02:24.935
In this result we see that
news2 was ranked first,

30
00:02:24.935 --> 00:02:29.566
news1 was ranked second, and

31
00:02:29.566 --> 00:02:33.500
news3 was ranked third.

32
00:02:35.230 --> 00:02:37.000
Now lets use query term waiting or

33
00:02:37.000 --> 00:02:40.920
boosting to increase
the relevance of voters.

34
00:02:40.920 --> 00:02:45.900
I can do this by ensuring
voters carat 5 delegates.

35
00:02:45.900 --> 00:02:53.524
The carat 5 notation is a syntax for

36
00:02:53.524 --> 00:02:58.360
Lucene for boosting.

37
00:02:59.890 --> 00:03:05.990
When we run this, we see that now,
news1 is ranked first,

38
00:03:05.990 --> 00:03:10.170
news2 is ranked second,
and news3 is ranked third.

39
00:03:10.170 --> 00:03:14.500
Notice this is different from the original
query with voters and delegates,

40
00:03:14.500 --> 00:03:18.100
where news2 is ranked first and
news1 was ranked second.

41
00:03:20.210 --> 00:03:23.890
Now let's look at the term frequency,
inverse document frequency or TF-IDF.

42
00:03:23.890 --> 00:03:28.465
We'll enter q to quit this and

43
00:03:28.465 --> 00:03:33.960
we'll run Lucene TF-IDF SH data.

44
00:03:45.380 --> 00:03:51.000
Let's look at the TF-IDF for voters.

45
00:03:51.000 --> 00:03:53.320
You can see that it ranked number 1 for
news1.

46
00:03:54.370 --> 00:03:58.160
Second news 2 and news 3 is last.

47
00:03:58.160 --> 00:03:59.170
Lets try delegates.

48
00:04:05.110 --> 00:04:09.040
Here we see that news 2 had
a higher score than news 1,

49
00:04:09.040 --> 00:04:12.850
and news 3 is not listed because
news 3 does not contain this term.

50
00:04:13.870 --> 00:04:14.590
Hit q to quit.

1
00:00:00.800 --> 00:00:05.529
So the next category of data we discuss
has the form of graphs or networks,

2
00:00:05.529 --> 00:00:08.749
the most obvious example
being social networks.

3
00:00:08.749 --> 00:00:10.782
Now speaking of social networks,

4
00:00:10.782 --> 00:00:15.760
Tim Libzek created a social network
from the Lord of the Rings Trilogy.

5
00:00:15.760 --> 00:00:18.545
This graph represents
the characters' allegiances,

6
00:00:18.545 --> 00:00:20.744
that is who is faithful
to whom in the books.

7
00:00:20.744 --> 00:00:24.670
So the nodes are characters and
other entities, like cities, and

8
00:00:24.670 --> 00:00:29.070
the edges connecting pairs of
nodes represent allegiances.

9
00:00:29.070 --> 00:00:35.480
So after this video, you'll be able to
identify graph data in practical problems

10
00:00:35.480 --> 00:00:41.150
and describe path, neighborhood, and
connectivity operations in graphs.

11
00:00:41.150 --> 00:00:44.860
But this specialization includes
a separate course in graph analytics

12
00:00:44.860 --> 00:00:48.470
that provides a much more detailed
treatment on the subject.

13
00:00:48.470 --> 00:00:51.810
Now what distinguishes a graph
from other data models

14
00:00:51.810 --> 00:00:55.400
is that it bears two kinds of information.

15
00:00:55.400 --> 00:01:00.080
One, properties and attributes of
entities and relationships, and

16
00:01:00.080 --> 00:01:04.130
two, the connectivity structure that
constitutes the network itself.

17
00:01:05.480 --> 00:01:08.260
One way to look at this data
is shown in the figure,

18
00:01:08.260 --> 00:01:10.070
borrowed from the Apache Spark system.

19
00:01:11.130 --> 00:01:12.360
In this representation,

20
00:01:12.360 --> 00:01:16.390
the graph on the left is represented
by two tables on the right.

21
00:01:16.390 --> 00:01:22.095
The vertex, or node table, gives IDs
to nodes and lists their properties.

22
00:01:23.170 --> 00:01:25.720
The edge table has two parts.

23
00:01:25.720 --> 00:01:29.210
The colored part represents
the properties of the edge,

24
00:01:29.210 --> 00:01:33.600
whereas the white part contains just the
direction of the arrows in the network.

25
00:01:33.600 --> 00:01:39.280
Thus, since there is a directed
edge going from node 3 to node 7,

26
00:01:39.280 --> 00:01:43.830
there is a tupple 3,
7 in that part of the edge table.

27
00:01:43.830 --> 00:01:48.540
Now this form of the graph model is
called the property graph model,

28
00:01:48.540 --> 00:01:52.760
which we'll see many times in this
course and in the specialization.

29
00:01:52.760 --> 00:01:57.740
Now representing connectivity information
gives graph data a new kind of

30
00:01:57.740 --> 00:02:01.910
computing ability that's different from
other data models we have seen so far.

31
00:02:03.090 --> 00:02:07.450
Even without looking at the properties
of the nodes and edges, one can get very

32
00:02:07.450 --> 00:02:12.640
interesting information just by analyzing
or querying this connectivity structure.

33
00:02:13.800 --> 00:02:18.990
Consider a social network with three
types of nodes, user, city, and

34
00:02:18.990 --> 00:02:24.250
restaurant, and three types of edges,
friend, likes, and lives in.

35
00:02:25.300 --> 00:02:28.110
The leftmost node, AG, represents me.

36
00:02:28.110 --> 00:02:31.930
And I'm interested in finding a good
Italian restaurant in New York

37
00:02:31.930 --> 00:02:36.360
that my friends, or their friends,
who also live in New York, like.

38
00:02:36.360 --> 00:02:41.765
I shall possibly choose IT3 because
it has the highest number of

39
00:02:41.765 --> 00:02:47.700
like edges coming into it from people
who have a lives in edge to New York.

40
00:02:47.700 --> 00:02:49.008
And at the same time,

41
00:02:49.008 --> 00:02:52.869
can be reached by following
the friend edges going out from me.

42
00:02:52.869 --> 00:02:57.306
Now this shows a very important class
of operations and ground data, namely

43
00:02:57.306 --> 00:03:01.830
traversal, that involves edge following
based on some sort of conditions.

44
00:03:03.090 --> 00:03:07.660
A number of path operations
required some sort of optimization.

45
00:03:07.660 --> 00:03:12.885
The simplest among these is the well known
shortest path query, which is applied to

46
00:03:12.885 --> 00:03:17.750
node networks to find the best route from
a source location to a target location.

47
00:03:17.750 --> 00:03:21.215
The second class of optimization
operations is required to find

48
00:03:21.215 --> 00:03:26.050
an optimal path that must include
some user specified nodes, for

49
00:03:26.050 --> 00:03:30.420
the operation has to determine the order
in which the nodes once we visited.

50
00:03:30.420 --> 00:03:32.795
The classical application
is a trip planner,

51
00:03:32.795 --> 00:03:36.610
where the user specifies the cities
she wishes to visit, and

52
00:03:36.610 --> 00:03:40.740
the operation will optimize the criterion,
like the total distance covered.

53
00:03:40.740 --> 00:03:45.260
The third category is a case where
the system must find the best possible

54
00:03:45.260 --> 00:03:47.690
path in the network, given two or

55
00:03:47.690 --> 00:03:52.070
more optimization criteria,
which cannot be satisfied simultaneously.

56
00:03:52.070 --> 00:03:56.350
For example, if I want to travel
from my house to the airport

57
00:03:56.350 --> 00:04:01.150
using the shortest distance, but also
minimizing the amount of highway travel,

58
00:04:01.150 --> 00:04:03.710
the algorithm must find a best compromise.

59
00:04:03.710 --> 00:04:07.510
This is called a pareto-optimality
problem on graphs.

60
00:04:07.510 --> 00:04:13.120
The neighborhood of a node N in a graph is
a set of edges directly connected to it.

61
00:04:13.120 --> 00:04:18.360
A K neighborhood of N is a collection
of edges between nodes that are,

62
00:04:18.360 --> 00:04:21.160
at most, K steps away from N.

63
00:04:21.160 --> 00:04:24.710
So going back to our mini social
network graph, Bob, Jill,

64
00:04:24.710 --> 00:04:29.630
and Sarah are the first neighbors of AG,
while Max, Tim and

65
00:04:29.630 --> 00:04:34.570
Pam belong to the second neighborhood and
not the first neighborhood of AG.

66
00:04:34.570 --> 00:04:37.910
Finally, Jen is a third level neighbor.

67
00:04:38.910 --> 00:04:43.790
An important class of analysis to perform
with neighborhoods is community finding.

68
00:04:43.790 --> 00:04:47.740
A community and a social network can
be a very close group of friends.

69
00:04:47.740 --> 00:04:51.340
So the graph shown in this
figure has four communities.

70
00:04:51.340 --> 00:04:56.327
One can see in the figure that each
community has a higher density of edges

71
00:04:56.327 --> 00:05:02.480
within the community and a lower density
across two different communities.

72
00:05:02.480 --> 00:05:05.440
Finding densely connected parts of a graph

73
00:05:05.440 --> 00:05:09.860
helps identify neighborhoods that
can be recognized as communities.

74
00:05:09.860 --> 00:05:15.032
A more complex class of operations include
finding the best possible clusters,

75
00:05:15.032 --> 00:05:17.620
which is another name for
communities in a graph, so

76
00:05:17.620 --> 00:05:22.030
that any other grouping of nodes into
communities will be less effective.

77
00:05:22.030 --> 00:05:27.430
Now, as graphs become bigger and denser,
these methods become harder to compute.

78
00:05:27.430 --> 00:05:32.240
Thus, neighborhood-based
optimization operation present

79
00:05:32.240 --> 00:05:34.520
significant scalability challenges.

80
00:05:34.520 --> 00:05:38.650
If we inspect the neighborhood of
every node in a graph, sometimes,

81
00:05:38.650 --> 00:05:42.650
we'll find neighborhoods that
are different from all others.

82
00:05:42.650 --> 00:05:45.390
These neighborhoods are called anomalous.

83
00:05:45.390 --> 00:05:49.720
Consider the following four graphs and
on the central red node.

84
00:05:49.720 --> 00:05:54.177
The first graph is odd because
it's almost perfectly star shaped.

85
00:05:54.177 --> 00:05:58.751
That is, the nodes that the red node
is connected to are almost unconnected

86
00:05:58.751 --> 00:06:00.137
amongst themselves.

87
00:06:00.137 --> 00:06:04.000
That's really odd because it
doesn't happen in reality much.

88
00:06:04.000 --> 00:06:05.620
So it's an anomalous node.

89
00:06:05.620 --> 00:06:09.630
The second figure shows
a neighborhood to which

90
00:06:09.630 --> 00:06:13.770
a significantly large number of neighbors
has connected amongst themselves.

91
00:06:13.770 --> 00:06:18.860
This makes the graph very cliquish,
where a clique refers to a neighborhood

92
00:06:18.860 --> 00:06:23.280
where each node is connected to all other
neighborhood nodes in the neighborhood.

93
00:06:23.280 --> 00:06:25.770
The third figure shows a neighborhood,

94
00:06:25.770 --> 00:06:31.040
where some edges have an unusually
heavy weight compared to the others.

95
00:06:31.040 --> 00:06:34.578
The fourth figure shows
a special case of the third,

96
00:06:34.578 --> 00:06:38.930
where one edge is predominantly high
rate compared to all the other edges.

97
00:06:40.120 --> 00:06:42.890
Connectedness is a fundamental
property of a graph.

98
00:06:43.960 --> 00:06:45.390
In a connected graph,

99
00:06:45.390 --> 00:06:49.570
each node is reachable from every
other node through some path.

100
00:06:49.570 --> 00:06:54.750
If a graph is not connected, but there
are subgraphs of it, which are connected,

101
00:06:54.750 --> 00:06:58.745
then these subgraphs are called connected
components of the original graph.

102
00:06:58.745 --> 00:07:01.799
In the figure on the right,
there are four connected components.

103
00:07:01.799 --> 00:07:05.805
A search gradient like
finding optimal paths

104
00:07:05.805 --> 00:07:09.075
should be performed only within
each component and not across them.

105
00:07:10.215 --> 00:07:14.325
For large graphs, there are several
new parallelized techniques for

106
00:07:14.325 --> 00:07:15.835
the detection of connected components.

107
00:07:17.070 --> 00:07:19.680
We will discuss a map
reduce based technique for

108
00:07:19.680 --> 00:07:21.830
connected components in a later course.

1
00:00:03.200 --> 00:00:05.569
We have discussed quite a few data models,
but

2
00:00:05.569 --> 00:00:08.669
there are many other data models
that have been developed for

3
00:00:08.669 --> 00:00:13.500
various purposes, and we really cannot
cover all of them in a single course.

4
00:00:13.500 --> 00:00:17.360
We'll end these lectures on data models
with an example that may give you

5
00:00:17.360 --> 00:00:21.560
an insight into a class of objects that
define in many different applications.

6
00:00:22.850 --> 00:00:27.972
So after this video you'll be able
to describe how arrays can serve

7
00:00:27.972 --> 00:00:33.278
as a data model, explain why images
can be modeled as vector arrays,

8
00:00:33.278 --> 00:00:37.689
specify a set of operations on scalar and
vector arrays.

9
00:00:39.969 --> 00:00:42.690
Now, we have all seen the arrays.

10
00:00:42.690 --> 00:00:46.390
In the simplest case,
an array is a matrix like this.

11
00:00:46.390 --> 00:00:48.380
Let's call this array A.

12
00:00:50.960 --> 00:00:55.140
The top row in yellow,
gives the column numbers and

13
00:00:55.140 --> 00:00:57.890
the left column, also in yellow,
gives the row numbers.

14
00:00:58.970 --> 00:01:03.263
When we need to refer to a value
of the array as A(3, 2),

15
00:01:03.263 --> 00:01:07.530
we mean the value of the cell in row 3 and
column 2.

16
00:01:07.530 --> 00:01:12.270
This is called indexed structure,
where 3 and

17
00:01:12.270 --> 00:01:18.070
2 are the row and column indices that are
necessary to get the value of a data item.

18
00:01:19.430 --> 00:01:21.830
The area has two dimensions.

19
00:01:21.830 --> 00:01:24.050
So hence there are two indexes.

20
00:01:24.050 --> 00:01:27.650
If these were a three dimensional array,
we would have three indexes.

21
00:01:28.860 --> 00:01:29.390
Now earlier,

22
00:01:29.390 --> 00:01:34.360
we have seen that we can represent the two
dimensional array as a three column table.

23
00:01:34.360 --> 00:01:37.790
One column for the row index,
one column for the column index, and

24
00:01:37.790 --> 00:01:38.849
the last column for the value.

25
00:01:40.310 --> 00:01:45.310
Thus a k dimensional array can be
represented as a relation with k

26
00:01:45.310 --> 00:01:46.090
plus one columns.

27
00:01:47.690 --> 00:01:51.100
The number of tuples in this
representation will be the product of

28
00:01:51.100 --> 00:01:55.220
the size of the first dimension times the
size of the second dimension and so forth.

29
00:01:56.670 --> 00:02:01.000
Then in this case,
the size is five in each dimension.

30
00:02:01.000 --> 00:02:08.070
So there are 25 C column tuples in
a relation representing the array.

31
00:02:08.070 --> 00:02:11.890
A more useful situation occurs
when the cells of an array have

32
00:02:11.890 --> 00:02:13.490
a vectors as values.

33
00:02:14.620 --> 00:02:19.610
As you can see in the 2D vector array
here, each cell has a three vector.

34
00:02:19.610 --> 00:02:21.340
That is a vector with three elements.

35
00:02:22.400 --> 00:02:25.150
Therefore, if we want to
receive a cell value and

36
00:02:25.150 --> 00:02:27.930
treat it like before,
we'll get back the whole vector.

37
00:02:29.220 --> 00:02:31.680
Now, this type of data
should look familiar to you,

38
00:02:31.680 --> 00:02:35.840
because images often have a red,
green and blue channels per pixel.

39
00:02:37.030 --> 00:02:38.050
In other words,

40
00:02:38.050 --> 00:02:43.780
images of vector valued arrays where each
array cell has a three color vector.

41
00:02:45.270 --> 00:02:49.000
We can also think of the array model
in the context of satellite images.

42
00:02:49.000 --> 00:02:52.850
Where there are many more channels
depending on the range of wavelengths

43
00:02:52.850 --> 00:02:54.100
each channel catches.

44
00:02:55.360 --> 00:02:58.710
Let us consider the operations
on arrays of vectors.

45
00:02:58.710 --> 00:03:03.430
Because it is a combination of two models,
one can create different combinations of

46
00:03:03.430 --> 00:03:08.140
array operations, vector operations and
composite operations.

47
00:03:08.140 --> 00:03:08.650
Here are some.

48
00:03:09.810 --> 00:03:12.580
The dimension of the array here,
the first operation, is two.

49
00:03:13.940 --> 00:03:16.289
If we pick up any dimension, say one.

50
00:03:16.289 --> 00:03:20.741
The size of it is also two
because they're two elements,

51
00:03:20.741 --> 00:03:23.260
zero and one in each dimension.

52
00:03:23.260 --> 00:03:31.259
As we saw before, the value of the cell
(1,1) is a vector 16, 301, 74.

53
00:03:31.259 --> 00:03:37.979
While the value of A11 component 2 is 74.

54
00:03:37.979 --> 00:03:42.279
The length of the vector is a square root
of the sum of the elements of the vector.

55
00:03:42.279 --> 00:03:48.999
So length of A11 would come to 310.375.

56
00:03:48.999 --> 00:03:50.623
The distance function can be so

57
00:03:50.623 --> 00:03:54.184
simple like the Euclidean distance
function between two vectors or

58
00:03:54.184 --> 00:03:57.820
the cosine of an angle between them
as we saw in the previous lecture.

59
00:03:59.080 --> 00:04:02.580
But it can also be something more complex,
based on the needs of the application.

60
00:04:04.320 --> 00:04:08.390
Obviously one can also perform operations
like selection over indices so

61
00:04:08.390 --> 00:04:13.170
we can ask which cells had
the zero value greater than 25.

62
00:04:13.170 --> 00:04:16.060
Giving as the result zero one and
one zero.

63
00:04:17.190 --> 00:04:20.050
You will experience some of these
operations in your hands on session.
