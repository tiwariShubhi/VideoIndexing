
1
00:00:00.870 --> 00:00:06.650
Big data, generated by organizations,
structured but often siloed.

2
00:00:06.650 --> 00:00:11.930
The last type of big data we will discuss
is big data generated by organizations.

3
00:00:13.260 --> 00:00:18.740
This type of data is the closest to
what most businesses currently have.

4
00:00:18.740 --> 00:00:21.550
But it's considered a bit out of fashion,
or

5
00:00:21.550 --> 00:00:25.180
traditional, compared to
other types of big data.

6
00:00:25.180 --> 00:00:29.760
However, it is at least as important
as other types of big data.

7
00:00:47.566 --> 00:00:51.350
So how do organizations produce data?

8
00:00:51.350 --> 00:00:54.940
The answer to how
an organization generates data

9
00:00:54.940 --> 00:00:57.260
is very unique to the organization and
context.

10
00:00:58.590 --> 00:01:02.540
Each organization has distinct
operation practices and

11
00:01:02.540 --> 00:01:07.860
business models, which result in
a variety of data generation platforms.

12
00:01:07.860 --> 00:01:12.697
For example, the type and source of data
that a bank gets is very different from

13
00:01:12.697 --> 00:01:15.650
what a hardware equipment
manufacturer gets.

14
00:01:17.610 --> 00:01:23.808
Some common types of organizational big
data come from commercial transactions,

15
00:01:23.808 --> 00:01:29.033
credit cards, government institutions,
e-commerce, banking or

16
00:01:29.033 --> 00:01:35.070
stock records, medical records, sensors,
transactions, clicks and so on.

17
00:01:36.350 --> 00:01:39.930
Almost every event can
be potentially stored.

18
00:01:41.500 --> 00:01:45.320
Organizations store this data for
current and

19
00:01:45.320 --> 00:01:49.040
future use, as well as for
analysis of the past.

20
00:01:50.380 --> 00:01:54.970
Let's say you're an organization
that collects sales transactions.

21
00:01:54.970 --> 00:01:59.500
You can use this data for pattern
recognition to detect correlated products,

22
00:02:00.530 --> 00:02:05.990
to estimate demand for
products likely to go up in sales, and

23
00:02:05.990 --> 00:02:07.780
capture fraudulent activity.

24
00:02:08.930 --> 00:02:14.560
Moreover, when you know your
sales record and can correlate it

25
00:02:14.560 --> 00:02:20.370
with your marketing records, you can find
which campaigns really made an impact.

26
00:02:20.370 --> 00:02:24.340
You are already becoming
a data savvy organization.

27
00:02:24.340 --> 00:02:29.180
Now think of bringing your
sales data together with other

28
00:02:29.180 --> 00:02:34.680
external open public data,
such as major world events in the news.

29
00:02:34.680 --> 00:02:38.420
You can ask, was it savvy marketing or

30
00:02:38.420 --> 00:02:43.310
a consequence of external events
that triggered your sales?

31
00:02:43.310 --> 00:02:45.037
Using proper analytics,

32
00:02:45.037 --> 00:02:50.800
you can now build inventories to match
your predicted growth and demand.

33
00:02:50.800 --> 00:02:57.060
In addition, organizations build and
apply processes to record and

34
00:02:57.060 --> 00:03:02.310
monitor business events of interest,
such as registering a customer,

35
00:03:02.310 --> 00:03:05.116
manufacturing a product or
taking an order.

36
00:03:05.116 --> 00:03:10.930
These processes collect
highly structured data

37
00:03:10.930 --> 00:03:14.460
that include transactions,
reference tables, and

38
00:03:14.460 --> 00:03:19.810
relationships, as well as
the metadata that sets its context.

39
00:03:21.130 --> 00:03:26.580
Usually, structured data is stored in
relational database management systems.

40
00:03:28.030 --> 00:03:31.805
However, we call any data
that is in the form of

41
00:03:31.805 --> 00:03:36.708
a record located in a fixed field or
file structured data.

42
00:03:36.708 --> 00:03:40.300
This definition also
includes spreadsheets.

43
00:03:41.750 --> 00:03:46.490
As I have mentioned before,
traditionally this type of

44
00:03:46.490 --> 00:03:51.070
highly structured data is the vast
majority of what IT managed and

45
00:03:51.070 --> 00:03:55.420
processed in both operational and
business intelligence systems.

46
00:03:56.750 --> 00:04:00.410
Let's look at the sales transaction
data in our previous example.

47
00:04:01.890 --> 00:04:05.575
If you look at the data in
the relational table on the right.

48
00:04:05.575 --> 00:04:09.687
As the name structured suggests,

49
00:04:09.687 --> 00:04:14.649
the table is organized to store data using

50
00:04:14.649 --> 00:04:18.493
a structure defined by a model.

51
00:04:18.493 --> 00:04:24.450
Each column is tagged to tell us what
data that column intends to store.

52
00:04:24.450 --> 00:04:26.950
This is what we call a data model.

53
00:04:28.310 --> 00:04:31.680
A data model defines
each of these columns and

54
00:04:31.680 --> 00:04:37.290
fields in the table, and
defines relationships between them.

55
00:04:37.290 --> 00:04:43.960
If you look at the product ID column,
you will see that it includes only

56
00:04:43.960 --> 00:04:48.940
identifiers that can potentially be linked
to another table defining these products.

57
00:04:50.350 --> 00:04:55.990
The ability to define such relationships,
easily make structured data,

58
00:04:55.990 --> 00:05:01.220
or in this case relational databases,
highly adopted by many organizations.

59
00:05:02.460 --> 00:05:06.330
There are commonly used
languages like SQL,

60
00:05:06.330 --> 00:05:11.600
the Structured Query Language, to extract
data of interest from such tables.

61
00:05:12.690 --> 00:05:15.800
This is referred to as querying the data.

62
00:05:17.160 --> 00:05:23.060
However, it could even be a challenge
to integrate such structured data.

63
00:05:23.060 --> 00:05:28.460
This image shows us a continuum
of technologies to model,

64
00:05:28.460 --> 00:05:32.880
collect and query unstructured
data coming from software and

65
00:05:32.880 --> 00:05:35.180
hardware components
within an organization.

66
00:05:36.400 --> 00:05:41.150
In the past, such challenges
led to information being stored

67
00:05:41.150 --> 00:05:45.050
in what we call silos,
even within an organization.

68
00:05:46.400 --> 00:05:51.700
Many organizations have traditionally
captured data at the department level,

69
00:05:51.700 --> 00:05:56.950
without proper infrastructure and
policy to share and integrate this data.

70
00:05:56.950 --> 00:06:01.550
This has hindered the growth of
scalable pattern recognition

71
00:06:01.550 --> 00:06:04.510
to the benefits of
the entire organization.

72
00:06:04.510 --> 00:06:08.700
Because no one system has access to
all data that the organization owns.

73
00:06:10.480 --> 00:06:14.080
Each data set is compartmentalized.

74
00:06:14.080 --> 00:06:20.320
If such silos are left untouched,
organizations risk having outdated,

75
00:06:20.320 --> 00:06:24.320
unsynchronized, and
even invisible data sets.

76
00:06:25.570 --> 00:06:29.260
Organizations are realizing
the detrimental outcomes of this rigid

77
00:06:29.260 --> 00:06:30.820
structure.

78
00:06:30.820 --> 00:06:36.270
And changing policies and infrastructure
to enable integrated processing of

79
00:06:36.270 --> 00:06:39.025
all data to the entire
organization's benefit.

80
00:06:39.025 --> 00:06:44.090
Cloud-based solutions
are seen as agile and

81
00:06:44.090 --> 00:06:47.550
low capital intensive
solutions in this area.

82
00:06:47.550 --> 00:06:53.309
As a summary, while highly structured
organizational data is very useful and

83
00:06:53.309 --> 00:06:57.671
trustworthy, and
thus a valuable source of information,

84
00:06:57.671 --> 00:07:01.860
organizations must pay special
attention to breaking up

85
00:07:01.860 --> 00:07:06.243
the silos of information to
make full use of its potential.

1
00:00:00.660 --> 00:00:03.542
The key, integrating diverse data.

2
00:00:13.178 --> 00:00:16.267
Whatever your big data application is, and

3
00:00:16.267 --> 00:00:21.527
the types of big data you are using
the real value will come from integrating

4
00:00:21.527 --> 00:00:26.056
different types of data sources,
and analyzing them at scale.

5
00:00:26.056 --> 00:00:28.947
So how do we start getting this value?

6
00:00:28.947 --> 00:00:30.706
Sometimes, all it takes,

7
00:00:30.706 --> 00:00:35.620
is looking at the data you already
collect in a different way.

8
00:00:35.620 --> 00:00:39.060
And it can mean a big difference
in your return on investment.

9
00:00:40.590 --> 00:00:45.970
This new story from June 2015
mentions that Carnival Cruises

10
00:00:45.970 --> 00:00:50.760
is using structured and unstructured
data from a variety of sources.

11
00:00:52.420 --> 00:00:55.200
Carnival turns it into profit

12
00:00:55.200 --> 00:00:58.640
using price optimization
techniques on the integrated data.

13
00:00:59.640 --> 00:01:02.460
For you to achieve such a success story,

14
00:01:02.460 --> 00:01:07.030
you will need to include data
integration into your big data practice.

15
00:01:07.030 --> 00:01:11.950
However, there are some unique challenges
when attempting to integrate these

16
00:01:11.950 --> 00:01:16.120
diverse data sources and
scaling the solutions.

17
00:01:16.120 --> 00:01:20.220
Course Two on on the specialization,
we'll teach you more about these issues.

18
00:01:21.260 --> 00:01:26.010
But let's take a moment to define why
effect of data integration is useful?

19
00:01:27.060 --> 00:01:33.040
Data integration means bringing
together data from diverse sources and

20
00:01:33.040 --> 00:01:37.170
turning them into coherent and
more useful information.

21
00:01:38.440 --> 00:01:40.390
We also call this knowledge.

22
00:01:41.740 --> 00:01:47.970
The main objective here is taming or
more technically managing data and

23
00:01:47.970 --> 00:01:52.520
turning it into something you can
make use of programmatically.

24
00:01:52.520 --> 00:01:57.460
A data integration process
involves many parts.

25
00:01:57.460 --> 00:02:03.250
It starts with discovering,
accessing, and monitoring data and

26
00:02:03.250 --> 00:02:09.340
continues with modeling and transforming
data from a variety of sources.

27
00:02:09.340 --> 00:02:13.160
But why do we need data
integration in the first place?

28
00:02:13.160 --> 00:02:16.900
Let's start by focusing on
differences between big data sets

29
00:02:16.900 --> 00:02:18.140
coming from different sources.

30
00:02:19.500 --> 00:02:24.640
You might have flat file formatted data,
relational database data,

31
00:02:24.640 --> 00:02:31.470
data encoded in XML or JSON,
both common for internet generated data.

32
00:02:31.470 --> 00:02:35.170
These different formats and
models are useful

33
00:02:35.170 --> 00:02:38.690
because they are designed to express
different data in unique ways.

34
00:02:39.880 --> 00:02:42.560
In a way, different data formats and

35
00:02:42.560 --> 00:02:49.100
models make big data more useful and
more challenging all at the same time.

36
00:02:49.100 --> 00:02:53.700
When you integrate data in different
formats, you make the final

37
00:02:53.700 --> 00:02:58.030
product richer in the number of
features you describe the data with.

38
00:02:58.030 --> 00:03:03.300
For example,
by integrating environmental sensor and

39
00:03:03.300 --> 00:03:08.300
camera data with geographical
information system data, such as in

40
00:03:08.300 --> 00:03:14.178
my wildfire prediction application,
I can use the spacial data capabilities

41
00:03:14.178 --> 00:03:19.970
with non-spacial data to more
accurately run fire simulations.

42
00:03:21.230 --> 00:03:25.920
In the past, although we were able
to see the images of the fire

43
00:03:27.280 --> 00:03:31.630
from mountain top cameras,
just like this image, we were still

44
00:03:31.630 --> 00:03:35.880
not able to tell what the exact
location of the fire is automatically.

45
00:03:37.460 --> 00:03:41.000
Now, when a fire is detected
from a mountain top camera,

46
00:03:42.180 --> 00:03:47.640
viewsheds are used to estimate
the location of the fire.

47
00:03:49.490 --> 00:03:54.760
This location information can
be fed into the fire simulator

48
00:03:54.760 --> 00:03:58.070
as early as it's detected
to predict the size and

49
00:03:58.070 --> 00:04:02.090
location of the fire in the next
hour more accurately and faster.

50
00:04:03.730 --> 00:04:08.920
Similarly, I can use real time
data with eye curve data sets, and

51
00:04:08.920 --> 00:04:09.940
use them all together.

52
00:04:10.980 --> 00:04:15.520
Additionally, by bringing
the data together, and

53
00:04:15.520 --> 00:04:22.100
providing programmable access to it, I'm
now making each data set more accessible.

54
00:04:23.200 --> 00:04:28.020
Moreover, integration of
diverse datasets significantly

55
00:04:28.020 --> 00:04:32.220
reduces the overall data complexity
in my data-driven product.

56
00:04:33.320 --> 00:04:39.610
The data becomes more available for
use and unified as a system of its own.

57
00:04:40.860 --> 00:04:45.010
One advantage of such an integration
is not often mentioned.

58
00:04:46.500 --> 00:04:49.830
Such a streamlined and
integrated data system

59
00:04:49.830 --> 00:04:53.550
can increase the collaboration between
different parts of your data systems.

60
00:04:54.930 --> 00:04:58.430
Each part can now clearly see

61
00:04:58.430 --> 00:05:02.000
how their data is integrated
into the overall system.

62
00:05:02.000 --> 00:05:07.860
Including the user scenarios and the
security and privacy processes around it.

63
00:05:09.080 --> 00:05:16.320
Overall by integrating diverse data
streams you add value to your big data and

64
00:05:16.320 --> 00:05:20.320
improve your business even
before you start analyzing it.

65
00:05:21.640 --> 00:05:25.783
Next, we will focus on the dimensions
of the scalability and

66
00:05:25.783 --> 00:05:29.937
discuss how we can start tackling
some of these challenges.

1
00:00:01.310 --> 00:00:03.380
What launched the big data era?

2
00:00:04.730 --> 00:00:09.092
In this video, you will learn about
two new opportunities that have

3
00:00:09.092 --> 00:00:12.088
contributed to the launch
of the big data era.

4
00:00:27.737 --> 00:00:32.070
Opportunities are often
a signal of changing times.

5
00:00:34.110 --> 00:00:40.100
In 2013, an influential report by
a company called McKinsey claimed that

6
00:00:40.100 --> 00:00:45.900
the area of data science will be the
number one catalyst for economic growth.

7
00:00:47.380 --> 00:00:50.320
McKinsey identified one of our new

8
00:00:50.320 --> 00:00:54.366
opportunities that contributed to
the launch of the big data era.

9
00:00:54.366 --> 00:00:59.190
A growing torrent of data.

10
00:00:59.190 --> 00:01:05.150
This refers to the idea that data seems to
be coming continuously and at a fast rate.

11
00:01:06.340 --> 00:01:11.330
Think about this,
today you can buy a hard drive to store

12
00:01:11.330 --> 00:01:15.624
all the music in the world for only $600.

13
00:01:16.710 --> 00:01:24.600
That's an amazing storage capability over
any previous forms of music storage.

14
00:01:24.600 --> 00:01:29.286
In 2010 there were 5 billion
mobile phones in use.

15
00:01:29.286 --> 00:01:33.018
You can be sure that
there are more today and

16
00:01:33.018 --> 00:01:37.476
as I'm sure you will understand,
these phones and

17
00:01:37.476 --> 00:01:42.244
the apps we install on them
are a big source of big data,

18
00:01:42.244 --> 00:01:47.342
which all the time, every day,
contributes to our core.

19
00:01:47.342 --> 00:01:50.444
And Facebook, which recently just set

20
00:01:50.444 --> 00:01:55.144
a record of having one billion
people login in a single day,

21
00:01:55.144 --> 00:02:00.320
has more that 30 billion pieces
of content shared every month.

22
00:02:01.430 --> 00:02:03.620
Well, that number's from 2013.

23
00:02:03.620 --> 00:02:07.310
So I'm sure that it's much
higher than that now.

24
00:02:08.720 --> 00:02:11.950
Does it make you think how many
Facebook share's you made last month?

25
00:02:13.760 --> 00:02:17.105
All this leads to projections
of serious growth.

26
00:02:17.105 --> 00:02:24.860
40% in global data per year,
and 5% in global IT spending.

27
00:02:24.860 --> 00:02:29.080
This much data has sure
pushed the data science field

28
00:02:29.080 --> 00:02:32.420
to start remaining itself and
the business world of today.

29
00:02:33.450 --> 00:02:38.970
But, there's something else contributing
to the catalyzing power of data science.

30
00:02:38.970 --> 00:02:41.820
It is called cloud computing.

31
00:02:41.820 --> 00:02:44.600
We call this on demand computing.

32
00:02:44.600 --> 00:02:48.480
Cloud computing is one of
the ways in which computing

33
00:02:48.480 --> 00:02:53.220
has now become something that
we ca do anytime, and anywhere.

34
00:02:53.220 --> 00:02:56.170
You may be surprised to know
that some of your favorite

35
00:02:56.170 --> 00:02:59.900
apps are from businesses
being run from coffee shops.

36
00:02:59.900 --> 00:03:04.420
This new ability,
combined with our torrent of data,

37
00:03:04.420 --> 00:03:10.220
gives us the opportunity to perform novel,
dynamic and scalable data analysis,

38
00:03:10.220 --> 00:03:13.824
to tell us new things about our world and
ourself.

39
00:03:13.824 --> 00:03:20.037
To summarize, a new torrent of big data
combined with computing capability

40
00:03:20.037 --> 00:03:25.968
anytime, anywhere has been at the core
of the launch of the big data era.

1
00:00:01.080 --> 00:00:02.360
Asking the Right Questions.

2
00:00:17.893 --> 00:00:23.810
The first step in any process is to define
what it is you are trying to tackle.

3
00:00:25.030 --> 00:00:27.975
What is the problem that
needs to be addressed, or

4
00:00:27.975 --> 00:00:30.240
the opportunity that
needs to be ascertained.

5
00:00:31.280 --> 00:00:35.223
Without this,
you won't have a clear goal in mind, or

6
00:00:35.223 --> 00:00:38.153
know when you've solved your problem.

7
00:00:38.153 --> 00:00:42.833
An example question is,
how can sales figures and

8
00:00:42.833 --> 00:00:48.072
call center logs be combined
to evaluate a new product,

9
00:00:48.072 --> 00:00:53.534
or in a manufacturing process,
how can data from multiple

10
00:00:53.534 --> 00:00:59.573
sensors in an instrument be used
to detect instrument failure?

11
00:00:59.573 --> 00:01:02.745
How can we understand our customers and

12
00:01:02.745 --> 00:01:07.273
market better to achieve
effective target marketing?

13
00:01:07.273 --> 00:01:11.320
Next you need to assess the situation
with respect to the problem or

14
00:01:11.320 --> 00:01:14.730
the opportunity you have defined.

15
00:01:14.730 --> 00:01:20.011
This is a step where you need to
exercise caution analyzing risks,

16
00:01:20.011 --> 00:01:23.609
costs, benefits, contingencies,

17
00:01:23.609 --> 00:01:28.650
regulations, resources and
requirements of the situation.

18
00:01:28.650 --> 00:01:30.910
What are the requirements of the problem?

19
00:01:30.910 --> 00:01:33.850
What are the assumptions and constraints?

20
00:01:33.850 --> 00:01:35.450
What resources are available?

21
00:01:35.450 --> 00:01:39.614
This is in terms of both personnel and
capital,

22
00:01:39.614 --> 00:01:43.685
such as computer systems, instruments etc.

23
00:01:43.685 --> 00:01:47.425
What are the main costs
associated with this project?

24
00:01:47.425 --> 00:01:49.335
What are the potential benefits?

25
00:01:49.335 --> 00:01:52.625
What risks are there in
pursuing the project?

26
00:01:52.625 --> 00:01:57.125
What are the contingencies to
potential risks, and so on?

27
00:01:57.125 --> 00:02:02.570
Answers to these questions will help you
get a better overview of the situation.

28
00:02:02.570 --> 00:02:05.700
And better understanding of
what the project involves.

29
00:02:05.700 --> 00:02:10.172
Then you need to define your goals and
objectives,

30
00:02:10.172 --> 00:02:13.813
based on the answers to these questions.

31
00:02:13.813 --> 00:02:17.850
Defining success criteria
is also very important.

32
00:02:17.850 --> 00:02:20.770
What do you hope to achieve
by the end of this project?

33
00:02:20.770 --> 00:02:22.368
Having clear goals and and

34
00:02:22.368 --> 00:02:28.280
success criteria will help you to assess
the project throughout its life cycle.

35
00:02:28.280 --> 00:02:32.780
Once you know the problem you want to
address and understand the constraints and

36
00:02:32.780 --> 00:02:37.990
goals, then you can formulate
the plan to come up with the answer,

37
00:02:37.990 --> 00:02:41.330
that is the solution to
your business problem.

38
00:02:41.330 --> 00:02:46.920
As a summary, defining the questions
you're looking to find answers for

39
00:02:46.920 --> 00:02:51.250
is a huge factor contributing to
the success of a data science project.

40
00:02:52.260 --> 00:02:56.830
By following the explained set of steps,
you can formulate better questions

41
00:02:56.830 --> 00:03:01.500
to solve using analytical skills and
link them to business value.

1
00:00:03.028 --> 00:00:04.788
Building a Big Data Strategy.

2
00:00:23.698 --> 00:00:29.760
Before we focus on big data strategy,
let's look at what strategy means.

3
00:00:30.790 --> 00:00:33.090
Although it is associated
with a military term,

4
00:00:34.600 --> 00:00:39.490
a dictionary search on strategy shows
the meaning as a plan of action or

5
00:00:39.490 --> 00:00:43.820
policy designed to achieve a major or
overall aim.

6
00:00:46.200 --> 00:00:51.990
This definition calls out the four major
parts that need to be in any strategy.

7
00:00:51.990 --> 00:00:58.110
Namely, aim, policy, plan, and action.

8
00:00:59.280 --> 00:01:02.620
Now, we are talking about
a big data strategy.

9
00:01:02.620 --> 00:01:05.770
So what do these four terms mean for us?

10
00:01:05.770 --> 00:01:10.382
When building our big data strategy,
we look at what we have,

11
00:01:10.382 --> 00:01:15.793
what high level goals we want to achieve,
what we need to do to get there,

12
00:01:15.793 --> 00:01:20.878
and what are the policies around
data from the beginning to the end.

13
00:01:24.568 --> 00:01:29.210
A big data strategy starts
with big objectives.

14
00:01:29.210 --> 00:01:33.939
Notice that I didn't say it
starts with collecting data

15
00:01:33.939 --> 00:01:39.171
because in this activity we
are really trying to identify what

16
00:01:39.171 --> 00:01:44.118
data is useful and
why by focusing on what data to collect.

17
00:01:44.118 --> 00:01:48.410
Every organization or team is unique.

18
00:01:48.410 --> 00:01:51.120
Different projects have
different objectives.

19
00:01:51.120 --> 00:01:55.960
Hence, it's important to first
define what your team's goals are.

20
00:01:55.960 --> 00:02:00.570
Have you ever had the scenario where you
see the temperature on the weather report

21
00:02:00.570 --> 00:02:03.350
and someone else highlights
the humidity instead?

22
00:02:04.500 --> 00:02:07.820
To find problems relevant to solve and

23
00:02:07.820 --> 00:02:13.290
data related to it, it might be
useful to start with your objectives.

24
00:02:13.290 --> 00:02:15.500
Once you define these objectives, or

25
00:02:15.500 --> 00:02:21.030
more generally speaking, questions to turn
big data into advantage for your business,

26
00:02:21.030 --> 00:02:25.450
you can look at what you have and
analyze the gaps and actions to get there.

27
00:02:26.940 --> 00:02:30.192
It is important to focus
on both short term and

28
00:02:30.192 --> 00:02:33.108
long term objectives in this activity.

29
00:02:33.108 --> 00:02:37.791
These objectives should also be linked
to big data analytics with business

30
00:02:37.791 --> 00:02:38.720
objectives.

31
00:02:38.720 --> 00:02:43.620
To make the best use of big data,
each company needs to evaluate how

32
00:02:43.620 --> 00:02:48.220
data science or big data analytics would
add value to their business objectives.

33
00:02:50.010 --> 00:02:53.730
Once you have established that
analytics can help your business,

34
00:02:53.730 --> 00:02:57.550
you need to create
a culture to embrace it.

35
00:02:57.550 --> 00:02:59.820
The first and foremost ingredient for

36
00:02:59.820 --> 00:03:04.790
a successful data science program
is organizational buy-in.

37
00:03:04.790 --> 00:03:07.810
A big data strategy must
have commitment and

38
00:03:07.810 --> 00:03:11.010
sponsorship from the company's leadership.

39
00:03:11.010 --> 00:03:16.930
Goals for using big data analytics should
be developed with all stakeholders and

40
00:03:16.930 --> 00:03:20.640
clearly communicated to
everyone in the organization.

41
00:03:20.640 --> 00:03:24.740
So that its value is understood and
appreciated by all.

42
00:03:27.350 --> 00:03:30.790
The next step is to build
your data science team.

43
00:03:32.160 --> 00:03:36.930
A diverse team with data scientists,
information technologists,

44
00:03:36.930 --> 00:03:42.520
application developers, and business
owners is necessary to be effective.

45
00:03:42.520 --> 00:03:46.830
As well as the mentality that everyone
works together as partners with

46
00:03:46.830 --> 00:03:47.950
common goals.

47
00:03:47.950 --> 00:03:48.970
Remember, one for all.

48
00:03:50.690 --> 00:03:54.960
No one is a customer or
service provider of another.

49
00:03:54.960 --> 00:03:59.360
Rather, everyone works together and
delivers as a team.

50
00:04:02.470 --> 00:04:05.930
Since big data is a team game,
and multi-disciplinary,

51
00:04:05.930 --> 00:04:10.180
a big part of a big data
strategy is constant training

52
00:04:10.180 --> 00:04:13.400
of team members on new big data tools and
analytics.

53
00:04:13.400 --> 00:04:17.050
As well as business practices and
objectives.

54
00:04:17.050 --> 00:04:22.095
This becomes even more critical if your
business depends on deep expertise

55
00:04:22.095 --> 00:04:27.381
on one or more subject areas with subject
matter experts working on problems,

56
00:04:27.381 --> 00:04:28.838
utilizing big data.

57
00:04:31.947 --> 00:04:37.138
Such businesses might have subject matter
experts who can be trained to add big

58
00:04:37.138 --> 00:04:42.730
data skills, and provide more value added
support than a newcomer would have.

59
00:04:42.730 --> 00:04:47.024
Similarly, any project member would
be trained to understand what

60
00:04:47.024 --> 00:04:50.564
the business objectives and
products are, and how he or

61
00:04:50.564 --> 00:04:55.478
she can utilize big data to improve those
objectives using his or her skills.

62
00:04:58.218 --> 00:05:04.186
Many organizations might benefit by having
a small data science team whose main job

63
00:05:04.186 --> 00:05:10.000
is do data experiments and test new ideas
before they get deployed at full scale.

64
00:05:12.740 --> 00:05:15.200
They might come up with
a new idea themselves

65
00:05:15.200 --> 00:05:16.990
based on the analysis they perform.

66
00:05:18.190 --> 00:05:20.220
They take more research level role.

67
00:05:21.240 --> 00:05:26.010
However, their findings can drastically
shape your business strategy

68
00:05:26.010 --> 00:05:27.540
almost on a daily basis.

69
00:05:29.060 --> 00:05:32.620
The impact of such teams
becomes evident over time

70
00:05:32.620 --> 00:05:36.950
as other parts of your organization starts
to see the results of their finding and

71
00:05:36.950 --> 00:05:38.790
analysis affecting their strategies.

72
00:05:40.110 --> 00:05:45.550
They become strategic partners of
all verticals in your business.

73
00:05:45.550 --> 00:05:47.820
Once you see that something works,

74
00:05:47.820 --> 00:05:52.210
you can start collecting more data to see
similar results at organizational scale.

75
00:05:55.270 --> 00:06:00.330
Since data is key to any big
data initiative, it is essential

76
00:06:00.330 --> 00:06:04.870
that data across the organization
is easily accessed and integrated.

77
00:06:06.280 --> 00:06:10.700
Data silos as you know, are like
a death knell on effective analytics.

78
00:06:12.820 --> 00:06:16.470
So barriers to data
access must be removed.

79
00:06:16.470 --> 00:06:21.430
Opening up the silos must be encouraged
and supported from the organization's

80
00:06:21.430 --> 00:06:26.020
leaders in order to promote a data
sharing mindset for the company.

81
00:06:28.360 --> 00:06:31.900
Another aspect of defining
your big data strategy

82
00:06:31.900 --> 00:06:34.700
is defining the policies around big data.

83
00:06:35.810 --> 00:06:40.190
Although it has an amazing amount
of potential for your business,

84
00:06:40.190 --> 00:06:44.540
using big data should also raise some
concerns in long term planning for data.

85
00:06:45.870 --> 00:06:49.270
Although this is a very complex issue,

86
00:06:49.270 --> 00:06:53.230
here are some questions you should
think of addressing around policy.

87
00:06:53.230 --> 00:06:55.350
What are the privacy concerns?

88
00:06:55.350 --> 00:06:58.770
Who should have access to,
or control data?

89
00:06:58.770 --> 00:07:04.190
What is the lifetime of data,
which is sometimes defined as volatility,

90
00:07:04.190 --> 00:07:05.290
anatomy of big data?

91
00:07:06.970 --> 00:07:09.890
How does data get curated and cleaned up?

92
00:07:11.780 --> 00:07:14.320
What ensures data quality
in the long term?

93
00:07:15.570 --> 00:07:19.530
How do different parts of your
organization communicate or

94
00:07:19.530 --> 00:07:21.400
interoperate using this data?

95
00:07:22.440 --> 00:07:26.070
Are there any legal and
regulatory standards in place?

96
00:07:28.295 --> 00:07:32.270
Cultivating an analytics
driven culture is crucial

97
00:07:32.270 --> 00:07:34.600
to the success of a big data strategy.

98
00:07:36.090 --> 00:07:39.690
The mindset that you want to
establish is that analytics

99
00:07:39.690 --> 00:07:44.200
is an integral part of doing business,
not a separate afterthought.

100
00:07:45.550 --> 00:07:50.170
Analytics activities must be tied
to your business objectives, and

101
00:07:50.170 --> 00:07:53.480
you must be willing to use analytics
in driving business decisions.

102
00:07:54.790 --> 00:07:59.598
Analytics and business together bring
about exciting opportunities and

103
00:07:59.598 --> 00:08:01.750
growth to your big data strategy.

104
00:08:04.980 --> 00:08:08.570
Finally, one size does not fit all.

105
00:08:08.570 --> 00:08:10.780
Hence, big data technologies and

106
00:08:10.780 --> 00:08:15.490
analytics is growing rapidly as your
business is an evolving entity.

107
00:08:16.740 --> 00:08:21.180
You have to iterate your strategy to
take advantage of new advances and

108
00:08:21.180 --> 00:08:24.310
also make your business more
dynamic in the face of change.

109
00:08:27.300 --> 00:08:31.380
As a summary,
when building a big data strategy,

110
00:08:31.380 --> 00:08:35.130
it is important to integrate big data
analytics with business objectives.

111
00:08:36.590 --> 00:08:40.450
Communicate goals and
provide organizational buy-in for

112
00:08:40.450 --> 00:08:42.420
analytics projects.

113
00:08:42.420 --> 00:08:46.990
Build teams with diverse talents,
and establish a teamwork mindset.

114
00:08:48.000 --> 00:08:52.370
Remove barriers to data access and
integration.

115
00:08:52.370 --> 00:08:57.160
Finally, these activities need to be
iterated to respond to new business

116
00:08:57.160 --> 00:08:59.568
goals and technological advances.
