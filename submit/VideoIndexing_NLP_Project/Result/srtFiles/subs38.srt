
1
00:00:01.020 --> 00:00:03.990
We will go over the logic of Dijkstra's
algorithm without writing code.

2
00:00:05.460 --> 00:00:06.940
If you are an advanced student and

3
00:00:06.940 --> 00:00:09.350
know the algorithm,
you can skip to the next lecture.

4
00:00:11.010 --> 00:00:14.840
The basic plan is to start from node I and

5
00:00:14.840 --> 00:00:17.210
progressively traverse
a sequence of notes.

6
00:00:18.580 --> 00:00:22.450
When the method attempts to choose
the next node to traverse to,

7
00:00:22.450 --> 00:00:27.850
it chooses a node for which the total rate
of the path to that node is the lowest.

8
00:00:29.120 --> 00:00:32.130
In the beginning,
the algorithm is on the start node I.

9
00:00:33.530 --> 00:00:36.311
The distance from I to I is 0.

10
00:00:36.311 --> 00:00:38.677
And the distance to all
other nodes is infinity,

11
00:00:38.677 --> 00:00:40.820
because the system doesn't know them yet.

12
00:00:42.560 --> 00:00:45.921
A second table, called a priority queue,

13
00:00:45.921 --> 00:00:51.255
is currently exactly the same as
the distance row of the first array.

14
00:00:52.330 --> 00:00:55.350
The system starts with processing node I,
the source node.

15
00:00:56.350 --> 00:01:01.050
That means it finds the nodes
that can reach from I, F and J.

16
00:01:02.270 --> 00:01:07.121
Note that the respective total weights,
that is 5 for F,

17
00:01:07.121 --> 00:01:11.882
and 15 for J,
to get to these nodes in the distance row.

18
00:01:11.882 --> 00:01:13.520
Then it marks I as done.

19
00:01:14.710 --> 00:01:18.330
We have made the node I agree,
because the node is processed.

20
00:01:20.080 --> 00:01:26.640
Next it looks at row d to find
the least distance, which is 5.

21
00:01:28.560 --> 00:01:30.470
The corresponding vertex is F.

22
00:01:31.870 --> 00:01:36.170
So next, the method traverses to F.

23
00:01:36.170 --> 00:01:41.086
Now the algorithm on node F and
has determined that out of its possible

24
00:01:41.086 --> 00:01:46.040
destinations, E, G, and J,
J is the least expensive.

25
00:01:46.040 --> 00:01:52.081
The total path, that is the weight
of the path to J, is 10.

26
00:01:52.081 --> 00:01:56.320
5 from I to F, plus 5 from F to J.

27
00:01:56.320 --> 00:02:00.440
This diagram shows that the priority
is now shorter because it has

28
00:02:00.440 --> 00:02:03.510
popped out the already processed load, I.

29
00:02:03.510 --> 00:02:08.260
At step three, we are processing node
J but face the following situation.

30
00:02:09.450 --> 00:02:12.454
We can go back to F from J, but

31
00:02:12.454 --> 00:02:17.227
that will cost 10 plus 15, that is 25.

32
00:02:17.227 --> 00:02:22.470
25 is worse than the cost of the current
path to F, which is 5 directly from I.

33
00:02:23.680 --> 00:02:28.220
Thus we do not go from F to J, and
do not update the distance shown.

34
00:02:28.220 --> 00:02:34.775
The other option is to go from J to G,
which incurs a cost of 10 plus 5, 15.

35
00:02:34.775 --> 00:02:39.559
Hm, this does not improve the current
cost to reach G through F,

36
00:02:39.559 --> 00:02:42.710
which is now at 15 already.

37
00:02:42.710 --> 00:02:46.720
Therefore, we do not
update the distance for G.

38
00:02:46.720 --> 00:02:50.880
So at this point,
we see that while J is processed,

39
00:02:50.880 --> 00:02:54.230
it had no impact on
the traversal process.

40
00:02:54.230 --> 00:02:58.916
We consider the distance row again, and
find that the next node to expand is G,

41
00:02:58.916 --> 00:03:00.630
which is reached through F.

42
00:03:01.630 --> 00:03:04.531
Continuing as before, G's processed.

43
00:03:04.531 --> 00:03:09.191
It opens up the possibility of
diverging to C, at a cost of 35.

44
00:03:09.191 --> 00:03:12.430
Or to D, at the cost of 25.

45
00:03:12.430 --> 00:03:17.254
But wait, we have an issue,
there are two competing nodes,

46
00:03:17.254 --> 00:03:23.760
E coming form F or D coming from G,
that are both expansion candidates.

47
00:03:23.760 --> 00:03:24.705
At this point,

48
00:03:24.705 --> 00:03:29.516
the algorithm can make a random choice
because there is no other information.

49
00:03:29.516 --> 00:03:32.640
Let's say we make an arbitrary choice,
we expand to E next.

50
00:03:33.650 --> 00:03:37.240
In an optional video, we will see

51
00:03:37.240 --> 00:03:42.140
how we can use the additional information
to make a more informed decision.

52
00:03:42.140 --> 00:03:43.930
After expanding to E,

53
00:03:43.930 --> 00:03:47.700
we can find that we have already
reached the node B, so we are done.

54
00:03:49.000 --> 00:03:54.080
The other choice, that is to go to D from
G, costs less than the path through B, but

55
00:03:54.080 --> 00:03:56.690
it doesn't matter now,
because the destination is reached.

56
00:03:57.960 --> 00:04:03.390
Just for the sake of completeness, if we
did let the algorithm continue to operate,

57
00:04:03.390 --> 00:04:06.440
it will terminate when all
reachable nodes are reached.

58
00:04:07.480 --> 00:04:11.310
We say all reachable nodes,
become some nodes like H,

59
00:04:12.850 --> 00:04:15.220
are not reachable because
it has no incoming edge.

60
00:04:16.220 --> 00:04:20.060
Such a node, as we said before,
is called the root node of the graph.

61
00:04:20.060 --> 00:04:23.400
In general,
a graph can have more than one root node.

62
00:04:25.040 --> 00:04:29.824
Now that we have reached our destination,
we need to construct the shortest path.

63
00:04:29.824 --> 00:04:35.630
We start by taking the destination B and
find its predecessor, E.

64
00:04:37.350 --> 00:04:43.069
Then we find the node E, and
check its predecessor, which is F.

65
00:04:43.069 --> 00:04:46.347
Finally, we find the predecessor
of F to obtain I,

66
00:04:46.347 --> 00:04:48.645
which is a source node for the task.

67
00:04:49.880 --> 00:04:52.610
So these nodes can then be
stretched together in reverse,

68
00:04:52.610 --> 00:04:57.570
thus building I to F, to E, to B,
which is highlighted in the film.

69
00:04:58.590 --> 00:05:03.380
So, how well does this algorithm work for
big graphs?

70
00:05:03.380 --> 00:05:06.020
Actually, not very well.

71
00:05:06.020 --> 00:05:08.940
We often measure
the performance of an algorithm

72
00:05:08.940 --> 00:05:10.710
in terms of the size of the data.

1
00:00:00.350 --> 00:00:05.040
So, we saw that the Dijkstra Algorithm
has a very high worst case complexity.

2
00:00:06.370 --> 00:00:09.770
Despite the high complexity of
the algorithm, there are several

3
00:00:09.770 --> 00:00:13.590
practical improvements that will
enhance the performance of the method.

4
00:00:13.590 --> 00:00:17.310
One of them is
Bi-directional Dijkstra Algorithm.

5
00:00:17.310 --> 00:00:21.340
The idea is very simple,
we will go forward from the source now and

6
00:00:21.340 --> 00:00:26.960
backward from the target node and stop
when the two expanding frontiers meet.

7
00:00:26.960 --> 00:00:29.840
We will briefly illustrate
the process without going deep into

8
00:00:29.840 --> 00:00:31.690
the details of every step.

9
00:00:31.690 --> 00:00:34.890
So the technique starts just
like the regular method.

10
00:00:34.890 --> 00:00:38.810
The control moves from
I to F at a cost of 5.

11
00:00:38.810 --> 00:00:40.500
But then, it switches and

12
00:00:40.500 --> 00:00:45.240
starts from the target, and
moves backward along the edges.

13
00:00:45.240 --> 00:00:47.950
So from A, it comes to D or C.

14
00:00:48.960 --> 00:00:53.460
We'll chose D,
because an AD is a least weight part.

15
00:00:53.460 --> 00:00:59.730
Now the forward step is performed again,
and we traverse from F to G.

16
00:00:59.730 --> 00:01:03.060
We are skipping the expansion
to J because as we saw before,

17
00:01:03.060 --> 00:01:05.040
it does not contribute to the path.

18
00:01:05.040 --> 00:01:08.510
So the total rate of the IFG path is 15.

19
00:01:08.510 --> 00:01:13.420
The backward process
then reaches G through D.

20
00:01:13.420 --> 00:01:16.000
The cost of the IFG is 15.

21
00:01:16.000 --> 00:01:18.930
And the cost of ADG is also 15.

22
00:01:18.930 --> 00:01:22.650
We stop because a common node is reached.

23
00:01:22.650 --> 00:01:25.570
We need to ensure that the weight
of the forward path and

24
00:01:25.570 --> 00:01:30.810
that of the reverse path are added, and
the total combined rate is minimal.

25
00:01:30.810 --> 00:01:34.701
At this point,
we can concatenate the partial paths

26
00:01:34.701 --> 00:01:38.777
to construct the full shortest path,
which is IFGDA.

27
00:01:38.777 --> 00:01:43.773
Now one point to remember is that
the length of the smallest weight path

28
00:01:43.773 --> 00:01:46.744
can be longer than the shortest hop path.

29
00:01:46.744 --> 00:01:52.444
Here the FCA path has 2 hops but
a weight of 20.

30
00:01:52.444 --> 00:01:57.366
But the weight of the 3 hop path,
FGDA, is 15.

31
00:01:57.366 --> 00:01:59.060
So just remember that.

1
00:00:00.580 --> 00:00:03.750
The final type of analytics
we'll discuss in this module

2
00:00:03.750 --> 00:00:05.425
is called Centrality Analysis.

3
00:00:06.850 --> 00:00:12.010
We call something central if it's
in the middle of a larger entity.

4
00:00:12.010 --> 00:00:16.370
We also call something central
if it's important and vital.

5
00:00:17.680 --> 00:00:23.770
For graphs, we'll identify important
notes by looking at how central they are.

6
00:00:23.770 --> 00:00:24.270
In the graph.

7
00:00:27.280 --> 00:00:31.673
Look at the the six node graph,
you don't have to know a lot

8
00:00:31.673 --> 00:00:36.345
to figure out that the orange
node is pretty important, Why?

9
00:00:36.345 --> 00:00:39.290
Well, one reason could be it's
the most vulnerable node.

10
00:00:39.290 --> 00:00:41.240
If you remove it the graph
will fall apart.

11
00:00:41.240 --> 00:00:44.210
because this is clearly one way to
look at the centrality of the node.

12
00:00:45.630 --> 00:00:46.650
But it's not the only way.

13
00:00:48.270 --> 00:00:50.440
Another way of looking at the orange node,

14
00:00:50.440 --> 00:00:53.880
is that it can reach all other
nodes quicker, than any other node.

15
00:00:55.390 --> 00:00:58.570
So this is the idea behind influences.

16
00:00:58.570 --> 00:01:02.480
People in a social network who
are connected enough to reach out and

17
00:01:02.480 --> 00:01:06.810
possibly influence a lot more
people than others will be able to.

18
00:01:08.340 --> 00:01:10.820
If the little graph represents
a transportation network.

19
00:01:11.820 --> 00:01:15.890
And you were asked to build a restaurant
somewhere around this network.

20
00:01:15.890 --> 00:01:18.780
You'll possibly choose an area
near the central node.

21
00:01:18.780 --> 00:01:23.250
Because more traffic will flow
through this node than other nodes.

22
00:01:23.250 --> 00:01:25.830
And some of them will get
out at the station and

23
00:01:25.830 --> 00:01:27.010
bring business to your restaurant.

24
00:01:28.250 --> 00:01:30.310
We can give many more examples.

25
00:01:30.310 --> 00:01:34.810
In biological networks,
house-keeping genes are genes needed for

26
00:01:34.810 --> 00:01:38.050
the maintenance of basic
cellular function, and

27
00:01:38.050 --> 00:01:42.060
are expressed in all cells of
an organism under normal and

28
00:01:42.060 --> 00:01:46.780
abnormal conditions, these genes
are central because they're vital and

29
00:01:46.780 --> 00:01:49.300
they're connected to many other
nodes in the biological network.

30
00:01:50.440 --> 00:01:54.330
So much so that they need to
be taken out of the network so

31
00:01:54.330 --> 00:01:56.930
that the rest of
the network can be studied.

32
00:01:56.930 --> 00:02:01.390
Out of all these examples,
we'll focus on a kind of problem

33
00:02:01.390 --> 00:02:04.480
researchers have called
the key player problem.

34
00:02:05.740 --> 00:02:07.380
The problem comes in two flavors.

35
00:02:08.460 --> 00:02:11.177
Now take a little time to
read the two examples.

36
00:02:23.255 --> 00:02:25.970
The first is sort of a negative piece.

37
00:02:27.250 --> 00:02:31.660
We have a network and we are trying
to find a small subset of people

38
00:02:31.660 --> 00:02:36.250
who's removal will maximally
disrupt the network.

39
00:02:37.260 --> 00:02:40.610
The key operative word here is maximally.

40
00:02:40.610 --> 00:02:46.360
So if there is a node which removal
breaks the network into two parts,

41
00:02:46.360 --> 00:02:50.300
it is not what we want if
there's another two nodes

42
00:02:50.300 --> 00:02:53.440
whose removal will break
the network into ten parts.

43
00:02:53.440 --> 00:02:58.270
The second case, however,
is a lot more conventional.

44
00:02:58.270 --> 00:03:03.000
The goal is to find a small set of nodes
with maximal combined reachability

45
00:03:03.000 --> 00:03:04.250
to other nodes.

46
00:03:04.250 --> 00:03:09.000
That means, taken together, these nodes
should reach almost all other nodes.

47
00:03:11.010 --> 00:03:14.180
You should know that
people use two terms to

48
00:03:14.180 --> 00:03:16.940
characterize a general concept
of network centrality.

49
00:03:18.340 --> 00:03:20.920
The first one, centrality,
is about a node.

50
00:03:22.290 --> 00:03:26.450
It measures how central the node
is with respect to the network.

51
00:03:27.450 --> 00:03:31.070
So the orange node in the left
graph has high centrality, and

52
00:03:31.070 --> 00:03:32.550
the blue nodes have low centrality.

53
00:03:34.140 --> 00:03:39.670
The second term, centralization,
is about the network.

54
00:03:39.670 --> 00:03:41.620
Now look at the graph on the right,

55
00:03:41.620 --> 00:03:46.830
where the dark orange node is still very
central for the light orange node, and

56
00:03:46.830 --> 00:03:50.770
between the other nodes, and
therefore has reasonable centrality.

57
00:03:52.050 --> 00:03:55.170
As more nodes start
having higher centrality,

58
00:03:55.170 --> 00:03:58.090
the centralization of the network drops

59
00:03:58.090 --> 00:04:02.330
because there is less variation in
the centrality values of the network.

60
00:04:03.940 --> 00:04:09.332
So for each type of centrality that we
discuss next we can compute the network

61
00:04:09.332 --> 00:04:14.812
centralization by considering the sum
of the difference between the maximum

62
00:04:14.812 --> 00:04:20.393
centrality and the centrality of the node
divided by the maximum centrality.

63
00:04:20.393 --> 00:04:23.310
There are thirty different
measures of centrality.

64
00:04:24.640 --> 00:04:27.190
We will consider only a few of them and

65
00:04:27.190 --> 00:04:30.645
explore the conceptual
principles supported here.

66
00:04:30.645 --> 00:04:34.080
The first and the most intuitive
measure is degree centrality.

67
00:04:34.080 --> 00:04:39.180
Quick measures, the degree of a node
divided by the possible edges it

68
00:04:39.180 --> 00:04:44.100
could have if it connected to each of
the other N- 1 nodes in the graph.

69
00:04:45.110 --> 00:04:45.960
Now, we have seen this before.

70
00:04:47.010 --> 00:04:50.850
This measure gives us a sense
of the hubness of the node.

71
00:04:50.850 --> 00:04:55.630
The higher the number is,
the more hub-like the node is.

72
00:04:55.630 --> 00:04:58.640
Thinking of our second key player problem.

73
00:04:58.640 --> 00:05:02.880
One way to approach this is to
find a hub-like measure for

74
00:05:02.880 --> 00:05:05.660
a group with multiple nodes.

75
00:05:05.660 --> 00:05:08.690
Instead of measuring the degree
centrality of individual nodes,

76
00:05:10.310 --> 00:05:15.520
the measure simply counts the number of
edges coming into the group as a whole

77
00:05:15.520 --> 00:05:17.430
compared to the number of outsiders.

78
00:05:18.620 --> 00:05:25.460
In our example two red nodes here
together connect to all blue nodes.

79
00:05:26.830 --> 00:05:31.230
Although there is just one
node neighboring both of them.

80
00:05:31.230 --> 00:05:34.950
Closeness centrality takes a different
approach to the centrality problem.

81
00:05:36.670 --> 00:05:41.222
It acts the shortest distance
of a node to all other nodes and

82
00:05:41.222 --> 00:05:43.232
divides it by a minus one.

83
00:05:43.232 --> 00:05:48.575
So in our graph, a node like I,
which is on the periphery of the graph,

84
00:05:48.575 --> 00:05:52.199
will be quite far from
all nodes in general, and

85
00:05:52.199 --> 00:05:56.930
therefore will have a very high
closeness centrality value.

86
00:05:56.930 --> 00:06:03.070
On the other hand, nodes like F, C and
H are much closer to all other nodes.

87
00:06:04.340 --> 00:06:07.580
Know that we define this measure
in terms of shortest paths.

88
00:06:09.810 --> 00:06:15.060
So if a moving object, like information,
flows to the shortest path

89
00:06:15.060 --> 00:06:20.810
in the network, F is more likely to
receive them earlier than other nodes.

90
00:06:20.810 --> 00:06:23.710
And therefore can process into
other nodes more quickly.

91
00:06:25.190 --> 00:06:26.590
Therefore.

92
00:06:26.590 --> 00:06:31.300
If we look to inject a new piece of
information into the network with the idea

93
00:06:31.300 --> 00:06:36.486
that it should read every other node
quickly, I should possibly inject it at F.

94
00:06:36.486 --> 00:06:43.940
Recall however, that node information
flow is two shortest routes.

95
00:06:43.940 --> 00:06:49.370
An example is gossip, that tends to
travel through centrality nodes.

96
00:06:49.370 --> 00:06:53.430
Closeness centrality does not work well
for these types of information nodes.

97
00:06:54.490 --> 00:06:58.410
Another very popular centrality measure
is called betweenness centrality.

98
00:06:59.640 --> 00:07:06.430
For any node it measures the fraction of
shortest paths flowing through that node.

99
00:07:06.430 --> 00:07:08.730
Compared to the number of
shortest paths in the graph.

100
00:07:10.590 --> 00:07:16.065
Since B is at one end,
its between a centrality is 0.

101
00:07:16.065 --> 00:07:17.935
But let's look at A.

102
00:07:17.935 --> 00:07:20.010
A is in the path from B to E.

103
00:07:21.320 --> 00:07:21.880
So is D.

104
00:07:23.060 --> 00:07:27.440
Therefore, A's score for
the B to E path is 0.5.

105
00:07:27.440 --> 00:07:35.280
Similarly, its score for the C to E path
is also 0.5, making the total score 1.

106
00:07:35.280 --> 00:07:40.623
E is in the path from A to D,
for there is one more

107
00:07:40.623 --> 00:07:46.651
path from A to do through C,
so E's score is 0.5.

108
00:07:46.651 --> 00:07:49.840
Now can you verify why C's score is 3.5?

109
00:07:49.840 --> 00:07:53.275
Betweenness centrality
is typically used for

110
00:07:53.275 --> 00:07:57.440
problem where a commodity is
flowing through the network.

111
00:07:57.440 --> 00:07:59.840
As in the case of closeness centrality.

112
00:08:01.080 --> 00:08:05.573
Any quantity that does not flow in
shortest path channels like infection or

113
00:08:05.573 --> 00:08:09.872
rumor on the internet doesn't work
well with betweeness centrality.

1
00:00:01.070 --> 00:00:04.570
This lesson on graph analytics,
is about identifying and

2
00:00:04.570 --> 00:00:07.290
tracking groups of interacting
entities in a network.

3
00:00:08.900 --> 00:00:10.740
We call these groups communities.

4
00:00:12.550 --> 00:00:16.060
Let's try to provide a more concrete
definition of communities in a network.

5
00:00:17.410 --> 00:00:19.800
We multiply the definition
by this diagram,

6
00:00:19.800 --> 00:00:22.820
showing a research study
on the Santa Fe Institute.

7
00:00:22.820 --> 00:00:27.680
A theoretical research institute
located in Santa Fe, New Mexico, US.

8
00:00:27.680 --> 00:00:31.850
They performed multidisciplinary
studies on fundamental principles

9
00:00:31.850 --> 00:00:34.220
of complex adaptive systems.

10
00:00:34.220 --> 00:00:37.110
The nodes in the graph are researchers.

11
00:00:37.110 --> 00:00:39.640
And an edge exists
between two researchers,

12
00:00:39.640 --> 00:00:40.830
if they collaborate with each other.

13
00:00:42.070 --> 00:00:47.020
As you can see, there are distinct
groups among researchers.

14
00:00:47.020 --> 00:00:49.300
A mathematically college researcher,

15
00:00:49.300 --> 00:00:53.090
does not collaborate with researchers
who work on structure of the RNA.

16
00:00:54.370 --> 00:00:58.450
So the graph,
is essentially a set of separate groups,

17
00:00:58.450 --> 00:01:02.570
thinly connected through a handful
of cross-disciplinary researchers,

18
00:01:02.570 --> 00:01:04.000
who collaborate across groups.

19
00:01:05.290 --> 00:01:08.320
These groups are then the communities
in this collaboration graph.

20
00:01:09.590 --> 00:01:10.280
What does this tell us?

21
00:01:11.650 --> 00:01:16.260
It tells us that communities are highly
interacting clusters in a graph.

22
00:01:16.260 --> 00:01:20.690
That is, they form pockets of
denser subgraphs that are more

23
00:01:20.690 --> 00:01:24.400
connected to each other,
than to members of the other clusters.

24
00:01:25.540 --> 00:01:30.020
Communities of humans, or otherwise,
are interesting things to study,

25
00:01:30.020 --> 00:01:33.740
because it gives us an insight
into the interaction patterns.

26
00:01:33.740 --> 00:01:35.280
And how they change with time.

27
00:01:36.340 --> 00:01:39.280
Here, are some analytics
questions about communities.

28
00:01:40.310 --> 00:01:42.740
We have divided them
into three categories.

29
00:01:44.080 --> 00:01:46.990
Analytics questions that do not
depend on time, are called static.

30
00:01:48.030 --> 00:01:52.290
Here, we ask questions about
the composition of the community,

31
00:01:52.290 --> 00:01:54.970
how tight-knit members are connected,
and so forth.

32
00:01:56.030 --> 00:02:02.470
In the second category, we involve the
formation, and evolution of the community.

33
00:02:03.740 --> 00:02:06.720
Communities can form temporal,
for example,

34
00:02:06.720 --> 00:02:09.720
around an event like a school shooting.

35
00:02:09.720 --> 00:02:12.620
Or some communities,
despite the comings and

36
00:02:12.620 --> 00:02:16.750
goings of members,
sustain themselves well.

37
00:02:16.750 --> 00:02:22.040
A Facebook group, a political party,
fans of a music band,

38
00:02:22.040 --> 00:02:25.515
are likely to continue over time,
and hence are non-transient.

39
00:02:27.070 --> 00:02:30.350
One can also be interested in
the history of formation of a community,

40
00:02:30.350 --> 00:02:31.100
like a criminal network.

41
00:02:32.230 --> 00:02:34.410
The third category is about predictions.

42
00:02:35.780 --> 00:02:39.740
Analysts would like to predict
how a community would grow.

43
00:02:39.740 --> 00:02:41.828
Whether it's composition
of members might change.

44
00:02:41.828 --> 00:02:46.300
Or whether there are emerging
power shifts within the community.

45
00:02:47.920 --> 00:02:51.490
Now before we ask these questions,
however,

46
00:02:51.490 --> 00:02:54.620
we need to first identify
communities in a large network.

47
00:02:55.900 --> 00:03:00.790
To find communities, we need to
formalize the idea that there are more

48
00:03:00.790 --> 00:03:05.520
connections within the community, and
fewer connections between two communities.

49
00:03:06.770 --> 00:03:11.180
One way to achieve this, is to think
of dividing the degree of a node,

50
00:03:11.180 --> 00:03:15.430
into an internal and
an external component.

51
00:03:15.430 --> 00:03:20.310
The internal component is the count
of edges within community.

52
00:03:20.310 --> 00:03:23.680
And the external degree, is the count
of edges outside the community.

53
00:03:25.480 --> 00:03:26.110
An example,

54
00:03:26.110 --> 00:03:30.450
will be to consider that my community
is where I live, that is San Diego.

55
00:03:30.450 --> 00:03:35.280
And then count the number of my friends
within San Diego versus outside San Diego.

56
00:03:36.660 --> 00:03:41.560
In the figure, the highlighted node
has four internal connections and

57
00:03:41.560 --> 00:03:42.640
one external connection.

58
00:03:44.730 --> 00:03:48.310
The next step, would be to think
of the internal degree, and

59
00:03:48.310 --> 00:03:50.660
the external degree of an entire cluster.

60
00:03:51.890 --> 00:03:56.080
We can sum up the internal degrees
of all nodes in a cluster.

61
00:03:56.080 --> 00:03:59.480
And call it the internal
degree of the whole cluster.

62
00:03:59.480 --> 00:04:02.089
And similarly,
sum the external degrees of the nodes,

63
00:04:02.089 --> 00:04:04.320
to compute the external
degree of the cluster.

64
00:04:05.460 --> 00:04:08.360
Now, we can define intra-cluster

65
00:04:08.360 --> 00:04:12.810
density to be the ratio of the number
of internal edges of the cluster,

66
00:04:12.810 --> 00:04:16.250
divided by the number of possible
connections inside the box.

67
00:04:17.670 --> 00:04:20.350
The denominator is N cues 2.

68
00:04:20.350 --> 00:04:24.190
Which is the number of pairwise
combination of nodes within the cluster.

69
00:04:24.190 --> 00:04:25.490
We call this delta int.

70
00:04:26.780 --> 00:04:30.750
Similarly, inter-cluster density,
delta x is the number

71
00:04:30.750 --> 00:04:35.040
of inter-cluster edges divided by
the possible pairings between nC,

72
00:04:36.120 --> 00:04:40.070
a node in the cluster, to n- nC,
the number of nodes outside the cluster.

73
00:04:41.970 --> 00:04:47.500
There are two kinds of methods used for
finding communities in the network.

74
00:04:47.500 --> 00:04:52.320
One of them focuses on local properties,
that is,

75
00:04:52.320 --> 00:04:56.560
properties for which one only
looks at a node and its neighbor.

76
00:04:57.720 --> 00:05:01.110
For the most ideal community
in a network is a subgraph,

77
00:05:01.110 --> 00:05:04.099
where every node is connected to
every other node in the subgraph.

78
00:05:05.230 --> 00:05:06.840
Such a structure is called a clique.

79
00:05:08.570 --> 00:05:11.740
To find a perfect community
structure as a clique,

80
00:05:11.740 --> 00:05:16.560
one can try to find the largest
clique within a graph, return cell.

81
00:05:17.580 --> 00:05:19.670
That is a computationally
challenging problem.

82
00:05:21.260 --> 00:05:25.780
It's much simpler to find cliques
if we know the value of k.

83
00:05:26.780 --> 00:05:30.312
That means the number of
members in the clique.

84
00:05:30.312 --> 00:05:33.000
We're going to show a simple
version of this in model three.

85
00:05:34.710 --> 00:05:38.230
The more general problem has been
solved by complex algorithms,

86
00:05:38.230 --> 00:05:39.990
that are beyond the scope of this course.

87
00:05:41.950 --> 00:05:47.620
In the real world, perfect cliques larger
than three or four are harder to find.

88
00:05:47.620 --> 00:05:49.450
So we need to relax the definition of it.

89
00:05:50.750 --> 00:05:53.680
Now, there are two types of relaxations,

90
00:05:53.680 --> 00:05:57.250
those based on distance,
and those based on density.

91
00:05:58.950 --> 00:06:04.970
Two distance based definitions
are n-clique and n-plan.

92
00:06:04.970 --> 00:06:08.660
We'll illustrate this over
a friendship graph shown here.

93
00:06:08.660 --> 00:06:12.050
n-clique, is a subgraph,

94
00:06:12.050 --> 00:06:16.990
such that the distance between each
node pair in that subgraph is n or less.

95
00:06:16.990 --> 00:06:22.790
By this definition, Holly, Paul,
and Gary form a two clique.

96
00:06:24.100 --> 00:06:27.090
That's a little awkward, isn't it?

97
00:06:27.090 --> 00:06:28.120
Yeah.

98
00:06:28.120 --> 00:06:30.550
They are within two
distance of each other.

99
00:06:30.550 --> 00:06:33.600
But the two clique does not
include intermediate nodes that

100
00:06:33.600 --> 00:06:34.480
connect the member nodes.

101
00:06:35.760 --> 00:06:39.690
So, Mike, is not in the two clique.

102
00:06:40.890 --> 00:06:44.570
This situation is corrected in n-clan.

103
00:06:44.570 --> 00:06:49.280
Where, to belong to the n-clan,
the shortest part between any members,

104
00:06:49.280 --> 00:06:51.438
without involving outsiders, is n or less.

105
00:06:51.438 --> 00:06:56.250
Now, Holly, Mike,

106
00:06:56.250 --> 00:07:01.030
Bill, Don, Harry, and
Gary form a two-clan.

107
00:07:01.030 --> 00:07:05.660
Clearly, this group is more cohesive
than the two-clique we saw before.

108
00:07:07.760 --> 00:07:11.490
n-clique and n-clan
are distance based measures for

109
00:07:11.490 --> 00:07:13.455
finding cohesive groups of communities.

110
00:07:13.455 --> 00:07:19.270
K-core is a density based method for
finding communities.

111
00:07:19.270 --> 00:07:22.117
Let's look at the dark, orange subgraph.

112
00:07:22.117 --> 00:07:27.410
Every node is connected to at least
three other nodes within the subgraph.

113
00:07:27.410 --> 00:07:28.867
They form a 3-core.

114
00:07:28.867 --> 00:07:33.110
Let's include the medium light
orange nodes in the subgraph now.

115
00:07:34.720 --> 00:07:39.380
Each node is connected to at least two
other members within the subgraph,

116
00:07:39.380 --> 00:07:40.495
they form a 2-core.

117
00:07:42.400 --> 00:07:49.894
Relaxing further, we can add the light
orange nodes and the graph as a 1-core.

1
00:00:00.910 --> 00:00:03.480
We have already defined
the degree of the node

2
00:00:03.480 --> 00:00:05.965
as the number of edges connected to it.

3
00:00:05.965 --> 00:00:08.815
Thus specifying if a node is
more connected than another.

4
00:00:10.210 --> 00:00:13.760
Looking closer we can separate out

5
00:00:13.760 --> 00:00:18.450
that two components denotes
degrees into in degree and out.

6
00:00:18.450 --> 00:00:23.150
Which are the counts of the incident and
the outgoing edges of a node respectively.

7
00:00:24.570 --> 00:00:27.970
In the example graph G has an indegree and

8
00:00:27.970 --> 00:00:33.860
outdegree of three making
the total degree equal to six.

9
00:00:33.860 --> 00:00:38.680
We first construct this degree table for
each node.

10
00:00:38.680 --> 00:00:42.460
It's a simple procedure where we count
the number of nodes with degree.

11
00:00:42.460 --> 00:00:43.960
0, 1, 2, and so forth.

12
00:00:43.960 --> 00:00:51.100
The degree versus count table is
a degree histogram of the graph.

13
00:00:51.100 --> 00:00:56.380
We can compare two graphs by computing
the vector distance between them.

14
00:00:56.380 --> 00:00:59.625
One simplistic measure is
just the Euclidean distance.

15
00:00:59.625 --> 00:01:04.331
For our case, the degree
histogram based on comparisons of

16
00:01:04.331 --> 00:01:08.390
the histograms find the graphs
to be very similar.

17
00:01:08.390 --> 00:01:12.036
The more sophisticated
methods are available but

18
00:01:12.036 --> 00:01:14.822
are outside the scope of this course.

19
00:01:26.919 --> 00:01:30.280
We can also compute histograms
of just the in degree or

20
00:01:30.280 --> 00:01:32.380
just the out degree of the graph.

21
00:01:33.495 --> 00:01:39.210
But perhaps more interesting is the joint
two dimensional histogram of the graph,

22
00:01:39.210 --> 00:01:43.970
the colorful histogram of the graph
can be interpreted here as follows.

23
00:01:43.970 --> 00:01:49.330
The graph has a maximum in degree of
three and a maximum out degree of three.

24
00:01:49.330 --> 00:01:52.644
This creates a two-dimensional
histogram with four times four

25
00:01:52.644 --> 00:01:54.740
equal to 16 different joined values.

26
00:01:56.020 --> 00:01:59.700
The actual value for any combination
is computed from the graph and

27
00:01:59.700 --> 00:02:00.740
color coded in the ratings.

28
00:02:01.900 --> 00:02:05.890
For example, there is no node with
in degree 0 and out degree 0.

29
00:02:05.890 --> 00:02:09.930
So the lower-left square of the graph
has value zero and color-coded blue.

30
00:02:11.540 --> 00:02:15.580
On the other hand, there are two nodes
with in-degree three and out-degree three.

31
00:02:15.580 --> 00:02:20.510
Thus, the top-right corner has the value
twp, which is 20% of the nodes.

32
00:02:21.810 --> 00:02:25.310
Color coded as light green.

33
00:02:25.310 --> 00:02:27.630
The 2D histogram provides
an interesting insight.

34
00:02:29.160 --> 00:02:32.940
The nodes with more incident
edges than outgoing edges

35
00:02:34.080 --> 00:02:37.740
represent entities that take
in more than they put out.

36
00:02:38.880 --> 00:02:44.090
In a social networking setting,
they represent members who are listeners.

37
00:02:45.450 --> 00:02:48.910
They receive a lot of posts,
but send much fewer posts.

38
00:02:50.360 --> 00:02:53.510
On the opposite side of the spectrum,

39
00:02:53.510 --> 00:02:59.242
there are talkers whose out-degrees
exceed their in-degrees.

40
00:03:00.710 --> 00:03:06.173
The entities that are in between,
we have both large values of in-degree and

41
00:03:06.173 --> 00:03:07.340
out-degree.

42
00:03:07.340 --> 00:03:09.460
These are communicators.

43
00:03:09.460 --> 00:03:13.180
In this graph, there seems to
be more talkers than listeners.

44
00:03:13.180 --> 00:03:14.850
Not surprising though.

45
00:03:14.850 --> 00:03:18.236
My social media friends
show similar statistics.
