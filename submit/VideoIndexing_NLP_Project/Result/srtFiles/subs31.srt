
1
00:00:01.850 --> 00:00:04.330
Let's now discuss what
feature transformation is.

2
00:00:05.640 --> 00:00:09.340
After this video you will be able
to articulate the purpose of

3
00:00:09.340 --> 00:00:14.170
feature transformation, list three
feature transformation operations, and

4
00:00:14.170 --> 00:00:16.180
discuss when scaling is important.

5
00:00:17.670 --> 00:00:19.390
In addition to feature selection,

6
00:00:19.390 --> 00:00:23.690
data pre-processing can also
include feature transformation.

7
00:00:23.690 --> 00:00:26.730
Feature transformation involves
mapping a set of values for

8
00:00:26.730 --> 00:00:30.860
the feature to a new set of values to
make the representation of the data more

9
00:00:30.860 --> 00:00:34.800
suitable or easier to process for
the downstream analysis.

10
00:00:36.760 --> 00:00:40.010
A common feature transformation
operation is scaling.

11
00:00:40.010 --> 00:00:42.200
This involves changing
the range of values for

12
00:00:42.200 --> 00:00:46.570
a feature of features to
another specified range.

13
00:00:46.570 --> 00:00:49.840
This is done to avoid allowing
features with large values to

14
00:00:49.840 --> 00:00:51.280
dominate the analysis results.

15
00:00:52.300 --> 00:00:56.030
For example, if your dataset has
both width and height as features,

16
00:00:56.030 --> 00:00:59.910
the magnitude of the weight values,
which are in pounds, will be much

17
00:00:59.910 --> 00:01:03.910
larger than the magnitude of the height
values which are in feet and inches.

18
00:01:03.910 --> 00:01:06.950
So scaling both features
to a common value range

19
00:01:06.950 --> 00:01:09.910
will make the contributions from
both weight and height equal.

20
00:01:11.350 --> 00:01:16.230
One way to perform scaling is to map all
values of a feature to a specific range

21
00:01:16.230 --> 00:01:18.690
such as between zero and one.

22
00:01:18.690 --> 00:01:21.342
For example,
let's say you have a feature for

23
00:01:21.342 --> 00:01:24.750
income that ranges from 30,000 to 100,000.

24
00:01:24.750 --> 00:01:29.420
And you have another feature for years
of employment that ranges from 0 to 50.

25
00:01:29.420 --> 00:01:32.500
These features have very different scales.

26
00:01:32.500 --> 00:01:35.840
If you want both features to have equal
weighting when you compare the data

27
00:01:35.840 --> 00:01:41.170
samples, then you can scale the range
of both features to be between 0 and 1.

28
00:01:41.170 --> 00:01:44.870
That way the income feature which is
on much largest scale than the years

29
00:01:44.870 --> 00:01:48.940
of employment feature will not
dominate the compares result.

30
00:01:48.940 --> 00:01:53.350
Alternatively, scaling can be perform
by transforming the features such that

31
00:01:53.350 --> 00:01:57.420
the results have zero mean,
and unit standard deviation.

32
00:01:57.420 --> 00:02:00.730
The steps to perform the scaling
is to first calculate the mean and

33
00:02:00.730 --> 00:02:03.930
standard deviation values for
the feature to be scaled.

34
00:02:03.930 --> 00:02:06.560
Then for each value, for this feature,

35
00:02:06.560 --> 00:02:11.510
subtract the mean value from that value,
and divide by the standard deviation.

36
00:02:11.510 --> 00:02:15.290
The transformed feature will end
up with a mean value of zero, and

37
00:02:15.290 --> 00:02:16.680
standard deviation of one.

38
00:02:17.720 --> 00:02:20.420
This effectively removes
the units of the features and

39
00:02:20.420 --> 00:02:25.370
converts each future value to number
of standard deviations from the mean.

40
00:02:26.540 --> 00:02:30.720
This scaling method is used when
the min and max values are known.

41
00:02:30.720 --> 00:02:34.760
This is also useful when there are
outliers which will skew the calculation

42
00:02:34.760 --> 00:02:38.380
for the range as the max value is
determined by the furthest outlier.

43
00:02:39.400 --> 00:02:43.980
This scaling operation is often
referred to as zero-normalization or

44
00:02:43.980 --> 00:02:45.030
as standardization.

45
00:02:46.390 --> 00:02:49.110
Filtering is another feature
transformation operation.

46
00:02:50.250 --> 00:02:55.450
This is commonly applied to time series
data, such as speech or audio signals.

47
00:02:55.450 --> 00:02:58.520
A low pass filter can be
used to filter out noise.

48
00:02:58.520 --> 00:03:02.910
Which usually manifests as the high
frequency component in the signal.

49
00:03:02.910 --> 00:03:07.090
A low pass filter removes components
above a certain frequency

50
00:03:07.090 --> 00:03:09.410
allowing the rest to
pass through unaltered.

51
00:03:10.750 --> 00:03:14.310
Filtering can also be used
to remove noise in images.

52
00:03:14.310 --> 00:03:19.530
Noise in an image is random variation
in intensity or color in the pixels.

53
00:03:19.530 --> 00:03:23.590
For example, a noise can cause
an image to appear grainy.

54
00:03:23.590 --> 00:03:28.180
A mean or median filter can be used to
replace the pixel value with the mean or

55
00:03:28.180 --> 00:03:30.240
median value of its neighboring pixels.

56
00:03:31.270 --> 00:03:33.730
This has the effect of
smoothing out the image,

57
00:03:33.730 --> 00:03:36.390
removing the noise that causes
the graininess in the image.

58
00:03:37.830 --> 00:03:41.911
Aggregation combines values for
a feature in order to summarize data or

59
00:03:41.911 --> 00:03:43.210
reduce variability.

60
00:03:44.340 --> 00:03:48.850
Aggregation combines values for a feature
in order to summarize the data or

61
00:03:48.850 --> 00:03:50.970
to reduce variability.

62
00:03:50.970 --> 00:03:55.660
Aggregation is done by summing or
averaging data values at a higher level.

63
00:03:55.660 --> 00:03:59.720
For example, hourly values can be
aggregated to the daily level.

64
00:03:59.720 --> 00:04:04.255
Or values within a city region can
be aggregated to a state level.

65
00:04:04.255 --> 00:04:07.010
These plots show
the results of aggregation.

66
00:04:07.010 --> 00:04:11.060
The left plot shows the wind
speed values in miles per hour

67
00:04:11.060 --> 00:04:14.730
averaged every 10 minutes
over a period of seven days.

68
00:04:14.730 --> 00:04:19.110
Notice that there's a lot of variability,
that is the values fluctuate a lot.

69
00:04:20.130 --> 00:04:23.730
The right plot shows wind speed
values averaged every hour so

70
00:04:23.730 --> 00:04:26.920
every 60 minutes instead
of every 10 minutes.

71
00:04:26.920 --> 00:04:31.340
Notice that the line is much smoother
here because the values are aggregated at

72
00:04:31.340 --> 00:04:32.750
a higher time scale level.

73
00:04:33.750 --> 00:04:36.900
So aggregation can have
the effect of removing noise

74
00:04:36.900 --> 00:04:40.160
to provide a clear representation
of the structure of your data.

75
00:04:41.240 --> 00:04:44.260
Another example is tracking stock prices.

76
00:04:44.260 --> 00:04:47.620
Hourly deviations of a stock
may be difficult to track but,

77
00:04:47.620 --> 00:04:52.900
aggregated daily changes may better reveal
any upward or downward trend of the stock.

78
00:04:54.280 --> 00:04:58.330
In summary, feature transformation
involves mapping the set of values for

79
00:04:58.330 --> 00:05:02.600
a feature to a new set of values to
make the representation of the data

80
00:05:02.600 --> 00:05:06.180
more suitable for the downstream analysis.

81
00:05:06.180 --> 00:05:10.090
Feature transformation should be used with
cautions since they change the nature of

82
00:05:10.090 --> 00:05:16.030
the data and unintentionally remove some
important characteristics of the data.

83
00:05:16.030 --> 00:05:19.757
So it's important to look at the effects
of the transformation you're applying

84
00:05:19.757 --> 00:05:22.897
to your data to make certain that
it has the intended consequences.

1
00:00:00.000 --> 00:00:04.871
In this activity we will be using
KNIME to handle missing data.

2
00:00:04.871 --> 00:00:09.045
First, we will create new workflow and
import our weather data.

3
00:00:09.045 --> 00:00:15.571
Next, we will remove missing values for
specific measurement in the data.

4
00:00:15.571 --> 00:00:18.813
We will then impute missing
values with the mean.

5
00:00:18.813 --> 00:00:21.554
And finally,
remove all the rows with missing values.

6
00:00:24.655 --> 00:00:28.145
Let's begin, first,
let's create a new workflow in KNIME.

7
00:00:34.723 --> 00:00:36.558
We'll name it, Handling Missing Values.

8
00:00:42.252 --> 00:00:47.453
Next, let's import the weather
data using the File Reader node.

9
00:00:47.453 --> 00:00:49.421
We'll add the File Reader
node to the canvas.

10
00:00:58.553 --> 00:01:02.060
We'll configure File Reader
to read daily_weather.csv.

11
00:01:08.409 --> 00:01:13.280
Next, we'll connect the Histogram
node to File Reader.

12
00:01:20.553 --> 00:01:24.909
Before we configure the Histogram node,
we need to run the File Reader node.

13
00:01:24.909 --> 00:01:28.331
We can do this by clicking on File Reader,
and

14
00:01:28.331 --> 00:01:31.677
clicking on the green
arrow in the toolbar.

15
00:01:35.224 --> 00:01:38.259
Next, double-click on
Histogram to configure it.

16
00:01:38.259 --> 00:01:43.561
We'll set both the binning column and
aggregation columns to air_temp_9am.

17
00:01:52.003 --> 00:01:54.259
Next, we'll add a missing value node.

18
00:02:02.931 --> 00:02:06.257
We'll connect this to
the file reader node.

19
00:02:06.257 --> 00:02:09.992
Double click on missing
value to configuring it.

20
00:02:09.992 --> 00:02:13.945
Go to column settings,

21
00:02:13.945 --> 00:02:21.466
select air_temp_9am, and click on Add.

22
00:02:21.466 --> 00:02:23.584
We'll then choose remove row.

23
00:02:26.982 --> 00:02:32.298
This will remove the measurements of
air_temp_9am that had missing values.

24
00:02:32.298 --> 00:02:32.948
Click OK.

25
00:02:35.838 --> 00:02:41.106
Next, we'll add another histogram
node to the output of missing value.

26
00:02:41.106 --> 00:02:45.162
We can do this easily by copying and
pasting the existing histogram node.

27
00:02:54.404 --> 00:02:57.144
Now let's run the workflow by
clicking on the double green arrows.

28
00:03:00.966 --> 00:03:04.491
Let's view the output in
the histogram nodes before and

29
00:03:04.491 --> 00:03:06.592
after we remove missing values.

30
00:03:06.592 --> 00:03:10.158
In this histogram view,
we can see that there are missing values.

31
00:03:10.158 --> 00:03:12.985
If we go and
look at the second histogram node,

32
00:03:12.985 --> 00:03:15.752
the one after we removed
the missing values.

33
00:03:18.766 --> 00:03:20.958
We can see that there are no
missing values in this chart.

34
00:03:23.678 --> 00:03:28.748
Next, instead of removing missing values,
let's impute the values to the mean.

35
00:03:28.748 --> 00:03:32.042
We could do this by configuring
the missing value node.

36
00:03:34.865 --> 00:03:39.454
And instead of remove row,
change that to mean.

37
00:03:39.454 --> 00:03:47.050
Click OK, and rerun the work flow.

38
00:03:47.050 --> 00:03:50.906
We could see the difference by comparing
the graphs and the two histogram nodes.

39
00:03:56.182 --> 00:03:57.700
If we go to visualization settings.

40
00:04:00.403 --> 00:04:04.412
And change labels to all elements, we'll
see the number of elements in each band.

41
00:04:11.379 --> 00:04:14.929
In the histogram,
before we removed the missing values,

42
00:04:14.929 --> 00:04:18.486
we can see the fifth column
has 216 measurements in it.

43
00:04:18.486 --> 00:04:25.952
Where as the fifth column after we imputed
the missing values has 221 values in it.

44
00:04:25.952 --> 00:04:26.680
Let's close these.

45
00:04:31.608 --> 00:04:36.756
Next, let's remove all the rows that
have missing values in the data.

46
00:04:36.756 --> 00:04:39.073
We can do this by double
clicking on missing value.

47
00:04:42.578 --> 00:04:44.770
Removing air_temp_ 9am.

48
00:04:46.918 --> 00:04:48.116
Clicking on the default tab.

49
00:04:50.728 --> 00:04:53.892
Changing this to remove row.

50
00:04:53.892 --> 00:04:59.224
I'll click OK, now we rerun the workflow.

51
00:04:59.224 --> 00:05:03.470
Again, we'll look at the histograms before
and after we remove the missing values.

52
00:05:15.128 --> 00:05:20.170
Again, we'll go to visualization settings
and change the labels to all elements so

53
00:05:20.170 --> 00:05:22.878
we can see the number of
elements in each bin.

54
00:05:27.213 --> 00:05:30.490
We can see the number of elements in
each bin in the different histograms

55
00:05:30.490 --> 00:05:31.149
has changed.

1
00:00:00.910 --> 00:00:05.280
In this activity we will see how
to handle missing values in Spark.

2
00:00:05.280 --> 00:00:08.320
First, we will load weather
data into a Spark DataFrame.

3
00:00:09.560 --> 00:00:14.092
We'll then examine the summary
statistics for air temperature, remove

4
00:00:14.092 --> 00:00:19.072
the rows with missing values, and finally
impute missing values with the mean.

5
00:00:22.037 --> 00:00:23.002
Let's begin.

6
00:00:23.002 --> 00:00:27.900
First, we'll open the notebook
called handling missing values.

7
00:00:28.920 --> 00:00:31.910
The first cell creates an SQLContext and

8
00:00:31.910 --> 00:00:34.720
then loads the weather data
csv into a data frame.

9
00:00:36.170 --> 00:00:41.060
Let's execute this,
we can view the summary statistics for

10
00:00:41.060 --> 00:00:47.570
the DataFrame by running
df.describe().show().

11
00:00:47.570 --> 00:00:50.238
Let's just look at the summary
statistics for the air temperature.

12
00:00:50.238 --> 00:00:51.913
We'll run

13
00:00:51.913 --> 00:01:00.852
df.describe(['air_score_9am']) .show.

14
00:01:03.730 --> 00:01:06.590
This says that there are 1090 rows.

15
00:01:07.790 --> 00:01:11.490
This does not include the rows of
missing values for the air temperature.

16
00:01:12.630 --> 00:01:17.381
We can count the total number of rows
in the DataFrame by running df.count.()

17
00:01:19.027 --> 00:01:23.026
Since there are 1095 total
rows in the DataFrame, but

18
00:01:23.026 --> 00:01:25.801
only 1090 in the air_temp column,

19
00:01:25.801 --> 00:01:30.630
that means there are five rows in
air_temp that have missing values.

20
00:01:32.240 --> 00:01:36.029
Next, let's remove all the rows in
the DataFrame that have missing values.

21
00:01:37.060 --> 00:01:40.220
We'll put these in a new data
frame called removeAllDF.

22
00:01:41.700 --> 00:01:46.476
To drop the missing values
we'll run df.na.drop.

23
00:01:48.971 --> 00:01:50.984
Let's look at the summary statistics for

24
00:01:50.984 --> 00:01:54.160
air_temp 9AM with
the missing values dropped.

25
00:01:54.160 --> 00:02:00.384
We'll run removeAllDF .describe([
air_temp_9am]) show.()

26
00:02:03.051 --> 00:02:06.146
We can see that the mean and
standard deviation values

27
00:02:06.146 --> 00:02:10.840
are close to the original values before
we removed the rows with missing values.

28
00:02:12.650 --> 00:02:16.784
Additionally, the count of
the number of rows is 1064.

29
00:02:18.380 --> 00:02:22.768
We can verify that this is the total
number of rows in the new DataFrame by

30
00:02:22.768 --> 00:02:24.758
running removeAllDF.count.

31
00:02:29.389 --> 00:02:32.075
Next, let's impute the missing
values with the mean,

32
00:02:32.075 --> 00:02:34.314
instead of removing them
from the DataFrame.

33
00:02:35.920 --> 00:02:39.712
First, we'll need to load
the average function from pyspark.

34
00:02:41.060 --> 00:02:50.030
We'll do this by running from
pyspark.sql.functions import avg.

35
00:02:50.030 --> 00:02:54.689
Next, we'll create a copy of the DataFrame
in which we will input the missing values.

36
00:02:55.910 --> 00:02:58.590
We'll call the new DataFrame imputeDF.

37
00:03:02.000 --> 00:03:06.349
To impute the missing values we'll
iterate through each column of

38
00:03:06.349 --> 00:03:11.317
the original DataFrame, first computing
the mean value for that column and

39
00:03:11.317 --> 00:03:15.913
then replacing the missing values
in that column with the mean value.

40
00:03:15.913 --> 00:03:20.145
To move through the columns
in the data frame,

41
00:03:20.145 --> 00:03:24.704
we'll enter for
x in imputeDF.columns: next,

42
00:03:24.704 --> 00:03:29.061
we'll compute the mean value for
that column.

43
00:03:32.105 --> 00:03:36.296
We'll use the data frame in which
we removed all the missing values,

44
00:03:36.296 --> 00:03:39.630
we'll call the agg function
to compute an aggregate.

45
00:03:39.630 --> 00:03:42.900
And the argument that we give it is avg.

46
00:03:42.900 --> 00:03:48.830
The argument avg is x, which is the column
we are trying to compute the average of.

47
00:03:50.730 --> 00:03:54.010
The agg function returns to DataFrame and

48
00:03:54.010 --> 00:03:57.220
we want to get the first
row of that data frame.

49
00:03:57.220 --> 00:04:02.280
We can do this by calling .first and
then you get the first value in

50
00:04:02.280 --> 00:04:07.420
this row or say [0].

51
00:04:07.420 --> 00:04:10.410
Next let's print the column
name in mean value.

52
00:04:11.890 --> 00:04:16.561
print(x, meanValue)

53
00:04:19.522 --> 00:04:23.202
Now let's update our new DataFrame,

54
00:04:23.202 --> 00:04:28.156
replacing the missing
values with the mean value.

55
00:04:28.156 --> 00:04:37.242
imputeDF = imputeDF.na.fill(meanValue,
[x]).

56
00:04:38.460 --> 00:04:39.135
Let's run this.

57
00:04:44.273 --> 00:04:46.344
We can see that the mean value for

58
00:04:46.344 --> 00:04:51.260
air_temp 9am matches the mean value
computed in the summary statistics of

59
00:04:51.260 --> 00:04:54.820
the data frame where the missing
values were removed.

60
00:04:56.710 --> 00:05:00.390
Finally, let's print the imputed
data summary statistics.

61
00:05:00.390 --> 00:05:04.450
First we'll show the summary
statistics for the original DataFrame.

62
00:05:04.450 --> 00:05:07.318
And then the summary statistics for
the imputed DataFrame.

63
00:05:07.318 --> 00:05:14.813
We'll enter df.describe([
air_temp_ 9am]) .show () and

64
00:05:14.813 --> 00:05:22.062
imputeDF.describe(['air_temp_9am']).sho-
w().

65
00:05:22.062 --> 00:05:24.997
Run this.

66
00:05:24.997 --> 00:05:29.869
We can see that the number of rows
in the imputed DataFrame is larger

67
00:05:29.869 --> 00:05:33.612
than the number of rows in
the original DataFrame.

68
00:05:34.875 --> 00:05:38.500
There were five rows in the original
DataFrame with missing values.

69
00:05:38.500 --> 00:05:40.210
And these have now been
replaced with the mean.

1
00:00:00.760 --> 00:00:04.340
Let's talk about what it means to
build a classification model and

2
00:00:04.340 --> 00:00:06.990
how building a model differs
from applying a model.

3
00:00:08.080 --> 00:00:09.290
After this video,

4
00:00:09.290 --> 00:00:14.170
you will be able to discuss what
building a classification model means.

5
00:00:14.170 --> 00:00:17.511
Explain the difference between
building and applying a model.

6
00:00:17.511 --> 00:00:20.910
And summarize why the parameters
of a model needs to be adjusted.

7
00:00:22.170 --> 00:00:25.150
A machine learning model
is a mathematical model.

8
00:00:25.150 --> 00:00:29.560
In the general sense, this means that
the model has parameters and uses

9
00:00:29.560 --> 00:00:33.630
equations to determine the relationship
between its inputs and outputs.

10
00:00:34.640 --> 00:00:39.730
The parameters are used by the model to
modify the inputs to generate the outputs.

11
00:00:39.730 --> 00:00:42.930
The model adjusts its parameters
in order to correct or

12
00:00:42.930 --> 00:00:45.770
refine this input, output relationship.

13
00:00:47.030 --> 00:00:49.630
Here's an example of a simple model.

14
00:00:49.630 --> 00:00:52.150
This mathematical model represents a line.

15
00:00:52.150 --> 00:00:58.554
y is the output, x is the input,
m determines the slope of the line and

16
00:00:58.554 --> 00:01:05.080
b determines the y-intercept or
where the line crosses the y-axis.

17
00:01:05.080 --> 00:01:07.720
m and b are the model's parameters.

18
00:01:07.720 --> 00:01:09.722
Given a specific value for

19
00:01:09.722 --> 00:01:14.368
x, the model uses as parameters
along with x to determine y.

20
00:01:14.368 --> 00:01:19.168
By adjusting the values for
the parameters m and b,

21
00:01:19.168 --> 00:01:24.760
the model can adjust how the input
x matched to the output y.

22
00:01:26.010 --> 00:01:29.262
Here we see how the output y changes for

23
00:01:29.262 --> 00:01:33.844
the same value of input x,
when parameter b changes.

24
00:01:33.844 --> 00:01:39.480
Recall that b is the y-intercept, or
where the line crosses the y-axis.

25
00:01:40.520 --> 00:01:48.048
The value of b is +1 for
the red line and -1 for the blue line.

26
00:01:48.048 --> 00:01:51.996
For the input x=1, the value of y is 3 for

27
00:01:51.996 --> 00:01:56.175
the red line,
as indicated by the red arrow.

28
00:01:56.175 --> 00:02:02.024
For the blue line, when the parameter
b changes from +1 to -1,

29
00:02:02.024 --> 00:02:07.670
for x=1, the value of y is 1,
as indicated by the blue arrow.

30
00:02:08.700 --> 00:02:12.710
So we see that with just a simple
change in one model parameter,

31
00:02:12.710 --> 00:02:15.074
the input to output mapping changes.

32
00:02:16.585 --> 00:02:19.495
A machine learning model
works in a similar way.

33
00:02:19.495 --> 00:02:22.285
It maps input values to output values.

34
00:02:22.285 --> 00:02:25.640
And it adjusts the parameters
in order to correct or

35
00:02:25.640 --> 00:02:28.145
refine this input-output mapping.

36
00:02:29.215 --> 00:02:32.665
The parameters of a machine
learning model are adjusted or

37
00:02:32.665 --> 00:02:36.185
estimated from the data
using a learning algorithm.

38
00:02:37.300 --> 00:02:41.240
This, in essence,
is what is involved in building a model.

39
00:02:41.240 --> 00:02:46.869
This process is referred to by many terms,
such as model building,

40
00:02:46.869 --> 00:02:51.326
model creation,
model training and model fitting.

41
00:02:51.326 --> 00:02:52.803
In building a model,

42
00:02:52.803 --> 00:02:57.736
we want to adjust the parameters in
order to reduce the model's error.

43
00:02:57.736 --> 00:03:02.424
In the case of supervised tasks,
such as classification, this means getting

44
00:03:02.424 --> 00:03:07.340
the model's outputs to match the targets
or desired outputs as much as possible.

45
00:03:08.580 --> 00:03:12.600
Since the classification task is
to predict the correct category or

46
00:03:12.600 --> 00:03:15.650
class, given the input variables,

47
00:03:15.650 --> 00:03:20.150
you can think of the classification
problem visually as carving out the input

48
00:03:20.150 --> 00:03:24.910
space as regions corresponding
to the different class labels.

49
00:03:24.910 --> 00:03:29.160
In this diagram for example,
the classification model needs to form

50
00:03:29.160 --> 00:03:34.350
the boundaries to define the regions
separating red triangles

51
00:03:34.350 --> 00:03:38.580
from blue diamonds, from green circles,
from yellow squares.

52
00:03:39.670 --> 00:03:45.180
In this example, if a sample falls within
the region in the upper right corner,

53
00:03:45.180 --> 00:03:47.160
it will be classified as a blue diamond.

54
00:03:48.280 --> 00:03:53.215
Classification decisions are based on
these regions, and the regions are defined

55
00:03:53.215 --> 00:03:57.316
by the boundaries, as indicated by
the dashed lines in the diagram.

56
00:03:57.316 --> 00:04:00.349
So these boundaries are referred
to as decision boundaries.

57
00:04:01.520 --> 00:04:06.720
Building a classification then means using
the data to adjust the model's parameters

58
00:04:06.720 --> 00:04:10.680
in order to form decision boundaries
to separate the target classes.

59
00:04:11.915 --> 00:04:15.770
Note that the term classifier is often
used to mean classification model.

60
00:04:17.480 --> 00:04:20.650
In general,
building a classification model,

61
00:04:20.650 --> 00:04:24.190
as well as other machine learning models,
involves two phases.

62
00:04:25.870 --> 00:04:30.515
The first is the training phase,
in which the model is constructed and

63
00:04:30.515 --> 00:04:34.190
its parameters adjusted using as
what referred to as training data.

64
00:04:35.350 --> 00:04:39.020
Training data is the data set
used to train or create a model.

65
00:04:40.500 --> 00:04:42.750
The second is the testing phase.

66
00:04:42.750 --> 00:04:45.984
This is where the learned
model is applied to new data.

67
00:04:45.984 --> 00:04:48.250
That is,
data not used in training the model.

68
00:04:49.790 --> 00:04:51.570
Here's another way to
look at the two phases.

69
00:04:53.010 --> 00:04:57.130
In a training phase, the learning
algorithm uses the training data

70
00:04:57.130 --> 00:05:00.070
to adjust the model's
parameters to minimize errors.

71
00:05:01.090 --> 00:05:03.700
At the end of the training phase,
you get the trained model.

72
00:05:05.370 --> 00:05:10.270
In the testing phase,
the trained model is applied to test data.

73
00:05:10.270 --> 00:05:16.220
Test data is separate from training data
and is previously unseen by the model.

74
00:05:16.220 --> 00:05:19.810
The model is then evaluated on
how it performs on the test data.

75
00:05:20.930 --> 00:05:24.607
The goal in building a classifier
model is to have the model

76
00:05:24.607 --> 00:05:27.696
perform well on training,
as well as test data.

77
00:05:27.696 --> 00:05:30.611
We will discuss in more detail
the use of training and

78
00:05:30.611 --> 00:05:35.470
test data sets in the next module,
when we discuss model evaluation.

79
00:05:35.470 --> 00:05:40.330
To adjust a model's parameters,
we need to apply a learning algorithm.

80
00:05:40.330 --> 00:05:44.460
We will discuss the specific algorithms
to build a classification model in

81
00:05:44.460 --> 00:05:45.510
the next few lectures.

1
00:00:01.070 --> 00:00:04.650
In this video, we will outline
some commonly used algorithms for

2
00:00:04.650 --> 00:00:07.260
building a classification model.

3
00:00:07.260 --> 00:00:11.540
After this video,
you will be able to describe the goal of

4
00:00:11.540 --> 00:00:16.260
a classification algorithm and name some
common algorithms for classification.

5
00:00:17.630 --> 00:00:22.140
Recall that a classification task is
to predict the category from the input

6
00:00:22.140 --> 00:00:23.580
variables.

7
00:00:23.580 --> 00:00:29.570
A classification model processes the input
data it receives and provides an output.

8
00:00:29.570 --> 00:00:33.860
Since classification is a supervised task,
a target or

9
00:00:33.860 --> 00:00:37.710
desired output is provided for
each sample.

10
00:00:37.710 --> 00:00:42.640
The goal is to get the model outputs to
match the targets as much as possible.

11
00:00:44.040 --> 00:00:47.350
A classification model
adjusts its parameters

12
00:00:47.350 --> 00:00:50.610
to get its outputs to match the targets.

13
00:00:50.610 --> 00:00:54.500
To adjust a model's parameters,
a learning algorithm is applied.

14
00:00:55.580 --> 00:00:59.080
This occurs in a training phase
when the model is constructed.

15
00:01:00.570 --> 00:01:03.950
There are many algorithms to
build a classification model.

16
00:01:03.950 --> 00:01:08.514
In this course,
we will cover the algorithms listed here,

17
00:01:08.514 --> 00:01:13.371
kNN or k Nearest Neighbors,
decision tree, and naive Bayes.

18
00:01:13.371 --> 00:01:17.870
kNN stands for k Nearest Neighbors.

19
00:01:17.870 --> 00:01:22.490
This technique relies on the notion that
samples with similar characteristics,

20
00:01:22.490 --> 00:01:28.440
that is samples with similar values for
input, likely belong to the same class.

21
00:01:28.440 --> 00:01:31.150
So classification of a sample is dependent

22
00:01:31.150 --> 00:01:33.650
on the target values of
the neighboring points.

23
00:01:35.410 --> 00:01:39.560
Another classification technique
is referred to as decision tree.

24
00:01:39.560 --> 00:01:44.270
A decision tree is a classification
model that uses a treelike structure

25
00:01:44.270 --> 00:01:47.398
to represent multiple decision paths.

26
00:01:47.398 --> 00:01:52.010
Traversing each path leads to a different
way to classify an input sample.

27
00:01:53.450 --> 00:01:57.770
A naive Bayes model uses a probabilistic
approach to classification.

28
00:01:58.870 --> 00:02:02.920
Baye's Theorem is used to capture the
relationship between the input data and

29
00:02:02.920 --> 00:02:03.830
the output class.

30
00:02:04.880 --> 00:02:09.130
Simply put, the Baye's Theorem
compares the probability of an event

31
00:02:09.130 --> 00:02:11.660
in the presence of another event.

32
00:02:11.660 --> 00:02:16.150
We see here the probability
of A if B is present.

33
00:02:16.150 --> 00:02:20.770
For example, probability of having
a fire if the weather is hot.

34
00:02:20.770 --> 00:02:24.100
You can imagine event B depending
on more than one variable.

35
00:02:24.100 --> 00:02:26.416
For example, weather is hot and windy.

36
00:02:26.416 --> 00:02:34.060
We will cover kNN, decision tree and naive
Bayes in detail in the next few lectures.

37
00:02:34.060 --> 00:02:38.150
There are many other classification
techniques, but we will focus on these

38
00:02:38.150 --> 00:02:41.970
since they are fundamental algorithms
that are commonly used and

39
00:02:41.970 --> 00:02:44.940
form the basis of other algorithms for
classification.
