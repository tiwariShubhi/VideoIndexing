
1
00:00:02.826 --> 00:00:04.162
Data Science.

2
00:00:04.162 --> 00:00:06.298
Getting value out of big data.

3
00:00:26.242 --> 00:00:31.860
We have all heard data science turned
data into insights or even actions.

4
00:00:31.860 --> 00:00:34.250
But what does that really mean?

5
00:00:34.250 --> 00:00:36.990
Data science can be
thought of as a basis for

6
00:00:36.990 --> 00:00:41.440
empirical research where data is used
to induce information for observations.

7
00:00:42.490 --> 00:00:46.310
These observations are mainly data,
in our case,

8
00:00:46.310 --> 00:00:49.985
big data, related to a business or
scientific case.

9
00:00:49.985 --> 00:00:54.430
Insight is a term

10
00:00:54.430 --> 00:00:59.050
we use to refer to the data
products of data science.

11
00:00:59.050 --> 00:01:03.523
It is extracted from a diverse amount
of data through a combination of

12
00:01:03.523 --> 00:01:07.686
exploratory data analysis and modeling.

13
00:01:07.686 --> 00:01:13.440
The questions are sometimes more specific,
and sometimes it requires

14
00:01:13.440 --> 00:01:18.170
looking at the data and patterns in it
to come up with the specific question.

15
00:01:20.420 --> 00:01:25.550
Another important point to recognize
is that data science is not static.

16
00:01:25.550 --> 00:01:28.020
It is not one time analysis.

17
00:01:28.020 --> 00:01:32.920
It involves a process where models
generated to lead to insights

18
00:01:32.920 --> 00:01:37.630
are constantly improved through further
empirical evidence, or simply, data.

19
00:01:39.640 --> 00:01:45.060
For example, a book retailer like
Amazon.com can constantly improve

20
00:01:45.060 --> 00:01:49.560
the model of a customer's book preferences
using the customer demographic,

21
00:01:50.580 --> 00:01:55.630
his or her previous purchases and
the book reviews of the customer.

22
00:01:57.920 --> 00:02:02.350
The book retailer can also
uses information to predict

23
00:02:02.350 --> 00:02:07.219
which customers are likely
to like any book,

24
00:02:07.219 --> 00:02:11.560
and take action to market
the book to those customers.

25
00:02:13.840 --> 00:02:16.955
This is where we see insights
being turned into action.

26
00:02:19.834 --> 00:02:24.718
As we have seen in the book marketing
example, using data science and analysis

27
00:02:24.718 --> 00:02:29.240
of the past and current information,
data science generates actions.

28
00:02:30.350 --> 00:02:33.200
This is not just an analysis of the past,
but

29
00:02:33.200 --> 00:02:36.980
rather generation of actionable
information for the future.

30
00:02:38.370 --> 00:02:42.070
This is what we can call a prediction,
like the weather forecast.

31
00:02:43.680 --> 00:02:49.140
When you decide what to wear for
the day based on the forecast of the day,

32
00:02:49.140 --> 00:02:53.880
you are taking action based
on insight delivered to you.

33
00:02:53.880 --> 00:02:58.110
Just like this, business leaders and
decision makers

34
00:02:58.110 --> 00:03:02.170
take action based on the evidence
provided by their data science teams.

35
00:03:04.140 --> 00:03:05.870
We set data science teams.

36
00:03:06.980 --> 00:03:11.290
This comes from the breadth of information
and skill that it takes to make it happen.

37
00:03:12.590 --> 00:03:17.010
You have probably seen diagrams like
this one that describe data science.

38
00:03:18.090 --> 00:03:22.220
Data science happens at
the intersection of computer science,

39
00:03:22.220 --> 00:03:24.820
mathematics and business expertise.

40
00:03:27.170 --> 00:03:29.750
If we zoom deeper into this diagram and

41
00:03:29.750 --> 00:03:35.030
open the sets of expertise we will
see a variation of this figure.

42
00:03:36.410 --> 00:03:42.690
Even at this level, all of these circles
require deeper knowledge and skills

43
00:03:42.690 --> 00:03:48.140
in areas like domain expertise, data
engineering, statistics and computing.

44
00:03:51.185 --> 00:03:55.790
An even deeper analysis of these
skills will lead you to skills like

45
00:03:55.790 --> 00:04:00.110
machine learning, statistical modeling,
relational algebra,

46
00:04:00.110 --> 00:04:04.640
business passion, problem solving and
data visualization.

47
00:04:04.640 --> 00:04:07.582
That's a lot of skills to have for
a single person.

48
00:04:12.922 --> 00:04:17.862
These wide range of skills and
definitions of data scientists having them

49
00:04:17.862 --> 00:04:21.855
all led to discussions like
are data scientists unicorns?

50
00:04:22.895 --> 00:04:24.115
Meaning they don't exist.

51
00:04:25.375 --> 00:04:29.795
There are data science experts who have
expertise in more than one of these

52
00:04:29.795 --> 00:04:31.380
skills, for sure.

53
00:04:31.380 --> 00:04:33.795
But they're relatively rare, and

54
00:04:33.795 --> 00:04:37.470
still would probably need help from
an expert on some of these areas.

55
00:04:38.730 --> 00:04:44.550
So, in reality, data scientists
are teams of people who act like one.

56
00:04:45.560 --> 00:04:48.860
They are passionate about the story and
the meaning behind data.

57
00:04:50.310 --> 00:04:54.700
They understand they problem
they are trying to solve, and

58
00:04:54.700 --> 00:04:57.910
aim to find the right analytical
methods to solve this problem.

59
00:04:57.910 --> 00:05:04.350
And they all have an interest in
engineering solutions to solve problems.

60
00:05:06.230 --> 00:05:11.360
They also have curiosity about each
others work, and have communication

61
00:05:11.360 --> 00:05:15.940
skills to interact with the team and
present their ideas and results to others.

62
00:05:17.900 --> 00:05:23.460
As a summary, a data science team often
comes together to analyze situations,

63
00:05:23.460 --> 00:05:29.370
business or scientific cases, which none
of the individuals can solve on their own.

64
00:05:29.370 --> 00:05:32.090
There are lots of moving
parts to the solution.

65
00:05:32.090 --> 00:05:36.370
But in the end, all these parts
should come together to provide

66
00:05:36.370 --> 00:05:39.055
actionable insight based on big data.

67
00:05:41.807 --> 00:05:46.732
Being able to use evidence-based
insight in business decisions is more

68
00:05:46.732 --> 00:05:48.517
important now than ever.

69
00:05:48.517 --> 00:05:52.594
Data scientists have a combination
of technical, business and

70
00:05:52.594 --> 00:05:54.716
soft skills to make this happen.

1
00:00:00.930 --> 00:00:03.310
Getting started,
characteristics of big data.

2
00:00:21.143 --> 00:00:25.786
By now you have seen that big data is a
blanket term that is used to refer to any

3
00:00:25.786 --> 00:00:30.140
collection of data so large and
complex that it exceeds the processing

4
00:00:30.140 --> 00:00:35.530
capability of conventional data
management systems and techniques.

5
00:00:35.530 --> 00:00:39.040
The applications of big data are endless.

6
00:00:39.040 --> 00:00:44.000
Every part of business and society
are changing in front our eyes due to that

7
00:00:44.000 --> 00:00:48.870
fact that we now have so much more
data and the ability for analyzing.

8
00:00:50.220 --> 00:00:52.120
But how can we characterize big data?

9
00:00:53.310 --> 00:00:56.630
You can say, I know it when i see it.

10
00:00:56.630 --> 00:00:59.440
But there are easier ways to do it.

11
00:00:59.440 --> 00:01:03.030
Big data is commonly characterized
using a number of V's.

12
00:01:04.040 --> 00:01:09.720
The first three are volume,
velocity, and variety.

13
00:01:09.720 --> 00:01:15.580
Volume refers to the vast amounts of
data that is generated every second,

14
00:01:15.580 --> 00:01:19.470
mInutes, hour, and
day in our digitized world.

15
00:01:21.130 --> 00:01:27.480
Variety refers to the ever increasing
different forms that data can come in

16
00:01:27.480 --> 00:01:32.090
such as text, images,
voice, and geospatial data.

17
00:01:34.100 --> 00:01:39.440
Velocity refers to the speed at
which data is being generated and

18
00:01:39.440 --> 00:01:42.980
the pace at which data moves
from one point to the next.

19
00:01:44.290 --> 00:01:46.510
Volume, variety, and

20
00:01:46.510 --> 00:01:52.430
velocity are the three main dimensions
that characterize big data.

21
00:01:52.430 --> 00:01:53.790
And describe its challenges.

22
00:01:55.180 --> 00:02:00.410
We have huge amounts of data
in different formats, and

23
00:02:00.410 --> 00:02:04.160
varying quality which must
be processed quickly.

24
00:02:05.200 --> 00:02:08.470
More Vs have been introduced
to the big data community

25
00:02:08.470 --> 00:02:12.070
as we discover new challenges and
ways to define big data.

26
00:02:13.310 --> 00:02:17.830
Veracity and
valence are two of these additional V's we

27
00:02:17.830 --> 00:02:21.720
will pay special attention to as
a part of this specialization.

28
00:02:22.860 --> 00:02:29.360
Veracity refers to the biases,
noise, and abnormality in data.

29
00:02:29.360 --> 00:02:34.680
Or, better yet, It refers to the often
unmeasurable uncertainties and

30
00:02:34.680 --> 00:02:38.540
truthfulness and trustworthiness of data.

31
00:02:38.540 --> 00:02:44.150
Valence refers to the connectedness
of big data in the form of graphs,

32
00:02:44.150 --> 00:02:45.180
just like atoms.

33
00:02:46.220 --> 00:02:52.180
Moreover, we must be sure to
never forget our sixth V, value.

34
00:02:53.220 --> 00:02:55.890
How do big data benefit you and
your organization?

35
00:02:57.560 --> 00:02:59.640
Without a clear strategy and

36
00:02:59.640 --> 00:03:03.700
an objective with the value
they are getting from big data.

37
00:03:03.700 --> 00:03:07.800
It is easy to imagine that organizations
will be sidetracked by all these

38
00:03:07.800 --> 00:03:12.630
challenges of big data, and not be
able to turn them into opportunities.

39
00:03:13.990 --> 00:03:17.993
Now let's start looking into the first
five of these V's in detail.

1
00:00:02.060 --> 00:00:04.400
How does data science happen?

2
00:00:04.400 --> 00:00:05.850
Five P's of data science.

3
00:00:07.810 --> 00:00:13.220
Now that we identified what data science
is and how companies can strategize around

4
00:00:13.220 --> 00:00:18.660
big data to start building a purpose,
let's come back to using data science

5
00:00:18.660 --> 00:00:23.870
to get value out of big data around
the purpose or questions they defined.

6
00:00:41.348 --> 00:00:43.591
Our experience with building and

7
00:00:43.591 --> 00:00:49.040
observing successful data science projects
led to a method around the craft with

8
00:00:49.040 --> 00:00:55.255
five distinct components that can be
defined as components of data science.

9
00:00:55.255 --> 00:00:59.859
Here we define data science
as a multi-disciplinary

10
00:00:59.859 --> 00:01:03.972
craft that combines
people teaming up around

11
00:01:03.972 --> 00:01:08.910
application-specific purpose that
can be achieved through a process,

12
00:01:10.420 --> 00:01:14.100
big data computing platforms,
and programmability.

13
00:01:17.260 --> 00:01:22.460
All of these should lead to
products where the focus really

14
00:01:22.460 --> 00:01:26.760
is on the questions or purpose that are
defined by your big data strategy ideas.

15
00:01:28.960 --> 00:01:33.540
There are many technology,
data and analytical research, and

16
00:01:33.540 --> 00:01:37.110
development related activities
around the questions.

17
00:01:37.110 --> 00:01:41.490
But in the end, everything we do
in this phase is to reach to that

18
00:01:41.490 --> 00:01:43.820
final product based on our purposes.

19
00:01:44.950 --> 00:01:47.830
So, it makes sense to start with it and

20
00:01:47.830 --> 00:01:50.860
build a process around how
we make this product happen.

21
00:01:52.740 --> 00:01:55.520
Remember the wild fire
prediction project I described?

22
00:01:56.830 --> 00:02:01.070
One of the products we described
there was the rate of spread and

23
00:02:01.070 --> 00:02:02.870
direction of an ongoing fire.

24
00:02:04.460 --> 00:02:06.780
We have identified questions and

25
00:02:06.780 --> 00:02:11.060
the process that led us to
the product in the end to solve it.

26
00:02:12.960 --> 00:02:17.280
We brought together experts around
the table for fire modeling,

27
00:02:17.280 --> 00:02:21.620
data management, time series analysis,
scalable computing,

28
00:02:21.620 --> 00:02:25.060
Geographical Information Systems,
and emergency response.

29
00:02:28.050 --> 00:02:31.770
I asked them,
let's not dive into the techniques yet.

30
00:02:31.770 --> 00:02:33.080
What is the problem at large?

31
00:02:34.100 --> 00:02:40.380
How do we see ourselves solving it?

32
00:02:40.380 --> 00:02:44.780
A typical conversation around
the process starts with this question.

33
00:02:46.230 --> 00:02:51.600
Then from then on,
drilling down to many areas of expertise,

34
00:02:51.600 --> 00:02:53.830
often we blur lines between the steps.

35
00:02:55.510 --> 00:03:00.280
My wildfire team would start listing
things like, we don't have an integrated

36
00:03:00.280 --> 00:03:05.400
system or we don't have real-time
access to data programmatically,

37
00:03:05.400 --> 00:03:08.210
so we can't analyze fires on the fly.

38
00:03:08.210 --> 00:03:12.950
Or they can say, I can't integrate
sensor data with satellite data.

39
00:03:14.370 --> 00:03:20.080
All of this leads me to challenges
I can then use to define problems.

40
00:03:21.300 --> 00:03:25.560
There are many dimensions of data science
to think about within this discussion.

41
00:03:26.570 --> 00:03:31.700
Let's start with the obvious ones,
people and purpose.

42
00:03:34.535 --> 00:03:39.800
People refers to a data science team or
the projects stakeholders.

43
00:03:39.800 --> 00:03:42.700
As you know by now,
they're expert in data and

44
00:03:42.700 --> 00:03:47.028
analytics, business, computing,
science, or big data management,

45
00:03:47.028 --> 00:03:53.154
like all the set of experts I
listed in my wildfire scenario.

46
00:03:53.154 --> 00:03:59.560
The purpose refers to the challenge or
set of challenges defined by your

47
00:03:59.560 --> 00:04:04.940
big data strategy, like solving the
question related to the rate of spread and

48
00:04:04.940 --> 00:04:08.170
direction of the fire perimeter
in the wildfire case.

49
00:04:12.003 --> 00:04:17.099
Since there's a predefined team
with a purpose, a great place for

50
00:04:17.099 --> 00:04:22.190
this team to start with is
a process they could iterate on.

51
00:04:22.190 --> 00:04:27.470
We can simply say, people with purpose
will define a process to collaborate and

52
00:04:27.470 --> 00:04:28.360
communicate around.

53
00:04:30.010 --> 00:04:33.500
The process is conceptual
in the beginning and

54
00:04:33.500 --> 00:04:37.390
defines the set of steps an how
everyone can contribute to it.

55
00:04:41.060 --> 00:04:43.230
There are many ways to
look at the process.

56
00:04:44.890 --> 00:04:50.250
One way of looking at it is
as two distinct activities,

57
00:04:50.250 --> 00:04:53.130
mainly big data engineering and

58
00:04:53.130 --> 00:04:57.500
big data analytics, or
computational big data science,

59
00:04:57.500 --> 00:05:01.910
as I like to call it, as more than simple
analytics is being performed here.

60
00:05:03.840 --> 00:05:10.180
A more detailed way of looking at
the process reveals five distinct steps or

61
00:05:10.180 --> 00:05:15.214
activities of this data science process,

62
00:05:15.214 --> 00:05:21.018
namely acquire, prepare,

63
00:05:21.018 --> 00:05:26.540
analyze, report, and act.

64
00:05:26.540 --> 00:05:29.190
We can simply say that
data science happens

65
00:05:29.190 --> 00:05:31.950
at the boundary of all these steps.

66
00:05:31.950 --> 00:05:36.510
Ideally, this process should
support experimental work and

67
00:05:36.510 --> 00:05:40.980
dynamic scalability on the big data and
computing platforms.

68
00:05:44.220 --> 00:05:49.050
This five step process can be used in
alternative ways in real life big data

69
00:05:49.050 --> 00:05:53.580
applications, if we add the dependencies
of different tools to each other.

70
00:05:54.930 --> 00:05:58.490
The influence of big data pushes for

71
00:05:58.490 --> 00:06:02.930
alternative scalability approaches
at each step of the process.

72
00:06:04.010 --> 00:06:07.080
Just like you would scale
each step on its own,

73
00:06:07.080 --> 00:06:10.780
you can scale the whole
process as a whole in the end.

74
00:06:14.613 --> 00:06:20.900
One can simply say, all of these steps
have reporting needs in different forms.

75
00:06:23.280 --> 00:06:30.290
Or there is a need to draw all these
activities as an iterating process,

76
00:06:30.290 --> 00:06:35.710
including build, explore, and
scale for big data as steps.

77
00:06:38.550 --> 00:06:42.550
Big data analysis needs alternative
data management techniques and

78
00:06:42.550 --> 00:06:47.290
systems, as well as analytical tools and
methods.

79
00:06:49.160 --> 00:06:54.730
Multiple modes of scalability is needed
based on dynamic data and computing loads.

80
00:06:55.750 --> 00:06:59.800
In addition,
change in physical infrastructure,

81
00:06:59.800 --> 00:07:04.320
streaming data specific urgencies
arising from special events

82
00:07:04.320 --> 00:07:07.780
can also require multiple
modes of scalability.

83
00:07:09.230 --> 00:07:11.490
In this intro course, for simplicity,

84
00:07:11.490 --> 00:07:17.470
we will refer to the process as a set of
five sequential activities that iterate.

85
00:07:18.500 --> 00:07:23.550
However, we'll touch on scalability as
needed in our example applications.

86
00:07:26.840 --> 00:07:30.190
As a part of building
your big data process,

87
00:07:30.190 --> 00:07:34.230
it's important to simply
mention two other P's.

88
00:07:34.230 --> 00:07:39.620
The first one is big data platforms,
like the ones in the Hadoop framework,

89
00:07:39.620 --> 00:07:43.610
or other computing platforms
to scale different steps.

90
00:07:43.610 --> 00:07:47.060
The scalability should be in
the mind of all team members and

91
00:07:47.060 --> 00:07:49.030
get communicated as an expectation.

92
00:07:51.390 --> 00:07:55.970
In addition, the scalable process
should be programmable through

93
00:07:55.970 --> 00:08:01.830
utilization of reusable and
reproducible programming interfaces

94
00:08:01.830 --> 00:08:06.308
to libraries, like systems middleware,
analytical tools,

95
00:08:06.308 --> 00:08:10.395
visualization environments, and
end user reporting environments.

96
00:08:14.010 --> 00:08:16.980
Thinking of big data
applications as a process,

97
00:08:16.980 --> 00:08:22.150
including a set of activities that
the team members can collaborate over,

98
00:08:22.150 --> 00:08:26.720
also helps to build metrics for
accountability to be built into it.

99
00:08:26.720 --> 00:08:31.000
This way, expectations on cost, time,

100
00:08:31.000 --> 00:08:34.610
optimization of deliverables,
and time lines can be discussed

101
00:08:34.610 --> 00:08:39.210
between the the members starting with
the beginning of the data science process.

102
00:08:42.325 --> 00:08:45.890
Sometimes we may not be able
to do this in one step.

103
00:08:47.550 --> 00:08:52.420
And joint explorations like statistical
evaluations of intermediate results or

104
00:08:52.420 --> 00:08:55.060
accuracy of sample data
sets become important.

105
00:08:57.150 --> 00:09:02.430
As a summary, data science can be
defined as a craft of using the five P's

106
00:09:02.430 --> 00:09:07.380
identified in this lecture,
leading to a sixth P, the data product.

107
00:09:08.580 --> 00:09:12.870
Having a process within the more
business-driven Ps, like people and

108
00:09:12.870 --> 00:09:17.120
purpose, and the more technically
driven P's, like platforms and

109
00:09:17.120 --> 00:09:22.220
programmability, leads to
a streamlined approach that starts and

110
00:09:22.220 --> 00:09:26.630
ends with the product, team
accountability, and collaboration in mind.

111
00:09:27.740 --> 00:09:31.311
Data science process
provides guidelines for

112
00:09:31.311 --> 00:09:36.621
implementing big data solution,
as it helps to organize efforts and

113
00:09:36.621 --> 00:09:43.137
ensures all critical steps taken conforms
to pre-define and agreed upon metrics.

1
00:00:03.008 --> 00:00:05.098
Step one, acquiring data.

2
00:00:19.898 --> 00:00:24.420
The first step in the data science
process is to acquire the data.

3
00:00:25.660 --> 00:00:29.860
You need to obtain the source material
before analyzing or acting on it.

4
00:00:31.950 --> 00:00:37.690
The first step in acquiring data is
to determine what data is available.

5
00:00:37.690 --> 00:00:41.680
Leave no stone unturned when it comes
to finding the right data sources.

6
00:00:42.770 --> 00:00:46.150
You want to identify suitable
data related to your problem and

7
00:00:47.200 --> 00:00:51.540
make use of all data that is relevant
to your problem for analysis.

8
00:00:52.720 --> 00:00:56.580
Leaving out even a small
amount of important data

9
00:00:56.580 --> 00:00:58.570
can lead to incorrect conclusions.

10
00:01:01.190 --> 00:01:04.460
Data, comes from, many places, local and

11
00:01:04.460 --> 00:01:09.280
remote, in many varieties,
structured and un-structured.

12
00:01:09.280 --> 00:01:12.250
And, with different velocities.

13
00:01:12.250 --> 00:01:17.780
There are many techniques and technologies
to access these different types of data.

14
00:01:17.780 --> 00:01:19.420
Let's discuss a few examples.

15
00:01:21.360 --> 00:01:26.020
A lot of data exists in
conventional relational databases,

16
00:01:26.020 --> 00:01:28.560
like structure big data
from organizations.

17
00:01:29.600 --> 00:01:35.270
The tool of choice to access data from
databases is structured query language or

18
00:01:35.270 --> 00:01:40.360
SQL, which is supported by all
relational databases management systems.

19
00:01:41.680 --> 00:01:47.260
Additionally, most data base systems
come with a graphical application

20
00:01:47.260 --> 00:01:52.250
environment that allows you to query and
explore the data sets in the database.

21
00:01:54.990 --> 00:02:01.870
Data can also exist in files such as
text files and Excel spreadsheets.

22
00:02:01.870 --> 00:02:05.638
Scripting languages are generally
used to get data from files.

23
00:02:05.638 --> 00:02:11.200
A scripting language is a high
level programming language

24
00:02:11.200 --> 00:02:16.020
that can be either general purpose or
specialized for specific functions.

25
00:02:17.960 --> 00:02:24.010
Common scripting languages with support
for processing files are Java Script,

26
00:02:24.010 --> 00:02:29.300
Python, PHP, Perl, R, and
MATLAB, and are many others.

27
00:02:31.780 --> 00:02:36.230
An increasingly popular way
to get data is from websites.

28
00:02:36.230 --> 00:02:40.956
Web pages are written using
a set of standards approved by

29
00:02:40.956 --> 00:02:44.906
a world wide web consortium or
shortly, W3C.

30
00:02:44.906 --> 00:02:48.650
This includes a variety of formats and
services.

31
00:02:49.920 --> 00:02:55.860
One common format is
the Extensible Markup Language, or XML,

32
00:02:55.860 --> 00:03:00.640
which uses markup symbols or tabs to
describe the contents on a webpage.

33
00:03:02.250 --> 00:03:07.920
Many websites also host web services which
produce program access to their data.

34
00:03:10.310 --> 00:03:13.110
There are several types of web services.

35
00:03:13.110 --> 00:03:16.990
The most popular is REST because it's so
easy to use.

36
00:03:18.080 --> 00:03:22.280
REST stand for
Representational State Transfer.

37
00:03:22.280 --> 00:03:26.640
And it is an approach to implementing
web services with performance,

38
00:03:26.640 --> 00:03:29.200
scalability and maintainability in mind.

39
00:03:30.940 --> 00:03:34.400
Web socket services are also
becoming more popular

40
00:03:34.400 --> 00:03:37.440
since they allow real time
modifications from web sites.

41
00:03:40.040 --> 00:03:44.840
NoSQL storage systems are increasingly
used to manage a variety of data

42
00:03:44.840 --> 00:03:45.770
types in big data.

43
00:03:46.950 --> 00:03:50.710
These data stores are databases
that do not represent data

44
00:03:50.710 --> 00:03:55.580
in a table format with columns and rows as
with conventional relational databases.

45
00:03:56.910 --> 00:04:02.180
Examples of these data stores include
Cassandra, MongoDB and HBASE.

46
00:04:03.815 --> 00:04:08.350
NoSQL data stores provide APIs
to allow users to access data.

47
00:04:09.460 --> 00:04:14.940
These APIs can be used directly or in an
application that needs to access the data.

48
00:04:16.660 --> 00:04:21.240
Additionally, most NoSQL
systems provide data access

49
00:04:21.240 --> 00:04:23.960
via a web service interface, such a REST.

50
00:04:26.780 --> 00:04:29.860
Now, let's discuss our wildfire case study

51
00:04:29.860 --> 00:04:34.350
as a real project that acquires data
using several different mechanisms.

52
00:04:35.780 --> 00:04:42.020
The WIFIRE project stores sensor data from
weather stations in a relational database.

53
00:04:43.140 --> 00:04:48.110
We use SQL to retrieve this data
from the database to create

54
00:04:48.110 --> 00:04:52.800
models to identify weather patterns
associated with Santa Anna conditions.

55
00:04:54.450 --> 00:04:58.910
To determine whether a particular weather
station is currently experiencing

56
00:04:58.910 --> 00:05:05.340
Santa Anna conditions, we access real
time data using a web socket service.

57
00:05:06.610 --> 00:05:10.170
Once we start listening to this service,

58
00:05:10.170 --> 00:05:12.980
we receive weather station
measurements as they occur.

59
00:05:14.330 --> 00:05:19.450
This data is then processed and
compared to patterns found by our models

60
00:05:19.450 --> 00:05:23.390
to determine if a weather station is
experiencing Santa Ana conditions.

61
00:05:25.060 --> 00:05:28.130
At the same time Tweets are retrieved

62
00:05:28.130 --> 00:05:32.870
using hashtags related to any fire
that is occurring in the region.

63
00:05:33.950 --> 00:05:38.080
The Tweet messages are retrieves
using the Twitter REST service.

64
00:05:38.080 --> 00:05:43.260
The idea is to determine the sentiment
of these tweets to see if people

65
00:05:43.260 --> 00:05:50.540
are expressing fear, anger or are simply
nonchalant about the nearby fire.

66
00:05:50.540 --> 00:05:55.617
The combination of sensor data and
tweet sentiments helps

67
00:05:55.617 --> 00:06:00.389
to give us a sense of the urgency
of the fire situation.

68
00:06:00.389 --> 00:06:04.570
As a summary,
big data comes from many places.

69
00:06:05.570 --> 00:06:08.000
Finding and evaluating data

70
00:06:08.000 --> 00:06:12.490
useful to your big data analytics is
important before you start acquiring data.

71
00:06:13.740 --> 00:06:15.080
Depending on the source and

72
00:06:15.080 --> 00:06:19.240
structure of data,
there are alternative ways to access it.

1
00:00:02.992 --> 00:00:05.893
Step 2-A: Exploring Data

2
00:00:18.798 --> 00:00:23.194
After you've put together the data
that you need for your application,

3
00:00:23.194 --> 00:00:27.460
you might be tempted to immediately
build models to analyze the data.

4
00:00:28.520 --> 00:00:29.830
Resist this temptation.

5
00:00:30.890 --> 00:00:35.370
The first step after getting
your data is to explore it.

6
00:00:35.370 --> 00:00:39.340
Exploring data is a part of
the two-step data preparation process.

7
00:00:41.450 --> 00:00:44.560
You want to do some
preliminary investigation

8
00:00:44.560 --> 00:00:50.080
in order to gain a better understanding of
the specific characteristics of your data.

9
00:00:50.080 --> 00:00:52.620
In this step, you'll be looking for

10
00:00:52.620 --> 00:00:56.350
things like correlations,
general trends, and outliers.

11
00:00:57.530 --> 00:01:01.700
Without this step, you will not be
able to use the data effectively.

12
00:01:04.100 --> 00:01:07.520
Correlation graphs can be used
to explore the dependencies

13
00:01:07.520 --> 00:01:09.630
between different variables in the data.

14
00:01:11.210 --> 00:01:15.910
Graphing the general trends of
variables will show you if there is

15
00:01:15.910 --> 00:01:21.120
a consistent direction in which the values
of these variables are moving towards,

16
00:01:22.150 --> 00:01:24.320
like sales prices going up or down.

17
00:01:26.200 --> 00:01:34.320
In statistics, an outlier is a data point
that's distant from other data points.

18
00:01:34.320 --> 00:01:37.256
Plotting outliers will
help you double check for

19
00:01:37.256 --> 00:01:39.698
errors in the data due to measurements.

20
00:01:39.698 --> 00:01:45.390
In some cases, outliers that are not
errors might make you find a rare event.

21
00:01:47.670 --> 00:01:53.860
Additionally, summary statistics provide
numerical values to describe your data.

22
00:01:55.040 --> 00:02:00.195
Summary statistics are quantities
that capture various characteristics

23
00:02:00.195 --> 00:02:04.540
of a set of values with a single number or
a small set of numbers.

24
00:02:06.540 --> 00:02:10.674
Some basic summary statistics
that you should compute for

25
00:02:10.674 --> 00:02:15.490
your data set are mean, median,
range, and standard deviation.

26
00:02:17.000 --> 00:02:22.510
Mean and median are measures of
the location of a set of values.

27
00:02:22.510 --> 00:02:26.590
Mode is the value that occurs
most frequently in your data set.

28
00:02:27.920 --> 00:02:32.220
And range and standard deviation
are measures of spread in your data.

29
00:02:33.640 --> 00:02:38.810
Looking at these measures will give you
an idea of the nature of your data.

30
00:02:40.630 --> 00:02:43.340
They can tell you if there's
something wrong with your data.

31
00:02:44.340 --> 00:02:48.957
For example, if the range of the values
for age in your data includes

32
00:02:48.957 --> 00:02:52.926
negative numbers, or
a number much greater than 100,

33
00:02:52.926 --> 00:02:57.708
there's something suspicious in
the data that needs to be examined.

34
00:03:00.629 --> 00:03:05.900
Visualization techniques also
provide a quick and effective, and

35
00:03:05.900 --> 00:03:11.830
overall a very useful way to look at
data in this preliminary analysis step.

36
00:03:11.830 --> 00:03:15.250
A heat map, such as the one shown here,

37
00:03:15.250 --> 00:03:19.530
can quickly give you the idea
of where the hotspots are.

38
00:03:19.530 --> 00:03:22.020
Many other different types
of graphs can be used.

39
00:03:23.340 --> 00:03:27.320
Histograms show that
the distribution of the data and

40
00:03:27.320 --> 00:03:30.520
can show skewness or unusual dispersion.

41
00:03:31.920 --> 00:03:36.168
Boxplots are another type of plot for
showing data distribution.

42
00:03:38.448 --> 00:03:44.410
Line graphs are useful for seeing how
values in your data change over time.

43
00:03:44.410 --> 00:03:47.140
Spikes in the data are also easy to spot.

44
00:03:49.550 --> 00:03:54.070
Scatter plots can show you
correlation between two variables.

45
00:03:54.070 --> 00:03:57.900
Overall, there are many types
of graph to visualize data.

46
00:03:58.935 --> 00:04:02.030
They are very useful in helping
you understand the data you have.

47
00:04:04.120 --> 00:04:08.720
In summary, what you get by
exploring your data is a better

48
00:04:08.720 --> 00:04:12.510
understanding of the complexity of
the data you have to work with.

49
00:04:13.780 --> 00:04:16.418
This, in turn,
will guide the rest of your process.
