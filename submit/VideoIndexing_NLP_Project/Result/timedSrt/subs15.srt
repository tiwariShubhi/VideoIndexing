1
00:00:03,030 --> 00:00:05,956
it can be said without a doubt and the internet and

2
00:00:05,956 --> 00:00:08,960
the worldwide web changed everything in our lives 

3
00:00:10,450 --> 00:00:15,100
the worldwide web is indeed the largest information source there is today 

4
00:00:15,100 --> 00:00:16,630
but what is the data model behind the web 

5
00:00:17,800 --> 00:00:20,740
we will say that it is the semi - structure data model 

6
00:00:22,310 --> 00:00:26,560
so after going through this video you will be able to distinguish between

7
00:00:26,560 --> 00:00:29,870
the structured data model that we talked about the last time and

8
00:00:29,870 --> 00:00:32,805
semi - structured data model 

9
00:00:32,805 --> 00:00:36,110
further you will recognize that the most

10
00:00:36,110 --> 00:00:40,680
times the semi - structured data refers to tree structured data 

11
00:00:41,720 --> 00:00:47,160
and you can explain why tree navigation operations are important for

12
00:00:47,160 --> 00:00:49,370
formats like xml and json 

13
00:00:51,050 --> 00:00:53,290
let a take a very simple web page 

14
00:00:55,090 --> 00:00:58,740
now this page does not have a lot of content or stylization 

15
00:00:58,740 --> 00:01:01,970
it does not even have links to other pages but

16
00:01:01,970 --> 00:01:04,390
let look at the corresponding html code 

17
00:01:06,560 --> 00:01:11,340
this code is used by the browser so that it can render the html and

18
00:01:11,340 --> 00:01:13,580
notice a few things in this data 

19
00:01:13,580 --> 00:01:19,710
the entire data comes within the html and slash html blocks 

20
00:01:21,760 --> 00:01:28,590
and we similarly have a body begin and end a header begin and

21
00:01:28,590 --> 00:01:33,950
end a list begin and end and a paragraph begin and end 

22
00:01:36,020 --> 00:01:40,330
everywhere here a block is nested within a larger block 

23
00:01:41,770 --> 00:01:46,350
the second item to notice is that unlike a relational structure

24
00:01:46,350 --> 00:01:49,940
there are multiple list items and multiple paragraphs 

25
00:01:49,940 --> 00:01:53,040
and any single document would have a different number of them 

26
00:01:54,100 --> 00:01:59,250
this means while the date object has some structure it is more flexible 

27
00:02:00,360 --> 00:02:05,930
so this is the hallmark office semi structure date model 

28
00:02:05,930 --> 00:02:07,020
now xml or

29
00:02:07,020 --> 00:02:12,100
the extensible markup language is another well known standard to represent data 

30
00:02:12,100 --> 00:02:16,880
you can think of xml as a generalization of html where the elements that 

31
00:02:16,880 --> 00:02:21,270
the beginning and end markers within the angular brackets can be any string 

32
00:02:21,270 --> 00:02:24,240
and not like the ones allowed by standard html 

33
00:02:25,530 --> 00:02:28,840
let see an example from a biological case 

34
00:02:30,140 --> 00:02:34,770
as you can see there are two elements called sample attribute 

35
00:02:36,100 --> 00:02:39,130
they do structurally different because they

36
00:02:39,130 --> 00:02:42,320
have different numbers of sub elements called the value 

37
00:02:44,030 --> 00:02:46,960
another interesting issue about xml data processing

38
00:02:46,960 --> 00:02:50,530
is that you can actually credit for the structure elements 

39
00:02:50,530 --> 00:02:55,560
for example it is perfectly fine to ask what is the name of the element

40
00:02:55,560 --> 00:02:59,250
which contains a sub - element whose textual content is cell type 

41
00:03:00,390 --> 00:03:06,990
as you can see you will get two results sample attribute 

42
00:03:06,990 --> 00:03:11,604
an experimental factor because sample attribute has a sub - element called

43
00:03:11,604 --> 00:03:15,940
category and experimental factor has a subelement called link and

44
00:03:15,940 --> 00:03:18,860
each of these subelements have the value celltape 

45
00:03:20,090 --> 00:03:23,860
now we cannot perform an operation like this in a relational data model 

46
00:03:23,860 --> 00:03:28,090
for example we cannot say which relation has a column with a value john 

47
00:03:30,730 --> 00:03:35,740
the same idea can also be seen in json or the java script object notation which

48
00:03:35,740 --> 00:03:41,610
is a very popular format used for many different data like twitter and facebook 

49
00:03:41,610 --> 00:03:47,170
consider the example here all of the format looks different 

50
00:03:47,170 --> 00:03:50,390
we have a similar nested structure varies that is lists

51
00:03:50,390 --> 00:03:55,030
containing other lists which will contain topples which consists of p value ps 

52
00:03:57,080 --> 00:04:01,160
so the key value pairs at atomic property names and their values 

53
00:04:02,660 --> 00:04:07,760
but one way to generalize about all these different forms of semi structured data

54
00:04:07,760 --> 00:04:10,460
is to model them as trees 

55
00:04:10,460 --> 00:04:12,380
let go back to xml 

56
00:04:12,380 --> 00:04:15,210
the left side shows an xml document and

57
00:04:15,210 --> 00:04:17,440
the right side shows the corresponding tree 

58
00:04:18,660 --> 00:04:22,030
since the top object of the root element is document 

59
00:04:22,030 --> 00:04:23,230
it is also the root of the tree 

60
00:04:24,550 --> 00:04:28,620
now under document we have a report element with author and

61
00:04:28,620 --> 00:04:34,450
date under it and also a paper element with title author and source under it 

62
00:04:34,450 --> 00:04:38,670
the actual values like is the textual content of an element 

63
00:04:40,480 --> 00:04:43,760
since a text data item cannot have any further components 

64
00:04:43,760 --> 00:04:46,590
these text values are always the leaves of the tree 

65
00:04:48,760 --> 00:04:53,950
now modeling a document as a tree has significant advantages 

66
00:04:53,950 --> 00:04:55,740
a tree is a well - known data structure 

67
00:04:55,740 --> 00:05:00,100
that allows what is called a navigational access to data 

68
00:05:00,100 --> 00:05:02,150
imagine you are standing on the note paper 

69
00:05:03,460 --> 00:05:08,610
now you can perform a getparent operation and navigate the document 

70
00:05:08,610 --> 00:05:14,460
or you can perform a getchildren operation to get to the title author and source 

71
00:05:14,460 --> 00:05:18,550
you can even perform a getsiblings operation and get to the report 

72
00:05:19,810 --> 00:05:24,970
you can also ask a textual query like which strings have the substring data and

73
00:05:24,970 --> 00:05:31,340
seek their root - to - node path to get to the path from document to the text nodes 

74
00:05:31,340 --> 00:05:34,590
you can possibly see how queries can be evaluated on the tree 

75
00:05:35,670 --> 00:05:37,250
now let us take the query 

76
00:05:37,250 --> 00:05:39,990
who is the author of xml query data model 

77
00:05:41,900 --> 00:05:47,987
in one evaluation scheme we can navigate up from the text note to title 

78
00:05:47,987 --> 00:05:53,580
to paper and then navigate down to author and then to don robie 

79
00:05:53,580 --> 00:05:56,990
well how do we know that we have to get up to paper before reversing the direction 

80
00:05:58,050 --> 00:06:01,470
well paper is the least that the lowest in the tree 

81
00:06:01,470 --> 00:06:07,170
common ancestor of the author note and the xm query data model note 

82
00:06:07,170 --> 00:06:10,207
we will come back to semi structure data in a later module 

1
00:06:11,851 --> 00:06:15,973
in this hands - on activity we will be looking at graph data in gephi 

2
00:06:15,973 --> 00:06:20,510
first we will import data into gephi and then examine the properties of the graph 

3
00:06:21,720 --> 00:06:25,160
next we will perform some statistical operations on the graph data 

4
00:06:25,160 --> 00:06:27,750
and then run some different layout algorithms 

5
00:06:30,090 --> 00:06:31,585
let begin 

6
00:06:31,585 --> 00:06:36,370
we are not running gephi in the cloudera virtual machine on the coursera website 

7
00:06:36,370 --> 00:06:40,450
there will be a reading with instructions on how to download install and

8
00:06:40,450 --> 00:06:44,050
run gephi on your native hardware instead of in the virtual machine 

9
00:06:45,960 --> 00:06:49,693
once you have gephi started let import the data into gephi 

10
00:06:49,693 --> 00:06:56,908
we will go to file import spreadsheet and in the csv dialog 

11
00:06:56,908 --> 00:07:02,338
we will click the button with dot dot dot 

12
00:07:02,338 --> 00:07:07,679
we will choose diseasegraph csv and click open 

13
00:07:11,336 --> 00:07:16,420
make sure that as table says edges table click next 

14
00:07:19,570 --> 00:07:23,535
and make sure create missing nodes is checked 

15
00:07:23,535 --> 00:07:27,215
we will click finish to import the csv file as a graph 

16
00:07:27,215 --> 00:07:29,630
gephi now shows the graph in the center pane 

17
00:07:31,270 --> 00:07:33,930
the little black circles are the nodes of the graph and

18
00:07:33,930 --> 00:07:36,500
the lines between them are the edges 

19
00:07:36,500 --> 00:07:42,707
in the top right we can see that there are 777 nodes and 998 edges 

20
00:07:42,707 --> 00:07:46,380
next let perform some statistical operations on this graph 

21
00:07:47,510 --> 00:07:52,145
in the statistics pane we can see average degree 

22
00:07:52,145 --> 00:07:54,980
let compute the average degree of the graph by clicking on run 

23
00:07:59,320 --> 00:08:03,260
this says that the average degree is 2 569 

24
00:08:03,260 --> 00:08:08,693
let close this let compute the connected

25
00:08:08,693 --> 00:08:12,583
components we will click on run 

26
00:08:12,583 --> 00:08:15,400
we will leave this as a directed since the graph is directed 

27
00:08:17,140 --> 00:08:19,930
click ok it says that there

28
00:08:19,930 --> 00:08:24,940
are 5 weakly connected components and 761 strongly connected components 

29
00:08:26,440 --> 00:08:27,400
let close this 

30
00:08:28,470 --> 00:08:31,710
next let run some different layout algorithms over the graph 

31
00:08:33,240 --> 00:08:35,760
the bottom left we will go to choose layout 

32
00:08:37,550 --> 00:08:42,019
we will choose force atlas and click run 

33
00:09:00,345 --> 00:09:02,080
click stop to stop the layout 

34
00:09:03,655 --> 00:09:08,200
we can see that gephi has grouped strongly connected components together

35
00:09:08,200 --> 00:09:09,320
in different clusters 

36
00:09:09,320 --> 00:09:11,920
we can also see that they are parts of the graph that are not connected 

37
00:09:13,440 --> 00:09:14,830
let run a different layout algorithm 

38
00:09:16,080 --> 00:09:21,395
the combo box choose fruchterman reingold click run 

39
00:09:28,234 --> 00:09:33,125
after it runs for a few seconds click stop then click the magnifying glass 

40
00:09:33,125 --> 00:09:35,680
center on graph to see the whole graph 

41
00:09:38,320 --> 00:09:41,700
in this layout all the nodes appear to be equally spaced 

42
00:09:41,700 --> 00:09:44,537
but we can also see the nodes with many edges 

1
00:09:45,350 --> 00:09:48,350
in this hands on activity we will be working with lucene 

2
00:09:48,350 --> 00:09:51,310
a search engine that uses a vector space model to index data 

3
00:09:52,430 --> 00:09:54,580
first we will open a terminal window and

4
00:09:54,580 --> 00:09:58,200
change into the directory containing the data and scripts 

5
00:09:58,200 --> 00:10:02,700
next we will index some text documents and query terms in lucene 

6
00:10:03,960 --> 00:10:06,538
after that we will query using weighted terms or

7
00:10:06,538 --> 00:10:09,810
boosting to see how this changes the rankings 

8
00:10:09,810 --> 00:10:14,045
finally we will show the term frequency - inverse document frequency or

9
00:10:14,045 --> 00:10:16,988
tf - idf in terms 

10
00:10:16,988 --> 00:10:19,300
let begin 

11
00:10:19,300 --> 00:10:23,139
first let open a terminal window by clicking on the terminal icon at the top

12
00:10:23,139 --> 00:10:23,953
of the tool bar 

13
00:10:27,413 --> 00:10:31,090
next let cd into the directory containing the scripts and data 

14
00:10:31,090 --> 00:10:32,679
we will run cd

15
00:10:32,679 --> 00:10:42,560
downloads big - data - 2 vector 

16
00:10:42,560 --> 00:10:46,201
we will run ls to see the scripts 

17
00:10:50,543 --> 00:10:53,670
the data directory contains three text files 

18
00:10:53,670 --> 00:10:57,310
each of these text files contains news data about elections 

19
00:10:59,730 --> 00:11:05,497
let index these files by running runlucenequery sh data 

20
00:11:12,974 --> 00:11:19,740
next let query lucene for some terms in these text documents and see the rankings 

21
00:11:19,740 --> 00:11:21,970
let query for the term voters 

22
00:11:27,242 --> 00:11:31,973
you can see the rankings and scores that news1 csv ranked first 

23
00:11:31,973 --> 00:11:36,900
news2 csv was the second ranking and the third was news3 csv 

24
00:11:36,900 --> 00:11:40,359
let query for delegates 

25
00:11:40,359 --> 00:11:45,324
for this term we see that news2 csv

26
00:11:45,324 --> 00:11:50,288
was first and news1 was second and

27
00:11:50,288 --> 00:11:55,260
news3 did not contain the term at all 

28
00:11:57,710 --> 00:12:01,275
now let query for both terms voters and delegates 

29
00:12:01,275 --> 00:12:08,935
in this result we see that news2 was ranked first 

30
00:12:08,935 --> 00:12:13,566
news1 was ranked second and

31
00:12:13,566 --> 00:12:17,500
news3 was ranked third 

32
00:12:19,230 --> 00:12:21,000
now lets use query term waiting or

33
00:12:21,000 --> 00:12:24,920
boosting to increase the relevance of voters 

34
00:12:24,920 --> 00:12:29,900
i can do this by ensuring voters carat 5 delegates 

35
00:12:29,900 --> 00:12:37,524
the carat 5 notation is a syntax for

36
00:12:37,524 --> 00:12:42,360
lucene for boosting 

37
00:12:43,890 --> 00:12:49,990
when we run this we see that now news1 is ranked first 

38
00:12:49,990 --> 00:12:54,170
news2 is ranked second and news3 is ranked third 

39
00:12:54,170 --> 00:12:58,500
notice this is different from the original query with voters and delegates 

40
00:12:58,500 --> 00:13:02,100
where news2 is ranked first and news1 was ranked second 

41
00:13:04,210 --> 00:13:07,890
now let look at the term frequency inverse document frequency or tf - idf 

42
00:13:07,890 --> 00:13:12,465
we will enter q to quit this and

43
00:13:12,465 --> 00:13:17,960
we will run lucene tf - idf sh data 

44
00:13:29,380 --> 00:13:35,000
let look at the tf - idf for voters 

45
00:13:35,000 --> 00:13:37,320
you can see that it ranked number 1 for news1 

46
00:13:38,370 --> 00:13:42,160
second news 2 and news 3 is last 

47
00:13:42,160 --> 00:13:43,170
lets try delegates 

48
00:13:49,110 --> 00:13:53,040
here we see that news 2 had a higher score than news 1 

49
00:13:53,040 --> 00:13:56,850
and news 3 is not listed because news 3 does not contain this term 

50
00:13:57,870 --> 00:13:58,590
hit q to quit 

1
00:13:58,800 --> 00:14:03,529
so the next category of data we discuss has the form of graphs or networks 

2
00:14:03,529 --> 00:14:06,749
the most obvious example being social networks 

3
00:14:06,749 --> 00:14:08,782
now speaking of social networks 

4
00:14:08,782 --> 00:14:13,760
tim libzek created a social network from the lord of the rings trilogy 

5
00:14:13,760 --> 00:14:16,545
this graph represents the characters allegiances 

6
00:14:16,545 --> 00:14:18,744
that is who is faithful to whom in the books 

7
00:14:18,744 --> 00:14:22,670
so the nodes are characters and other entities like cities and

8
00:14:22,670 --> 00:14:27,070
the edges connecting pairs of nodes represent allegiances 

9
00:14:27,070 --> 00:14:33,480
so after this video you will be able to identify graph data in practical problems

10
00:14:33,480 --> 00:14:39,150
and describe path neighborhood and connectivity operations in graphs 

11
00:14:39,150 --> 00:14:42,860
but this specialization includes a separate course in graph analytics

12
00:14:42,860 --> 00:14:46,470
that provides a much more detailed treatment on the subject 

13
00:14:46,470 --> 00:14:49,810
now what distinguishes a graph from other data models

14
00:14:49,810 --> 00:14:53,400
is that it bears two kinds of information 

15
00:14:53,400 --> 00:14:58,080
one properties and attributes of entities and relationships and

16
00:14:58,080 --> 00:15:02,130
two the connectivity structure that constitutes the network itself 

17
00:15:03,480 --> 00:15:06,260
one way to look at this data is shown in the figure 

18
00:15:06,260 --> 00:15:08,070
borrowed from the apache spark system 

19
00:15:09,130 --> 00:15:10,360
in this representation 

20
00:15:10,360 --> 00:15:14,390
the graph on the left is represented by two tables on the right 

21
00:15:14,390 --> 00:15:20,095
the vertex or node table gives ids to nodes and lists their properties 

22
00:15:21,170 --> 00:15:23,720
the edge table has two parts 

23
00:15:23,720 --> 00:15:27,210
the colored part represents the properties of the edge 

24
00:15:27,210 --> 00:15:31,600
whereas the white part contains just the direction of the arrows in the network 

25
00:15:31,600 --> 00:15:37,280
thus since there is a directed edge going from node 3 to node 7 

26
00:15:37,280 --> 00:15:41,830
there is a tupple 3 7 in that part of the edge table 

27
00:15:41,830 --> 00:15:46,540
now this form of the graph model is called the property graph model 

28
00:15:46,540 --> 00:15:50,760
which we will see many times in this course and in the specialization 

29
00:15:50,760 --> 00:15:55,740
now representing connectivity information gives graph data a new kind of

30
00:15:55,740 --> 00:15:59,910
computing ability that different from other data models we have seen so far 

31
00:16:01,090 --> 00:16:05,450
even without looking at the properties of the nodes and edges one can get very

32
00:16:05,450 --> 00:16:10,640
interesting information just by analyzing or querying this connectivity structure 

33
00:16:11,800 --> 00:16:16,990
consider a social network with three types of nodes user city and

34
00:16:16,990 --> 00:16:22,250
restaurant and three types of edges friend likes and lives in 

35
00:16:23,300 --> 00:16:26,110
the leftmost node ag represents me 

36
00:16:26,110 --> 00:16:29,930
and i am interested in finding a good italian restaurant in new york

37
00:16:29,930 --> 00:16:34,360
that my friends or their friends who also live in new york like 

38
00:16:34,360 --> 00:16:39,765
i shall possibly choose it3 because it has the highest number of

39
00:16:39,765 --> 00:16:45,700
like edges coming into it from people who have a lives in edge to new york 

40
00:16:45,700 --> 00:16:47,008
and at the same time 

41
00:16:47,008 --> 00:16:50,869
can be reached by following the friend edges going out from me 

42
00:16:50,869 --> 00:16:55,306
now this shows a very important class of operations and ground data namely

43
00:16:55,306 --> 00:16:59,830
traversal that involves edge following based on some sort of conditions 

44
00:17:01,090 --> 00:17:05,660
a number of path operations required some sort of optimization 

45
00:17:05,660 --> 00:17:10,885
the simplest among these is the well known shortest path query which is applied to

46
00:17:10,885 --> 00:17:15,750
node networks to find the best route from a source location to a target location 

47
00:17:15,750 --> 00:17:19,215
the second class of optimization operations is required to find

48
00:17:19,215 --> 00:17:24,050
an optimal path that must include some user specified nodes for

49
00:17:24,050 --> 00:17:28,420
the operation has to determine the order in which the nodes once we visited 

50
00:17:28,420 --> 00:17:30,795
the classical application is a trip planner 

51
00:17:30,795 --> 00:17:34,610
where the user specifies the cities she wishes to visit and

52
00:17:34,610 --> 00:17:38,740
the operation will optimize the criterion like the total distance covered 

53
00:17:38,740 --> 00:17:43,260
the third category is a case where the system must find the best possible

54
00:17:43,260 --> 00:17:45,690
path in the network given two or

55
00:17:45,690 --> 00:17:50,070
more optimization criteria which cannot be satisfied simultaneously 

56
00:17:50,070 --> 00:17:54,350
for example if i want to travel from my house to the airport

57
00:17:54,350 --> 00:17:59,150
using the shortest distance but also minimizing the amount of highway travel 

58
00:17:59,150 --> 00:18:01,710
the algorithm must find a best compromise 

59
00:18:01,710 --> 00:18:05,510
this is called a pareto - optimality problem on graphs 

60
00:18:05,510 --> 00:18:11,120
the neighborhood of a node n in a graph is a set of edges directly connected to it 

61
00:18:11,120 --> 00:18:16,360
a k neighborhood of n is a collection of edges between nodes that are 

62
00:18:16,360 --> 00:18:19,160
at most k steps away from n 

63
00:18:19,160 --> 00:18:22,710
so going back to our mini social network graph bob jill 

64
00:18:22,710 --> 00:18:27,630
and sarah are the first neighbors of ag while max tim and

65
00:18:27,630 --> 00:18:32,570
pam belong to the second neighborhood and not the first neighborhood of ag 

66
00:18:32,570 --> 00:18:35,910
finally jen is a third level neighbor 

67
00:18:36,910 --> 00:18:41,790
an important class of analysis to perform with neighborhoods is community finding 

68
00:18:41,790 --> 00:18:45,740
a community and a social network can be a very close group of friends 

69
00:18:45,740 --> 00:18:49,340
so the graph shown in this figure has four communities 

70
00:18:49,340 --> 00:18:54,327
one can see in the figure that each community has a higher density of edges

71
00:18:54,327 --> 00:19:00,480
within the community and a lower density across two different communities 

72
00:19:00,480 --> 00:19:03,440
finding densely connected parts of a graph

73
00:19:03,440 --> 00:19:07,860
helps identify neighborhoods that can be recognized as communities 

74
00:19:07,860 --> 00:19:13,032
a more complex class of operations include finding the best possible clusters 

75
00:19:13,032 --> 00:19:15,620
which is another name for communities in a graph so

76
00:19:15,620 --> 00:19:20,030
that any other grouping of nodes into communities will be less effective 

77
00:19:20,030 --> 00:19:25,430
now as graphs become bigger and denser these methods become harder to compute 

78
00:19:25,430 --> 00:19:30,240
thus neighborhood - based optimization operation present

79
00:19:30,240 --> 00:19:32,520
significant scalability challenges 

80
00:19:32,520 --> 00:19:36,650
if we inspect the neighborhood of every node in a graph sometimes 

81
00:19:36,650 --> 00:19:40,650
we will find neighborhoods that are different from all others 

82
00:19:40,650 --> 00:19:43,390
these neighborhoods are called anomalous 

83
00:19:43,390 --> 00:19:47,720
consider the following four graphs and on the central red node 

84
00:19:47,720 --> 00:19:52,177
the first graph is odd because it almost perfectly star shaped 

85
00:19:52,177 --> 00:19:56,751
that is the nodes that the red node is connected to are almost unconnected

86
00:19:56,751 --> 00:19:58,137
amongst themselves 

87
00:19:58,137 --> 00:20:02,000
that really odd because it does not happen in reality much 

88
00:20:02,000 --> 00:20:03,620
so it an anomalous node 

89
00:20:03,620 --> 00:20:07,630
the second figure shows a neighborhood to which

90
00:20:07,630 --> 00:20:11,770
a significantly large number of neighbors has connected amongst themselves 

91
00:20:11,770 --> 00:20:16,860
this makes the graph very cliquish where a clique refers to a neighborhood

92
00:20:16,860 --> 00:20:21,280
where each node is connected to all other neighborhood nodes in the neighborhood 

93
00:20:21,280 --> 00:20:23,770
the third figure shows a neighborhood 

94
00:20:23,770 --> 00:20:29,040
where some edges have an unusually heavy weight compared to the others 

95
00:20:29,040 --> 00:20:32,578
the fourth figure shows a special case of the third 

96
00:20:32,578 --> 00:20:36,930
where one edge is predominantly high rate compared to all the other edges 

97
00:20:38,120 --> 00:20:40,890
connectedness is a fundamental property of a graph 

98
00:20:41,960 --> 00:20:43,390
in a connected graph 

99
00:20:43,390 --> 00:20:47,570
each node is reachable from every other node through some path 

100
00:20:47,570 --> 00:20:52,750
if a graph is not connected but there are subgraphs of it which are connected 

101
00:20:52,750 --> 00:20:56,745
then these subgraphs are called connected components of the original graph 

102
00:20:56,745 --> 00:20:59,799
in the figure on the right there are four connected components 

103
00:20:59,799 --> 00:21:03,805
a search gradient like finding optimal paths

104
00:21:03,805 --> 00:21:07,075
should be performed only within each component and not across them 

105
00:21:08,215 --> 00:21:12,325
for large graphs there are several new parallelized techniques for

106
00:21:12,325 --> 00:21:13,835
the detection of connected components 

107
00:21:15,070 --> 00:21:17,680
we will discuss a map reduce based technique for

108
00:21:17,680 --> 00:21:19,830
connected components in a later course 

1
00:21:22,200 --> 00:21:24,569
we have discussed quite a few data models but

2
00:21:24,569 --> 00:21:27,669
there are many other data models that have been developed for

3
00:21:27,669 --> 00:21:32,500
various purposes and we really cannot cover all of them in a single course 

4
00:21:32,500 --> 00:21:36,360
we will end these lectures on data models with an example that may give you

5
00:21:36,360 --> 00:21:40,560
an insight into a class of objects that define in many different applications 

6
00:21:41,850 --> 00:21:46,972
so after this video you will be able to describe how arrays can serve

7
00:21:46,972 --> 00:21:52,278
as a data model explain why images can be modeled as vector arrays 

8
00:21:52,278 --> 00:21:56,689
specify a set of operations on scalar and vector arrays 

9
00:21:58,969 --> 00:22:01,690
now we have all seen the arrays 

10
00:22:01,690 --> 00:22:05,390
in the simplest case an array is a matrix like this 

11
00:22:05,390 --> 00:22:07,380
let call this array a 

12
00:22:09,960 --> 00:22:14,140
the top row in yellow gives the column numbers and

13
00:22:14,140 --> 00:22:16,890
the left column also in yellow gives the row numbers 

14
00:22:17,970 --> 00:22:22,263
when we need to refer to a value of the array as a 3 2 

15
00:22:22,263 --> 00:22:26,530
we mean the value of the cell in row 3 and column 2 

16
00:22:26,530 --> 00:22:31,270
this is called indexed structure where 3 and

17
00:22:31,270 --> 00:22:37,070
2 are the row and column indices that are necessary to get the value of a data item 

18
00:22:38,430 --> 00:22:40,830
the area has two dimensions 

19
00:22:40,830 --> 00:22:43,050
so hence there are two indexes 

20
00:22:43,050 --> 00:22:46,650
if these were a three dimensional array we would have three indexes 

21
00:22:47,860 --> 00:22:48,390
now earlier 

22
00:22:48,390 --> 00:22:53,360
we have seen that we can represent the two dimensional array as a three column table 

23
00:22:53,360 --> 00:22:56,790
one column for the row index one column for the column index and

24
00:22:56,790 --> 00:22:57,849
the last column for the value 

25
00:22:59,310 --> 00:23:04,310
thus a k dimensional array can be represented as a relation with k

26
00:23:04,310 --> 00:23:05,090
plus one columns 

27
00:23:06,690 --> 00:23:10,100
the number of tuples in this representation will be the product of

28
00:23:10,100 --> 00:23:14,220
the size of the first dimension times the size of the second dimension and so forth 

29
00:23:15,670 --> 00:23:20,000
then in this case the size is five in each dimension 

30
00:23:20,000 --> 00:23:27,070
so there are 25 c column tuples in a relation representing the array 

31
00:23:27,070 --> 00:23:30,890
a more useful situation occurs when the cells of an array have

32
00:23:30,890 --> 00:23:32,490
a vectors as values 

33
00:23:33,620 --> 00:23:38,610
as you can see in the 2d vector array here each cell has a three vector 

34
00:23:38,610 --> 00:23:40,340
that is a vector with three elements 

35
00:23:41,400 --> 00:23:44,150
therefore if we want to receive a cell value and

36
00:23:44,150 --> 00:23:46,930
treat it like before we will get back the whole vector 

37
00:23:48,220 --> 00:23:50,680
now this type of data should look familiar to you 

38
00:23:50,680 --> 00:23:54,840
because images often have a red green and blue channels per pixel 

39
00:23:56,030 --> 00:23:57,050
in other words 

40
00:23:57,050 --> 00:24:02,780
images of vector valued arrays where each array cell has a three color vector 

41
00:24:04,270 --> 00:24:08,000
we can also think of the array model in the context of satellite images 

42
00:24:08,000 --> 00:24:11,850
where there are many more channels depending on the range of wavelengths

43
00:24:11,850 --> 00:24:13,100
each channel catches 

44
00:24:14,360 --> 00:24:17,710
let us consider the operations on arrays of vectors 

45
00:24:17,710 --> 00:24:22,430
because it is a combination of two models one can create different combinations of

46
00:24:22,430 --> 00:24:27,140
array operations vector operations and composite operations 

47
00:24:27,140 --> 00:24:27,650
here are some 

48
00:24:28,810 --> 00:24:31,580
the dimension of the array here the first operation is two 

49
00:24:32,940 --> 00:24:35,289
if we pick up any dimension say one 

50
00:24:35,289 --> 00:24:39,741
the size of it is also two because they are two elements 

51
00:24:39,741 --> 00:24:42,260
zero and one in each dimension 

52
00:24:42,260 --> 00:24:50,259
as we saw before the value of the cell 1 1 is a vector 16 301 74 

53
00:24:50,259 --> 00:24:56,979
while the value of a11 component 2 is 74 

54
00:24:56,979 --> 00:25:01,279
the length of the vector is a square root of the sum of the elements of the vector 

55
00:25:01,279 --> 00:25:07,999
so length of a11 would come to 310 375 

56
00:25:07,999 --> 00:25:09,623
the distance function can be so

57
00:25:09,623 --> 00:25:13,184
simple like the euclidean distance function between two vectors or

58
00:25:13,184 --> 00:25:16,820
the cosine of an angle between them as we saw in the previous lecture 

59
00:25:18,080 --> 00:25:21,580
but it can also be something more complex based on the needs of the application 

60
00:25:23,320 --> 00:25:27,390
obviously one can also perform operations like selection over indices so

61
00:25:27,390 --> 00:25:32,170
we can ask which cells had the zero value greater than 25 

62
00:25:32,170 --> 00:25:35,060
giving as the result zero one and one zero 

63
00:25:36,190 --> 00:25:39,050
you will experience some of these operations in your hands on session 

