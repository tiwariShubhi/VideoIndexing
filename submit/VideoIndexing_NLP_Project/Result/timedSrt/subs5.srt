1
00:00:02,826 --> 00:00:04,162
data science 

2
00:00:04,162 --> 00:00:06,298
getting value out of big data 

3
00:00:26,242 --> 00:00:31,860
we have all heard data science turned data into insights or even actions 

4
00:00:31,860 --> 00:00:34,250
but what does that really mean 

5
00:00:34,250 --> 00:00:36,990
data science can be thought of as a basis for

6
00:00:36,990 --> 00:00:41,440
empirical research where data is used to induce information for observations 

7
00:00:42,490 --> 00:00:46,310
these observations are mainly data in our case 

8
00:00:46,310 --> 00:00:49,985
big data related to a business or scientific case 

9
00:00:49,985 --> 00:00:54,430
insight is a term

10
00:00:54,430 --> 00:00:59,050
we use to refer to the data products of data science 

11
00:00:59,050 --> 00:01:03,523
it is extracted from a diverse amount of data through a combination of

12
00:01:03,523 --> 00:01:07,686
exploratory data analysis and modeling 

13
00:01:07,686 --> 00:01:13,440
the questions are sometimes more specific and sometimes it requires

14
00:01:13,440 --> 00:01:18,170
looking at the data and patterns in it to come up with the specific question 

15
00:01:20,420 --> 00:01:25,550
another important point to recognize is that data science is not static 

16
00:01:25,550 --> 00:01:28,020
it is not one time analysis 

17
00:01:28,020 --> 00:01:32,920
it involves a process where models generated to lead to insights

18
00:01:32,920 --> 00:01:37,630
are constantly improved through further empirical evidence or simply data 

19
00:01:39,640 --> 00:01:45,060
for example a book retailer like amazon com can constantly improve

20
00:01:45,060 --> 00:01:49,560
the model of a customer book preferences using the customer demographic 

21
00:01:50,580 --> 00:01:55,630
his or her previous purchases and the book reviews of the customer 

22
00:01:57,920 --> 00:02:02,350
the book retailer can also uses information to predict

23
00:02:02,350 --> 00:02:07,219
which customers are likely to like any book 

24
00:02:07,219 --> 00:02:11,560
and take action to market the book to those customers 

25
00:02:13,840 --> 00:02:16,955
this is where we see insights being turned into action 

26
00:02:19,834 --> 00:02:24,718
as we have seen in the book marketing example using data science and analysis

27
00:02:24,718 --> 00:02:29,240
of the past and current information data science generates actions 

28
00:02:30,350 --> 00:02:33,200
this is not just an analysis of the past but

29
00:02:33,200 --> 00:02:36,980
rather generation of actionable information for the future 

30
00:02:38,370 --> 00:02:42,070
this is what we can call a prediction like the weather forecast 

31
00:02:43,680 --> 00:02:49,140
when you decide what to wear for the day based on the forecast of the day 

32
00:02:49,140 --> 00:02:53,880
you are taking action based on insight delivered to you 

33
00:02:53,880 --> 00:02:58,110
just like this business leaders and decision makers

34
00:02:58,110 --> 00:03:02,170
take action based on the evidence provided by their data science teams 

35
00:03:04,140 --> 00:03:05,870
we set data science teams 

36
00:03:06,980 --> 00:03:11,290
this comes from the breadth of information and skill that it takes to make it happen 

37
00:03:12,590 --> 00:03:17,010
you have probably seen diagrams like this one that describe data science 

38
00:03:18,090 --> 00:03:22,220
data science happens at the intersection of computer science 

39
00:03:22,220 --> 00:03:24,820
mathematics and business expertise 

40
00:03:27,170 --> 00:03:29,750
if we zoom deeper into this diagram and

41
00:03:29,750 --> 00:03:35,030
open the sets of expertise we will see a variation of this figure 

42
00:03:36,410 --> 00:03:42,690
even at this level all of these circles require deeper knowledge and skills

43
00:03:42,690 --> 00:03:48,140
in areas like domain expertise data engineering statistics and computing 

44
00:03:51,185 --> 00:03:55,790
an even deeper analysis of these skills will lead you to skills like

45
00:03:55,790 --> 00:04:00,110
machine learning statistical modeling relational algebra 

46
00:04:00,110 --> 00:04:04,640
business passion problem solving and data visualization 

47
00:04:04,640 --> 00:04:07,582
that a lot of skills to have for a single person 

48
00:04:12,922 --> 00:04:17,862
these wide range of skills and definitions of data scientists having them

49
00:04:17,862 --> 00:04:21,855
all led to discussions like are data scientists unicorns 

50
00:04:22,895 --> 00:04:24,115
meaning they do not exist 

51
00:04:25,375 --> 00:04:29,795
there are data science experts who have expertise in more than one of these

52
00:04:29,795 --> 00:04:31,380
skills for sure 

53
00:04:31,380 --> 00:04:33,795
but they are relatively rare and

54
00:04:33,795 --> 00:04:37,470
still would probably need help from an expert on some of these areas 

55
00:04:38,730 --> 00:04:44,550
so in reality data scientists are teams of people who act like one 

56
00:04:45,560 --> 00:04:48,860
they are passionate about the story and the meaning behind data 

57
00:04:50,310 --> 00:04:54,700
they understand they problem they are trying to solve and

58
00:04:54,700 --> 00:04:57,910
aim to find the right analytical methods to solve this problem 

59
00:04:57,910 --> 00:05:04,350
and they all have an interest in engineering solutions to solve problems 

60
00:05:06,230 --> 00:05:11,360
they also have curiosity about each others work and have communication

61
00:05:11,360 --> 00:05:15,940
skills to interact with the team and present their ideas and results to others 

62
00:05:17,900 --> 00:05:23,460
as a summary a data science team often comes together to analyze situations 

63
00:05:23,460 --> 00:05:29,370
business or scientific cases which none of the individuals can solve on their own 

64
00:05:29,370 --> 00:05:32,090
there are lots of moving parts to the solution 

65
00:05:32,090 --> 00:05:36,370
but in the end all these parts should come together to provide

66
00:05:36,370 --> 00:05:39,055
actionable insight based on big data 

67
00:05:41,807 --> 00:05:46,732
being able to use evidence - based insight in business decisions is more

68
00:05:46,732 --> 00:05:48,517
important now than ever 

69
00:05:48,517 --> 00:05:52,594
data scientists have a combination of technical business and

70
00:05:52,594 --> 00:05:54,716
soft skills to make this happen 

1
00:05:54,930 --> 00:05:57,310
getting started characteristics of big data 

2
00:06:15,143 --> 00:06:19,786
by now you have seen that big data is a blanket term that is used to refer to any

3
00:06:19,786 --> 00:06:24,140
collection of data so large and complex that it exceeds the processing

4
00:06:24,140 --> 00:06:29,530
capability of conventional data management systems and techniques 

5
00:06:29,530 --> 00:06:33,040
the applications of big data are endless 

6
00:06:33,040 --> 00:06:38,000
every part of business and society are changing in front our eyes due to that

7
00:06:38,000 --> 00:06:42,870
fact that we now have so much more data and the ability for analyzing 

8
00:06:44,220 --> 00:06:46,120
but how can we characterize big data 

9
00:06:47,310 --> 00:06:50,630
you can say i know it when i see it 

10
00:06:50,630 --> 00:06:53,440
but there are easier ways to do it 

11
00:06:53,440 --> 00:06:57,030
big data is commonly characterized using a number of v 

12
00:06:58,040 --> 00:07:03,720
the first three are volume velocity and variety 

13
00:07:03,720 --> 00:07:09,580
volume refers to the vast amounts of data that is generated every second 

14
00:07:09,580 --> 00:07:13,470
minutes hour and day in our digitized world 

15
00:07:15,130 --> 00:07:21,480
variety refers to the ever increasing different forms that data can come in

16
00:07:21,480 --> 00:07:26,090
such as text images voice and geospatial data 

17
00:07:28,100 --> 00:07:33,440
velocity refers to the speed at which data is being generated and

18
00:07:33,440 --> 00:07:36,980
the pace at which data moves from one point to the next 

19
00:07:38,290 --> 00:07:40,510
volume variety and

20
00:07:40,510 --> 00:07:46,430
velocity are the three main dimensions that characterize big data 

21
00:07:46,430 --> 00:07:47,790
and describe its challenges 

22
00:07:49,180 --> 00:07:54,410
we have huge amounts of data in different formats and

23
00:07:54,410 --> 00:07:58,160
varying quality which must be processed quickly 

24
00:07:59,200 --> 00:08:02,470
more vs have been introduced to the big data community

25
00:08:02,470 --> 00:08:06,070
as we discover new challenges and ways to define big data 

26
00:08:07,310 --> 00:08:11,830
veracity and valence are two of these additional v we

27
00:08:11,830 --> 00:08:15,720
will pay special attention to as a part of this specialization 

28
00:08:16,860 --> 00:08:23,360
veracity refers to the biases noise and abnormality in data 

29
00:08:23,360 --> 00:08:28,680
or better yet it refers to the often unmeasurable uncertainties and

30
00:08:28,680 --> 00:08:32,540
truthfulness and trustworthiness of data 

31
00:08:32,540 --> 00:08:38,150
valence refers to the connectedness of big data in the form of graphs 

32
00:08:38,150 --> 00:08:39,180
just like atoms 

33
00:08:40,220 --> 00:08:46,180
moreover we must be sure to never forget our sixth v value 

34
00:08:47,220 --> 00:08:49,890
how do big data benefit you and your organization 

35
00:08:51,560 --> 00:08:53,640
without a clear strategy and

36
00:08:53,640 --> 00:08:57,700
an objective with the value they are getting from big data 

37
00:08:57,700 --> 00:09:01,800
it is easy to imagine that organizations will be sidetracked by all these

38
00:09:01,800 --> 00:09:06,630
challenges of big data and not be able to turn them into opportunities 

39
00:09:07,990 --> 00:09:11,993
now let start looking into the first five of these v in detail 

1
00:09:13,060 --> 00:09:15,400
how does data science happen 

2
00:09:15,400 --> 00:09:16,850
five p of data science 

3
00:09:18,810 --> 00:09:24,220
now that we identified what data science is and how companies can strategize around

4
00:09:24,220 --> 00:09:29,660
big data to start building a purpose let come back to using data science

5
00:09:29,660 --> 00:09:34,870
to get value out of big data around the purpose or questions they defined 

6
00:09:52,348 --> 00:09:54,591
our experience with building and

7
00:09:54,591 --> 00:10:00,040
observing successful data science projects led to a method around the craft with

8
00:10:00,040 --> 00:10:06,255
five distinct components that can be defined as components of data science 

9
00:10:06,255 --> 00:10:10,859
here we define data science as a multi - disciplinary

10
00:10:10,859 --> 00:10:14,972
craft that combines people teaming up around

11
00:10:14,972 --> 00:10:19,910
application - specific purpose that can be achieved through a process 

12
00:10:21,420 --> 00:10:25,100
big data computing platforms and programmability 

13
00:10:28,260 --> 00:10:33,460
all of these should lead to products where the focus really

14
00:10:33,460 --> 00:10:37,760
is on the questions or purpose that are defined by your big data strategy ideas 

15
00:10:39,960 --> 00:10:44,540
there are many technology data and analytical research and

16
00:10:44,540 --> 00:10:48,110
development related activities around the questions 

17
00:10:48,110 --> 00:10:52,490
but in the end everything we do in this phase is to reach to that

18
00:10:52,490 --> 00:10:54,820
final product based on our purposes 

19
00:10:55,950 --> 00:10:58,830
so it makes sense to start with it and

20
00:10:58,830 --> 00:11:01,860
build a process around how we make this product happen 

21
00:11:03,740 --> 00:11:06,520
remember the wild fire prediction project i described 

22
00:11:07,830 --> 00:11:12,070
one of the products we described there was the rate of spread and

23
00:11:12,070 --> 00:11:13,870
direction of an ongoing fire 

24
00:11:15,460 --> 00:11:17,780
we have identified questions and

25
00:11:17,780 --> 00:11:22,060
the process that led us to the product in the end to solve it 

26
00:11:23,960 --> 00:11:28,280
we brought together experts around the table for fire modeling 

27
00:11:28,280 --> 00:11:32,620
data management time series analysis scalable computing 

28
00:11:32,620 --> 00:11:36,060
geographical information systems and emergency response 

29
00:11:39,050 --> 00:11:42,770
i asked them let not dive into the techniques yet 

30
00:11:42,770 --> 00:11:44,080
what is the problem at large 

31
00:11:45,100 --> 00:11:51,380
how do we see ourselves solving it 

32
00:11:51,380 --> 00:11:55,780
a typical conversation around the process starts with this question 

33
00:11:57,230 --> 00:12:02,600
then from then on drilling down to many areas of expertise 

34
00:12:02,600 --> 00:12:04,830
often we blur lines between the steps 

35
00:12:06,510 --> 00:12:11,280
my wildfire team would start listing things like we do not have an integrated

36
00:12:11,280 --> 00:12:16,400
system or we do not have real - time access to data programmatically 

37
00:12:16,400 --> 00:12:19,210
so we cannot analyze fires on the fly 

38
00:12:19,210 --> 00:12:23,950
or they can say i cannot integrate sensor data with satellite data 

39
00:12:25,370 --> 00:12:31,080
all of this leads me to challenges i can then use to define problems 

40
00:12:32,300 --> 00:12:36,560
there are many dimensions of data science to think about within this discussion 

41
00:12:37,570 --> 00:12:42,700
let start with the obvious ones people and purpose 

42
00:12:45,535 --> 00:12:50,800
people refers to a data science team or the projects stakeholders 

43
00:12:50,800 --> 00:12:53,700
as you know by now they are expert in data and

44
00:12:53,700 --> 00:12:58,028
analytics business computing science or big data management 

45
00:12:58,028 --> 00:13:04,154
like all the set of experts i listed in my wildfire scenario 

46
00:13:04,154 --> 00:13:10,560
the purpose refers to the challenge or set of challenges defined by your

47
00:13:10,560 --> 00:13:15,940
big data strategy like solving the question related to the rate of spread and

48
00:13:15,940 --> 00:13:19,170
direction of the fire perimeter in the wildfire case 

49
00:13:23,003 --> 00:13:28,099
since there a predefined team with a purpose a great place for

50
00:13:28,099 --> 00:13:33,190
this team to start with is a process they could iterate on 

51
00:13:33,190 --> 00:13:38,470
we can simply say people with purpose will define a process to collaborate and

52
00:13:38,470 --> 00:13:39,360
communicate around 

53
00:13:41,010 --> 00:13:44,500
the process is conceptual in the beginning and

54
00:13:44,500 --> 00:13:48,390
defines the set of steps an how everyone can contribute to it 

55
00:13:52,060 --> 00:13:54,230
there are many ways to look at the process 

56
00:13:55,890 --> 00:14:01,250
one way of looking at it is as two distinct activities 

57
00:14:01,250 --> 00:14:04,130
mainly big data engineering and

58
00:14:04,130 --> 00:14:08,500
big data analytics or computational big data science 

59
00:14:08,500 --> 00:14:12,910
as i like to call it as more than simple analytics is being performed here 

60
00:14:14,840 --> 00:14:21,180
a more detailed way of looking at the process reveals five distinct steps or

61
00:14:21,180 --> 00:14:26,214
activities of this data science process 

62
00:14:26,214 --> 00:14:32,018
namely acquire prepare 

63
00:14:32,018 --> 00:14:37,540
analyze report and act 

64
00:14:37,540 --> 00:14:40,190
we can simply say that data science happens

65
00:14:40,190 --> 00:14:42,950
at the boundary of all these steps 

66
00:14:42,950 --> 00:14:47,510
ideally this process should support experimental work and

67
00:14:47,510 --> 00:14:51,980
dynamic scalability on the big data and computing platforms 

68
00:14:55,220 --> 00:15:00,050
this five step process can be used in alternative ways in real life big data

69
00:15:00,050 --> 00:15:04,580
applications if we add the dependencies of different tools to each other 

70
00:15:05,930 --> 00:15:09,490
the influence of big data pushes for

71
00:15:09,490 --> 00:15:13,930
alternative scalability approaches at each step of the process 

72
00:15:15,010 --> 00:15:18,080
just like you would scale each step on its own 

73
00:15:18,080 --> 00:15:21,780
you can scale the whole process as a whole in the end 

74
00:15:25,613 --> 00:15:31,900
one can simply say all of these steps have reporting needs in different forms 

75
00:15:34,280 --> 00:15:41,290
or there is a need to draw all these activities as an iterating process 

76
00:15:41,290 --> 00:15:46,710
including build explore and scale for big data as steps 

77
00:15:49,550 --> 00:15:53,550
big data analysis needs alternative data management techniques and

78
00:15:53,550 --> 00:15:58,290
systems as well as analytical tools and methods 

79
00:16:00,160 --> 00:16:05,730
multiple modes of scalability is needed based on dynamic data and computing loads 

80
00:16:06,750 --> 00:16:10,800
in addition change in physical infrastructure 

81
00:16:10,800 --> 00:16:15,320
streaming data specific urgencies arising from special events

82
00:16:15,320 --> 00:16:18,780
can also require multiple modes of scalability 

83
00:16:20,230 --> 00:16:22,490
in this intro course for simplicity 

84
00:16:22,490 --> 00:16:28,470
we will refer to the process as a set of five sequential activities that iterate 

85
00:16:29,500 --> 00:16:34,550
however we will touch on scalability as needed in our example applications 

86
00:16:37,840 --> 00:16:41,190
as a part of building your big data process 

87
00:16:41,190 --> 00:16:45,230
it important to simply mention two other p 

88
00:16:45,230 --> 00:16:50,620
the first one is big data platforms like the ones in the hadoop framework 

89
00:16:50,620 --> 00:16:54,610
or other computing platforms to scale different steps 

90
00:16:54,610 --> 00:16:58,060
the scalability should be in the mind of all team members and

91
00:16:58,060 --> 00:17:00,030
get communicated as an expectation 

92
00:17:02,390 --> 00:17:06,970
in addition the scalable process should be programmable through

93
00:17:06,970 --> 00:17:12,830
utilization of reusable and reproducible programming interfaces

94
00:17:12,830 --> 00:17:17,308
to libraries like systems middleware analytical tools 

95
00:17:17,308 --> 00:17:21,395
visualization environments and end user reporting environments 

96
00:17:25,010 --> 00:17:27,980
thinking of big data applications as a process 

97
00:17:27,980 --> 00:17:33,150
including a set of activities that the team members can collaborate over 

98
00:17:33,150 --> 00:17:37,720
also helps to build metrics for accountability to be built into it 

99
00:17:37,720 --> 00:17:42,000
this way expectations on cost time 

100
00:17:42,000 --> 00:17:45,610
optimization of deliverables and time lines can be discussed

101
00:17:45,610 --> 00:17:50,210
between the the members starting with the beginning of the data science process 

102
00:17:53,325 --> 00:17:56,890
sometimes we may not be able to do this in one step 

103
00:17:58,550 --> 00:18:03,420
and joint explorations like statistical evaluations of intermediate results or

104
00:18:03,420 --> 00:18:06,060
accuracy of sample data sets become important 

105
00:18:08,150 --> 00:18:13,430
as a summary data science can be defined as a craft of using the five p 

106
00:18:13,430 --> 00:18:18,380
identified in this lecture leading to a sixth p the data product 

107
00:18:19,580 --> 00:18:23,870
having a process within the more business - driven ps like people and

108
00:18:23,870 --> 00:18:28,120
purpose and the more technically driven p like platforms and

109
00:18:28,120 --> 00:18:33,220
programmability leads to a streamlined approach that starts and

110
00:18:33,220 --> 00:18:37,630
ends with the product team accountability and collaboration in mind 

111
00:18:38,740 --> 00:18:42,311
data science process provides guidelines for

112
00:18:42,311 --> 00:18:47,621
implementing big data solution as it helps to organize efforts and

113
00:18:47,621 --> 00:18:54,137
ensures all critical steps taken conforms to pre - define and agreed upon metrics 

1
00:18:57,008 --> 00:18:59,098
step one acquiring data 

2
00:19:13,898 --> 00:19:18,420
the first step in the data science process is to acquire the data 

3
00:19:19,660 --> 00:19:23,860
you need to obtain the source material before analyzing or acting on it 

4
00:19:25,950 --> 00:19:31,690
the first step in acquiring data is to determine what data is available 

5
00:19:31,690 --> 00:19:35,680
leave no stone unturned when it comes to finding the right data sources 

6
00:19:36,770 --> 00:19:40,150
you want to identify suitable data related to your problem and

7
00:19:41,200 --> 00:19:45,540
make use of all data that is relevant to your problem for analysis 

8
00:19:46,720 --> 00:19:50,580
leaving out even a small amount of important data

9
00:19:50,580 --> 00:19:52,570
can lead to incorrect conclusions 

10
00:19:55,190 --> 00:19:58,460
data comes from many places local and

11
00:19:58,460 --> 00:20:03,280
remote in many varieties structured and un - structured 

12
00:20:03,280 --> 00:20:06,250
and with different velocities 

13
00:20:06,250 --> 00:20:11,780
there are many techniques and technologies to access these different types of data 

14
00:20:11,780 --> 00:20:13,420
let discuss a few examples 

15
00:20:15,360 --> 00:20:20,020
a lot of data exists in conventional relational databases 

16
00:20:20,020 --> 00:20:22,560
like structure big data from organizations 

17
00:20:23,600 --> 00:20:29,270
the tool of choice to access data from databases is structured query language or

18
00:20:29,270 --> 00:20:34,360
sql which is supported by all relational databases management systems 

19
00:20:35,680 --> 00:20:41,260
additionally most data base systems come with a graphical application

20
00:20:41,260 --> 00:20:46,250
environment that allows you to query and explore the data sets in the database 

21
00:20:48,990 --> 00:20:55,870
data can also exist in files such as text files and excel spreadsheets 

22
00:20:55,870 --> 00:20:59,638
scripting languages are generally used to get data from files 

23
00:20:59,638 --> 00:21:05,200
a scripting language is a high level programming language

24
00:21:05,200 --> 00:21:10,020
that can be either general purpose or specialized for specific functions 

25
00:21:11,960 --> 00:21:18,010
common scripting languages with support for processing files are java script 

26
00:21:18,010 --> 00:21:23,300
python php perl r and matlab and are many others 

27
00:21:25,780 --> 00:21:30,230
an increasingly popular way to get data is from websites 

28
00:21:30,230 --> 00:21:34,956
web pages are written using a set of standards approved by

29
00:21:34,956 --> 00:21:38,906
a world wide web consortium or shortly w3c 

30
00:21:38,906 --> 00:21:42,650
this includes a variety of formats and services 

31
00:21:43,920 --> 00:21:49,860
one common format is the extensible markup language or xml 

32
00:21:49,860 --> 00:21:54,640
which uses markup symbols or tabs to describe the contents on a webpage 

33
00:21:56,250 --> 00:22:01,920
many websites also host web services which produce program access to their data 

34
00:22:04,310 --> 00:22:07,110
there are several types of web services 

35
00:22:07,110 --> 00:22:10,990
the most popular is rest because it so easy to use 

36
00:22:12,080 --> 00:22:16,280
rest stand for representational state transfer 

37
00:22:16,280 --> 00:22:20,640
and it is an approach to implementing web services with performance 

38
00:22:20,640 --> 00:22:23,200
scalability and maintainability in mind 

39
00:22:24,940 --> 00:22:28,400
web socket services are also becoming more popular

40
00:22:28,400 --> 00:22:31,440
since they allow real time modifications from web sites 

41
00:22:34,040 --> 00:22:38,840
nosql storage systems are increasingly used to manage a variety of data

42
00:22:38,840 --> 00:22:39,770
types in big data 

43
00:22:40,950 --> 00:22:44,710
these data stores are databases that do not represent data

44
00:22:44,710 --> 00:22:49,580
in a table format with columns and rows as with conventional relational databases 

45
00:22:50,910 --> 00:22:56,180
examples of these data stores include cassandra mongodb and hbase 

46
00:22:57,815 --> 00:23:02,350
nosql data stores provide apis to allow users to access data 

47
00:23:03,460 --> 00:23:08,940
these apis can be used directly or in an application that needs to access the data 

48
00:23:10,660 --> 00:23:15,240
additionally most nosql systems provide data access

49
00:23:15,240 --> 00:23:17,960
via a web service interface such a rest 

50
00:23:20,780 --> 00:23:23,860
now let discuss our wildfire case study

51
00:23:23,860 --> 00:23:28,350
as a real project that acquires data using several different mechanisms 

52
00:23:29,780 --> 00:23:36,020
the wifire project stores sensor data from weather stations in a relational database 

53
00:23:37,140 --> 00:23:42,110
we use sql to retrieve this data from the database to create

54
00:23:42,110 --> 00:23:46,800
models to identify weather patterns associated with santa anna conditions 

55
00:23:48,450 --> 00:23:52,910
to determine whether a particular weather station is currently experiencing

56
00:23:52,910 --> 00:23:59,340
santa anna conditions we access real time data using a web socket service 

57
00:24:00,610 --> 00:24:04,170
once we start listening to this service 

58
00:24:04,170 --> 00:24:06,980
we receive weather station measurements as they occur 

59
00:24:08,330 --> 00:24:13,450
this data is then processed and compared to patterns found by our models

60
00:24:13,450 --> 00:24:17,390
to determine if a weather station is experiencing santa ana conditions 

61
00:24:19,060 --> 00:24:22,130
at the same time tweets are retrieved

62
00:24:22,130 --> 00:24:26,870
using hashtags related to any fire that is occurring in the region 

63
00:24:27,950 --> 00:24:32,080
the tweet messages are retrieves using the twitter rest service 

64
00:24:32,080 --> 00:24:37,260
the idea is to determine the sentiment of these tweets to see if people

65
00:24:37,260 --> 00:24:44,540
are expressing fear anger or are simply nonchalant about the nearby fire 

66
00:24:44,540 --> 00:24:49,617
the combination of sensor data and tweet sentiments helps

67
00:24:49,617 --> 00:24:54,389
to give us a sense of the urgency of the fire situation 

68
00:24:54,389 --> 00:24:58,570
as a summary big data comes from many places 

69
00:24:59,570 --> 00:25:02,000
finding and evaluating data

70
00:25:02,000 --> 00:25:06,490
useful to your big data analytics is important before you start acquiring data 

71
00:25:07,740 --> 00:25:09,080
depending on the source and

72
00:25:09,080 --> 00:25:13,240
structure of data there are alternative ways to access it 

1
00:25:15,992 --> 00:25:18,893
step 2 - a : exploring data

2
00:25:31,798 --> 00:25:36,194
after you have put together the data that you need for your application 

3
00:25:36,194 --> 00:25:40,460
you might be tempted to immediately build models to analyze the data 

4
00:25:41,520 --> 00:25:42,830
resist this temptation 

5
00:25:43,890 --> 00:25:48,370
the first step after getting your data is to explore it 

6
00:25:48,370 --> 00:25:52,340
exploring data is a part of the two - step data preparation process 

7
00:25:54,450 --> 00:25:57,560
you want to do some preliminary investigation

8
00:25:57,560 --> 00:26:03,080
in order to gain a better understanding of the specific characteristics of your data 

9
00:26:03,080 --> 00:26:05,620
in this step you will be looking for

10
00:26:05,620 --> 00:26:09,350
things like correlations general trends and outliers 

11
00:26:10,530 --> 00:26:14,700
without this step you will not be able to use the data effectively 

12
00:26:17,100 --> 00:26:20,520
correlation graphs can be used to explore the dependencies

13
00:26:20,520 --> 00:26:22,630
between different variables in the data 

14
00:26:24,210 --> 00:26:28,910
graphing the general trends of variables will show you if there is

15
00:26:28,910 --> 00:26:34,120
a consistent direction in which the values of these variables are moving towards 

16
00:26:35,150 --> 00:26:37,320
like sales prices going up or down 

17
00:26:39,200 --> 00:26:47,320
in statistics an outlier is a data point that distant from other data points 

18
00:26:47,320 --> 00:26:50,256
plotting outliers will help you double check for

19
00:26:50,256 --> 00:26:52,698
errors in the data due to measurements 

20
00:26:52,698 --> 00:26:58,390
in some cases outliers that are not errors might make you find a rare event 

21
00:27:00,670 --> 00:27:06,860
additionally summary statistics provide numerical values to describe your data 

22
00:27:08,040 --> 00:27:13,195
summary statistics are quantities that capture various characteristics

23
00:27:13,195 --> 00:27:17,540
of a set of values with a single number or a small set of numbers 

24
00:27:19,540 --> 00:27:23,674
some basic summary statistics that you should compute for

25
00:27:23,674 --> 00:27:28,490
your data set are mean median range and standard deviation 

26
00:27:30,000 --> 00:27:35,510
mean and median are measures of the location of a set of values 

27
00:27:35,510 --> 00:27:39,590
mode is the value that occurs most frequently in your data set 

28
00:27:40,920 --> 00:27:45,220
and range and standard deviation are measures of spread in your data 

29
00:27:46,640 --> 00:27:51,810
looking at these measures will give you an idea of the nature of your data 

30
00:27:53,630 --> 00:27:56,340
they can tell you if there something wrong with your data 

31
00:27:57,340 --> 00:28:01,957
for example if the range of the values for age in your data includes

32
00:28:01,957 --> 00:28:05,926
negative numbers or a number much greater than 100 

33
00:28:05,926 --> 00:28:10,708
there something suspicious in the data that needs to be examined 

34
00:28:13,629 --> 00:28:18,900
visualization techniques also provide a quick and effective and

35
00:28:18,900 --> 00:28:24,830
overall a very useful way to look at data in this preliminary analysis step 

36
00:28:24,830 --> 00:28:28,250
a heat map such as the one shown here 

37
00:28:28,250 --> 00:28:32,530
can quickly give you the idea of where the hotspots are 

38
00:28:32,530 --> 00:28:35,020
many other different types of graphs can be used 

39
00:28:36,340 --> 00:28:40,320
histograms show that the distribution of the data and

40
00:28:40,320 --> 00:28:43,520
can show skewness or unusual dispersion 

41
00:28:44,920 --> 00:28:49,168
boxplots are another type of plot for showing data distribution 

42
00:28:51,448 --> 00:28:57,410
line graphs are useful for seeing how values in your data change over time 

43
00:28:57,410 --> 00:29:00,140
spikes in the data are also easy to spot 

44
00:29:02,550 --> 00:29:07,070
scatter plots can show you correlation between two variables 

45
00:29:07,070 --> 00:29:10,900
overall there are many types of graph to visualize data 

46
00:29:11,935 --> 00:29:15,030
they are very useful in helping you understand the data you have 

47
00:29:17,120 --> 00:29:21,720
in summary what you get by exploring your data is a better

48
00:29:21,720 --> 00:29:25,510
understanding of the complexity of the data you have to work with 

49
00:29:26,780 --> 00:29:29,418
this in turn will guide the rest of your process 

