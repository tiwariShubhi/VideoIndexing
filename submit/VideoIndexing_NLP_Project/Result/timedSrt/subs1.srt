1
00:00:00,870 --> 00:00:04,540
in this short video we ll talk about how meltwater

2
00:00:04,540 --> 00:00:09,240
helped danone using sentiment analysis 

3
00:00:09,240 --> 00:00:12,600
meltwater is a company that helps other companies

4
00:00:12,600 --> 00:00:17,100
analyze what people are saying about them and manage their online reputation 

5
00:00:18,700 --> 00:00:22,980
one of the case studies on their website is about danone baby nutrition 

6
00:00:24,180 --> 00:00:29,358
meltwater helped danone to monitor the opinions through social media for

7
00:00:29,358 --> 00:00:31,873
one of their marketing campaigns 

8
00:00:31,873 --> 00:00:35,204
they were able to measure what was impactful and what was not 

9
00:00:35,204 --> 00:00:36,690
through such monitoring 

10
00:00:37,890 --> 00:00:42,610
meltwater also helped danone manage a potential reputation issue 

11
00:00:42,610 --> 00:00:45,380
when a crisis occurred related to horse dna

12
00:00:45,380 --> 00:00:48,320
being in some meat products across europe 

13
00:00:48,320 --> 00:00:52,830
while danone was confident that they did not have an issue with their products 

14
00:00:52,830 --> 00:00:57,730
having the information a couple of hours before news hit the uk press 

15
00:00:57,730 --> 00:00:59,690
allowed them to check and

16
00:00:59,690 --> 00:01:04,580
reassure their customers that their products were safe to consume 

17
00:01:04,580 --> 00:01:09,040
you can imagine millions of mothers having been reassured and happy for

18
00:01:09,040 --> 00:01:11,180
danone efforts on this 

19
00:01:11,180 --> 00:01:16,375
this is an excellent story about how big data helped manage public opinion 

20
00:01:16,375 --> 00:01:21,136
and i am sure meltwater was able to help them to measure the opinion impact through

21
00:01:21,136 --> 00:01:22,448
social media as well 

1
00:01:23,240 --> 00:01:26,508
big data is now being generated all around us 

2
00:01:26,508 --> 00:01:27,750
so what 

3
00:01:27,750 --> 00:01:29,450
it the applications 

4
00:01:29,450 --> 00:01:34,170
it is the way in which big data can serve human needs that makes it valued 

5
00:01:35,430 --> 00:01:39,971
let look at a few examples of the applications big data is allowing us to

6
00:01:39,971 --> 00:01:41,226
imagine and build 

7
00:01:59,796 --> 00:02:05,420
big data allows us to build better models which produce higher precision results 

8
00:02:06,470 --> 00:02:11,021
we are witnessing hugely innovative approaches in how companies

9
00:02:11,021 --> 00:02:13,758
market themselves and sell products 

10
00:02:13,758 --> 00:02:15,958
how human resources are managed 

11
00:02:15,958 --> 00:02:18,178
how disasters are responded to 

12
00:02:18,178 --> 00:02:22,389
and many other applications that evidenced based data is being

13
00:02:22,389 --> 00:02:24,460
used to influence decisions 

14
00:02:26,660 --> 00:02:28,670
what exactly does that mean 

15
00:02:28,670 --> 00:02:30,150
here is one example 

16
00:02:30,150 --> 00:02:32,640
many of you might have experienced it i do 

17
00:02:34,540 --> 00:02:38,140
data amazon keeps some things i have been looking at

18
00:02:38,140 --> 00:02:41,610
allows them to personalize what they show me 

19
00:02:41,610 --> 00:02:46,300
which hopefully helps narrow down the huge raft of options i might get

20
00:02:46,300 --> 00:02:49,380
than just searching on dinner plates 

21
00:02:49,380 --> 00:02:54,307
now businesses can leverage technology to make better informed decisions

22
00:02:54,307 --> 00:02:59,246
that are actually based on signals generated by actual consumers like me 

23
00:03:01,454 --> 00:03:05,980
big data enables you to hear the voice of each consumer as

24
00:03:05,980 --> 00:03:08,590
opposed to consumers at large 

25
00:03:09,920 --> 00:03:13,500
now many companies including walmart and target 

26
00:03:13,500 --> 00:03:19,160
use this information to personalize their communications with their costumers which

27
00:03:19,160 --> 00:03:23,240
in turns leads to better met consumer expectations and happier customers 

28
00:03:25,550 --> 00:03:31,628
which basically is to say big data has enabled personalized marketing 

29
00:03:31,628 --> 00:03:36,250
consumers are copiously generating publicly accessible data through

30
00:03:36,250 --> 00:03:38,560
social media sites like twitter or facebook 

31
00:03:39,690 --> 00:03:44,130
through such data the companies are able to see their purchase history 

32
00:03:44,130 --> 00:03:48,970
what they searched for what they watched where they have been and

33
00:03:48,970 --> 00:03:51,650
what they are interested in through their likes and shares 

34
00:03:52,920 --> 00:03:57,717
let look at some examples of how companies are putting this information to

35
00:03:57,717 --> 00:04:01,868
build better marketing campaigns and reach the right customers 

36
00:04:04,718 --> 00:04:08,985
one area we are all familiar with are the recommendation engines 

37
00:04:08,985 --> 00:04:14,190
these engines leverage user patterns and product features

38
00:04:14,190 --> 00:04:20,028
to predict best match product for enriching the user experience 

39
00:04:20,028 --> 00:04:22,303
if you ever shopped on amazon 

40
00:04:22,303 --> 00:04:26,596
you know you get recommendations based on your purchase 

41
00:04:26,596 --> 00:04:29,861
similarly netflix would recommend you to watch

42
00:04:29,861 --> 00:04:32,590
new shows based on your viewing history 

43
00:04:34,610 --> 00:04:40,550
another technique that companies use is sentiment analysis or in simple terms 

44
00:04:40,550 --> 00:04:44,550
analysis of the feelings around events and products 

45
00:04:45,880 --> 00:04:49,900
remember the blue plates i purchased on amazon com 

46
00:04:49,900 --> 00:04:53,680
i not only can read the reviews before purchasing them 

47
00:04:53,680 --> 00:04:57,460
i can also write a product review once i receive my plates 

48
00:04:59,280 --> 00:05:02,500
this way other customers can be informed 

49
00:05:03,880 --> 00:05:09,110
but more importantly amazon can keep a watch on the product reviews and

50
00:05:09,110 --> 00:05:11,220
trends for a particular product 

51
00:05:11,220 --> 00:05:13,951
in this case blue plates 

52
00:05:13,951 --> 00:05:20,300
for example they can judge if a product review is positive or negative 

53
00:05:21,940 --> 00:05:25,140
in this case while the first review is negative 

54
00:05:27,140 --> 00:05:29,530
the next two reviews are positive 

55
00:05:31,180 --> 00:05:35,130
since these reviews are written in english using a technique called natural

56
00:05:35,130 --> 00:05:38,780
language processing and other analytical methods 

57
00:05:38,780 --> 00:05:44,120
amazon can analyze the general opinion of a person or public about such a product 

58
00:05:46,130 --> 00:05:51,128
this is why sentiment analysis often gets referred to as opinion mining 

59
00:05:53,298 --> 00:05:57,729
news channels are filled with twitter feed analysis every time

60
00:05:57,729 --> 00:06:01,420
an event of importance occurs such as elections 

61
00:06:02,810 --> 00:06:08,162
brands utilize sentiment analysis to understand how customers

62
00:06:08,162 --> 00:06:13,630
relate to their product positively negatively neutral 

63
00:06:13,630 --> 00:06:16,448
this depends heavily on use of natural language processing 

64
00:06:19,183 --> 00:06:21,289
mobile devices are ubiquitous and

65
00:06:21,289 --> 00:06:24,780
people almost always carry their cellphones with them 

66
00:06:25,810 --> 00:06:29,200
mobile advertising is a huge market for businesses 

67
00:06:30,760 --> 00:06:35,581
platforms utilize the sensors in mobile devices 

68
00:06:35,581 --> 00:06:40,847
such as gps and provide real time location based ads 

69
00:06:40,847 --> 00:06:45,456
offer discounts based on this deluge of data 

70
00:06:45,456 --> 00:06:50,063
this time let imagine that i bought a new house and

71
00:06:50,063 --> 00:06:54,178
i happen to be in a few miles range of a home depot 

72
00:06:54,178 --> 00:06:57,444
sending me mobile coupons about paint shelves and

73
00:06:57,444 --> 00:07:01,290
other new home related purchases would remind me of home depot 

74
00:07:02,550 --> 00:07:05,590
there a big chance i would stop by home depot 

75
00:07:05,590 --> 00:07:06,790
bingo ! 

76
00:07:06,790 --> 00:07:11,470
now i would like to take a moment to analyze what kinds of big data are needed

77
00:07:11,470 --> 00:07:12,270
to make this happen 

78
00:07:13,495 --> 00:07:17,990
there definitely the integration of my consumer information and

79
00:07:17,990 --> 00:07:22,710
the online and offline databases that include my recent purchases 

80
00:07:22,710 --> 00:07:27,569
but more importantly the geolocation data that falls under

81
00:07:27,569 --> 00:07:31,128
a larger type of big data spacial big data 

82
00:07:31,128 --> 00:07:33,780
we will talk about spacial data later in this class 

83
00:07:35,190 --> 00:07:39,770
let now talk about how the global consumer behavior can be used for

84
00:07:39,770 --> 00:07:40,360
product growth 

85
00:07:42,170 --> 00:07:45,760
we are now moving from personalize marketing

86
00:07:45,760 --> 00:07:48,230
to the consumer behavior as a whole 

87
00:07:49,920 --> 00:07:54,758
every business wants to understand their consumer s collective

88
00:07:54,758 --> 00:07:59,258
behavior in order to capture the ever - changing landscape 

89
00:07:59,258 --> 00:08:04,809
several big data products enable this by developing models to capture user

90
00:08:04,809 --> 00:08:10,730
behavior and allow businesses to target the right audience for their product 

91
00:08:12,340 --> 00:08:15,250
or develop new products for uncharted territories 

92
00:08:17,570 --> 00:08:19,550
let look at this example 

93
00:08:19,550 --> 00:08:22,797
after an analysis of their sales for weekdays 

94
00:08:22,797 --> 00:08:28,319
an airline company might notice that their morning flights are always sold out 

95
00:08:28,319 --> 00:08:31,908
while their afternoon flights run below capacity 

96
00:08:31,908 --> 00:08:37,545
this company might decide to add more morning flights based on such analysis 

97
00:08:38,900 --> 00:08:43,550
notice that they are not using individual consumer choices but

98
00:08:43,550 --> 00:08:48,760
using all the flights purchased without consideration to who purchased them 

99
00:08:50,200 --> 00:08:51,150
they might however 

100
00:08:51,150 --> 00:08:56,400
decide to pay closer attention to the demographic of these consumers

101
00:08:56,400 --> 00:09:00,850
using big data to also add similar flights in other geographical regions 

102
00:09:02,350 --> 00:09:07,240
with rapid advances in genome sequencing technology 

103
00:09:07,240 --> 00:09:13,580
the life sciences industry is experiencing an enormous draw in biomedical big data 

104
00:09:15,120 --> 00:09:21,168
this biomedical data is being used by many applications in research and

105
00:09:21,168 --> 00:09:23,398
personalized medicine 

106
00:09:23,398 --> 00:09:28,400
did you know genomics data is one of the largest growing big data types 

107
00:09:28,400 --> 00:09:35,940
between 100 million and 2 billion human genomes could be sequenced by year 2025 

108
00:09:35,940 --> 00:09:36,520
impressive 

109
00:09:38,700 --> 00:09:41,080
this inaudible sequence data demands for

110
00:09:41,080 --> 00:09:46,580
between 2 exabytes and 40 exabytes in data storage 

111
00:09:46,580 --> 00:09:52,190
in comparison all of youtube only requires 1 to 2 exabytes a year 

112
00:09:54,530 --> 00:09:57,886
an exabyte is 10 to the power 18 bites 

113
00:09:57,886 --> 00:10:03,818
that is 18 zeros after 40 

114
00:10:03,818 --> 00:10:10,270
of course analysis of such massive volumes of sequence data is expensive 

115
00:10:10,270 --> 00:10:12,680
it could take up to 10 000 trillion cpu hours 

116
00:10:16,580 --> 00:10:21,040
one of the biomedical applications that this much data is enabling

117
00:10:21,040 --> 00:10:22,450
is personalized medicine 

118
00:10:24,060 --> 00:10:28,879
before personalized medicine most patients without a specific type and

119
00:10:28,879 --> 00:10:31,862
stage of cancer received the same treatment 

120
00:10:31,862 --> 00:10:34,858
which worked better for some than the others 

121
00:10:36,968 --> 00:10:42,086
research in this area is enabling development of methods to analyze

122
00:10:42,086 --> 00:10:47,472
large scale data to develop solutions that tailor to each individual 

123
00:10:47,472 --> 00:10:50,900
and hence hypothesize to be more effective 

124
00:10:52,450 --> 00:10:58,260
a person with cancer may now still receive a treatment plan that is standard 

125
00:10:58,260 --> 00:11:00,620
such as surgery to remove a tumor 

126
00:11:01,870 --> 00:11:05,750
however the doctor may also be able to recommend

127
00:11:05,750 --> 00:11:07,460
some type of personalized cancer treatment 

128
00:11:09,150 --> 00:11:13,900
a big challenge in biomedical big data applications like many other fields 

129
00:11:13,900 --> 00:11:18,860
is how we can integrate many types of data sources to gain further insight problem 

130
00:11:20,300 --> 00:11:23,648
in one of our future lectures my colleagues here at

131
00:11:23,648 --> 00:11:28,670
the supercomputer center will explain how he and his colleague have

132
00:11:28,670 --> 00:11:33,940
used big data from a variety of sources for personalized patient interventions 

133
00:11:35,840 --> 00:11:41,010
another application of big data comes from interconnected mesh of large

134
00:11:41,010 --> 00:11:46,490
number of sensors implanted across smart cities 

135
00:11:46,490 --> 00:11:50,790
analysis of data generated from sensors in real time

136
00:11:50,790 --> 00:11:55,140
allows cities to deliver better service quality to inhabitants 

137
00:11:55,140 --> 00:12:00,340
and reduce unwanted affect such as pollution traffic congestion 

138
00:12:00,340 --> 00:12:03,470
higher than optimal cost on delivering urban services 

139
00:12:04,910 --> 00:12:06,750
let take our city san diego 

140
00:12:08,370 --> 00:12:13,794
san diego generates a huge volumes of data from many sources 

141
00:12:13,794 --> 00:12:19,676
traffic sensors satellites camera networks and more 

142
00:12:19,676 --> 00:12:21,694
what if we could integrate and

143
00:12:21,694 --> 00:12:26,140
synthesize these data streams to do even more for our community 

144
00:12:27,260 --> 00:12:29,000
using such big data 

145
00:12:29,000 --> 00:12:34,170
we can work toward making san diego the prototype digital city 

146
00:12:34,170 --> 00:12:37,020
not only for life - threatening hazards but

147
00:12:37,020 --> 00:12:42,280
making our daily lives better such as managing traffic flow more efficiently or

148
00:12:42,280 --> 00:12:46,870
maximizing energy savings even as we will see next wildfires 

149
00:12:48,450 --> 00:12:53,390
if you want to read more here a link to the at kearney report 

150
00:12:53,390 --> 00:12:56,120
where they talk about other areas using big data 

151
00:12:57,580 --> 00:13:01,880
as a summary big data has a huge potential

152
00:13:01,880 --> 00:13:06,140
to enable models with higher precision in many application areas 

153
00:13:07,230 --> 00:13:12,175
and these highly precise models are influencing and transforming business 

1
00:13:13,140 --> 00:13:16,170
big data generated by people how is it being used 

2
00:13:17,740 --> 00:13:20,370
we listed a number of challenges for

3
00:13:20,370 --> 00:13:23,400
using unstructured data generated by human activities 

4
00:13:25,080 --> 00:13:29,730
now let look at some of the emerging technologies to tackle these challenges 

5
00:13:29,730 --> 00:13:34,930
and see some examples that turn unstructured data into valuable insights 

6
00:13:53,968 --> 00:13:58,780
although unstructured data specially the kind generated by people has

7
00:13:58,780 --> 00:14:01,320
a number of challenges 

8
00:14:01,320 --> 00:14:05,300
the good news is that the business culture of today is shifting

9
00:14:05,300 --> 00:14:09,070
to tackle these challenges and take full advantage of such data 

10
00:14:10,150 --> 00:14:14,120
as it is often said a challenge is a perfect opportunity 

11
00:14:15,130 --> 00:14:18,150
this is certainly the case for big data and

12
00:14:18,150 --> 00:14:21,680
these challenges have created a tech industry of it own 

13
00:14:22,860 --> 00:14:27,730
this industry is mostly centered or as we would say layered or

14
00:14:27,730 --> 00:14:33,493
stacked around a few fundamental open source big data frameworks 

15
00:14:33,493 --> 00:14:37,320
need big data tools are designed from scratch

16
00:14:37,320 --> 00:14:40,990
to manage unstructured information and analyze it 

17
00:14:40,990 --> 00:14:44,860
a majority of these tools are based on an open source

18
00:14:44,860 --> 00:14:46,620
big data framework called hadoop 

19
00:14:47,840 --> 00:14:52,010
hadoop is designed to support the processing of large data sets

20
00:14:52,010 --> 00:14:54,960
in a distributed computing environment 

21
00:14:54,960 --> 00:14:59,510
this definition would already give you a hint that it tackles the first challenge 

22
00:14:59,510 --> 00:15:03,140
namely the volume of unstructured information 

23
00:15:04,150 --> 00:15:07,740
hadoop can handle big batches of distributed information but

24
00:15:07,740 --> 00:15:10,250
most often there a need for

25
00:15:10,250 --> 00:15:15,650
a real time processing of people generated data like twitter or facebook updates 

26
00:15:17,220 --> 00:15:21,950
financial compliance monitoring is another area of our central time processing is

27
00:15:21,950 --> 00:15:25,380
needed in particular to reduce market data 

28
00:15:26,750 --> 00:15:32,780
social media and market data are two types of what we call high velocity data 

29
00:15:33,840 --> 00:15:37,720
storm and spark are two other open source frameworks

30
00:15:37,720 --> 00:15:41,480
that handle such real time data generated at a fast rate 

31
00:15:42,490 --> 00:15:43,490
both storm and

32
00:15:43,490 --> 00:15:48,230
spark can integrate data with any database or data storage technology 

33
00:15:49,560 --> 00:15:53,810
as we have emphasized before unstructured data

34
00:15:53,810 --> 00:15:57,740
does not have a relational data model so it does not generally

35
00:15:57,740 --> 00:16:02,150
fit into the traditional data warehouse model based on relational databases 

36
00:16:03,350 --> 00:16:07,870
data warehouses are central repositories of integrated data from one or

37
00:16:07,870 --> 00:16:08,490
more sources 

38
00:16:10,300 --> 00:16:16,260
the data that gets stored in warehouses gets extracted from multiple sources 

39
00:16:17,460 --> 00:16:21,850
it gets transformed into a common structured form and

40
00:16:21,850 --> 00:16:25,120
it can slow that into the central database for

41
00:16:25,120 --> 00:16:29,400
use by workers creating analytical reports throughout an enterprise 

42
00:16:30,650 --> 00:16:37,613
this exact transform load process is commonly called etl 

43
00:16:37,613 --> 00:16:41,480
this approach was fairly standard in enterprise data systems until recently 

44
00:16:42,560 --> 00:16:46,530
as you probably noticed it is fairly static and

45
00:16:46,530 --> 00:16:49,350
does not fit well with today dynamic big data world 

46
00:16:50,550 --> 00:16:55,073
so how do today businesses get around this problem 

47
00:16:55,073 --> 00:16:59,834
many businesses today are using a hybrid approach in which their smaller

48
00:16:59,834 --> 00:17:03,894
structured data remains in their relational databases and

49
00:17:03,894 --> 00:17:08,750
large unstructured datasets get stored in nosql databases in the cloud 

50
00:17:10,200 --> 00:17:15,840
nosql data technologies are based on non - relational concepts and

51
00:17:15,840 --> 00:17:20,310
provide data storage options typically on computing clouds

52
00:17:20,310 --> 00:17:24,180
beyond the traditional relational databases centered rate houses 

53
00:17:26,170 --> 00:17:31,640
the main advantage of using nosql solutions is their ability

54
00:17:31,640 --> 00:17:36,430
to organize the data for scalable access to fit the problem and

55
00:17:36,430 --> 00:17:39,330
objectives pertaining to how the data will be used 

56
00:17:41,130 --> 00:17:46,990
for example if the data will be used in an analysis to find connections

57
00:17:46,990 --> 00:17:52,130
between data sets then the best solution is a graph database 

58
00:17:53,960 --> 00:17:56,800
neo4j is an example of a graph database 

59
00:17:57,910 --> 00:18:01,630
graph networks is a topic that the graph analytics course

60
00:18:01,630 --> 00:18:05,140
later in this specialization we will explain in depth 

61
00:18:05,140 --> 00:18:11,350
if the data will be best accessed using key value pairs like a search engine

62
00:18:11,350 --> 00:18:17,490
scenario the best solution is probably a dedicated key value paired database 

63
00:18:20,710 --> 00:18:23,660
cassandra is an example of a key value database 

64
00:18:25,060 --> 00:18:26,010
these and

65
00:18:26,010 --> 00:18:30,710
many other types of nosql systems will be explained further in course two 

66
00:18:32,190 --> 00:18:36,160
so we are now confident that there are emerging technologies for

67
00:18:36,160 --> 00:18:40,410
individual challenges to manage people generated unstructured data 

68
00:18:41,610 --> 00:18:45,800
but how does one take advantage of these to generate value 

69
00:18:47,710 --> 00:18:55,150
as we saw big data must pass through a series of steps before it generates value 

70
00:18:55,150 --> 00:18:59,520
namely data access storage cleaning and analysis 

71
00:19:01,220 --> 00:19:07,210
one approach to solve this problem is to run each stage as a different layer 

72
00:19:08,400 --> 00:19:12,210
and use tools available to fit the problem at hand and

73
00:19:12,210 --> 00:19:15,730
scale analytical solutions to big data 

74
00:19:15,730 --> 00:19:20,110
in coming lectures we will see important tools that you can use

75
00:19:20,110 --> 00:19:23,840
to solve your big data problems in addition to the ones you have seen today 

76
00:19:25,070 --> 00:19:29,960
now let take a step back and remind ourselves what some of the value was 

77
00:19:31,220 --> 00:19:35,920
remember how companies can listen to the real voice of customers using big data 

78
00:19:37,540 --> 00:19:41,150
it is this type of generated data that enabled it 

79
00:19:42,190 --> 00:19:47,150
sentiment analysis analyzes social media and other data to find

80
00:19:47,150 --> 00:19:52,750
whether people associate positively or negatively with you business 

81
00:19:52,750 --> 00:19:57,210
organizations are utilizing processing of personal data to

82
00:19:57,210 --> 00:19:59,980
understand the true preferences of their customers 

83
00:20:01,130 --> 00:20:06,110
now let take a fun quiz to guess how much twitter data companies analyze

84
00:20:06,110 --> 00:20:09,510
every day to measure sentiment around their product 

85
00:20:11,080 --> 00:20:13,900
the answer is 12 terabytes a day 

86
00:20:15,040 --> 00:20:20,160
for comparison you would need to listen continuously for

87
00:20:20,160 --> 00:20:23,620
two years to finish listening to 1 terabyte of music 

88
00:20:25,000 --> 00:20:27,332
another example application area for

89
00:20:27,332 --> 00:20:30,960
people generated data is customer behavior modeling and prediction 

90
00:20:32,100 --> 00:20:36,730
amazon netflix and a lot of other organizations 

91
00:20:36,730 --> 00:20:40,400
use analytics to analyze preferences of their customers 

92
00:20:41,710 --> 00:20:47,260
based on consumer behavior organizations suggest better products to customers 

93
00:20:48,350 --> 00:20:52,430
and in turn have happier customers and higher profits 

94
00:20:53,610 --> 00:20:59,250
another application area where the value comes in the form of societal impact and

95
00:20:59,250 --> 00:21:02,540
social welfare is disaster management 

96
00:21:03,720 --> 00:21:06,550
as you have seen in my wildfire example 

97
00:21:06,550 --> 00:21:10,030
there are many types of big data that can help with disaster response 

98
00:21:11,450 --> 00:21:15,420
data in the form of pictures and tweets helps facilitate

99
00:21:15,420 --> 00:21:20,380
a collective response to disaster situations such as evacuations through

100
00:21:20,380 --> 00:21:24,310
the safest route based on community feedback through social media 

101
00:21:25,340 --> 00:21:28,820
there are also networks that turn crowd sourcing and

102
00:21:28,820 --> 00:21:32,650
big data analytics into collective disaster response tools 

103
00:21:33,920 --> 00:21:36,990
the international network of crisis mappers 

104
00:21:36,990 --> 00:21:41,950
also called crisis mappers net is the largest of such networks and

105
00:21:41,950 --> 00:21:46,360
includes an active international community of volunteers 

106
00:21:46,360 --> 00:21:52,190
crisis mappers use big data in the form of aerial and satellite imagery 

107
00:21:52,190 --> 00:21:57,440
participatory maps and live twitter updates to analyze

108
00:21:57,440 --> 00:22:03,611
the data using geospatial platforms advanced visualization 

109
00:22:03,611 --> 00:22:08,300
live simulation and computational and statistical models 

110
00:22:09,780 --> 00:22:15,220
once analyzed the results get reported to rapid response and humanitarian agencies

111
00:22:16,220 --> 00:22:22,020
in the form of mobile and web applications 

112
00:22:22,020 --> 00:22:28,510
in 2015 right after the nepal earthquake crises mappers crowd source the analysis

113
00:22:28,510 --> 00:22:33,990
of tweets and mainstream media to rapidly access disaster damage and

114
00:22:33,990 --> 00:22:39,210
needs and to identify where humanitarian help is needed 

115
00:22:39,210 --> 00:22:44,663
this example is amazing and shows how big data can have huge impacts for

116
00:22:44,663 --> 00:22:47,263
social welfare in times of need 

117
00:22:47,263 --> 00:22:50,400
you can learn more about this story at the following link 

118
00:22:52,790 --> 00:22:57,679
as a summary although there are challenges in working with unstructured

119
00:22:57,679 --> 00:23:02,270
people generated data at a scale and speed that applications demand 

120
00:23:02,270 --> 00:23:06,124
there are also emerging technologies and solutions that are being

121
00:23:06,124 --> 00:23:10,890
used by many applications to generate value from the rich source of information 

1
00:23:10,920 --> 00:23:15,485
big data generated by people the unstructured challenge 

2
00:23:31,404 --> 00:23:36,471
people are generating massive amounts of data every day through their activities on

3
00:23:36,471 --> 00:23:41,405
various social media networking sites like facebook twitter and linkedin 

4
00:23:41,405 --> 00:23:46,655
or online photo sharing sites like instagram flickr or picasa 

5
00:23:48,540 --> 00:23:50,620
and video sharing websites like youtube 

6
00:23:52,020 --> 00:23:57,100
in addition an enormous amount of information gets generated via

7
00:23:57,100 --> 00:24:02,250
blogging and commenting internet searches more via text messages 

8
00:24:03,320 --> 00:24:06,460
email and through personal documents 

9
00:24:07,920 --> 00:24:12,711
most of this data is text - heavy and unstructured 

10
00:24:12,711 --> 00:24:18,780
that is non - conforming to a well - defined data model 

11
00:24:18,780 --> 00:24:22,690
we can also consider this data to be content with

12
00:24:22,690 --> 00:24:25,630
occasionally some description attached to it 

13
00:24:25,630 --> 00:24:30,360
this much activity leads to a huge growth in data 

14
00:24:31,480 --> 00:24:35,980
did you know that in a single day facebook users produce

15
00:24:35,980 --> 00:24:40,550
more data than combined us academic research libraries 

16
00:24:42,100 --> 00:24:45,130
let look at some similar daily data volume numbers

17
00:24:46,130 --> 00:24:48,460
from some of the biggest online platforms 

18
00:24:49,720 --> 00:24:53,790
it is amazing that some of these numbers are in the petabyte range for

19
00:24:53,790 --> 00:24:54,700
daily activity 

20
00:24:55,840 --> 00:24:58,930
a petabyte is a thousand terabytes 

21
00:25:00,470 --> 00:25:05,360
the sheer size of mostly unstructured data generated by humans

22
00:25:05,360 --> 00:25:06,950
brings a lot of challenges 

23
00:25:08,730 --> 00:25:15,250
unstructured data refers to data that does not conform to a predefined data model 

24
00:25:17,200 --> 00:25:20,380
so no relation model and no sql 

25
00:25:21,980 --> 00:25:26,190
it is mostly anything that we do not store in a traditional

26
00:25:26,190 --> 00:25:27,640
relational database management system 

27
00:25:29,170 --> 00:25:32,480
consider a sales receipt that you get from a grocery store 

28
00:25:33,580 --> 00:25:36,790
it has a section for a date a section for

29
00:25:36,790 --> 00:25:40,690
store name and a section for total amount 

30
00:25:42,200 --> 00:25:44,310
this is an example of structure 

31
00:25:45,340 --> 00:25:50,550
humans generate a lot of unstructured data in form of text 

32
00:25:50,550 --> 00:25:52,960
there no given format to that 

33
00:25:52,960 --> 00:25:56,180
look at all the documents that you have written with your hand so far 

34
00:25:57,220 --> 00:26:02,550
collectively it is a bank of unstructured data you have personally generated 

35
00:26:03,580 --> 00:26:08,160
in fact 80 to 90 of all data in

36
00:26:08,160 --> 00:26:12,980
the world is unstructured and this number is rapidly growing 

37
00:26:14,490 --> 00:26:19,531
examples of unstructured data generated by people includes texts images 

38
00:26:19,531 --> 00:26:26,190
videos audio internet searches and emails 

39
00:26:26,190 --> 00:26:31,580
in addition to it rapid growth major challenges of unstructured data

40
00:26:31,580 --> 00:26:36,768
include multiple data formats like webpages images pdfs 

41
00:26:36,768 --> 00:26:44,170
power point xml and other formats that were mainly built for human consumption 

42
00:26:44,170 --> 00:26:51,030
think of it although i can sort my email with date sender and subject 

43
00:26:51,030 --> 00:26:54,690
it would be really difficult to write a program 

44
00:26:54,690 --> 00:26:59,430
to categorize all my email messages based on their content and

45
00:26:59,430 --> 00:27:05,460
organize them for me accordingly another challenge of human generated data

46
00:27:05,460 --> 00:27:11,560
is the volume and fast generation of data which is what we call velocity 

47
00:27:11,560 --> 00:27:17,504
just take a moment to study this info graphic and observe what happens in one

48
00:27:17,504 --> 00:27:22,728
minute on the internet and consider how much to contribute to it 

49
00:27:25,088 --> 00:27:32,586
moreover confirmation of unstructured data is often time consuming and costly 

50
00:27:32,586 --> 00:27:38,130
the costs and time of the process of acquiring storing 

51
00:27:38,130 --> 00:27:44,110
cleaning retrieving and processing unstructured data can add up to quite and

52
00:27:44,110 --> 00:27:47,220
investment before we can start reaping value from this process 

53
00:27:49,230 --> 00:27:51,790
it can be pretty hard to find the tools and

54
00:27:51,790 --> 00:27:55,810
people to implement such a process and reap value in the end 

55
00:27:57,030 --> 00:28:00,570
as a summary although there is an enormous amount of

56
00:28:00,570 --> 00:28:04,780
data generated by people most of this data is unstructured 

57
00:28:05,890 --> 00:28:09,230
the challenges of working with unstructured data should not

58
00:28:09,230 --> 00:28:11,170
be taken lightly 

59
00:28:11,170 --> 00:28:17,310
next we will look at how businesses are tackling these challenges to gain insight 

60
00:28:17,310 --> 00:28:20,770
and thus value out of working with people generated data 

1
00:28:21,120 --> 00:28:25,970
as we have already seen there are many different exciting applications

2
00:28:25,970 --> 00:28:28,060
that are being enabled by the big data era 

3
00:28:29,160 --> 00:28:33,500
as part of my core research here at the san diego supercomputer center 

4
00:28:33,500 --> 00:28:35,420
i work on building methodologies and

5
00:28:35,420 --> 00:28:40,150
tools to make big data useful to dynamic data driven scientific applications 

6
00:28:41,170 --> 00:28:45,600
my colleagues and i work on many grand challenge data science applications 

7
00:28:45,600 --> 00:28:50,903
in all areas of science and engineering including genomics geoinformatics 

8
00:28:50,903 --> 00:28:56,050
metro science energy management biomedicine and personalized health 

9
00:28:57,660 --> 00:29:02,492
what is common to all these applications is their unique way of bringing

10
00:29:02,492 --> 00:29:06,046
together new modes of data and computing research 

11
00:29:06,046 --> 00:29:12,195
let me tell you the one i am passionate

12
00:29:12,195 --> 00:29:17,192
about wildfire analytics 

13
00:29:17,192 --> 00:29:23,342
which breaks up into two components 

14
00:29:23,342 --> 00:29:27,779
prediction and response 

15
00:29:38,713 --> 00:29:41,629
why is this so important 

16
00:29:41,629 --> 00:29:45,608
on may 2014 in san diego county where

17
00:29:45,608 --> 00:29:50,804
the instructors of this specialization live and work 

18
00:29:50,804 --> 00:29:56,330
there were 14 fires burning as many as nine at one time 

19
00:29:56,330 --> 00:30:02,964
which burned a total of 26 000 acres 11 000 hectares 

20
00:30:02,964 --> 00:30:08,520
an area just less than the size of the city of san francisco 

21
00:30:10,710 --> 00:30:16,087
six people were injured and one person died 

22
00:30:16,087 --> 00:30:20,093
and these wildfires resulted in a total cost of

23
00:30:20,093 --> 00:30:24,620
over 60 million us in damage and firefighting 

24
00:30:25,870 --> 00:30:30,925
these wildfires can become so severe that we actually call them firestorms 

25
00:30:32,150 --> 00:30:35,170
although we cannot control such fire storms 

26
00:30:35,170 --> 00:30:39,070
something we can do is to get ahead of them by predicting their behavior 

27
00:30:40,790 --> 00:30:45,670
this is why disaster management of ongoing wildfires relies

28
00:30:45,670 --> 00:30:49,470
heavily on understanding their direction and rate of spread 

29
00:30:50,560 --> 00:30:53,000
as these fires are a part of our lives 

30
00:30:53,000 --> 00:30:58,410
we wanted to see if we can use big data to monitor predict and manage a firestorm 

31
00:31:00,400 --> 00:31:02,337
why can big data help 

32
00:31:02,337 --> 00:31:06,685
as we will see in this video indeed wildfire prevention and

33
00:31:06,685 --> 00:31:10,879
response can benefit from many streams in our data torrent 

34
00:31:10,879 --> 00:31:16,087
some streams are generated by people through devices they carry 

35
00:31:16,087 --> 00:31:21,713
a lot come from sensors and satellites things that measure environmental factors 

36
00:31:21,713 --> 00:31:25,855
and some come from organizational data including area maps 

37
00:31:25,855 --> 00:31:30,839
better service updates and field content databases which archive how much

38
00:31:30,839 --> 00:31:35,991
registers vegetation and other types of fuel are in the way of a potential fire 

39
00:31:37,840 --> 00:31:40,337
what makes this a big data problem 

40
00:31:40,337 --> 00:31:45,233
because novel approaches and responses can be taken if

41
00:31:45,233 --> 00:31:49,712
we can integrate this many diverse data streams 

42
00:31:49,712 --> 00:31:54,504
many such data sources have already existed for quite some time 

43
00:31:54,504 --> 00:31:59,835
but what is lacking in disaster management today is a dynamic system

44
00:31:59,835 --> 00:32:04,979
integration of real time sensor networks satellite imagery 

45
00:32:04,979 --> 00:32:10,405
near real time data management tools wildfire simulation tools 

46
00:32:10,405 --> 00:32:14,334
connectivity to emergency command centers and

47
00:32:14,334 --> 00:32:18,470
all these before during and after a firestorm 

48
00:32:19,800 --> 00:32:23,450
as you will see the integration of diverse streams and

49
00:32:23,450 --> 00:32:27,760
novel ways is really what is driving our ability to see new things and

50
00:32:27,760 --> 00:32:31,270
develop predictive analytics which may help improve our world 

51
00:32:32,770 --> 00:32:34,140
what are these diverse sources 

52
00:32:35,940 --> 00:32:40,870
one of the most important data sources is sensor data streaming in from weather

53
00:32:40,870 --> 00:32:46,540
stations and satellites such sensed data include temperature 

54
00:32:46,540 --> 00:32:47,960
humidity air pressure 

55
00:32:49,280 --> 00:32:53,346
we can also include image data streaming from

56
00:32:53,346 --> 00:32:58,379
mountaintop cameras and satellites in this category 

57
00:32:58,379 --> 00:33:01,967
another important data source comes from institutions

58
00:33:01,967 --> 00:33:04,867
such as the san diego supercomputer center 

59
00:33:04,867 --> 00:33:08,250
which generate data related to wildfire modeling 

60
00:33:08,250 --> 00:33:13,585
these include past and current fire perimeter maps put together by

61
00:33:13,585 --> 00:33:18,824
the authorities and fuel maps that tell us about the vegetation 

62
00:33:18,824 --> 00:33:22,171
and other types of fuel in a fire path 

63
00:33:22,171 --> 00:33:28,171
these types of data sources are often static or updated at a slow rate 

64
00:33:28,171 --> 00:33:34,083
but they provide valuable data that is well - curated and verified 

65
00:33:35,560 --> 00:33:40,450
a huge part of data on fires is actually generated by the public

66
00:33:40,450 --> 00:33:44,820
on social media sites such as twitter which support photo sharing resources 

67
00:33:46,650 --> 00:33:52,050
these are the hardest data sources to streamline during an existing fire but

68
00:33:52,050 --> 00:33:56,340
they can be very valuable once integrated with other data sources 

69
00:33:58,820 --> 00:34:04,380
imagine synthesizing all the pictures on twitter about an ongoing fire or

70
00:34:04,380 --> 00:34:07,850
checking the public sentiment around the boundaries of a fire 

71
00:34:09,960 --> 00:34:13,710
once you have access to such information at your fingertips 

72
00:34:13,710 --> 00:34:16,610
there are many things you can do with such data 

73
00:34:16,610 --> 00:34:21,300
you can simply monitor it or maybe you can visualize it 

74
00:34:23,310 --> 00:34:28,370
but it not until you bring all these different types of data sources together

75
00:34:28,370 --> 00:34:32,910
and integrate them with real time analysis and predictive modeling that you can

76
00:34:32,910 --> 00:34:37,570
really make contributions in predicting and responding to wildfire emergencies 

77
00:34:39,000 --> 00:34:42,221
so now i will like you to take a moment and

78
00:34:42,221 --> 00:34:47,296
imagine how big data might help with firefighting in the future 

79
00:34:47,296 --> 00:34:52,280
all these streams of data will come together in 3d displays that can show

80
00:34:52,280 --> 00:34:57,182
all the related information along with weather and fire predictions 

81
00:34:57,182 --> 00:35:00,462
just like the way tornadoes are managed today 

