WEBVTT

1
00:00:01.280 --> 00:00:03.990
Ingestion means the process
of getting the data

2
00:00:03.990 --> 00:00:07.040
into the data system that
we are building or using.

3
00:00:07.040 --> 00:00:10.440
Now you might think,
why is it worth talking about?

4
00:00:11.600 --> 00:00:14.560
We'll just read the data from somewhere,
like a file.

5
00:00:14.560 --> 00:00:17.320
And then using some command,
place it into the data system.

6
00:00:18.550 --> 00:00:22.180
Or we'll have have some
kind of a web form or

7
00:00:22.180 --> 00:00:25.870
other visual interface and just fill it
in so that the data goes into the system.

8
00:00:27.500 --> 00:00:31.180
Both of these ways of
data ingestion are valid.

9
00:00:31.180 --> 00:00:32.650
In fact, they're valid for

10
00:00:32.650 --> 00:00:35.420
some big data systems like your
airline reservation system.

11
00:00:36.470 --> 00:00:39.850
However when you think
of a large scale system

12
00:00:39.850 --> 00:00:43.990
you wold like to have more automation
in the data ingestion processes.

13
00:00:43.990 --> 00:00:48.060
And data ingestion then becomes a part of
the big data management infrastructure.

14
00:00:49.200 --> 00:00:53.920
So here are some questions you might want
to ask when you automate data ingestion.

15
00:00:55.140 --> 00:00:56.830
Now take a minute to read the questions.

16
00:00:59.140 --> 00:01:03.350
We'll look at two examples to
explore them in greater detail.

17
00:01:04.660 --> 00:01:08.380
The first example is that of a hospital
information system that we discussed in

18
00:01:08.380 --> 00:01:11.600
course one in the context
of precision medicine.

19
00:01:11.600 --> 00:01:15.650
We said that hospitals collect
terabytes of medical record

20
00:01:15.650 --> 00:01:18.780
from different departments and
be considered big data systems.

21
00:01:20.450 --> 00:01:24.630
The second example is a cloud based data
store where many people upload their

22
00:01:24.630 --> 00:01:29.030
messages, chats, pictures,
videos, music and so fourth.

23
00:01:29.030 --> 00:01:33.100
The cloud storage also supports active
communication between the members and

24
00:01:33.100 --> 00:01:34.920
store their communication in real time.

25
00:01:36.650 --> 00:01:41.060
So let's think of a hypothetical
hospital information information and

26
00:01:41.060 --> 00:01:43.970
the answer to depressions
that we are putting there.

27
00:01:43.970 --> 00:01:45.750
Now, do not take the numbers
to be very accurate.

28
00:01:45.750 --> 00:01:46.630
They are just examples.

29
00:01:47.630 --> 00:01:50.330
But it illustrates some important points.

30
00:01:50.330 --> 00:01:56.260
One, note that there are two kinds
of likeness associated with data.

31
00:01:56.260 --> 00:02:00.410
Some data like medical images
are large data objects by themselves.

32
00:02:02.300 --> 00:02:06.880
Secondly, the records
themselves are quite small but

33
00:02:06.880 --> 00:02:09.740
the size of the total collection
of records is very high.

34
00:02:11.580 --> 00:02:16.546
Two, while there is a lot of patient data,
the number of data sources that is

35
00:02:16.546 --> 00:02:21.517
the different departmental systems
contributing to the total information

36
00:02:21.517 --> 00:02:24.408
system does not change
very much over time.

37
00:02:24.408 --> 00:02:29.739
Three, the rate of data ingestion is
not enormous and is often proportional

38
00:02:29.739 --> 00:02:34.668
to the number of patient activities
that takes place at the hospital.

39
00:02:34.668 --> 00:02:37.993
Four, the system contains
medical records so

40
00:02:37.993 --> 00:02:42.778
data can never be discarded even
when there are errors in the data.

41
00:02:42.778 --> 00:02:48.020
The errors in this specific case
are flagged but the data is retained.

42
00:02:49.590 --> 00:02:53.740
Now this is the kind of rule
called an error handling policy.

43
00:02:53.740 --> 00:02:56.120
Which might be different for
different application problems.

44
00:02:57.960 --> 00:03:03.180
An air handling policy is part of
a larger scheme of policies called

45
00:03:03.180 --> 00:03:04.050
ingestion policies.

46
00:03:06.530 --> 00:03:10.630
Another kind of ingestion policy involves
decisions regarding what the system should

47
00:03:10.630 --> 00:03:14.480
do if the data rate suddenly increases or
becomes suspiciously low.

48
00:03:15.540 --> 00:03:19.420
In this example we have deliberately
decided not to include it in the design.

49
00:03:20.630 --> 00:03:24.150
Now compare the previous case with
the case of the online store of

50
00:03:24.150 --> 00:03:26.000
personal information.

51
00:03:26.000 --> 00:03:28.800
Again this is just an imaginary example.

52
00:03:28.800 --> 00:03:31.270
So don't think of all
the parameters to be exact.

53
00:03:32.670 --> 00:03:37.390
Now in this case one, the store will
have a fast growing membership.

54
00:03:38.680 --> 00:03:42.370
Each member will use multiple devices
to capture and ingest their data.

55
00:03:44.060 --> 00:03:50.510
Two, over all members together, the site
will be updated at a really fast rate,

56
00:03:50.510 --> 00:03:56.182
making this a large volume data
store with a fast ingest rate.

57
00:03:56.182 --> 00:04:01.150
Three, in this system, our primary
challenge is to keep up with the data

58
00:04:01.150 --> 00:04:06.780
rate, and hence, erroneous data will be
discarded after just one edit to reinvest.

59
00:04:08.760 --> 00:04:13.380
Four, now there is an actual policy for
handling data overflow,

60
00:04:13.380 --> 00:04:17.720
which essentially says,
keep the excess data in a site store.

61
00:04:17.720 --> 00:04:20.560
And ingest them when the data
rate becomes slower.

62
00:04:20.560 --> 00:04:24.660
But if the site store starts getting full

63
00:04:24.660 --> 00:04:28.980
start dropping some incoming data
at a rate of 0.1% at a time.

64
00:04:30.470 --> 00:04:33.220
Now we should see why data
ingestion together with it's

65
00:04:33.220 --> 00:04:36.320
policies should be an integral
part of a big data system.

66
00:04:36.320 --> 00:04:39.390
Especially when it involves
storing fast data.