WEBVTT

1
00:00:01.140 --> 00:00:04.170
Big Data Generated By People,
how is it being used?

2
00:00:05.740 --> 00:00:08.370
We listed a number of challenges for

3
00:00:08.370 --> 00:00:11.400
using unstructured data
generated by human activities.

4
00:00:13.080 --> 00:00:17.730
Now let's look at some of the emerging
technologies to tackle these challenges.

5
00:00:17.730 --> 00:00:22.930
And see some examples that turn
unstructured data into valuable insights.

6
00:00:41.968 --> 00:00:46.780
Although unstructured data specially
the kind generated by people has

7
00:00:46.780 --> 00:00:49.320
a number of challenges.

8
00:00:49.320 --> 00:00:53.300
The good news is that the business
culture of today is shifting

9
00:00:53.300 --> 00:00:57.070
to tackle these challenges and
take full advantage of such data.

10
00:00:58.150 --> 00:01:02.120
As it is often said,
a challenge is a perfect opportunity.

11
00:01:03.130 --> 00:01:06.150
This is certainly the case for
big data and

12
00:01:06.150 --> 00:01:09.680
these challenges have created
a tech industry of it's own.

13
00:01:10.860 --> 00:01:15.730
This industry is mostly centered or
as we would say, layered or

14
00:01:15.730 --> 00:01:21.493
stacked, around a few fundamental
open source big data frameworks.

15
00:01:21.493 --> 00:01:25.320
Need big data tools
are designed from scratch

16
00:01:25.320 --> 00:01:28.990
to manage unstructured information and
analyze it.

17
00:01:28.990 --> 00:01:32.860
A majority of these tools
are based on an open source

18
00:01:32.860 --> 00:01:34.620
big data framework called Hadoop.

19
00:01:35.840 --> 00:01:40.010
Hadoop is designed to support
the processing of large data sets

20
00:01:40.010 --> 00:01:42.960
in a distributed computing environment.

21
00:01:42.960 --> 00:01:47.510
This definition would already give you a
hint that it tackles the first challenge.

22
00:01:47.510 --> 00:01:51.140
Namely, the volume of
unstructured information.

23
00:01:52.150 --> 00:01:55.740
Hadoop can handle big batches
of distributed information but

24
00:01:55.740 --> 00:01:58.250
most often there's a need for

25
00:01:58.250 --> 00:02:03.650
a real time processing of people generated
data like Twitter or Facebook updates.

26
00:02:05.220 --> 00:02:09.950
Financial compliance monitoring is another
area of our central time processing is

27
00:02:09.950 --> 00:02:13.380
needed, in particular
to reduce market data.

28
00:02:14.750 --> 00:02:20.780
Social media and market data are two
types of what we call high velocity data.

29
00:02:21.840 --> 00:02:25.720
Storm and
Spark are two other open source frameworks

30
00:02:25.720 --> 00:02:29.480
that handle such real time
data generated at a fast rate.

31
00:02:30.490 --> 00:02:31.490
Both Storm and

32
00:02:31.490 --> 00:02:36.230
Spark can integrate data with any
database or data storage technology.

33
00:02:37.560 --> 00:02:41.810
As we have emphasized
before unstructured data

34
00:02:41.810 --> 00:02:45.740
does not have a relational data model so
it doesn't generally

35
00:02:45.740 --> 00:02:50.150
fit into the traditional data warehouse
model based on relational databases.

36
00:02:51.350 --> 00:02:55.870
Data warehouses are central repositories
of integrated data from one or

37
00:02:55.870 --> 00:02:56.490
more sources.

38
00:02:58.300 --> 00:03:04.260
The data that gets stored in warehouses,
gets extracted from multiple sources.

39
00:03:05.460 --> 00:03:09.850
It gets transformed into
a common structured form and

40
00:03:09.850 --> 00:03:13.120
it can slow that into
the central database for

41
00:03:13.120 --> 00:03:17.400
use by workers creating analytical
reports throughout an enterprise.

42
00:03:18.650 --> 00:03:25.613
This Exact Transform Load
process is commonly called ETL.

43
00:03:25.613 --> 00:03:29.480
This approach was fairly standard in
enterprise data systems until recently.

44
00:03:30.560 --> 00:03:34.530
As you probably noticed,
it is fairly static and

45
00:03:34.530 --> 00:03:37.350
does not fit well with today's
dynamic big data world.

46
00:03:38.550 --> 00:03:43.073
So how do today's businesses
get around this problem?

47
00:03:43.073 --> 00:03:47.834
Many businesses today are using a hybrid
approach in which their smaller

48
00:03:47.834 --> 00:03:51.894
structured data remains in
their relational databases, and

49
00:03:51.894 --> 00:03:56.750
large unstructured datasets get stored
in NoSQL databases in the cloud.

50
00:03:58.200 --> 00:04:03.840
NoSQL Data technologies are based
on non-relational concepts and

51
00:04:03.840 --> 00:04:08.310
provide data storage options
typically on computing clouds

52
00:04:08.310 --> 00:04:12.180
beyond the traditional relational
databases centered rate houses.

53
00:04:14.170 --> 00:04:19.640
The main advantage of using
NoSQL solutions is their ability

54
00:04:19.640 --> 00:04:24.430
to organize the data for
scalable access to fit the problem and

55
00:04:24.430 --> 00:04:27.330
objectives pertaining to
how the data will be used.

56
00:04:29.130 --> 00:04:34.990
For example, if the data will be used
in an analysis to find connections

57
00:04:34.990 --> 00:04:40.130
between data sets, then the best
solution is a graph database.

58
00:04:41.960 --> 00:04:44.800
Neo4j is an example of a graph database.

59
00:04:45.910 --> 00:04:49.630
Graph networks is a topic that
the graph analytics course

60
00:04:49.630 --> 00:04:53.140
later in this specialization,
we'll explain in depth.

61
00:04:53.140 --> 00:04:59.350
If the data will be best accessed using
key value pairs like a search engine

62
00:04:59.350 --> 00:05:05.490
scenario, the best solution is probably
a dedicated key value paired database.

63
00:05:08.710 --> 00:05:11.660
Cassandra is an example
of a key value database.

64
00:05:13.060 --> 00:05:14.010
These, and

65
00:05:14.010 --> 00:05:18.710
many other types of NoSQL systems will
be explained further in course two.

66
00:05:20.190 --> 00:05:24.160
So we are now confident that there
are emerging technologies for

67
00:05:24.160 --> 00:05:28.410
individual challenges to manage
people generated unstructured data.

68
00:05:29.610 --> 00:05:33.800
But how does one take advantage
of these to generate value?

69
00:05:35.710 --> 00:05:43.150
As we saw big data must pass through a
series of steps before it generates value.

70
00:05:43.150 --> 00:05:47.520
Namely data access, storage,
cleaning, and analysis.

71
00:05:49.220 --> 00:05:55.210
One approach to solve this problem is
to run each stage as a different layer.

72
00:05:56.400 --> 00:06:00.210
And use tools available to
fit the problem at hand, and

73
00:06:00.210 --> 00:06:03.730
scale analytical solutions to big data.

74
00:06:03.730 --> 00:06:08.110
In coming lectures, we will see
important tools that you can use

75
00:06:08.110 --> 00:06:11.840
to solve your big data problems in
addition to the ones you have seen today.

76
00:06:13.070 --> 00:06:17.960
Now let's take a step back and remind
ourselves what some of the value was.

77
00:06:19.220 --> 00:06:23.920
Remember how companies can listen to the
real voice of customers using big data?

78
00:06:25.540 --> 00:06:29.150
It is this type of generated
data that enabled it.

79
00:06:30.190 --> 00:06:35.150
Sentiment analysis analyzes social
media and other data to find

80
00:06:35.150 --> 00:06:40.750
whether people associate positively or
negatively with you business.

81
00:06:40.750 --> 00:06:45.210
Organizations are utilizing
processing of personal data to

82
00:06:45.210 --> 00:06:47.980
understand the true
preferences of their customers.

83
00:06:49.130 --> 00:06:54.110
Now let's take a fun quiz to guess how
much Twitter data companies analyze

84
00:06:54.110 --> 00:06:57.510
every day to measure sentiment
around their product.

85
00:06:59.080 --> 00:07:01.900
The answer is 12 terabytes a day.

86
00:07:03.040 --> 00:07:08.160
For comparison,
you would need to listen continuously for

87
00:07:08.160 --> 00:07:11.620
two years to finish listening
to 1 terabyte of music.

88
00:07:13.000 --> 00:07:15.332
Another example application area for

89
00:07:15.332 --> 00:07:18.960
people generated data is customer
behavior modeling and prediction.

90
00:07:20.100 --> 00:07:24.730
Amazon, Netflix and
a lot of other organizations,

91
00:07:24.730 --> 00:07:28.400
use analytics to analyze
preferences of their customers.

92
00:07:29.710 --> 00:07:35.260
Based on consumer behavior, organizations
suggest better products to customers,

93
00:07:36.350 --> 00:07:40.430
and in turn have happier customers and
higher profits.

94
00:07:41.610 --> 00:07:47.250
Another application area where the value
comes in the form of societal impact and

95
00:07:47.250 --> 00:07:50.540
social welfare, is disaster management.

96
00:07:51.720 --> 00:07:54.550
As you have seen in my wildfire example,

97
00:07:54.550 --> 00:07:58.030
there are many types of big data that
can help with disaster response.

98
00:07:59.450 --> 00:08:03.420
Data in the form of pictures and
tweets, helps facilitate

99
00:08:03.420 --> 00:08:08.380
a collective response to disaster
situations, such as evacuations through

100
00:08:08.380 --> 00:08:12.310
the safest route based on community
feedback through social media.

101
00:08:13.340 --> 00:08:16.820
There are also networks that
turn crowd sourcing and

102
00:08:16.820 --> 00:08:20.650
big data analytics into collective
disaster response tools.

103
00:08:21.920 --> 00:08:24.990
The International Network
of Crisis Mappers,

104
00:08:24.990 --> 00:08:29.950
also called Crisis Mappers Net,
is the largest of such networks and

105
00:08:29.950 --> 00:08:34.360
includes an active international
community of volunteers.

106
00:08:34.360 --> 00:08:40.190
Crisis Mappers use big data in the form
of aerial and satellite imagery,

107
00:08:40.190 --> 00:08:45.440
participatory maps and
live Twitter updates to analyze

108
00:08:45.440 --> 00:08:51.611
the data using geospatial platforms,
advanced visualization,

109
00:08:51.611 --> 00:08:56.300
live simulation and computational and
statistical models.

110
00:08:57.780 --> 00:09:03.220
Once analyzed the results get reported to
rapid response and humanitarian agencies

111
00:09:04.220 --> 00:09:10.020
in the form of mobile and
web applications.

112
00:09:10.020 --> 00:09:16.510
In 2015, right after the Nepal earthquake
Crises Mappers crowd source the analysis

113
00:09:16.510 --> 00:09:21.990
of tweets and mainstream media to
rapidly access disaster damage and

114
00:09:21.990 --> 00:09:27.210
needs and to identify where
humanitarian help is needed.

115
00:09:27.210 --> 00:09:32.663
This example is amazing and shows how
big data can have huge impacts for

116
00:09:32.663 --> 00:09:35.263
social welfare in times of need.

117
00:09:35.263 --> 00:09:38.400
You can learn more about this
story at the following link.

118
00:09:40.790 --> 00:09:45.679
As a summary, although there are
challenges in working with unstructured

119
00:09:45.679 --> 00:09:50.270
people generated data at a scale and
speed that applications demand.

120
00:09:50.270 --> 00:09:54.124
There are also emerging technologies and
solutions that are being

121
00:09:54.124 --> 00:09:58.890
used by many applications to generate
value from the rich source of information.