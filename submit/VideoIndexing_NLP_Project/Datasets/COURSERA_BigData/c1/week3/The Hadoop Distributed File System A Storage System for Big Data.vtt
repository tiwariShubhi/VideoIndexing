WEBVTT

1
00:00:02.950 --> 00:00:07.672
The Hadoop Distributed File System,
a storage system for big data.

2
00:00:21.994 --> 00:00:28.920
As a storage layer, the Hadoop distributed
file system, or the way we call it HDFS.

3
00:00:30.070 --> 00:00:33.890
Serves as the foundation for
most tools in the Hadoop ecosystem.

4
00:00:35.700 --> 00:00:39.900
It provides two capabilities that
are essential for managing big data.

5
00:00:40.900 --> 00:00:43.500
Scalability to large data sets.

6
00:00:43.500 --> 00:00:46.570
And reliability to cope
with hardware failures.

7
00:00:48.760 --> 00:00:53.060
HDFS allows you to store and
access large datasets.

8
00:00:53.060 --> 00:00:57.700
According to Hortonworks,
a leading vendor of Hadoop services,

9
00:00:57.700 --> 00:01:03.950
HDFS has shown production
scalability up to 200 petabytes and

10
00:01:03.950 --> 00:01:06.980
a single cluster of 4,500 servers.

11
00:01:06.980 --> 00:01:09.684
With close to a billion files and blocks.

12
00:01:11.504 --> 00:01:16.610
If you run out of space, you can simply
add more nodes to increase the space.

13
00:01:19.000 --> 00:01:22.470
HDFS achieves scalability
by partitioning or

14
00:01:22.470 --> 00:01:26.630
splitting large files
across multiple computers.

15
00:01:26.630 --> 00:01:31.040
This allows parallel access to very large
files since the computations run in

16
00:01:31.040 --> 00:01:34.300
parallel on each node
where the data is stored.

17
00:01:35.760 --> 00:01:38.920
Typical file size is
gigabytes to terabytes.

18
00:01:40.150 --> 00:01:47.900
The default chunk size, the size of
each piece of a file is 64 megabytes.

19
00:01:47.900 --> 00:01:50.390
But you can configure this to any size.

20
00:01:51.950 --> 00:01:55.180
By spreading the file across many nodes,

21
00:01:55.180 --> 00:01:59.740
the chances are increased that a node
storing one of the blocks will fail.

22
00:02:01.650 --> 00:02:03.440
What happens next?

23
00:02:03.440 --> 00:02:05.970
Do we lose the information
stored in block C?

24
00:02:09.560 --> 00:02:12.350
HDFS is designed for
full tolerance in such case.

25
00:02:13.640 --> 00:02:16.080
HDFS replicates, or

26
00:02:16.080 --> 00:02:21.530
makes a copy of, file blocks on
different nodes to prevent data loss.

27
00:02:23.820 --> 00:02:28.945
In this example,
the node that crashed stored block C.

28
00:02:28.945 --> 00:02:37.270
But block C was replicated on
two other nodes in the cluster.

29
00:02:39.820 --> 00:02:44.380
By default, HDFS maintains
three copies of every block.

30
00:02:45.760 --> 00:02:47.850
This is the default replication factor.

31
00:02:48.890 --> 00:02:52.300
But you can change it globally for
every file, or

32
00:02:52.300 --> 00:02:57.180
on a per file basis.

33
00:02:57.180 --> 00:03:01.370
HDFS is also designed to handle
a variety of data types aligned with

34
00:03:01.370 --> 00:03:02.450
big data variety.

35
00:03:04.370 --> 00:03:10.220
To read a file in HDFS you must
specify the input file format.

36
00:03:10.220 --> 00:03:14.610
Similarly to write the file you must
provide the output file format.

37
00:03:16.885 --> 00:03:21.040
HDFS provides a set of formats for
common data types.

38
00:03:21.040 --> 00:03:27.310
But this is extensible and you can provide
custom formats for your data types.

39
00:03:27.310 --> 00:03:30.830
For example text files can be read.

40
00:03:32.030 --> 00:03:34.620
Line by line or a word at a time.

41
00:03:36.950 --> 00:03:42.650
Geospatial data can be read as vectors or
rasters.

42
00:03:43.660 --> 00:03:49.210
Data formats specific to geospatial data,
or

43
00:03:49.210 --> 00:03:52.670
other domain specific data formats.

44
00:03:52.670 --> 00:03:57.588
Like FASTA, or FASTQ formats for
sequence data genomics.

45
00:03:59.660 --> 00:04:02.665
HDFS is comprised of two components.

46
00:04:02.665 --> 00:04:06.582
NameNode, and DataNode.

47
00:04:06.582 --> 00:04:11.180
These operate using a master
slave relationship.

48
00:04:12.350 --> 00:04:16.960
Where the NameNode issues comments
to DataNodes across the cluster.

49
00:04:18.360 --> 00:04:22.110
The NameNode is responsible for metadata.

50
00:04:22.110 --> 00:04:25.072
And DataNodes provide block storage.

51
00:04:27.174 --> 00:04:31.064
There is usually one NameNode per cluster,

52
00:04:31.064 --> 00:04:35.910
a DataNode however,
runs on each node in the cluster.

53
00:04:37.920 --> 00:04:42.280
In some sense the NameNode
is the administrator or

54
00:04:42.280 --> 00:04:44.850
the coordinator of the HDFS cluster.

55
00:04:46.030 --> 00:04:50.560
When the file is created,
the NameNode records the name,

56
00:04:50.560 --> 00:04:54.010
location in the directory hierarchy and
other metadata.

57
00:04:55.820 --> 00:05:01.730
The NameNode also decides which data nodes
to store the contents of the file and

58
00:05:01.730 --> 00:05:03.370
remembers this mapping.

59
00:05:05.420 --> 00:05:08.400
The DataNode runs on
each node in the cluster.

60
00:05:09.460 --> 00:05:12.880
And is responsible for
storing the file blocks.

61
00:05:14.600 --> 00:05:19.328
The data node listens to commands from
the name node for block creation,

62
00:05:19.328 --> 00:05:25.980
deletion, and replication.

63
00:05:25.980 --> 00:05:29.530
Replication provides two key capabilities.

64
00:05:29.530 --> 00:05:32.260
Fault tolerance and data locality.

65
00:05:33.390 --> 00:05:38.730
As discussed earlier, when a machine in
the cluster has a hardware failure there

66
00:05:38.730 --> 00:05:43.100
are two other copies of each block
that are stored on that node.

67
00:05:43.100 --> 00:05:44.470
So no data is lost.

68
00:05:45.660 --> 00:05:49.920
Replication also means that the same block
will be stored on different nodes on

69
00:05:49.920 --> 00:05:54.780
the system which are in different
geographical locations.

70
00:05:56.370 --> 00:06:01.280
A location may mean a specific rack or
a data center in a different town.

71
00:06:03.140 --> 00:06:07.950
The location is important since we
want to move computation to data and

72
00:06:07.950 --> 00:06:08.890
not the other way around.

73
00:06:11.100 --> 00:06:15.030
We'll talk about what moving computation
to data means later in this module.

74
00:06:16.650 --> 00:06:21.120
As I mentioned earlier, the default
replication factor is three, but

75
00:06:21.120 --> 00:06:23.080
you can change this.

76
00:06:23.080 --> 00:06:28.370
A high replication factor means more
protection against hardware failures,

77
00:06:28.370 --> 00:06:31.350
and better chances for data locality.

78
00:06:31.350 --> 00:06:34.660
But it also means increased
storage space is used.

79
00:06:37.190 --> 00:06:40.050
As a summary HDFS provides

80
00:06:40.050 --> 00:06:44.440
scalable big data storage by
partitioning files over multiple nodes.

81
00:06:45.950 --> 00:06:50.160
This helps to scale big data
analytics to large data volumes.

82
00:06:51.530 --> 00:06:54.730
The application protects
against hardware failures and

83
00:06:54.730 --> 00:06:59.420
provides data locality when we move
analytical complications to data.