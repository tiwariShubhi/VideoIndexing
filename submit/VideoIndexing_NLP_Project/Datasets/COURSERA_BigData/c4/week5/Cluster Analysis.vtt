WEBVTT

1
00:00:01.000 --> 00:00:04.770
In addition to classification and
regression, machine learning tasks and

2
00:00:04.770 --> 00:00:09.720
techniques can also fall into another
category known as cluster analysis.

3
00:00:09.720 --> 00:00:15.460
After this video, you will be able to
articulate the goal of cluster analysis,

4
00:00:15.460 --> 00:00:20.310
discuss whether cluster analysis
is supervised or unsupervised, and

5
00:00:20.310 --> 00:00:22.530
list some ways that cluster
results can be applied.

6
00:00:24.110 --> 00:00:25.150
In cluster analysis,

7
00:00:25.150 --> 00:00:30.870
the goal is to organize similar items in
your data set into groups or clusters.

8
00:00:30.870 --> 00:00:36.550
By segmenting your data into clusters, you
can analyze each cluster more carefully.

9
00:00:36.550 --> 00:00:39.790
Note that cluster analysis is
also referred to as clustering.

10
00:00:41.480 --> 00:00:46.220
A very common application of cluster
analysis that we have discussed before is

11
00:00:46.220 --> 00:00:50.720
to divide your customer base into segments
based on their purchasing histories.

12
00:00:50.720 --> 00:00:54.180
For example, you can segment customers
into those who have purchased science

13
00:00:54.180 --> 00:00:58.960
fiction books and videos, versus those
who tend to buy nonfiction books,

14
00:00:58.960 --> 00:01:02.240
versus those who have bought
many children's books.

15
00:01:02.240 --> 00:01:06.860
This way, you can provide more targeted
suggestions to each different group.

16
00:01:06.860 --> 00:01:11.620
Some other examples of cluster analysis
are characterizing different weather

17
00:01:11.620 --> 00:01:15.620
patterns for a region,
grouping the latest news articles into

18
00:01:15.620 --> 00:01:19.450
topics to identify the trending
topics of the day, and

19
00:01:19.450 --> 00:01:23.650
discovering hot spots for different
types of crime from police reports

20
00:01:23.650 --> 00:01:26.690
in order to provide sufficient
police presence for problem areas.

21
00:01:28.550 --> 00:01:32.100
Cluster analysis divides all
the samples in a data set into groups.

22
00:01:33.340 --> 00:01:36.620
In this diagram,
we see that the red, green, and

23
00:01:36.620 --> 00:01:38.550
purple data points are clustered together.

24
00:01:39.570 --> 00:01:43.630
Which group a sample is placed in is
based on some measure of similarity.

25
00:01:45.060 --> 00:01:50.070
The goal of cluster analysis is to segment
data so that differences between samples

26
00:01:50.070 --> 00:01:55.240
in the same cluster are minimized, as
shown by the yellow arrow, and differences

27
00:01:55.240 --> 00:02:00.280
between samples of different clusters are
maximized, as shown by the orange arrow.

28
00:02:00.280 --> 00:02:03.190
Visually, you can think of
this as getting samples in

29
00:02:03.190 --> 00:02:06.750
each cluster to be as close
together as possible, and

30
00:02:06.750 --> 00:02:10.540
the samples from different clusters
to be as far apart as possible.

31
00:02:11.810 --> 00:02:14.380
Cluster analysis requires
some sort of metric to

32
00:02:14.380 --> 00:02:17.425
measure similarity between two samples.

33
00:02:17.425 --> 00:02:22.490
Some common similarity measures are
Euclidean distance, which is the distance

34
00:02:22.490 --> 00:02:26.325
along a straight line between two points,
A and B, as shown in this plot.

35
00:02:27.390 --> 00:02:31.920
Manhattan distance, which is
calculated on a strictly horizontal and

36
00:02:31.920 --> 00:02:35.100
vertical path, as shown in the right plot.

37
00:02:35.100 --> 00:02:39.932
To go from point A to point B, you can
only step along either the x-axis or

38
00:02:39.932 --> 00:02:43.490
the y-axis in a two-dimensional case.

39
00:02:43.490 --> 00:02:47.390
So the path to calculate the Manhattan
distance consists of segments

40
00:02:47.390 --> 00:02:52.240
along the axes instead of along a diagonal
path, as with Euclidean distance.

41
00:02:53.580 --> 00:02:58.010
Cosine similarity measures the cosine
of the angle between points A and

42
00:02:58.010 --> 00:03:00.460
B, as shown in the bottom plot.

43
00:03:00.460 --> 00:03:05.400
Since distance measures such as Euclidean
distance are often used to measure

44
00:03:05.400 --> 00:03:09.048
similarity between samples
in clustering algorithms,

45
00:03:09.048 --> 00:03:13.152
note that it may be necessary to
normalize the input variables so

46
00:03:13.152 --> 00:03:16.889
that no one value dominates
the similarity calculation.

47
00:03:16.889 --> 00:03:18.071
We discussed scaling and

48
00:03:18.071 --> 00:03:22.250
normalizing variables in the lecture
on feature transformation.

49
00:03:22.250 --> 00:03:25.500
Normalizing is one method
to scale variables.

50
00:03:25.500 --> 00:03:30.560
Essentially, scaling the input variables
puts the variables on the same scale so

51
00:03:30.560 --> 00:03:33.840
that all variables have equal
weighting in the calculation

52
00:03:33.840 --> 00:03:35.889
to determine similarity between samples.

53
00:03:36.930 --> 00:03:41.110
Scaling is necessary when you have
variables that have very different scales,

54
00:03:41.110 --> 00:03:43.040
such as weight and height.

55
00:03:43.040 --> 00:03:47.250
The magnitude of the height values,
which are in feet and inches,

56
00:03:47.250 --> 00:03:51.910
will be much smaller than the magnitude of
the weight values, which are in pounds.

57
00:03:51.910 --> 00:03:55.500
So scaling both variables to
a common value range will

58
00:03:55.500 --> 00:03:58.130
make the contributions from
both weight and height equal.

59
00:03:59.410 --> 00:04:01.710
Here are some things to note
about cluster analysis.

60
00:04:02.720 --> 00:04:06.220
First, unlike classification or
regression, in general,

61
00:04:06.220 --> 00:04:09.500
cluster analysis is an unsupervised task.

62
00:04:09.500 --> 00:04:13.590
This means that there is no target
label for any sample in the data set.

63
00:04:15.020 --> 00:04:18.660
In general,
there is no correct clustering results.

64
00:04:18.660 --> 00:04:21.260
The best set of clusters
is highly dependent

65
00:04:21.260 --> 00:04:24.230
on how the resulting
clusters will be used.

66
00:04:24.230 --> 00:04:28.070
There are numerical measures to
compare two different clusters, but

67
00:04:28.070 --> 00:04:31.000
since there are no labels to
determine whether a sample has

68
00:04:31.000 --> 00:04:34.190
been correctly clustered,
there is no ground truth

69
00:04:34.190 --> 00:04:38.520
to determine if a set of clustering
results are truly correct or incorrect.

70
00:04:40.120 --> 00:04:42.190
Clusters don't come with labels.

71
00:04:42.190 --> 00:04:45.410
You may end up with five different
clusters at the end of a cluster

72
00:04:45.410 --> 00:04:49.630
analysis process, but you don't
know what each cluster represents.

73
00:04:49.630 --> 00:04:52.410
Only by analyzing
the samples in each cluster

74
00:04:52.410 --> 00:04:56.450
can you come out with reasonable
labels for your clusters.

75
00:04:56.450 --> 00:05:00.150
Given all this, it is important to
keep in mind that interpretation and

76
00:05:00.150 --> 00:05:05.400
analysis of the clusters
are required to make sense of and

77
00:05:05.400 --> 00:05:07.869
make use of the results
of cluster analysis.

78
00:05:09.330 --> 00:05:13.040
There are several ways that the results
of cluster analysis can be used.

79
00:05:13.040 --> 00:05:16.570
The most obvious is data segmentation and
the benefits that come from that.

80
00:05:17.570 --> 00:05:21.230
If you segment your customer base
into different types of readers,

81
00:05:21.230 --> 00:05:25.280
the resulting insights can be used
to provide more effective marketing

82
00:05:25.280 --> 00:05:28.540
to the different customer groups
based on their preferences.

83
00:05:28.540 --> 00:05:32.840
For example, analyzing each segment
separately can provide valuable insights

84
00:05:32.840 --> 00:05:37.570
into each group's likes, dislikes and
purchasing behavior, just like we see

85
00:05:37.570 --> 00:05:40.730
science fiction, non-fiction and
children's books preferences here.

86
00:05:42.450 --> 00:05:45.910
Clusters can also be used to
classify new data samples.

87
00:05:45.910 --> 00:05:48.030
When a new sample is received,

88
00:05:48.030 --> 00:05:52.520
like the orange sample here, compute
the similarity measure between it and

89
00:05:52.520 --> 00:05:57.160
the centers of all clusters, and assign
a new sample to the closest cluster.

90
00:05:57.160 --> 00:05:58.850
The label of that cluster,

91
00:05:58.850 --> 00:06:04.020
manually determined through analysis,
is then used to classify the new sample.

92
00:06:04.020 --> 00:06:08.770
In our book buyers' preferences example,
a new customer can be classified as being

93
00:06:08.770 --> 00:06:12.950
either a science fiction, non-fiction or
children's books customer

94
00:06:12.950 --> 00:06:16.000
depending on which cluster the new
customer is most similar to.

95
00:06:17.530 --> 00:06:19.880
Once cluster labels have been determined,

96
00:06:19.880 --> 00:06:24.660
samples in each cluster can be used as
labeled data for a classification task.

97
00:06:24.660 --> 00:06:28.090
The samples would be the input
to the classification model.

98
00:06:28.090 --> 00:06:31.750
And the cluster label would be
the target class for each sample.

99
00:06:31.750 --> 00:06:35.840
This process can be used to provide much
needed labeled data for classification.

100
00:06:37.240 --> 00:06:42.260
Yet another use of cluster results
is as a basis for anomaly detection.

101
00:06:42.260 --> 00:06:44.470
If a sample is very far away, or

102
00:06:44.470 --> 00:06:49.490
very different from any of the cluster
centers, like the yellow sample here,

103
00:06:49.490 --> 00:06:53.720
then that sample is a cluster outlier and
can be flagged as an anomaly.

104
00:06:54.740 --> 00:06:58.060
However, these anomalies
require further analysis.

105
00:06:58.060 --> 00:07:02.130
Depending on the application, these
anomalies can be considered noise, and

106
00:07:02.130 --> 00:07:04.470
should be removed from the data set.

107
00:07:04.470 --> 00:07:08.580
An example of this would be a sample
with a value of 150 for age.

108
00:07:09.840 --> 00:07:14.610
For other cases, these anomalous cases
should be studied more carefully.

109
00:07:14.610 --> 00:07:18.290
Examples of this are in a credit
card fraud detection, or,

110
00:07:18.290 --> 00:07:21.280
a network intrusion detection application.

111
00:07:21.280 --> 00:07:25.810
In these applications, examples outside
of the norm are the interesting cases

112
00:07:25.810 --> 00:07:29.840
that should be looked at to determine
if they represent potential problems.

113
00:07:30.880 --> 00:07:36.440
To summarize, cluster analysis is used to
organize similar data items into groups or

114
00:07:36.440 --> 00:07:37.990
clusters.

115
00:07:37.990 --> 00:07:41.830
Analyzing the resulting clusters
often leads to useful insights

116
00:07:41.830 --> 00:07:44.400
about the characteristics of each group,

117
00:07:44.400 --> 00:07:47.240
as well as the underlying
structure of the entire data set.

118
00:07:48.530 --> 00:07:50.400
Clusters require analysis and

119
00:07:50.400 --> 00:07:53.140
interpretation to make
sense of the results,

120
00:07:53.140 --> 00:07:58.580
since there are no labels associated with
samples or clusters in a clustering task.

121
00:07:58.580 --> 00:08:02.109
In the next lecture, we will discuss
a specific algorithm for cluster analysis.