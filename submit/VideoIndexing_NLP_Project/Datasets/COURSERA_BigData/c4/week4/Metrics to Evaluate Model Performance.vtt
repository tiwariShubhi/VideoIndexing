WEBVTT

1
00:00:00.970 --> 00:00:03.490
How do you evaluate
your model performance?

2
00:00:03.490 --> 00:00:06.870
In this lecture, we will look at
different metrics that can be used to

3
00:00:06.870 --> 00:00:09.360
evaluate the performance of
your classification model.

4
00:00:10.470 --> 00:00:13.280
After this video, you will be able to

5
00:00:13.280 --> 00:00:17.480
discuss how performance metrics
can be used to evaluate models.

6
00:00:17.480 --> 00:00:22.270
Name three model evaluation metrics, and
explain why accuracy may be misleading.

7
00:00:24.110 --> 00:00:28.090
For the classification task, an error
occurs when the model's prediction of

8
00:00:28.090 --> 00:00:32.370
the class label is different
from the true class label.

9
00:00:32.370 --> 00:00:36.330
We can also define the different types
of errors in classification depending on

10
00:00:36.330 --> 00:00:37.920
the predicted and true labels.

11
00:00:38.930 --> 00:00:43.535
Let's take the case with the task is
to predict whether a given animal is

12
00:00:43.535 --> 00:00:44.627
a mammal or not.

13
00:00:44.627 --> 00:00:50.596
This is a binary classification task
with the class label being either yes,

14
00:00:50.596 --> 00:00:54.830
indicating mammal, or
no indicating non-mammal.

15
00:00:54.830 --> 00:00:57.450
Then the different types
of errors are as follows.

16
00:00:58.600 --> 00:01:03.693
If the true label is yes and
the predicted label is yes,

17
00:01:03.693 --> 00:01:08.367
then this is a true positive,
abbreviated as TP.

18
00:01:08.367 --> 00:01:12.380
This is the case where the label is
correctly predicted as positive.

19
00:01:14.270 --> 00:01:18.500
If the true label is no and
the predicted label is no,

20
00:01:18.500 --> 00:01:22.640
then this is a true negative,
abbreviated as TN.

21
00:01:24.080 --> 00:01:27.760
This is the case where the label is
correctly predicted as negative.

22
00:01:29.910 --> 00:01:34.120
If the true label is no and
the predicted label is yes,

23
00:01:34.120 --> 00:01:39.380
then this is a false positive,
abbreviated as FP.

24
00:01:39.380 --> 00:01:43.800
This is the case with the label is
incorrectly predicted as positive,

25
00:01:43.800 --> 00:01:44.660
when it should be negative.

26
00:01:46.770 --> 00:01:48.720
If the true label is yes and

27
00:01:48.720 --> 00:01:53.830
the predicted label is no, then this
is a false negative abbreviated as FN.

28
00:01:54.950 --> 00:01:59.380
This is the case where the label is
incorrectly predicted as negative,

29
00:01:59.380 --> 00:02:00.630
when it should be positive.

30
00:02:01.990 --> 00:02:06.400
These definitions can take a while to sink
in, so feel free to hit the pause button

31
00:02:06.400 --> 00:02:09.080
and replay button several times
here to review this part.

32
00:02:11.110 --> 00:02:14.940
These four different types of errors
are used in calculating many evaluation

33
00:02:14.940 --> 00:02:17.110
metrics for classifiers.

34
00:02:17.110 --> 00:02:21.270
The most commonly used evaluation
metric is the accuracy rate, or

35
00:02:21.270 --> 00:02:23.410
accuracy for short.

36
00:02:23.410 --> 00:02:26.880
For classication,
accuracy is calculated as the number of

37
00:02:26.880 --> 00:02:30.160
correct predictions divided by
the total number of predictions.

38
00:02:31.300 --> 00:02:34.770
Note that the number of correct
predictions is the sum of the true

39
00:02:34.770 --> 00:02:39.660
positives, and the true negatives, since
the true and predicted labels match for

40
00:02:39.660 --> 00:02:41.160
those cases.

41
00:02:41.160 --> 00:02:44.640
The accuracy rate is an intuitive
way to measure the performance

42
00:02:44.640 --> 00:02:45.780
of a classification model.

43
00:02:47.410 --> 00:02:51.540
Model performance can also be
expressed in terms of error rate.

44
00:02:51.540 --> 00:02:54.140
Error rate is the opposite
of accuracy rate.

45
00:02:55.420 --> 00:03:00.290
Let's look at an example to see how
accuracy and error rates are calculated.

46
00:03:00.290 --> 00:03:04.660
The table on the left, lists the true
label along with the model's prediction

47
00:03:04.660 --> 00:03:06.760
for a data set of ten samples.

48
00:03:08.130 --> 00:03:11.345
First, letâ€™s figure out
the number of true positives.

49
00:03:11.345 --> 00:03:15.930
Recall that a true positive occurs when
the output is correctly predicted as

50
00:03:15.930 --> 00:03:17.250
positive.

51
00:03:17.250 --> 00:03:21.820
In other words the true label is yes,
and the model's prediction is yes.

52
00:03:21.820 --> 00:03:26.298
In this example there are three true
positives as indicated by the red arrows.

53
00:03:26.298 --> 00:03:30.690
So, TP=3, remember that value
as we'll need it later.

54
00:03:32.870 --> 00:03:36.000
Now, let's figure out
the number of true negatives.

55
00:03:36.000 --> 00:03:40.860
A true negative occurs when the output
is correctly predicted as negative.

56
00:03:40.860 --> 00:03:44.940
In other words, the true label is no and
the model's prediction is no.

57
00:03:44.940 --> 00:03:48.719
In this example there are four true
negatives as indicated by the green

58
00:03:48.719 --> 00:03:49.239
arrows.

59
00:03:49.239 --> 00:03:53.160
So TN = 4,
we'll need to remember this value as well.

60
00:03:55.330 --> 00:04:00.490
Now we use the values for TP and
TN to calculate the accuracy rate.

61
00:04:00.490 --> 00:04:06.660
Using the equation for accuracy rate,
we plug in three for TP and four for TN.

62
00:04:06.660 --> 00:04:10.270
We get seven correct predictions for
the numerator.

63
00:04:10.270 --> 00:04:15.480
The denominator is simply the total number
of samples in our data set, which is ten.

64
00:04:15.480 --> 00:04:18.201
So the accuracy rate for

65
00:04:18.201 --> 00:04:24.063
example is 7 out of 10 which is 0.7 or
70%.

66
00:04:24.063 --> 00:04:27.890
The error rate is the exact
opposite of the accuracy rate.

67
00:04:27.890 --> 00:04:33.120
To calculate the error rate, we simply
subtract the accuracy rate from 1.

68
00:04:33.120 --> 00:04:38.555
For our example that is
1- 0.7 which is 0.3.

69
00:04:38.555 --> 00:04:43.771
So the error rate for
this example is 0.3 or 30%.

70
00:04:43.771 --> 00:04:47.440
There's a limitation with accuracy and

71
00:04:47.440 --> 00:04:50.370
error rates when you have
a class imbalance problem.

72
00:04:51.740 --> 00:04:55.060
This is when there are very few
samples of the class of interest, and

73
00:04:55.060 --> 00:04:56.710
the majority are negative examples.

74
00:04:57.870 --> 00:05:02.540
An example of this is identifying
if a tumor is cancerous or not.

75
00:05:02.540 --> 00:05:06.820
What is of interest is identifying
samples with cancerous tumors, but

76
00:05:06.820 --> 00:05:11.200
these positive cases where the tumor
is cancerous are very rare.

77
00:05:11.200 --> 00:05:15.110
So, you end up with a very small
fraction of positive samples, and

78
00:05:15.110 --> 00:05:17.150
most of the samples are negative.

79
00:05:17.150 --> 00:05:19.310
Thus the name, class imbalance problem.

80
00:05:20.880 --> 00:05:24.260
What could be the problem with using
accuracy for a class imbalance problem?

81
00:05:25.590 --> 00:05:30.130
Consider the situation where only 3%
of the cases are cancerous tumors.

82
00:05:31.400 --> 00:05:35.480
If the classification model
always predicts non-cancer,

83
00:05:35.480 --> 00:05:39.090
it will have an accuracy rate of 97%,

84
00:05:39.090 --> 00:05:43.910
since 97% of the samples will
have non-cancerous tumors.

85
00:05:43.910 --> 00:05:49.370
But note that in this case, the model
fails to detect any cancer cases at all.

86
00:05:49.370 --> 00:05:52.600
So the accuracy rate is
very misleading here.

87
00:05:52.600 --> 00:05:55.430
You may think that your model is
performing very well with such

88
00:05:55.430 --> 00:05:56.740
a high accuracy rate.

89
00:05:56.740 --> 00:06:02.190
But in fact it cannot identify any of
the cases in the class of interest.

90
00:06:02.190 --> 00:06:05.860
In these cases we need evaluation
metrics that can capture how

91
00:06:05.860 --> 00:06:09.680
well the model classifies positive,
versus negative classes.

92
00:06:11.290 --> 00:06:14.942
A pair of evaluations metrics that
are commonly used when there is a class

93
00:06:14.942 --> 00:06:18.220
imbalance are precision and recall.

94
00:06:18.220 --> 00:06:22.950
Precision is defined as the number of
true positives divided by the sum of

95
00:06:22.950 --> 00:06:25.640
true positives and false positives.

96
00:06:25.640 --> 00:06:29.670
In other words, it is the number of true
positives divided by the total number

97
00:06:29.670 --> 00:06:32.730
of samples predicted as being positive.

98
00:06:34.060 --> 00:06:38.430
Recall is defined as the number of
true positives divided by the sum of

99
00:06:38.430 --> 00:06:40.860
true positives and false negatives.

100
00:06:40.860 --> 00:06:45.440
It is the number of true positives
divided by the total number of samples,

101
00:06:45.440 --> 00:06:47.379
actually belonging to the true class.

102
00:06:48.895 --> 00:06:52.050
Here's an illustration that
shows precision and recall.

103
00:06:52.050 --> 00:06:57.320
The selected elements indicated by the
green half circle are the true positives.

104
00:06:57.320 --> 00:07:01.240
That is samples predicted as positive and
are actually positive.

105
00:07:02.670 --> 00:07:05.830
The relevant elements indicated
by the green half circle and

106
00:07:05.830 --> 00:07:11.140
the green half rectangle, are the true
positives, plus the false negatives.

107
00:07:11.140 --> 00:07:15.520
That is samples that are actually
positive, but some are correctly predicted

108
00:07:15.520 --> 00:07:18.990
as positive, and some are incorrectly
predicted as negative.

109
00:07:19.990 --> 00:07:24.140
Recall then is the number of samples
correctly predicted as positive,

110
00:07:24.140 --> 00:07:27.250
divided by all samples that
are actually positive.

111
00:07:29.140 --> 00:07:32.200
The entire circle indicated
by the green half circle and

112
00:07:32.200 --> 00:07:37.000
the pink half circle, are the true
positives plus the false positives.

113
00:07:37.000 --> 00:07:39.710
That is samples that were
predicted as positive

114
00:07:39.710 --> 00:07:42.950
although some were actually positive and
some were actually negative.

115
00:07:44.175 --> 00:07:48.530
Then precision is the number of samples
correctly predicted as positive,

116
00:07:48.530 --> 00:07:52.670
divided by the number of all
samples predicted as positive.

117
00:07:54.230 --> 00:07:57.790
Precision is considered a measure
of exactness because it calculates

118
00:07:57.790 --> 00:08:00.710
the percentage of samples
predicted as positive,

119
00:08:00.710 --> 00:08:02.420
which are actually in a positive class.

120
00:08:03.460 --> 00:08:07.270
Recall is considered a measure of
completeness, because it calculates

121
00:08:07.270 --> 00:08:10.940
the percentage of positive samples
that the model correctly identified.

122
00:08:12.720 --> 00:08:15.950
There is a trade off between precision and
recall.

123
00:08:15.950 --> 00:08:20.762
A perfect precision score of one for
a class C means that every sample

124
00:08:20.762 --> 00:08:25.506
predicted as belonging to class C,
does indeed belong to class C.

125
00:08:25.506 --> 00:08:29.674
But this says nothing about the number of
samples from class C that were predicted

126
00:08:29.674 --> 00:08:30.480
incorrectly.

127
00:08:31.550 --> 00:08:33.790
A perfect recall score of one for

128
00:08:33.790 --> 00:08:38.710
a class C, means that every sample
from class C was correctly labeled.

129
00:08:38.710 --> 00:08:41.870
But this doesn't say anything
about how many other samples were

130
00:08:41.870 --> 00:08:44.850
incorrectly labeled as
belonging to class C.

131
00:08:44.850 --> 00:08:46.640
So they are used together.

132
00:08:46.640 --> 00:08:51.070
For example, precision values can be
compared for a fixed value of recall or

133
00:08:51.070 --> 00:08:52.360
vice versa.

134
00:08:52.360 --> 00:08:56.140
The goal for classification is to
maximize both precision and recall.

135
00:08:57.870 --> 00:09:03.060
Precision and recall can be combined into
a single metric called the F-measure.

136
00:09:03.060 --> 00:09:06.830
The equation for that is 2 times
the product of precision and

137
00:09:06.830 --> 00:09:09.630
recall divided by their sum.

138
00:09:09.630 --> 00:09:12.350
There are different
versions of the F-measure.

139
00:09:12.350 --> 00:09:13.910
The equation on this side is for

140
00:09:13.910 --> 00:09:18.860
the F1 measure which is the most
commonly used variant of the F measure.

141
00:09:18.860 --> 00:09:22.990
With the F1 measure, precision and
recall are equally weighted.

142
00:09:22.990 --> 00:09:26.245
The F2 measure weights recall
higher than precision.

143
00:09:26.245 --> 00:09:31.410
And the F0.5 measure weights
precision higher than recall.

144
00:09:32.410 --> 00:09:35.560
The value for
the F1 measure ranges from zero to one,

145
00:09:35.560 --> 00:09:38.720
with higher values giving better
classification performance.

146
00:09:40.160 --> 00:09:42.580
In summary, there are several metrics for

147
00:09:42.580 --> 00:09:45.990
evaluating the performance
of a classification model.

148
00:09:45.990 --> 00:09:49.170
They are defined in terms of
the types of errors you can get in

149
00:09:49.170 --> 00:09:50.390
a classification problem.

150
00:09:51.450 --> 00:09:56.348
We covered some of the most commonly
used evaluation metrics in this lecture,

151
00:09:56.348 --> 00:10:00.960
namely accuracy and error rates,
precision and recall and F1 measure.