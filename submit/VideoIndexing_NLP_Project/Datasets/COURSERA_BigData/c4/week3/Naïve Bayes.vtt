WEBVTT

1
00:00:00.920 --> 00:00:05.020
In this lecture, we will discuss
the Naive Bayes classifier.

2
00:00:05.020 --> 00:00:09.680
After this video, you will be able
to discuss how a Naive Bayes model

3
00:00:09.680 --> 00:00:14.550
works fro classification,
define the components of Bayes' Rule and

4
00:00:14.550 --> 00:00:17.349
explain what the naive
means in Naive Bayes.

5
00:00:18.570 --> 00:00:24.500
A Naive Bayes classification model uses a
probabilistic approach to classification.

6
00:00:24.500 --> 00:00:28.190
What this means is that the relationships
between the input features and

7
00:00:28.190 --> 00:00:31.980
the class is expressed as probabilities.

8
00:00:31.980 --> 00:00:36.390
So given the input features for
a sample, the probability for

9
00:00:36.390 --> 00:00:38.880
each class is estimated.

10
00:00:38.880 --> 00:00:43.980
The class with the highest probability
then, determines the label for the sample.

11
00:00:45.800 --> 00:00:50.250
In addition to using a probabilistic
framework for classification,

12
00:00:50.250 --> 00:00:55.060
the Naive Bayes classifier also uses
what is known as Bayes' theorem.

13
00:00:55.060 --> 00:00:58.930
The application of Bayes' theorem makes
estimating the probabilities easier.

14
00:01:00.100 --> 00:01:04.850
In addition, Naive Bayes assumes that
the input features are statistically

15
00:01:04.850 --> 00:01:07.400
independent of one another.

16
00:01:07.400 --> 00:01:10.070
This means that, for a given class,

17
00:01:10.070 --> 00:01:15.060
the value of one feature does not
affect the value of any other feature.

18
00:01:15.060 --> 00:01:19.060
This independence assumption is
an oversimplified one that does not always

19
00:01:19.060 --> 00:01:23.330
hold true, and so
is considered a naive assumption.

20
00:01:23.330 --> 00:01:25.510
The naive independence assumption and

21
00:01:25.510 --> 00:01:28.830
the use of Bayes theorem gives this
classification model its name.

22
00:01:29.910 --> 00:01:31.320
We will cover Bayes theorem and

23
00:01:31.320 --> 00:01:34.120
the independence assumption in
more detail in this lecture.

24
00:01:35.580 --> 00:01:38.200
Before we look at naive
Bayes in more detail,

25
00:01:38.200 --> 00:01:40.630
let's first start with some
background on probability.

26
00:01:41.800 --> 00:01:45.970
Probability is the measure
of how likely an event is,

27
00:01:45.970 --> 00:01:51.060
the probability of an event A occurring
is denoted P and in parenthesis A.

28
00:01:52.330 --> 00:01:57.710
It is calculated by dividing
the number of ways event A can occur,

29
00:01:57.710 --> 00:01:59.769
by the total number of possible outcomes.

30
00:02:01.370 --> 00:02:07.170
For example, what is the probability
of rolling a die and getting six?

31
00:02:07.170 --> 00:02:10.890
When you roll a die you can get
a number from one to six, so

32
00:02:10.890 --> 00:02:13.060
the number of possible outcomes is six.

33
00:02:14.330 --> 00:02:17.310
The number of ways fro getting six is one,

34
00:02:17.310 --> 00:02:21.850
since the way you can get six is if
the die shows six when it stops rolling.

35
00:02:22.850 --> 00:02:27.800
That means that the probability of getting
the number six when you roll a die

36
00:02:27.800 --> 00:02:30.080
is one over six.

37
00:02:30.080 --> 00:02:34.790
This is denoted p of six and
that's equal to one over six and

38
00:02:34.790 --> 00:02:38.670
is read as probability
of six is one over six.

39
00:02:40.440 --> 00:02:45.020
There's also a joint probability,
the joint probability specifies

40
00:02:45.020 --> 00:02:49.930
the probability of event A and
event B occurring together.

41
00:02:51.090 --> 00:02:56.620
In this diagram, the probability of event
A occurring is shown as the blue circle

42
00:02:57.650 --> 00:03:01.710
and the probability of event B
occurring is shown as the green circle.

43
00:03:02.770 --> 00:03:07.630
Then the joint probability,
that is the probability of A and

44
00:03:07.630 --> 00:03:12.340
B occurring together is shown as
the overlap of these two circles.

45
00:03:13.470 --> 00:03:18.635
The joint probability of A and
B is denoted,

46
00:03:18.635 --> 00:03:21.610
P(A,B) for

47
00:03:21.610 --> 00:03:26.860
an example of joint probability,
let's consider rolling 2 dice together.

48
00:03:26.860 --> 00:03:31.090
What is the probability in getting
2 sixes or a six from each die.

49
00:03:32.250 --> 00:03:37.020
If the two events are independent, then
the joint probability is simply the result

50
00:03:37.020 --> 00:03:40.880
of multiplying the probabilities
of the individual events together.

51
00:03:42.000 --> 00:03:45.690
In this case then, we have
the probability of rolling a six for

52
00:03:45.690 --> 00:03:51.040
each die is one over six so
the joint probability is one over 36,

53
00:03:51.040 --> 00:03:56.480
this leads us to conditional probability.

54
00:03:56.480 --> 00:04:01.250
The conditional probability is
the probability of event A occurring

55
00:04:01.250 --> 00:04:04.090
Given that event B has already occurred.

56
00:04:05.150 --> 00:04:11.570
Another way to say this is that
event A is conditioned on event B.

57
00:04:11.570 --> 00:04:16.170
The conditional probability is
the noted P and in parentheses A,

58
00:04:16.170 --> 00:04:21.390
vertical line B and is read as,
probability of A Given B.

59
00:04:22.730 --> 00:04:27.510
This diagram gives a graphical
definition of conditional probability.

60
00:04:27.510 --> 00:04:33.030
As before, the blue circle is
the probability of event A occurring,

61
00:04:33.030 --> 00:04:37.260
the green circle is the probability
of event B occurring.

62
00:04:37.260 --> 00:04:40.610
The overlap is a joint
probability of A and B.

63
00:04:41.730 --> 00:04:46.500
The conditional probability,
P(A given B) then is

64
00:04:46.500 --> 00:04:51.870
calculated as the join probability
divided by the probability of B.

65
00:04:53.040 --> 00:04:56.980
The conditional probability is
an important concept in classification

66
00:04:56.980 --> 00:04:58.800
as we will see later.

67
00:04:58.800 --> 00:05:03.020
It provides the means to specify
the probability of a class label,

68
00:05:03.020 --> 00:05:04.240
given the input values.

69
00:05:05.960 --> 00:05:12.390
The relationship between conditional
probabilities P of B given A and

70
00:05:12.390 --> 00:05:17.295
P of A given B can be expressed
through Bayes' Theorem.

71
00:05:17.295 --> 00:05:21.811
This theorem is named after a reverend
named Thomas Bayes who lived in the 1700s.

72
00:05:21.811 --> 00:05:26.691
It is a way to look at how the probability
of a hypothesis is affected by

73
00:05:26.691 --> 00:05:29.170
new evidence gathered from data.

74
00:05:29.170 --> 00:05:34.925
Bayes' theorem expresses the relationship
between probability of B

75
00:05:34.925 --> 00:05:40.285
given A and probability of A given
B as shown in this equation.

76
00:05:40.285 --> 00:05:46.033
Bayes' Theorem is also known
as Bayes' Rule or Bayes' Law.

77
00:05:46.033 --> 00:05:49.750
Now that we have reviewed some
background on probability

78
00:05:49.750 --> 00:05:52.610
let's see how all this relates
to the classification problem.

79
00:05:53.840 --> 00:05:58.249
With the probabilistic framework the
classification task is defined as follows.

80
00:05:59.530 --> 00:06:03.930
Capital X is the set of values for
the input features in the sample,

81
00:06:05.090 --> 00:06:09.780
given a sample with features X,
predict the corresponding class C.

82
00:06:10.860 --> 00:06:15.930
Another way to state this is,
what is the class label associated with

83
00:06:15.930 --> 00:06:21.550
the feature vector X or how should
the feature vector x be classified?

84
00:06:22.910 --> 00:06:27.470
To find the class label C we need to
calculate the conditional probability of

85
00:06:27.470 --> 00:06:31.540
class C, given X for all classes and

86
00:06:31.540 --> 00:06:33.990
select a class with
the highest probability.

87
00:06:35.450 --> 00:06:37.010
So for classification,

88
00:06:37.010 --> 00:06:43.460
we want to find the value of C that
maximizes the probability of C given X.

89
00:06:43.460 --> 00:06:47.140
The problem is that it is difficult
to estimate this probability,

90
00:06:47.140 --> 00:06:52.530
because we would need to enumerate every
possible combination of feature values and

91
00:06:52.530 --> 00:06:54.460
to know the conditional probability.

92
00:06:54.460 --> 00:06:59.570
For each class given every
possible feature combination and

93
00:06:59.570 --> 00:07:02.330
here is where Bayes'
theorem comes into play.

94
00:07:02.330 --> 00:07:06.710
The classification problem can be
reformulated using Bayes' theorem

95
00:07:06.710 --> 00:07:08.876
to simplify the classification problem.

96
00:07:08.876 --> 00:07:16.060
Specifically, using Bayes' theorem, the
probability of c given x, can be expressed

97
00:07:16.060 --> 00:07:20.600
using other probability quantities,
which can be estimated from the data.

98
00:07:22.030 --> 00:07:26.010
Here's Bayes' theorem again, but
some additional terms defined.

99
00:07:26.010 --> 00:07:31.870
Probability of C I X is referred to
as the posterior probability since

100
00:07:31.870 --> 00:07:37.930
it is the probability of the class label
being C after observing input features X.

101
00:07:39.400 --> 00:07:43.850
Probability of X given
C is the probability of

102
00:07:43.850 --> 00:07:48.450
observing input features X given
that c is the class label.

103
00:07:49.540 --> 00:07:54.480
This is the class conditional probability
since it is conditioned on the class.

104
00:07:55.790 --> 00:08:00.440
Probability of c is the probability
of the class label being C,

105
00:08:01.470 --> 00:08:06.560
this is the probability of each class
prior to observing any input data.

106
00:08:06.560 --> 00:08:09.350
And so
is referred to as the prior probability.

107
00:08:10.810 --> 00:08:14.240
The probability of X is the probability

108
00:08:14.240 --> 00:08:18.479
of observing input features X
regardless of what the class label is.

109
00:08:20.070 --> 00:08:24.617
So for classification we want
to calculate the posterior

110
00:08:24.617 --> 00:08:27.939
probability P(C | X) for each class C.

111
00:08:27.939 --> 00:08:34.343
From Bayes' theorem P(C | X) is related to

112
00:08:34.343 --> 00:08:40.250
the P(X | C) P(C And probability of X.

113
00:08:41.790 --> 00:08:45.680
Probability of X does not depend
on the class C, therefore,

114
00:08:45.680 --> 00:08:49.040
it is a constant value, given the input X.

115
00:08:49.040 --> 00:08:52.940
Since it the same value for
all classes, the probability

116
00:08:52.940 --> 00:08:57.610
of X can be removed from the calculation
of probability of C, given X.

117
00:08:58.610 --> 00:09:03.910
What's left then are probability of
X given C and the probability of C.

118
00:09:05.630 --> 00:09:09.826
So estimating the probability
of C given X boils down to

119
00:09:09.826 --> 00:09:14.583
estimating the probability of X
given C and probability of C.

120
00:09:14.583 --> 00:09:18.397
The nice thing is that
probability of X given C and

121
00:09:18.397 --> 00:09:22.220
probability of C can be
estimated from the data.

122
00:09:23.390 --> 00:09:28.200
So now we have a way to calculate probably
of C given X which is what we need for

123
00:09:28.200 --> 00:09:28.940
classification.

124
00:09:30.150 --> 00:09:31.720
To the estimate the probability of C

125
00:09:32.780 --> 00:09:37.450
which is the probability of the class
of C before observing any input data.

126
00:09:37.450 --> 00:09:41.690
We simply calculate the fraction
of samples with that class label

127
00:09:41.690 --> 00:09:42.910
C in the training data.

128
00:09:44.170 --> 00:09:48.650
For this example, there are four samples
labeled as green circles out of 10

129
00:09:48.650 --> 00:09:55.740
samples, so probability of green
circle is 4 out of 10, or 0.4.

130
00:09:55.740 --> 00:10:01.650
Similarly, the fraction of samples labeled
as red triangles is 6 out of 10, or 0.6.

131
00:10:01.650 --> 00:10:06.500
So estimating the prior
probabilities is a simple count

132
00:10:06.500 --> 00:10:09.660
of number of samples with each class label

133
00:10:09.660 --> 00:10:12.970
divided by the total number of
samples in the training data center.

134
00:10:14.410 --> 00:10:19.000
In estimating probability of X
given C which is the probability

135
00:10:19.000 --> 00:10:23.520
of observing feature factor
X given that the class is C,

136
00:10:23.520 --> 00:10:26.760
we can use the independent
assumption to simplify the problem.

137
00:10:27.790 --> 00:10:31.820
The Independence Assumption of
the Naive Bayes classifier assumes

138
00:10:31.820 --> 00:10:36.620
that each feature X sub I
in the featured vector X

139
00:10:36.620 --> 00:10:41.410
is conditionally independent of every
other feature, given the class C.

140
00:10:42.520 --> 00:10:47.600
This means that we only need to
estimate the probability of X

141
00:10:47.600 --> 00:10:51.360
sub i given C, instead of having to

142
00:10:51.360 --> 00:10:56.030
estimate the probability of
the entire feature X given C.

143
00:10:56.030 --> 00:10:58.070
For every combination of values for

144
00:10:58.070 --> 00:11:03.900
the features in X then we would simply
multiply these individual probabilities

145
00:11:03.900 --> 00:11:08.630
together to get the probability
of the entire feature vector X.

146
00:11:08.630 --> 00:11:14.660
Given the class C to estimate
the probability of X sub I,

147
00:11:14.660 --> 00:11:19.210
given C, we count up the number
of times a particular input

148
00:11:19.210 --> 00:11:23.820
value is observed for
the class c in the training data.

149
00:11:23.820 --> 00:11:28.880
For example, the number of times that we
see the value of yes for the future home

150
00:11:28.880 --> 00:11:35.380
owner, when the class label is no it's
three as indicated by the green arrows.

151
00:11:35.380 --> 00:11:40.980
This is divided by the number of samples
with no as the class label which is seven.

152
00:11:40.980 --> 00:11:43.160
This fraction, three out of seven,

153
00:11:43.160 --> 00:11:47.910
is the probability that home owner
is Yes given that the class is No.

154
00:11:50.090 --> 00:11:53.420
Similarly, the samples with
the value of Single for

155
00:11:53.420 --> 00:11:58.710
the feature Marital Status when it crosses
Yes are indicated by the red arrows.

156
00:11:58.710 --> 00:12:02.050
And the probability that
Marital Status is Single,

157
00:12:02.050 --> 00:12:07.520
given that the class label
is Yes is 2/3 or 0.67.

158
00:12:09.050 --> 00:12:14.360
Some things to know about the Naive Bayes
classification model are it is a fast and

159
00:12:14.360 --> 00:12:15.840
simple algorithm.

160
00:12:15.840 --> 00:12:19.940
The algorithm boils down to calculating
counts for probabilities and

161
00:12:19.940 --> 00:12:24.930
performing some multiplication, so
it is very simple to implement.

162
00:12:24.930 --> 00:12:28.000
And the probabilities that are needed
can be calculated with a single

163
00:12:28.000 --> 00:12:30.180
scan of the data set and
stored in a table.

164
00:12:31.610 --> 00:12:35.610
Either two processing of the data
is not necessary as with many other

165
00:12:35.610 --> 00:12:36.500
machine learning algorithms.

166
00:12:37.540 --> 00:12:42.150
So model building and
testing of both task, it scales well.

167
00:12:42.150 --> 00:12:44.970
Due today independent assumption,
the probability for

168
00:12:44.970 --> 00:12:48.000
each feature can be
independently estimated.

169
00:12:48.000 --> 00:12:52.120
These means that featured probability
is can be calculated in parallel,

170
00:12:52.120 --> 00:12:56.270
this also means that the data set size
does not have to grow exponentially

171
00:12:56.270 --> 00:12:57.630
with a number of features.

172
00:12:58.780 --> 00:13:02.130
This avoids the many problems associated
with the curse of dimensionality,

173
00:13:03.480 --> 00:13:06.480
this also means that you do not need
a lot of data to build the model.

174
00:13:07.700 --> 00:13:11.410
The number of parameters scales
linearly with the number of features.

175
00:13:12.740 --> 00:13:16.700
The Independence assumption does
not hold true in many cases.

176
00:13:16.700 --> 00:13:20.850
In practice however, the Naive Bayes
classifier still tends to perform very

177
00:13:20.850 --> 00:13:24.070
well this is because even
though Naive Bayes may

178
00:13:24.070 --> 00:13:28.400
not provide good estimates of
the correct class probabilities.

179
00:13:28.400 --> 00:13:32.870
As long as the correct class is
more probable than any other class,

180
00:13:32.870 --> 00:13:35.420
the correct classification
results will be reached.

181
00:13:36.920 --> 00:13:40.940
The independence assumption also prevents
the naive base classifier to model

182
00:13:40.940 --> 00:13:45.810
interactions between features which
limits its classification power.

183
00:13:45.810 --> 00:13:50.040
The increased risk of smoking in a history
of cancer would not be captured,

184
00:13:50.040 --> 00:13:50.560
for example.

185
00:13:52.510 --> 00:13:56.230
The Naive Bays classifier has been
applied to many real world problems

186
00:13:56.230 --> 00:14:00.620
including spam filtering, document
classification, and sentiment analysis.

187
00:14:01.650 --> 00:14:06.530
To summarize, the Naive Bayes classifier
uses a probabilistic framework for

188
00:14:06.530 --> 00:14:08.120
classification.

189
00:14:08.120 --> 00:14:11.916
It applies Bayes Theorem and
the Feature Independence Assumption,

190
00:14:11.916 --> 00:14:16.570
to simplify the problem of estimating
probabilities for the classification task.