WEBVTT

1
00:00:00.681 --> 00:00:05.059
We will start with a very simple
classification technique called k-Nearest

2
00:00:05.059 --> 00:00:05.807
Neighbors.

3
00:00:05.807 --> 00:00:10.451
After this video, you will be able
to describe how kNN is used for

4
00:00:10.451 --> 00:00:12.710
classification.

5
00:00:12.710 --> 00:00:17.995
Discuss the assumption behind kNN and
explain what the k stands for in kNN.

6
00:00:17.995 --> 00:00:21.995
kNN stands for k-Nearest Neighbors.

7
00:00:21.995 --> 00:00:25.924
This is one of the simplest techniques
to build a classification model.

8
00:00:25.924 --> 00:00:30.623
The basic idea is to classify
a sample based on its neighbors.

9
00:00:30.623 --> 00:00:35.774
So when you get a new sample as shown by
the green circle in the figure, the class

10
00:00:35.774 --> 00:00:40.944
label for that sample is determined by
looking at the labels of its neighbors.

11
00:00:40.944 --> 00:00:44.030
KNN relies on the notion
of the so-called duck test.

12
00:00:44.030 --> 00:00:48.724
That is if it looks like a duck,
swims like a duck and quacks like a duck,

13
00:00:48.724 --> 00:00:51.430
then it probably is a duck.

14
00:00:51.430 --> 00:00:53.260
In the classification context,

15
00:00:53.260 --> 00:00:59.320
this means that samples with similar input
values likely belong to the same class.

16
00:00:59.320 --> 00:01:03.640
So, samples with similar input values
should be labeled with the same

17
00:01:03.640 --> 00:01:05.390
target label.

18
00:01:05.390 --> 00:01:08.820
This means that classification
of a sample is dependent

19
00:01:08.820 --> 00:01:12.250
on the target labels of
the neighboring points.

20
00:01:12.250 --> 00:01:15.140
In more detail then,
this is how kNN works.

21
00:01:15.140 --> 00:01:17.084
Given a new sample, look for

22
00:01:17.084 --> 00:01:21.710
the samples in the training data
that are closest to the new sample.

23
00:01:21.710 --> 00:01:23.740
These are the neighbors.

24
00:01:23.740 --> 00:01:28.550
Use the labels of this neighboring points
to determine the label for the new sample.

25
00:01:29.590 --> 00:01:32.410
This figure illustrate how kNN works.

26
00:01:32.410 --> 00:01:36.240
The problem here is to determine if
a sample should be classified as

27
00:01:36.240 --> 00:01:37.940
a blue square or red triangle.

28
00:01:38.990 --> 00:01:40.980
The green circle is the new sample.

29
00:01:40.980 --> 00:01:45.780
To determine a class label for this new
sample, look at its closest neighbors.

30
00:01:45.780 --> 00:01:49.230
These neighbors are the samples
within the dashed circle.

31
00:01:50.270 --> 00:01:53.180
Two blue squares and one red triangle.

32
00:01:53.180 --> 00:01:56.590
The class labels of the neighboring
samples determine the label for

33
00:01:56.590 --> 00:01:57.290
the new sample.

34
00:01:58.690 --> 00:02:03.480
The value of k determines the number
of nearest neighbor to consider.

35
00:02:03.480 --> 00:02:08.160
So if k equals 1,
then only the closest neighbor is examined

36
00:02:08.160 --> 00:02:12.030
to determine the class of the new
sample as shown in the left figure.

37
00:02:13.170 --> 00:02:14.197
If k equals 2,

38
00:02:14.197 --> 00:02:19.182
then the 2 nearest neighbors are
considered as seen in the middle figure.

39
00:02:19.182 --> 00:02:23.815
If k equal 3, then the 3 nearest
neighbors are considered as

40
00:02:23.815 --> 00:02:26.101
in the right figure and so on.

41
00:02:26.101 --> 00:02:30.311
If k equal 1 and only 1 neighbor is used,
then the label for

42
00:02:30.311 --> 00:02:34.030
the new sample is simpler
the label of the neighbor.

43
00:02:34.030 --> 00:02:37.090
This is shown in the left figure.

44
00:02:37.090 --> 00:02:39.610
The label of the new sample is then A,

45
00:02:39.610 --> 00:02:42.510
since that is the label of
its one nearest neighbor.

46
00:02:43.630 --> 00:02:48.270
When multiple neighbors are considered,
then a voting scheme is used.

47
00:02:48.270 --> 00:02:52.290
Majority of vote is commonly used,
so the label associated with

48
00:02:52.290 --> 00:02:57.670
the majority of the neighbors is
used as the label of the new sample.

49
00:02:57.670 --> 00:03:00.730
This is what we see in the right figure.

50
00:03:00.730 --> 00:03:04.090
With k equals 3,
3 nearest neighbors are considered.

51
00:03:05.398 --> 00:03:08.830
With two neighbors labeled as A and
one as B,

52
00:03:08.830 --> 00:03:12.560
the majority of vote determines that
the new sample should be labeled as A.

53
00:03:13.680 --> 00:03:18.168
In case of a tie which could be
possible if the value of k is even,

54
00:03:18.168 --> 00:03:21.154
then some tight breaking rule is needed.

55
00:03:21.154 --> 00:03:24.853
For example, the label of
the closer neighbor is used or

56
00:03:24.853 --> 00:03:28.250
the label is chosen randomly
among the neighbors.

57
00:03:28.250 --> 00:03:30.480
This is seen in the middle figure.

58
00:03:31.550 --> 00:03:35.970
With two nearest neighbors and each with
a different class label, the label for

59
00:03:35.970 --> 00:03:38.640
the new sample is randomly
chosen here to be B.

60
00:03:40.000 --> 00:03:44.230
With kNN, some measure of similarity
is needed to determine how

61
00:03:44.230 --> 00:03:47.030
close two samples are together.

62
00:03:47.030 --> 00:03:50.690
This is necessary to determine which
samples are the nearest neighbors.

63
00:03:51.690 --> 00:03:55.650
Distance measures such as
distance are commonly used.

64
00:03:55.650 --> 00:03:59.480
Other distance measures that can be used,
include Manhattan and hemming distance.

65
00:04:00.640 --> 00:04:05.671
To summarize, kNN is a very
simple classification technique.

66
00:04:05.671 --> 00:04:08.970
Note that there is no
separate training phase.

67
00:04:08.970 --> 00:04:13.490
There is no separate part where a model is
constructed and its parameter is adjusted.

68
00:04:13.490 --> 00:04:16.970
This is unlike most other
classification algorithms.

69
00:04:18.510 --> 00:04:22.230
KNN can generate complex
decision boundaries allowing for

70
00:04:22.230 --> 00:04:24.980
complex classification
decisions to be made.

71
00:04:26.400 --> 00:04:28.410
It can be susceptible to noise,however,

72
00:04:28.410 --> 00:04:32.910
because classification decisions
are made using only information about

73
00:04:32.910 --> 00:04:35.735
a few neighboring points
instead of the entire dataset.

74
00:04:37.570 --> 00:04:41.540
KNN can be slow, however, since
the distance between a new sample and

75
00:04:41.540 --> 00:04:45.630
all sample points in the data must
be calculated in order to determine

76
00:04:45.630 --> 00:04:49.796
the k-Nearest Neighbors.