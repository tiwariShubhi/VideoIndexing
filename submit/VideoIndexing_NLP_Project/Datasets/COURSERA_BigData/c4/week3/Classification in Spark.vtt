WEBVTT

1
00:00:01.720 --> 00:00:05.366
In this activity we will perform
classification in Spark is in

2
00:00:05.366 --> 00:00:06.355
decision tree.

3
00:00:06.355 --> 00:00:10.889
First, we will load weather
data into DataFrame and

4
00:00:10.889 --> 00:00:13.578
drop unused and missing data.

5
00:00:13.578 --> 00:00:16.686
We'll then,
create a categorical variable for

6
00:00:16.686 --> 00:00:21.100
low humidity days and aggregate
features used to make predictions.

7
00:00:21.100 --> 00:00:25.110
We will then split our data
into training and test sets.

8
00:00:26.400 --> 00:00:28.330
And then create and
train the decision tree.

9
00:00:29.730 --> 00:00:34.414
Finally, we will save our
predictions to a CSV file.

10
00:00:34.414 --> 00:00:38.635
Let's begin, first, we'll open
the notebook called classification.

11
00:00:38.635 --> 00:00:46.344
The first cell contains the classes
we need to load to run this exercise.

12
00:00:46.344 --> 00:00:47.122
Let's run this.

13
00:00:50.152 --> 00:00:55.540
Next we create SQL context and load
the weather data CSV into a data frame.

14
00:00:55.540 --> 00:00:58.570
The second cell also prints all
the columns in this data frame.

15
00:00:59.770 --> 00:01:00.270
Run this.

16
00:01:05.731 --> 00:01:08.923
The third cell defines the columns
in the weather data we will use for

17
00:01:08.923 --> 00:01:10.440
the decision tree classifier.

18
00:01:11.460 --> 00:01:11.960
Let's run it.

19
00:01:14.570 --> 00:01:16.710
We will now use the column name number.

20
00:01:16.710 --> 00:01:23.030
So, let's drop that from the data frame,
df = df.drop Number.

21
00:01:24.960 --> 00:01:31.422
Now, let's revolve the rows with
missing data, df = df.na.drop.

22
00:01:33.486 --> 00:01:40.129
Now, let's print the number of rows and
columns in our resulting data frame,

23
00:01:40.129 --> 00:01:43.664
df.count(), len(df.columns).

24
00:01:47.994 --> 00:01:52.800
Next, let's create a categorical variable
to denote if the humidity is low.

25
00:01:54.070 --> 00:01:59.793
We'll enter binarizer = Binarizer ().

26
00:01:59.793 --> 00:02:03.663
The first argument specifies
a threshold value for the variable.

27
00:02:03.663 --> 00:02:09.598
We want the categorical variable to be 1,
if the humidity is greater than 25%.

28
00:02:09.598 --> 00:02:15.555
So we'll enter a threshold=24.9999.

29
00:02:15.555 --> 00:02:20.820
The next argument specifies the column to
use to create the categorical variable.

30
00:02:20.820 --> 00:02:26.409
We'll input,
inputCol = relative_humidity_3pm.

31
00:02:26.409 --> 00:02:33.610
The final argument specifies the new
column name, outputCol = label.

32
00:02:33.610 --> 00:02:36.206
Now, let's create a new data frame
with this categorical variable.

33
00:02:36.206 --> 00:02:42.123
binarizeredDF = binarizer.transform df.

34
00:02:42.123 --> 00:02:43.702
Let's run this.

35
00:02:45.787 --> 00:02:48.774
Let's look at the first four
rows in this new data frame.

36
00:02:48.774 --> 00:02:54.519
We'll run binarizedDF.select
('relative_humidity_3pm',

37
00:02:54.519 --> 00:02:56.918
'label').show(4).

38
00:03:00.238 --> 00:03:06.630
The relative humidity in the first row
is greater than 25% and the label is 1.

39
00:03:06.630 --> 00:03:10.720
The relative humidity in the second,
third, and fourth rows are less than 25%,

40
00:03:10.720 --> 00:03:14.060
and the label is 0.

41
00:03:14.060 --> 00:03:20.014
Next, let's aggregate the features we will
use to make predictions into a single col,

42
00:03:20.014 --> 00:03:22.517
assembler = VectorAssember( ).

43
00:03:22.517 --> 00:03:26.248
The first argument is a list of
the columns to be aggregated.

44
00:03:26.248 --> 00:03:31.946
inputCols=featureColumns, and
the second argument is the name

45
00:03:31.946 --> 00:03:38.876
of the new column containing the
aggregated features, outputCol=features.

46
00:03:38.876 --> 00:03:42.836
We can create the new
data frame by running

47
00:03:42.836 --> 00:03:47.829
assembled=assembler.transform binarizedDF.

48
00:03:47.829 --> 00:03:49.141
Let's run this.

49
00:03:51.707 --> 00:03:55.410
Next we will split our data
set into two parts, one for

50
00:03:55.410 --> 00:03:57.972
training data and one for test data.

51
00:03:57.972 --> 00:04:03.198
You can do this by entering (training

52
00:04:03.198 --> 00:04:11.693
Data,
testData)=assembled.randomSplit([0.8,

53
00:04:11.693 --> 00:04:16.602
0.2], seed=13234).

54
00:04:16.602 --> 00:04:21.049
We can see the size of the two
sets by running count,

55
00:04:21.049 --> 00:04:25.818
trainingData.count(), testData.count ().

56
00:04:28.419 --> 00:04:30.996
Next let's create and
train the decision tree.

57
00:04:30.996 --> 00:04:35.490
We'll enter dt = DecisionTreeClassifier.

58
00:04:36.510 --> 00:04:41.763
The first argument is the column we're
trying to predict, labelCol='label'.

59
00:04:41.763 --> 00:04:46.036
The second argument is the name
of the column containing your

60
00:04:46.036 --> 00:04:49.989
aggregated features,
featuresCol='features'.

61
00:04:51.310 --> 00:04:53.880
The third argument is
the stopping criteria for

62
00:04:53.880 --> 00:04:58.700
tree induction based on the maximum
depth of the tree, maxDepth=5.

63
00:04:58.700 --> 00:05:04.135
The fourth argument is the stopping
criteria for tree induction based

64
00:05:04.135 --> 00:05:09.954
on the minimum number of samples
in a node, minInstancesPerNode=20.

65
00:05:09.954 --> 00:05:14.942
And finally, the last argument
specifies the impurity measure

66
00:05:14.942 --> 00:05:20.127
used to split the nodes,
impurity="gini", let's run this.

67
00:05:22.507 --> 00:05:25.665
Next, we can create a model by
training the decision tree.

68
00:05:25.665 --> 00:05:28.463
We'll do this by executing
it in a pipeline.

69
00:05:28.463 --> 00:05:36.020
We'll enter pipeline=Pipeline
(stages=[dt]).

70
00:05:36.020 --> 00:05:40.719
We'll create them all by putting
a training data, model =

71
00:05:40.719 --> 00:05:45.740
pipeline.fit trainingData.

72
00:05:45.740 --> 00:05:52.695
Let's run this Now, we can make
predictions using our test data.

73
00:05:52.695 --> 00:05:58.520
We'll enter predictions =
model.transform(testData).

74
00:05:58.520 --> 00:06:02.824
You can look at the first 10 rows
of the prediction by running,

75
00:06:02.824 --> 00:06:07.619
predictions.select('prediction', label')
show(10).

76
00:06:10.232 --> 00:06:13.611
You can see in the first ten rows,
the prediction matches the label.

77
00:06:13.611 --> 00:06:16.595
Now let's save our
predictions to a CSV file.

78
00:06:16.595 --> 00:06:20.190
In the next Spark hands-on activity
we will evaluate the accuracy.

79
00:06:20.190 --> 00:06:25.861
You can save it by running
predictions.select('prediction',

80
00:06:25.861 --> 00:06:30.753
'label').write.save(path='file File:///

81
00:06:30.753 --> 00:06:36.631
home/cloudera/downloads/big-data-4/predic-
tions.csv.

82
00:06:36.631 --> 00:06:40.365
We'll specify the format to use Spark csv.

83
00:06:40.365 --> 00:06:46.101
Format='com.databricks.spark.csv.

84
00:06:46.101 --> 00:06:49.735
Finally, we'll enter header='true'.

85
00:06:49.735 --> 00:06:52.823
Run this to save
the predictions to a .csv file.