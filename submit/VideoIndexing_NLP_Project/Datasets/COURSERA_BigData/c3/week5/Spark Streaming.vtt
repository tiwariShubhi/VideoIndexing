WEBVTT

1
00:00:01.630 --> 00:00:05.040
Next we will talk about Spark streaming.

2
00:00:06.650 --> 00:00:12.474
After this video you will be able to
summarize how Spark reads streaming data,

3
00:00:12.474 --> 00:00:17.240
list several sources of streaming
data supported by Spark, and

4
00:00:17.240 --> 00:00:20.077
describe Spark's sliding windows.

5
00:00:20.077 --> 00:00:25.778
Spark streaming provides
scalable processing for

6
00:00:25.778 --> 00:00:30.954
real-time data and
runs on top of Spark Core.

7
00:00:32.816 --> 00:00:36.319
Continuous data streams are converted or

8
00:00:36.319 --> 00:00:42.130
grouped into discrete RDDs which
can then be processed in parallel.

9
00:00:43.620 --> 00:00:47.357
Spark Streaming provides APIs for Scala,

10
00:00:47.357 --> 00:00:51.620
Java, and Python,
like other Spark products.

11
00:00:55.370 --> 00:01:00.820
Spark Streaming can read
data from many different

12
00:01:00.820 --> 00:01:06.278
types of resources,
including Kafka and Flume.

13
00:01:06.278 --> 00:01:12.778
Kafka is a high throughput published
subscribed messaging system,

14
00:01:12.778 --> 00:01:17.044
and Flume collects and
aggregates log data.

15
00:01:17.044 --> 00:01:22.125
Spark Streaming can also read from batch

16
00:01:22.125 --> 00:01:26.609
input data sources, such as HDFS,

17
00:01:26.609 --> 00:01:31.720
S3, and many other non SQL databases.

18
00:01:31.720 --> 00:01:39.368
Additionally, Spark Streaming can read
directly from Twitter, raw TCP sockets,

19
00:01:39.368 --> 00:01:45.306
and many other data sources that
are real-time data providers.

20
00:01:48.358 --> 00:01:49.280
So how does it all work?

21
00:01:50.320 --> 00:01:55.910
Here we show you a flow of
transformations and actions

22
00:01:55.910 --> 00:02:01.131
which you will try in the upcoming reading
and hands-on exercises on Spark Streaming.

23
00:02:01.131 --> 00:02:05.102
Spark streaming reads

24
00:02:05.102 --> 00:02:10.760
streaming data and
converts it into micro batches

25
00:02:10.760 --> 00:02:16.350
which we call DStreams which is short for
discretized stream.

26
00:02:19.378 --> 00:02:24.736
In this example a 10 second
stream gets converted

27
00:02:24.736 --> 00:02:29.980
into five RDDs using a batch
length of 2 seconds.

28
00:02:31.800 --> 00:02:36.920
Similar to other RDDs,
transformations such as map, reduce,

29
00:02:36.920 --> 00:02:39.515
and filter can be applied to DStreams.

30
00:02:39.515 --> 00:02:45.720
DStreams can be aggregated

31
00:02:45.720 --> 00:02:52.310
into Windows allowing you to apply
computations on sliding window of data.

32
00:02:53.860 --> 00:02:58.165
In this example the Window
size is 4 seconds and

33
00:02:58.165 --> 00:03:01.510
the sliding interval is 2 seconds.

34
00:03:04.826 --> 00:03:09.594
In summary, Spark Streaming is
Spark's library to work with

35
00:03:09.594 --> 00:03:12.260
streaming data in near real time.

36
00:03:14.375 --> 00:03:19.210
DStreams can be used just
like any other RDD and

37
00:03:19.210 --> 00:03:22.940
can go through the same
transformation as batch datasets.

38
00:03:24.560 --> 00:03:31.907
DStreams can create a sliding window to
perform calculations on a window of time.