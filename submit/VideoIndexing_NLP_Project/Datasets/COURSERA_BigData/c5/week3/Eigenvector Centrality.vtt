WEBVTT

1
00:00:00.752 --> 00:00:05.460
So far, the centrality of a node
is defined using the degree and

2
00:00:05.460 --> 00:00:08.250
the shortest distance to other nodes.

3
00:00:08.250 --> 00:00:09.830
Now, we introduce a different idea.

4
00:00:11.050 --> 00:00:14.480
We would like to say that
if you are important and

5
00:00:14.480 --> 00:00:17.350
I'm connected to you,
then I must be somewhat important too.

6
00:00:18.540 --> 00:00:21.510
In other words, my centrality

7
00:00:21.510 --> 00:00:26.030
is proportional to the combined
centrality values of my neighbors.

8
00:00:26.030 --> 00:00:30.720
Now, if we write that down mathematically
it would look like the top formula.

9
00:00:30.720 --> 00:00:34.690
The centrality of (Vi) is
the sum of its neighbors.

10
00:00:34.690 --> 00:00:37.140
Now, we can write that as an equation

11
00:00:37.140 --> 00:00:39.300
where Lambda is
a proportionality constant.

12
00:00:40.710 --> 00:00:45.040
The resulting equation looks exactly
like the Eigenvector equation we have

13
00:00:45.040 --> 00:00:46.210
seen before.

14
00:00:46.210 --> 00:00:49.790
Now again, you really do not
have to understand how it works.

15
00:00:49.790 --> 00:00:53.010
It's fine to know that if
we solve that equation,

16
00:00:53.010 --> 00:00:55.390
we will get the Eigen values lambda.

17
00:00:56.690 --> 00:00:59.130
Now, let's take the largest lambda and

18
00:00:59.130 --> 00:01:04.130
find the corresponding Eigenvector, which
will give you the centrality of each note.

19
00:01:05.680 --> 00:01:08.390
Notice the difference between
the degree centrality and

20
00:01:08.390 --> 00:01:10.790
the Eigenvector centrality
in the same graph.

21
00:01:12.290 --> 00:01:15.600
The yellow node in the middle
has a low degree centrality

22
00:01:15.600 --> 00:01:17.540
compared to its neighbors.

23
00:01:17.540 --> 00:01:23.470
However, with the Eigenvector centrality,
the node becomes comparably more important

24
00:01:23.470 --> 00:01:27.880
because the neighbor centrality status
boost some of the centrality of this node.

25
00:01:29.260 --> 00:01:32.790
In contrast,
consider the second highlighted note.

26
00:01:32.790 --> 00:01:37.350
It had the same on normalize degree
centrality as the previous node.

27
00:01:38.520 --> 00:01:42.610
But because the neighbors
are low centrality nodes,

28
00:01:42.610 --> 00:01:45.440
the eigenvector centrality goes down.

29
00:01:45.440 --> 00:01:51.730
So, this is the intended consequence
of the eigenvector centrality measure.

30
00:01:51.730 --> 00:01:57.060
Speaking of consequences,
Eigenvector centrality essentially says

31
00:01:57.060 --> 00:01:59.810
if you know the right people,
your importance will go up.

32
00:02:01.090 --> 00:02:02.760
Well, that's kind of risky proposition.

33
00:02:03.880 --> 00:02:09.090
Here is me in a social network,
and let's say I'm connected to

34
00:02:09.090 --> 00:02:13.250
this somewhat dubious character, and
I think it doesn't really matter.

35
00:02:14.430 --> 00:02:19.490
What I don't know is that my connection
has it's own set of connections, and

36
00:02:19.490 --> 00:02:20.160
look at who they are.

37
00:02:21.480 --> 00:02:25.740
So, on the one hand, these shady
characters that are now connected to

38
00:02:25.740 --> 00:02:30.020
indirectly does raise my
eigenvector centrality, but

39
00:02:30.020 --> 00:02:34.000
it also has quite a damaging
effect on my reputation.

40
00:02:34.000 --> 00:02:37.230
My EV centrality almost makes
me look like a suspect.

41
00:02:38.230 --> 00:02:38.950
Now, think about that.

42
00:02:40.320 --> 00:02:44.090
Now Brin and Page, the founders of Google

43
00:02:44.090 --> 00:02:47.260
had an interesting way to think
about the eigenvector centrality.

44
00:02:48.660 --> 00:02:50.750
They thought about a server.

45
00:02:50.750 --> 00:02:53.410
Well no, not that kind of server.

46
00:02:53.410 --> 00:02:54.770
This kind of server.

47
00:02:54.770 --> 00:02:56.860
The kind that surfs the web.

48
00:02:56.860 --> 00:02:59.855
But, this is a special web
surfer called a random surfer.

49
00:03:01.050 --> 00:03:06.050
And here is what he does, he picks
a web page and looks at the links.

50
00:03:08.040 --> 00:03:13.320
Then he chooses a random link, goes to
that page and does the same thing again.

51
00:03:15.000 --> 00:03:19.600
Except sometimes when you kind of get
bored and goes to totally new page.

52
00:03:21.220 --> 00:03:23.510
How often does he do this random jump?

53
00:03:24.720 --> 00:03:29.250
Let's say, there's always sort
of a 15% chance that he will.

54
00:03:29.250 --> 00:03:32.490
Or more generally,
with the probability of 1 minus alpha.

55
00:03:34.040 --> 00:03:37.210
So, Page and Brin's idea was to figure out

56
00:03:37.210 --> 00:03:41.780
that this surfer will visit a page with
a high chance, if the page is central.

57
00:03:43.060 --> 00:03:44.390
They came up with a measure for

58
00:03:44.390 --> 00:03:49.010
this stationary probability of a page
being visited by the random surfer.

59
00:03:50.160 --> 00:03:52.820
They did not call it centrality,
they called it pagerank.

60
00:03:54.330 --> 00:03:57.114
Let's see a YouTube video to
understand how pagerank behaves.

61
00:05:15.194 --> 00:05:19.427
Okay, now we are talking about
the World Wide Web which is a huge graph.

62
00:05:19.427 --> 00:05:22.565
How do we of such a graph?

63
00:05:23.575 --> 00:05:28.550
The answer is iteratively using
a method called power iteration.

64
00:05:29.710 --> 00:05:33.440
This method can be used because we
are looking for the largest eigenvalue.

65
00:05:33.440 --> 00:05:34.070
Let me show you.

66
00:05:35.390 --> 00:05:36.360
Let's take a small graph.

67
00:05:37.990 --> 00:05:41.730
Let's initialize the still unknown
page rank as zero for all nodes.

68
00:05:43.020 --> 00:05:48.930
Now, page rank A is a 0.15 chance,
that I was at A already.

69
00:05:48.930 --> 00:05:53.740
And 0.85 chance that I come to A from B,
or I come to A from C.

70
00:05:55.270 --> 00:05:58.370
However, at this point,
the page rank of B and C are at zero.

71
00:05:59.700 --> 00:06:03.134
So, in the first iteration,
page rank of A is 0.15.

72
00:06:04.670 --> 00:06:10.210
Now for B, I can only come to B from A,
but we cannot claim all of pagerank of A,

73
00:06:10.210 --> 00:06:14.901
because there is always a half chance
that the surfer will come to B

74
00:06:14.901 --> 00:06:18.255
from A because he could
also get to here from C.

75
00:06:18.255 --> 00:06:24.042
This, plus the 0.15 chance that
the surfer is already at B,

76
00:06:24.042 --> 00:06:26.733
makes Bs pagerank 0.21.

77
00:06:26.733 --> 00:06:30.221
Now after doing a few rounds of this
computation, between 50 and 100 iteration,

78
00:06:30.221 --> 00:06:30.760
let's say.

79
00:06:31.820 --> 00:06:32.830
The values will converge.

80
00:06:33.830 --> 00:06:37.570
Now, this computation has been
demonstrated to perform well

81
00:06:37.570 --> 00:06:38.740
in the MapReduce framework.

82
00:06:40.020 --> 00:06:44.100
In module four, we'll talk about
another way of computing this metric.