WEBVTT

1
00:00:00.890 --> 00:00:05.257
So in this video,
we'll going to talk about graph analytics

2
00:00:05.257 --> 00:00:09.820
within the context of this
big data specialization.

3
00:00:12.080 --> 00:00:16.330
So in the previous courses you know about
the three important V's of big data.

4
00:00:17.410 --> 00:00:21.580
So the three well-known V's are volume,
velocity, and variety.

5
00:00:22.940 --> 00:00:27.440
We will also talk about a lesser-known V,
which is called valence.

6
00:00:29.170 --> 00:00:33.976
Okay, what we want to talk about is,

7
00:00:33.976 --> 00:00:39.881
what impact these things
have on graph data.

8
00:00:39.881 --> 00:00:44.693
So for volume, let's take a dataset like

9
00:00:44.693 --> 00:00:49.238
the load network of the United States.

10
00:00:49.238 --> 00:00:51.770
Well, that's a pretty large graph.

11
00:00:51.770 --> 00:00:55.870
So when we say volume,
I mean that the size of the graph

12
00:00:55.870 --> 00:00:59.790
is much larger than what you might have

13
00:01:00.960 --> 00:01:05.400
in the memory of a reasonable computer or
real computing infrastructure.

14
00:01:07.130 --> 00:01:12.462
Now, we will see what impact the size

15
00:01:12.462 --> 00:01:18.490
of the graph has on analytic operations.

16
00:01:18.490 --> 00:01:21.497
What we mean by velocity
when it comes to graphs?

17
00:01:21.497 --> 00:01:22.620
Well, think of Facebook again.

18
00:01:23.860 --> 00:01:25.680
So these little graphs are updates.

19
00:01:26.700 --> 00:01:34.190
So you write a post, then like somebody
else's post, and make a comment.

20
00:01:34.190 --> 00:01:35.774
That's a bunch of updates.

21
00:01:35.774 --> 00:01:37.810
That comes and adds to your graph.

22
00:01:39.110 --> 00:01:43.440
Well, then ten minutes later,
you do something similar, and

23
00:01:43.440 --> 00:01:45.000
that also comes and adds to the graph.

24
00:01:45.000 --> 00:01:48.060
Then your friend does the same thing,
it adds to your graph.

25
00:01:48.060 --> 00:01:53.530
So as time goes by,
you are sending more edges to your graph.

26
00:01:55.060 --> 00:01:59.240
And the speed at which
you are doing this for

27
00:01:59.240 --> 00:02:02.870
at least like Facebook can be really,
really high.

28
00:02:02.870 --> 00:02:05.590
So the rate of update in
Facebook is really high.

29
00:02:06.910 --> 00:02:13.997
This is what is called
streaming edges into graphs.

30
00:02:13.997 --> 00:02:19.314
And there can be multiple streams for
various reasons.

31
00:02:19.314 --> 00:02:20.515
What do we mean by variety?

32
00:02:20.515 --> 00:02:28.790
For graphs, it means that the graph is
collecting data from various places.

33
00:02:28.790 --> 00:02:33.002
And all these different places are giving
different kinds of information to

34
00:02:33.002 --> 00:02:33.671
the graph.

35
00:02:33.671 --> 00:02:36.905
So in the end,
the graph has more non-uniform and

36
00:02:36.905 --> 00:02:41.217
complex information potentially
coming from multiple sources.

37
00:02:41.217 --> 00:02:44.210
That's what we mean by variety
when we refer to graphs.

38
00:02:45.690 --> 00:02:48.930
That picture there, by the way, is
different kinds of protein interactions.

39
00:02:52.600 --> 00:02:55.850
The next one,
the less-known one is valence.

40
00:02:57.510 --> 00:03:02.578
Now, if you remember your chemistry,
this comes from valence electrons,

41
00:03:02.578 --> 00:03:06.470
which are electrons in an atom
which are used for bonding.

42
00:03:06.470 --> 00:03:09.790
The other electrons
are called core electrons.

43
00:03:09.790 --> 00:03:14.274
So the idea is if we increase
the valence of the graphs,

44
00:03:14.274 --> 00:03:18.183
you increase the connectiveness
of the graph.

45
00:03:18.183 --> 00:03:19.901
How, we will see.

46
00:03:22.920 --> 00:03:27.894
Now, graph size clearly impacts analytics.

47
00:03:27.894 --> 00:03:35.041
Why, a, it takes more space, but more
importantly, it increases the algorithmic

48
00:03:35.041 --> 00:03:40.190
complexity of any operation that
you want it to on the graph.

49
00:03:40.190 --> 00:03:45.344
Now, we'll see an example of that,
but what happens as

50
00:03:45.344 --> 00:03:50.948
a result is that the data-to-analysis
time becomes high.

51
00:03:50.948 --> 00:03:54.240
So I put in some data, and
I wanted to do this analysis.

52
00:03:54.240 --> 00:03:59.120
But there is so much data, that my
analysis takes way longer than it should.

53
00:04:00.620 --> 00:04:05.343
Let's give a simple example,
an example we have seen before.

54
00:04:05.343 --> 00:04:10.373
Remember, we had this little graph
from our biological example where we

55
00:04:10.373 --> 00:04:16.488
were asking, find a simple path between
Alzheimer's Disease and Colorectal Cancer.

56
00:04:16.488 --> 00:04:18.925
And in this case, the result is obvious.

57
00:04:22.524 --> 00:04:25.589
Now, let's pause and ask.

58
00:04:25.589 --> 00:04:29.248
There are two nodes that I mentioned,
in this case,

59
00:04:29.248 --> 00:04:33.170
my Colorectal Cancer and
Alzheimer's Disease nodes.

60
00:04:34.800 --> 00:04:39.940
And we are asking,
is there a simple path connecting them?

61
00:04:42.290 --> 00:04:44.470
This is called a decision problem.

62
00:04:44.470 --> 00:04:51.988
I give you a data, and I'm asking does
such a simple path exist or not exist?

63
00:04:51.988 --> 00:04:55.124
But this is actually a very
hard decision problem.

64
00:04:55.124 --> 00:04:59.964
And the computer scientists will tell you
that this is a very complicated problem

65
00:04:59.964 --> 00:05:02.321
because it has a very high complexity.

66
00:05:02.321 --> 00:05:04.340
Let's ask another question.

67
00:05:04.340 --> 00:05:07.070
Well, how many simple paths,
now I want to count.

68
00:05:07.070 --> 00:05:09.430
How many simple paths exist
between these two nodes?

69
00:05:10.810 --> 00:05:13.580
Indeed, it is another
hard computing problem.

70
00:05:15.370 --> 00:05:19.750
And if you really want to know,
the size of the result,

71
00:05:19.750 --> 00:05:24.780
in the worst case is exponential
in the number of nodes.

72
00:05:24.780 --> 00:05:27.930
So if we increase the number of nodes and
edges, if we increase the size of

73
00:05:27.930 --> 00:05:35.490
the graph such a seemingly simple question
can take a very, very, very long time.

74
00:05:37.180 --> 00:05:40.890
So that it's almost practically
impossible to compute it for

75
00:05:40.890 --> 00:05:44.910
a really large graph if we have
no other information supporting.

76
00:05:44.910 --> 00:05:47.008
That's the worst case.

77
00:05:47.008 --> 00:05:52.441
But when we say algorithmic complexity
increases, that's what we mean.

78
00:05:55.692 --> 00:06:00.894
Let's talk about velocity, and
I said our favorite example is Facebook.

79
00:06:00.894 --> 00:06:05.096
So we are adding a bunch of updates, which
means we are adding a bunch of edges.

80
00:06:05.096 --> 00:06:11.490
We are streaming the edges into the data,
and we want to compute a metric.

81
00:06:11.490 --> 00:06:15.791
We want to see what is
the shortest distance

82
00:06:15.791 --> 00:06:20.697
between person a and
person b or item a and item b.

83
00:06:20.697 --> 00:06:25.155
Or I want to know that
Facebook has communities.

84
00:06:25.155 --> 00:06:26.690
Twitter has communities like we saw.

85
00:06:28.220 --> 00:06:32.540
And how many people out there,
in these communities, and

86
00:06:32.540 --> 00:06:35.600
how many such communities are there,
like a Facebook group?

87
00:06:37.560 --> 00:06:42.180
Now, if you want to compute this metric,
and you get this

88
00:06:42.180 --> 00:06:46.449
edges very fast, it is very difficult
to know when you have the answer.

89
00:06:47.480 --> 00:06:55.450
Because you are going to get an increasing
number of edges in the system,

90
00:06:56.450 --> 00:07:01.660
and you keep computing this metric that
you want to find the answer for, and

91
00:07:01.660 --> 00:07:07.040
it will turn out that your continuous
stream does not fit in memory.

92
00:07:07.040 --> 00:07:11.820
Because your memory is limited
compared to the amount of

93
00:07:13.930 --> 00:07:17.680
edges, or edge updates you
are streaming into the system.

94
00:07:19.010 --> 00:07:23.221
So that's what's happened when you
have high velocity information.

95
00:07:23.221 --> 00:07:28.584
Very soon, your memory runs out,
and you want to compute

96
00:07:28.584 --> 00:07:33.290
your answer right now from
the data that you have.

97
00:07:36.110 --> 00:07:39.640
Okay, let's look at variety,
also known as heterogeneity.

98
00:07:41.460 --> 00:07:44.881
There are two aspects of heterogeneity.

99
00:07:44.881 --> 00:07:50.803
One, we have already mentioned, graph data
is often created through integration,

100
00:07:50.803 --> 00:07:53.449
like we saw in the case of the biology.

101
00:07:56.446 --> 00:08:02.530
And therefore, the variety comes because
the nature of data is very different.

102
00:08:04.510 --> 00:08:07.800
Also, they may not be all
the same kind of data.

103
00:08:08.890 --> 00:08:13.340
For example, the data may come
from a relational database,

104
00:08:14.440 --> 00:08:16.580
it may come from an XML database.

105
00:08:16.580 --> 00:08:18.900
It may come from another graph.

106
00:08:18.900 --> 00:08:20.130
It may come from a document.

107
00:08:21.470 --> 00:08:26.910
It may even come from complex
things like social networks,

108
00:08:26.910 --> 00:08:31.890
like citation networks between papers or
patents, between interaction networks,

109
00:08:33.070 --> 00:08:36.300
between web entities,
which are connected through links.

110
00:08:37.780 --> 00:08:43.360
And from human knowledge that has been
represented as graphs through ontologists.

111
00:08:43.360 --> 00:08:49.476
So all of these graphs, the nodes and
the edges do not mean the same thing.

112
00:08:49.476 --> 00:08:54.083
And somehow in there,
you need to capture what it means to have

113
00:08:54.083 --> 00:08:58.971
an edge because that will determine
what you can do with the edge.

114
00:08:58.971 --> 00:09:03.490
A simple example, in an ontology,

115
00:09:03.490 --> 00:09:10.280
is something that says a is a b,
and b is a c, so a is a c.

116
00:09:11.730 --> 00:09:18.430
The a is a c is an inference that you do,
given the other two relationships.

117
00:09:18.430 --> 00:09:21.133
What would be an example?

118
00:09:21.133 --> 00:09:27.570
My pet is a dog, and the dog is a mammal,
therefore, my pet is a mammal.

119
00:09:28.770 --> 00:09:32.320
You want to do inferences for
some edges likes is a.

120
00:09:33.480 --> 00:09:36.520
Now, you need to know this.

121
00:09:36.520 --> 00:09:40.140
You do not do this with the biology
example where you are looking at genes and

122
00:09:40.140 --> 00:09:45.447
proteins because that operation does
not make sense when you have genes and

123
00:09:45.447 --> 00:09:46.230
proteins.

124
00:09:46.230 --> 00:09:50.720
So therefore, every graph may
have a different semantics.

125
00:09:50.720 --> 00:09:53.880
And what happens with variety is
the number of sub-semantics and

126
00:09:53.880 --> 00:09:56.670
the number of valid
operations that you can do.

127
00:09:56.670 --> 00:09:58.730
That changes, and
that becomes more complex.

128
00:10:00.395 --> 00:10:04.720
Now, valence I said,
is about connectedness.

129
00:10:04.720 --> 00:10:07.760
It is also about
interdependency among data.

130
00:10:07.760 --> 00:10:12.844
So if I have a higher valence which means,
I have more data elements that

131
00:10:12.844 --> 00:10:18.040
are more strongly related, and
these relationships can be exploited.

132
00:10:20.820 --> 00:10:25.437
In most cases,
the part where valence becomes important,

133
00:10:25.437 --> 00:10:31.806
is that it increases over time, which
means, parts of the graph becomes denser,

134
00:10:31.806 --> 00:10:36.264
and the average distance
between node pairs decreases.

135
00:10:36.264 --> 00:10:39.210
Let me show you, here is my Gmail.

136
00:10:41.080 --> 00:10:49.140
And I have plotted my Gmail graphs
from 2006 to about two months back.

137
00:10:50.950 --> 00:10:54.810
When I first started using it,
I had these users,

138
00:10:54.810 --> 00:10:59.140
a very few users, and
they are not really related.

139
00:11:00.170 --> 00:11:02.220
Now with time, more and

140
00:11:02.220 --> 00:11:06.200
more people started communicating
with me through Gmail.

141
00:11:07.290 --> 00:11:11.618
And more and more of these people were
also talking amongst themselves and

142
00:11:11.618 --> 00:11:13.240
copying me and responding to me.

143
00:11:14.410 --> 00:11:20.130
By the end, you would see that
you can find dense groups

144
00:11:20.130 --> 00:11:24.290
within my Gmail because
the information and

145
00:11:24.290 --> 00:11:31.020
the connectedness between people have
evolved and become more dense over time.

146
00:11:31.020 --> 00:11:36.620
This is the phenomenon of valence, and
this is very important to study because

147
00:11:36.620 --> 00:11:43.120
you want to study things like, what parts
of the graph have become more dense?

148
00:11:43.120 --> 00:11:44.801
And why have they become more dense?

149
00:11:44.801 --> 00:11:46.490
Maybe something was going on.

150
00:11:46.490 --> 00:11:49.010
Maybe there was an event that
brought these people together,

151
00:11:49.010 --> 00:11:52.550
and you want to analyze that and
find that out from your graph analytics.

152
00:11:54.090 --> 00:11:59.067
That's why you want to understand
the effect of valence.

153
00:11:59.067 --> 00:12:04.361
You also want to understand what do I do
if the graph becomes very, very dense

154
00:12:04.361 --> 00:12:09.840
in a place, so that finding a path through
that dense space becomes very hard.

155
00:12:11.830 --> 00:12:17.532
You will see in a later video that when
this happens, the computer system,

156
00:12:17.532 --> 00:12:23.597
that is trying to process these graphs in
a parallel and distributed way has to do

157
00:12:23.597 --> 00:12:29.507
something special to handle these
increasing density in parts of the graph.