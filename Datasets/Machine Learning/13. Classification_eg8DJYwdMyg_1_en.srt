1
00:00:00,000 --> 00:00:00,790


2
00:00:00,790 --> 00:00:03,129
The following content is
provided under a Creative

3
00:00:03,129 --> 00:00:04,549
Commons license.

4
00:00:04,549 --> 00:00:06,759
Your support will help
MIT OpenCourseWare

5
00:00:06,759 --> 00:00:10,849
continue to offer high quality
educational resources for free.

6
00:00:10,849 --> 00:00:13,390
To make a donation, or to
view additional materials

7
00:00:13,390 --> 00:00:17,320
from hundreds of MIT courses,
visit MIT OpenCourseWare

8
00:00:17,320 --> 00:00:18,269
at ocw.mit.edu.

9
00:00:18,269 --> 00:00:28,430


10
00:00:28,431 --> 00:00:30,380
PROFESSOR: Hello, everybody.

11
00:00:30,379 --> 00:00:35,060
Before we start the material,
a couple of announcements.

12
00:00:35,060 --> 00:00:37,280
As usual, there's some
reading assignments,

13
00:00:37,280 --> 00:00:40,939
and you might be surprised
to see something from Chapter

14
00:00:40,939 --> 00:00:43,369
5 suddenly popping up.

15
00:00:43,369 --> 00:00:45,379
But this is my
relentless attempt

16
00:00:45,380 --> 00:00:47,050
to introduce more Python.

17
00:00:47,049 --> 00:00:51,689
We'll see one new concept later
today, list comprehension.

18
00:00:51,689 --> 00:00:55,649
Today we're going to
look at classification.

19
00:00:55,649 --> 00:00:58,579
And you remember
last, on Monday,

20
00:00:58,579 --> 00:01:01,640
we looked at
unsupervised learning.

21
00:01:01,640 --> 00:01:04,489
Today we're looking at
supervised learning.

22
00:01:04,489 --> 00:01:08,660
It can usually be divided
into two categories.

23
00:01:08,659 --> 00:01:11,939
Regression, where
you try and predict

24
00:01:11,939 --> 00:01:15,420
some real number associated
with the feature vector,

25
00:01:15,420 --> 00:01:18,540
and this is something
we've already done really,

26
00:01:18,540 --> 00:01:22,980
back when we looked at curve
fitting, linear regression

27
00:01:22,980 --> 00:01:24,150
in particular.

28
00:01:24,150 --> 00:01:28,080
It was exactly building a model
that, given some features,

29
00:01:28,079 --> 00:01:30,231
would predict a point.

30
00:01:30,231 --> 00:01:31,689
In this case, it
was pretty simple.

31
00:01:31,689 --> 00:01:33,810
It was given x predict y.

32
00:01:33,810 --> 00:01:38,640
You can imagine generalizing
that to multi dimensions.

33
00:01:38,640 --> 00:01:42,659
Today I'm going to talk
about classification,

34
00:01:42,659 --> 00:01:45,840
which is very common,
in many ways more

35
00:01:45,840 --> 00:01:48,390
common than regression for--

36
00:01:48,390 --> 00:01:50,549
in the machine learning world.

37
00:01:50,549 --> 00:01:55,170
And here the goal is to predict
a discrete value, often called

38
00:01:55,170 --> 00:02:00,420
a label, associated with
some feature vector.

39
00:02:00,420 --> 00:02:04,400
So this is the sort of thing
where you try and, for example,

40
00:02:04,400 --> 00:02:07,550
predict whether a
person will have

41
00:02:07,549 --> 00:02:10,340
an adverse reaction to a drug.

42
00:02:10,340 --> 00:02:12,289
You're not looking
for a real number,

43
00:02:12,289 --> 00:02:17,719
you're looking for will they get
sick, will they not get sick.

44
00:02:17,719 --> 00:02:21,229
Maybe you're trying to predict
the grade in a course A, B, C,

45
00:02:21,229 --> 00:02:25,349
D, and other grades
we won't mention.

46
00:02:25,349 --> 00:02:27,019
Again, those are
labels, so it doesn't

47
00:02:27,020 --> 00:02:32,860
have to be a binary label but
it's a finite number of labels.

48
00:02:32,860 --> 00:02:34,720
So here's an example
to start with.

49
00:02:34,719 --> 00:02:37,580
We won't linger on it too long.

50
00:02:37,580 --> 00:02:40,660
This is basically
something you saw

51
00:02:40,659 --> 00:02:44,469
in an earlier lecture, where
we had a bunch of animals

52
00:02:44,469 --> 00:02:48,069
and a bunch of properties,
and a label identifying

53
00:02:48,069 --> 00:02:49,884
whether or not they
were a reptile.

54
00:02:49,884 --> 00:02:55,810


55
00:02:55,810 --> 00:03:01,640
So we start by building
a distance matrix.

56
00:03:01,639 --> 00:03:07,269
How far apart they are,
an in fact, in this case,

57
00:03:07,270 --> 00:03:11,020
I'm not using the
representation you just saw.

58
00:03:11,020 --> 00:03:15,010
I'm going to use the
binary representation,

59
00:03:15,009 --> 00:03:17,667
As Professor Grimson showed
you, and for the reasons

60
00:03:17,667 --> 00:03:18,250
he showed you.

61
00:03:18,250 --> 00:03:21,240


62
00:03:21,240 --> 00:03:25,320
If you're interested, I didn't
produce this table by hand,

63
00:03:25,319 --> 00:03:28,500
I wrote some Python
code to produce it,

64
00:03:28,500 --> 00:03:30,419
not only to compute
the distances,

65
00:03:30,419 --> 00:03:36,030
but more delicately to
produce the actual table.

66
00:03:36,030 --> 00:03:39,030
And you'll probably find it
instructive at some point

67
00:03:39,030 --> 00:03:41,699
to at least remember
that that code is there,

68
00:03:41,699 --> 00:03:45,909
in case you need to ever
produce a table for some paper.

69
00:03:45,909 --> 00:03:51,099
In general, you probably noticed
I spent relatively little time

70
00:03:51,099 --> 00:03:53,560
going over the actual
vast amounts of codes

71
00:03:53,560 --> 00:03:55,930
we've been posting.

72
00:03:55,930 --> 00:03:58,930
That doesn't mean you
shouldn't look at it.

73
00:03:58,930 --> 00:04:02,379
In part, a lot of
it's there because I'm

74
00:04:02,379 --> 00:04:04,509
hoping at some point in
the future it will be handy

75
00:04:04,509 --> 00:04:08,679
for you to have a model
on how to do something.

76
00:04:08,680 --> 00:04:09,610
All right.

77
00:04:09,610 --> 00:04:12,640
So we have all these distances.

78
00:04:12,639 --> 00:04:18,069
And we can tell how far apart
one animal is from another.

79
00:04:18,069 --> 00:04:22,319
Now how do we use those
to classify animals?

80
00:04:22,319 --> 00:04:25,019
And the simplest approach
to classification,

81
00:04:25,019 --> 00:04:28,319
and it's actually one that's
used a fair amount in practice

82
00:04:28,319 --> 00:04:31,750
is called nearest neighbor.

83
00:04:31,750 --> 00:04:35,139
So the learning part is trivial.

84
00:04:35,139 --> 00:04:39,009
We don't actually learn anything
other than we just remember.

85
00:04:39,009 --> 00:04:42,009
So we remember
the training data.

86
00:04:42,009 --> 00:04:45,639
And when we want to predict
the label of a new example,

87
00:04:45,639 --> 00:04:48,240
we find the nearest example
in the training data,

88
00:04:48,240 --> 00:04:53,030
and just choose the label
associated with that example.

89
00:04:53,029 --> 00:04:55,569
So here I've just
drawing a cloud

90
00:04:55,569 --> 00:04:59,060
of red dots and black dots.

91
00:04:59,060 --> 00:05:02,060
I have a fuschia
colored X. And if I

92
00:05:02,060 --> 00:05:05,230
want to classify
X as black or red,

93
00:05:05,230 --> 00:05:08,100
I'd say well its
nearest neighbor is red.

94
00:05:08,100 --> 00:05:10,010
So we'll call X red.

95
00:05:10,009 --> 00:05:12,649


96
00:05:12,649 --> 00:05:14,209
Doesn't get much
simpler than that.

97
00:05:14,209 --> 00:05:18,781


98
00:05:18,781 --> 00:05:19,280
All right.

99
00:05:19,279 --> 00:05:22,969
Let's try and do it
now for our animals.

100
00:05:22,970 --> 00:05:26,250
I've blocked out this
lower right hand corner,

101
00:05:26,250 --> 00:05:30,259
because I want to classify these
three animals that are in gray.

102
00:05:30,259 --> 00:05:34,310
So my training data, very
small, are these animals.

103
00:05:34,310 --> 00:05:37,170
And these are my test set here.

104
00:05:37,170 --> 00:05:40,980
So let's first try and
classify the zebra.

105
00:05:40,980 --> 00:05:43,710
We look at the zebra's
nearest neighbor.

106
00:05:43,709 --> 00:05:48,229
Well it's either a
guppy or a dart frog.

107
00:05:48,230 --> 00:05:49,430
Well, let's just choose one.

108
00:05:49,430 --> 00:05:51,050
Let's choose the guppy.

109
00:05:51,050 --> 00:05:54,650
And if we look at the
guppy, it's not a reptile,

110
00:05:54,649 --> 00:05:57,779
so we say the zebra
is not a reptile.

111
00:05:57,779 --> 00:05:59,299
So got one right.

112
00:05:59,300 --> 00:06:02,759


113
00:06:02,759 --> 00:06:05,129
Look at the python, choose
its nearest neighbor,

114
00:06:05,129 --> 00:06:06,920
say it's a cobra.

115
00:06:06,920 --> 00:06:09,990
The label associated
with cobra is reptile,

116
00:06:09,990 --> 00:06:12,535
so we win again on the python.

117
00:06:12,535 --> 00:06:16,030


118
00:06:16,029 --> 00:06:22,169
Alligator, it's nearest
neighbor is clearly a chicken.

119
00:06:22,170 --> 00:06:27,770
And so we classify the
alligator as not a reptile.

120
00:06:27,769 --> 00:06:31,449


121
00:06:31,449 --> 00:06:33,399
Oh, dear.

122
00:06:33,399 --> 00:06:34,839
Clearly the wrong answer.

123
00:06:34,839 --> 00:06:38,719


124
00:06:38,720 --> 00:06:39,890
All right.

125
00:06:39,889 --> 00:06:43,539
What might have gone wrong?

126
00:06:43,540 --> 00:06:49,040
Well, the problem with
K nearest neighbors,

127
00:06:49,040 --> 00:06:52,340
we can illustrate it by
looking at this example.

128
00:06:52,339 --> 00:06:55,310
So one of the things people do
with classifiers these days is

129
00:06:55,310 --> 00:06:57,750
handwriting recognition.

130
00:06:57,750 --> 00:07:01,800
So I just copied from a
website a bunch of numbers,

131
00:07:01,800 --> 00:07:06,810
then I wrote the number 40 in
my own inimitable handwriting.

132
00:07:06,810 --> 00:07:09,300
So if we go and we look for,
say, the nearest neighbor

133
00:07:09,300 --> 00:07:10,500
of four--

134
00:07:10,500 --> 00:07:13,019
or sorry, of whatever
that digit is.

135
00:07:13,019 --> 00:07:17,529


136
00:07:17,529 --> 00:07:20,079
It is, I believe, this one.

137
00:07:20,079 --> 00:07:23,639
And sure enough that's
the row of fours.

138
00:07:23,639 --> 00:07:24,810
We're OK on this.

139
00:07:24,810 --> 00:07:27,569


140
00:07:27,569 --> 00:07:32,909
Now if we want to
classify my zero,

141
00:07:32,910 --> 00:07:35,610
the actual nearest
neighbor, in terms

142
00:07:35,610 --> 00:07:39,770
of the bitmaps if you will,
turns out to be this guy.

143
00:07:39,769 --> 00:07:42,240
A very poorly written nine.

144
00:07:42,240 --> 00:07:45,930
I didn't make up this nine,
it was it was already there.

145
00:07:45,930 --> 00:07:50,670
And the problem we see here
when we use nearest neighbor is

146
00:07:50,670 --> 00:07:55,540
if something is noisy, if you
have one noisy piece of data,

147
00:07:55,540 --> 00:07:59,040
in this case, it's rather
ugly looking version of nine,

148
00:07:59,040 --> 00:08:01,170
you can get the wrong
answer because you match it.

149
00:08:01,170 --> 00:08:03,830


150
00:08:03,829 --> 00:08:07,490
And indeed, in this case, you
would get the wrong answer.

151
00:08:07,490 --> 00:08:10,960
What is usually done to
avoid that is something

152
00:08:10,959 --> 00:08:12,939
called K nearest neighbors.

153
00:08:12,939 --> 00:08:16,300


154
00:08:16,300 --> 00:08:19,930
And the basic idea here
is that we don't just

155
00:08:19,930 --> 00:08:22,600
take the nearest
neighbors, we take

156
00:08:22,600 --> 00:08:26,439
some number of nearest
neighbors, usually

157
00:08:26,439 --> 00:08:30,730
an odd number, and we
just let them vote.

158
00:08:30,730 --> 00:08:36,899
So now if we want to
classify this fuchsia X,

159
00:08:36,899 --> 00:08:39,629
and we said K equal to
three, we say well these

160
00:08:39,629 --> 00:08:42,600
are it's three
nearest neighbors.

161
00:08:42,600 --> 00:08:45,570
One is red, two
are black, so we're

162
00:08:45,570 --> 00:08:49,540
going to call X black
is our better guess.

163
00:08:49,539 --> 00:08:51,669
And maybe that actually
is a better guess,

164
00:08:51,669 --> 00:08:54,309
because it looks like this
red point here is really

165
00:08:54,309 --> 00:08:59,319
an outlier, and we don't want
to let the outliers dominate

166
00:08:59,320 --> 00:09:01,450
our classification.

167
00:09:01,450 --> 00:09:05,560
And this is why people almost
always use K nearest neighbors

168
00:09:05,559 --> 00:09:09,409
rather than just
nearest neighbor.

169
00:09:09,409 --> 00:09:14,519
Now if we look at this, and
we use K nearest neighbors,

170
00:09:14,519 --> 00:09:18,269
those are the three nearest
to the first numeral,

171
00:09:18,269 --> 00:09:21,131
and they are all fours.

172
00:09:21,131 --> 00:09:22,839
And if we look at the
K nearest neighbors

173
00:09:22,840 --> 00:09:25,840
for the second numeral,
we still have this nine

174
00:09:25,840 --> 00:09:28,600
but now we have two zeros.

175
00:09:28,600 --> 00:09:32,149
And so we vote and we
decide it's a zero.

176
00:09:32,149 --> 00:09:33,289
Is it infallible?

177
00:09:33,289 --> 00:09:34,129
No.

178
00:09:34,129 --> 00:09:37,129
But it's typically
much more reliable

179
00:09:37,129 --> 00:09:41,620
than just nearest neighbors,
hence used much more often.

180
00:09:41,620 --> 00:09:45,879


181
00:09:45,879 --> 00:09:49,120
And that was our problem, by
the way, with the alligator.

182
00:09:49,120 --> 00:09:51,830
The nearest neighbor
was the chicken,

183
00:09:51,830 --> 00:09:54,170
but if we went back
and looked at it--

184
00:09:54,169 --> 00:09:55,469
maybe we should go do that.

185
00:09:55,470 --> 00:10:01,950


186
00:10:01,950 --> 00:10:04,930
And we take the alligator's
three nearest neighbors,

187
00:10:04,929 --> 00:10:09,870
it would be the chicken, a
cobra, and the rattlesnake--

188
00:10:09,870 --> 00:10:12,120
or the boa, we
don't care, and we

189
00:10:12,120 --> 00:10:15,179
would end up correctly
classifying it now

190
00:10:15,179 --> 00:10:17,069
as a reptile.

191
00:10:17,070 --> 00:10:18,304
Yes?

192
00:10:18,303 --> 00:10:22,221
AUDIENCE: Is there like a
limit to how many [INAUDIBLE]?

193
00:10:22,221 --> 00:10:23,679
PROFESSOR: The
question is is there

194
00:10:23,679 --> 00:10:26,979
a limit to how many nearest
neighbors you'd want?

195
00:10:26,980 --> 00:10:29,560
Absolutely.

196
00:10:29,559 --> 00:10:33,849
Most obviously, there's no point
in setting K equal to-- whoops.

197
00:10:33,850 --> 00:10:36,090
Ooh, on the rebound--

198
00:10:36,090 --> 00:10:40,269
to the size of the training set.

199
00:10:40,269 --> 00:10:42,939
So one of the problems
with K nearest neighbors

200
00:10:42,940 --> 00:10:44,980
is efficiency.

201
00:10:44,980 --> 00:10:47,680
If you're trying to
define K nearest neighbors

202
00:10:47,679 --> 00:10:51,529
and K is bigger,
it takes longer.

203
00:10:51,529 --> 00:10:55,459
So we worry about
how big K should be.

204
00:10:55,460 --> 00:10:58,400
And if we make it too big--

205
00:10:58,399 --> 00:11:00,649
and this is a crucial thing--

206
00:11:00,649 --> 00:11:07,240
we end up getting dominated
by the size of the class.

207
00:11:07,240 --> 00:11:10,649
So let's look at this
picture we had before.

208
00:11:10,649 --> 00:11:14,649
It happens to be more
red dots than black dots.

209
00:11:14,649 --> 00:11:20,439
If I make K 10 or 15, I'm going
to classify a lot of things

210
00:11:20,440 --> 00:11:26,230
as red, just because red is so
much more prevalent than black.

211
00:11:26,230 --> 00:11:29,139
And so when you have an
imbalance, which you usually

212
00:11:29,139 --> 00:11:34,250
do, you have to be very careful
about K. Does that make sense?

213
00:11:34,250 --> 00:11:36,524
AUDIENCE: [INAUDIBLE] choose K?

214
00:11:36,524 --> 00:11:38,709
PROFESSOR: So how
do you choose K?

215
00:11:38,710 --> 00:11:43,780
Remember back on Monday when we
talked about choosing K for K

216
00:11:43,779 --> 00:11:45,899
means clustering?

217
00:11:45,899 --> 00:11:49,740
We typically do a very
similar kind of thing.

218
00:11:49,740 --> 00:11:56,230
We take our training data and
we split it into two parts.

219
00:11:56,230 --> 00:11:58,550
So we have training
and testing, but now

220
00:11:58,549 --> 00:12:01,259
we just take the training,
and we split that

221
00:12:01,259 --> 00:12:05,269
into training and
testing multiple times.

222
00:12:05,269 --> 00:12:08,539
And we experiment with
different K's, and we

223
00:12:08,539 --> 00:12:13,110
see which K's gives us the best
result on the training data.

224
00:12:13,110 --> 00:12:19,190
And then that becomes our K.
And that's a very common method.

225
00:12:19,190 --> 00:12:22,570
It's called
cross-validation, and it's--

226
00:12:22,570 --> 00:12:26,760
for almost all of machine
learning, the algorithms

227
00:12:26,759 --> 00:12:30,960
have parameters in this case,
it's just one parameter, K.

228
00:12:30,960 --> 00:12:34,170
And the way we typically
choose the parameter values

229
00:12:34,169 --> 00:12:37,110
is by searching
through the space using

230
00:12:37,110 --> 00:12:40,720
this cross-validation
in the training data.

231
00:12:40,720 --> 00:12:43,350
Does that makes
sense to everybody?

232
00:12:43,350 --> 00:12:44,480
Great question.

233
00:12:44,480 --> 00:12:46,230
And there was someone
else had a question,

234
00:12:46,230 --> 00:12:47,361
but maybe it was the same.

235
00:12:47,361 --> 00:12:48,569
Do you still have a question?

236
00:12:48,570 --> 00:12:52,310
AUDIENCE: Well, just that
you were using like K nearest

237
00:12:52,309 --> 00:12:54,350
and you get, like
if my K is three

238
00:12:54,350 --> 00:12:56,683
and I get three different
clusters for the K [INAUDIBLE]

239
00:12:56,683 --> 00:12:58,182
PROFESSOR: Three
different clusters?

240
00:12:58,182 --> 00:12:59,125
AUDIENCE: [INAUDIBLE]

241
00:12:59,125 --> 00:13:00,370
PROFESSOR: Well, right.

242
00:13:00,370 --> 00:13:05,250
So if K is 3, and I had
red, black, and purple

243
00:13:05,250 --> 00:13:08,190
and I get one of each,
then what do I do?

244
00:13:08,190 --> 00:13:10,120
And then I'm kind of stuck.

245
00:13:10,120 --> 00:13:13,259
So you need to typically
choose K in such a way

246
00:13:13,259 --> 00:13:16,139
that when you vote
you get a winner.

247
00:13:16,139 --> 00:13:16,669
Nice.

248
00:13:16,669 --> 00:13:19,879
So if there's two, any
odd number will do.

249
00:13:19,879 --> 00:13:22,070
If it's three, well then
you need another number

250
00:13:22,070 --> 00:13:25,410
so that there's some-- so
there's always a majority.

251
00:13:25,409 --> 00:13:27,069
Right?

252
00:13:27,070 --> 00:13:30,920
You want to make sure
that there is a winner.

253
00:13:30,919 --> 00:13:31,939
Also a good question.

254
00:13:31,940 --> 00:13:36,900


255
00:13:36,899 --> 00:13:39,209
Let's see if I get
this to you directly.

256
00:13:39,210 --> 00:13:41,870


257
00:13:41,870 --> 00:13:45,560
I'm much better at
throwing overhand, I guess.

258
00:13:45,559 --> 00:13:46,429
Wow.

259
00:13:46,429 --> 00:13:48,139
Finally got applause
for something.

260
00:13:48,139 --> 00:13:52,769
All right, advantages
and disadvantages KNN?

261
00:13:52,769 --> 00:13:54,929
The learning is
really fast, right?

262
00:13:54,929 --> 00:13:57,120
I just remember everything.

263
00:13:57,120 --> 00:13:59,471
No math is required.

264
00:13:59,471 --> 00:14:00,929
Didn't have to show
you any theory.

265
00:14:00,929 --> 00:14:03,659
Was obviously an idea.

266
00:14:03,659 --> 00:14:06,899
It's easy to explain the method
to somebody, and the results.

267
00:14:06,899 --> 00:14:08,429
Why did I label it black?

268
00:14:08,429 --> 00:14:12,209
Because that's who
it was closest to.

269
00:14:12,210 --> 00:14:15,730
The disadvantages is
it's memory intensive.

270
00:14:15,730 --> 00:14:19,740
If I've got a million examples,
I have to store them all.

271
00:14:19,740 --> 00:14:23,840
And the predictions
can take a long time.

272
00:14:23,840 --> 00:14:27,649
If I have an example and I
want to find its K nearest

273
00:14:27,649 --> 00:14:30,480
neighbors, I'm doing
a lot of comparisons.

274
00:14:30,480 --> 00:14:30,980
Right?

275
00:14:30,980 --> 00:14:33,710
If I have a million
tank training points

276
00:14:33,710 --> 00:14:37,550
I have to compare my
example to all a million.

277
00:14:37,549 --> 00:14:41,099
So I have no real
pre-processing overhead.

278
00:14:41,100 --> 00:14:43,460
But each time I need
to do a classification,

279
00:14:43,460 --> 00:14:46,030
it takes a long time.

280
00:14:46,029 --> 00:14:48,699
Now there are better
algorithms and brute force

281
00:14:48,700 --> 00:14:53,230
that give you approximate
K nearest neighbors.

282
00:14:53,230 --> 00:14:56,759
But on the whole,
it's still not fast.

283
00:14:56,759 --> 00:15:02,919
And we're not getting any
information about what process

284
00:15:02,919 --> 00:15:06,209
might have generated the data.

285
00:15:06,210 --> 00:15:10,290
We don't have a model of the
data in the way we say when

286
00:15:10,289 --> 00:15:13,679
we did our linear regression
for curve fitting,

287
00:15:13,679 --> 00:15:18,279
we had a model for the data that
sort of described the pattern.

288
00:15:18,279 --> 00:15:23,240
We don't get that out
of k nearest neighbors.

289
00:15:23,240 --> 00:15:25,340
I'm going to show you a
different approach where

290
00:15:25,340 --> 00:15:27,180
we do get that.

291
00:15:27,179 --> 00:15:29,539
And I'm going to do it on
a more interesting example

292
00:15:29,539 --> 00:15:32,029
than reptiles.

293
00:15:32,029 --> 00:15:36,230
I apologize to those of
you who are reptologists.

294
00:15:36,230 --> 00:15:40,159
So you probably all
heard of the Titanic.

295
00:15:40,159 --> 00:15:43,669
There was a movie
about it, I'm told.

296
00:15:43,669 --> 00:15:47,610
It was one of the great
sea disasters of all time,

297
00:15:47,610 --> 00:15:50,300
a so-called unsinkable ship--

298
00:15:50,299 --> 00:15:53,059
they had advertised
it as unsinkable--

299
00:15:53,059 --> 00:15:55,024
hit an iceberg and went down.

300
00:15:55,024 --> 00:15:58,759
Of the 1,300
passengers, 812 died.

301
00:15:58,759 --> 00:16:00,828
The crew did way worse.

302
00:16:00,828 --> 00:16:02,870
So at least it looks as
if the curve was actually

303
00:16:02,870 --> 00:16:04,070
pretty heroic.

304
00:16:04,070 --> 00:16:06,530
They had a higher death rate.

305
00:16:06,529 --> 00:16:08,870
So we're going to
use machine learning

306
00:16:08,870 --> 00:16:12,529
to see if we can predict
which passengers survived.

307
00:16:12,529 --> 00:16:15,939


308
00:16:15,940 --> 00:16:17,960
There's an online
database I'm using.

309
00:16:17,960 --> 00:16:20,280
It doesn't have all
1,200 passengers,

310
00:16:20,279 --> 00:16:24,789
but it has information
about 1,046 of them.

311
00:16:24,789 --> 00:16:27,219
Some of them they couldn't
get the information.

312
00:16:27,220 --> 00:16:29,830
Says what cabin class they
were in first, second,

313
00:16:29,830 --> 00:16:33,759
or third, how old they
were, and their gender.

314
00:16:33,759 --> 00:16:36,100
Also has their
name and their home

315
00:16:36,100 --> 00:16:39,450
address and things,
which I'm not using.

316
00:16:39,450 --> 00:16:42,990
We want to use these
features to see

317
00:16:42,990 --> 00:16:46,019
if we can predict
which passengers were

318
00:16:46,019 --> 00:16:50,029
going to survive the disaster.

319
00:16:50,029 --> 00:16:52,870
Well, the first
question is something

320
00:16:52,870 --> 00:16:57,529
that Professor Grimson
alluded to is, is it OK,

321
00:16:57,529 --> 00:16:58,939
just to look at accuracy?

322
00:16:58,940 --> 00:17:03,560
How are we going to evaluate
our machine learning?

323
00:17:03,559 --> 00:17:04,328
And it's not.

324
00:17:04,328 --> 00:17:08,289
If we just predict died
for everybody, well then

325
00:17:08,289 --> 00:17:14,318
we'll be 62% accurate for the
passengers and 76% accurate

326
00:17:14,318 --> 00:17:16,269
for the crew members.

327
00:17:16,269 --> 00:17:18,759
Usually machine
learning, if you're 76%

328
00:17:18,759 --> 00:17:20,710
you say that's not bad.

329
00:17:20,710 --> 00:17:25,328
Well, here I can get that
just by predicting died.

330
00:17:25,328 --> 00:17:30,490
So whenever you have a class
imbalance that much more of one

331
00:17:30,490 --> 00:17:33,960
than the other, accuracy isn't
a particularly meaningful

332
00:17:33,960 --> 00:17:34,460
measure.

333
00:17:34,460 --> 00:17:37,340


334
00:17:37,339 --> 00:17:41,459
I discovered this early on
in my work and medical area.

335
00:17:41,460 --> 00:17:43,549
There are a lot of
diseases that rarely occur,

336
00:17:43,549 --> 00:17:46,970
they occur in say 0.1%
of the population.

337
00:17:46,970 --> 00:17:49,279
And I can build a great
model for predicting it

338
00:17:49,279 --> 00:17:51,500
by just saying,
no, you don't have

339
00:17:51,500 --> 00:17:57,170
it, which will be 0.999%
accurate, but totally useless.

340
00:17:57,170 --> 00:18:00,650


341
00:18:00,650 --> 00:18:02,810
Unfortunately, you do see
people doing that sort

342
00:18:02,809 --> 00:18:04,200
of thing in the literature.

343
00:18:04,200 --> 00:18:06,750


344
00:18:06,750 --> 00:18:10,710
You saw these in an earlier
lecture, just to remind you,

345
00:18:10,710 --> 00:18:15,110
we're going to be
looking at other metrics.

346
00:18:15,109 --> 00:18:18,869
Sensitivity, think
of that as how good

347
00:18:18,869 --> 00:18:22,259
is it at identifying
the positive cases.

348
00:18:22,259 --> 00:18:26,980
In this case, positive
is going to be dead.

349
00:18:26,980 --> 00:18:33,110
How specific is it, and the
positive predictive value.

350
00:18:33,109 --> 00:18:35,819
If we say somebody died,
what's the probability

351
00:18:35,819 --> 00:18:38,172
is that they really did?

352
00:18:38,172 --> 00:18:40,130
And then there's the
negative predictive value.

353
00:18:40,130 --> 00:18:41,900
If we say they
didn't die, what's

354
00:18:41,900 --> 00:18:43,430
the probability they didn't die?

355
00:18:43,430 --> 00:18:46,380


356
00:18:46,380 --> 00:18:50,040
So these are four
very common metrics.

357
00:18:50,039 --> 00:18:54,659
There is something called an
F score that combines them,

358
00:18:54,660 --> 00:18:58,500
but I'm not going to be
showing you that today.

359
00:18:58,500 --> 00:19:00,809
I will mention that
in the literature,

360
00:19:00,809 --> 00:19:04,169
people often use the word
recall to mean sensitivity

361
00:19:04,170 --> 00:19:09,480
or sensitivity I mean recall,
and specificity and precision

362
00:19:09,480 --> 00:19:12,160
are used pretty much
interchangeably.

363
00:19:12,160 --> 00:19:16,080
So you might see various
combinations of these words.

364
00:19:16,079 --> 00:19:18,839
Typically, people talk
about recall n precision

365
00:19:18,839 --> 00:19:22,730
or sensitivity and specificity.

366
00:19:22,730 --> 00:19:24,400
Does that makes
sense, why we want

367
00:19:24,400 --> 00:19:27,009
to look at the measures
other than accuracy?

368
00:19:27,009 --> 00:19:31,329
We will look at accuracy,
too, and how they all tell us

369
00:19:31,329 --> 00:19:34,509
kind of different
things, and how you might

370
00:19:34,509 --> 00:19:37,839
choose a different balance.

371
00:19:37,839 --> 00:19:42,549
For example, if I'm running
a screening test, say

372
00:19:42,549 --> 00:19:47,599
for breast cancer, a
mammogram, and trying

373
00:19:47,599 --> 00:19:49,309
to find the people
who should get on

374
00:19:49,309 --> 00:19:52,579
for a more extensive
examination,

375
00:19:52,579 --> 00:19:55,990
what do I want to
emphasize here?

376
00:19:55,990 --> 00:19:58,609
Which of these is likely
to be the most important?

377
00:19:58,609 --> 00:20:02,049


378
00:20:02,049 --> 00:20:04,750
Or what would you
care about most?

379
00:20:04,750 --> 00:20:08,190


380
00:20:08,190 --> 00:20:10,830
Well, maybe I want sensitivity.

381
00:20:10,829 --> 00:20:15,389
Since I'm going to send this
person on for future tests,

382
00:20:15,390 --> 00:20:19,759
I really don't want to miss
somebody who has cancer,

383
00:20:19,759 --> 00:20:22,579
and so I might
think sensitivity is

384
00:20:22,579 --> 00:20:27,460
more important than specificity
in that particular case.

385
00:20:27,460 --> 00:20:30,720
On the other hand,
if I'm deciding

386
00:20:30,720 --> 00:20:36,710
who is so sick I should do
open heart surgery on them,

387
00:20:36,710 --> 00:20:39,860
maybe I want to be
pretty specific.

388
00:20:39,859 --> 00:20:43,189
Because the risk of the
surgery itself are very high.

389
00:20:43,190 --> 00:20:47,059
I don't want to do it on
people who don't need it.

390
00:20:47,059 --> 00:20:51,529
So we end up having to choose
a balance between these things,

391
00:20:51,529 --> 00:20:53,210
depending upon our application.

392
00:20:53,210 --> 00:20:57,160


393
00:20:57,160 --> 00:21:01,050
The other thing I want to talk
about before actually building

394
00:21:01,049 --> 00:21:07,759
a classifier is how we
test our classifier,

395
00:21:07,759 --> 00:21:09,869
because this is very important.

396
00:21:09,869 --> 00:21:13,189
I'm going to talk about
two different methods,

397
00:21:13,190 --> 00:21:17,150
leave one out class of
testing and repeated

398
00:21:17,150 --> 00:21:21,730
random subsampling.

399
00:21:21,730 --> 00:21:24,779
For leave one out,
it's typically

400
00:21:24,779 --> 00:21:31,139
used when you have a
small number of examples,

401
00:21:31,140 --> 00:21:34,200
so you want as much
training data as possible

402
00:21:34,200 --> 00:21:36,730
as you build your model.

403
00:21:36,730 --> 00:21:41,680
So you take all of your n
examples, remove one of them,

404
00:21:41,680 --> 00:21:45,850
train on n minus
1, test on the 1.

405
00:21:45,849 --> 00:21:49,449
Then you put that 1 back
and remove another 1.

406
00:21:49,450 --> 00:21:53,110
Train on n minus 1, test on 1.

407
00:21:53,109 --> 00:21:56,283
And you do this for each
element of the data,

408
00:21:56,284 --> 00:21:57,700
and then you average
your results.

409
00:21:57,700 --> 00:22:02,670


410
00:22:02,670 --> 00:22:05,490
Repeated random
subsampling is done

411
00:22:05,490 --> 00:22:10,859
when you have a larger set of
data, and there you might say

412
00:22:10,859 --> 00:22:13,729
split your data 80/20.

413
00:22:13,730 --> 00:22:20,130
Take 80% of the data to
train on, test it on 20.

414
00:22:20,130 --> 00:22:23,910
So this is very similar to
what I talked about earlier,

415
00:22:23,910 --> 00:22:26,310
and answered the
question about how

416
00:22:26,309 --> 00:22:32,339
to choose K. I haven't
seen the future examples,

417
00:22:32,339 --> 00:22:35,929
but in order to
believe in my model

418
00:22:35,930 --> 00:22:38,930
and say my parameter
settings, I do this repeated

419
00:22:38,930 --> 00:22:44,090
random subsampling or
leave one out, either one.

420
00:22:44,089 --> 00:22:45,679
There's the code
for leave one out.

421
00:22:45,680 --> 00:22:48,789


422
00:22:48,789 --> 00:22:51,339
Absolutely nothing
interesting about it,

423
00:22:51,339 --> 00:22:54,669
so I'm not going to waste
your time looking at it.

424
00:22:54,670 --> 00:22:57,430


425
00:22:57,430 --> 00:23:04,110
Repeated random subsampling
is a little more interesting.

426
00:23:04,109 --> 00:23:10,639
What I've done here
is I first sample--

427
00:23:10,640 --> 00:23:13,600
this one is just
to splitted 80/20.

428
00:23:13,599 --> 00:23:15,809
It's not doing
anything repeated,

429
00:23:15,809 --> 00:23:27,444
and I start by sampling 20% of
the indices, not the samples.

430
00:23:27,444 --> 00:23:30,039


431
00:23:30,039 --> 00:23:31,690
And I want to do that at random.

432
00:23:31,690 --> 00:23:33,654
I don't want to say
get consecutive ones.

433
00:23:33,654 --> 00:23:37,839


434
00:23:37,839 --> 00:23:42,049
So we do that, and then
once I've got the indices,

435
00:23:42,049 --> 00:23:44,549
I just go through and
assign each example,

436
00:23:44,549 --> 00:23:50,559
to either test or training,
and then return the two sets.

437
00:23:50,559 --> 00:23:54,500
But if I just sort
of sampled one,

438
00:23:54,500 --> 00:23:56,480
then I'd have to do a
more complicated thing

439
00:23:56,480 --> 00:23:57,740
to subtract it from the other.

440
00:23:57,740 --> 00:23:59,779
This is just efficiency.

441
00:23:59,779 --> 00:24:02,369
And then here's the--

442
00:24:02,369 --> 00:24:04,639
sorry about the yellow there--

443
00:24:04,640 --> 00:24:05,520
the random splits.

444
00:24:05,519 --> 00:24:09,109


445
00:24:09,109 --> 00:24:10,819
Obviously, I was
searching for results

446
00:24:10,819 --> 00:24:12,439
when I did my screen capture.

447
00:24:12,440 --> 00:24:15,578


448
00:24:15,578 --> 00:24:17,619
I'm just going to for
range and number of splits,

449
00:24:17,619 --> 00:24:19,616
I'm going to split it 80/20.

450
00:24:19,616 --> 00:24:22,240


451
00:24:22,240 --> 00:24:26,549
It takes a parameter method,
and that's interesting,

452
00:24:26,549 --> 00:24:29,799
and we'll see the
ramifications of that later.

453
00:24:29,799 --> 00:24:32,519
That's going to be the
machine learning method.

454
00:24:32,519 --> 00:24:35,849
We're going to compare KNN
to another method called

455
00:24:35,849 --> 00:24:37,619
logistic regression.

456
00:24:37,619 --> 00:24:41,159
I didn't want to
have to do this code

457
00:24:41,160 --> 00:24:45,259
twice, so I made the
method itself a parameter.

458
00:24:45,259 --> 00:24:47,869
We'll see that introduces
a slight complication,

459
00:24:47,869 --> 00:24:51,139
but we'll get to it
when we get to it.

460
00:24:51,140 --> 00:24:54,090
So I split it, I apply
whatever that method is

461
00:24:54,089 --> 00:25:01,039
the training the test
set, I get the results,

462
00:25:01,039 --> 00:25:05,329
true positive false positive,
true negative false negatives.

463
00:25:05,329 --> 00:25:08,210
And then I call this
thing get stats,

464
00:25:08,210 --> 00:25:11,299
but I'm dividing it by
the number of splits,

465
00:25:11,299 --> 00:25:13,579
so that will give me
the average number

466
00:25:13,579 --> 00:25:18,319
of true positives, the average
number of false positives, etc.

467
00:25:18,319 --> 00:25:22,339
And then I'm just going
to return the average.

468
00:25:22,339 --> 00:25:27,769
Get stats actually just prints
a bunch of statistics for us.

469
00:25:27,769 --> 00:25:29,839
Any questions about
the two methods,

470
00:25:29,839 --> 00:25:32,299
leave one out versus
repeated random sampling?

471
00:25:32,299 --> 00:25:38,690


472
00:25:38,690 --> 00:25:41,870
Let's try it for
KNN on the Titanic.

473
00:25:41,869 --> 00:25:45,119


474
00:25:45,119 --> 00:25:50,399
So I'm not going to show you
the code for K nearest classify.

475
00:25:50,400 --> 00:25:53,160
It's in the code we uploaded.

476
00:25:53,160 --> 00:25:56,519
It takes four arguments
the training set,

477
00:25:56,519 --> 00:26:01,619
the test set, the label that
we're trying to classify.

478
00:26:01,619 --> 00:26:03,269
Are we looking for
the people who died?

479
00:26:03,269 --> 00:26:04,478
Or the people who didn't die?

480
00:26:04,478 --> 00:26:07,410
Are we looking for
reptiles or not reptiles?

481
00:26:07,410 --> 00:26:09,240
Or if case there
were six labels,

482
00:26:09,240 --> 00:26:11,910
which one are we
trying to detect?

483
00:26:11,910 --> 00:26:16,470
And K as in how many
nearest neighbors?

484
00:26:16,470 --> 00:26:18,990
And then it returns the true
positives, the false positives,

485
00:26:18,990 --> 00:26:20,970
the true negatives, and
the false negatives.

486
00:26:20,970 --> 00:26:26,440


487
00:26:26,440 --> 00:26:30,820
Then you'll recall we'd
already looked at lambda

488
00:26:30,819 --> 00:26:32,950
in a different context.

489
00:26:32,950 --> 00:26:41,250
The issue here is K nearest
classify takes four arguments,

490
00:26:41,250 --> 00:26:47,180
yet if we go back here, for
example, to random splits,

491
00:26:47,180 --> 00:26:51,320
what we're seeing is I'm
calling the method with only two

492
00:26:51,319 --> 00:26:53,639
arguments.

493
00:26:53,640 --> 00:26:56,910
Because after all, if I'm not
doing K nearest neighbors,

494
00:26:56,910 --> 00:27:02,120
maybe I don't need to pass
in K. I'm sure I don't.

495
00:27:02,119 --> 00:27:04,069
Different methods will
take different numbers

496
00:27:04,069 --> 00:27:09,919
of parameters, and yet I want
to use the same function here

497
00:27:09,920 --> 00:27:12,630
method.

498
00:27:12,630 --> 00:27:14,760
So the trick I use
to get around that--

499
00:27:14,759 --> 00:27:17,900
and this is a very common
programming trick--

500
00:27:17,900 --> 00:27:18,550
in math.

501
00:27:18,549 --> 00:27:22,379
It's called currying, after
the mathematician Curry,

502
00:27:22,380 --> 00:27:25,990
not the Indian dish.

503
00:27:25,990 --> 00:27:30,519
I'm creating a function a
new function called KNN.

504
00:27:30,519 --> 00:27:33,579
This will be a function of
two arguments, the training

505
00:27:33,579 --> 00:27:36,069
set and the test
set, and it will

506
00:27:36,069 --> 00:27:40,240
be K nearest classifier
with training set and test

507
00:27:40,240 --> 00:27:46,970
set as variables, and
two constants, survived--

508
00:27:46,970 --> 00:27:48,890
so I'm going to
predict who survived--

509
00:27:48,890 --> 00:27:53,420
and 3, the K.

510
00:27:53,420 --> 00:27:56,450
I've been able to turn a
function of four arguments,

511
00:27:56,450 --> 00:28:00,140
K nearest classify, into a
function of two arguments

512
00:28:00,140 --> 00:28:05,570
KNN by using lambda abstraction.

513
00:28:05,569 --> 00:28:09,000
This is something that
people do fairly frequently,

514
00:28:09,000 --> 00:28:12,690
because it lets you build much
more general programs when

515
00:28:12,690 --> 00:28:16,029
you don't have to worry about
the number of arguments.

516
00:28:16,029 --> 00:28:19,500
So it's a good trick to
keeping your bag of tricks.

517
00:28:19,500 --> 00:28:23,109
Again, it's a trick
we've used before.

518
00:28:23,109 --> 00:28:26,740
Then I've just chosen 10
for the number of splits,

519
00:28:26,740 --> 00:28:36,849
and we'll try it, and we'll try
it for both methods of testing.

520
00:28:36,849 --> 00:28:38,990
Any questions before
I run this code?

521
00:28:38,990 --> 00:28:52,720


522
00:28:52,720 --> 00:28:53,308
So here it is.

523
00:28:53,308 --> 00:28:53,849
We'll run it.

524
00:28:53,849 --> 00:28:59,469


525
00:28:59,470 --> 00:29:02,019
Well, I should learn how to
spell finished, shouldn't I?

526
00:29:02,019 --> 00:29:03,049
But that's OK.

527
00:29:03,049 --> 00:29:11,220


528
00:29:11,220 --> 00:29:16,680
Here we have the
results, and they're--

529
00:29:16,680 --> 00:29:18,779
well, what can we
say about them?

530
00:29:18,779 --> 00:29:21,750
They're not much
different to start with,

531
00:29:21,750 --> 00:29:24,630
so it doesn't appear that
our testing methodology had

532
00:29:24,630 --> 00:29:29,640
much of a difference on
how well the KNN worked,

533
00:29:29,640 --> 00:29:33,060
and that's actually
kind of comforting.

534
00:29:33,059 --> 00:29:36,480
The accurate-- none of
the evaluation criteria

535
00:29:36,480 --> 00:29:39,660
are radically different,
so that's kind of good.

536
00:29:39,660 --> 00:29:42,880
We hoped that was true.

537
00:29:42,880 --> 00:29:45,390
The other thing to notice
is that we're actually

538
00:29:45,390 --> 00:29:50,040
doing considerably better than
just always predicting, say,

539
00:29:50,039 --> 00:29:50,744
didn't survive.

540
00:29:50,744 --> 00:29:56,069


541
00:29:56,069 --> 00:29:59,750
We're doing better than
a random prediction.

542
00:29:59,750 --> 00:30:01,616
Let's go back now
to the Power Point.

543
00:30:01,616 --> 00:30:08,075


544
00:30:08,075 --> 00:30:08,950
Here are the results.

545
00:30:08,950 --> 00:30:10,738
We don't need to
study them anymore.

546
00:30:10,738 --> 00:30:14,019


547
00:30:14,019 --> 00:30:18,549
Better than 62% accuracy,
but not much difference

548
00:30:18,549 --> 00:30:21,490
between the experiments.

549
00:30:21,490 --> 00:30:23,769
So that's one method.

550
00:30:23,769 --> 00:30:26,240
Now let's look at
a different method,

551
00:30:26,240 --> 00:30:28,339
and this is probably
the most common method

552
00:30:28,339 --> 00:30:30,289
used in machine learning.

553
00:30:30,289 --> 00:30:34,829
It's called logistic regression.

554
00:30:34,829 --> 00:30:37,799
It's, in some ways, if
you look at it, similar

555
00:30:37,799 --> 00:30:40,200
to a linear regression,
but different

556
00:30:40,200 --> 00:30:41,475
in some important ways.

557
00:30:41,474 --> 00:30:44,490


558
00:30:44,490 --> 00:30:49,900
Linear regression, you
will I'm sure recall,

559
00:30:49,900 --> 00:30:51,870
is designed to
predict a real number.

560
00:30:51,869 --> 00:30:54,919


561
00:30:54,920 --> 00:31:02,220
Now what we want here
is a probability, so

562
00:31:02,220 --> 00:31:04,769
the probability of some event.

563
00:31:04,769 --> 00:31:07,139
We know that the dependent
variable can only

564
00:31:07,140 --> 00:31:17,020
take on a finite set of values,
so we want to predict survived

565
00:31:17,019 --> 00:31:18,819
or didn't survive.

566
00:31:18,819 --> 00:31:23,309
It's no good to say we predict
this person half survived,

567
00:31:23,309 --> 00:31:25,480
you know survived, but is
brain dead or something.

568
00:31:25,480 --> 00:31:27,039
I don't know.

569
00:31:27,039 --> 00:31:29,500
That's not what
we're trying to do.

570
00:31:29,500 --> 00:31:33,369
The problem with just using
regular linear regression

571
00:31:33,369 --> 00:31:37,239
is a lot of time you get
nonsense predictions.

572
00:31:37,240 --> 00:31:41,049
Now you can claim,
OK 0.5 is there,

573
00:31:41,049 --> 00:31:44,859
and it means has a half
probability of dying,

574
00:31:44,859 --> 00:31:47,319
not that half died.

575
00:31:47,319 --> 00:31:49,899
But in fact, if you
look at what goes on,

576
00:31:49,900 --> 00:31:54,740
you could get more
than one or less than 0

577
00:31:54,740 --> 00:31:57,670
out of linear
regression, and that's

578
00:31:57,670 --> 00:32:01,130
nonsense when we're talking
about probabilities.

579
00:32:01,130 --> 00:32:06,520
So we need a different method,
and that's logistic regression.

580
00:32:06,519 --> 00:32:10,420
What logistic
regression does is it

581
00:32:10,420 --> 00:32:14,330
finds what are called the
weights for each feature.

582
00:32:14,329 --> 00:32:17,710
You may recall I complained when
Professor [? Grimson ?] used

583
00:32:17,710 --> 00:32:22,450
the word weights to mean
something somewhat different.

584
00:32:22,450 --> 00:32:27,640
We take each feature, for
example the gender, the cabin

585
00:32:27,640 --> 00:32:37,114
class, the age, and
compute for that weight

586
00:32:37,114 --> 00:32:39,029
that we're going to use
in making predictions.

587
00:32:39,029 --> 00:32:42,379
So think of the weights
as corresponding

588
00:32:42,380 --> 00:32:46,410
to the coefficients we get
when we do a linear regression.

589
00:32:46,410 --> 00:32:51,400
So we have now a coefficient
associated with each variable.

590
00:32:51,400 --> 00:32:53,710
We're going to take
those coefficients,

591
00:32:53,710 --> 00:32:56,710
add them up, multiply
them by something,

592
00:32:56,710 --> 00:32:59,200
and make a prediction.

593
00:32:59,200 --> 00:33:02,819
A positive weight implies--

594
00:33:02,819 --> 00:33:04,869
and I'll come back
to this later--

595
00:33:04,869 --> 00:33:08,529
it almost implies that
the variable is positively

596
00:33:08,529 --> 00:33:11,839
correlated with the outcome.

597
00:33:11,839 --> 00:33:18,029
So we would, for
example, say the

598
00:33:18,029 --> 00:33:20,670
have scales is
positively correlated

599
00:33:20,670 --> 00:33:24,100
with being a reptile.

600
00:33:24,099 --> 00:33:27,819
A negative weight implies that
the variable is negatively

601
00:33:27,819 --> 00:33:32,649
correlated with the
outcome, so number of legs

602
00:33:32,650 --> 00:33:34,840
might have a negative weight.

603
00:33:34,839 --> 00:33:37,329
The more legs an animal
has, the less likely

604
00:33:37,329 --> 00:33:40,149
it is to be a reptile.

605
00:33:40,150 --> 00:33:47,019
It's not absolute, it's
just a correlation.

606
00:33:47,019 --> 00:33:49,389
The absolute
magnitude is related

607
00:33:49,390 --> 00:33:52,230
to the strength of
the correlation,

608
00:33:52,230 --> 00:33:54,230
so if it's being
positive it means

609
00:33:54,230 --> 00:33:55,970
it's a really strong indicator.

610
00:33:55,970 --> 00:33:58,460
If it's big negative,
it's a really strong

611
00:33:58,460 --> 00:33:59,893
negative indicator.

612
00:33:59,893 --> 00:34:04,150


613
00:34:04,150 --> 00:34:07,960
And then we use an
optimization process

614
00:34:07,960 --> 00:34:11,949
to compute these weights
from the training data.

615
00:34:11,949 --> 00:34:13,659
It's a little bit complex.

616
00:34:13,659 --> 00:34:17,110
It's key is the way it uses
the log function, hence

617
00:34:17,110 --> 00:34:21,610
the name logistic, but I'm not
going to make you look at it.

618
00:34:21,610 --> 00:34:24,269


619
00:34:24,269 --> 00:34:28,090
But I will show
you how to use it.

620
00:34:28,090 --> 00:34:31,805
You start by importing something
called sklearn.linear_model.

621
00:34:31,804 --> 00:34:35,138


622
00:34:35,139 --> 00:34:42,300
Sklearn is a Python library,
and in that is a class

623
00:34:42,300 --> 00:34:44,440
called logistic regression.

624
00:34:44,440 --> 00:34:47,329
It's the name of a
class, and here are

625
00:34:47,329 --> 00:34:50,759
three methods of that class.

626
00:34:50,760 --> 00:34:56,610
Fit, which takes a
sequence of feature vectors

627
00:34:56,610 --> 00:34:59,640
and a sequence of
labels and returns

628
00:34:59,639 --> 00:35:05,179
an object of type
logistic regression.

629
00:35:05,179 --> 00:35:09,960
So this is the place where
the optimization is done.

630
00:35:09,960 --> 00:35:13,230
Now all the examples
I'm going to show you,

631
00:35:13,230 --> 00:35:17,500
these two sequences will be--

632
00:35:17,500 --> 00:35:18,219
well all right.

633
00:35:18,219 --> 00:35:20,859
So think of this as the
sequence of feature vectors,

634
00:35:20,860 --> 00:35:25,870
one per passenger, and the
labels associated with those.

635
00:35:25,869 --> 00:35:28,049
So this and this have
to be the same length.

636
00:35:28,050 --> 00:35:33,280


637
00:35:33,280 --> 00:35:37,470
That produces an
object of this type,

638
00:35:37,469 --> 00:35:41,989
and then I can ask for
the coefficients, which

639
00:35:41,989 --> 00:35:47,349
will return the weight of
each variable, each feature.

640
00:35:47,349 --> 00:35:51,319
And then I can
make a prediction,

641
00:35:51,320 --> 00:35:55,039
given a feature vector
returned the probabilities

642
00:35:55,039 --> 00:35:59,119
of different labels.

643
00:35:59,119 --> 00:36:02,549
Let's look at it as an example.

644
00:36:02,550 --> 00:36:03,980
So first let's build the model.

645
00:36:03,980 --> 00:36:06,869


646
00:36:06,869 --> 00:36:09,690
To build the model, we'll take
the examples, the training

647
00:36:09,690 --> 00:36:13,409
data, and I just said whether
we're going to print something.

648
00:36:13,409 --> 00:36:15,599
You'll notice from
this slide I've

649
00:36:15,599 --> 00:36:18,019
elighted the printed stuff.

650
00:36:18,019 --> 00:36:22,090
We'll come back in a later slide
and look at what's in there.

651
00:36:22,090 --> 00:36:24,980
But for now I want to focus on
actually building the model.

652
00:36:24,980 --> 00:36:28,159


653
00:36:28,159 --> 00:36:32,269
I need to create two vectors,
two lists in this case,

654
00:36:32,269 --> 00:36:34,940
the feature vectors
and the labels.

655
00:36:34,940 --> 00:36:36,695
For e in examples,
featurevectors.a

656
00:36:36,695 --> 00:36:40,870
ppend(e.getfeatures
e.getfeatures e.getlabel.

657
00:36:40,869 --> 00:36:45,829
Couldn't be much
simpler than that.

658
00:36:45,829 --> 00:36:50,360
Then, just because it wouldn't
fit on a line on my slide,

659
00:36:50,360 --> 00:36:52,700
I've created this
identifier called

660
00:36:52,699 --> 00:36:56,494
logistic regression,
which is sklearn.linearmo

661
00:36:56,494 --> 00:37:00,009
del.logisticregression.

662
00:37:00,010 --> 00:37:04,340
So this is the thing I
imported, and this is a class,

663
00:37:04,340 --> 00:37:06,890
and now I'll get
a model by first

664
00:37:06,889 --> 00:37:10,670
creating an instance of the
class, logistic regression.

665
00:37:10,670 --> 00:37:13,070
Here I'm getting an
instance, and then I'll

666
00:37:13,070 --> 00:37:16,730
call dot fit with
that instance, passing

667
00:37:16,730 --> 00:37:19,409
it feature vecs and labels.

668
00:37:19,409 --> 00:37:21,659
I now have built a
logistic regression

669
00:37:21,659 --> 00:37:25,259
model, which is simply
a set of weights

670
00:37:25,260 --> 00:37:27,507
for each of the variables.

671
00:37:27,507 --> 00:37:28,215
This makes sense?

672
00:37:28,215 --> 00:37:32,769


673
00:37:32,769 --> 00:37:35,590
Now we're going to
apply the model,

674
00:37:35,590 --> 00:37:39,039
and I think this is the
last piece of Python

675
00:37:39,039 --> 00:37:42,130
I'm going to introduce this
semester, in case you're

676
00:37:42,130 --> 00:37:44,619
tired of learning about Python.

677
00:37:44,619 --> 00:37:48,049
And this is at least
list comprehension.

678
00:37:48,050 --> 00:37:53,140
This is how I'm going to build
my set of test feature vectors.

679
00:37:53,139 --> 00:37:56,469
So before we go and
look at the code,

680
00:37:56,469 --> 00:38:00,689
let's look at how list
comprehension works.

681
00:38:00,690 --> 00:38:04,380
In its simplest form,
says some expression

682
00:38:04,380 --> 00:38:06,840
for some identifier
in some list,

683
00:38:06,840 --> 00:38:14,235
L. It creates a new list by
evaluating this expression Len

684
00:38:14,235 --> 00:38:19,860
(L) times with the ID in
the expression replaced

685
00:38:19,860 --> 00:38:23,400
by each element of
the list L. So let's

686
00:38:23,400 --> 00:38:25,500
look at a simple example.

687
00:38:25,500 --> 00:38:32,150
Here I'm saying L equals x
times x for x in range 10.

688
00:38:32,150 --> 00:38:34,019
What's that going to do?

689
00:38:34,019 --> 00:38:37,653
It's going to,
essentially, create a list.

690
00:38:37,653 --> 00:38:39,070
Think of it as a
list, or at least

691
00:38:39,070 --> 00:38:43,620
a sequence of values, a range
type actually in Python 3--

692
00:38:43,619 --> 00:38:47,199
of values 0 to 9.

693
00:38:47,199 --> 00:38:51,079
It will then create a
list of length 10, where

694
00:38:51,079 --> 00:38:54,259
the first element is
going to be 0 times 0.

695
00:38:54,260 --> 00:38:58,630
The second element
1 times 1, etc.

696
00:38:58,630 --> 00:38:59,820
OK?

697
00:38:59,820 --> 00:39:01,559
So it's a simple
way for me to create

698
00:39:01,559 --> 00:39:05,029
a list that looks like that.

699
00:39:05,030 --> 00:39:12,800
I can be fancier and say for x
times L equals x times x for x

700
00:39:12,800 --> 00:39:15,810
in range 10, and I add and if.

701
00:39:15,809 --> 00:39:20,079
If x mod 2 is equal to 0.

702
00:39:20,079 --> 00:39:22,539
Now instead of returning all--

703
00:39:22,539 --> 00:39:25,880
building a list using
each value in range 10,

704
00:39:25,880 --> 00:39:29,753
it will use only those values
that satisfy that test.

705
00:39:29,753 --> 00:39:34,880


706
00:39:34,880 --> 00:39:37,220
We can go look at what
happens when we run that code.

707
00:39:37,219 --> 00:39:51,699


708
00:39:51,699 --> 00:39:54,609
You can see the first
list is 1 times 1, 2 times

709
00:39:54,610 --> 00:39:57,099
2, et cetera, and
the second list

710
00:39:57,099 --> 00:40:00,819
is much shorter, because I'm
only squaring even numbers.

711
00:40:00,820 --> 00:40:07,059


712
00:40:07,059 --> 00:40:09,279
Well, you can see that
list comprehension gives us

713
00:40:09,280 --> 00:40:13,940
a convenient compact way to
do certain kinds of things.

714
00:40:13,940 --> 00:40:19,460
Like lambda expressions,
they're easy to misuse.

715
00:40:19,460 --> 00:40:22,220
I hate reading code where I
have list comprehensions that

716
00:40:22,219 --> 00:40:26,059
go over multiple lines on
my screen, for example.

717
00:40:26,059 --> 00:40:29,750
So I use it quite a lot
for small things like this.

718
00:40:29,750 --> 00:40:33,110
If it's very large, I
find another way to do it.

719
00:40:33,110 --> 00:40:48,410


720
00:40:48,409 --> 00:40:49,719
Now we can move forward.

721
00:40:49,719 --> 00:40:58,789


722
00:40:58,789 --> 00:41:03,480
In applying the model, I
first build my testing feature

723
00:41:03,480 --> 00:41:07,159
of x, my e.getfeatures
for e in test set,

724
00:41:07,159 --> 00:41:09,289
so that will give me
the features associated

725
00:41:09,289 --> 00:41:11,570
with each element
in the test set.

726
00:41:11,570 --> 00:41:14,930
I could obviously have written
a for loop to do the same thing,

727
00:41:14,929 --> 00:41:18,250
but this was just
a little cooler.

728
00:41:18,250 --> 00:41:22,690
Then we get model.predict
for each of these.

729
00:41:22,690 --> 00:41:28,119
Model.predict_proba is nice in
that I don't have to predict it

730
00:41:28,119 --> 00:41:30,339
for one example at a time.

731
00:41:30,340 --> 00:41:33,880
I can pass it as set of
examples, and what I get back

732
00:41:33,880 --> 00:41:42,890
is a list of predictions,
so that's just convenient.

733
00:41:42,889 --> 00:41:50,420
And then setting these to 0,
and for I in range len of probs,

734
00:41:50,420 --> 00:41:53,280
here a probability of 0.5.

735
00:41:53,280 --> 00:42:00,200
What's that's saying is what I
get out of logistic regression

736
00:42:00,199 --> 00:42:04,569
is a probability of
something having a label.

737
00:42:04,570 --> 00:42:08,950
I then have to build a
classifier, give a threshold.

738
00:42:08,949 --> 00:42:11,649
And here what I've said, if the
probability of it being true

739
00:42:11,650 --> 00:42:14,889
is over a 0.5, call it true.

740
00:42:14,889 --> 00:42:17,650
So if the probability
of survival is over 0.5,

741
00:42:17,650 --> 00:42:19,030
call it survived.

742
00:42:19,030 --> 00:42:22,600
If it's below, call
it not survived.

743
00:42:22,599 --> 00:42:27,429
We'll later see that, again,
setting that probability

744
00:42:27,429 --> 00:42:31,629
is itself an interesting thing,
but the default in most systems

745
00:42:31,630 --> 00:42:34,390
is half, for obvious reasons.

746
00:42:34,389 --> 00:42:38,279


747
00:42:38,280 --> 00:42:41,970
I get my probabilities
for each feature vector,

748
00:42:41,969 --> 00:42:44,819
and then for I in ranged
lens of probabilities,

749
00:42:44,820 --> 00:42:48,840
I'm just testing whether
the predicted label is

750
00:42:48,840 --> 00:42:54,000
the same as the actual label,
and updating true positives,

751
00:42:54,000 --> 00:42:56,940
false positives, true
negatives, and false negatives

752
00:42:56,940 --> 00:42:59,518
accordingly.

753
00:42:59,518 --> 00:43:00,492
So far, so good?

754
00:43:00,492 --> 00:43:05,860


755
00:43:05,860 --> 00:43:09,200
All right, let's
put it all together.

756
00:43:09,199 --> 00:43:13,224
I'm defining something called
LR, for logistic regression.

757
00:43:13,224 --> 00:43:17,719
It takes the training data,
the test data, the probability,

758
00:43:17,719 --> 00:43:21,809
it builds a model, and
then it gets the results

759
00:43:21,809 --> 00:43:24,519
by calling apply model
with the label survived

760
00:43:24,519 --> 00:43:27,840
and whatever this prob was.

761
00:43:27,840 --> 00:43:30,430
Again, we'll do it
for both leave one out

762
00:43:30,429 --> 00:43:34,949
and random splits, and
again for 10 random splits.

763
00:43:34,949 --> 00:44:03,789


764
00:44:03,789 --> 00:44:05,820
You'll notice it actually runs--

765
00:44:05,820 --> 00:44:10,700
maybe you won't notice, but
it does run faster than KNN.

766
00:44:10,699 --> 00:44:13,460
One of the nice things
about logistic regression

767
00:44:13,460 --> 00:44:16,010
is building the
model takes a while,

768
00:44:16,010 --> 00:44:18,590
but once you've got
the model, applying it

769
00:44:18,590 --> 00:44:23,660
to a large number of variables--
feature vectors is fast.

770
00:44:23,659 --> 00:44:25,940
It's independent of the
number of training examples,

771
00:44:25,940 --> 00:44:29,000
because we've got our weights.

772
00:44:29,000 --> 00:44:32,449
So solving the optimization
problem, getting the weights,

773
00:44:32,449 --> 00:44:35,179
depends upon the number
of training examples.

774
00:44:35,179 --> 00:44:39,349
Once we've got the weights, it's
just evaluating a polynomial.

775
00:44:39,349 --> 00:44:42,985
It's very fast, so
that's a nice advantage.

776
00:44:42,985 --> 00:44:46,719


777
00:44:46,719 --> 00:44:47,594
If we look at those--

778
00:44:47,594 --> 00:44:55,169


779
00:44:55,170 --> 00:44:59,289
and we should probably compare
them to our earlier KNN

780
00:44:59,289 --> 00:45:04,559
results, so KNN on the
left, logistic regression

781
00:45:04,559 --> 00:45:06,289
on the right.

782
00:45:06,289 --> 00:45:12,000
And I guess if I look at it, it
looks like logistic regression

783
00:45:12,000 --> 00:45:13,099
did a little bit better.

784
00:45:13,099 --> 00:45:18,099


785
00:45:18,099 --> 00:45:20,579
That's not guaranteed,
but it often

786
00:45:20,579 --> 00:45:25,172
does outperform because it's
more subtle in what it does,

787
00:45:25,172 --> 00:45:26,880
in being able to assign
different weights

788
00:45:26,880 --> 00:45:30,329
to different variables.

789
00:45:30,329 --> 00:45:31,400
It's a little bit better.

790
00:45:31,400 --> 00:45:36,800
That's probably a good
thing, but there's

791
00:45:36,800 --> 00:45:40,039
another reason that's really
important that people prefer

792
00:45:40,039 --> 00:45:42,679
logistic regression,
is it provides

793
00:45:42,679 --> 00:45:46,569
insights about the variables.

794
00:45:46,570 --> 00:45:48,245
We can look at the
feature weights.

795
00:45:48,244 --> 00:45:51,099


796
00:45:51,099 --> 00:45:56,130
This code does that, so remember
we looked at build model

797
00:45:56,130 --> 00:45:58,390
and I left out the printing?

798
00:45:58,389 --> 00:46:01,629
Well here I'm leaving out
everything except the printing.

799
00:46:01,630 --> 00:46:04,900
Same function, but leaving out
everything except the printing.

800
00:46:04,900 --> 00:46:07,410


801
00:46:07,409 --> 00:46:10,250
We can do model
underbar classes,

802
00:46:10,250 --> 00:46:16,110
so model.classes underbar
gives you the classes.

803
00:46:16,110 --> 00:46:19,707
In this case, the classes
are survived, didn't survive.

804
00:46:19,706 --> 00:46:20,789
I forget what I called it.

805
00:46:20,789 --> 00:46:22,199
We'll see.

806
00:46:22,199 --> 00:46:24,269
So I can see what the
classes it's using

807
00:46:24,269 --> 00:46:30,509
are, and then for I in range
len model dot cof underbar,

808
00:46:30,510 --> 00:46:32,910
these are giving the
weights of each variable.

809
00:46:32,909 --> 00:46:36,655
The coefficients, I can
print what they are.

810
00:46:36,655 --> 00:46:39,529


811
00:46:39,530 --> 00:46:41,450
So let's run that
and see what we get.

812
00:46:41,449 --> 00:46:47,889


813
00:46:47,889 --> 00:46:50,940
We get a syntax error
because I turned a comment

814
00:46:50,940 --> 00:46:51,940
into a line of code.

815
00:46:51,940 --> 00:47:03,320


816
00:47:03,320 --> 00:47:08,650
Our model classes are
died and survived,

817
00:47:08,650 --> 00:47:12,460
and for label survived--

818
00:47:12,460 --> 00:47:15,099
what I've done, by the
way, in the representation

819
00:47:15,099 --> 00:47:18,819
is I represented the cabin
class as a binary variable.

820
00:47:18,820 --> 00:47:22,600
It's either 0 or 1, because
it doesn't make sense

821
00:47:22,599 --> 00:47:26,895
to treat them as if they were
really numbers because we don't

822
00:47:26,896 --> 00:47:28,269
know, for example,
the difference

823
00:47:28,269 --> 00:47:31,030
between first and second is
the same as the difference

824
00:47:31,030 --> 00:47:33,050
between second and third.

825
00:47:33,050 --> 00:47:35,570
If we treated the class,
we just said cabin class

826
00:47:35,570 --> 00:47:39,610
and used an integer, implicitly
the learning algorithm

827
00:47:39,610 --> 00:47:42,250
is going to assume that the
difference between 1 and 2

828
00:47:42,250 --> 00:47:44,769
is the same as between 2 and 3.

829
00:47:44,769 --> 00:47:47,320
If you, for example, look at
the prices of these cabins,

830
00:47:47,320 --> 00:47:50,690
you'll see that that's not true.

831
00:47:50,690 --> 00:47:53,119
The difference in an
airplane between economy plus

832
00:47:53,119 --> 00:47:58,039
and economy is way smaller than
between economy plus him first.

833
00:47:58,039 --> 00:48:00,840
Same thing on the Titanic.

834
00:48:00,840 --> 00:48:06,059
But what we see here is
that for the label survived,

835
00:48:06,059 --> 00:48:08,340
pretty good sized
positive weight

836
00:48:08,340 --> 00:48:10,320
for being in first class cabin.

837
00:48:10,320 --> 00:48:13,000


838
00:48:13,000 --> 00:48:14,559
Moderate for being
in the second,

839
00:48:14,559 --> 00:48:18,130
and if you're in the third
class well, tough luck.

840
00:48:18,130 --> 00:48:20,590
So what we see here is
that rich people did better

841
00:48:20,590 --> 00:48:22,180
than the poor people.

842
00:48:22,179 --> 00:48:25,134
Shocking.

843
00:48:25,135 --> 00:48:29,820
If We look at age, we'll see
it's negatively correlated.

844
00:48:29,820 --> 00:48:32,010
What does this mean?

845
00:48:32,010 --> 00:48:34,110
It's not a huge weight,
but it basically

846
00:48:34,110 --> 00:48:39,780
says that if you're older,
the bigger your age,

847
00:48:39,780 --> 00:48:44,769
the less likely you are to
have survived the disaster.

848
00:48:44,769 --> 00:48:47,860
And finally, it
says it's really bad

849
00:48:47,860 --> 00:48:52,329
to be a male, that the men--

850
00:48:52,329 --> 00:48:57,039
being a male was very negatively
correlated with surviving.

851
00:48:57,039 --> 00:49:01,059
We see a nice thing here is
we get these labels, which

852
00:49:01,059 --> 00:49:03,039
we can make sense of.

853
00:49:03,039 --> 00:49:05,079
One more slide
and then I'm done.

854
00:49:05,079 --> 00:49:09,889


855
00:49:09,889 --> 00:49:11,909
These values are
slightly different,

856
00:49:11,909 --> 00:49:15,269
because different randomization,
different example,

857
00:49:15,269 --> 00:49:17,820
but the main point
I want to say is

858
00:49:17,820 --> 00:49:19,830
you have to be a little
bit wary of reading

859
00:49:19,829 --> 00:49:22,289
too much into these weights.

860
00:49:22,289 --> 00:49:26,219
Because not in this example,
but other examples--

861
00:49:26,219 --> 00:49:30,579
well, also in these features
are often correlated,

862
00:49:30,579 --> 00:49:36,210
and if they're
correlated, you run--

863
00:49:36,210 --> 00:49:37,619
actually it's 3:56.

864
00:49:37,619 --> 00:49:40,589
I'm going to explain the
problem with this on Monday

865
00:49:40,590 --> 00:49:42,900
when I have time
to do it properly.

866
00:49:42,900 --> 00:49:45,440
So I'll see you then.

867
00:49:45,440 --> 00:49:53,028


