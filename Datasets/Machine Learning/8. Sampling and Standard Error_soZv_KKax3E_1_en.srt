1
00:00:00,000 --> 00:00:00,790


2
00:00:00,790 --> 00:00:03,129
The following content is
provided under a Creative

3
00:00:03,129 --> 00:00:04,549
Commons license.

4
00:00:04,549 --> 00:00:06,759
Your support will help
MIT OpenCourseWare

5
00:00:06,759 --> 00:00:10,849
continue to offer high quality
educational resources for free.

6
00:00:10,849 --> 00:00:13,390
To make a donation, or to
view additional materials

7
00:00:13,390 --> 00:00:17,320
from hundreds of MIT courses,
visit MIT OpenCourseWare

8
00:00:17,320 --> 00:00:18,570
at ocw.mit.edu.

9
00:00:18,570 --> 00:00:28,870


10
00:00:28,870 --> 00:00:32,359
PROFESSOR: Good
afternoon, everybody.

11
00:00:32,359 --> 00:00:35,170
Welcome to Lecture 8.

12
00:00:35,170 --> 00:00:39,730
So we're now more than
halfway through the lectures.

13
00:00:39,729 --> 00:00:43,989
All right, the topic
of today is sampling.

14
00:00:43,990 --> 00:00:48,760
I want to start by reminding
you about this whole business

15
00:00:48,759 --> 00:00:51,159
of inferential statistics.

16
00:00:51,159 --> 00:00:53,979
We make references
about populations

17
00:00:53,979 --> 00:00:56,799
by examining one or more
random samples drawn

18
00:00:56,799 --> 00:00:59,619
from that population.

19
00:00:59,619 --> 00:01:04,090
We used Monte Carlo simulation
over the last two lectures.

20
00:01:04,090 --> 00:01:06,189
And the key idea
there, as we saw

21
00:01:06,189 --> 00:01:09,159
in trying to find
the value of pi,

22
00:01:09,159 --> 00:01:13,329
was that we can generate
lots of random samples,

23
00:01:13,329 --> 00:01:17,370
and then use them to compute
confidence intervals.

24
00:01:17,370 --> 00:01:20,250
And then we use the
empirical rule to say,

25
00:01:20,250 --> 00:01:23,450
all right, we really
have good reason

26
00:01:23,450 --> 00:01:28,820
to believe that 95% of the
time we run this simulation,

27
00:01:28,819 --> 00:01:32,909
our answer will be
between here and here.

28
00:01:32,909 --> 00:01:37,429
Well, that's all well and good
when we're doing simulations.

29
00:01:37,430 --> 00:01:41,690
But what happens when you to
actually sample something real?

30
00:01:41,689 --> 00:01:43,730
For example, you
run an experiment,

31
00:01:43,730 --> 00:01:46,189
and you get some data points.

32
00:01:46,189 --> 00:01:50,390
And it's too hard to do
it over and over again.

33
00:01:50,390 --> 00:01:51,969
Think about political polls.

34
00:01:51,969 --> 00:01:56,030
Here was an interesting poll.

35
00:01:56,030 --> 00:01:57,859
How were these created?

36
00:01:57,859 --> 00:02:01,010
Not by simulation.

37
00:02:01,010 --> 00:02:04,460
They didn't run 1,000
polls and then compute

38
00:02:04,459 --> 00:02:05,509
the confidence interval.

39
00:02:05,510 --> 00:02:08,539
They ran one poll--

40
00:02:08,538 --> 00:02:12,139
of 835 people, in this case.

41
00:02:12,139 --> 00:02:15,639
And yet they claim to have
a confidence interval.

42
00:02:15,639 --> 00:02:17,599
That's what that
margin of error is.

43
00:02:17,599 --> 00:02:20,239


44
00:02:20,240 --> 00:02:25,439
Obviously they needed that
large confidence interval.

45
00:02:25,439 --> 00:02:26,479
So how is this done?

46
00:02:26,479 --> 00:02:29,939


47
00:02:29,939 --> 00:02:32,969
Backing up for a minute,
let's talk about how sampling

48
00:02:32,969 --> 00:02:37,169
is done when you are not
running a simulation.

49
00:02:37,169 --> 00:02:41,869
You want to do what's called
probability sampling, in which

50
00:02:41,870 --> 00:02:46,250
each member of the population
has a non-zero probability

51
00:02:46,250 --> 00:02:47,615
of being included in a sample.

52
00:02:47,615 --> 00:02:50,150


53
00:02:50,150 --> 00:02:53,539
There are, roughly
speaking, two kinds.

54
00:02:53,539 --> 00:02:55,799
We'll spend, really,
all of our time

55
00:02:55,800 --> 00:02:59,180
on something called
simple random sampling.

56
00:02:59,180 --> 00:03:03,980
And the key idea here is that
each member of the population

57
00:03:03,979 --> 00:03:09,090
has an equal probability of
being chosen in the sample

58
00:03:09,090 --> 00:03:12,629
so there's no bias.

59
00:03:12,629 --> 00:03:14,859
Now, that's not
always appropriate.

60
00:03:14,860 --> 00:03:18,690
I do want to take a
minute to talk about why.

61
00:03:18,689 --> 00:03:25,009
So suppose we wanted to survey
MIT students to find out what

62
00:03:25,009 --> 00:03:27,500
fraction of them are nerds--

63
00:03:27,500 --> 00:03:31,099
which, by the way, I
consider a compliment.

64
00:03:31,099 --> 00:03:34,189
So suppose we wanted to
consider a random sample of 100

65
00:03:34,189 --> 00:03:35,870
students.

66
00:03:35,870 --> 00:03:40,200
We could walk around campus and
choose 100 people at random.

67
00:03:40,199 --> 00:03:43,399
And if 12% of them
were nerds, we

68
00:03:43,400 --> 00:03:46,860
would say 12% of the MIT
undergraduates are nerds--

69
00:03:46,860 --> 00:03:49,860
if 98%, et cetera.

70
00:03:49,860 --> 00:03:53,950
Well, the problem with that
is, let's look at the majors

71
00:03:53,949 --> 00:03:54,544
by school.

72
00:03:54,544 --> 00:03:57,219


73
00:03:57,219 --> 00:04:02,090
This is actually the
majors at MIT by school.

74
00:04:02,090 --> 00:04:06,780
And you can see that they're
not exactly evenly distributed.

75
00:04:06,780 --> 00:04:08,509
And so if you went
around and just

76
00:04:08,509 --> 00:04:11,889
sampled 100 students
at random, there'd

77
00:04:11,889 --> 00:04:14,859
be a reasonably high probability
that they would all be

78
00:04:14,860 --> 00:04:18,480
from engineering and science.

79
00:04:18,480 --> 00:04:22,740
And that might give
you a misleading notion

80
00:04:22,740 --> 00:04:26,220
of the fraction of MIT
students that were nerds,

81
00:04:26,220 --> 00:04:29,800
or it might not.

82
00:04:29,800 --> 00:04:32,980
In such situations we do
something called stratified

83
00:04:32,980 --> 00:04:38,470
sampling, where we partition
the population into subgroups,

84
00:04:38,470 --> 00:04:43,570
and then take a simple random
sample from each subgroup.

85
00:04:43,569 --> 00:04:48,550
And we do that proportional
to the size of the subgroups.

86
00:04:48,550 --> 00:04:50,650
So we would certainly
want to take more students

87
00:04:50,649 --> 00:04:53,829
from engineering than
from architecture.

88
00:04:53,829 --> 00:04:56,139
But we probably want to
make sure we got somebody

89
00:04:56,139 --> 00:04:59,870
from architecture in our sample.

90
00:04:59,870 --> 00:05:03,230
This, by the way, is the way
most political polls are done.

91
00:05:03,230 --> 00:05:04,879
They're stratified.

92
00:05:04,879 --> 00:05:08,360
They say, we want to get
so many rural people,

93
00:05:08,360 --> 00:05:14,150
so many city people, so many
minorities-- things like that.

94
00:05:14,149 --> 00:05:17,719
And in fact, that's probably
where the election recent polls

95
00:05:17,720 --> 00:05:19,280
were all messed up.

96
00:05:19,279 --> 00:05:22,129
They did a very,
retrospectively at least,

97
00:05:22,129 --> 00:05:23,810
a bad job of stratifying.

98
00:05:23,810 --> 00:05:26,439


99
00:05:26,439 --> 00:05:28,540
So we use stratified
sampling when

100
00:05:28,540 --> 00:05:31,510
there are small groups,
subgroups, that we want

101
00:05:31,509 --> 00:05:34,099
to make sure are represented.

102
00:05:34,100 --> 00:05:36,620
And we want to represent them
proportional to their size

103
00:05:36,620 --> 00:05:39,649
in the population.

104
00:05:39,649 --> 00:05:44,870
This can also be used to reduce
the needed size of the sample.

105
00:05:44,870 --> 00:05:47,680
If we wanted to make sure
we got some architecture

106
00:05:47,680 --> 00:05:49,750
students in our
sample, we'd need

107
00:05:49,750 --> 00:05:52,660
to get more than 100
people to start with.

108
00:05:52,660 --> 00:05:56,335
But if we stratify, we
can take fewer samples.

109
00:05:56,334 --> 00:05:58,889


110
00:05:58,889 --> 00:06:01,629
It works well when
you do it properly.

111
00:06:01,629 --> 00:06:04,589
But it can be tricky
to do it properly.

112
00:06:04,589 --> 00:06:08,769
And we are going to stick to
simple random samples here.

113
00:06:08,769 --> 00:06:11,659
All right, let's
look at an example.

114
00:06:11,660 --> 00:06:16,870
So this is a map of temperatures
in the United States.

115
00:06:16,870 --> 00:06:18,699
And so our running
example today will

116
00:06:18,699 --> 00:06:24,909
be sampling to get information
about the average temperatures.

117
00:06:24,910 --> 00:06:27,220
And of course, as you can
see, they're highly variable.

118
00:06:27,220 --> 00:06:31,100
And we live in one
of the cooler areas.

119
00:06:31,100 --> 00:06:34,120
The data we're going
to use is real data--

120
00:06:34,120 --> 00:06:38,680
and it's in the zip file
that I put up for the class--

121
00:06:38,680 --> 00:06:42,310
from the US Centers for
Environmental Information.

122
00:06:42,310 --> 00:06:44,589
And it's got the daily
high and low temperatures

123
00:06:44,589 --> 00:06:49,389
for 21 different American
cities, every day

124
00:06:49,389 --> 00:06:53,089
from 1961 through 2015.

125
00:06:53,089 --> 00:06:55,759
So it's an
interesting data set--

126
00:06:55,759 --> 00:07:01,129
a total of about 422,000
examples in the dataset.

127
00:07:01,129 --> 00:07:03,560
So a fairly good sized dataset.

128
00:07:03,560 --> 00:07:06,170
It's fun to play with.

129
00:07:06,170 --> 00:07:11,210
All right, so we're sort of
in the part of the course

130
00:07:11,209 --> 00:07:14,599
where the next series of
lectures, including today,

131
00:07:14,600 --> 00:07:20,150
is going to be about data
science, how to analyze data.

132
00:07:20,149 --> 00:07:24,679
I always like to start by
actually looking at the data--

133
00:07:24,680 --> 00:07:28,250
not looking at all
421,000 samples,

134
00:07:28,250 --> 00:07:30,379
but giving a plot
to sort of give me

135
00:07:30,379 --> 00:07:34,639
a sense of what the
data looks like.

136
00:07:34,639 --> 00:07:36,709
I'm not going to walk
you through the code that

137
00:07:36,709 --> 00:07:38,449
does this plot.

138
00:07:38,449 --> 00:07:41,719
I do want to point out that
there are two things in it

139
00:07:41,720 --> 00:07:43,550
that we may not
have seen before.

140
00:07:43,550 --> 00:07:47,710


141
00:07:47,709 --> 00:07:50,949
Simply enough, I'm going
to use numpy.std to get

142
00:07:50,949 --> 00:07:55,329
standard deviations instead
of my own code for it.

143
00:07:55,329 --> 00:08:02,259
And random.sample to take
simple random samples

144
00:08:02,259 --> 00:08:04,149
from the population.

145
00:08:04,149 --> 00:08:06,439
random.sample takes
two arguments.

146
00:08:06,439 --> 00:08:10,540
The first is some sort
of a sequence of values.

147
00:08:10,540 --> 00:08:12,700
And the second is an
integer telling you

148
00:08:12,699 --> 00:08:15,310
how many samples you want.

149
00:08:15,310 --> 00:08:18,790
And it returns a list
containing sample size,

150
00:08:18,790 --> 00:08:22,340
randomly chosen
distinct elements.

151
00:08:22,339 --> 00:08:26,509
Distinct elements is important,
because there are two ways

152
00:08:26,509 --> 00:08:28,759
that people do sampling.

153
00:08:28,759 --> 00:08:31,279
You can do sampling
without replacement,

154
00:08:31,279 --> 00:08:33,860
which is what's done here.

155
00:08:33,860 --> 00:08:37,980
You take a sample, and then
it's out of the population.

156
00:08:37,980 --> 00:08:40,279
So you won't draw
it the next time.

157
00:08:40,279 --> 00:08:42,259
Or you can do sampling
with replacement,

158
00:08:42,259 --> 00:08:45,639
which allows you to draw the
same sample multiple times--

159
00:08:45,639 --> 00:08:48,649
the same example multiple times.

160
00:08:48,649 --> 00:08:50,299
We'll see later in
the term that there

161
00:08:50,299 --> 00:08:52,309
are good reasons that
we sometimes prefer

162
00:08:52,309 --> 00:08:55,309
sampling with replacement.

163
00:08:55,309 --> 00:08:57,799
But usually we're doing
sampling without replacement.

164
00:08:57,799 --> 00:08:59,479
And that's what we'll do here.

165
00:08:59,480 --> 00:09:04,539
So we won't get Boston on April
3rd multiple times-- or, not

166
00:09:04,539 --> 00:09:06,560
the same year, at least.

167
00:09:06,561 --> 00:09:07,060
All right.

168
00:09:07,059 --> 00:09:10,629
So here's the histogram
the code produces.

169
00:09:10,629 --> 00:09:12,580
You can run it yourself
now, if you want,

170
00:09:12,580 --> 00:09:15,080
or you can run it later.

171
00:09:15,080 --> 00:09:16,990
And here's what it looks like.

172
00:09:16,990 --> 00:09:24,909
The daily high temperatures, the
mean is 16.3 degrees Celsius.

173
00:09:24,909 --> 00:09:29,209
I sort of vaguely know
what that feels like.

174
00:09:29,210 --> 00:09:32,190
And as you can see, it's kind
of an interesting distribution.

175
00:09:32,190 --> 00:09:34,550
It's not normal.

176
00:09:34,549 --> 00:09:38,069
But it's not that far, right?

177
00:09:38,070 --> 00:09:41,690
We have a little tail of these
cold temperatures on the left.

178
00:09:41,690 --> 00:09:43,380
And it is what it is.

179
00:09:43,379 --> 00:09:44,689
It's not a normal distribution.

180
00:09:44,690 --> 00:09:48,380
And we'll later see that
doesn't really matter.

181
00:09:48,379 --> 00:09:51,049
OK, so this gives me a sense.

182
00:09:51,049 --> 00:09:54,529
The next thing I'll
get is some statistics.

183
00:09:54,529 --> 00:09:59,389
So we know the mean is 16.3
and the standard deviation

184
00:09:59,389 --> 00:10:01,955
is approximately 9.4 degrees.

185
00:10:01,955 --> 00:10:04,660


186
00:10:04,659 --> 00:10:09,259
So if you look at it,
you can believe that.

187
00:10:09,259 --> 00:10:18,189
Well, here's a histogram of
one random sample of size 100.

188
00:10:18,190 --> 00:10:20,080
Looks pretty different,
as you might expect.

189
00:10:20,080 --> 00:10:23,690


190
00:10:23,690 --> 00:10:28,880
Its standard deviation
is 10.4, its mean 17.7.

191
00:10:28,879 --> 00:10:34,059
So even though the figures look
a little different, in fact,

192
00:10:34,059 --> 00:10:37,739
the means and standard
deviations are pretty similar.

193
00:10:37,740 --> 00:10:40,940
If we look at the population
mean and the sample mean--

194
00:10:40,940 --> 00:10:44,740
and I'll try and be careful
to use those terms--

195
00:10:44,740 --> 00:10:45,730
they're not the same.

196
00:10:45,730 --> 00:10:48,750
But they're in
the same ballpark.

197
00:10:48,750 --> 00:10:55,059
And the same is true of the
two standard deviations.

198
00:10:55,059 --> 00:10:58,859
Well, that raises
the question, did

199
00:10:58,860 --> 00:11:04,320
we get lucky or is
something we should expect?

200
00:11:04,320 --> 00:11:09,770
If we draw 100 random
examples, should we

201
00:11:09,769 --> 00:11:15,620
expect them to correspond to
the population as a whole?

202
00:11:15,620 --> 00:11:18,350
And the answer is sometimes
yeah and sometimes no.

203
00:11:18,350 --> 00:11:22,009
And that's one of the issues
I want to explore today.

204
00:11:22,009 --> 00:11:25,700
So one way to see whether
it's a happy accident

205
00:11:25,700 --> 00:11:28,009
is to try it 1,000 times.

206
00:11:28,009 --> 00:11:33,649
We can draw 1,000 samples of
size 100 and plot the results.

207
00:11:33,649 --> 00:11:36,470


208
00:11:36,470 --> 00:11:38,279
Again, I'm not going
to go over the code.

209
00:11:38,279 --> 00:11:40,399
There's something in
that code, as well,

210
00:11:40,399 --> 00:11:42,509
that we haven't seen before.

211
00:11:42,509 --> 00:11:46,610
And that's the ax.vline
plotting command.

212
00:11:46,610 --> 00:11:49,909
V for vertical.

213
00:11:49,909 --> 00:11:52,309
It just, in this case,
will draw a red line--

214
00:11:52,309 --> 00:11:55,209
because I've said
the color is r--

215
00:11:55,210 --> 00:11:57,400
at population mean
on the x-axis.

216
00:11:57,399 --> 00:11:59,919
So just a vertical line.

217
00:11:59,919 --> 00:12:02,360
So that'll just show
us where the mean is.

218
00:12:02,360 --> 00:12:04,240
If we wanted to draw
a horizontal line,

219
00:12:04,240 --> 00:12:07,430
we'd use ax.hline.

220
00:12:07,429 --> 00:12:12,870
Just showing you a couple
of useful functions.

221
00:12:12,870 --> 00:12:17,389
When we try it 1,000 times,
here's what it looks like.

222
00:12:17,389 --> 00:12:22,840
So here we see what we had
originally, same picture

223
00:12:22,840 --> 00:12:24,460
I showed you before.

224
00:12:24,460 --> 00:12:26,530
And here's what we
get when we look

225
00:12:26,529 --> 00:12:30,449
at the means of 100 samples.

226
00:12:30,450 --> 00:12:35,540
So this plot on the
left looks a lot more

227
00:12:35,539 --> 00:12:38,879
like it's a normal distribution
than the one on the right.

228
00:12:38,879 --> 00:12:44,830
Should that surprise
us, or is there

229
00:12:44,830 --> 00:12:49,190
a reason we should have
expected that to happen?

230
00:12:49,190 --> 00:12:50,440
Well, what's the answer?

231
00:12:50,440 --> 00:12:53,790
Someone tell me why we
should have expected it.

232
00:12:53,789 --> 00:12:56,750
It's because of the central
limit theorem, right?

233
00:12:56,750 --> 00:12:59,419
That's exactly what the
central limit theorem

234
00:12:59,419 --> 00:13:01,610
promised us would happen.

235
00:13:01,610 --> 00:13:06,461
And, sure enough, it's
pretty close to normal.

236
00:13:06,461 --> 00:13:07,419
So that's a good thing.

237
00:13:07,419 --> 00:13:12,909


238
00:13:12,909 --> 00:13:15,519
And now if we look
at it, we can see

239
00:13:15,519 --> 00:13:20,699
that the mean of the
sample means is 16.3,

240
00:13:20,700 --> 00:13:28,629
and the standard deviation
of the sample means is 0.94.

241
00:13:28,629 --> 00:13:35,350
So if we go back to
what we saw here,

242
00:13:35,350 --> 00:13:39,250
we see that, actually,
when we run it 1,000 times

243
00:13:39,250 --> 00:13:43,419
and look at the
means, we get very

244
00:13:43,419 --> 00:13:47,740
close to what we had initially.

245
00:13:47,740 --> 00:13:51,289
So, indeed, it's not
a happy accident.

246
00:13:51,289 --> 00:13:54,129
It's something we can
in general expect.

247
00:13:54,129 --> 00:13:58,309


248
00:13:58,309 --> 00:14:03,479
All right, what's the 95%
confidence interval here?

249
00:14:03,480 --> 00:14:11,639
Well, it's going to be 16.28
plus or minus 1.96 times 0.94,

250
00:14:11,639 --> 00:14:14,939
the standard deviation
of the sample means.

251
00:14:14,940 --> 00:14:18,300
And so it tells us that
the confidence interval is,

252
00:14:18,299 --> 00:14:20,759
the mean high
temperature, is somewhere

253
00:14:20,759 --> 00:14:26,289
between 14.5 and 18.1.

254
00:14:26,289 --> 00:14:29,709
Well, that's actually a
pretty big range, right?

255
00:14:29,710 --> 00:14:32,163
It's sort of enough to
where you wear a sweater

256
00:14:32,163 --> 00:14:33,580
or where you don't
wear a sweater.

257
00:14:33,580 --> 00:14:36,910


258
00:14:36,909 --> 00:14:41,529
So the good news is it
includes the population mean.

259
00:14:41,529 --> 00:14:43,419
That's nice.

260
00:14:43,419 --> 00:14:48,789
But the bad news is
it's pretty wide.

261
00:14:48,789 --> 00:14:52,360
Suppose we wanted
it tighter bound.

262
00:14:52,360 --> 00:14:55,180
I said, all right, sure enough,
the central limit theorem

263
00:14:55,179 --> 00:14:58,179
is going to tell me the
mean of the means is

264
00:14:58,179 --> 00:15:06,109
going to give me a good estimate
of the actual population mean.

265
00:15:06,110 --> 00:15:08,300
But I want it tighter bound.

266
00:15:08,299 --> 00:15:10,029
What can I do?

267
00:15:10,029 --> 00:15:15,779
Well, let's think about a
couple of things we could try.

268
00:15:15,779 --> 00:15:22,250
Well, one thing we could think
about is drawing more samples.

269
00:15:22,250 --> 00:15:24,980
Suppose instead
of 1,000 samples,

270
00:15:24,980 --> 00:15:29,500
I'd taken 2,000
or 3,000 samples.

271
00:15:29,500 --> 00:15:31,870
We can ask the question,
would that have given me

272
00:15:31,870 --> 00:15:33,710
a smaller standard deviation?

273
00:15:33,710 --> 00:15:36,149


274
00:15:36,149 --> 00:15:37,940
For those of you who
have not looked ahead,

275
00:15:37,940 --> 00:15:38,930
what do you think?

276
00:15:38,929 --> 00:15:43,139
Who thinks it will give you
a smaller standard deviation?

277
00:15:43,139 --> 00:15:46,199
Who thinks it won't?

278
00:15:46,200 --> 00:15:48,450
And the rest of you
have either looked ahead

279
00:15:48,450 --> 00:15:50,220
or refused to think.

280
00:15:50,220 --> 00:15:53,870
I prefer to believe
you looked ahead.

281
00:15:53,870 --> 00:15:56,350
Well, we can run the experiment.

282
00:15:56,350 --> 00:15:57,350
You can go to the code.

283
00:15:57,350 --> 00:16:00,950
And you'll see that there is
a constant of 1,000, which

284
00:16:00,950 --> 00:16:03,560
you can easily change to 2,000.

285
00:16:03,559 --> 00:16:07,779
And lo and behold, the standard
deviation barely budges.

286
00:16:07,779 --> 00:16:09,829
It got a little bit
bigger, as it happens,

287
00:16:09,830 --> 00:16:12,770
but that's kind of an accident.

288
00:16:12,769 --> 00:16:15,919
It just, more or
less, doesn't change.

289
00:16:15,919 --> 00:16:19,669
And it won't change if I go
to 3,000 or 4,000 or 5,000.

290
00:16:19,669 --> 00:16:22,000
It'll wiggle around.

291
00:16:22,000 --> 00:16:23,149
But it won't help much.

292
00:16:23,149 --> 00:16:26,590


293
00:16:26,590 --> 00:16:30,310
What we can see is doing
that more often is not

294
00:16:30,309 --> 00:16:33,209
going to help.

295
00:16:33,210 --> 00:16:35,220
Suppose we take larger samples?

296
00:16:35,220 --> 00:16:37,779


297
00:16:37,779 --> 00:16:40,480
Is that going to help?

298
00:16:40,480 --> 00:16:43,730
Who thinks that will help?

299
00:16:43,730 --> 00:16:46,539
And who thinks it won't?

300
00:16:46,539 --> 00:16:47,189
OK.

301
00:16:47,190 --> 00:16:51,520
Well, we can again
run the experiment.

302
00:16:51,519 --> 00:16:52,750
I did run the experiment.

303
00:16:52,750 --> 00:16:57,230
I changed the sample
size from 100 to 200.

304
00:16:57,230 --> 00:16:59,680
And, again, you can
run this if you want.

305
00:16:59,679 --> 00:17:01,489
And if you run it,
you'll get a result--

306
00:17:01,490 --> 00:17:05,700
maybe not exactly this, but
something very similar-- that,

307
00:17:05,700 --> 00:17:10,460
indeed, as I increase
the size of the sample

308
00:17:10,460 --> 00:17:12,769
rather than the
number of the samples,

309
00:17:12,769 --> 00:17:18,730
the standard deviation
drops fairly dramatically,

310
00:17:18,730 --> 00:17:26,289
in this case from 0.94 0.66.

311
00:17:26,289 --> 00:17:29,279
So that's a good thing.

312
00:17:29,279 --> 00:17:34,079
I now want to digress a little
bit before we come back to this

313
00:17:34,079 --> 00:17:36,864
and look at how you
can visualize this--

314
00:17:36,864 --> 00:17:38,279
Because this is a
technique you'll

315
00:17:38,279 --> 00:17:41,849
want to use as you write
papers and things like that--

316
00:17:41,849 --> 00:17:46,379
is how do we visualize the
variability of the data?

317
00:17:46,380 --> 00:17:48,910
And it's usually done with
something called an error bar.

318
00:17:48,910 --> 00:17:51,509
You've all seen
these things here.

319
00:17:51,509 --> 00:17:54,960
And this is one I took
from the literature.

320
00:17:54,960 --> 00:18:01,980
This is plotting pulse
rate against how much

321
00:18:01,980 --> 00:18:06,130
exercise you do or how
frequently you exercise.

322
00:18:06,130 --> 00:18:09,130
And what you can see here
is there's definitely

323
00:18:09,130 --> 00:18:14,350
a downward trend suggesting
that the more you exercise,

324
00:18:14,349 --> 00:18:17,779
the lower your
average resting pulse.

325
00:18:17,779 --> 00:18:20,500
That's probably worth knowing.

326
00:18:20,500 --> 00:18:26,380
And these error bars give us
the 95% confidence intervals

327
00:18:26,380 --> 00:18:29,800
for different subpopulations.

328
00:18:29,799 --> 00:18:32,329


329
00:18:32,329 --> 00:18:38,309
And what we can see here is
that some of them overlap.

330
00:18:38,309 --> 00:18:40,989
So, yes, once a fortnight--

331
00:18:40,989 --> 00:18:43,154
two weeks for those of you
who don't speak British--

332
00:18:43,154 --> 00:18:45,980


333
00:18:45,980 --> 00:18:49,670
it does get a little bit
smaller than rarely or never.

334
00:18:49,670 --> 00:18:53,100
But the confidence
interval is very big.

335
00:18:53,099 --> 00:18:56,490
And so maybe we really
shouldn't feel very comfortable

336
00:18:56,490 --> 00:18:59,400
that it would actually help.

337
00:18:59,400 --> 00:19:04,170
The thing we can say is that if
the confidence intervals don't

338
00:19:04,170 --> 00:19:09,240
overlap, we can conclude
that the means are actually

339
00:19:09,240 --> 00:19:12,650
statistically
significantly different,

340
00:19:12,650 --> 00:19:15,780
in this case at the 95% level.

341
00:19:15,779 --> 00:19:19,519
So here we see that
the more than weekly

342
00:19:19,519 --> 00:19:23,869
does not overlap with
the rarely or never.

343
00:19:23,869 --> 00:19:26,739
And from that, we can conclude
that this is actually,

344
00:19:26,740 --> 00:19:29,200
statistically true--

345
00:19:29,200 --> 00:19:31,150
that if you exercise
more than weekly,

346
00:19:31,150 --> 00:19:35,590
your pulse is likely to be
lower than if you don't.

347
00:19:35,589 --> 00:19:39,449
If confidence
intervals do overlap,

348
00:19:39,450 --> 00:19:42,900
you cannot conclude that there
is no statistically significant

349
00:19:42,900 --> 00:19:43,920
difference.

350
00:19:43,920 --> 00:19:45,930
There might be, and
you can use other tests

351
00:19:45,930 --> 00:19:47,799
to find out whether there are.

352
00:19:47,799 --> 00:19:50,190
When they don't overlap,
it's a good thing.

353
00:19:50,190 --> 00:19:52,410
We can conclude
something strong.

354
00:19:52,410 --> 00:19:57,960
When they do overlap, we
need to investigate further.

355
00:19:57,960 --> 00:19:59,460
All right, let's
look at the error

356
00:19:59,460 --> 00:20:01,799
bars for our temperatures.

357
00:20:01,799 --> 00:20:03,964
And again, we can plot
those using something called

358
00:20:03,964 --> 00:20:04,589
pylab.errorbar.

359
00:20:04,589 --> 00:20:14,859
Lab So what it takes is two
values, the usual x-axis

360
00:20:14,859 --> 00:20:25,549
and y-axis, and then
it takes another list

361
00:20:25,549 --> 00:20:28,519
of the same length, or
sequence of the same length,

362
00:20:28,519 --> 00:20:31,450
which is the y errors.

363
00:20:31,450 --> 00:20:34,910
And here I'm just
going to say 1.96

364
00:20:34,910 --> 00:20:38,269
times the standard deviations.

365
00:20:38,269 --> 00:20:39,799
Where these variables
come from you

366
00:20:39,799 --> 00:20:43,339
can tell by looking at the code.

367
00:20:43,339 --> 00:20:46,579
And then I can say
the format, I want

368
00:20:46,579 --> 00:20:51,039
an o to show the mean,
and then a label.

369
00:20:51,039 --> 00:20:54,129
Fmt stands for format.

370
00:20:54,130 --> 00:20:58,390
errorbar has different
keyword arguments than plot.

371
00:20:58,390 --> 00:21:00,310
You'll find that you
look at different ways

372
00:21:00,309 --> 00:21:04,529
like histograms and bar
plots, scatterplots--

373
00:21:04,529 --> 00:21:07,359
they all have different
available keyword arguments.

374
00:21:07,359 --> 00:21:10,299
So you have to look
up each individually.

375
00:21:10,299 --> 00:21:13,230
But other than this,
everything in the code

376
00:21:13,230 --> 00:21:16,750
should look very
familiar to you.

377
00:21:16,750 --> 00:21:19,220
And when I run the
code, I get this.

378
00:21:19,220 --> 00:21:22,850


379
00:21:22,849 --> 00:21:29,000
And so what I've plotted here
is the mean against the sample

380
00:21:29,000 --> 00:21:31,430
size with errorbars.

381
00:21:31,430 --> 00:21:34,250


382
00:21:34,250 --> 00:21:37,559
And 100 trials, in this case.

383
00:21:37,559 --> 00:21:46,529
So what you can see is that,
as the sample size gets bigger,

384
00:21:46,529 --> 00:21:48,139
the errorbars get smaller.

385
00:21:48,140 --> 00:21:52,009


386
00:21:52,009 --> 00:21:56,769
The estimates of the mean don't
necessarily get any better.

387
00:21:56,769 --> 00:22:01,819
In fact, we can look
here, and this is actually

388
00:22:01,819 --> 00:22:05,329
a worse estimate,
relative to the true mean,

389
00:22:05,329 --> 00:22:07,909
than the previous two estimates.

390
00:22:07,910 --> 00:22:10,370
But we can have more
confidence in it.

391
00:22:10,369 --> 00:22:12,829
The same thing we
saw on Monday when

392
00:22:12,829 --> 00:22:16,189
we looked at estimating
pi, dropping more needles

393
00:22:16,190 --> 00:22:19,549
didn't necessarily give us
a more accurate estimate.

394
00:22:19,549 --> 00:22:22,990
But it gave us more
confidence in our estimate.

395
00:22:22,990 --> 00:22:25,749
And the same thing
is happening here.

396
00:22:25,749 --> 00:22:27,289
And we can see that,
steadily, we can

397
00:22:27,289 --> 00:22:28,909
get more and more confidence.

398
00:22:28,910 --> 00:22:31,620


399
00:22:31,619 --> 00:22:39,009
So larger samples
seem to be better.

400
00:22:39,009 --> 00:22:41,009
That's a good thing.

401
00:22:41,009 --> 00:22:46,200
Going from a sample size of
50 to a sample size of 600

402
00:22:46,200 --> 00:22:48,269
reduced the confidence
interval, as you

403
00:22:48,269 --> 00:22:56,799
can see, from a fairly large
confidence interval here,

404
00:22:56,799 --> 00:23:03,430
ran from just below 14 to almost
19, as opposed to 15 and a half

405
00:23:03,430 --> 00:23:04,795
or so to 17.

406
00:23:04,795 --> 00:23:07,940


407
00:23:07,940 --> 00:23:09,620
I said confidence interval here.

408
00:23:09,619 --> 00:23:10,549
I should not have.

409
00:23:10,549 --> 00:23:13,849
I should have said
standard deviations.

410
00:23:13,849 --> 00:23:15,454
That's an error on the slides.

411
00:23:15,454 --> 00:23:18,369


412
00:23:18,369 --> 00:23:20,909
OK, what's the catch?

413
00:23:20,910 --> 00:23:27,990
Well, we're now looking at
100 samples, each of size 600.

414
00:23:27,990 --> 00:23:36,259
So we've looked at a
total of 600,000 examples.

415
00:23:36,259 --> 00:23:38,690
What has this bought us?

416
00:23:38,690 --> 00:23:39,789
Absolutely nothing.

417
00:23:39,789 --> 00:23:42,349


418
00:23:42,349 --> 00:23:45,859
The entire population only
contained about 422,000

419
00:23:45,859 --> 00:23:47,229
samples.

420
00:23:47,230 --> 00:23:51,019
We might as well have
looked at the whole thing,

421
00:23:51,019 --> 00:23:53,589
rather than take a few of them.

422
00:23:53,589 --> 00:23:55,990
So it's like, you might
as well hold an election

423
00:23:55,990 --> 00:24:00,130
rather than ask 800
people a million times

424
00:24:00,130 --> 00:24:03,250
who they're going to vote for.

425
00:24:03,250 --> 00:24:04,029
Sure, it's good.

426
00:24:04,029 --> 00:24:05,259
But it gave us nothing.

427
00:24:05,259 --> 00:24:10,440


428
00:24:10,440 --> 00:24:13,170
Suppose we did it only once.

429
00:24:13,170 --> 00:24:18,150
Suppose we took only one sample,
as we see in political polls.

430
00:24:18,150 --> 00:24:21,470
What can we can
conclude from that?

431
00:24:21,470 --> 00:24:24,910
And the answer is actually
kind of surprising,

432
00:24:24,910 --> 00:24:28,420
how much we can conclude, in
a real mathematical sense,

433
00:24:28,420 --> 00:24:30,350
from one sample.

434
00:24:30,349 --> 00:24:32,439
And, again, this is
thanks to our old friend,

435
00:24:32,440 --> 00:24:35,930
the central limit theorem.

436
00:24:35,930 --> 00:24:39,730
So if you recall the
theorem, it had three parts.

437
00:24:39,730 --> 00:24:43,680
Up till now, we've
exploited the first two.

438
00:24:43,680 --> 00:24:48,990


439
00:24:48,990 --> 00:24:50,690
We've used the fact
that the means will

440
00:24:50,690 --> 00:24:54,200
be normally distributed so that
we could use the empirical rule

441
00:24:54,200 --> 00:24:57,769
to get confidence
intervals, and the fact

442
00:24:57,769 --> 00:25:00,980
that the mean of
the sample means

443
00:25:00,980 --> 00:25:04,420
would be close to the
mean of the population.

444
00:25:04,420 --> 00:25:09,259
Now I want to use the
third piece of it, which

445
00:25:09,259 --> 00:25:12,079
is that the variance
of the sample means

446
00:25:12,079 --> 00:25:15,799
will be close to the variance
of the population divided

447
00:25:15,799 --> 00:25:18,440
by the sample size.

448
00:25:18,440 --> 00:25:20,990
And we're going to use
that to compute something

449
00:25:20,990 --> 00:25:24,650
called the standard error--

450
00:25:24,650 --> 00:25:27,470
formerly the standard
error of the mean.

451
00:25:27,470 --> 00:25:30,740
People often just call
it the standard error.

452
00:25:30,740 --> 00:25:35,029
And I will be,
alas, inconsistent.

453
00:25:35,029 --> 00:25:40,059
I sometimes call it one,
sometimes the other.

454
00:25:40,059 --> 00:25:43,301
It's an incredibly
simple formula.

455
00:25:43,301 --> 00:25:47,139
It says the standard
error is going

456
00:25:47,140 --> 00:25:53,720
to be equal to sigma, where
sigma is the population

457
00:25:53,720 --> 00:26:01,410
standard deviation divided by
the square root of n, which

458
00:26:01,410 --> 00:26:03,060
is going to be the
size of the sample.

459
00:26:03,059 --> 00:26:09,960


460
00:26:09,960 --> 00:26:12,450
And then there's just
this very small function

461
00:26:12,450 --> 00:26:14,880
that implements it.

462
00:26:14,880 --> 00:26:16,740
So we can compute
this thing called

463
00:26:16,740 --> 00:26:21,029
the standard error of the mean
in a very straightforward way.

464
00:26:21,029 --> 00:26:27,579


465
00:26:27,579 --> 00:26:29,429
We can compute it.

466
00:26:29,430 --> 00:26:30,960
But does it work?

467
00:26:30,960 --> 00:26:34,079
What do I mean by work?

468
00:26:34,079 --> 00:26:37,409
I mean, what's the relationship
of the standard error

469
00:26:37,410 --> 00:26:39,690
to the standard deviation?

470
00:26:39,690 --> 00:26:42,059
Because, remember,
that was our goal,

471
00:26:42,059 --> 00:26:46,839
was to understand the
standard deviation so we

472
00:26:46,839 --> 00:26:49,339
could use the empirical rule.

473
00:26:49,339 --> 00:26:52,720
Well, let's test the
standard error of the mean.

474
00:26:52,720 --> 00:26:57,480
So here's a slightly
longer piece of code.

475
00:26:57,480 --> 00:27:00,730
I'm going to look at a bunch
of different sample sizes,

476
00:27:00,730 --> 00:27:10,390
from 25 to 600, 50 trials each.

477
00:27:10,390 --> 00:27:15,970
So getHighs is just a function
that returns the temperatures.

478
00:27:15,970 --> 00:27:17,769
I'm going to get the
standard deviation

479
00:27:17,769 --> 00:27:24,119
of the whole population, then
the standard error of the means

480
00:27:24,119 --> 00:27:29,250
and the sample standard
deviations, both.

481
00:27:29,250 --> 00:27:32,220
And then I'm just going
to go through and run it.

482
00:27:32,220 --> 00:27:35,240
So for size and
sample size, I'm going

483
00:27:35,240 --> 00:27:40,059
to append the standard
error of the mean.

484
00:27:40,059 --> 00:27:43,679
And remember, that uses the
population standard deviation

485
00:27:43,680 --> 00:27:46,769
and the size of the sample.

486
00:27:46,769 --> 00:27:50,029
So I'll compute all the SEMs.

487
00:27:50,029 --> 00:27:53,599
And then I'm going to compute
all the actual standard

488
00:27:53,599 --> 00:27:56,889
deviations, as well.

489
00:27:56,890 --> 00:27:59,270
And then we'll produce
a bunch of plots--

490
00:27:59,269 --> 00:28:02,210
or a plot, actually.

491
00:28:02,210 --> 00:28:05,090
All right, so let's see
what that plot looks like.

492
00:28:05,089 --> 00:28:08,409


493
00:28:08,410 --> 00:28:11,540
Pretty striking.

494
00:28:11,539 --> 00:28:17,059
So we see the blue solid line
is the standard deviation

495
00:28:17,059 --> 00:28:20,549
of the 50 means.

496
00:28:20,549 --> 00:28:26,799
And the red dotted line is the
standard error of the mean.

497
00:28:26,799 --> 00:28:30,970
So we can see, quite strikingly
here, that they really

498
00:28:30,970 --> 00:28:32,650
track each other very well.

499
00:28:32,650 --> 00:28:35,950


500
00:28:35,950 --> 00:28:41,870
And this is saying
that I can anticipate

501
00:28:41,869 --> 00:28:45,109
what the standard deviation
would be by computing

502
00:28:45,109 --> 00:28:47,759
the standard error.

503
00:28:47,759 --> 00:28:51,529
Which is really useful,
because now I have one sample.

504
00:28:51,529 --> 00:28:54,509
I computed standard error.

505
00:28:54,509 --> 00:28:57,089
And I get something
very similar to what

506
00:28:57,089 --> 00:29:00,689
I get of the standard
deviation if I took 50 samples

507
00:29:00,690 --> 00:29:04,710
and looked at the standard
deviation of those 50 samples.

508
00:29:04,710 --> 00:29:09,259
All right, so not obvious that
this would be true, right?

509
00:29:09,259 --> 00:29:12,170
That I could use
this simple formula,

510
00:29:12,170 --> 00:29:14,550
and the two things would
track each other so well.

511
00:29:14,549 --> 00:29:17,730


512
00:29:17,730 --> 00:29:20,680
And it's not a
coincidence, by the way,

513
00:29:20,680 --> 00:29:22,200
that as I get out
here near the end,

514
00:29:22,200 --> 00:29:26,039
they're really lying
on top of each other.

515
00:29:26,039 --> 00:29:29,351
As the sample size
gets much larger,

516
00:29:29,352 --> 00:29:30,435
they really will coincide.

517
00:29:30,434 --> 00:29:33,240


518
00:29:33,240 --> 00:29:36,960
So one, does everyone
understand the difference

519
00:29:36,960 --> 00:29:40,600
between the standard deviation
and the standard error?

520
00:29:40,599 --> 00:29:41,289
No.

521
00:29:41,289 --> 00:29:42,399
OK.

522
00:29:42,400 --> 00:29:44,500
So how do we compute
a standard deviation?

523
00:29:44,500 --> 00:29:48,519
To do that, we have to
look at many samples--

524
00:29:48,519 --> 00:29:50,379
in this case 50--

525
00:29:50,380 --> 00:29:52,540
and we compute
how much variation

526
00:29:52,539 --> 00:29:56,109
there is in those 50 samples.

527
00:29:56,109 --> 00:30:00,129
For the standard error,
we look at one sample,

528
00:30:00,130 --> 00:30:03,340
and we compute this thing
called the standard error.

529
00:30:03,339 --> 00:30:07,569
And we argue that we get the
same number, more or less,

530
00:30:07,569 --> 00:30:12,039
that we would have gotten had we
taken 50 samples or 100 samples

531
00:30:12,039 --> 00:30:15,379
and computed the
standard deviation.

532
00:30:15,380 --> 00:30:19,810
So I can avoid
taking all 50 samples

533
00:30:19,809 --> 00:30:21,700
if my only reason
for doing it was

534
00:30:21,700 --> 00:30:24,410
to get the standard deviation.

535
00:30:24,410 --> 00:30:27,160
I can take one sample
instead and use

536
00:30:27,160 --> 00:30:30,220
the standard error of the mean.

537
00:30:30,220 --> 00:30:32,950
So going back to
my temperature--

538
00:30:32,950 --> 00:30:36,840
instead of having to
look at lots of samples,

539
00:30:36,839 --> 00:30:38,909
I only have to look at one.

540
00:30:38,910 --> 00:30:40,440
And I can get a
confidence interval.

541
00:30:40,440 --> 00:30:42,410
That make sense?

542
00:30:42,410 --> 00:30:44,189
OK.

543
00:30:44,189 --> 00:30:44,855
There's a catch.

544
00:30:44,855 --> 00:30:48,390


545
00:30:48,390 --> 00:30:51,270
Notice that the formula
for the standard error

546
00:30:51,269 --> 00:30:56,099
includes the standard
deviation of the population--

547
00:30:56,099 --> 00:31:00,250
the standard deviation
of the sample.

548
00:31:00,250 --> 00:31:04,180
Well, that's kind of a bummer.

549
00:31:04,180 --> 00:31:06,160
Because how can I get
the standard deviation

550
00:31:06,160 --> 00:31:08,110
of the population
without looking

551
00:31:08,109 --> 00:31:09,646
at the whole population?

552
00:31:09,646 --> 00:31:11,980
And if we're going to look
at the whole population, then

553
00:31:11,980 --> 00:31:15,930
what's the point of
sampling in the first place?

554
00:31:15,930 --> 00:31:19,700
So we have a catch, that
we've got something that's

555
00:31:19,700 --> 00:31:28,730
a really good approximation, but
it uses a value we don't know.

556
00:31:28,730 --> 00:31:31,870
So what should we do about that?

557
00:31:31,869 --> 00:31:39,829
Well, what would be, really,
the only obvious thing to try?

558
00:31:39,829 --> 00:31:42,319
What's our best guess at
the standard deviation

559
00:31:42,319 --> 00:31:45,419
of the population if we have
only one sample to look at?

560
00:31:45,420 --> 00:31:49,070


561
00:31:49,069 --> 00:31:52,000
What would you use?

562
00:31:52,000 --> 00:31:53,819
Somebody?

563
00:31:53,819 --> 00:31:55,669
I know I forgot to
bring the candy today,

564
00:31:55,670 --> 00:31:57,154
so no one wants to
answer any questions.

565
00:31:57,154 --> 00:31:58,899
AUDIENCE: The standard
deviation of the sample?

566
00:31:58,900 --> 00:32:00,900
PROFESSOR: The standard
deviation of the sample.

567
00:32:00,900 --> 00:32:02,220
It's all I got.

568
00:32:02,220 --> 00:32:05,660
So let's ask the question,
how good is that?

569
00:32:05,660 --> 00:32:08,259


570
00:32:08,259 --> 00:32:10,930
Shockingly good.

571
00:32:10,930 --> 00:32:15,140
So I looked at our example
here for the temperatures.

572
00:32:15,140 --> 00:32:17,509
And I'm plotting the
sample standard deviation

573
00:32:17,509 --> 00:32:20,690
versus the population
standard deviation

574
00:32:20,690 --> 00:32:28,470
for different sample sizes,
ranging from 0 to 600 by one,

575
00:32:28,470 --> 00:32:30,100
I think.

576
00:32:30,099 --> 00:32:35,369
So what you can see here is
when the sample size is small,

577
00:32:35,369 --> 00:32:36,439
I'm pretty far off.

578
00:32:36,440 --> 00:32:39,600
I'm off by 14% here.

579
00:32:39,599 --> 00:32:43,179
And I think that's 25.

580
00:32:43,180 --> 00:32:48,250
But when the sample
sizes is larger, say 600,

581
00:32:48,250 --> 00:32:49,839
I'm off by about 2%.

582
00:32:49,839 --> 00:32:56,269


583
00:32:56,269 --> 00:33:01,670
So what we see, at least for
this data set of temperatures--

584
00:33:01,670 --> 00:33:06,500
if the sample size
is large enough,

585
00:33:06,500 --> 00:33:10,509
the sample standard deviation
is a pretty good approximation

586
00:33:10,509 --> 00:33:12,250
of the population
standard deviation.

587
00:33:12,250 --> 00:33:15,170


588
00:33:15,170 --> 00:33:15,670
Well.

589
00:33:15,670 --> 00:33:20,590
Now we should ask the
question, what good is this?

590
00:33:20,589 --> 00:33:26,709
Well, as I said, once the sample
reaches a reasonable size--

591
00:33:26,710 --> 00:33:32,980
and we see here, reasonable is
probably somewhere around 500--

592
00:33:32,980 --> 00:33:35,589
it becomes a good approximation.

593
00:33:35,589 --> 00:33:40,459
But is it true only
for this example?

594
00:33:40,460 --> 00:33:42,500
The fact that it
happened to work

595
00:33:42,500 --> 00:33:46,430
for high temperatures
in the US doesn't mean

596
00:33:46,430 --> 00:33:49,810
that it will always be true.

597
00:33:49,809 --> 00:33:53,429
So there are at least two
things we should consider

598
00:33:53,430 --> 00:33:55,170
to asking the question,
when will this

599
00:33:55,170 --> 00:33:58,410
be true, when won't it be true.

600
00:33:58,410 --> 00:34:04,540
One is, does the distribution
of the population matter?

601
00:34:04,539 --> 00:34:08,650
So here we saw, in
our very first plot,

602
00:34:08,650 --> 00:34:11,050
the distribution of
the high temperatures.

603
00:34:11,050 --> 00:34:17,350
And it was kind of symmetric
around a point-- not perfectly.

604
00:34:17,349 --> 00:34:20,940
But not everything
looks that way, right?

605
00:34:20,940 --> 00:34:22,530
So we should say,
well, suppose we

606
00:34:22,530 --> 00:34:24,690
have a different distribution.

607
00:34:24,690 --> 00:34:30,469
Would that change
this conclusion?

608
00:34:30,469 --> 00:34:32,030
And the other
thing we should ask

609
00:34:32,030 --> 00:34:36,080
is, well, suppose we had a
different sized population.

610
00:34:36,079 --> 00:34:38,559
Suppose instead of
400,000 temperatures

611
00:34:38,559 --> 00:34:41,860
I had 20 million temperatures.

612
00:34:41,860 --> 00:34:45,070
Would I need more than 600
samples for the two things

613
00:34:45,070 --> 00:34:47,530
to be about the same?

614
00:34:47,530 --> 00:34:52,350
Well, let's explore
both of those questions.

615
00:34:52,349 --> 00:34:54,319
First, let's look at
the distributions.

616
00:34:54,320 --> 00:34:58,200
And we'll look at three
common distributions--

617
00:34:58,199 --> 00:35:01,480
a uniform distribution,
a normal distribution,

618
00:35:01,480 --> 00:35:04,820
and an exponential distribution.

619
00:35:04,820 --> 00:35:07,680
And we'll look at each of
them for, what is this,

620
00:35:07,679 --> 00:35:10,980
100,000 points.

621
00:35:10,980 --> 00:35:13,610
So we know we can generate
a uniform distribution

622
00:35:13,610 --> 00:35:17,740
by calling random.random.

623
00:35:17,739 --> 00:35:19,869
Gives me a uniform
distribution of real numbers

624
00:35:19,869 --> 00:35:22,179
between 0 and 1.

625
00:35:22,179 --> 00:35:25,629
We know that we can generate
our normal distribution

626
00:35:25,630 --> 00:35:28,630
by calling random.gauss.

627
00:35:28,630 --> 00:35:32,829
In this case, I'm looking
at it between the mean of 0

628
00:35:32,829 --> 00:35:34,389
and a standard deviation of 1.

629
00:35:34,389 --> 00:35:36,699
But as we saw in
the last lecture,

630
00:35:36,699 --> 00:35:40,009
the shape will be the same,
independent of these values.

631
00:35:40,010 --> 00:35:44,330
And, finally, an
exponential distribution,

632
00:35:44,329 --> 00:35:46,420
which we get by calling
random.expovariate.

633
00:35:46,420 --> 00:35:52,059
Very And this number,
0.5, is something

634
00:35:52,059 --> 00:35:56,650
called lambda, which
has to do with how

635
00:35:56,650 --> 00:35:59,860
quickly the exponential
either decays or goes up,

636
00:35:59,860 --> 00:36:01,990
depending upon which direction.

637
00:36:01,989 --> 00:36:04,569
And I'm not going to give
you the formula for it

638
00:36:04,570 --> 00:36:05,530
at the moment.

639
00:36:05,530 --> 00:36:07,630
But we'll look at the pictures.

640
00:36:07,630 --> 00:36:11,050
And we'll plot each of these
discrete approximations

641
00:36:11,050 --> 00:36:12,370
to these distributions.

642
00:36:12,369 --> 00:36:15,130


643
00:36:15,130 --> 00:36:18,510
So here's what they look like.

644
00:36:18,510 --> 00:36:21,020
Quite different, right?

645
00:36:21,019 --> 00:36:22,849
We've looked at
uniform and we've

646
00:36:22,849 --> 00:36:24,799
looked at Gaussian before.

647
00:36:24,800 --> 00:36:28,840
And here we see an
exponential, which basically

648
00:36:28,840 --> 00:36:32,800
decays and will asymptote
towards zero, never quite

649
00:36:32,800 --> 00:36:35,019
getting there.

650
00:36:35,019 --> 00:36:38,570
But as you can see,
it is certainly not

651
00:36:38,570 --> 00:36:42,019
very symmetric around the mean.

652
00:36:42,019 --> 00:36:47,019
All right, so let's
see what happens.

653
00:36:47,019 --> 00:36:52,739
If we run the experiment on
these three distributions,

654
00:36:52,739 --> 00:36:58,169
each of 100,000 point examples,
and look at different sample

655
00:36:58,170 --> 00:37:00,780
sizes, we actually see
that the difference

656
00:37:00,780 --> 00:37:06,150
between the standard deviation
and the sample standard

657
00:37:06,150 --> 00:37:10,800
deviation of the population
standard deviation

658
00:37:10,800 --> 00:37:11,680
is not the same.

659
00:37:11,679 --> 00:37:14,599


660
00:37:14,599 --> 00:37:18,909
We see, down here--

661
00:37:18,909 --> 00:37:23,139
this looks kind of like
what we saw before.

662
00:37:23,139 --> 00:37:26,509
But the exponential one
is really quite different.

663
00:37:26,510 --> 00:37:29,940


664
00:37:29,940 --> 00:37:33,010
You know, its worst
case is up here at 25.

665
00:37:33,010 --> 00:37:35,520


666
00:37:35,519 --> 00:37:37,780
The normal is about 14.

667
00:37:37,780 --> 00:37:40,560
So that's not too surprising,
since our temperatures

668
00:37:40,559 --> 00:37:42,539
were kind of
normally distributed

669
00:37:42,539 --> 00:37:44,219
when we looked at it.

670
00:37:44,219 --> 00:37:51,351
And the uniform is, initially,
much better an approximation.

671
00:37:51,351 --> 00:37:54,449


672
00:37:54,449 --> 00:37:56,569
And the reason
for this has to do

673
00:37:56,570 --> 00:37:59,539
with a fundamental difference
in these distributions,

674
00:37:59,539 --> 00:38:02,670
something called skew.

675
00:38:02,670 --> 00:38:07,380
Skew is a measure of the
asymmetry of a probability

676
00:38:07,380 --> 00:38:09,610
distribution.

677
00:38:09,610 --> 00:38:15,950
And what we can see here is
that skew actually matters.

678
00:38:15,949 --> 00:38:20,279
The more skew you
have, the more samples

679
00:38:20,280 --> 00:38:25,519
you're going to need to
get a good approximation.

680
00:38:25,519 --> 00:38:28,690
So if the population is
very skewed, very asymmetric

681
00:38:28,690 --> 00:38:31,150
in the distribution, you
need a lot of samples

682
00:38:31,150 --> 00:38:33,700
to figure out what's going on.

683
00:38:33,699 --> 00:38:36,449
If it's very uniform,
as in, for example,

684
00:38:36,449 --> 00:38:41,509
the uniform population, you
need many fewer samples.

685
00:38:41,510 --> 00:38:44,150
OK, so that's an
important thing.

686
00:38:44,150 --> 00:38:47,360
When we go about deciding
how many samples we need,

687
00:38:47,360 --> 00:38:52,809
we need to have some estimate
of the skew in our population.

688
00:38:52,809 --> 00:38:56,079
All right, how about size?

689
00:38:56,079 --> 00:38:59,110
Does size matter?

690
00:38:59,110 --> 00:39:01,660
Shockingly-- at least it
was to me the first time

691
00:39:01,659 --> 00:39:03,625
I looked at this--
the answer is no.

692
00:39:03,625 --> 00:39:09,030


693
00:39:09,030 --> 00:39:11,700
If we look at this-- and I'm
looking just for the uniform

694
00:39:11,699 --> 00:39:15,355
distribution, but we'll see
the same thing for all three--

695
00:39:15,355 --> 00:39:18,990


696
00:39:18,989 --> 00:39:20,509
it more or less doesn't matter.

697
00:39:20,510 --> 00:39:25,730


698
00:39:25,730 --> 00:39:29,000
Quite amazing, right?

699
00:39:29,000 --> 00:39:32,434
If you have a bigger population,
you don't need more samples.

700
00:39:32,434 --> 00:39:35,440


701
00:39:35,440 --> 00:39:40,200
And it's really almost
counterintuitive

702
00:39:40,199 --> 00:39:44,730
to think that you don't need
any more samples to find out

703
00:39:44,730 --> 00:39:48,990
what's going to happen if you
have a million people or 100

704
00:39:48,989 --> 00:39:51,339
million people.

705
00:39:51,340 --> 00:39:54,990
And that's why, when we look
at, say, political polls,

706
00:39:54,989 --> 00:39:57,429
they're amazingly small.

707
00:39:57,429 --> 00:40:00,039
They poll 1,000 people and
claim they're representative

708
00:40:00,039 --> 00:40:01,130
of Massachusetts.

709
00:40:01,130 --> 00:40:05,380


710
00:40:05,380 --> 00:40:08,000
This is good news.

711
00:40:08,000 --> 00:40:11,469
So to estimate the mean
of a population, given

712
00:40:11,469 --> 00:40:16,359
a single sample, we
choose a sample size

713
00:40:16,360 --> 00:40:19,720
based upon some estimate
of skew in the population.

714
00:40:19,719 --> 00:40:22,599


715
00:40:22,599 --> 00:40:25,529
This is important, because
if we get that wrong,

716
00:40:25,530 --> 00:40:29,334
we might choose a sample
size that is too small.

717
00:40:29,333 --> 00:40:30,750
And in some sense,
you always want

718
00:40:30,750 --> 00:40:33,780
to choose the smallest
sample size you can

719
00:40:33,780 --> 00:40:36,870
that will give you an
accurate answer, because it's

720
00:40:36,869 --> 00:40:41,809
more economical to have small
samples than big samples.

721
00:40:41,809 --> 00:40:43,610
And I've been
talking about polls,

722
00:40:43,610 --> 00:40:46,630
but the same is true
in an experiment.

723
00:40:46,630 --> 00:40:48,490
How many pieces of
data do you need

724
00:40:48,489 --> 00:40:52,209
to collect when you run
an experiment in a lab.

725
00:40:52,210 --> 00:40:56,559
And how much will depend,
again, on the skew of the data.

726
00:40:56,559 --> 00:41:00,090
And that will help you decide.

727
00:41:00,090 --> 00:41:03,850
When you know the size,
you choose a random sample

728
00:41:03,849 --> 00:41:04,929
from the population.

729
00:41:04,929 --> 00:41:09,659


730
00:41:09,659 --> 00:41:12,299
Then you compute the mean
and the standard deviation

731
00:41:12,300 --> 00:41:13,280
of that sample.

732
00:41:13,280 --> 00:41:17,350


733
00:41:17,349 --> 00:41:20,679
And then use the standard
deviation of that sample

734
00:41:20,679 --> 00:41:23,929
to estimate the standard error.

735
00:41:23,929 --> 00:41:26,239
And I want to emphasize that
what you're getting here

736
00:41:26,239 --> 00:41:30,109
is an estimate of the standard
error, not the standard error

737
00:41:30,110 --> 00:41:34,340
itself, which would require you
to know the population standard

738
00:41:34,340 --> 00:41:36,260
deviation.

739
00:41:36,260 --> 00:41:40,280
But if you've chosen the
sample size to be appropriate,

740
00:41:40,280 --> 00:41:44,735
this will turn out to
be a good estimate.

741
00:41:44,735 --> 00:41:46,110
And then once
we've done that, we

742
00:41:46,110 --> 00:41:49,300
use the estimated
standard error to generate

743
00:41:49,300 --> 00:41:52,289
confidence intervals
around the sample mean.

744
00:41:52,289 --> 00:41:55,110
And we're done.

745
00:41:55,110 --> 00:41:57,390
Now this works
great when we choose

746
00:41:57,389 --> 00:42:00,299
independent random samples.

747
00:42:00,300 --> 00:42:04,289
And, as we've seen
before, that if you

748
00:42:04,289 --> 00:42:07,739
don't choose
independent samples,

749
00:42:07,739 --> 00:42:10,169
it doesn't work so well.

750
00:42:10,170 --> 00:42:13,740
And, again, this is an issue
where if you assume that,

751
00:42:13,739 --> 00:42:15,959
in an election, each
state is independent

752
00:42:15,960 --> 00:42:20,590
of every other state, and
you'll get the wrong answer,

753
00:42:20,590 --> 00:42:23,320
because they're not.

754
00:42:23,320 --> 00:42:27,360
All right, let's go back
to our temperature example

755
00:42:27,360 --> 00:42:30,910
and pose a simple question.

756
00:42:30,909 --> 00:42:34,159
Are 200 samples enough?

757
00:42:34,159 --> 00:42:35,569
I don't know why I chose 200.

758
00:42:35,570 --> 00:42:36,980
I did.

759
00:42:36,980 --> 00:42:40,280
So we'll do an experiment here.

760
00:42:40,280 --> 00:42:44,280
This is similar to an
experiment we saw on Monday.

761
00:42:44,280 --> 00:42:48,990
So I'm starting with the
number of mistakes I make.

762
00:42:48,989 --> 00:42:50,849
For t in a range
number of trials,

763
00:42:50,849 --> 00:42:54,960
sample will be random.sample of
the temperatures in the sample

764
00:42:54,960 --> 00:42:56,889
size.

765
00:42:56,889 --> 00:42:58,150
This is a key step.

766
00:42:58,150 --> 00:43:01,389


767
00:43:01,389 --> 00:43:04,659
The first time I did
this, I messed it up.

768
00:43:04,659 --> 00:43:07,089
And instead of doing
this very simple thing,

769
00:43:07,090 --> 00:43:10,360
I did a more complicated
thing of just choosing

770
00:43:10,360 --> 00:43:12,370
some point in my
list of temperatures

771
00:43:12,369 --> 00:43:16,949
and taking the next
200 temperatures.

772
00:43:16,949 --> 00:43:18,614
Why did that give
me the wrong answer?

773
00:43:18,614 --> 00:43:21,529


774
00:43:21,530 --> 00:43:24,090
Because it's organized by city.

775
00:43:24,090 --> 00:43:27,350
So if I happen to choose
the first day of Phoenix,

776
00:43:27,349 --> 00:43:30,329
all 200 temperatures
were Phoenix--

777
00:43:30,329 --> 00:43:32,090
which is not a very
good approximation

778
00:43:32,090 --> 00:43:35,318
of the temperature in
the country as a whole.

779
00:43:35,318 --> 00:43:36,110
But this will work.

780
00:43:36,110 --> 00:43:38,539
I'm using random.sample.

781
00:43:38,539 --> 00:43:39,889
I'll then get the sample mean.

782
00:43:39,889 --> 00:43:43,179


783
00:43:43,179 --> 00:43:46,690
Then I'll compute my estimate
of the standard error

784
00:43:46,690 --> 00:43:49,570
by taking that as seen here.

785
00:43:49,570 --> 00:43:55,140
And then if the absolute
value of the population

786
00:43:55,139 --> 00:44:01,509
minus the sample mean is more
than 1.96 standard errors,

787
00:44:01,510 --> 00:44:03,720
I'm going to say I messed up.

788
00:44:03,719 --> 00:44:06,369


789
00:44:06,369 --> 00:44:07,889
It's outside.

790
00:44:07,889 --> 00:44:10,170
And then at the end,
I'm going to look

791
00:44:10,170 --> 00:44:14,280
at the fraction outside the
95% confidence intervals.

792
00:44:14,280 --> 00:44:17,180
And what do I hope
it should print?

793
00:44:17,179 --> 00:44:20,509
What would be the perfect
answer when I run this?

794
00:44:20,510 --> 00:44:26,020


795
00:44:26,019 --> 00:44:28,289
What fraction should
lie outside that?

796
00:44:28,289 --> 00:44:34,050


797
00:44:34,050 --> 00:44:35,820
It's a pretty
simple calculation.

798
00:44:35,820 --> 00:44:44,000


799
00:44:44,000 --> 00:44:46,739
Five, right?

800
00:44:46,739 --> 00:44:49,169
Because if they all
were inside, then

801
00:44:49,170 --> 00:44:52,829
I'm being too conservative
in my interval, right?

802
00:44:52,829 --> 00:44:58,440
I want 5% of the tests to fall
outside the 95% confidence

803
00:44:58,440 --> 00:45:00,500
interval.

804
00:45:00,500 --> 00:45:02,659
If I wanted fewer,
then I would look

805
00:45:02,659 --> 00:45:04,489
at three standard deviations.

806
00:45:04,489 --> 00:45:08,599
Instead of 1.96, then I
would expect less than 1%

807
00:45:08,599 --> 00:45:10,759
to fall outside.

808
00:45:10,760 --> 00:45:13,220
So this is something we have
to always keep in mind when

809
00:45:13,219 --> 00:45:14,989
we do this kind of thing.

810
00:45:14,989 --> 00:45:19,099
If your answer is too
good, you've messed up.

811
00:45:19,099 --> 00:45:22,789
Shouldn't be too bad, but it
shouldn't be too good, either.

812
00:45:22,789 --> 00:45:25,289
That's what probabilities
are all about.

813
00:45:25,289 --> 00:45:28,340
If you called every
election correctly,

814
00:45:28,340 --> 00:45:29,390
then your math is wrong.

815
00:45:29,389 --> 00:45:33,239


816
00:45:33,239 --> 00:45:40,719
Well, when we run this,
we get this lovely answer,

817
00:45:40,719 --> 00:45:44,349
that the fraction outside
the 95% confidence interval

818
00:45:44,349 --> 00:45:49,963
is 0.0511.

819
00:45:49,963 --> 00:45:51,880
That's exactly-- well,
close to what you want.

820
00:45:51,880 --> 00:45:55,240
It's almost exactly 5%.

821
00:45:55,239 --> 00:45:57,539
And if I run it
multiple times, I

822
00:45:57,539 --> 00:45:59,469
get slightly different numbers.

823
00:45:59,469 --> 00:46:03,029
But they're all in that
range, showing that, here,

824
00:46:03,030 --> 00:46:05,690
in fact, it really does work.

825
00:46:05,690 --> 00:46:08,369


826
00:46:08,369 --> 00:46:12,500
So that's what I want to say,
and it's really important,

827
00:46:12,500 --> 00:46:15,019
this notion of the
standard error.

828
00:46:15,019 --> 00:46:17,869
When I talk to other
departments about what

829
00:46:17,869 --> 00:46:22,159
we should cover in 60002,
about the only thing everybody

830
00:46:22,159 --> 00:46:25,219
agrees on was we should
talk about standard error.

831
00:46:25,219 --> 00:46:29,059
So now I hope I have
made everyone happy.

832
00:46:29,059 --> 00:46:32,420
And we will talk
about fitting curves

833
00:46:32,420 --> 00:46:35,340
to experimental data
starting next week.

834
00:46:35,340 --> 00:46:37,930
All right, thanks a lot.

835
00:46:37,929 --> 00:46:44,945


