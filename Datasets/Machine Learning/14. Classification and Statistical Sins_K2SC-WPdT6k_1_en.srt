1
00:00:00,000 --> 00:00:00,790


2
00:00:00,790 --> 00:00:03,129
The following content is
provided under a Creative

3
00:00:03,129 --> 00:00:04,549
Commons license.

4
00:00:04,549 --> 00:00:06,759
Your support will help
MIT OpenCourseWare

5
00:00:06,759 --> 00:00:10,849
continue to offer high quality
educational resources for free.

6
00:00:10,849 --> 00:00:13,390
To make a donation, or to
view additional materials

7
00:00:13,390 --> 00:00:17,320
from hundreds of MIT courses,
visit MIT OpenCourseWare

8
00:00:17,320 --> 00:00:18,570
at ocw.mit.edu.

9
00:00:18,570 --> 00:00:29,589


10
00:00:29,589 --> 00:00:33,320
JOHN GUTTAG: Hello, everybody.

11
00:00:33,320 --> 00:00:36,880
Some announcements.

12
00:00:36,880 --> 00:00:42,310
The last reading assignment of
the semester, at least from us.

13
00:00:42,310 --> 00:00:48,850
Course evaluations are still
available through this Friday.

14
00:00:48,850 --> 00:00:50,530
But only till noon.

15
00:00:50,530 --> 00:00:53,929
Again, I urge you all to do it.

16
00:00:53,929 --> 00:00:58,179
And then finally,
for the final exam,

17
00:00:58,179 --> 00:00:59,890
we're going to be
giving you some code

18
00:00:59,890 --> 00:01:02,799
to study in advance of the exam.

19
00:01:02,799 --> 00:01:05,109
And then we will ask
questions about that code

20
00:01:05,109 --> 00:01:07,480
on the exam itself.

21
00:01:07,480 --> 00:01:10,480
This was described in the
announcement for the exam.

22
00:01:10,480 --> 00:01:14,290
And we will be making this
code available later today.

23
00:01:14,290 --> 00:01:17,109
Now, I would suggest
that you try and get

24
00:01:17,109 --> 00:01:18,609
your heads around it.

25
00:01:18,609 --> 00:01:21,159
If you are confused,
that's a good thing

26
00:01:21,159 --> 00:01:24,849
to talk about in office hours,
to get some help with it,

27
00:01:24,849 --> 00:01:28,159
as opposed to waiting till
20 minutes before the exam

28
00:01:28,159 --> 00:01:29,409
and realizing you're confused.

29
00:01:29,409 --> 00:01:31,939


30
00:01:31,939 --> 00:01:32,719
All right.

31
00:01:32,719 --> 00:01:36,849
I want to pick up where
we left off on Monday.

32
00:01:36,849 --> 00:01:43,849
So you may recall that we
were comparing results of KNN

33
00:01:43,849 --> 00:01:47,969
and logistic regression
on our Titanic data.

34
00:01:47,969 --> 00:01:56,239
And we have this up using 10
80/20 splits for KNN equals 3

35
00:01:56,239 --> 00:02:01,169
and logistic regression
with p equals 0.5.

36
00:02:01,170 --> 00:02:06,799
And what I observed is that
logistic regression happened

37
00:02:06,799 --> 00:02:09,978
to perform slightly
better, but certainly

38
00:02:09,979 --> 00:02:13,810
nothing that you would
choose to write home about.

39
00:02:13,810 --> 00:02:15,343
It's a little bit better.

40
00:02:15,343 --> 00:02:17,134
That isn't to say it
will always be better.

41
00:02:17,134 --> 00:02:19,620
It happens to be here.

42
00:02:19,620 --> 00:02:23,390
But the point I
closed with is one

43
00:02:23,389 --> 00:02:27,679
of the things we care about when
we use machine learning is not

44
00:02:27,680 --> 00:02:31,890
only our ability to make
predictions with the model.

45
00:02:31,889 --> 00:02:37,119
But what can we learn by
studying the model itself?

46
00:02:37,120 --> 00:02:39,629
Remember, the idea is
that the model is somehow

47
00:02:39,629 --> 00:02:43,590
capturing the system
or the process that

48
00:02:43,590 --> 00:02:45,330
generated the data.

49
00:02:45,330 --> 00:02:48,719
And by studying the model we
can learn something useful.

50
00:02:48,719 --> 00:02:51,629


51
00:02:51,629 --> 00:02:55,169
So to do that for
logistic regression,

52
00:02:55,169 --> 00:02:57,389
we begin by looking
at the weights

53
00:02:57,389 --> 00:03:00,399
of the different variables.

54
00:03:00,400 --> 00:03:04,849
And we had this up
in the last slide.

55
00:03:04,849 --> 00:03:09,069
The model classes are
"Died" and "Survived."

56
00:03:09,069 --> 00:03:12,500
For the label Survived,
we said that if you

57
00:03:12,500 --> 00:03:15,879
were in a first-class
cabin, that

58
00:03:15,879 --> 00:03:20,169
had a positive impact on
your survival, a pretty

59
00:03:20,169 --> 00:03:23,739
strong positive impact.

60
00:03:23,740 --> 00:03:27,920
You can't interpret these
weights in and of themselves.

61
00:03:27,919 --> 00:03:32,539
If I said it's 1.6, that
really doesn't mean anything.

62
00:03:32,539 --> 00:03:36,969
So what you have to look at
is the relative weights, not

63
00:03:36,969 --> 00:03:38,080
the absolute weights.

64
00:03:38,080 --> 00:03:42,420
And we see that it's a pretty
strong relative weight.

65
00:03:42,419 --> 00:03:45,939
A second-class cabin also has a
positive weight, in this case,

66
00:03:45,939 --> 00:03:49,229
of 0.46.

67
00:03:49,229 --> 00:03:52,299
So it was indicating
you better had

68
00:03:52,300 --> 00:03:55,870
a better-than-average
chance of surviving,

69
00:03:55,870 --> 00:04:00,409
but much less strong
than a first class.

70
00:04:00,409 --> 00:04:03,990
And if you are one of those poor
people in a third-class cabin,

71
00:04:03,990 --> 00:04:07,170
well, that had a negative
weight on survival.

72
00:04:07,169 --> 00:04:10,639
You were less likely to survive.

73
00:04:10,639 --> 00:04:16,899
Age had a very small effect
here, slightly negative.

74
00:04:16,899 --> 00:04:20,160
What that meant is the older
you were, the less likely

75
00:04:20,160 --> 00:04:21,959
you were to have survived.

76
00:04:21,959 --> 00:04:27,610
But it's a very
small negative value.

77
00:04:27,610 --> 00:04:32,720
The male gender had a relatively
large negative gender,

78
00:04:32,720 --> 00:04:34,280
suggesting that
if you were a male

79
00:04:34,279 --> 00:04:36,649
you were more likely to die
than if you were a female.

80
00:04:36,649 --> 00:04:39,250


81
00:04:39,250 --> 00:04:41,300
This might be true in
the general population,

82
00:04:41,300 --> 00:04:46,009
but it was especially
true on the Titanic.

83
00:04:46,009 --> 00:04:50,360
Finally, I warned you that
while what I just went through

84
00:04:50,360 --> 00:04:54,500
is something you will read in
lots of papers that use machine

85
00:04:54,500 --> 00:04:58,519
learning, you will
hear in lots of talks

86
00:04:58,519 --> 00:05:00,799
about people who have
used machine learning.

87
00:05:00,800 --> 00:05:04,610
But you should be very wary
when people speak that way.

88
00:05:04,610 --> 00:05:09,290
It's not nonsense, but
some cautionary notes.

89
00:05:09,290 --> 00:05:12,200
In particular,
there's a big issue

90
00:05:12,199 --> 00:05:16,949
because the features are often
correlated with one another.

91
00:05:16,949 --> 00:05:20,344
And so you can't interpret the
weights one feature at a time.

92
00:05:20,345 --> 00:05:23,380


93
00:05:23,379 --> 00:05:25,980
To get a little bit
technical, there

94
00:05:25,980 --> 00:05:31,410
are two major ways people
use logistic regression.

95
00:05:31,410 --> 00:05:33,180
They're called L1 and L2.

96
00:05:33,180 --> 00:05:36,129


97
00:05:36,129 --> 00:05:36,959
We used an L2.

98
00:05:36,959 --> 00:05:38,609
I'll come back to
that in a minute.

99
00:05:38,610 --> 00:05:44,460
Because that's the default
in Python, or in [INAUDIBLE].

100
00:05:44,459 --> 00:05:49,289
You can set that parameter at L2
and do that to L1 if you want.

101
00:05:49,290 --> 00:05:50,460
I experimented with it.

102
00:05:50,459 --> 00:05:53,579
It didn't change the
results that much.

103
00:05:53,579 --> 00:05:56,759
But what an L1 regression
is designed to do

104
00:05:56,759 --> 00:05:59,594
is to find some weights
and drive them to 0.

105
00:05:59,595 --> 00:06:03,200


106
00:06:03,199 --> 00:06:05,149
This is particularly
useful when you

107
00:06:05,149 --> 00:06:10,159
have a very high-dimensional
problem relative to the number

108
00:06:10,160 --> 00:06:11,840
of examples.

109
00:06:11,839 --> 00:06:14,029
And this gets back
to that question

110
00:06:14,029 --> 00:06:17,649
we've talked about many
times, of overfitting.

111
00:06:17,649 --> 00:06:23,289
If you've got 1,000
variables and 1,000 examples,

112
00:06:23,290 --> 00:06:24,564
you're very likely to overfit.

113
00:06:24,564 --> 00:06:27,329


114
00:06:27,329 --> 00:06:30,269
L1 is designed to
avoid overfitting

115
00:06:30,269 --> 00:06:33,449
by taking many of
those 1,000 variables

116
00:06:33,449 --> 00:06:36,469
and just giving them 0 weight.

117
00:06:36,470 --> 00:06:41,030
And it does typically
generalize better.

118
00:06:41,029 --> 00:06:45,000
But if you have two variables
that are correlated,

119
00:06:45,000 --> 00:06:47,160
L1 will drive 1
to 0, and it will

120
00:06:47,160 --> 00:06:49,602
look like it's unimportant.

121
00:06:49,601 --> 00:06:51,060
But in fact, it
might be important.

122
00:06:51,060 --> 00:06:53,370
It's just correlated
with another, which

123
00:06:53,370 --> 00:06:56,879
has gotten all the credit.

124
00:06:56,879 --> 00:07:00,879
L2, which is what we
did, does the opposite.

125
00:07:00,879 --> 00:07:04,550
Is spreads the weight
across the variables.

126
00:07:04,550 --> 00:07:07,300
So have a bunch of
correlated variables,

127
00:07:07,300 --> 00:07:10,060
it might look like none of
them are very important.

128
00:07:10,060 --> 00:07:14,230
Because each of them gets a
small amount of the weight.

129
00:07:14,230 --> 00:07:17,350
Again, not so important when
you have four or five variables,

130
00:07:17,350 --> 00:07:18,850
is what I'm showing you.

131
00:07:18,850 --> 00:07:24,180
But it matters when you
have 100 or 1,000 variables.

132
00:07:24,180 --> 00:07:27,110
Let's look at an example.

133
00:07:27,110 --> 00:07:36,410
So the cabin classes, the way we
set it up, c1 plus c2 plus c3--

134
00:07:36,410 --> 00:07:38,600
whoops-- is not equal to 0.

135
00:07:38,600 --> 00:07:42,000
What is it equal to?

136
00:07:42,000 --> 00:07:45,915
I'll fix this right now.

137
00:07:45,915 --> 00:07:47,040
What should that have said?

138
00:07:47,040 --> 00:07:51,720


139
00:07:51,720 --> 00:07:55,040
What's the invariant here?

140
00:07:55,040 --> 00:07:58,010
Well, a person is in
exactly one class.

141
00:07:58,009 --> 00:07:59,509
I guess if you're
really rich, maybe

142
00:07:59,509 --> 00:08:03,589
you rented two cabins, one
in first and one in second.

143
00:08:03,589 --> 00:08:05,109
But probably not.

144
00:08:05,110 --> 00:08:08,900
Or if you did, you put your
servants in second or third.

145
00:08:08,899 --> 00:08:11,289
But what does this
got to add up to?

146
00:08:11,290 --> 00:08:11,790
Yeah?

147
00:08:11,790 --> 00:08:12,289
AUDIENCE: 1.

148
00:08:12,288 --> 00:08:14,043
JOHN GUTTAG: Has to add up to 1.

149
00:08:14,043 --> 00:08:15,012
Thank you.

150
00:08:15,012 --> 00:08:18,889


151
00:08:18,889 --> 00:08:20,240
So it adds up to 1.

152
00:08:20,240 --> 00:08:24,139


153
00:08:24,139 --> 00:08:25,899
Whoa.

154
00:08:25,899 --> 00:08:28,989
Got his attention, at least.

155
00:08:28,990 --> 00:08:33,250
So what this tells us is the
values are not independent.

156
00:08:33,250 --> 00:08:37,590
Because if c1 is 1, then
c2 and c3 must be 0.

157
00:08:37,590 --> 00:08:38,090
Right?

158
00:08:38,090 --> 00:08:40,870


159
00:08:40,870 --> 00:08:44,409
And so now we could go
back to the previous slide

160
00:08:44,409 --> 00:08:49,000
and ask the question well, is
it that being in first class

161
00:08:49,000 --> 00:08:50,590
is protective?

162
00:08:50,590 --> 00:08:57,070
Or is it that being in second
or third class is risky?

163
00:08:57,070 --> 00:09:00,580
And there's no simple
answer to that.

164
00:09:00,580 --> 00:09:02,139
So let's do an experiment.

165
00:09:02,139 --> 00:09:04,909
We have these
correlated variables.

166
00:09:04,909 --> 00:09:10,269
Suppose we eliminate
c1 altogether.

167
00:09:10,269 --> 00:09:18,019
So I did that by changing the
init method of class passenger.

168
00:09:18,019 --> 00:09:25,649
Takes the same arguments,
but we'll look at the code.

169
00:09:25,649 --> 00:09:29,189
Because it's a little
bit clearer there.

170
00:09:29,190 --> 00:09:32,240
So there was the original one.

171
00:09:32,240 --> 00:09:36,190
And I'm going to
replace that by this.

172
00:09:36,190 --> 00:09:41,120


173
00:09:41,120 --> 00:09:42,669
combine that with
the original one.

174
00:09:42,669 --> 00:09:47,329


175
00:09:47,330 --> 00:09:54,700
So what you see is that instead
of having five features,

176
00:09:54,700 --> 00:09:55,870
I now have four.

177
00:09:55,870 --> 00:09:59,289
I've eliminated the
c1 binary feature.

178
00:09:59,289 --> 00:10:09,329
And then the code
is straightforward,

179
00:10:09,330 --> 00:10:12,240
that I've just
come through here,

180
00:10:12,240 --> 00:10:15,899
and I've just enumerated
the possibilities.

181
00:10:15,899 --> 00:10:19,829
So if you're in first
class, then second and third

182
00:10:19,830 --> 00:10:21,570
are both 0.

183
00:10:21,570 --> 00:10:25,150
Otherwise, one of them is a 1.

184
00:10:25,149 --> 00:10:29,689
So my invariant is
gone now, right?

185
00:10:29,690 --> 00:10:32,030
It's not the case that
we know that these two

186
00:10:32,029 --> 00:10:34,740
things have to add up
to 1, because maybe I'm

187
00:10:34,740 --> 00:10:35,534
in the third case.

188
00:10:35,534 --> 00:10:38,980


189
00:10:38,980 --> 00:10:46,120
OK, let's go run that
code and see what happens.

190
00:10:46,120 --> 00:10:59,970


191
00:10:59,970 --> 00:11:03,680
Well, if you remember, we
see that our accuracy has not

192
00:11:03,679 --> 00:11:05,569
really declined much.

193
00:11:05,570 --> 00:11:08,960
Pretty much the same
results we got before.

194
00:11:08,960 --> 00:11:12,320
But our weights are
really quite different.

195
00:11:12,320 --> 00:11:17,000
Now, suddenly, c2 and c3
have large negative weights.

196
00:11:17,000 --> 00:11:20,169


197
00:11:20,169 --> 00:11:22,319
We can look at them
side by side here.

198
00:11:22,320 --> 00:11:32,110


199
00:11:32,110 --> 00:11:33,730
So you see, not much difference.

200
00:11:33,730 --> 00:11:36,039
It actually performs maybe--

201
00:11:36,039 --> 00:11:39,144
well, really no real
difference in performance.

202
00:11:39,144 --> 00:11:41,019
But you'll notice that
the weights are really

203
00:11:41,019 --> 00:11:42,490
quite different.

204
00:11:42,490 --> 00:11:49,399
That now, what had been a strong
positive weight and relatively

205
00:11:49,399 --> 00:11:52,370
weak negative weights
is now replaced

206
00:11:52,370 --> 00:11:54,095
by two strong negative weights.

207
00:11:54,095 --> 00:11:58,870


208
00:11:58,870 --> 00:12:04,120
And age and gender
change just a little bit.

209
00:12:04,120 --> 00:12:05,769
So the whole point
here is that we

210
00:12:05,769 --> 00:12:08,710
have to be very careful, when
you have correlated features,

211
00:12:08,710 --> 00:12:11,590
about over-interpreting
the weights.

212
00:12:11,590 --> 00:12:14,560
It is generally pretty
safe to rely on the sign,

213
00:12:14,559 --> 00:12:18,159
whether it's
negative or positive.

214
00:12:18,159 --> 00:12:22,740
All right, changing
the topic but sticking

215
00:12:22,740 --> 00:12:26,759
with logistic regression,
there is this parameter

216
00:12:26,759 --> 00:12:30,341
you may recall, p, which
is the probability.

217
00:12:30,341 --> 00:12:32,809
And that was the cut-off.

218
00:12:32,809 --> 00:12:36,439
And we set it to 0.5,
saying if it estimates

219
00:12:36,440 --> 00:12:41,660
the probability of survival
to be 0.5 or higher,

220
00:12:41,659 --> 00:12:45,110
then we're going to guess
survived, predict survived.

221
00:12:45,110 --> 00:12:48,440
Otherwise, deceased.

222
00:12:48,440 --> 00:12:51,510
You can change that.

223
00:12:51,509 --> 00:12:57,299
And so I'm going to try two
extreme values, setting p

224
00:12:57,299 --> 00:13:04,500
to 0.1 and p to 0.9.

225
00:13:04,500 --> 00:13:06,875
Now, what do we think
that's likely to change?

226
00:13:06,875 --> 00:13:10,620


227
00:13:10,620 --> 00:13:13,682
Remember, we looked at a
bunch of different attributes.

228
00:13:13,682 --> 00:13:16,740


229
00:13:16,740 --> 00:13:18,360
In particular, what
attributes do we

230
00:13:18,360 --> 00:13:20,310
think are most likely to change?

231
00:13:20,309 --> 00:13:24,061
Anyone who has not answered
a question want to volunteer?

232
00:13:24,062 --> 00:13:25,769
I have nothing against
you, it's just I'm

233
00:13:25,769 --> 00:13:27,490
trying to spread the wealth.

234
00:13:27,490 --> 00:13:30,555
And I don't want to give you
diabetes, with all the candy.

235
00:13:30,554 --> 00:13:33,209


236
00:13:33,210 --> 00:13:35,552
All right, you get to go again.

237
00:13:35,552 --> 00:13:37,355
AUDIENCE: Sensitivity.

238
00:13:37,355 --> 00:13:38,289
JOHN GUTTAG: Pardon?

239
00:13:38,289 --> 00:13:40,038
AUDIENCE: The sensitivity
and specificity.

240
00:13:40,038 --> 00:13:42,629
JOHN GUTTAG: Sensitivity
and specificity,

241
00:13:42,629 --> 00:13:44,399
positive predictive value.

242
00:13:44,399 --> 00:13:46,079
Because we're shifting.

243
00:13:46,080 --> 00:13:51,845
And we're saying, well, by
changing the probability,

244
00:13:51,845 --> 00:13:53,220
we're making a
decision that it's

245
00:13:53,220 --> 00:13:57,570
more important to
not miss survivors

246
00:13:57,570 --> 00:14:03,020
than it is to, say,
ask gets too high.

247
00:14:03,019 --> 00:14:05,870
So let's look at what
happens when we run that.

248
00:14:05,870 --> 00:14:08,909


249
00:14:08,909 --> 00:14:10,230
I won't run it for you.

250
00:14:10,230 --> 00:14:13,899
But these are the
results we got.

251
00:14:13,899 --> 00:14:18,379
So as it happens, 0.9
gave me higher accuracy.

252
00:14:18,379 --> 00:14:22,039
But the key thing is, notice
the big difference here.

253
00:14:22,039 --> 00:14:28,849


254
00:14:28,850 --> 00:14:32,460
So what is that telling me?

255
00:14:32,460 --> 00:14:34,800
Well, it's telling me that
if I predict you're going

256
00:14:34,799 --> 00:14:38,519
to survive you probably did.

257
00:14:38,519 --> 00:14:40,980
But look what it did
to the sensitivity.

258
00:14:40,980 --> 00:14:46,320


259
00:14:46,320 --> 00:14:49,379
It means that most
of the survivors,

260
00:14:49,379 --> 00:14:53,299
I'm predicting they died.

261
00:14:53,299 --> 00:14:56,909
Why is the accuracy still OK?

262
00:14:56,909 --> 00:15:00,059
Well, because most people
died on the boat, on the ship,

263
00:15:00,059 --> 00:15:01,139
right?

264
00:15:01,139 --> 00:15:03,740
So we would have done
pretty well, you recall,

265
00:15:03,740 --> 00:15:06,134
if we just guessed
died for everybody.

266
00:15:06,134 --> 00:15:11,250


267
00:15:11,250 --> 00:15:14,960
So it's important to
understand these things.

268
00:15:14,960 --> 00:15:16,799
I once did some
work using machine

269
00:15:16,798 --> 00:15:18,340
learning for an
insurance company who

270
00:15:18,340 --> 00:15:21,440
was trying to set rates.

271
00:15:21,440 --> 00:15:23,570
And I asked them what
they wanted to do.

272
00:15:23,570 --> 00:15:28,059
And they said they didn't
want to lose money.

273
00:15:28,058 --> 00:15:29,600
They didn't want to
insure people who

274
00:15:29,600 --> 00:15:32,060
were going to get in accidents.

275
00:15:32,059 --> 00:15:35,750
So I was able to
change this p parameter

276
00:15:35,750 --> 00:15:38,730
so that it did a great job.

277
00:15:38,730 --> 00:15:43,129
The problem was they got to
write almost no policies.

278
00:15:43,129 --> 00:15:46,009
Because I could pretty much
guarantee the people I said

279
00:15:46,009 --> 00:15:47,787
wouldn't get in an
accident wouldn't.

280
00:15:47,787 --> 00:15:49,370
But there were a
whole bunch of people

281
00:15:49,370 --> 00:15:51,980
who didn't, who they
wouldn't write policies for.

282
00:15:51,980 --> 00:15:53,844
So they ended up not
making any money.

283
00:15:53,844 --> 00:15:54,760
It was a bad decision.

284
00:15:54,759 --> 00:15:59,529


285
00:15:59,529 --> 00:16:03,959
So we can change the cutoff.

286
00:16:03,960 --> 00:16:09,720
That leads to a really
important concept

287
00:16:09,720 --> 00:16:15,330
of something called the Receiver
Operating Characteristic.

288
00:16:15,330 --> 00:16:17,870
And it's a funny name, having
to do with it originally

289
00:16:17,870 --> 00:16:20,990
going back to radio receivers.

290
00:16:20,990 --> 00:16:22,049
But we can ignore that.

291
00:16:22,049 --> 00:16:24,579


292
00:16:24,580 --> 00:16:27,970
The goal here is
to say, suppose I

293
00:16:27,970 --> 00:16:33,160
don't want to make a decision
about where the cutoff is,

294
00:16:33,159 --> 00:16:38,319
but I want to look at, in some
sense, all possible cutoffs

295
00:16:38,320 --> 00:16:39,980
and look at the shape of it.

296
00:16:39,980 --> 00:16:42,500


297
00:16:42,500 --> 00:16:46,190
And that's what this
code is designed to do.

298
00:16:46,190 --> 00:16:54,180
So the way it works is I'll
take a training set and a test

299
00:16:54,179 --> 00:16:56,449
set, usual thing.

300
00:16:56,450 --> 00:16:58,990
I'll build one model.

301
00:16:58,990 --> 00:17:01,620
And that's an important thing,
that there's only one model

302
00:17:01,620 --> 00:17:06,588
getting built. And then
I'm going to vary p.

303
00:17:06,588 --> 00:17:09,549


304
00:17:09,549 --> 00:17:13,299
And I'm going to
call apply model

305
00:17:13,299 --> 00:17:16,389
with the same model
and the same test set,

306
00:17:16,390 --> 00:17:23,234
but different p's and keep
track of all of those results.

307
00:17:23,233 --> 00:17:27,990


308
00:17:27,990 --> 00:17:33,049
I'm then going to plot
a two-dimensional plot.

309
00:17:33,049 --> 00:17:36,589
The y-axis will
have sensitivity.

310
00:17:36,589 --> 00:17:39,829


311
00:17:39,829 --> 00:17:43,939
And the x-axis will have
one minus specificity.

312
00:17:43,940 --> 00:17:55,259


313
00:17:55,259 --> 00:17:57,694
So I am accumulating
a bunch of results.

314
00:17:57,694 --> 00:18:05,019


315
00:18:05,019 --> 00:18:08,920
And then I'm going to
produce this curve calling

316
00:18:08,920 --> 00:18:14,050
sklearn.metrics.auc,
that's not the curve.

317
00:18:14,049 --> 00:18:20,454
AUC stands for Area
Under the Curve.

318
00:18:20,454 --> 00:18:23,569


319
00:18:23,569 --> 00:18:27,174
And we'll see why we want to
get that area under the curve.

320
00:18:27,174 --> 00:18:31,690


321
00:18:31,690 --> 00:18:37,410
When I run that,
it produces this.

322
00:18:37,410 --> 00:18:40,201
So here's the curve,
the blue line.

323
00:18:40,201 --> 00:18:42,269
And there's some things
to note about it.

324
00:18:42,269 --> 00:18:44,869


325
00:18:44,869 --> 00:18:55,639
Way down at this end
I can have 0, right?

326
00:18:55,640 --> 00:18:58,430
I can set it so that I
don't make any predictions.

327
00:18:58,430 --> 00:19:02,740


328
00:19:02,740 --> 00:19:05,210
And this is interesting.

329
00:19:05,210 --> 00:19:09,069
So at this end it
is saying what?

330
00:19:09,069 --> 00:19:12,730


331
00:19:12,730 --> 00:19:16,180
Remember that my x-axis
is not specificity,

332
00:19:16,180 --> 00:19:20,279
but 1 minus specificity.

333
00:19:20,279 --> 00:19:27,599
So what we see is this corner
is highly sensitive and very

334
00:19:27,599 --> 00:19:28,299
unspecific.

335
00:19:28,299 --> 00:19:31,000


336
00:19:31,000 --> 00:19:34,569
So I'll get a lot
of false positives.

337
00:19:34,569 --> 00:19:40,329
This corner is very specific,
because 1 minus specificity

338
00:19:40,329 --> 00:19:42,609
is 0, and very insensitive.

339
00:19:42,609 --> 00:19:46,669


340
00:19:46,670 --> 00:19:50,210
So way down at the bottom,
I'm declaring nobody

341
00:19:50,210 --> 00:19:51,100
to be positive.

342
00:19:51,099 --> 00:19:54,109


343
00:19:54,109 --> 00:19:56,959
And way up here, everybody.

344
00:19:56,960 --> 00:19:58,579
Clearly, I don't
want to be at either

345
00:19:58,579 --> 00:20:00,470
of these places on
the curve, right?

346
00:20:00,470 --> 00:20:03,680
Typically I want to be
somewhere in the middle.

347
00:20:03,680 --> 00:20:07,820
And here, we can see, there's
a nice knee in the curve here.

348
00:20:07,819 --> 00:20:10,909
We can choose a place.

349
00:20:10,910 --> 00:20:13,490
What does this green line
represent, do you think?

350
00:20:13,490 --> 00:20:20,970


351
00:20:20,970 --> 00:20:26,430
The green line represents
a random classifier.

352
00:20:26,430 --> 00:20:30,880


353
00:20:30,880 --> 00:20:33,370
I flip a coin and I
just classify something

354
00:20:33,369 --> 00:20:38,429
positive or negative, depending
on the heads or tails,

355
00:20:38,430 --> 00:20:39,000
in this case.

356
00:20:39,000 --> 00:20:43,819


357
00:20:43,819 --> 00:20:49,189
So now we can look at an
interesting region, which

358
00:20:49,190 --> 00:20:57,559
is this region, the
area between the curve

359
00:20:57,559 --> 00:20:59,779
and a random classifier.

360
00:20:59,779 --> 00:21:02,649
And that sort of tells me how
much better I am than random.

361
00:21:02,650 --> 00:21:06,400


362
00:21:06,400 --> 00:21:12,810
I can look at the whole area,
the area under the curve.

363
00:21:12,809 --> 00:21:15,329


364
00:21:15,329 --> 00:21:20,279
And that's this, the area under
the Receiver Operating Curve.

365
00:21:20,279 --> 00:21:25,299


366
00:21:25,299 --> 00:21:33,159
In the best of all worlds,
the curve would be 1.

367
00:21:33,160 --> 00:21:35,290
That would be a
perfect classifier.

368
00:21:35,289 --> 00:21:38,049


369
00:21:38,049 --> 00:21:40,109
In the worst of all
worlds, it would be 0.

370
00:21:40,109 --> 00:21:45,189
But it's never 0 because
we don't do worse than 0.5.

371
00:21:45,190 --> 00:21:47,170
We hope not to do
worse than random.

372
00:21:47,170 --> 00:21:50,320
If so, we just reverse
our predictions.

373
00:21:50,319 --> 00:21:52,839
And then we're
better than random.

374
00:21:52,839 --> 00:21:56,750
So random is as bad
as you can do, really.

375
00:21:56,750 --> 00:21:59,599
And so this is a very
important concept.

376
00:21:59,599 --> 00:22:05,119
And it lets us evaluate how good
a classifier is independently

377
00:22:05,119 --> 00:22:07,000
of what we choose
to be the cutoff.

378
00:22:07,000 --> 00:22:10,329


379
00:22:10,329 --> 00:22:13,019
So when you read the
literature and people say,

380
00:22:13,019 --> 00:22:16,049
I have this wonderful method
of making predictions,

381
00:22:16,049 --> 00:22:18,779
you'll almost always
see them cite the AUROC.

382
00:22:18,779 --> 00:22:25,500


383
00:22:25,500 --> 00:22:29,430
Any questions about
this or about machine

384
00:22:29,430 --> 00:22:30,900
learning in general?

385
00:22:30,900 --> 00:22:33,420
If so, this would be a
good time to ask them,

386
00:22:33,420 --> 00:22:35,640
since I'm about to
totally change the topic.

387
00:22:35,640 --> 00:22:41,300


388
00:22:41,299 --> 00:22:43,750
Yes?

389
00:22:43,750 --> 00:22:45,710
AUDIENCE: At what
level does AUROC

390
00:22:45,710 --> 00:22:48,160
start to be statistically
significant?

391
00:22:48,160 --> 00:22:50,610
And how many data
points do you need

392
00:22:50,609 --> 00:22:52,079
to also prove that [INAUDIBLE]?

393
00:22:52,079 --> 00:22:52,870
JOHN GUTTAG: Right.

394
00:22:52,871 --> 00:22:54,340
So the question
is, at what point

395
00:22:54,339 --> 00:22:59,259
does the AUROC become
statistically significant?

396
00:22:59,259 --> 00:23:02,515
And that is, essentially,
an unanswerable question.

397
00:23:02,515 --> 00:23:06,840


398
00:23:06,839 --> 00:23:10,259
Whoops, relay it back.

399
00:23:10,259 --> 00:23:13,046
Needed to put more
air under the throw.

400
00:23:13,046 --> 00:23:18,750
I look like the
quarterback for the Rams,

401
00:23:18,750 --> 00:23:21,789
if you saw them play lately.

402
00:23:21,789 --> 00:23:27,369
So if you ask this question
about significance,

403
00:23:27,369 --> 00:23:30,889
it will depend upon
a number of things.

404
00:23:30,890 --> 00:23:36,147
So you're always asking, is it
significantly better than x?

405
00:23:36,146 --> 00:23:38,230
And so the question is,
is it significantly better

406
00:23:38,230 --> 00:23:40,150
than random?

407
00:23:40,150 --> 00:23:44,769
And you can't just say, for
example, that 0.6 isn't and 0.7

408
00:23:44,769 --> 00:23:45,879
is.

409
00:23:45,880 --> 00:23:50,320
Because it depends how
many points you have.

410
00:23:50,319 --> 00:23:51,909
If you have a lot
of points, it could

411
00:23:51,910 --> 00:23:54,279
be only a tiny bit
better than 0.5

412
00:23:54,279 --> 00:23:57,359
and still be
statistically significant.

413
00:23:57,359 --> 00:24:00,990
It may be
uninterestingly better.

414
00:24:00,990 --> 00:24:03,120
It may not be significant
in the English sense,

415
00:24:03,119 --> 00:24:07,279
but you still get
statistical significance.

416
00:24:07,279 --> 00:24:10,569
So that's a problem when
studies have lots of points.

417
00:24:10,569 --> 00:24:14,619
In general, it depends
upon the application.

418
00:24:14,619 --> 00:24:17,949
For a lot of applications,
you'll see things in the 0.7's

419
00:24:17,950 --> 00:24:20,769
being considered pretty useful.

420
00:24:20,769 --> 00:24:23,740
And the real question shouldn't
be whether it's significant,

421
00:24:23,740 --> 00:24:25,450
but whether it's useful.

422
00:24:25,450 --> 00:24:29,269
Can you make useful
decisions based upon it?

423
00:24:29,269 --> 00:24:31,420
And the other thing
is, typically,

424
00:24:31,420 --> 00:24:36,009
when you're talking about that,
you're selecting some point

425
00:24:36,009 --> 00:24:40,410
and really talking about a
region relative to that point.

426
00:24:40,410 --> 00:24:43,710
We usually don't really
care what it does out here.

427
00:24:43,710 --> 00:24:46,950
Because we hardly ever
operate out there anyway.

428
00:24:46,950 --> 00:24:49,970
We're usually somewhere
in the middle.

429
00:24:49,970 --> 00:24:52,039
But good question.

430
00:24:52,039 --> 00:24:53,576
Yeah?

431
00:24:53,576 --> 00:24:55,836
AUDIENCE: Why are we
doing 1 minus specificity?

432
00:24:55,836 --> 00:24:58,299
JOHN GUTTAG: Why are we
doing 1 minus specificity

433
00:24:58,299 --> 00:25:00,319
instead of specificity?

434
00:25:00,319 --> 00:25:02,500
Is that the question?

435
00:25:02,500 --> 00:25:04,720
And the answer is,
essentially, so we

436
00:25:04,720 --> 00:25:07,630
can do this trick of
computing the area.

437
00:25:07,630 --> 00:25:11,480
It gives us this nice curve.

438
00:25:11,480 --> 00:25:14,620
This nice, if you
will, concave curve

439
00:25:14,619 --> 00:25:16,750
which lets us compute
this area under here

440
00:25:16,750 --> 00:25:21,849
nicely if you were to take
specificity and just draw it,

441
00:25:21,849 --> 00:25:23,054
it would look different.

442
00:25:23,055 --> 00:25:26,289


443
00:25:26,289 --> 00:25:29,500
Obviously, mathematically,
they're, in some sense,

444
00:25:29,500 --> 00:25:30,819
the same right.

445
00:25:30,819 --> 00:25:35,919
If you have 1 minus x and x, you
can get either from the other.

446
00:25:35,920 --> 00:25:37,750
So it really just has
to do with the way

447
00:25:37,750 --> 00:25:39,903
people want to
draw this picture.

448
00:25:39,903 --> 00:25:42,748
AUDIENCE: [INAUDIBLE]?

449
00:25:42,748 --> 00:25:43,696
JOHN GUTTAG: Pardon?

450
00:25:43,695 --> 00:25:45,591
AUDIENCE: Does that
not change [INAUDIBLE]?

451
00:25:45,592 --> 00:25:47,694
JOHN GUTTAG: Does it not--

452
00:25:47,693 --> 00:25:49,907
AUDIENCE: Doesn't it
change the meaning

453
00:25:49,907 --> 00:25:51,111
of what you're [INAUDIBLE]?

454
00:25:51,112 --> 00:25:53,570
JOHN GUTTAG: Well, you'd have
to use a different statistic.

455
00:25:53,569 --> 00:25:59,549
You couldn't cite the AUROC if
you did specificity directly.

456
00:25:59,549 --> 00:26:02,759
Which is why they do 1 minus.

457
00:26:02,759 --> 00:26:06,650
The goal is you want to have
this point at 0 and this 0.00

458
00:26:06,650 --> 00:26:08,350
and 1.1.

459
00:26:08,349 --> 00:26:10,480
And playing 1 minus
gives you this trick,

460
00:26:10,480 --> 00:26:12,759
of anchoring those two points.

461
00:26:12,759 --> 00:26:15,170
And so then you get a
curve connecting them,

462
00:26:15,170 --> 00:26:19,330
which you can then easily
compare to the random curve.

463
00:26:19,329 --> 00:26:21,329
It's just one of
these little tricks

464
00:26:21,329 --> 00:26:23,759
that statisticians
like to play to make

465
00:26:23,759 --> 00:26:28,529
things easy to visualize and
easy to compute statistics

466
00:26:28,529 --> 00:26:29,309
about.

467
00:26:29,309 --> 00:26:31,919
It's not a fundamentally
important issue.

468
00:26:31,920 --> 00:26:34,500


469
00:26:34,500 --> 00:26:35,329
Anything else?

470
00:26:35,329 --> 00:26:39,919


471
00:26:39,920 --> 00:26:44,315
All right, so I told you I
was going to change topics--

472
00:26:44,315 --> 00:26:48,029


473
00:26:48,029 --> 00:26:49,759
finally got one completed--

474
00:26:49,759 --> 00:26:52,000
and I am.

475
00:26:52,000 --> 00:26:56,299
And this is a topic I
approach with some reluctance.

476
00:26:56,299 --> 00:26:59,539
So you have probably all
heard this expression,

477
00:26:59,539 --> 00:27:04,289
that there are three
kinds of lies, lies,

478
00:27:04,289 --> 00:27:08,480
damn lies, and statistics.

479
00:27:08,480 --> 00:27:11,390
And we've been talking
a lot about statistics.

480
00:27:11,390 --> 00:27:14,810
And now I want to spend
the rest of today's lecture

481
00:27:14,809 --> 00:27:19,339
and the start of
Wednesday's lecture

482
00:27:19,339 --> 00:27:24,079
talking about how to
lie with statistics.

483
00:27:24,079 --> 00:27:29,750
So at this point, I usually put
on my "Numbers Never Lie" hat.

484
00:27:29,750 --> 00:27:36,259
But do say that numbers never
lie, but liars use numbers.

485
00:27:36,259 --> 00:27:39,529
And I hope none of you will
ever go work for a politician

486
00:27:39,529 --> 00:27:42,440
and put this
knowledge to bad use.

487
00:27:42,440 --> 00:27:43,960
This quote is well known.

488
00:27:43,960 --> 00:27:46,220
It's variously
attributed, often,

489
00:27:46,220 --> 00:27:49,490
to Mark Twain, the
fellow on the left.

490
00:27:49,490 --> 00:27:51,589
He claimed not to
have invented it,

491
00:27:51,589 --> 00:27:55,459
but said it was invented
by Benjamin Disraeli.

492
00:27:55,460 --> 00:27:57,620
And I prefer to
believe that, since it

493
00:27:57,619 --> 00:28:02,689
does seem like something a
Prime Minister would invent.

494
00:28:02,690 --> 00:28:04,880
So let's think about this.

495
00:28:04,880 --> 00:28:08,090
The issue here is the
way the human mind works

496
00:28:08,089 --> 00:28:09,319
and statistics.

497
00:28:09,319 --> 00:28:11,839


498
00:28:11,839 --> 00:28:15,439
Darrell Huff, a
well-known statistician

499
00:28:15,440 --> 00:28:19,064
who did write a book called
How to Lie with Statistics,

500
00:28:19,064 --> 00:28:20,480
says, "if you can't
prove what you

501
00:28:20,480 --> 00:28:24,740
want to prove, demonstrate
something else and pretend

502
00:28:24,740 --> 00:28:27,200
they are the same thing.

503
00:28:27,200 --> 00:28:29,779
In the daze that follows
the collision of statistics

504
00:28:29,779 --> 00:28:32,299
with the human
mind, hardly anyone

505
00:28:32,299 --> 00:28:34,940
will notice the difference."

506
00:28:34,940 --> 00:28:39,910
And indeed, empirically,
he seems to be right.

507
00:28:39,910 --> 00:28:42,529
So let's look at some examples.

508
00:28:42,529 --> 00:28:43,359
Here's one I like.

509
00:28:43,359 --> 00:28:47,889
This is from another famous
statistician called Anscombe.

510
00:28:47,890 --> 00:28:50,890
And he invented this thing
called Anscombe's Quartet.

511
00:28:50,890 --> 00:28:52,600
I take my hat off now.

512
00:28:52,599 --> 00:28:53,529
It's too hot in here.

513
00:28:53,529 --> 00:28:56,559


514
00:28:56,559 --> 00:29:00,744
A bunch of numbers,
11 x, y pairs.

515
00:29:00,744 --> 00:29:03,674


516
00:29:03,674 --> 00:29:05,549
I know you don't want
to look at the numbers,

517
00:29:05,549 --> 00:29:08,980
so here are some
statistics about them.

518
00:29:08,980 --> 00:29:14,089
Each of those pairs
has the same mean value

519
00:29:14,089 --> 00:29:17,839
for x, the same mean
for y, the same variance

520
00:29:17,839 --> 00:29:20,750
for x, the same variance for y.

521
00:29:20,750 --> 00:29:23,630
And then I went and I fit a
linear regression model to it.

522
00:29:23,630 --> 00:29:27,560
And lo and behold, I got the
same equation for everyone,

523
00:29:27,559 --> 00:29:31,429
y equals 0.5x plus 3.

524
00:29:31,430 --> 00:29:36,019
So that raises the
question, if we go back,

525
00:29:36,019 --> 00:29:40,740
is there really much difference
between these pairs of x and y?

526
00:29:40,740 --> 00:29:44,190


527
00:29:44,190 --> 00:29:46,211
Are they really similar?

528
00:29:46,211 --> 00:29:47,670
And the answer is,
that's what they

529
00:29:47,670 --> 00:29:49,009
look like if you plot them.

530
00:29:49,009 --> 00:29:51,769


531
00:29:51,769 --> 00:29:53,710
So even though
statistically they

532
00:29:53,710 --> 00:29:56,579
appear to be kind
of the same, they

533
00:29:56,579 --> 00:29:59,109
could hardly be more
different, right?

534
00:29:59,109 --> 00:30:02,439
Those are not the
same distributions.

535
00:30:02,440 --> 00:30:05,559
So there's an
important moral here,

536
00:30:05,559 --> 00:30:07,960
which is that
statistics about data

537
00:30:07,960 --> 00:30:12,380
is not the same thing
as the data itself.

538
00:30:12,380 --> 00:30:15,460
And this seems obvious,
but it's amazing

539
00:30:15,460 --> 00:30:17,590
how easy it is to forget it.

540
00:30:17,589 --> 00:30:19,269
The number of papers
I've read where

541
00:30:19,269 --> 00:30:21,470
I see a bunch of
statistics about the data

542
00:30:21,470 --> 00:30:24,549
but don't see the
data is enormous.

543
00:30:24,549 --> 00:30:27,940
And it's easy to lose
track of the fact

544
00:30:27,940 --> 00:30:33,470
that the statistics don't
tell the whole story.

545
00:30:33,470 --> 00:30:36,799
So the answer is the
old Chinese proverb,

546
00:30:36,799 --> 00:30:39,823
a picture is worth
a thousand words,

547
00:30:39,824 --> 00:30:41,740
I urge you, the first
thing you should do when

548
00:30:41,740 --> 00:30:44,740
you get a data set, is plot it.

549
00:30:44,740 --> 00:30:47,980
If it's got too many points
to plot all the points,

550
00:30:47,980 --> 00:30:52,049
subsample it and
plot of subsample.

551
00:30:52,049 --> 00:30:57,529
Use some visualization tool
to look at the data itself.

552
00:30:57,529 --> 00:31:00,819
Now, that said,
pictures are wonderful.

553
00:31:00,819 --> 00:31:03,399
But you can lie with pictures.

554
00:31:03,400 --> 00:31:05,380
So here's an interesting chart.

555
00:31:05,380 --> 00:31:10,020
These are grades in
6.0001 by gender.

556
00:31:10,019 --> 00:31:12,599
So the males are blue
and the females are pink.

557
00:31:12,599 --> 00:31:15,439
Sorry for being such
a traditionalist.

558
00:31:15,440 --> 00:31:20,519
And as you can see, the women
did way better than the men.

559
00:31:20,519 --> 00:31:23,970
Now, I know for some of you
this is confirmation bias.

560
00:31:23,970 --> 00:31:25,680
You say, of course.

561
00:31:25,680 --> 00:31:29,350
Others say, impossible, But in
fact, if you look carefully,

562
00:31:29,349 --> 00:31:32,730
you'll see that's not what
this chart says at all.

563
00:31:32,730 --> 00:31:37,049
Because if you look
at the axis here,

564
00:31:37,049 --> 00:31:41,509
you'll see that actually
there's not much difference.

565
00:31:41,509 --> 00:31:45,490
Here's what I get if
I plot it from 0 to 5.

566
00:31:45,490 --> 00:31:47,390
Yeah, the women did
a little bit better.

567
00:31:47,390 --> 00:31:50,620
But that's not a
statistically-significant

568
00:31:50,619 --> 00:31:53,189
difference.

569
00:31:53,190 --> 00:31:57,509
And by the way, when I plotted
it last year for 6.0002,

570
00:31:57,509 --> 00:32:00,779
the blue was about that
much higher than the pink.

571
00:32:00,779 --> 00:32:03,480
Don't read much
into either of them.

572
00:32:03,480 --> 00:32:07,349
But the trick was
here, I took the y-axis

573
00:32:07,349 --> 00:32:13,279
and ran it from 3.9 to 4.05.

574
00:32:13,279 --> 00:32:16,549
I cleverly chose my
baseline in such a way

575
00:32:16,549 --> 00:32:20,159
to make the difference look
much bigger than it is.

576
00:32:20,160 --> 00:32:22,690


577
00:32:22,690 --> 00:32:26,740
Here I did the honest thing
of put the baseline at 0

578
00:32:26,740 --> 00:32:28,750
and run it to 5.

579
00:32:28,750 --> 00:32:33,140
Because that's the
range of grades at MIT.

580
00:32:33,140 --> 00:32:37,820
And so when you look
at a chart, it's

581
00:32:37,819 --> 00:32:39,409
important to keep
in mind that you

582
00:32:39,410 --> 00:32:43,650
need to look at the axis
labels and the scales.

583
00:32:43,650 --> 00:32:47,092


584
00:32:47,092 --> 00:32:48,800
Let's look at another
chart, just in case

585
00:32:48,799 --> 00:32:53,579
you think I'm the only one who
likes to play with graphics.

586
00:32:53,579 --> 00:32:57,279
This is a chart from Fox News.

587
00:32:57,279 --> 00:33:01,210
And they're arguing here.

588
00:33:01,210 --> 00:33:03,860
It's the shocking
statistics that there

589
00:33:03,859 --> 00:33:06,639
are 108.6 million
people on welfare,

590
00:33:06,640 --> 00:33:11,980
and 101.7 with a full-time job.

591
00:33:11,980 --> 00:33:14,779
And you can imagine the rhetoric
that accompanies this chart.

592
00:33:14,779 --> 00:33:18,539


593
00:33:18,539 --> 00:33:20,254
This is actually correct.

594
00:33:20,255 --> 00:33:22,770


595
00:33:22,769 --> 00:33:24,879
It is true from the
Census Bureau data.

596
00:33:24,880 --> 00:33:26,230
Sort of.

597
00:33:26,230 --> 00:33:30,009
But notice that
I said you should

598
00:33:30,009 --> 00:33:33,970
read the labels on the axes.

599
00:33:33,970 --> 00:33:35,150
There is no label here.

600
00:33:35,150 --> 00:33:39,150


601
00:33:39,150 --> 00:33:45,640
But you can bet that the
y-intercept is not 0 on this.

602
00:33:45,640 --> 00:33:50,670
Because you can see how
small 101.7 looks like.

603
00:33:50,670 --> 00:33:53,420
So it makes the difference
look bigger than it is.

604
00:33:53,420 --> 00:33:56,310


605
00:33:56,309 --> 00:34:02,000
Now, that's not the only
funny thing about it.

606
00:34:02,000 --> 00:34:06,650
I said you should look at
the labels on the x-axis.

607
00:34:06,650 --> 00:34:08,289
Well, they've labeled them.

608
00:34:08,289 --> 00:34:11,639
But what do these things mean?

609
00:34:11,639 --> 00:34:13,679
Well, I looked it
up, and I'll tell you

610
00:34:13,679 --> 00:34:15,960
what they actually mean.

611
00:34:15,960 --> 00:34:19,760
People on welfare
counts the number

612
00:34:19,760 --> 00:34:23,581
of people in a household in
which at least one person is

613
00:34:23,581 --> 00:34:24,079
on welfare.

614
00:34:24,079 --> 00:34:26,779


615
00:34:26,780 --> 00:34:31,340
So if there is say, two
parents, one is working

616
00:34:31,340 --> 00:34:33,590
and one is collecting
welfare and there

617
00:34:33,590 --> 00:34:36,949
are four kids, that counts
as six people on welfare.

618
00:34:36,949 --> 00:34:40,469


619
00:34:40,469 --> 00:34:43,860
People with a full-time
job, is actually

620
00:34:43,860 --> 00:34:46,370
does not count households.

621
00:34:46,369 --> 00:34:50,029
So in the same
family, you would have

622
00:34:50,030 --> 00:34:56,179
six on the bar on the left, and
one on the bar on the right.

623
00:34:56,179 --> 00:35:00,670
Clearly giving a very
different impression.

624
00:35:00,670 --> 00:35:04,599
And so again,
pictures can be good.

625
00:35:04,599 --> 00:35:09,969
But if you don't dive deep into
them, they really can fool you.

626
00:35:09,969 --> 00:35:12,009
Now, before I should
leave this slide,

627
00:35:12,010 --> 00:35:14,740
I should say that it's not the
case that you can't believe

628
00:35:14,739 --> 00:35:17,500
anything you read on Fox News.

629
00:35:17,500 --> 00:35:20,559
Because in fact, the Red Sox
did beat the St. Louis Cardinals

630
00:35:20,559 --> 00:35:21,539
4 to 2 that day.

631
00:35:21,539 --> 00:35:27,090


632
00:35:27,090 --> 00:35:30,840
So the moral here is to ask
whether the things being

633
00:35:30,840 --> 00:35:33,990
compared are
actually comparable.

634
00:35:33,989 --> 00:35:36,859
Or you're really comparing
apples and oranges,

635
00:35:36,860 --> 00:35:39,599
as they say.

636
00:35:39,599 --> 00:35:44,369
OK, this is probably the
most common statistical sin.

637
00:35:44,369 --> 00:35:46,259
It's called GIGO.

638
00:35:46,260 --> 00:35:49,110
And perhaps this
picture can make

639
00:35:49,110 --> 00:35:52,510
you guess what the
G's stand for GIGO

640
00:35:52,510 --> 00:35:55,590
is Garbage In, Garbage Out.

641
00:35:55,590 --> 00:36:00,280


642
00:36:00,280 --> 00:36:03,260
So here's a great,
again, quote about it.

643
00:36:03,260 --> 00:36:10,840
So Charles Babbage designed
the first digital computer,

644
00:36:10,840 --> 00:36:12,750
the first actual
computation engine.

645
00:36:12,750 --> 00:36:15,429
He was unable to build it.

646
00:36:15,429 --> 00:36:17,649
But hundreds of years
after he died one

647
00:36:17,650 --> 00:36:22,030
was built according to his
design, and it actually worked.

648
00:36:22,030 --> 00:36:24,460
No electronics, really.

649
00:36:24,460 --> 00:36:27,220
So he was a famous person.

650
00:36:27,219 --> 00:36:31,089
And he was asked by Parliament
about his machine, which

651
00:36:31,090 --> 00:36:32,890
he was asking them to fund.

652
00:36:32,889 --> 00:36:36,639
Well, if you put wrong
numbers into the machine,

653
00:36:36,639 --> 00:36:41,449
will the machine have right
numbers come out the other end?

654
00:36:41,449 --> 00:36:44,250
And of course, he
was a very smart guy.

655
00:36:44,250 --> 00:36:45,829
And he was totally baffled.

656
00:36:45,829 --> 00:36:48,019
This question
seems so stupid, he

657
00:36:48,019 --> 00:36:50,509
couldn't believe anyone
would even ask it.

658
00:36:50,510 --> 00:36:53,810
That it was just computation.

659
00:36:53,809 --> 00:36:58,420
And the answers you get are
based on the data you put in.

660
00:36:58,420 --> 00:37:03,269
If you put in garbage,
you get out garbage.

661
00:37:03,269 --> 00:37:08,099
So here is an example
from the 1840s.

662
00:37:08,099 --> 00:37:10,139
They did a census in the 1840s.

663
00:37:10,139 --> 00:37:13,839
And for those of you who are not
familiar with American history,

664
00:37:13,840 --> 00:37:17,610
it was a very contentious
time in the US.

665
00:37:17,610 --> 00:37:20,640
The country was divided
between states that had slavery

666
00:37:20,639 --> 00:37:22,609
and states that didn't.

667
00:37:22,610 --> 00:37:28,160
And that was the dominant
political issue of the day.

668
00:37:28,159 --> 00:37:31,549
John Calhoun, who was
Secretary of State

669
00:37:31,550 --> 00:37:37,010
and a leader in the Senate,
was from South Carolina

670
00:37:37,010 --> 00:37:41,420
and probably the strongest
proponent of slavery.

671
00:37:41,420 --> 00:37:47,480
And he used the census data to
say that slavery was actually

672
00:37:47,480 --> 00:37:49,920
good for the slaves.

673
00:37:49,920 --> 00:37:51,780
Kind of an amazing thought.

674
00:37:51,780 --> 00:37:56,040
Basically saying that
this data claimed

675
00:37:56,039 --> 00:37:59,759
that freed slaves
were more likely to be

676
00:37:59,760 --> 00:38:01,590
insane than enslaved slaves.

677
00:38:01,590 --> 00:38:07,760


678
00:38:07,760 --> 00:38:13,250
He was rebutted in the
House by John Quincy

679
00:38:13,250 --> 00:38:17,119
Adams, who had formerly been
President of the United States.

680
00:38:17,119 --> 00:38:20,809
After he stopped being
President, he ran for Congress.

681
00:38:20,809 --> 00:38:23,389
From Braintree, Massachusetts.

682
00:38:23,389 --> 00:38:24,980
Actually now called
Quincy, the part

683
00:38:24,980 --> 00:38:28,280
he's from, after his family.

684
00:38:28,280 --> 00:38:30,920
And he claimed that
atrocious misrepresentations

685
00:38:30,920 --> 00:38:34,550
had been made on a subject
of deep importance.

686
00:38:34,550 --> 00:38:37,100
He was an abolitionist.

687
00:38:37,099 --> 00:38:39,291
So you don't even have to
look at that statistics

688
00:38:39,291 --> 00:38:40,250
to know who to believe.

689
00:38:40,250 --> 00:38:42,137
Just look at these pictures.

690
00:38:42,137 --> 00:38:43,970
Are you going to believe
this nice gentleman

691
00:38:43,969 --> 00:38:49,099
from Braintree or this scary
guy from South Carolina?

692
00:38:49,099 --> 00:38:53,779
But setting looks aside,
Calhoun eventually

693
00:38:53,780 --> 00:38:58,080
admitted that the census
was indeed full of errors.

694
00:38:58,079 --> 00:38:59,599
But he said that was fine.

695
00:38:59,599 --> 00:39:01,991
Because there were
so many of them

696
00:39:01,992 --> 00:39:03,950
that they would balance
each other out and lead

697
00:39:03,949 --> 00:39:07,500
to the same conclusion, as
if they were all correct.

698
00:39:07,500 --> 00:39:09,880
So he didn't believe in
garbage in, garbage out.

699
00:39:09,880 --> 00:39:12,050
He said yeah, it is garbage.

700
00:39:12,050 --> 00:39:16,160
But it'll all come
out in the end OK.

701
00:39:16,159 --> 00:39:19,789
Well, now we know enough
to ask the question.

702
00:39:19,789 --> 00:39:22,539


703
00:39:22,539 --> 00:39:27,480
This isn't totally
brain dead, in that

704
00:39:27,480 --> 00:39:30,090
we've already looked
at experiments

705
00:39:30,090 --> 00:39:32,579
and said we get
experimental error.

706
00:39:32,579 --> 00:39:37,750
And under some circumstances,
you can manage the error.

707
00:39:37,750 --> 00:39:38,889
The data isn't garbage.

708
00:39:38,889 --> 00:39:40,389
It just has errors.

709
00:39:40,389 --> 00:39:42,369
But it's true if the
measurement errors

710
00:39:42,369 --> 00:39:46,839
are unbiased and
independent of each other.

711
00:39:46,840 --> 00:39:50,740
And almost identically
distributed on either side

712
00:39:50,739 --> 00:39:52,329
of the mean, right?

713
00:39:52,329 --> 00:39:54,369
That's why we spend
so much time looking

714
00:39:54,369 --> 00:39:58,989
at the normal distribution,
and why it's called Gaussian.

715
00:39:58,989 --> 00:40:00,879
Because Gauss
said, yes, I know I

716
00:40:00,880 --> 00:40:04,730
have errors in my
astronomical measurements.

717
00:40:04,730 --> 00:40:08,199
But I believe my errors are
distributed in what we now

718
00:40:08,199 --> 00:40:10,119
call a Gaussian curve.

719
00:40:10,119 --> 00:40:12,130
And therefore, I can
still work with them

720
00:40:12,130 --> 00:40:13,960
and get an accurate
estimate of the values.

721
00:40:13,960 --> 00:40:16,820


722
00:40:16,820 --> 00:40:21,010
Now, of course, that
wasn't true here.

723
00:40:21,010 --> 00:40:22,570
The errors were not random.

724
00:40:22,570 --> 00:40:25,360
They were, in fact,
quite systematic,

725
00:40:25,360 --> 00:40:27,970
designed to produce
a certain thing.

726
00:40:27,969 --> 00:40:31,569
And the last word was
from another abolitionist

727
00:40:31,570 --> 00:40:35,710
who claimed it was the
census that was insane.

728
00:40:35,710 --> 00:40:40,070
All right, that's
Garbage In, Garbage Out.

729
00:40:40,070 --> 00:40:45,130
The moral here is that
analysis of bad data

730
00:40:45,130 --> 00:40:49,090
is worse than no
analysis at all, really.

731
00:40:49,090 --> 00:40:52,269
Time and again we see people
doing, actually often,

732
00:40:52,269 --> 00:40:57,190
correct statistical
analysis of incorrect data

733
00:40:57,190 --> 00:41:00,400
and reaching conclusions.

734
00:41:00,400 --> 00:41:02,500
And that's really risky.

735
00:41:02,500 --> 00:41:04,539
So before one goes
off and starts

736
00:41:04,539 --> 00:41:08,769
using statistical techniques of
the sort we've been discussing,

737
00:41:08,769 --> 00:41:10,690
the first question
you have to ask is,

738
00:41:10,690 --> 00:41:14,619
is the data itself
worth analyzing?

739
00:41:14,619 --> 00:41:18,250
And it often isn't.

740
00:41:18,250 --> 00:41:21,219
Now, you could argue that
this is a thing of the past,

741
00:41:21,219 --> 00:41:25,089
and no modern politician would
make these kinds of mistakes.

742
00:41:25,090 --> 00:41:27,130
I'm not going to
insert a photo here.

743
00:41:27,130 --> 00:41:30,700
But I leave it to you to
think which politician's photo

744
00:41:30,699 --> 00:41:32,809
you might paste in this frame.

745
00:41:32,809 --> 00:41:35,369


746
00:41:35,369 --> 00:41:38,489
All right, onto another
statistical sin.

747
00:41:38,489 --> 00:41:42,061
This is a picture of a
World War II fighter plane.

748
00:41:42,061 --> 00:41:44,519
I don't know enough about planes
to know what kind of plane

749
00:41:44,519 --> 00:41:45,018
it is.

750
00:41:45,018 --> 00:41:45,809
Anyone here?

751
00:41:45,809 --> 00:41:47,190
There must be an
Aero student who

752
00:41:47,190 --> 00:41:50,659
will be able to tell
me what plane this is.

753
00:41:50,659 --> 00:41:54,460
Don't they teach you guys
anything in Aero these days?

754
00:41:54,460 --> 00:41:55,150
Shame on them.

755
00:41:55,150 --> 00:41:56,079
All right.

756
00:41:56,079 --> 00:41:57,309
Anyway, it's a plane.

757
00:41:57,309 --> 00:41:58,690
That much I know.

758
00:41:58,690 --> 00:42:00,340
And it has a propeller.

759
00:42:00,340 --> 00:42:03,100
And that's all I can tell
you about the airplane.

760
00:42:03,099 --> 00:42:07,980
So this was a photo taken
at a airfield in Britain.

761
00:42:07,980 --> 00:42:12,960
And the Allies would
send planes over Germany

762
00:42:12,960 --> 00:42:16,920
for bombing runs and fighters
to protect the bombers.

763
00:42:16,920 --> 00:42:20,610
And when they came back, the
planes were often damaged.

764
00:42:20,610 --> 00:42:23,070
And they would inspect
the damage and say look,

765
00:42:23,070 --> 00:42:24,990
there's a lot of flak there.

766
00:42:24,989 --> 00:42:28,739
The Germans shot
flak at the planes.

767
00:42:28,739 --> 00:42:31,559
And that would be
a part of the plane

768
00:42:31,559 --> 00:42:34,989
that maybe we should
reinforce in the future.

769
00:42:34,989 --> 00:42:38,739
So when it gets hit by
flak it survives it.

770
00:42:38,739 --> 00:42:40,418
It does less damage.

771
00:42:40,418 --> 00:42:42,460
So you can analyze where
the Germans were hitting

772
00:42:42,460 --> 00:42:46,030
the planes, and you would
add a little extra armor

773
00:42:46,030 --> 00:42:49,160
to that part of the plane.

774
00:42:49,159 --> 00:42:53,527
What's the flaw in that?

775
00:42:53,527 --> 00:42:54,491
Yeah?

776
00:42:54,492 --> 00:42:56,365
AUDIENCE: They didn't
look at the planes that

777
00:42:56,365 --> 00:42:57,784
actually got shot down.

778
00:42:57,784 --> 00:42:59,079
JOHN GUTTAG: Yeah.

779
00:42:59,079 --> 00:43:03,069
This is what's called, in
the jargon, survivor bias.

780
00:43:03,070 --> 00:43:09,955


781
00:43:09,954 --> 00:43:16,119
S-U-R-V-I-V-O-R.

782
00:43:16,119 --> 00:43:18,440
The planes they really
should have been analyzing

783
00:43:18,440 --> 00:43:20,720
were the ones that
got shot down.

784
00:43:20,719 --> 00:43:23,000
But those were hard to analyze.

785
00:43:23,000 --> 00:43:26,369
So they analyzed
the ones they had

786
00:43:26,369 --> 00:43:29,819
and drew conclusions,
and perhaps totally

787
00:43:29,820 --> 00:43:31,254
the wrong conclusion.

788
00:43:31,253 --> 00:43:33,420
Maybe the conclusion they
should have drawn is well,

789
00:43:33,420 --> 00:43:35,430
it's OK if you get hit here.

790
00:43:35,429 --> 00:43:38,069
Let's reinforce
the other places.

791
00:43:38,070 --> 00:43:40,590
I don't know enough to know
what the right answer was.

792
00:43:40,590 --> 00:43:44,250
I do know that this was
statistically the wrong thing

793
00:43:44,250 --> 00:43:47,420
to be thinking about doing.

794
00:43:47,420 --> 00:43:51,630
And this is an issue we have
whenever we do sampling.

795
00:43:51,630 --> 00:43:55,619
All statistical techniques
are based upon the assumption

796
00:43:55,619 --> 00:43:59,139
that by sampling a
subset of the population

797
00:43:59,139 --> 00:44:02,909
we can infer things about
the population as a whole.

798
00:44:02,909 --> 00:44:06,210
Everything we've done this
term has been based on that.

799
00:44:06,210 --> 00:44:11,220
When we were fitting
curves we were doing that.

800
00:44:11,219 --> 00:44:15,269
When we were talking about the
empirical rule and Monte Carlo

801
00:44:15,269 --> 00:44:17,362
Simulation, we were
doing that, when

802
00:44:17,362 --> 00:44:19,320
we were building models,
with machine learning,

803
00:44:19,320 --> 00:44:20,170
we were doing that.

804
00:44:20,170 --> 00:44:23,000


805
00:44:23,000 --> 00:44:26,289
And if random
sampling is used, you

806
00:44:26,289 --> 00:44:29,800
can make meaningful
mathematical statements

807
00:44:29,800 --> 00:44:34,000
about the relation of the
sample to the entire population.

808
00:44:34,000 --> 00:44:36,949


809
00:44:36,949 --> 00:44:40,699
And that's why so much
of what we did works.

810
00:44:40,699 --> 00:44:42,149
And when we're
doing simulations,

811
00:44:42,150 --> 00:44:45,010
that's really easy.

812
00:44:45,010 --> 00:44:48,970
When we were choosing
random values of the needles

813
00:44:48,969 --> 00:44:52,044
for trying to find
pi, or random value

814
00:44:52,045 --> 00:44:54,490
if the roulette wheel spins.

815
00:44:54,489 --> 00:44:57,129
We could be pretty sure our
samples were, indeed, random.

816
00:44:57,130 --> 00:44:59,930


817
00:44:59,929 --> 00:45:04,289
In the field, it's not so easy.

818
00:45:04,289 --> 00:45:05,039
Right?

819
00:45:05,039 --> 00:45:06,750
Because some samples
are much more

820
00:45:06,750 --> 00:45:10,050
convenient to
acquire than others.

821
00:45:10,050 --> 00:45:13,470
It's much easier to acquire a
plane on the field in Britain

822
00:45:13,469 --> 00:45:15,959
than a plane on the
ground in France.

823
00:45:15,960 --> 00:45:19,210


824
00:45:19,210 --> 00:45:21,730
Convenient sampling,
as it's often called,

825
00:45:21,730 --> 00:45:25,289
is not usually random.

826
00:45:25,289 --> 00:45:27,619
So you have survivor bias.

827
00:45:27,619 --> 00:45:31,829
So I asked you to do
course evaluations.

828
00:45:31,829 --> 00:45:34,369
Well, there's
survivor bias there.

829
00:45:34,369 --> 00:45:38,239
The people who really hated this
course have already dropped it.

830
00:45:38,239 --> 00:45:40,309
And so we won't sample them.

831
00:45:40,309 --> 00:45:43,400
That's good for me, at least.

832
00:45:43,400 --> 00:45:45,889
But we see that.

833
00:45:45,889 --> 00:45:47,960
We see that with grades.

834
00:45:47,960 --> 00:45:49,769
The people who are
really struggling,

835
00:45:49,768 --> 00:45:51,559
who were most likely
to fail, have probably

836
00:45:51,559 --> 00:45:53,715
dropped the course too.

837
00:45:53,715 --> 00:45:56,090
That's one of the reasons I
don't think it's fair to say,

838
00:45:56,090 --> 00:45:57,256
we're going to have a curve.

839
00:45:57,255 --> 00:45:59,839
And we're going to always
fail this fraction,

840
00:45:59,840 --> 00:46:01,910
and give A's to this fraction.

841
00:46:01,909 --> 00:46:05,799
Because by the end of the term,
we have a lot of survivor bias.

842
00:46:05,800 --> 00:46:08,430
The students who are left
are, on average, better

843
00:46:08,429 --> 00:46:11,149
than the students who
started the semester.

844
00:46:11,150 --> 00:46:15,090
So you need to take
that into account.

845
00:46:15,090 --> 00:46:19,050
Another kind of
non-representative sampling

846
00:46:19,050 --> 00:46:23,620
or convenience sampling
is opinion polls,

847
00:46:23,619 --> 00:46:28,199
in that you have something
there called non-response bias.

848
00:46:28,199 --> 00:46:29,699
So I don't know
about you, but I get

849
00:46:29,699 --> 00:46:33,469
phone calls asking my
opinion about things.

850
00:46:33,469 --> 00:46:36,009
Surveys about
products, whatever.

851
00:46:36,010 --> 00:46:36,850
I never answer.

852
00:46:36,849 --> 00:46:38,469
I just hang up the phone.

853
00:46:38,469 --> 00:46:39,909
I get a zillion emails.

854
00:46:39,909 --> 00:46:42,879
Every time I stay in a
hotel, I get an email

855
00:46:42,880 --> 00:46:45,460
asking me to rate the hotel.

856
00:46:45,460 --> 00:46:48,309
When I fly I get e-mails
from the airline.

857
00:46:48,309 --> 00:46:51,009
I don't answer any
of those surveys.

858
00:46:51,010 --> 00:46:54,580
But some people do, presumably,
or they wouldn't send them out.

859
00:46:54,579 --> 00:46:56,590
But why should they
think that the people who

860
00:46:56,590 --> 00:46:59,079
answer the survey
are representative

861
00:46:59,079 --> 00:47:02,789
of all the people who stay in
the hotel or all the people who

862
00:47:02,789 --> 00:47:04,179
fly on the plane?

863
00:47:04,179 --> 00:47:05,089
They're not.

864
00:47:05,090 --> 00:47:06,789
They're the kind
of people who maybe

865
00:47:06,789 --> 00:47:09,349
have time to answer surveys.

866
00:47:09,349 --> 00:47:13,549
And so you get a
non-response bias.

867
00:47:13,550 --> 00:47:16,039
And that tends to
distort your results.

868
00:47:16,039 --> 00:47:19,269


869
00:47:19,269 --> 00:47:23,969
When samples are not
random and independent,

870
00:47:23,969 --> 00:47:25,919
we can still run
statistics on them.

871
00:47:25,920 --> 00:47:28,559
We can compute means
and standard deviations.

872
00:47:28,559 --> 00:47:30,630
And that's fine.

873
00:47:30,630 --> 00:47:33,450
But we can't draw
conclusions using

874
00:47:33,449 --> 00:47:36,779
things like the Empirical
Rule or the Central Limit

875
00:47:36,780 --> 00:47:38,940
Theorem, Standard Error.

876
00:47:38,940 --> 00:47:43,200
Because the basic assumption
underlying all of that

877
00:47:43,199 --> 00:47:48,179
is that the samples are
random and independent.

878
00:47:48,179 --> 00:47:51,190
This is one of the reasons
why political polls are

879
00:47:51,190 --> 00:47:53,019
so unreliable.

880
00:47:53,019 --> 00:47:56,230
They compute statistics
using Standard Error,

881
00:47:56,230 --> 00:47:59,940
assuming that the samples
are random and independent.

882
00:47:59,940 --> 00:48:06,030
But they, for example, get them
mostly by calling landlines.

883
00:48:06,030 --> 00:48:09,090
And so they get a bias
towards people who actually

884
00:48:09,090 --> 00:48:11,621
answer the phone on a landline.

885
00:48:11,621 --> 00:48:13,619
How many of you have a
land line where you live?

886
00:48:13,619 --> 00:48:16,150


887
00:48:16,150 --> 00:48:16,950
Not many, right?

888
00:48:16,949 --> 00:48:20,469
Mostly you rely on
your cell phones.

889
00:48:20,469 --> 00:48:24,069
And so any survey that
depends on landlines

890
00:48:24,070 --> 00:48:27,090
is going to leave a lot
of the population out.

891
00:48:27,090 --> 00:48:30,280
They'll get a lot of
people of my vintage,

892
00:48:30,280 --> 00:48:33,220
not of your vintage.

893
00:48:33,219 --> 00:48:36,819
And that gets you in trouble.

894
00:48:36,820 --> 00:48:42,200
So the moral here
is always understand

895
00:48:42,199 --> 00:48:48,109
how the data was collected, what
the assumptions in the analysis

896
00:48:48,110 --> 00:48:53,700
were, and whether
they're satisfied.

897
00:48:53,699 --> 00:48:56,000
If these things
are not true, you

898
00:48:56,000 --> 00:48:59,690
need to be very
wary of the results.

899
00:48:59,690 --> 00:49:02,099
All right, I think
I'll stop here.

900
00:49:02,099 --> 00:49:07,130
We'll finish up our
panoply of statistical sins

901
00:49:07,130 --> 00:49:09,340
on Wednesday, in the first half.

902
00:49:09,340 --> 00:49:11,210
Then we'll do a course wrap-up.

903
00:49:11,210 --> 00:49:14,449
Then I'll wish you all
godspeed and a good final.

904
00:49:14,449 --> 00:49:16,569
See you Wednesday.

905
00:49:16,570 --> 00:49:24,583


