1
00:00:00,000 --> 00:00:00,790


2
00:00:00,790 --> 00:00:03,129
The following content is
provided under a Creative

3
00:00:03,129 --> 00:00:04,549
Commons license.

4
00:00:04,549 --> 00:00:06,759
Your support will help
MIT OpenCourseWare

5
00:00:06,759 --> 00:00:10,849
continue to offer high quality
educational resources for free.

6
00:00:10,849 --> 00:00:13,390
To make a donation or to
view additional materials

7
00:00:13,390 --> 00:00:17,320
from hundreds of MIT courses,
visit MIT OpenCourseWare

8
00:00:17,320 --> 00:00:18,570
at ocw.mit.edu.

9
00:00:18,570 --> 00:00:28,780


10
00:00:28,780 --> 00:00:30,578
JOHN GUTTAG: Hello, everybody.

11
00:00:30,577 --> 00:00:33,329


12
00:00:33,329 --> 00:00:36,570
Well, here we are
at the last lecture.

13
00:00:36,570 --> 00:00:39,667
We're going to finish talking
about statistical sins

14
00:00:39,667 --> 00:00:41,250
and then do a little
bit of a wrap-up.

15
00:00:41,250 --> 00:00:44,880


16
00:00:44,880 --> 00:00:46,440
Let's look at a hot topic--

17
00:00:46,439 --> 00:00:52,349
global fiction-- or global
warming, fact or fiction.

18
00:00:52,350 --> 00:00:55,550
You've done a
problem set related

19
00:00:55,549 --> 00:00:58,189
to temperatures in the US.

20
00:00:58,189 --> 00:01:01,039
Here is a plot
generally accepted

21
00:01:01,039 --> 00:01:06,650
of the change in temperatures
on the planet between 1880

22
00:01:06,650 --> 00:01:10,430
and 2014.

23
00:01:10,430 --> 00:01:14,210
Now, if we look at this plot,
we could see this commits one

24
00:01:14,209 --> 00:01:18,489
of the statistical sins I
complained about on Monday,

25
00:01:18,489 --> 00:01:21,969
that look where it's
starting the y-axis, way down

26
00:01:21,969 --> 00:01:24,189
here at 55.

27
00:01:24,189 --> 00:01:27,700
And you remember, I told you to
beware of charts for the y-axis

28
00:01:27,700 --> 00:01:30,540
doesn't start at 0.

29
00:01:30,540 --> 00:01:32,190
So maybe the people
who are trying

30
00:01:32,189 --> 00:01:35,909
to claim about global
warming are just deceiving us

31
00:01:35,909 --> 00:01:38,619
with this trick of the axis.

32
00:01:38,620 --> 00:01:41,969
So here's what happens
when you put it at 0.

33
00:01:41,969 --> 00:01:44,019
And as you can see--

34
00:01:44,019 --> 00:01:48,689
or barely see-- this axis
runs from 0 up to 110

35
00:01:48,689 --> 00:01:51,030
as the average temperature.

36
00:01:51,030 --> 00:01:52,710
And as you can
see quite clearly,

37
00:01:52,709 --> 00:01:55,939
it's hardly changed at all.

38
00:01:55,939 --> 00:01:59,019
So what's the deal here?

39
00:01:59,019 --> 00:02:04,449
Well, which is a more accurate
presentation of the facts?

40
00:02:04,450 --> 00:02:07,760
Which conveys the
accurate impression?

41
00:02:07,760 --> 00:02:09,909
Let's look at another
example, maybe

42
00:02:09,909 --> 00:02:14,530
a little less controversial
than climate change--

43
00:02:14,530 --> 00:02:16,629
fever and flu.

44
00:02:16,629 --> 00:02:19,479
It's generally accepted
that when you get the flu

45
00:02:19,479 --> 00:02:21,789
you might run a fever.

46
00:02:21,789 --> 00:02:23,439
So here is someone
who had the flu.

47
00:02:23,439 --> 00:02:29,680
And this is plotting their fever
from the beginning to its peak.

48
00:02:29,680 --> 00:02:32,379
And it does appear, if we
were to fit a curve to this,

49
00:02:32,379 --> 00:02:35,919
it would look pretty
much like that.

50
00:02:35,919 --> 00:02:38,559
On the other hand, if we assume
that somebody's temperature

51
00:02:38,560 --> 00:02:42,334
could range between
0 and 200, we

52
00:02:42,334 --> 00:02:44,500
can see that, in fact, your
temperature doesn't move

53
00:02:44,500 --> 00:02:47,280
at all when you get the flu.

54
00:02:47,280 --> 00:02:50,550
So the moral is
pretty clear, I think.

55
00:02:50,550 --> 00:02:54,689
Even though on Monday I talked
about being suspicious when

56
00:02:54,689 --> 00:03:00,039
people start the
y-axis too far from 0,

57
00:03:00,039 --> 00:03:02,019
you should truncate
it to eliminate

58
00:03:02,020 --> 00:03:05,680
totally preposterous values.

59
00:03:05,680 --> 00:03:07,510
No living person
has a temperature

60
00:03:07,509 --> 00:03:11,539
of 0 degrees Fahrenheit.

61
00:03:11,539 --> 00:03:18,090
So again, don't truncate it
just to make something look

62
00:03:18,090 --> 00:03:24,000
like it isn't, but don't
expand it to deceive either.

63
00:03:24,000 --> 00:03:27,419
Let's return to global warming.

64
00:03:27,419 --> 00:03:29,039
This is a chart
that was actually

65
00:03:29,039 --> 00:03:31,919
shown on the floor
of the US Senate

66
00:03:31,919 --> 00:03:38,969
by a senator from Texas,
who I shall not name.

67
00:03:38,969 --> 00:03:41,189
And obviously, the
argument here was

68
00:03:41,189 --> 00:03:45,609
that, well, sure global
warming bounces up and down.

69
00:03:45,610 --> 00:03:53,220
But if we go back, we can
see here, the date is 19--

70
00:03:53,219 --> 00:03:53,969
can we see it?

71
00:03:53,969 --> 00:03:56,240
I can see it.

72
00:03:56,240 --> 00:04:00,050
Maybe 1986, I think.

73
00:04:00,050 --> 00:04:01,969
You can see that
the argument here

74
00:04:01,969 --> 00:04:05,629
is, in fact, if you fit a trend
line to this, as he's done,

75
00:04:05,629 --> 00:04:08,669
it hasn't changed at all.

76
00:04:08,669 --> 00:04:12,139
And so even though we've had
a lot of carbon emissions

77
00:04:12,139 --> 00:04:16,069
during this period,
maybe global warming

78
00:04:16,069 --> 00:04:18,709
is not actually happening.

79
00:04:18,709 --> 00:04:24,370
This is in contradiction to
the trend I showed before.

80
00:04:24,370 --> 00:04:26,909
Well, what's going on here?

81
00:04:26,910 --> 00:04:33,140
This is a very common way that
people use statistics poorly.

82
00:04:33,139 --> 00:04:37,399
They confuse
fluctuations with trends.

83
00:04:37,399 --> 00:04:40,879
What we see in any
theories of data--

84
00:04:40,879 --> 00:04:43,040
time series, or other series--

85
00:04:43,040 --> 00:04:44,580
you always have fluctuations.

86
00:04:44,579 --> 00:04:48,529


87
00:04:48,529 --> 00:04:52,579
And that's not to be
confused with the trend.

88
00:04:52,579 --> 00:04:55,459
And in particular, what you
need to think about when you're

89
00:04:55,459 --> 00:04:58,310
looking at a
phenomenon is choose

90
00:04:58,310 --> 00:05:03,860
an interval consistent with the
thing that's being considered.

91
00:05:03,860 --> 00:05:06,050
So we believe that
climate change

92
00:05:06,050 --> 00:05:10,910
is something that happens over
very long periods of time.

93
00:05:10,910 --> 00:05:12,860
And it's a little bit
silly to look at it

94
00:05:12,860 --> 00:05:15,259
on a short period of time.

95
00:05:15,259 --> 00:05:17,120
Some of you may
remember two years ago,

96
00:05:17,120 --> 00:05:20,016
we had a very cold winter here.

97
00:05:20,016 --> 00:05:21,850
And there were people
who were saying, well,

98
00:05:21,850 --> 00:05:24,650
that shows we don't
have global warming.

99
00:05:24,649 --> 00:05:27,339
Well, you can't really conclude
anything about climate change

100
00:05:27,339 --> 00:05:30,879
looking at a year, or
probably not even looking

101
00:05:30,879 --> 00:05:33,529
at 10 years or 20 years.

102
00:05:33,529 --> 00:05:35,859
It's a very slow phenomenon.

103
00:05:35,860 --> 00:05:37,600
On the other hand,
if you're looking

104
00:05:37,600 --> 00:05:40,270
at the change in
somebody's heart rate,

105
00:05:40,269 --> 00:05:42,310
seeing if they have
a heart condition,

106
00:05:42,310 --> 00:05:46,329
you probably don't want to look
at it over a 10-year period.

107
00:05:46,329 --> 00:05:48,099
So you have to decide
what you're doing

108
00:05:48,100 --> 00:05:52,780
and find an interval that lets
you look at the trends rather

109
00:05:52,779 --> 00:05:56,179
than the fluctuations.

110
00:05:56,180 --> 00:05:59,090
Any rate, maybe even if we're
having global warming, at least

111
00:05:59,089 --> 00:06:02,659
the Arctic ice isn't
melting, though apparently,

112
00:06:02,660 --> 00:06:07,580
I read in the paper this morning
they found a huge crack in it.

113
00:06:07,579 --> 00:06:10,399
So this was reported
in the Financial Post

114
00:06:10,399 --> 00:06:14,750
on April 15, 2013.

115
00:06:14,750 --> 00:06:17,069
You can read it yourself.

116
00:06:17,069 --> 00:06:20,509
But the basic
import of it is they

117
00:06:20,509 --> 00:06:29,899
took the period from April
14, 1989 to April 15, 2013

118
00:06:29,899 --> 00:06:35,009
and said, look,
it's not changing.

119
00:06:35,009 --> 00:06:41,250
In fact, the amount of
arctic ice is unchanged.

120
00:06:41,250 --> 00:06:42,879
Well, what's the financial--

121
00:06:42,879 --> 00:06:46,089
not the financial-- what's
the statistical sin being

122
00:06:46,089 --> 00:06:46,929
committed here?

123
00:06:46,930 --> 00:06:50,290


124
00:06:50,290 --> 00:06:56,580
If we look at this data,
this is an anomaly chart.

125
00:06:56,579 --> 00:06:59,519
I think you saw one of these
in one of the problems sets,

126
00:06:59,519 --> 00:07:01,859
where you fix
something at 0 and then

127
00:07:01,860 --> 00:07:05,340
you show fluctuations
relative to that.

128
00:07:05,339 --> 00:07:10,573
So here, it's the Arctic
ice relative to a point.

129
00:07:10,574 --> 00:07:16,160
And what we see here is
that if you go and choose

130
00:07:16,160 --> 00:07:18,380
the right date--

131
00:07:18,379 --> 00:07:30,110
say this one in 1989--

132
00:07:30,110 --> 00:07:38,389
and you come over here and you
choose the right date in 2013--

133
00:07:38,389 --> 00:07:45,569
say this one-- you can then
draw a line and say, oh, look,

134
00:07:45,569 --> 00:07:46,324
it hasn't changed.

135
00:07:46,324 --> 00:07:49,459


136
00:07:49,459 --> 00:07:52,459
This is something
people frequently do,

137
00:07:52,459 --> 00:07:56,569
is they take a
whole set of data,

138
00:07:56,569 --> 00:07:59,959
and they find two points that
are consistent with something

139
00:07:59,959 --> 00:08:02,139
they believe.

140
00:08:02,139 --> 00:08:05,560
And they draw a line
between those two points,

141
00:08:05,560 --> 00:08:09,476
fit a curve to those two points,
and draw some conclusion.

142
00:08:09,476 --> 00:08:12,279


143
00:08:12,279 --> 00:08:15,489
This is what we
call cherry picking,

144
00:08:15,490 --> 00:08:17,290
I guess from the
notion that when

145
00:08:17,290 --> 00:08:20,800
you go to pick cherries you only
want to pick the right ones,

146
00:08:20,800 --> 00:08:23,079
leave the others to ripen
for a bit on the tree.

147
00:08:23,079 --> 00:08:25,990


148
00:08:25,990 --> 00:08:28,615
It's really bad.

149
00:08:28,615 --> 00:08:30,240
And it's something
that, unfortunately,

150
00:08:30,240 --> 00:08:34,009
the scientific literature
is replete with,

151
00:08:34,009 --> 00:08:35,720
where people look
at a lot of data,

152
00:08:35,720 --> 00:08:40,220
and they pick the points that
match what they want to prove.

153
00:08:40,220 --> 00:08:43,610
And so as you can see, while
the trend is quite clear,

154
00:08:43,610 --> 00:08:47,419
you could prove almost anything
you wanted by selecting

155
00:08:47,419 --> 00:08:50,129
two points very carefully.

156
00:08:50,129 --> 00:08:52,470
I could also show
that it's crashing

157
00:08:52,470 --> 00:08:57,660
much faster than people think it
is by picking these two points.

158
00:08:57,659 --> 00:09:00,409
If I wanted to argue
that it's catastrophic,

159
00:09:00,409 --> 00:09:03,029
I'd pick those two points
and say, look at that,

160
00:09:03,029 --> 00:09:06,959
it's disappearing at
an incredible rate.

161
00:09:06,960 --> 00:09:10,170
So you can lie in either
direction with this data

162
00:09:10,169 --> 00:09:11,519
by careful cherry picking.

163
00:09:11,519 --> 00:09:16,009


164
00:09:16,009 --> 00:09:18,500
As a service to you, I know
the holidays are coming

165
00:09:18,500 --> 00:09:21,590
and many of you have not bought
presents for your parents,

166
00:09:21,590 --> 00:09:25,470
so here's a modest
gift suggestion,

167
00:09:25,470 --> 00:09:29,920
that the family that shoots
together something or other.

168
00:09:29,919 --> 00:09:34,629
Well, all right, so we can
ask, is this a good gift?

169
00:09:34,629 --> 00:09:36,149
Well, probably.

170
00:09:36,149 --> 00:09:37,829
We can look at this statistic.

171
00:09:37,830 --> 00:09:39,750
It's not dangerous at least.

172
00:09:39,750 --> 00:09:43,440
We see that 99.8% of
the firearms in the US

173
00:09:43,440 --> 00:09:46,850
will not be used to
commit a violent crime.

174
00:09:46,850 --> 00:09:50,360
So guns apparently are not
actually dangerous, or at least

175
00:09:50,360 --> 00:09:53,990
not in the hands of criminals.

176
00:09:53,990 --> 00:09:56,120
Well, let's look at this.

177
00:09:56,120 --> 00:09:59,529
How many privately owned
firearms are there in the US?

178
00:09:59,529 --> 00:10:03,531
And anyone want to guess
who hasn't looked ahead?

179
00:10:03,532 --> 00:10:04,508
Yeah.

180
00:10:04,508 --> 00:10:05,972
AUDIENCE: 400 million.

181
00:10:05,971 --> 00:10:07,579
JOHN GUTTAG: 400 million.

182
00:10:07,580 --> 00:10:11,210
340 million people
and 400 million guns

183
00:10:11,210 --> 00:10:15,350
is the guess, more
than one per person.

184
00:10:15,350 --> 00:10:17,540
You certainly are the
right order of magnitude.

185
00:10:17,539 --> 00:10:21,799
I think it's about 300 million,
but it's hard to count them.

186
00:10:21,799 --> 00:10:25,889
Maybe this doesn't
count water pistols.

187
00:10:25,889 --> 00:10:30,929
So if you assume there
are 300 million firearms

188
00:10:30,929 --> 00:10:35,169
and 0.2% of them
are used to commit

189
00:10:35,169 --> 00:10:38,959
a violent crime
in every year, we

190
00:10:38,960 --> 00:10:41,629
see that how many
crimes is that?

191
00:10:41,629 --> 00:10:45,539
600,000.

192
00:10:45,539 --> 00:10:49,799
So in fact, it's
not necessarily very

193
00:10:49,799 --> 00:10:51,479
meaningful to say
that most of them

194
00:10:51,480 --> 00:10:54,210
are not used to commit a crime.

195
00:10:54,210 --> 00:10:57,600
Well, let's look
at another place

196
00:10:57,600 --> 00:11:01,379
where we look at a statistic.

197
00:11:01,379 --> 00:11:05,580
Probably most of you don't even
remember the scary swine flu

198
00:11:05,580 --> 00:11:08,370
epidemic.

199
00:11:08,370 --> 00:11:10,799
This was a big headline.

200
00:11:10,799 --> 00:11:13,049
And people got so
scared of the swine flu

201
00:11:13,049 --> 00:11:16,229
they were doing things
like closing schools

202
00:11:16,230 --> 00:11:19,110
to try limit the
spread of the flu.

203
00:11:19,110 --> 00:11:21,899
New York City closed some
schools because of it,

204
00:11:21,899 --> 00:11:23,519
for example.

205
00:11:23,519 --> 00:11:26,860
So is this a scary statistic?

206
00:11:26,860 --> 00:11:31,610
Well, maybe, but here's
an interesting statistic.

207
00:11:31,610 --> 00:11:34,220
How many deaths per year
are from the seasonal flu

208
00:11:34,220 --> 00:11:35,610
in the US--

209
00:11:35,610 --> 00:11:39,470
the ones we try and
prevent with a flu shot?

210
00:11:39,470 --> 00:11:42,259
36,000.

211
00:11:42,259 --> 00:11:47,179
So what we see is that, it
doesn't make a lot of sense

212
00:11:47,179 --> 00:11:52,949
to panic over 159 in the
light of this number.

213
00:11:52,950 --> 00:11:56,800


214
00:11:56,799 --> 00:12:00,219
So the point here for
both this and the issue

215
00:12:00,220 --> 00:12:04,300
about the firearms is
that context matters.

216
00:12:04,299 --> 00:12:07,199


217
00:12:07,200 --> 00:12:08,480
Yeah, I love this cartoon.

218
00:12:08,480 --> 00:12:12,289


219
00:12:12,289 --> 00:12:16,789
A number without context
is just a number.

220
00:12:16,789 --> 00:12:20,329
And numbers by themselves
don't mean anything.

221
00:12:20,330 --> 00:12:23,720
So to say that there were
159 deaths from the swine flu

222
00:12:23,720 --> 00:12:27,830
is not very meaningful
without some context.

223
00:12:27,830 --> 00:12:32,330
To say that only
0.2% of firearms

224
00:12:32,330 --> 00:12:34,610
are used to commit
a violent crime

225
00:12:34,610 --> 00:12:37,580
is not very meaningful
without context.

226
00:12:37,580 --> 00:12:40,580
Whenever you're
presenting a statistic,

227
00:12:40,580 --> 00:12:42,230
reading about a
statistic, and you just

228
00:12:42,230 --> 00:12:44,750
see a number that
seems comforting

229
00:12:44,750 --> 00:12:48,919
or terrifying, try and put
some context around it.

230
00:12:48,919 --> 00:12:51,849


231
00:12:51,850 --> 00:12:56,300
So a related thing
is relative to what?

232
00:12:56,299 --> 00:12:59,539
Suppose I told you that
skipping lectures increases

233
00:12:59,539 --> 00:13:04,409
your probability of
failing this course by 50%.

234
00:13:04,409 --> 00:13:07,649
Well, you would all feel
great, because you're here.

235
00:13:07,649 --> 00:13:10,470
And you would be laughing at
your friends who are not here,

236
00:13:10,470 --> 00:13:14,879
because figuring that will leave
much better grades for you.

237
00:13:14,879 --> 00:13:17,519
What does this mean, though?

238
00:13:17,519 --> 00:13:22,309
Well, if I told
you that it changed

239
00:13:22,309 --> 00:13:28,059
the probability of failing
from a half to 0.75,

240
00:13:28,059 --> 00:13:32,299
you would be very tempted
to come to lectures.

241
00:13:32,299 --> 00:13:37,549
On the other hand, if I told you
that it changed the probability

242
00:13:37,549 --> 00:13:42,868
from 0.005 to 0.0075, you
might say, the heck with it,

243
00:13:42,869 --> 00:13:43,910
I'd rather go to the gym.

244
00:13:43,909 --> 00:13:48,089


245
00:13:48,090 --> 00:13:49,500
Again this, is an issue.

246
00:13:49,500 --> 00:13:53,070
And this is something that we
see all the time when people

247
00:13:53,070 --> 00:13:57,440
talk about percentage change.

248
00:13:57,440 --> 00:14:03,510
This is particularly prominent
in the pharmaceutical field.

249
00:14:03,509 --> 00:14:09,179
You will read a headline saying
that drug x for arthritis

250
00:14:09,179 --> 00:14:16,979
increases the probability of
a heart attack by 1% or 5%.

251
00:14:16,980 --> 00:14:18,810
Well, what does that mean?

252
00:14:18,809 --> 00:14:22,859
If the probability was already
very low, increasing it by 5%,

253
00:14:22,860 --> 00:14:25,620
it's still very low.

254
00:14:25,620 --> 00:14:30,149
And maybe it's worth it not
to be in pain from arthritis.

255
00:14:30,149 --> 00:14:34,439
So talking in
percentages is, again,

256
00:14:34,440 --> 00:14:38,010
one of these issues of
it doesn't make sense

257
00:14:38,009 --> 00:14:39,299
without the context.

258
00:14:39,299 --> 00:14:42,000
In order to know
what this means,

259
00:14:42,000 --> 00:14:44,879
I need to know what regime
I'm in here in order

260
00:14:44,879 --> 00:14:48,419
to make a intelligent decisions
about whether to attend lecture

261
00:14:48,419 --> 00:14:50,469
or not.

262
00:14:50,470 --> 00:14:55,180
It goes without saying, you have
all made the right decision.

263
00:14:55,179 --> 00:14:58,349
So beware of
percentage change when

264
00:14:58,350 --> 00:15:01,019
you don't know the denominator.

265
00:15:01,019 --> 00:15:03,210
You get a percentage by
dividing by something.

266
00:15:03,210 --> 00:15:05,759
And if you don't know
what you're dividing by,

267
00:15:05,759 --> 00:15:09,179
then the percentage is
itself a meaningless number.

268
00:15:09,179 --> 00:15:14,439


269
00:15:14,440 --> 00:15:18,220
While we're sort of talking
about medical things,

270
00:15:18,220 --> 00:15:21,639
let's look at cancer
clusters to illustrate

271
00:15:21,639 --> 00:15:26,090
another statistical question.

272
00:15:26,090 --> 00:15:29,970
So this is a definition of a
cancer cluster by the CDC--

273
00:15:29,970 --> 00:15:33,160
"a greater-than-expected
number of cancer cases

274
00:15:33,159 --> 00:15:36,370
that occurs in a group of
people in a geographic area

275
00:15:36,370 --> 00:15:39,210
over a period of time."

276
00:15:39,210 --> 00:15:41,280
And the key part
of this definition

277
00:15:41,279 --> 00:15:44,370
is greater-than-expected.

278
00:15:44,370 --> 00:15:48,090


279
00:15:48,090 --> 00:15:54,870
About 1,000 cancer clusters per
year are reported in the US,

280
00:15:54,870 --> 00:15:57,060
mostly to the Centers
for Disease Control,

281
00:15:57,059 --> 00:16:01,939
but in general to
other health agencies.

282
00:16:01,940 --> 00:16:08,200
Upon analysis, almost none
of them pass this test.

283
00:16:08,200 --> 00:16:12,460
So the vast majority,
some years all of them,

284
00:16:12,460 --> 00:16:14,915
are deemed actually not
to be cancer clusters.

285
00:16:14,914 --> 00:16:18,405


286
00:16:18,405 --> 00:16:20,529
So I don't know if-- has
anyone here seen the movie

287
00:16:20,529 --> 00:16:23,209
Erin Brockovich?

288
00:16:23,210 --> 00:16:25,790
Subsequent analysis showed
that was actually not

289
00:16:25,789 --> 00:16:26,750
a cancer cluster.

290
00:16:26,750 --> 00:16:29,789


291
00:16:29,789 --> 00:16:35,449
It's a good movie, but turns
out statistically wrong.

292
00:16:35,450 --> 00:16:37,400
This, by the way, is
not a cancer cluster.

293
00:16:37,399 --> 00:16:40,009
This is a constellation.

294
00:16:40,009 --> 00:16:43,309
So let's look at a
hypothetical example.

295
00:16:43,309 --> 00:16:46,189
By the way, the other
movie about cancer clusters

296
00:16:46,190 --> 00:16:48,590
was the one set
in Massachusetts.

297
00:16:48,590 --> 00:16:49,340
What was the name?

298
00:16:49,340 --> 00:16:50,120
A Civil Action.

299
00:16:50,120 --> 00:16:52,190
Anyone see that?

300
00:16:52,190 --> 00:16:53,000
No.

301
00:16:53,000 --> 00:16:54,379
That was a cancer cluster.

302
00:16:54,379 --> 00:16:57,070


303
00:16:57,070 --> 00:17:01,230
Massachusetts is about
10,000 square miles.

304
00:17:01,230 --> 00:17:04,289
And there are about 36,000
cancer cases per year

305
00:17:04,289 --> 00:17:07,019
reported in Massachusetts.

306
00:17:07,019 --> 00:17:08,730
Those two numbers are accurate.

307
00:17:08,730 --> 00:17:12,368
And the rest of this
is pure fiction.

308
00:17:12,368 --> 00:17:16,899
So let's assume that we had
some ambitious attorney who

309
00:17:16,900 --> 00:17:22,550
partitioned the state into 1,000
regions of 10 square miles each

310
00:17:22,549 --> 00:17:25,068
and looked at the
distribution of cancer cases

311
00:17:25,068 --> 00:17:29,000
in these regions trying to find
cancer clusters that he or she

312
00:17:29,000 --> 00:17:32,119
could file a lawsuit about.

313
00:17:32,119 --> 00:17:34,739
Well, you can do
some arithmetic.

314
00:17:34,740 --> 00:17:38,910
And if there are 36,000
new cancer cases a year

315
00:17:38,910 --> 00:17:41,420
and we have 1,000
regions, that should

316
00:17:41,420 --> 00:17:47,150
say that we should
get about 36 cancer

317
00:17:47,150 --> 00:17:50,540
cases per year and per region.

318
00:17:50,539 --> 00:17:53,409
Well, when the attorney
look at the data,

319
00:17:53,410 --> 00:17:56,529
this mythical
attorney, he discovered

320
00:17:56,529 --> 00:18:01,960
that region number
111 had 143 new cancer

321
00:18:01,960 --> 00:18:05,240
cases over a three-year period.

322
00:18:05,240 --> 00:18:09,870
He compared that to 3
times 36 and said, wow,

323
00:18:09,869 --> 00:18:11,629
that's 32% more than expected.

324
00:18:11,630 --> 00:18:14,140


325
00:18:14,140 --> 00:18:15,220
I've got a lawsuit.

326
00:18:15,220 --> 00:18:16,779
So he went to tell
all these people--

327
00:18:16,779 --> 00:18:18,539
they lived in a cancer cluster.

328
00:18:18,539 --> 00:18:20,569
And the question is,
should they be worried?

329
00:18:20,569 --> 00:18:24,349


330
00:18:24,349 --> 00:18:28,849
Well, another way to look at the
question is, how likely is it

331
00:18:28,849 --> 00:18:32,509
that it was just bad luck?

332
00:18:32,509 --> 00:18:34,430
That's the question
we've always ask when

333
00:18:34,430 --> 00:18:36,920
we do statistical analysis--

334
00:18:36,920 --> 00:18:40,820
is this result meaningful, or
is it just random variation

335
00:18:40,819 --> 00:18:43,589
that you would expect to see?

336
00:18:43,589 --> 00:18:50,369
So I wrote some code to simulate
it to see what happens--

337
00:18:50,369 --> 00:18:55,619
so number of cases,
36,000, number of years, 3.

338
00:18:55,619 --> 00:19:00,149
So all of this is just the
numbers I had on the slide.

339
00:19:00,150 --> 00:19:02,880
We'll do a simulation.

340
00:19:02,880 --> 00:19:04,920
We'll take 100 trials.

341
00:19:04,920 --> 00:19:07,565


342
00:19:07,565 --> 00:19:09,690
And then what I'm going to
do is for t in the range

343
00:19:09,690 --> 00:19:14,299
number of trials, the locations,
the regions, if you will,

344
00:19:14,299 --> 00:19:19,470
I'll initialize each to 0,
1,000 of them, in this case.

345
00:19:19,470 --> 00:19:22,289
And then for i in the
range number of years

346
00:19:22,289 --> 00:19:28,740
times number of cases per year,
so this will be 3 times 36,000.

347
00:19:28,740 --> 00:19:34,509
At random, I will assign the
case to one of these regions.

348
00:19:34,509 --> 00:19:36,549
This is the random.

349
00:19:36,549 --> 00:19:40,279
Nothing to do with cancer
clusters, just at random,

350
00:19:40,279 --> 00:19:44,200
this case gets assigned to
one of the 1,000 regions.

351
00:19:44,200 --> 00:19:55,600
And then I'm going to check if
region number 111 had greater

352
00:19:55,599 --> 00:20:01,029
than or equal to 143, the number
of cases we assumed it had.

353
00:20:01,029 --> 00:20:05,079
If so, we'll increment the
variable num greater by 1,

354
00:20:05,079 --> 00:20:11,849
saying, in this trial of 100,
indeed, it had that many.

355
00:20:11,849 --> 00:20:14,579
And then we'll see how
often that happens.

356
00:20:14,579 --> 00:20:20,039
That will tell us how improbable
it is that region 111 actually

357
00:20:20,039 --> 00:20:20,940
had that many cases.

358
00:20:20,940 --> 00:20:23,779


359
00:20:23,779 --> 00:20:25,675
And then we'll print it.

360
00:20:25,675 --> 00:20:27,819
Does that makes
sense to everyone,

361
00:20:27,819 --> 00:20:30,970
that here I am
doing my simulation

362
00:20:30,970 --> 00:20:36,160
to see whether or not how
probable is it that 111

363
00:20:36,160 --> 00:20:39,230
would have had this many cases?

364
00:20:39,230 --> 00:20:41,870
Any questions?

365
00:20:41,869 --> 00:20:42,500
Let's run it.

366
00:20:42,500 --> 00:20:52,480


367
00:20:52,480 --> 00:20:54,150
So here's the code
we just looked at.

368
00:20:54,150 --> 00:21:05,430


369
00:21:05,430 --> 00:21:06,840
Takes just a second.

370
00:21:06,839 --> 00:21:17,309
That's why I did only 100
trials instead of 1,000.

371
00:21:17,309 --> 00:21:20,099
I know the suspense
is killing you.

372
00:21:20,099 --> 00:21:20,898
It's killing me.

373
00:21:20,898 --> 00:21:22,439
I don't know why
it's taking so long.

374
00:21:22,440 --> 00:21:29,130


375
00:21:29,130 --> 00:21:29,830
We'll finish.

376
00:21:29,829 --> 00:21:33,159


377
00:21:33,160 --> 00:21:35,560
I wish I had the Jeopardy
music or something

378
00:21:35,559 --> 00:21:37,976
to play while we
waited for this.

379
00:21:37,977 --> 00:21:40,060
Anna, can you home some
music or something to keep

380
00:21:40,059 --> 00:21:42,339
people amused?

381
00:21:42,339 --> 00:21:43,963
She will not.

382
00:21:43,963 --> 00:21:44,462
Wow.

383
00:21:44,462 --> 00:21:48,769


384
00:21:48,769 --> 00:21:49,549
So here it is.

385
00:21:49,549 --> 00:21:51,980
The estimated
probability of region 111

386
00:21:51,980 --> 00:21:53,569
having at least 1 case--

387
00:21:53,569 --> 00:21:57,980


388
00:21:57,980 --> 00:22:00,920
at least 143 cases--

389
00:22:00,920 --> 00:22:13,390
easier to read if I
spread this out is 0.01.

390
00:22:13,390 --> 00:22:18,110
So it seems, in fact, that
it's pretty surprising--

391
00:22:18,109 --> 00:22:20,019
unlikely to have
happened at random.

392
00:22:20,019 --> 00:22:22,990


393
00:22:22,990 --> 00:22:23,710
Do you buy it?

394
00:22:23,710 --> 00:22:25,240
Or is there a flaw here?

395
00:22:25,240 --> 00:22:29,440


396
00:22:29,440 --> 00:22:32,070
Getting back to
this whole question.

397
00:22:32,069 --> 00:22:33,548
Yes.

398
00:22:33,548 --> 00:22:36,726
AUDIENCE: I think it's
flawed because first off you

399
00:22:36,726 --> 00:22:38,438
have to look at the population.

400
00:22:38,438 --> 00:22:39,416
That is more important.

401
00:22:39,415 --> 00:22:41,371
JOHN GUTTAG: You
have to look at what?

402
00:22:41,372 --> 00:22:45,284
AUDIENCE: Population as opposed
to like the number of areas,

403
00:22:45,284 --> 00:22:48,218
because when you get past the
Boston area, you'd expect a--

404
00:22:48,218 --> 00:22:50,509
JOHN GUTTAG: Let's assume
that, in fact, instead of

405
00:22:50,509 --> 00:22:52,059
by square miles--

406
00:22:52,059 --> 00:22:54,700
let's assume the
populations were balanced.

407
00:22:54,700 --> 00:22:56,324
AUDIENCE: Then I also
think it's flawed

408
00:22:56,324 --> 00:22:59,691
because I don't think the
importance of block 111

409
00:22:59,691 --> 00:23:01,615
having 143 is important.

410
00:23:01,615 --> 00:23:04,509
I think the importance is just
one area having a higher--

411
00:23:04,509 --> 00:23:07,960
JOHN GUTTAG: Exactly right.

412
00:23:07,960 --> 00:23:09,049
Exactly right.

413
00:23:09,049 --> 00:23:13,029


414
00:23:13,029 --> 00:23:16,720
I'm sorry, I forgot
my candy bag today.

415
00:23:16,720 --> 00:23:18,720
Just means there'll be
more candy for the final.

416
00:23:18,720 --> 00:23:26,690


417
00:23:26,690 --> 00:23:34,960
What we have here is a
variant of cherry picking.

418
00:23:34,960 --> 00:23:37,779
What I have done
in this simulation

419
00:23:37,779 --> 00:23:41,119
is I've looked at 1,000
different regions.

420
00:23:41,119 --> 00:23:44,879


421
00:23:44,880 --> 00:23:47,520
What the attorney did
is, not in a simulation,

422
00:23:47,519 --> 00:23:50,490
is he looked at 1,000
different regions,

423
00:23:50,490 --> 00:23:54,690
found the one with the most
cancer cases, and said,

424
00:23:54,690 --> 00:23:57,029
aha, there are too many here.

425
00:23:57,029 --> 00:24:01,529


426
00:24:01,529 --> 00:24:06,129
And that's not what I
did in my simulation.

427
00:24:06,130 --> 00:24:08,650
My simulation didn't
ask the question,

428
00:24:08,650 --> 00:24:11,410
how likely is it that there
is at least one region

429
00:24:11,410 --> 00:24:13,480
with that many cases.

430
00:24:13,480 --> 00:24:16,059
But it asked the
question, how likely is it

431
00:24:16,059 --> 00:24:20,399
that this specific region
has that many cases.

432
00:24:20,400 --> 00:24:24,750
Now, if the attorney had reason
in advance to be suspicious

433
00:24:24,750 --> 00:24:27,750
of region 111, then
maybe it would have

434
00:24:27,750 --> 00:24:29,910
been OK to just go check that.

435
00:24:29,910 --> 00:24:32,190
But having looked at
1,000 and then cherry

436
00:24:32,190 --> 00:24:36,400
pick the best is not right.

437
00:24:36,400 --> 00:24:41,410
So this is a simulation
that does the right thing.

438
00:24:41,410 --> 00:24:45,290


439
00:24:45,289 --> 00:24:47,639
I've left out the
initialization.

440
00:24:47,640 --> 00:24:50,390
But what you can
see I'm doing here

441
00:24:50,390 --> 00:24:52,160
is I'm looking at the
probability of there

442
00:24:52,160 --> 00:24:57,220
being any region that
has at least 143 cases.

443
00:24:57,220 --> 00:25:02,630


444
00:25:02,630 --> 00:25:06,910
What this is called in the
technical literature, what

445
00:25:06,910 --> 00:25:11,220
the attorney did is multiple
hypothesis checking.

446
00:25:11,220 --> 00:25:15,450
So rather than having a single
hypothesis, that region 111 is

447
00:25:15,450 --> 00:25:20,430
bad, he checked 1,000
different hypotheses,

448
00:25:20,430 --> 00:25:25,380
and then chose the one
that met what he wanted.

449
00:25:25,380 --> 00:25:27,780
Now, there are good
statistical techniques

450
00:25:27,779 --> 00:25:31,769
that exist for dealing with
multiple hypotheses, things

451
00:25:31,769 --> 00:25:34,529
like the Bonferroni correction.

452
00:25:34,529 --> 00:25:37,349
I love to say that name.

453
00:25:37,349 --> 00:25:41,369
But you have to worry about it.

454
00:25:41,369 --> 00:25:59,689
And in fact, if we
go back to the code

455
00:25:59,690 --> 00:26:14,710
and comment out this
one and run this one,

456
00:26:14,710 --> 00:26:16,720
we'll see we get a
very different answer.

457
00:26:16,720 --> 00:26:42,190


458
00:26:42,190 --> 00:26:44,890
The answer we get is--

459
00:26:44,890 --> 00:26:47,580
let's see.

460
00:26:47,579 --> 00:26:48,449
Oh, I see.

461
00:26:48,450 --> 00:26:52,670
All right, let me
just comment this out.

462
00:26:52,670 --> 00:27:02,970


463
00:27:02,970 --> 00:27:04,220
Yeah, this should work, right?

464
00:27:04,220 --> 00:27:15,951


465
00:27:15,951 --> 00:27:17,700
Well, maybe you don't
want to wait for it.

466
00:27:17,700 --> 00:27:20,730
But the answer you'll
get is that it's actually

467
00:27:20,730 --> 00:27:22,380
very probable.

468
00:27:22,380 --> 00:27:26,520
My recollection is it's a
0.6 probability that at least

469
00:27:26,519 --> 00:27:29,400
one region has that many cases.

470
00:27:29,400 --> 00:27:30,923
And that's really
what's going on

471
00:27:30,923 --> 00:27:32,339
with this whole
business of people

472
00:27:32,339 --> 00:27:34,829
reporting cancer clusters.

473
00:27:34,829 --> 00:27:38,039
It's just by accident,
by pure randomness,

474
00:27:38,039 --> 00:27:40,599
some region has
more than its share.

475
00:27:40,599 --> 00:27:43,289


476
00:27:43,289 --> 00:27:49,889
This particular form
of cherry picking

477
00:27:49,890 --> 00:27:53,100
also goes by the name of the
Texas sharpshooter fallacy.

478
00:27:53,099 --> 00:27:55,959


479
00:27:55,960 --> 00:27:59,230
I don't know why people
pick on Texas for this.

480
00:27:59,230 --> 00:28:00,880
But they seem to.

481
00:28:00,880 --> 00:28:03,280
But the notion is, you're
driving down a road in Texas

482
00:28:03,279 --> 00:28:08,079
and you see a barn with a bunch
of bullet holes in the wall

483
00:28:08,079 --> 00:28:11,740
right in the middle of a target.

484
00:28:11,740 --> 00:28:16,359
But what actually happened
was you had a barn.

485
00:28:16,359 --> 00:28:19,849
The farmer just shot some
things at random at the barn,

486
00:28:19,849 --> 00:28:22,909
then got out his paint brush and
painted a target right around

487
00:28:22,910 --> 00:28:25,220
where they happened to land.

488
00:28:25,220 --> 00:28:29,269
And that's what happens when
you cherry pick hypotheses.

489
00:28:29,269 --> 00:28:33,019


490
00:28:33,019 --> 00:28:36,920
What's the bottom line of all
these statistical fallacies?

491
00:28:36,920 --> 00:28:41,700
When drawing inferences from
data, skepticism is merited.

492
00:28:41,700 --> 00:28:43,440
There are,
unfortunately, more ways

493
00:28:43,440 --> 00:28:46,336
to go wrong than to go right.

494
00:28:46,336 --> 00:28:48,210
And you'll read the
literature that tells you

495
00:28:48,210 --> 00:28:51,120
that in the scientific
literature more than half

496
00:28:51,119 --> 00:28:53,069
of the papers were
later shown to be wrong.

497
00:28:53,069 --> 00:28:56,069


498
00:28:56,069 --> 00:28:59,039
You do need to remember that
skepticism and denial are

499
00:28:59,039 --> 00:29:00,089
different.

500
00:29:00,089 --> 00:29:01,679
It's good to be skeptical.

501
00:29:01,680 --> 00:29:06,390
And I love Ambrose Bierce's
description of the difference

502
00:29:06,390 --> 00:29:08,050
here.

503
00:29:08,049 --> 00:29:10,079
If you had never
read Ambrose Bierce,

504
00:29:10,079 --> 00:29:11,919
he's well worth reading.

505
00:29:11,920 --> 00:29:14,769
He wrote something called
The Devil's Dictionary,

506
00:29:14,769 --> 00:29:18,670
among other things, in which
he has his own definition

507
00:29:18,670 --> 00:29:19,720
of a lot of words.

508
00:29:19,720 --> 00:29:25,029
And he went by the
nickname Bitter Bierce.

509
00:29:25,029 --> 00:29:28,299
And if you read The Devil's
Dictionary, you'll see why.

510
00:29:28,299 --> 00:29:30,789
But this, I think, has
a lot of wisdom in it.

511
00:29:30,789 --> 00:29:33,359


512
00:29:33,359 --> 00:29:37,389
Let's, in the remaining few
minutes, wrap up the course.

513
00:29:37,390 --> 00:29:40,790
So what did we cover in 6.0002?

514
00:29:40,789 --> 00:29:43,629
A lot of things.

515
00:29:43,630 --> 00:29:47,750
If you look at the technical,
things were three major units--

516
00:29:47,750 --> 00:29:51,950
optimization problems,
stochastic thinking,

517
00:29:51,950 --> 00:29:56,299
and modeling aspects
of the world.

518
00:29:56,299 --> 00:29:59,329
But there was a big
subtext amongst all of it,

519
00:29:59,329 --> 00:30:01,750
which was this.

520
00:30:01,750 --> 00:30:06,220
There was a reason our problem
sets were not pencil and paper

521
00:30:06,220 --> 00:30:09,940
probability problems,
but all coding.

522
00:30:09,940 --> 00:30:13,562
And that's because
we really want,

523
00:30:13,561 --> 00:30:15,019
as an important
part of the course,

524
00:30:15,019 --> 00:30:18,309
is to make you a
better programmer.

525
00:30:18,309 --> 00:30:21,669
We introduced a few
extra features of Python.

526
00:30:21,670 --> 00:30:24,970
But more importantly,
we emphasized

527
00:30:24,970 --> 00:30:29,089
the use of libraries, because
in the real world when

528
00:30:29,089 --> 00:30:33,709
you're trying to build things,
you rarely start from scratch.

529
00:30:33,710 --> 00:30:35,150
And if you do
start from scratch,

530
00:30:35,150 --> 00:30:37,600
you're probably
making a mistake.

531
00:30:37,599 --> 00:30:40,490
And so we wanted to get you
used to the idea of finding

532
00:30:40,490 --> 00:30:42,500
and using libraries.

533
00:30:42,500 --> 00:30:45,529
So we looked at plotting
libraries and machine

534
00:30:45,529 --> 00:30:49,250
learning libraries
and numeric libraries.

535
00:30:49,250 --> 00:30:51,791
And hopefully, you
got a lot of practice

536
00:30:51,791 --> 00:30:53,750
in that you're a way
better programmer than you

537
00:30:53,750 --> 00:30:57,230
were six weeks ago.

538
00:30:57,230 --> 00:30:59,370
A little more detailed--

539
00:30:59,369 --> 00:31:04,959
the optimization problems,
the probably most important

540
00:31:04,960 --> 00:31:08,420
takeaway is that many
important problems

541
00:31:08,420 --> 00:31:12,320
can be formulated in terms of
an objective function that you

542
00:31:12,319 --> 00:31:18,250
either maximize or minimize
and some set of constraints.

543
00:31:18,250 --> 00:31:20,980
Once you've done
that, there are lots

544
00:31:20,980 --> 00:31:23,110
of toolboxes, lots
of libraries that you

545
00:31:23,109 --> 00:31:26,149
can use to solve the problem.

546
00:31:26,150 --> 00:31:28,440
You wrote some
optimization code yourself.

547
00:31:28,440 --> 00:31:31,340
But most of the time, we
don't solve them ourselves.

548
00:31:31,339 --> 00:31:34,250
We just call a built-in
function that does it.

549
00:31:34,250 --> 00:31:37,309
So the hard part is
not writing the code,

550
00:31:37,309 --> 00:31:38,929
but doing the formulation.

551
00:31:38,930 --> 00:31:41,529


552
00:31:41,529 --> 00:31:44,410
We talked about
different algorithms--

553
00:31:44,410 --> 00:31:48,790
greedy algorithms,
very often useful,

554
00:31:48,789 --> 00:31:53,509
but often don't find
the optimal solution.

555
00:31:53,509 --> 00:31:57,690
So for example, we looked
at k-means clustering.

556
00:31:57,690 --> 00:32:01,019
It was a very efficient
way to find clusters.

557
00:32:01,019 --> 00:32:04,879
But it did not necessarily find
the optimal set of clusters.

558
00:32:04,880 --> 00:32:07,810


559
00:32:07,809 --> 00:32:10,359
We then observed that
many optimization problems

560
00:32:10,359 --> 00:32:12,119
are inherently exponential.

561
00:32:12,119 --> 00:32:14,729


562
00:32:14,730 --> 00:32:18,269
But even so, dynamic
programming often

563
00:32:18,269 --> 00:32:24,750
works and gives us a
really fast solution.

564
00:32:24,750 --> 00:32:27,690
And the notion here is this is
not an approximate solution.

565
00:32:27,690 --> 00:32:30,269
It's not like using
a greedy algorithm.

566
00:32:30,269 --> 00:32:34,900
It gives you an exact solution
and in many circumstances

567
00:32:34,900 --> 00:32:37,700
gives it to you quickly.

568
00:32:37,700 --> 00:32:39,590
And the other thing I
want you to take away

569
00:32:39,589 --> 00:32:43,609
is, outside the context
of dynamic programming,

570
00:32:43,609 --> 00:32:47,609
memoization is a generally
useful technique.

571
00:32:47,609 --> 00:32:53,789
What we've done there is
we've traded time for space.

572
00:32:53,789 --> 00:32:56,129
We compute something, we
save it, and when we need it,

573
00:32:56,130 --> 00:32:58,130
we look it up.

574
00:32:58,130 --> 00:33:02,670
And that's a very common
programming technique.

575
00:33:02,670 --> 00:33:04,590
And we looked at a lot
of different examples

576
00:33:04,589 --> 00:33:08,129
of optimization-- knapsack
problems, several graph

577
00:33:08,130 --> 00:33:12,390
problems, curve fitting,
clustering, logistic

578
00:33:12,390 --> 00:33:14,070
regression.

579
00:33:14,069 --> 00:33:17,279
Those are all
optimization problems,

580
00:33:17,279 --> 00:33:20,980
can all be formulated as
optimization problems.

581
00:33:20,980 --> 00:33:26,559
So it's very powerful
and fits lots of needs.

582
00:33:26,559 --> 00:33:28,769
The next unit--
and, of course, I'm

583
00:33:28,769 --> 00:33:31,049
speaking as if these things
were discrete in time,

584
00:33:31,049 --> 00:33:32,539
but they're not.

585
00:33:32,539 --> 00:33:34,859
We talked about optimization
at the beginning.

586
00:33:34,859 --> 00:33:37,829
And I talk to an
optimization last week.

587
00:33:37,829 --> 00:33:41,250
So these things were sort
of spread out over the term.

588
00:33:41,250 --> 00:33:43,890
We talked about
stochastic thinking.

589
00:33:43,890 --> 00:33:48,600
And the basic notion here is
the world is nondeterministic,

590
00:33:48,599 --> 00:33:51,689
or at least predictably
nondeterministic.

591
00:33:51,690 --> 00:33:53,850
And therefore, we need
to think about things

592
00:33:53,849 --> 00:33:59,259
in terms of probabilities most
of the time, or frequently.

593
00:33:59,259 --> 00:34:03,430
And randomness is a powerful
tool for building computations

594
00:34:03,430 --> 00:34:05,230
that model the world.

595
00:34:05,230 --> 00:34:07,720
If you think the
world is stochastic,

596
00:34:07,720 --> 00:34:09,909
then you need to have
ways to write programs

597
00:34:09,909 --> 00:34:11,920
that are stochastic,
if you're trying

598
00:34:11,920 --> 00:34:15,429
to model the world itself.

599
00:34:15,429 --> 00:34:18,300
The other point we made is
that random computations--

600
00:34:18,300 --> 00:34:21,719
randomness is a
computational technique--

601
00:34:21,719 --> 00:34:24,239
is useful even for
problems that don't appear

602
00:34:24,239 --> 00:34:26,429
to involve any randomness.

603
00:34:26,429 --> 00:34:29,219
So we used it to
find the value of pi.

604
00:34:29,219 --> 00:34:32,309
We showed you can use
it to do integration.

605
00:34:32,309 --> 00:34:34,110
There's nothing
random about the value

606
00:34:34,110 --> 00:34:36,360
of the integral of a function.

607
00:34:36,360 --> 00:34:39,389
Yet, the easiest way to
solve it in a program

608
00:34:39,389 --> 00:34:42,030
is to use randomness.

609
00:34:42,030 --> 00:34:45,090
So randomness is a
very powerful tool.

610
00:34:45,090 --> 00:34:48,409
And there's this whole
area of random algorithms--

611
00:34:48,409 --> 00:34:50,849
research area and
practical area that's

612
00:34:50,849 --> 00:34:56,279
used to solve
non-probabilistic problems.

613
00:34:56,280 --> 00:35:01,269
Modeling the world-- well, we
just talked about part of it.

614
00:35:01,269 --> 00:35:02,849
Models are always inaccurate.

615
00:35:02,849 --> 00:35:07,389
They're providing some
abstraction of reality.

616
00:35:07,389 --> 00:35:11,204
We looked at
deterministic models--

617
00:35:11,204 --> 00:35:12,204
the graph theory models.

618
00:35:12,204 --> 00:35:14,679
There was nothing
nondeterministic

619
00:35:14,679 --> 00:35:18,129
about the graphs we looked at.

620
00:35:18,130 --> 00:35:21,460
And then we spent more
time on statistical models.

621
00:35:21,460 --> 00:35:23,199
We looked at simulation models.

622
00:35:23,199 --> 00:35:25,689
In particular, spent
quite a bit of time

623
00:35:25,690 --> 00:35:28,360
on the Monte Carlo simulation.

624
00:35:28,360 --> 00:35:30,460
We looked at models
based on sampling.

625
00:35:30,460 --> 00:35:33,940


626
00:35:33,940 --> 00:35:37,420
And there-- and also when
we talked about simulation--

627
00:35:37,420 --> 00:35:42,130
I really hope I emphasized
enough the notion

628
00:35:42,130 --> 00:35:45,220
that we need to be
able to characterize

629
00:35:45,219 --> 00:35:47,789
how believable the results are.

630
00:35:47,789 --> 00:35:50,619
It's not good enough
to just run a program

631
00:35:50,619 --> 00:35:53,210
and say, oh, it has an answer.

632
00:35:53,210 --> 00:35:56,780
You need to know whether
to believe the answer.

633
00:35:56,780 --> 00:36:01,280
And the point we made is
it's not a binary question.

634
00:36:01,280 --> 00:36:04,910
It's not yes, it's
right, no, it's wrong.

635
00:36:04,909 --> 00:36:10,789
Typically, what we do is we have
some statement about confidence

636
00:36:10,789 --> 00:36:14,360
intervals and confidence levels.

637
00:36:14,360 --> 00:36:16,610
We used two
variables to describe

638
00:36:16,610 --> 00:36:18,260
how believable the answer is.

639
00:36:18,260 --> 00:36:21,000


640
00:36:21,000 --> 00:36:22,389
And that's an important thing.

641
00:36:22,389 --> 00:36:25,375
And then we looked at tools
we use for doing that.

642
00:36:25,376 --> 00:36:27,000
We looked at the
central limit theorem.

643
00:36:27,000 --> 00:36:28,440
We looked at the empirical rule.

644
00:36:28,440 --> 00:36:31,230
We talked about
different distributions.

645
00:36:31,230 --> 00:36:33,150
And especially, we spent
a fair amount of time

646
00:36:33,150 --> 00:36:37,289
on the normal or
Gaussian distribution.

647
00:36:37,289 --> 00:36:39,750
And then finally, we looked
at statistical models

648
00:36:39,750 --> 00:36:42,599
based upon machine learning.

649
00:36:42,599 --> 00:36:45,839
We looked at
unsupervised learning,

650
00:36:45,840 --> 00:36:49,050
basically just clustering,
looked at two algorithms--

651
00:36:49,050 --> 00:36:51,360
hierarchical and k-means.

652
00:36:51,360 --> 00:36:53,970
And we looked at
supervised learning.

653
00:36:53,969 --> 00:36:57,239
And there, we essentially
focused mostly

654
00:36:57,239 --> 00:36:59,699
on classification.

655
00:36:59,699 --> 00:37:01,259
And we looked at
two ways of doing

656
00:37:01,260 --> 00:37:05,082
that-- k-nearest neighbors
and logistic regression.

657
00:37:05,081 --> 00:37:08,529


658
00:37:08,530 --> 00:37:14,080
Finally, we talked about
presentation of data--

659
00:37:14,079 --> 00:37:17,619
how to build plots,
utility of plots,

660
00:37:17,619 --> 00:37:21,009
and recently, over the last
two lectures, good and bad

661
00:37:21,010 --> 00:37:24,330
practices in presenting
results about data.

662
00:37:24,329 --> 00:37:26,940


663
00:37:26,940 --> 00:37:29,490
So my summary is,
I hope that you

664
00:37:29,489 --> 00:37:32,399
think you've come a long way,
particularly those of you--

665
00:37:32,400 --> 00:37:34,349
how many of you were
here in September when

666
00:37:34,349 --> 00:37:36,909
we started 6.0001?

667
00:37:36,909 --> 00:37:39,980
All right, most of you.

668
00:37:39,980 --> 00:37:42,389
Yeah, this, by the way,
was a very popular ad

669
00:37:42,389 --> 00:37:45,779
for a long time, saying that,
finally women are allowed

670
00:37:45,780 --> 00:37:48,570
to smoke, isn't this great.

671
00:37:48,570 --> 00:37:51,480
And Virginia Slims sponsored
tennis-- the women's tennis

672
00:37:51,480 --> 00:37:55,260
tour to show how good it
was that women were now

673
00:37:55,260 --> 00:37:56,880
able to smoke.

674
00:37:56,880 --> 00:38:01,380
But anyway, I know not everyone
in this class is a woman.

675
00:38:01,380 --> 00:38:05,340
So just for the men
in the room, you too

676
00:38:05,340 --> 00:38:06,539
could have come a long way.

677
00:38:06,539 --> 00:38:09,829


678
00:38:09,829 --> 00:38:12,049
I hope you think
that, if you look back

679
00:38:12,050 --> 00:38:15,230
at how you struggled in
those early problems sets,

680
00:38:15,230 --> 00:38:17,960
I hope you really feel that
you've learned a lot about how

681
00:38:17,960 --> 00:38:20,269
to build programs.

682
00:38:20,269 --> 00:38:22,873
And if you spend enough
time in front of a terminal,

683
00:38:22,873 --> 00:38:24,289
this is what you
get to look like.

684
00:38:24,289 --> 00:38:27,199


685
00:38:27,199 --> 00:38:29,779
What might be next?

686
00:38:29,780 --> 00:38:32,630
I should start by saying,
this is a hard course.

687
00:38:32,630 --> 00:38:34,849
We know that many
of you worked hard.

688
00:38:34,849 --> 00:38:38,989
And the staff and I
really do appreciate it.

689
00:38:38,989 --> 00:38:42,529
You know your return
on investment.

690
00:38:42,530 --> 00:38:45,560
I'd like you to remember
that you can now write

691
00:38:45,559 --> 00:38:48,409
programs to do useful things.

692
00:38:48,409 --> 00:38:51,859
So if you're doing a UROP,
you're sitting in a lab,

693
00:38:51,860 --> 00:38:55,490
and you get a bunch of data from
some experiments, don't just

694
00:38:55,489 --> 00:38:56,209
stare at it.

695
00:38:56,210 --> 00:38:58,730
Sit down and write
some code to plot it

696
00:38:58,730 --> 00:39:00,990
to do something useful with it.

697
00:39:00,989 --> 00:39:06,343
Don't be afraid to write
programs to help you out.

698
00:39:06,344 --> 00:39:08,260
There are some courses
that I think you're now

699
00:39:08,260 --> 00:39:10,840
well-prepared to take.

700
00:39:10,840 --> 00:39:16,490
I've listed the ones I know
best-- the courses in course 6.

701
00:39:16,489 --> 00:39:20,769
6.009 is a sort of introduction
to computer science.

702
00:39:20,769 --> 00:39:22,690
I think many of you
will find that too

703
00:39:22,690 --> 00:39:25,210
easy after taking this course.

704
00:39:25,210 --> 00:39:28,900
But maybe, that's
not a downside.

705
00:39:28,900 --> 00:39:33,619
6.005 is a software
engineering course,

706
00:39:33,619 --> 00:39:36,440
where they'll switch
programming languages on you.

707
00:39:36,440 --> 00:39:39,750
You get to program in Java.

708
00:39:39,750 --> 00:39:45,219
6.006 is a algorithms
course in Python

709
00:39:45,219 --> 00:39:47,589
and I think actually
quite interesting.

710
00:39:47,590 --> 00:39:49,480
And students seem
to like it a lot,

711
00:39:49,480 --> 00:39:53,079
and they learn about algorithms
and implementing them.

712
00:39:53,079 --> 00:39:58,269
And 6.034 is an introduction
to artificial intelligence

713
00:39:58,269 --> 00:39:59,380
also in Python.

714
00:39:59,380 --> 00:40:03,880
And I should have listed
6.036, another introduction

715
00:40:03,880 --> 00:40:05,690
to machine learning in Python.

716
00:40:05,690 --> 00:40:10,500


717
00:40:10,500 --> 00:40:14,130
You should go look for
an interesting UROP.

718
00:40:14,130 --> 00:40:16,050
A lot of students come
out of this course

719
00:40:16,050 --> 00:40:18,269
and go do UROPs,
where they use what

720
00:40:18,269 --> 00:40:20,099
they've learned in this course.

721
00:40:20,099 --> 00:40:24,099
And many of them really have
a very positive experience.

722
00:40:24,099 --> 00:40:27,569
So if you were worried that
you're not ready for a UROP,

723
00:40:27,570 --> 00:40:29,220
you probably are--

724
00:40:29,219 --> 00:40:31,889
a UROP using what's
been done here.

725
00:40:31,889 --> 00:40:33,730
You can minor in
computer science.

726
00:40:33,730 --> 00:40:36,840
This is now available for
the first time this year.

727
00:40:36,840 --> 00:40:38,730
But really, if
you have time, you

728
00:40:38,730 --> 00:40:40,639
should major in
computer science,

729
00:40:40,639 --> 00:40:44,159
because it is really the
best major on campus--

730
00:40:44,159 --> 00:40:46,289
not even close, as
somebody I know would say.

731
00:40:46,289 --> 00:40:49,070


732
00:40:49,070 --> 00:40:51,120
Finally, sometimes
people ask me where

733
00:40:51,119 --> 00:40:53,039
I think computing is headed.

734
00:40:53,039 --> 00:40:56,070
And I'll quote one of my
favorite baseball players.

735
00:40:56,070 --> 00:41:00,660
"It's tough to make predictions,
especially about the future."

736
00:41:00,659 --> 00:41:02,519
And instead of my
predictions, let

737
00:41:02,519 --> 00:41:05,759
me show you the predictions
of some famous people.

738
00:41:05,760 --> 00:41:09,270
So Thomas Watson, who
was the chairman of IBM--

739
00:41:09,269 --> 00:41:12,119
a company you've
probably heard of--

740
00:41:12,119 --> 00:41:14,969
and he said, "I think there is
a world market for maybe five

741
00:41:14,969 --> 00:41:17,159
computers."

742
00:41:17,159 --> 00:41:18,659
This was in response
to, should they

743
00:41:18,659 --> 00:41:22,529
become a computer company,
which they were not at the time.

744
00:41:22,530 --> 00:41:24,330
He was off by a little bit.

745
00:41:24,329 --> 00:41:27,420


746
00:41:27,420 --> 00:41:29,070
A few years later,
there was an article

747
00:41:29,070 --> 00:41:33,880
in Popular Mechanics, which was
saying, computers are amazing.

748
00:41:33,880 --> 00:41:35,700
They're going to
change enormously.

749
00:41:35,699 --> 00:41:39,509
Someday, they may be no
more than 1 and 1/2 tons.

750
00:41:39,510 --> 00:41:43,230
You might get a computer that's
no more than 3,000 pounds--

751
00:41:43,230 --> 00:41:44,519
someday.

752
00:41:44,519 --> 00:41:46,769
So we're still waiting
for that, I guess.

753
00:41:46,769 --> 00:41:51,250


754
00:41:51,250 --> 00:41:52,099
I like this one.

755
00:41:52,099 --> 00:41:54,579
This is, having written
a book recently,

756
00:41:54,579 --> 00:41:57,420
the editor in charge of
books for Prentice Hall.

757
00:41:57,420 --> 00:42:00,206
"I traveled the length and
breadth of this country

758
00:42:00,206 --> 00:42:01,539
and talked with the best people.

759
00:42:01,539 --> 00:42:05,050
And I can assure you that
data processing is a fad that

760
00:42:05,050 --> 00:42:09,060
won't last out the year."

761
00:42:09,059 --> 00:42:11,099
MIT had that
attitude for a while.

762
00:42:11,099 --> 00:42:15,029
For about 35 years, computer
science was in a building

763
00:42:15,030 --> 00:42:20,080
off campus, because they weren't
sure we were here to stay.

764
00:42:20,079 --> 00:42:24,190
Maybe that's not why, but
that's why I interpret it.

765
00:42:24,190 --> 00:42:26,950
Ken Olsen, an MIT graduate--

766
00:42:26,949 --> 00:42:30,279
I should say, a
course 6 graduate--

767
00:42:30,280 --> 00:42:33,700
was the founder and
president and chair

768
00:42:33,699 --> 00:42:37,869
of Digital Equipment
Corporation, which in 1977 was

769
00:42:37,869 --> 00:42:42,250
the second largest computer
manufacturer in the world based

770
00:42:42,250 --> 00:42:44,914
in Maynard, Massachusetts.

771
00:42:44,914 --> 00:42:46,330
None of you have
ever heard of it.

772
00:42:46,329 --> 00:42:47,389
They disappeared.

773
00:42:47,389 --> 00:42:50,139
And this is in part
why, because Ken said,

774
00:42:50,139 --> 00:42:54,369
"there's no reason anyone would
want a computer in their home,"

775
00:42:54,369 --> 00:43:00,759
and totally missed that
part of computation.

776
00:43:00,760 --> 00:43:05,890
Finally, since this is the
end of some famous last words,

777
00:43:05,889 --> 00:43:09,279
Douglas Fairbanks,
Sr., a famous actor--

778
00:43:09,280 --> 00:43:11,710
this is true-- the last
thing he said before he died

779
00:43:11,710 --> 00:43:13,090
was, "never felt better."

780
00:43:13,090 --> 00:43:16,170


781
00:43:16,170 --> 00:43:18,320
Amazing.

782
00:43:18,320 --> 00:43:22,400
This was from the movie
The Mark of Zorro.

783
00:43:22,400 --> 00:43:23,389
Scientists are better.

784
00:43:23,389 --> 00:43:28,029
Luther Burbank, his last words
were, I don't feel so good.

785
00:43:28,030 --> 00:43:30,890
And well, I guess not.

786
00:43:30,889 --> 00:43:34,819
[LAUGHTER]

787
00:43:34,820 --> 00:43:37,470
And this is the last one.

788
00:43:37,469 --> 00:43:41,639
John Sedgwick was a Union
general in the Civil War.

789
00:43:41,639 --> 00:43:42,679
This is a true story.

790
00:43:42,679 --> 00:43:46,219
He was riding behind
the lines and trying

791
00:43:46,219 --> 00:43:53,250
to rally his men
to not hide behind

792
00:43:53,250 --> 00:43:57,719
the stone walls but to stand
up and shoot at the enemy.

793
00:43:57,719 --> 00:44:02,039
And he said, "they couldn't hit
an elephant at this distance."

794
00:44:02,039 --> 00:44:06,029
Moments later, he was
shot in the face and died.

795
00:44:06,030 --> 00:44:07,980
[LAUGHTER]

796
00:44:07,980 --> 00:44:10,559
And I thought this was
an apocryphal story.

797
00:44:10,559 --> 00:44:13,170
But in fact, there's a
plaque at the battlefield

798
00:44:13,170 --> 00:44:15,960
where this happened,
documenting this story.

799
00:44:15,960 --> 00:44:17,789
And apparently, it's quite true.

800
00:44:17,789 --> 00:44:21,119


801
00:44:21,119 --> 00:44:22,980
So with that, I'll
say my last words

802
00:44:22,980 --> 00:44:26,519
for the chorus, which is I
appreciate all your coming.

803
00:44:26,519 --> 00:44:29,500
And I guess you
were the survivors.

804
00:44:29,500 --> 00:44:31,980
So thank you for being here.

805
00:44:31,980 --> 00:44:36,230
[APPLAUSE]

806
00:44:36,230 --> 00:44:42,878


