WEBVTT

1
00:00:00.005 --> 00:00:06.278
Step 2-B: Pre-processing Data

2
00:00:20.283 --> 00:00:24.904
The raw data that you get directly from
your sources are never in the format that

3
00:00:24.904 --> 00:00:26.875
you need to perform analysis on.

4
00:00:27.995 --> 00:00:31.885
There are two main goals in
the data pre-processing step.

5
00:00:31.885 --> 00:00:36.455
The first is to clean the data to
address data quality issues, and

6
00:00:36.455 --> 00:00:41.320
the second is to transform the raw
data to make it suitable for analysis.

7
00:00:43.850 --> 00:00:47.050
A very important part of data preparation

8
00:00:47.050 --> 00:00:49.606
is to address quality
of issues in your data.

9
00:00:49.606 --> 00:00:54.000
Real-world data is messy.

10
00:00:54.000 --> 00:01:00.160
There are many examples of quality issues
with data from real applications including

11
00:01:00.160 --> 00:01:05.990
inconsistent data like a customer with two
different addresses, duplicate customer

12
00:01:05.990 --> 00:01:11.460
records, for example, customers address
recorded at two different sales locations.

13
00:01:12.490 --> 00:01:14.600
And the two recordings don't agree.

14
00:01:16.190 --> 00:01:19.160
Missing customer agent demographics or
studies.

15
00:01:21.010 --> 00:01:25.750
Missing values like missing a customer
age in the demographic studies.

16
00:01:27.000 --> 00:01:32.160
invalid data like an invalid zip code for
example, a six digit code.

17
00:01:33.350 --> 00:01:39.060
And outliers like a sense of failure
causing values to be much higher or

18
00:01:39.060 --> 00:01:41.480
lower than expected for a period of time.

19
00:01:43.340 --> 00:01:45.860
Since we get the data downstream

20
00:01:45.860 --> 00:01:48.960
we usually have little control
over how the data is collected.

21
00:01:50.390 --> 00:01:55.120
Preventing data quality problems as
the data is being collected is not

22
00:01:55.120 --> 00:01:56.070
often an option.

23
00:01:57.740 --> 00:01:59.950
So we have the data that we get and

24
00:01:59.950 --> 00:02:03.770
we have to address quality issues
by detecting and correcting them.

25
00:02:05.710 --> 00:02:09.170
Here are some approaches we can take
to address this quality issues.

26
00:02:11.780 --> 00:02:14.630
We can remove data records
with missing values.

27
00:02:16.330 --> 00:02:19.290
We can merge duplicate records.

28
00:02:19.290 --> 00:02:23.430
This will require a way to determine
how to resolve conflicting values.

29
00:02:24.780 --> 00:02:29.320
Perhaps it makes sense to retain the newer
value whenever there's a conflict.

30
00:02:31.030 --> 00:02:34.500
For invalid values, the best estimate for

31
00:02:34.500 --> 00:02:38.330
a reasonable value can be
used as a replacement.

32
00:02:38.330 --> 00:02:42.800
For example, for
a missing age value for an employee,

33
00:02:42.800 --> 00:02:47.320
a reasonable value can be estimated based
on the employee's length of employment.

34
00:02:49.820 --> 00:02:54.340
Outliers can also be removed if
they are not important to the task.

35
00:02:56.590 --> 00:03:00.370
In order to address data
quality issues effectively,

36
00:03:00.370 --> 00:03:04.590
knowledge about the application,
such as how the data was collected,

37
00:03:04.590 --> 00:03:09.880
the user population, and the intended
uses of the application is important.

38
00:03:11.680 --> 00:03:15.280
This domain knowledge is
essential to making informed

39
00:03:15.280 --> 00:03:19.070
decisions on how to handle incomplete or
incorrect data.

40
00:03:22.740 --> 00:03:25.480
The second part of preparing data

41
00:03:25.480 --> 00:03:29.299
is to manipulate the clean data into
the format needed for analysis.

42
00:03:30.500 --> 00:03:32.910
The step is known by many names.

43
00:03:34.940 --> 00:03:39.894
Data manipulation,
data preprocessing, data wrangling,

44
00:03:39.894 --> 00:03:43.419
and even data munging, some operations for

45
00:03:43.419 --> 00:03:49.420
this type of operation I mean data
munging, wrangling, preprocessing,

46
00:03:49.420 --> 00:03:54.373
include, scaling, transformation,
feature selection,

47
00:03:54.373 --> 00:03:58.778
dimensionality reduction,
and data manipulation.

48
00:04:01.628 --> 00:04:08.840
Scaling involves changing the range of
values to be between a specified range.

49
00:04:08.840 --> 00:04:10.970
Such as from zero to one.

50
00:04:12.220 --> 00:04:16.460
This is done to avoid having certain
features that large values from

51
00:04:16.460 --> 00:04:18.570
dominating the results.

52
00:04:18.570 --> 00:04:22.990
For example,
in analyzing data with height and weight.

53
00:04:22.990 --> 00:04:27.350
To magnitude of weight values is much
greater than of the height values.

54
00:04:29.220 --> 00:04:32.880
So scaling all values
to be between zero and

55
00:04:32.880 --> 00:04:37.670
one will equalize contributions from
both height and weight features.

56
00:04:40.900 --> 00:04:46.000
Various transformations can be performed
on the data to reduce noise and

57
00:04:46.000 --> 00:04:46.720
variability.

58
00:04:48.380 --> 00:04:50.970
One such transformation is aggregation.

59
00:04:52.430 --> 00:04:57.260
Aggregate data generally results
in data with less variability,

60
00:04:57.260 --> 00:04:58.890
which may help with your analysis.

61
00:05:00.190 --> 00:05:05.720
For example, daily sales figures
may have many serious changes.

62
00:05:06.730 --> 00:05:11.880
Aggregating values to weekly or monthly
sales figures will result in similar data.

63
00:05:14.720 --> 00:05:19.340
Other filtering techniques can also be
used to remove variability in the data.

64
00:05:19.340 --> 00:05:23.530
Of course, this comes at
the cost of less detailed data.

65
00:05:23.530 --> 00:05:27.760
So these factors must be weighed for
the specific application.

66
00:05:30.570 --> 00:05:36.400
Future selection can involve removing
redundant or irrelevant features,

67
00:05:36.400 --> 00:05:40.300
combining features, and
creating new features.

68
00:05:41.460 --> 00:05:44.070
During the exploring data step,

69
00:05:44.070 --> 00:05:47.860
you might have discovered that
two features are correlated.

70
00:05:49.030 --> 00:05:52.350
In that case one of these
features can be removed

71
00:05:52.350 --> 00:05:55.090
without negatively affecting
the analysis results.

72
00:05:56.170 --> 00:05:59.870
For example,
the purchase price of a product and

73
00:05:59.870 --> 00:06:03.360
the amount of sales tax paid,
are likely to be correlated.

74
00:06:04.800 --> 00:06:08.550
Eliminating the sales tax amount,
then will be beneficial.

75
00:06:10.860 --> 00:06:12.810
Removing redundant or

76
00:06:12.810 --> 00:06:17.010
irrelevant features will make
the subsequent analysis much simpler.

77
00:06:19.320 --> 00:06:25.220
In other cases, you may want to combine
features or create new ones.

78
00:06:25.220 --> 00:06:29.540
For example,
adding the applicant's education level

79
00:06:29.540 --> 00:06:32.820
as a feature to a loan approval
application would make sense.

80
00:06:34.940 --> 00:06:39.790
There are also algorithms to automatically
determine the most relevant features,

81
00:06:39.790 --> 00:06:42.130
based on various mathematical properties.

82
00:06:45.440 --> 00:06:50.200
Dimensionality reduction is useful when
the data set has a large number of

83
00:06:50.200 --> 00:06:50.750
dimensions.

84
00:06:52.020 --> 00:06:55.830
It involves finding a smaller
subset of dimensions that

85
00:06:55.830 --> 00:06:58.350
captures most of
the variation in the data.

86
00:07:00.030 --> 00:07:03.600
This reduces the dimensions of the data

87
00:07:03.600 --> 00:07:08.400
while eliminating irrelevant features and
makes analysis simpler.

88
00:07:10.130 --> 00:07:12.089
A technique commonly used for

89
00:07:12.089 --> 00:07:16.878
dimensional reduction is called
principle component analysis or PCA.

90
00:07:20.528 --> 00:07:26.120
Raw data often has to be manipulated to
be in the correct format for analysis.

91
00:07:27.140 --> 00:07:32.890
For example, from samples recording
daily changes in stock prices,

92
00:07:32.890 --> 00:07:35.520
we may want the capture price changes for

93
00:07:35.520 --> 00:07:38.990
a particular market segments
like real estate or health care.

94
00:07:40.230 --> 00:07:45.150
This would require determining which
stocks belong to which market segment.

95
00:07:45.150 --> 00:07:49.540
Grouping them together, and
perhaps computing the mean, range,

96
00:07:49.540 --> 00:07:51.530
standard deviation for each group.

97
00:07:54.120 --> 00:07:54.760
In summary,

98
00:07:55.910 --> 00:08:00.470
data preparation is a very important
part of the data science process.

99
00:08:00.470 --> 00:08:05.530
In fact, this is where you will spend most
of your time on any data science effort.

100
00:08:06.710 --> 00:08:11.680
It can be a tedious process,
but it is a crucial step.

101
00:08:11.680 --> 00:08:15.200
Always remember, garbage in, garbage out.

102
00:08:15.200 --> 00:08:16.987
If you don't spend the time and

103
00:08:16.987 --> 00:08:21.250
effort to create good data for the
analysis, you will not get good results

104
00:08:21.250 --> 00:08:25.399
no matter how sophisticated
the analysis technique you're using is.