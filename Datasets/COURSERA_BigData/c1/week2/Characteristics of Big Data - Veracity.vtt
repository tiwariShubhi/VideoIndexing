WEBVTT

1
00:00:01.640 --> 00:00:04.377
Characteristics of Big Data, Veracity.

2
00:00:20.366 --> 00:00:23.980
Veracity of Big Data refers
to the quality of the data.

3
00:00:25.010 --> 00:00:29.760
It sometimes gets referred
to as validity or

4
00:00:29.760 --> 00:00:33.250
volatility referring to
the lifetime of the data.

5
00:00:34.840 --> 00:00:38.360
Veracity is very important for
making big data operational.

6
00:00:39.810 --> 00:00:43.900
Because big data can be noisy and
uncertain.

7
00:00:43.900 --> 00:00:50.885
It can be full of biases,
abnormalities and it can be imprecise.

8
00:00:50.885 --> 00:00:54.741
Data is of no value if it's not accurate,

9
00:00:54.741 --> 00:00:59.900
the results of big data analysis are only
as good as the data being analyzed.

10
00:01:01.480 --> 00:01:06.840
This is often described in analytics
as junk in equals junk out.

11
00:01:08.040 --> 00:01:12.760
So we can say although big data
provides many opportunities to make

12
00:01:12.760 --> 00:01:18.100
data enabled decisions,
the evidence provided by data

13
00:01:18.100 --> 00:01:22.710
is only valuable if the data
is of a satisfactory quality.

14
00:01:23.880 --> 00:01:26.870
There are many different
ways to define data quality.

15
00:01:27.890 --> 00:01:30.200
In the context of big data,

16
00:01:30.200 --> 00:01:34.040
quality can be defined as a function
of a couple of different variables.

17
00:01:35.230 --> 00:01:41.310
Accuracy of the data, the trustworthiness
or reliability of the data source.

18
00:01:41.310 --> 00:01:44.940
And how the data was generated
are all important factors

19
00:01:44.940 --> 00:01:46.720
that affect the quality of data.

20
00:01:48.100 --> 00:01:53.910
Additionally how meaningful the data
is with respect to the program that

21
00:01:53.910 --> 00:02:00.360
analyzes it, is an important factor, and
makes context a part of the quality.

22
00:02:02.040 --> 00:02:07.850
In this chart from 2015,
we see the volumes of data increasing,

23
00:02:07.850 --> 00:02:12.380
starting with small amounts
of enterprise data to larger,

24
00:02:12.380 --> 00:02:17.480
people generated voice over IP and
social media data and

25
00:02:17.480 --> 00:02:21.010
even larger machine generated sensor data.

26
00:02:21.010 --> 00:02:26.040
We also see that the uncertainty of
the data increases as we go from

27
00:02:26.040 --> 00:02:28.840
enterprise data to sensor data.

28
00:02:28.840 --> 00:02:31.320
This is as we would expect it to be.

29
00:02:31.320 --> 00:02:35.980
Traditional enterprise
data in warehouses have

30
00:02:35.980 --> 00:02:41.080
standardized quality solutions
like master processes for extract,

31
00:02:41.080 --> 00:02:46.310
transform and load of the data which
we referred to as before as ETL.

32
00:02:46.310 --> 00:02:51.080
As enterprises started incorporating less
structured and unstructured people and

33
00:02:51.080 --> 00:02:54.870
machine data into their
big data solutions,

34
00:02:54.870 --> 00:02:58.670
the data become messier and
more uncertain.

35
00:02:58.670 --> 00:03:00.210
There are many reasons for this.

36
00:03:01.220 --> 00:03:07.620
First, unstructured data on the internet
is imprecise and uncertain.

37
00:03:07.620 --> 00:03:13.730
In addition, high velocity big data
leaves very little or no time for

38
00:03:13.730 --> 00:03:19.810
ETL, and in turn hindering the quality
assurance processes of the data.

39
00:03:19.810 --> 00:03:24.090
Let's look at these product reviews for
a banana slicer on amazon.com.

40
00:03:24.090 --> 00:03:28.960
One of the five star reviews say that

41
00:03:28.960 --> 00:03:34.190
it saved her marriage and compared it
to the greatest inventions in history.

42
00:03:34.190 --> 00:03:38.900
Another five star reviewer said
that his parole officer recommended

43
00:03:38.900 --> 00:03:42.470
the slicer as he is not
allowed to be around knives.

44
00:03:42.470 --> 00:03:44.400
These are obviously fake reviewers.

45
00:03:45.410 --> 00:03:49.070
Now think of an automated product
assessment going through such

46
00:03:49.070 --> 00:03:53.930
splendid reviews and estimating lots
of sales for the banana slicer and

47
00:03:53.930 --> 00:03:58.390
in turn suggesting stocking more
of the slicer in the inventory.

48
00:03:58.390 --> 00:03:59.670
Amazon will have problems.

49
00:04:00.760 --> 00:04:06.420
For a more serious case let's look at
the Google flu trends case from 2013.

50
00:04:06.420 --> 00:04:11.788
For January 2013,
the Google Friends actually

51
00:04:11.788 --> 00:04:17.280
estimated almost twice as many
flu cases as was reported by CDC,

52
00:04:17.280 --> 00:04:20.870
the Centers for
Disease Control and Prevention.

53
00:04:22.390 --> 00:04:26.960
The primary reason behind this was that
Google Flu Trends used a big data on

54
00:04:26.960 --> 00:04:32.070
the internet and did not account properly
for uncertainties about the data.

55
00:04:33.210 --> 00:04:37.780
Maybe the news and social media
attention paid to the particularly

56
00:04:37.780 --> 00:04:42.450
serious level of flu that
year effected the estimate.

57
00:04:42.450 --> 00:04:46.840
And resulted in what we
call an over estimation.

58
00:04:46.840 --> 00:04:49.050
This is a perfect example for

59
00:04:49.050 --> 00:04:55.470
how inaccurate the results can be if
only big data is used in the analysis.

60
00:04:55.470 --> 00:04:59.820
Imagine the economical impact of
making health care preparations for

61
00:04:59.820 --> 00:05:01.920
twice the amount of flu cases.

62
00:05:01.920 --> 00:05:03.610
That would be huge.

63
00:05:03.610 --> 00:05:07.850
The Google flu trends example
also brings up the need for

64
00:05:07.850 --> 00:05:13.350
being able to identify where exactly
the big data they used comes from.

65
00:05:13.350 --> 00:05:16.300
What transformation did
big data go through up

66
00:05:16.300 --> 00:05:19.420
until the moment it was used for
a estimate?

67
00:05:19.420 --> 00:05:23.460
This is what we refer
to as data providence.

68
00:05:23.460 --> 00:05:27.630
Just like we refer to
an artifacts provenance.

69
00:05:27.630 --> 00:05:33.180
As a summary, the growing
torrents of big data pushes for

70
00:05:33.180 --> 00:05:36.590
fast solutions to utilize
it in analytical solutions.

71
00:05:37.630 --> 00:05:42.000
This creates challenges on
keeping track of data quality.

72
00:05:42.000 --> 00:05:46.510
What has been collected,
where it came from, and

73
00:05:46.510 --> 00:05:48.940
how it was analyzed prior to its use.

74
00:05:50.150 --> 00:05:52.910
This is akin to an art artifact

75
00:05:52.910 --> 00:05:55.490
having providence of everything
it has gone through.

76
00:05:56.540 --> 00:06:01.560
But even more complicated to achieve
with large volumes of data coming

77
00:06:01.560 --> 00:06:04.130
in varieties and velocities.