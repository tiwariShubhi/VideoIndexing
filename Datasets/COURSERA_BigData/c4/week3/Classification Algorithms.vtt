WEBVTT

1
00:00:01.070 --> 00:00:04.650
In this video, we will outline
some commonly used algorithms for

2
00:00:04.650 --> 00:00:07.260
building a classification model.

3
00:00:07.260 --> 00:00:11.540
After this video,
you will be able to describe the goal of

4
00:00:11.540 --> 00:00:16.260
a classification algorithm and name some
common algorithms for classification.

5
00:00:17.630 --> 00:00:22.140
Recall that a classification task is
to predict the category from the input

6
00:00:22.140 --> 00:00:23.580
variables.

7
00:00:23.580 --> 00:00:29.570
A classification model processes the input
data it receives and provides an output.

8
00:00:29.570 --> 00:00:33.860
Since classification is a supervised task,
a target or

9
00:00:33.860 --> 00:00:37.710
desired output is provided for
each sample.

10
00:00:37.710 --> 00:00:42.640
The goal is to get the model outputs to
match the targets as much as possible.

11
00:00:44.040 --> 00:00:47.350
A classification model
adjusts its parameters

12
00:00:47.350 --> 00:00:50.610
to get its outputs to match the targets.

13
00:00:50.610 --> 00:00:54.500
To adjust a model's parameters,
a learning algorithm is applied.

14
00:00:55.580 --> 00:00:59.080
This occurs in a training phase
when the model is constructed.

15
00:01:00.570 --> 00:01:03.950
There are many algorithms to
build a classification model.

16
00:01:03.950 --> 00:01:08.514
In this course,
we will cover the algorithms listed here,

17
00:01:08.514 --> 00:01:13.371
kNN or k Nearest Neighbors,
decision tree, and naive Bayes.

18
00:01:13.371 --> 00:01:17.870
kNN stands for k Nearest Neighbors.

19
00:01:17.870 --> 00:01:22.490
This technique relies on the notion that
samples with similar characteristics,

20
00:01:22.490 --> 00:01:28.440
that is samples with similar values for
input, likely belong to the same class.

21
00:01:28.440 --> 00:01:31.150
So classification of a sample is dependent

22
00:01:31.150 --> 00:01:33.650
on the target values of
the neighboring points.

23
00:01:35.410 --> 00:01:39.560
Another classification technique
is referred to as decision tree.

24
00:01:39.560 --> 00:01:44.270
A decision tree is a classification
model that uses a treelike structure

25
00:01:44.270 --> 00:01:47.398
to represent multiple decision paths.

26
00:01:47.398 --> 00:01:52.010
Traversing each path leads to a different
way to classify an input sample.

27
00:01:53.450 --> 00:01:57.770
A naive Bayes model uses a probabilistic
approach to classification.

28
00:01:58.870 --> 00:02:02.920
Baye's Theorem is used to capture the
relationship between the input data and

29
00:02:02.920 --> 00:02:03.830
the output class.

30
00:02:04.880 --> 00:02:09.130
Simply put, the Baye's Theorem
compares the probability of an event

31
00:02:09.130 --> 00:02:11.660
in the presence of another event.

32
00:02:11.660 --> 00:02:16.150
We see here the probability
of A if B is present.

33
00:02:16.150 --> 00:02:20.770
For example, probability of having
a fire if the weather is hot.

34
00:02:20.770 --> 00:02:24.100
You can imagine event B depending
on more than one variable.

35
00:02:24.100 --> 00:02:26.416
For example, weather is hot and windy.

36
00:02:26.416 --> 00:02:34.060
We will cover kNN, decision tree and naive
Bayes in detail in the next few lectures.

37
00:02:34.060 --> 00:02:38.150
There are many other classification
techniques, but we will focus on these

38
00:02:38.150 --> 00:02:41.970
since they are fundamental algorithms
that are commonly used and

39
00:02:41.970 --> 00:02:44.940
form the basis of other algorithms for
classification.