WEBVTT

1
00:00:00.930 --> 00:00:04.150
Now that we have seen what
association analysis is,

2
00:00:04.150 --> 00:00:07.460
let's go over the association
analysis process in more detail.

3
00:00:08.930 --> 00:00:13.970
After this video, you will be able
to define the terms support and

4
00:00:13.970 --> 00:00:18.790
confidence, describe the steps
in association analysis, and

5
00:00:18.790 --> 00:00:22.330
explain how association rules
are formed from item sets.

6
00:00:23.540 --> 00:00:26.240
Let's review the set in
association analysis.

7
00:00:26.240 --> 00:00:31.920
They are create item the sets,
then identify the frequent item sets.

8
00:00:31.920 --> 00:00:33.230
Finally, generate the rules.

9
00:00:34.240 --> 00:00:37.010
We will continue with
this example dataset,

10
00:00:37.010 --> 00:00:39.530
there are five transactions
in the dataset.

11
00:00:39.530 --> 00:00:42.650
Each with a set of items
purchased together.

12
00:00:42.650 --> 00:00:46.770
The goal is to come up with rules
describing associations between items.

13
00:00:48.280 --> 00:00:50.390
The first step is to create item sets.

14
00:00:51.450 --> 00:00:54.780
Item sets have different sizes
which need to be created.

15
00:00:54.780 --> 00:00:56.220
We will color code the items so

16
00:00:56.220 --> 00:00:59.350
that each one is easier to pick
out from the transactions table.

17
00:01:00.880 --> 00:01:05.250
We start out with just 1-item sets,
that is, sets with just one item.

18
00:01:06.260 --> 00:01:09.530
The left table is
the dataset of transactions.

19
00:01:09.530 --> 00:01:13.920
The right table contains the 1-item sets
that can be created from this dataset.

20
00:01:15.170 --> 00:01:19.940
As each item set is created, we also need
to keep track of the frequency at which

21
00:01:19.940 --> 00:01:21.910
these item set occurs in the dataset.

22
00:01:23.275 --> 00:01:27.130
This is referred to support for
the item set and

23
00:01:27.130 --> 00:01:32.580
is calculated by dividing the number of
times the item set occurs in the dataset

24
00:01:32.580 --> 00:01:35.250
by the total number of transactions.

25
00:01:35.250 --> 00:01:39.050
This is what is in the Support
column in the right table.

26
00:01:39.050 --> 00:01:42.586
For example, eggs the last
item in the right table occurs

27
00:01:42.586 --> 00:01:46.502
just occurs just once in the dataset,
in the transaction two.

28
00:01:46.502 --> 00:01:51.838
So if Support is 1/5 or
one fifth,the item set with

29
00:01:51.838 --> 00:01:59.330
diaper occurs in all transactions,
so if Support is 5/5 or 1.

30
00:01:59.330 --> 00:02:03.410
The Support for each item set will be
used to identify frequent item sets

31
00:02:03.410 --> 00:02:07.930
in the next step, specifically,
the Support issues to prune, or

32
00:02:07.930 --> 00:02:10.990
remove, item sets that
do not occur frequently.

33
00:02:12.605 --> 00:02:15.825
The support of each item set
will be used to identify

34
00:02:15.825 --> 00:02:18.155
frequent item sets in the next step.

35
00:02:18.155 --> 00:02:20.905
Specifically, the support
is used to prune or

36
00:02:20.905 --> 00:02:24.705
remove item sets that do
not occur frequently.

37
00:02:24.705 --> 00:02:28.525
For example, the minimum support
threshold is set to 3/5.

38
00:02:28.525 --> 00:02:33.271
So looking at the 1-item sets table
We can remove any item set with

39
00:02:33.271 --> 00:02:35.656
the support of less than 3/5.

40
00:02:35.656 --> 00:02:40.348
These item sets are highlighted in pink,
they will be removed before the sets for

41
00:02:40.348 --> 00:02:42.360
two items are created.

42
00:02:42.360 --> 00:02:48.300
The final one item sets are then the item
sets with bread, milk, beer and diaper.

43
00:02:49.770 --> 00:02:53.640
We only consider items that were in
the one item sets, that were not pruned.

44
00:02:54.740 --> 00:02:56.990
The two item sets are shown
in the right table.

45
00:02:58.000 --> 00:03:00.370
We, again,
need to keep track of the support for

46
00:03:00.370 --> 00:03:03.100
these item sets,
just as we did with the one item sets.

47
00:03:04.180 --> 00:03:06.730
For example, for
the last item set, with beer, and

48
00:03:06.730 --> 00:03:10.280
diaper, we see, by looking at
the left table, that beer and

49
00:03:10.280 --> 00:03:16.000
diaper occur together three times In
transactions two, three and four.

50
00:03:16.000 --> 00:03:18.700
So with support is 3/5.

51
00:03:18.700 --> 00:03:21.870
Again, we need to prune
item sets with low support.

52
00:03:21.870 --> 00:03:25.450
The ones highlighted in pink
in the two item sets table.

53
00:03:25.450 --> 00:03:29.800
Those would be the item set with bread and
beer and the item set with milk and beer.

54
00:03:30.830 --> 00:03:33.120
The remaining two items that end.

55
00:03:33.120 --> 00:03:36.150
One item such or
then use to create the three item sets.

56
00:03:37.560 --> 00:03:39.460
Let's look now at
creating three item sets.

57
00:03:40.750 --> 00:03:45.190
The only three item sets that has a
support value greater than minimum support

58
00:03:45.190 --> 00:03:47.320
is the one shown in the right table.

59
00:03:47.320 --> 00:03:50.240
Namely the items start with bread,
milk and diaper.

60
00:03:51.940 --> 00:03:57.130
The second step in association analysis
is to identify the frequent item sets.

61
00:03:57.130 --> 00:03:59.860
But note that the process
that we just described for

62
00:03:59.860 --> 00:04:03.530
creating item sets already
identifies frequent item sets.

63
00:04:04.530 --> 00:04:07.880
A frequent item set is one whose
support is greater than or

64
00:04:07.880 --> 00:04:10.590
equal to the minimum support.

65
00:04:10.590 --> 00:04:15.580
So by keeping track of the support of
each item set as it is being created and

66
00:04:15.580 --> 00:04:18.020
removing item sets with low support,

67
00:04:18.020 --> 00:04:20.570
we are already identifying
frequent item sets.

68
00:04:21.650 --> 00:04:26.740
For our example, the frequent one,
two and three item sets are shown here.

69
00:04:28.230 --> 00:04:31.940
Now that we identified the frequent
item sets, the last step is to

70
00:04:31.940 --> 00:04:35.650
generate the rules to capture
associations that we see in the data.

71
00:04:37.050 --> 00:04:41.627
Let's first define some terms we'll
need to discuss association rules.

72
00:04:41.627 --> 00:04:45.690
The format of an association
rule is shown at the top.

73
00:04:45.690 --> 00:04:51.910
It's written as X arrow Y and
is read as if X, then Y.

74
00:04:51.910 --> 00:04:55.190
The X part is called the antecedent and

75
00:04:55.190 --> 00:04:57.770
the Y part is called
the consequent of the rule.

76
00:04:58.870 --> 00:05:00.520
X and Y are item sets.

77
00:05:01.560 --> 00:05:04.920
An important term in rule
generation is the rule confidence.

78
00:05:06.120 --> 00:05:08.680
This is to find as a support for X and

79
00:05:08.680 --> 00:05:14.080
Y together divided by the support for
X only.

80
00:05:14.080 --> 00:05:16.680
So rule confidence
calculates the frequency of

81
00:05:16.680 --> 00:05:18.710
instances to which the rule applies.

82
00:05:20.210 --> 00:05:24.830
Recall that the support for
X is the frequency of item set X and

83
00:05:24.830 --> 00:05:28.470
is defined as the number of
transactions containing items in X

84
00:05:28.470 --> 00:05:31.595
divided by the total
number of transactions.

85
00:05:31.595 --> 00:05:34.875
The rule confidence measures
how frequently items in

86
00:05:34.875 --> 00:05:38.585
Y appear in the transaction
that contain X.

87
00:05:38.585 --> 00:05:43.365
In other words, the confidence measures
the reliability of the rule by determining

88
00:05:43.365 --> 00:05:48.635
how often, if X and
Y is found to be true in the data.

89
00:05:48.635 --> 00:05:50.835
How is rule confidence
used in rule generation?

90
00:05:52.230 --> 00:05:57.160
Association rules are generated from the
frequent item sets created from the data.

91
00:05:57.160 --> 00:06:01.070
Each item in an item set can be
used as a part of the antecedent or

92
00:06:01.070 --> 00:06:03.200
consequent of the rule.

93
00:06:03.200 --> 00:06:06.970
And you can have many ways to combine
items to form the antecedent and

94
00:06:06.970 --> 00:06:07.490
consequent.

95
00:06:08.810 --> 00:06:12.210
So if we just simply generate
rules from each frequent item set,

96
00:06:12.210 --> 00:06:14.390
we would end up with lots and
lots of rules.

97
00:06:15.390 --> 00:06:20.680
Each item set with k items can
generate 2 to the k-2 rules.

98
00:06:20.680 --> 00:06:22.330
That's a lot of rules.

99
00:06:22.330 --> 00:06:25.240
And the majority of those rules
would not be found in the data.

100
00:06:26.330 --> 00:06:28.070
This is where rule confidence comes in.

101
00:06:29.380 --> 00:06:33.140
We can use rule confidence to
constrain the number of rules to keep.

102
00:06:34.190 --> 00:06:38.140
Specifically, a minimum
confidence threshold is set and

103
00:06:38.140 --> 00:06:40.810
only rules with confidence greater than or

104
00:06:40.810 --> 00:06:45.410
equal to the minimum confidence are
significant and only those will be kept.

105
00:06:46.850 --> 00:06:49.320
Let's look at how this works
with our example dataset.

106
00:06:50.520 --> 00:06:55.150
We call that only one three item set
was created from the transactions.

107
00:06:55.150 --> 00:06:58.380
That three items that contains
items bread, milk and

108
00:06:58.380 --> 00:06:59.630
diaper as shown at the top.

109
00:07:00.730 --> 00:07:05.000
With these three item set let's see
how we can generate rules from it and

110
00:07:05.000 --> 00:07:07.670
determine which rules to keep and
which one to prune.

111
00:07:09.150 --> 00:07:12.507
Let's set the minimum confidence to 0.95.

112
00:07:12.507 --> 00:07:15.350
And here again is the definition for
confidence.

113
00:07:16.890 --> 00:07:21.250
For candidate rule if bread and
milk then diaper,

114
00:07:21.250 --> 00:07:24.940
we can calculate it's confidence
as follows the support for

115
00:07:24.940 --> 00:07:29.790
both antecedent and consequent is
the number of times we see bread, milk and

116
00:07:29.790 --> 00:07:35.570
diaper together in the data, divided
by the total number of transactions.

117
00:07:35.570 --> 00:07:40.240
Items bread, milk and diaper appear
together in transaction 1, 4 and

118
00:07:40.240 --> 00:07:43.720
5 so the support is 3/5.

119
00:07:43.720 --> 00:07:47.570
The support for just the antecedent is
the number of times we see bread and

120
00:07:47.570 --> 00:07:50.690
milk together divided by
the total number of transactions.

121
00:07:51.770 --> 00:07:56.778
Items bread and milk appear together
also in transactions 1, 4, and 5.

122
00:07:56.778 --> 00:07:59.880
So the support is 3/5.

123
00:07:59.880 --> 00:08:04.119
The confidence of this rule is then 1,
or 100%.

124
00:08:04.119 --> 00:08:07.743
This means that the rule is correct 100%.

125
00:08:07.743 --> 00:08:11.540
Every time bread and milk are bought
together, diaper is bought as well.

126
00:08:12.780 --> 00:08:14.780
For candidate rule if bread and

127
00:08:14.780 --> 00:08:19.170
diaper than milk,
we calculate its confidence the same way.

128
00:08:19.170 --> 00:08:23.780
The support for bread, diaper and
milk is 3/5 as before.

129
00:08:23.780 --> 00:08:29.920
Items bread and diaper are paired
together in transactions 1, 2, 4 and 5.

130
00:08:29.920 --> 00:08:34.230
So the support for
the items set with bread and milk is 4/5.

131
00:08:34.230 --> 00:08:39.324
Then the confidence with
this rule is 0.75 or 75%.

132
00:08:39.324 --> 00:08:42.845
Since the minimum confidence is 0.95 or
95%,

133
00:08:42.845 --> 00:08:47.710
the first rule is kept and the second
rule is removed from consideration.

134
00:08:49.430 --> 00:08:52.670
There are several algorithms for
association analysis.

135
00:08:52.670 --> 00:08:56.820
Each uses a different set of methods
to make frequent items set creation and

136
00:08:56.820 --> 00:08:58.970
rule generation efficient.

137
00:08:58.970 --> 00:09:03.080
The more popular algorithms are Apriori,
FP Growth and Eclat.

138
00:09:04.560 --> 00:09:09.410
As a summary, we just looked at the steps
in association analysis in more detail.

139
00:09:09.410 --> 00:09:14.055
We saw how items sets can be created from
a dataset, how frequent items sets can be

140
00:09:14.055 --> 00:09:18.770
identified, and how association rules can
be created from frequent item sets and

141
00:09:18.770 --> 00:09:20.633
pruned using rule confidence.