WEBVTT

1
00:00:01.130 --> 00:00:04.720
In addition to the evaluation
matrix covered n the last lecture.

2
00:00:04.720 --> 00:00:08.170
The performance of a classification
model can also be evaluated

3
00:00:08.170 --> 00:00:10.430
using a Confusion Matrix.

4
00:00:10.430 --> 00:00:12.990
We will introduce the Confusion Matrix,
in this lecture.

5
00:00:14.180 --> 00:00:16.990
After this video you will be able to,

6
00:00:16.990 --> 00:00:21.410
describe how a confusion matrix can
be used to evaluate a classifier.

7
00:00:21.410 --> 00:00:24.240
Interpret the confusion matrix of a model.

8
00:00:24.240 --> 00:00:27.160
And relate accuracy to values
in a confusion matrix.

9
00:00:28.430 --> 00:00:34.080
Let's use our example again of predicting
whether a given animal is a mammal or not.

10
00:00:34.080 --> 00:00:38.280
Recall that this is a binary
classification task, with the Class Label

11
00:00:38.280 --> 00:00:42.849
being either Yes, indicating mammal or
No indicating non-mammal.

12
00:00:44.360 --> 00:00:47.190
Now let's review the different Types
of Errors that you can get with

13
00:00:47.190 --> 00:00:48.810
Classification.

14
00:00:48.810 --> 00:00:50.650
If the True Label is Yes and

15
00:00:50.650 --> 00:00:55.760
the Predicted Label is Yes, then this
is a True Positive, abbreviated as TP.

16
00:00:56.850 --> 00:01:00.450
This is a case where the label is
correctly predicted as positive.

17
00:01:01.500 --> 00:01:03.430
If the True Label is No and

18
00:01:03.430 --> 00:01:08.280
the Predicted Label is No, then this
is a True Negative, abbreviated as TN.

19
00:01:09.350 --> 00:01:12.710
This is the case where the label is
correctly predicted as negative.

20
00:01:13.900 --> 00:01:15.970
If the True Label is No and

21
00:01:15.970 --> 00:01:20.960
the Predicted Label is Yes, then this
is a False Positive, abbreviated as FP.

22
00:01:22.240 --> 00:01:26.910
This is the case where the label is
incorrectly predicted as positive

23
00:01:26.910 --> 00:01:27.830
when it should be negative.

24
00:01:29.110 --> 00:01:32.850
If the True Label is Yes and
the Predicted Label is No,

25
00:01:32.850 --> 00:01:36.180
then this is a False Negative
abbreviated as FN.

26
00:01:37.300 --> 00:01:41.350
This is the case where the label is
incorrectly predicted as negative,

27
00:01:41.350 --> 00:01:42.390
when it should be positive.

28
00:01:44.390 --> 00:01:47.910
A Confusion Matrix can be used to
summarize the different types of

29
00:01:47.910 --> 00:01:49.050
classification errors.

30
00:01:50.100 --> 00:01:54.710
The True Positive cell corresponds to the
samples that are correctly predicted as

31
00:01:54.710 --> 00:01:56.180
positive by the model.

32
00:01:57.290 --> 00:02:01.460
The True Negative cell corresponds to
the samples that are correctly predicted

33
00:02:01.460 --> 00:02:02.030
as negative.

34
00:02:03.070 --> 00:02:07.240
The False Positive cell corresponds to
samples that are incorrectly predicted

35
00:02:07.240 --> 00:02:07.940
as positive.

36
00:02:08.950 --> 00:02:13.310
The False Negative cell corresponds to
samples that are incorrectly predicted

37
00:02:13.310 --> 00:02:13.810
as negative.

38
00:02:15.110 --> 00:02:19.280
Each cell has the count, or percentage
of samples, with each type of errors.

39
00:02:20.940 --> 00:02:25.520
Let's look at an example to see how
a Confusion Matrix is filled in.

40
00:02:25.520 --> 00:02:29.750
The table on the lists True Labels
along with the models prediction for

41
00:02:29.750 --> 00:02:31.910
a data set of ten samples.

42
00:02:31.910 --> 00:02:34.610
We'll summarize this results
in a Confusion Matrix.

43
00:02:36.100 --> 00:02:39.140
First let's figure out
the number of true positives.

44
00:02:39.140 --> 00:02:42.860
We call that a true positive occurrence
when the output is correctly predicted

45
00:02:42.860 --> 00:02:44.320
as positive.

46
00:02:44.320 --> 00:02:49.140
In other words, the true label is Yes and
the model's prediction is also Yes.

47
00:02:49.140 --> 00:02:54.680
In this example, there are three true
positives as indicated by the red arrows.

48
00:02:54.680 --> 00:02:58.390
We enter three and the true positive
cell in the Confusion Matrix.

49
00:02:59.660 --> 00:03:02.310
Now let's look at the true negatives.

50
00:03:02.310 --> 00:03:06.950
A true negative occurs when the output
is correctly predicted as negative.

51
00:03:06.950 --> 00:03:12.150
In other words, the true label is No and
the models prediction is also No.

52
00:03:12.150 --> 00:03:13.160
In this example,

53
00:03:13.160 --> 00:03:17.360
there are four true negatives as
indicated by the green arrows.

54
00:03:17.360 --> 00:03:21.050
We enter four in the true negative
cell in the Confusion Matrix.

55
00:03:22.800 --> 00:03:24.720
What about false negatives?

56
00:03:24.720 --> 00:03:29.490
A false negative occurs when the output
is incorrectly predicted as negative,

57
00:03:29.490 --> 00:03:31.390
when it should be positive.

58
00:03:31.390 --> 00:03:34.890
That is the true label is Yes,
and the model's prediction is No.

59
00:03:35.890 --> 00:03:36.690
In this example,

60
00:03:36.690 --> 00:03:40.990
there are two false negatives as
indicated by the purple arrows.

61
00:03:40.990 --> 00:03:44.530
We enter two in the false negative
cell in the Confusion Matrix.

62
00:03:45.970 --> 00:03:49.030
Finally we need to look
at false positives.

63
00:03:49.030 --> 00:03:50.730
A false positive occurs,

64
00:03:50.730 --> 00:03:55.760
when the input is incorrectly predicted
as positive, when it should be negative.

65
00:03:55.760 --> 00:04:00.380
That is the true label is No and
models prediction is Yes.

66
00:04:00.380 --> 00:04:04.020
In this example there is one false
positive as indicated by the yellow arrow.

67
00:04:05.300 --> 00:04:08.870
We enter one in the false positive
cell in the Confusion Matrix.

68
00:04:10.210 --> 00:04:13.460
This is our complete Confusion Matrix for
this example.

69
00:04:13.460 --> 00:04:16.630
We see that the sum of the numbers
in the cells add up to ten,

70
00:04:16.630 --> 00:04:18.820
which is the number of
samples in our dataset.

71
00:04:20.650 --> 00:04:23.320
Note that the diagonal values,
true positives and

72
00:04:23.320 --> 00:04:27.150
true negatives are samples
with Correct Predictions.

73
00:04:27.150 --> 00:04:30.010
In our example, these values sum up to 7.

74
00:04:30.010 --> 00:04:34.340
Meaning that 7 out of 10 samples were
correctly predicted by the model.

75
00:04:35.370 --> 00:04:39.910
The higher the sum of the diagonal values,
the better the performance of the model.

76
00:04:41.430 --> 00:04:46.050
The off diagonal values capture
the misclassified samples.

77
00:04:46.050 --> 00:04:49.040
Where the model's predictions
do not match the true labels.

78
00:04:50.070 --> 00:04:55.242
In this example these values indicate
that there were three misclassifications.

79
00:04:55.242 --> 00:04:56.410
Smaller values for

80
00:04:56.410 --> 00:05:01.130
the off diagonal cells in the confusion
matrix indicate better model performance.

81
00:05:03.320 --> 00:05:06.230
Note that the diagonal values,
true positives and

82
00:05:06.230 --> 00:05:10.080
true negatives are samples
with correct predictions.

83
00:05:10.080 --> 00:05:13.970
In our example,
these values sum up to 7 meaning

84
00:05:13.970 --> 00:05:18.030
that 7 out of 10 samples were
correctly predicted by the model.

85
00:05:18.030 --> 00:05:22.350
The higher the sum of the diagonal values,
the better the performance of the model.

86
00:05:23.460 --> 00:05:27.910
You may have noticed that the diagonal
values are related to Accuracy Rate.

87
00:05:27.910 --> 00:05:31.390
Recall that accuracy is defined
as the sum of two positives and

88
00:05:31.390 --> 00:05:34.610
true negatives, divided by all samples.

89
00:05:34.610 --> 00:05:36.352
The sum of two positives and

90
00:05:36.352 --> 00:05:40.839
two negatives is the sum of the diagonal
values in a confusion matrix.

91
00:05:40.839 --> 00:05:44.516
The sum of the diagonal
values in a confusion matrix,

92
00:05:44.516 --> 00:05:49.187
divided by the total number of
samples gives you the Accuracy Rate.

93
00:05:49.187 --> 00:05:54.200
Similarly, the off diagonal values
are related to the Error Rate.

94
00:05:54.200 --> 00:05:58.320
Recall that the Error Rate is
the opposite of the Accuracy Rate.

95
00:05:58.320 --> 00:06:02.222
The sum of the off diagonal
values in a confusion matrix,

96
00:06:02.222 --> 00:06:06.783
divided by the total number of samples,
gives you the Error Rate.

97
00:06:06.783 --> 00:06:10.399
Looking at the values in the Confusion
Matrix can help you understand

98
00:06:10.399 --> 00:06:13.529
the kind of Misclassifications
that your model is making.

99
00:06:13.529 --> 00:06:16.937
A high value for
this cell indicated by the yellow arrow,

100
00:06:16.937 --> 00:06:22.250
means that classifying the Positive
class is problematic for the model.

101
00:06:22.250 --> 00:06:26.460
A high value for the cell indicated by
the orange arrow on the other hand,

102
00:06:26.460 --> 00:06:30.260
means that classifying the Negative
class is problematic for the model.

103
00:06:31.340 --> 00:06:34.230
In summary,
the Confusion Matrix is a table

104
00:06:34.230 --> 00:06:37.980
used to summarize the different
types of errors for a classifier.

105
00:06:37.980 --> 00:06:41.990
The values in a Confusion Matrix can
be used to evaluate the performance of

106
00:06:41.990 --> 00:06:47.470
a classifier and are related to evaluation
metrics, such as accuracy and error rates.

107
00:06:47.470 --> 00:06:51.020
They also indicate what other types of
misclassifications the model is making.

108
00:06:52.040 --> 00:06:56.334
Note that in some implementations
of a Confusion Matrix, the true and

109
00:06:56.334 --> 00:06:58.460
predictive labels are switched.

110
00:06:58.460 --> 00:07:03.025
Be sure to review the documentation for
the software you're using to generate

111
00:07:03.025 --> 00:07:06.565
a Confusion Matrix to understand
what each cell specifies.