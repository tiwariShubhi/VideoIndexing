WEBVTT

1
00:00:01.000 --> 00:00:05.150
Generalization and overfitting are very
important concepts in machine learning.

2
00:00:05.150 --> 00:00:07.390
We will cover them in
the next three lectures.

3
00:00:08.690 --> 00:00:13.660
After this video you will be able
to define what overfitting is,

4
00:00:13.660 --> 00:00:17.690
describe how overfitting is
related to generalization, and

5
00:00:17.690 --> 00:00:19.799
explain why overfitting should be avoided.

6
00:00:21.140 --> 00:00:25.860
Before we look at generalization and
overfitting, let's first define some terms

7
00:00:25.860 --> 00:00:28.710
that we will need to know to
discuss errors in classification.

8
00:00:30.140 --> 00:00:35.400
Recall that a machine learning model
maps the input it receives to an output.

9
00:00:35.400 --> 00:00:39.760
For a classification model, the model's
output is the predicted class label for

10
00:00:39.760 --> 00:00:43.580
the input variables and
the true class label is the target.

11
00:00:45.210 --> 00:00:48.640
Then if the classifier predicts
the correct classes label for

12
00:00:48.640 --> 00:00:51.610
a sample, that is a success.

13
00:00:51.610 --> 00:00:55.370
If the predicted class label is
different from the true class label,

14
00:00:55.370 --> 00:00:56.530
then that is an error.

15
00:00:57.880 --> 00:01:02.950
The error rate, then, is the percentage
of errors made over the entire data set.

16
00:01:02.950 --> 00:01:07.330
That is, it is the number of errors
divided by the total number of samples

17
00:01:07.330 --> 00:01:07.950
in a data set.

18
00:01:08.980 --> 00:01:13.260
Error rate is also known as
misclassification rate, or simply error.

19
00:01:14.690 --> 00:01:19.160
In our lesson on classification we discuss
that there is a training phase in which

20
00:01:19.160 --> 00:01:24.910
the model is built, and a testing phase in
which the model is applied to new data.

21
00:01:24.910 --> 00:01:30.070
The model is built using training data and
evaluated on test data.

22
00:01:30.070 --> 00:01:33.760
The training and
test data are two different data sets.

23
00:01:33.760 --> 00:01:37.810
The goal in building a machine learning
model is to have the model perform well

24
00:01:37.810 --> 00:01:41.450
on training, as well as test data.

25
00:01:41.450 --> 00:01:46.290
Error rate, or simply error, on the
training data is refered to as training

26
00:01:46.290 --> 00:01:50.960
error, and the error on test data
is referred to as test error.

27
00:01:52.030 --> 00:01:56.120
The error on the test data is
an indication of how well the classifier

28
00:01:56.120 --> 00:01:57.560
will perform on new data.

29
00:01:58.690 --> 00:02:01.210
This is known as generalization.

30
00:02:01.210 --> 00:02:05.180
Generalization refers to how well
your model performs on new data,

31
00:02:05.180 --> 00:02:07.240
that is data not used to train the model.

32
00:02:08.260 --> 00:02:11.530
You want your model to
generalize well to new data.

33
00:02:11.530 --> 00:02:15.200
If your model generalizes well, then
it will perform well on data sets that

34
00:02:15.200 --> 00:02:18.210
are similar in structure
to the training data, but

35
00:02:18.210 --> 00:02:21.330
doesn't contain exactly the same
samples as in the training set.

36
00:02:22.700 --> 00:02:27.280
Since the test error indicates how well
your model generalizes to new data,

37
00:02:27.280 --> 00:02:30.330
note that the test error is also
called generalization error.

38
00:02:31.780 --> 00:02:35.280
A related concept to
generalization is overfitting.

39
00:02:35.280 --> 00:02:37.810
If your model has very
low training error but

40
00:02:37.810 --> 00:02:41.290
high generalization error,
then it is overfitting.

41
00:02:41.290 --> 00:02:45.680
This means that the model has learned to
model the noise in the training data,

42
00:02:45.680 --> 00:02:48.250
instead of learning the underlying
structure of the data.

43
00:02:49.530 --> 00:02:52.180
These plots illustrate what
happens when a model overfits.

44
00:02:53.190 --> 00:02:55.470
Training samples are shown as points, and

45
00:02:55.470 --> 00:03:00.340
the input to output mapping that the model
has learned is indicated as a curve.

46
00:03:00.340 --> 00:03:04.260
The plot on the left shows that the model
has learned the underlying structure of

47
00:03:04.260 --> 00:03:09.530
the data, as the curve follows
the trend of the sample point as well.

48
00:03:09.530 --> 00:03:10.580
The plot on the right,

49
00:03:10.580 --> 00:03:14.640
however, shows that the model has learned
to model the noise in a data set.

50
00:03:15.640 --> 00:03:18.640
The model tries to capture
every sample point,

51
00:03:18.640 --> 00:03:21.210
instead of the general trend
of the samples together.

52
00:03:22.220 --> 00:03:23.220
The training error and

53
00:03:23.220 --> 00:03:26.440
the generalization error are plotted
together, during model training.

54
00:03:28.400 --> 00:03:32.000
What is the connection between
overfitting and generalization?

55
00:03:32.000 --> 00:03:35.820
A model that overfits will not
generalize well to new data.

56
00:03:35.820 --> 00:03:39.525
So the model will do well on just
the data it was trained on, but

57
00:03:39.525 --> 00:03:42.410
given a new data set,
it will perform poorly.

58
00:03:43.420 --> 00:03:46.650
A classifier that performs well
on just the training data set

59
00:03:46.650 --> 00:03:48.270
will not be very useful.

60
00:03:48.270 --> 00:03:52.370
So it is essential that the goal of good
generalization performance is kept in mind

61
00:03:52.370 --> 00:03:53.170
when building a model.

62
00:03:54.890 --> 00:03:57.380
A problem related to
overfitting is underfitting.

63
00:03:58.890 --> 00:04:03.540
Overfitting occurs when the model is
fitting to the noise in the training data.

64
00:04:03.540 --> 00:04:06.770
This results in low training error and
high test error.

65
00:04:08.330 --> 00:04:09.830
Underfitting on the other hand,

66
00:04:09.830 --> 00:04:13.940
occurs when the model has not
learned the structure of the data.

67
00:04:13.940 --> 00:04:16.930
This results in high training error and
high test error.

68
00:04:18.590 --> 00:04:20.164
Both are undesirable,

69
00:04:20.164 --> 00:04:24.658
since both mean that the model will
not generalize well to new data.

70
00:04:24.658 --> 00:04:28.903
Overfitting generally occurs when
a model is too complex, that is,

71
00:04:28.903 --> 00:04:33.306
it has too many parameters relative
to the number of training samples.

72
00:04:33.306 --> 00:04:37.911
So to avoid overfitting, the model needs
to be kept as simple as possible, and yet

73
00:04:37.911 --> 00:04:42.400
still solve the input/output mapping for
the given data set.

74
00:04:42.400 --> 00:04:43.880
We will discuss methods for

75
00:04:43.880 --> 00:04:46.470
avoiding overfitting in
the next couple of lectures.

76
00:04:47.940 --> 00:04:53.220
In summary, overfitting is when your model
has learned the noise in the training data

77
00:04:53.220 --> 00:04:55.870
instead of the underlying
structure of the data.

78
00:04:55.870 --> 00:05:00.353
You want to avoid overfitting so that your
model will generalize well to new data.