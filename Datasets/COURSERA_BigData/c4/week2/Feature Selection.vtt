WEBVTT

1
00:00:01.190 --> 00:00:04.590
In addition to cleaning your data
to address data quality issues,

2
00:00:04.590 --> 00:00:09.470
data preparation also includes
selecting features to use for analysis.

3
00:00:09.470 --> 00:00:14.980
After this video, you will be able to
explain what feature selection involves,

4
00:00:14.980 --> 00:00:18.880
discuss the goal of feature selection,
and list three approaches for

5
00:00:18.880 --> 00:00:20.940
selecting features.

6
00:00:20.940 --> 00:00:23.720
Feature selection refers to
choosing the set of features to

7
00:00:23.720 --> 00:00:27.000
use that is appropriate for
the subsequent analysis.

8
00:00:27.000 --> 00:00:30.930
The goal of feature selection is to come
up with the smallest set of features

9
00:00:30.930 --> 00:00:35.030
that best captures the characteristics
of the problem being addressed.

10
00:00:35.030 --> 00:00:39.030
The smaller the number of features used,
the simpler the analysis will be.

11
00:00:39.030 --> 00:00:39.980
But of course,

12
00:00:39.980 --> 00:00:44.430
the set of features used must include
all features relevant to the problem.

13
00:00:44.430 --> 00:00:47.100
So, there must be a balance
between expressiveness and

14
00:00:47.100 --> 00:00:48.830
compactness of the feature set.

15
00:00:48.830 --> 00:00:53.560
There are several methods to
consider in selecting features.

16
00:00:53.560 --> 00:00:55.010
New features can be added,

17
00:00:55.010 --> 00:01:00.430
some features can be removed, features can
be re-coded or features can be combined.

18
00:01:00.430 --> 00:01:04.060
All these operations affect the final
set of features that will be used for

19
00:01:04.060 --> 00:01:05.270
analysis.

20
00:01:05.270 --> 00:01:07.790
Of course some features
can be kept as is as well.

21
00:01:09.210 --> 00:01:12.070
New features can be derived
from existing features.

22
00:01:12.070 --> 00:01:17.200
For example, a new feature to specify
whether a student is in state or

23
00:01:17.200 --> 00:01:21.700
out of state can be added based on
the student's state of residence.

24
00:01:21.700 --> 00:01:24.490
For an application such
as college admissions,

25
00:01:24.490 --> 00:01:28.300
this new feature represents an important
aspect of an application and so

26
00:01:28.300 --> 00:01:30.450
would be very helpful
as a separate feature.

27
00:01:31.480 --> 00:01:35.510
Another example is adding a feature
to indicate the color of a vehicle,

28
00:01:35.510 --> 00:01:38.870
which can play an important role
in an auto insurance application.

29
00:01:39.940 --> 00:01:43.000
Features can also be removed,
candidates for

30
00:01:43.000 --> 00:01:45.950
removal are features
that are very correlated.

31
00:01:45.950 --> 00:01:48.630
During data exploration,
you may have discovered that

32
00:01:48.630 --> 00:01:53.820
two features are very correlated,
that is they change in very similar ways.

33
00:01:53.820 --> 00:01:55.990
For example,
the purchase price of a product and

34
00:01:55.990 --> 00:02:00.480
the amount of sales tax paid
are likely to be very correlated.

35
00:02:00.480 --> 00:02:04.250
The higher the purchase price
the higher the sales tax.

36
00:02:04.250 --> 00:02:07.110
In this case, you might want
to drop one of these features,

37
00:02:07.110 --> 00:02:10.340
since these features have
essentially duplicate information.

38
00:02:10.340 --> 00:02:13.280
And keeping both features makes
the feature set larger and

39
00:02:13.280 --> 00:02:16.770
the analysis unnecessarily more complex.

40
00:02:16.770 --> 00:02:19.810
Features with a high percentage of
missing values may also be good

41
00:02:19.810 --> 00:02:21.640
candidates for removal.

42
00:02:21.640 --> 00:02:22.740
The validity and

43
00:02:22.740 --> 00:02:26.360
usefulness of features with a lot
of missing values are in question.

44
00:02:26.360 --> 00:02:29.810
So removing may not result
in any loss of information.

45
00:02:29.810 --> 00:02:32.350
Again these features would have
been discovered during the data

46
00:02:32.350 --> 00:02:33.090
exploration step.

47
00:02:34.240 --> 00:02:37.880
Irrelevant features should also
be removed from the data set.

48
00:02:37.880 --> 00:02:41.550
Irrelevant features are those that
contain no information that is useful for

49
00:02:41.550 --> 00:02:43.320
the analysis task.

50
00:02:43.320 --> 00:02:46.778
An example of this is employee
ID in predicting income.

51
00:02:46.778 --> 00:02:51.770
Other fields used simply for
identification such as row number,

52
00:02:51.770 --> 00:02:54.850
person's ID, etc.,
are good candidates for removal.

53
00:02:55.940 --> 00:02:59.990
Features can also be combined if the new
feature presents important information

54
00:02:59.990 --> 00:03:03.570
that is not represented by looking at
the original features individually.

55
00:03:05.160 --> 00:03:09.640
For example, BMI, which is body
mass index is an indicator of

56
00:03:09.640 --> 00:03:13.290
whether a person is underweight,
average weight, or overweight.

57
00:03:14.290 --> 00:03:17.890
This is an important feature to have for
a weight loss application.

58
00:03:17.890 --> 00:03:22.270
It represents information about how much
a person weighs relative to their height

59
00:03:22.270 --> 00:03:26.230
that is not available by looking at just
the person's height or weight alone.

60
00:03:27.840 --> 00:03:31.480
A feature can be re-coded as
appropriate for the application.

61
00:03:31.480 --> 00:03:35.550
A common example of this is when you
want to turn a continuous feature in to

62
00:03:35.550 --> 00:03:37.420
a categorical one.

63
00:03:37.420 --> 00:03:42.324
For example for a marketing application
you might want to re-code customer's age

64
00:03:42.324 --> 00:03:48.910
into customer categories such as teenager,
young adult, adult and senior citizen.

65
00:03:48.910 --> 00:03:55.310
So you would map ages 13 to 19 to
teenager, ages 20 to 25 to young adult,

66
00:03:55.310 --> 00:04:00.370
26 to 55 as adult and over 55 as senior.

67
00:04:01.590 --> 00:04:05.640
For some applications you may want
to make use of finery features.

68
00:04:05.640 --> 00:04:09.550
As an example, you might want a feature to
capture whether a customer tends to buy

69
00:04:09.550 --> 00:04:11.810
expensive items or not.

70
00:04:11.810 --> 00:04:16.100
In this case, you would want a feature
that maps to one for a customer with

71
00:04:16.100 --> 00:04:20.484
an average purchase price over a certain
amount and maps to zero otherwise.

72
00:04:20.484 --> 00:04:24.960
Re-coding features can also
result in breaking one feature

73
00:04:24.960 --> 00:04:26.920
into multiple features.

74
00:04:26.920 --> 00:04:30.730
A common example of this is to
separate an address feature

75
00:04:30.730 --> 00:04:36.580
into its constituent parts street address,
city, state and zip code.

76
00:04:36.580 --> 00:04:39.280
This way you can more easily
group records by state, for

77
00:04:39.280 --> 00:04:42.130
example, to provide
a state by state analysis.

78
00:04:43.550 --> 00:04:48.280
Future selection aims to select
the smallest set of features to best

79
00:04:48.280 --> 00:04:51.100
capture the characteristics of
the data for your application.

80
00:04:52.390 --> 00:04:56.310
Know from the examples represented
that domain knowledge once again place

81
00:04:56.310 --> 00:05:00.400
a key role in choosing
the appropriate features to use.

82
00:05:00.400 --> 00:05:04.934
Good understanding of the application is
essential in deciding which features to

83
00:05:04.934 --> 00:05:06.190
add, drop or modify.

84
00:05:07.350 --> 00:05:11.256
It should also be noted that feature
selection can be referred to as feature

85
00:05:11.256 --> 00:05:15.596
engineering, since what you're doing here
is to engineer the best feature set for

86
00:05:15.596 --> 00:05:16.661
your application.