WEBVTT

1
00:00:03.530 --> 00:00:04.180
Our third and

2
00:00:04.180 --> 00:00:09.080
final case is applicable to most companies
that create customer-focused products.

3
00:00:11.030 --> 00:00:15.270
They want to understand how their
customers are responding to the products,

4
00:00:15.270 --> 00:00:17.800
how the product marketing
efforts are performing,

5
00:00:17.800 --> 00:00:21.470
what kind of problems customers
are encountering, and what new features or

6
00:00:21.470 --> 00:00:24.330
feature improvements the customers
are seeking, and so forth.

7
00:00:25.380 --> 00:00:28.010
But how does the company
get this information?

8
00:00:28.010 --> 00:00:31.060
What kind of data sources
would carry this information?

9
00:00:31.060 --> 00:00:33.710
The figure show some of these sources.

10
00:00:33.710 --> 00:00:40.210
They are in focused user surveys,
emails sent by the customers, in blogs and

11
00:00:40.210 --> 00:00:46.350
product review forums, specialized
groups on social media and user forums.

12
00:00:46.350 --> 00:00:52.990
In short, they are on the Internet or
in material received through the Internet.

13
00:00:52.990 --> 00:00:54.250
Now, how many sources are there?

14
00:00:55.830 --> 00:00:57.200
Two.

15
00:00:57.200 --> 00:00:58.940
The number would vary.

16
00:00:58.940 --> 00:01:01.060
A new sites, a new postings, and

17
00:01:01.060 --> 00:01:02.950
new discussion threads
would come up all the time.

18
00:01:04.000 --> 00:01:07.870
In all of these,
the goal is to identify information that

19
00:01:07.870 --> 00:01:12.130
truly relates to the companies product,
its features and its utility.

20
00:01:14.470 --> 00:01:17.530
To cast this as a type
of big data problem,

21
00:01:17.530 --> 00:01:20.871
we look at a task that computer
scientists called Data Fusion.

22
00:01:22.690 --> 00:01:27.840
Consider a set of data sources, S,
as we mentioned on the last slide and

23
00:01:27.840 --> 00:01:29.320
a set of data items, D.

24
00:01:30.630 --> 00:01:35.420
A data item represents a particular
aspect of a real world entity

25
00:01:35.420 --> 00:01:37.290
which in our case is
a product of the company.

26
00:01:39.180 --> 00:01:44.860
For each data item, a source can, but
not necessarily will, provide a value.

27
00:01:44.860 --> 00:01:46.020
For example,

28
00:01:46.020 --> 00:01:51.450
the usability of an ergonomically
split keyboard can have a value good.

29
00:01:52.590 --> 00:01:57.500
The value can be atomic,
like good, or a set, or a list or

30
00:01:57.500 --> 00:01:58.910
sometimes embedded in the string.

31
00:02:00.370 --> 00:02:04.270
For example, the cursor sometimes
freezes when using the touchpad,

32
00:02:05.410 --> 00:02:09.300
is a string which has
a value about the touchpad.

33
00:02:11.400 --> 00:02:16.130
The goal of Data Fusion is to find
the values of Data Items from a source.

34
00:02:18.060 --> 00:02:23.160
In many cases, the system would find
a unique true value of an item.

35
00:02:23.160 --> 00:02:27.580
For example, the launch data of a product
in Europe should be the same true value

36
00:02:27.580 --> 00:02:29.530
regardless of the data
source one looks at.

37
00:02:30.700 --> 00:02:34.520
In other cases, we could find
a value distribution of an item.

38
00:02:34.520 --> 00:02:37.960
For example, the usability of our
keyboard may have a value distribution.

39
00:02:39.070 --> 00:02:43.960
That's with Data Fusion, we should be
able to collect the values of real world

40
00:02:43.960 --> 00:02:46.790
items from a subset of data sources.

41
00:02:46.790 --> 00:02:49.850
It is a subset because
not all Data Sources

42
00:02:49.850 --> 00:02:51.940
will have relevant information
about the Data Item.

43
00:02:53.440 --> 00:02:56.212
There are some other versions
of what a Data Fusion is but for

44
00:02:56.212 --> 00:02:58.992
our purposes we'll stick with
this general description.

45
00:03:01.190 --> 00:03:05.856
Now one obvious problem with the Internet
is that there are too many data

46
00:03:05.856 --> 00:03:09.530
sources at any time,
these lead to many difficulties.

47
00:03:10.910 --> 00:03:14.570
First, it is to be understood
that with too many data sources

48
00:03:14.570 --> 00:03:17.310
there will be many values for
the same item.

49
00:03:18.530 --> 00:03:21.190
Often these will differ and
sometimes they will conflict.

50
00:03:22.580 --> 00:03:25.740
A standard technique in this case
is to use a voting mechanism.

51
00:03:27.200 --> 00:03:31.820
However, even a voting
mechanism can be complex

52
00:03:31.820 --> 00:03:33.350
due to problems with the data source.

53
00:03:35.040 --> 00:03:38.800
One of the problems is to estimate
the trustworthiness of the source.

54
00:03:40.130 --> 00:03:42.080
For each data source,

55
00:03:42.080 --> 00:03:48.260
we need to evaluate whether it's reporting
some basic or known facts correctly.

56
00:03:48.260 --> 00:03:51.510
If a source mentions details
about a rainbow colored iPhone,

57
00:03:51.510 --> 00:03:55.010
which does not exist,
it's trustworthiness reduces

58
00:03:55.010 --> 00:03:57.960
because of the falsity of
the provided value of this data item.

59
00:03:59.270 --> 00:04:03.140
Accordingly, a higher vote count can be
assigned to a more trustworthy source.

60
00:04:04.590 --> 00:04:07.640
And then, this can be used in voting.

61
00:04:09.560 --> 00:04:11.260
The second aspect is Copy Detection.

62
00:04:12.700 --> 00:04:16.670
Detecting weather once was has copied
information from another can be very

63
00:04:16.670 --> 00:04:19.490
important for
detail fusion task in customer analytics.

64
00:04:20.660 --> 00:04:22.680
If a source has copied information,

65
00:04:23.750 --> 00:04:28.900
it's such that discounted vote count
can be assigned to a copy value and

66
00:04:28.900 --> 00:04:34.190
voting that means the copy in
source will have less weight.

67
00:04:35.210 --> 00:04:40.160
Now this is especially relevant when we
compute value distributions, because if we

68
00:04:40.160 --> 00:04:45.680
treat copies as genuine information, we
will statistically bias the distribution.

69
00:04:45.680 --> 00:04:50.279
Now here is active research on how to
detect copies, how to determine bias and

70
00:04:50.279 --> 00:04:54.685
then arrive at a statistically sound
estimation of value distribution.

71
00:04:54.685 --> 00:04:59.628
But to our knowledge, these methods are
yet to be applied to existing software for

72
00:04:59.628 --> 00:05:01.070
big data integration.

73
00:05:04.473 --> 00:05:06.240
It should be very clear by now but

74
00:05:06.240 --> 00:05:10.240
there are two kinds of big data
situations when it comes to information.

75
00:05:11.400 --> 00:05:16.180
The first two uses cases that we
saw requires an integration system

76
00:05:16.180 --> 00:05:20.380
to consider all sources because
the application demand so.

77
00:05:21.620 --> 00:05:27.660
In contrast, problems where data comes
from too many redundant, potentially

78
00:05:27.660 --> 00:05:32.380
unreliable sources like the Internet, the
best results can be obtained if we have

79
00:05:32.380 --> 00:05:36.490
a way of evaluating the worthiness of
sources before information integration.

80
00:05:37.660 --> 00:05:40.790
But this problem is
called Source Selection.

81
00:05:40.790 --> 00:05:44.570
The picture on the right shows the result
of a cost benefit analysis for

82
00:05:44.570 --> 00:05:46.080
data fusion.

83
00:05:46.080 --> 00:05:49.239
The x-axis indicates the number
of sources used, and

84
00:05:49.239 --> 00:05:53.400
the y-axis measures the proportion
of true results that were returned.

85
00:05:55.090 --> 00:05:59.480
We can clearly see that the plot peaks
around six-to-eight sources, and

86
00:05:59.480 --> 00:06:01.990
that the efficiency falls
as more sources are added.

87
00:06:03.790 --> 00:06:08.930
In a cost benefit analysis,
the cost must include both the human and

88
00:06:08.930 --> 00:06:10.580
the computational costs,

89
00:06:10.580 --> 00:06:14.270
while the benefit is a function of
the accuracy of the fusion result.

90
00:06:14.270 --> 00:06:19.210
The technique for
solving this problem comes from economics.

91
00:06:20.520 --> 00:06:23.870
Assuming that cost and
benefits are measure in the same unit, for

92
00:06:23.870 --> 00:06:24.660
example, dollars.

93
00:06:25.780 --> 00:06:28.580
They proposed to continue
selecting sources

94
00:06:28.580 --> 00:06:32.740
until the marginal benefit is
less than the marginal cost.

95
00:06:34.150 --> 00:06:38.210
Now recent techniques were performing
this computation at quite scalable.

96
00:06:38.210 --> 00:06:41.910
In one setting,
selecting the most beneficial sources

97
00:06:41.910 --> 00:06:45.520
from a total of one million
sources took less than one hour.

98
00:06:47.900 --> 00:06:51.980
This completes our coverage of
the big data integration problems.