WEBVTT

1
00:00:00.025 --> 00:00:05.173
[SOUND] Let us consider
a real life application to

2
00:00:05.173 --> 00:00:11.360
demonstrate the utility and
the challenges of big data.

3
00:00:11.360 --> 00:00:14.540
Many industries naturally deal
with large amounts of data.

4
00:00:15.540 --> 00:00:19.585
For our discussion, we consider
an energy company that provides gas and

5
00:00:19.585 --> 00:00:22.248
electricity to its
consumers in an urban area.

6
00:00:24.230 --> 00:00:28.354
In this news report, you can see
that Commonwealth Edison, or Con Ed,

7
00:00:28.354 --> 00:00:30.829
the gas and electric provider of New York,

8
00:00:30.829 --> 00:00:35.440
decided to place smart meters
all through its jurisdictions.

9
00:00:35.440 --> 00:00:38.000
That comes to 4.7 million smart meters.

10
00:00:39.540 --> 00:00:45.150
Now smart meters are smart because aside
from measuring energy consumption,

11
00:00:45.150 --> 00:00:48.730
they have a two way communication
capability between the meter and

12
00:00:48.730 --> 00:00:51.800
the central system at the gas and
electric company.

13
00:00:51.800 --> 00:00:56.590
In other words, they generate real time
data from the meters to be stored and

14
00:00:56.590 --> 00:00:58.060
processed at the central facility.

15
00:00:59.710 --> 00:01:00.280
How much data?

16
00:01:01.610 --> 00:01:03.070
According to this report,

17
00:01:03.070 --> 00:01:06.680
the number of data received at
the center is 1.5 billion per day.

18
00:01:06.680 --> 00:01:13.480
So the system will not only consume
this data, but process it and

19
00:01:14.650 --> 00:01:20.750
produce output at 15-minute intervals,
and sometimes, 5 minute intervals.

20
00:01:20.750 --> 00:01:22.270
Let's do the math.

21
00:01:22.270 --> 00:01:24.650
That comes to ingesting and processing

22
00:01:26.620 --> 00:01:30.360
about 10.5 million data
points per 15 minutes.

23
00:01:32.150 --> 00:01:36.830
So what kind of computation must
take place within these 15 minutes?

24
00:01:36.830 --> 00:01:41.760
Well, one obvious computation is billing,
where one needs to compute who, especially

25
00:01:41.760 --> 00:01:46.290
in the commercial sector, who actually
owns the meter and should be billed?

26
00:01:47.490 --> 00:01:49.980
This requires combining the meter data

27
00:01:49.980 --> 00:01:53.640
with the data in the customer
database maintained by the company.

28
00:01:53.640 --> 00:01:57.390
But let's just consider
computation related to analytics.

29
00:01:57.390 --> 00:02:01.180
We can list at least four
different kinds of computations.

30
00:02:02.610 --> 00:02:07.980
The first is computing the consumption
pattern per user, not per meter.

31
00:02:07.980 --> 00:02:12.840
Per user, where the output is
a histogram of hourly usage.

32
00:02:12.840 --> 00:02:16.270
So the x axis of the histogram
is hourly intervals And

33
00:02:16.270 --> 00:02:19.380
the y-axis is a number of units consumed.

34
00:02:20.950 --> 00:02:25.310
This leads to the computed both daily and
over larger time periods.

35
00:02:25.310 --> 00:02:28.560
To determine the hourly requirements for
this consumer.

36
00:02:30.430 --> 00:02:33.970
The second computation relates
to estimating the effects of

37
00:02:33.970 --> 00:02:37.160
outdoor temperature on the electricity
consumption of each consumer.

38
00:02:38.380 --> 00:02:40.890
For those you who
are statistically inclined,

39
00:02:40.890 --> 00:02:45.860
this often involves fitting a piece-wise
linear progression model to the data.

40
00:02:48.000 --> 00:02:53.080
The third task is to extract the daily
consumption trends that occur

41
00:02:53.080 --> 00:02:55.470
regardless of the outdoor temperature.

42
00:02:55.470 --> 00:02:59.590
This is again a statistical computation,
and may require something like

43
00:02:59.590 --> 00:03:03.570
a periodic alter regression algorithm,
for time series theta.

44
00:03:03.570 --> 00:03:05.290
The algorithm is not that important there.

45
00:03:06.410 --> 00:03:11.360
What's more important is the ability to
make make good prediction has a direct

46
00:03:11.360 --> 00:03:15.690
economic impact because the company
needs to buy energy from others.

47
00:03:15.690 --> 00:03:20.280
For example, an under-prediction
implies they'll end up paying more for

48
00:03:20.280 --> 00:03:23.940
buying energy at the last moment to
meet the consumer's requirements.

49
00:03:25.870 --> 00:03:30.590
The fourth task is to find groups of
similar consumers based on their usage

50
00:03:30.590 --> 00:03:34.860
pattern so that the company can determine
how many distinct groups of customers

51
00:03:34.860 --> 00:03:39.330
there are and design targeted energy
saving campaigns for each group.

52
00:03:40.370 --> 00:03:45.730
This requires finding similarities
over large number of time series data,

53
00:03:45.730 --> 00:03:47.140
which is a complex computation.

54
00:03:48.430 --> 00:03:52.110
Regardless of the number and
complexity of computation required,

55
00:03:52.110 --> 00:03:57.220
the company's constrained by the fact that
it has only 15 minutes to process the data

56
00:03:57.220 --> 00:04:02.942
before the next and
computation has to be performed.

57
00:04:02.942 --> 00:04:06.550
That issue is not just
the bigness of the data, but

58
00:04:06.550 --> 00:04:09.610
the strip and
strings of the arrival to output time.

59
00:04:11.520 --> 00:04:18.150
The analytics has value, only if it can be
completed within the life-cycle deadline.

60
00:04:18.150 --> 00:04:22.220
So if we were to design a big data system
for such a company, you would need to

61
00:04:22.220 --> 00:04:26.730
understand how much are the computation
can be executed in parallel And

62
00:04:26.730 --> 00:04:31.180
how many machines with what kind of
capability are required to handle the data

63
00:04:31.180 --> 00:04:35.510
rate and the number and complexity of
the analytical computations needed?