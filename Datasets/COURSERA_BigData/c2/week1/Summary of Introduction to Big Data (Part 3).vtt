WEBVTT

1
00:00:00.450 --> 00:00:04.280
The Hadoop ecosystem frameworks and
applications

2
00:00:04.280 --> 00:00:08.870
provide such functionality through
several overarching themes and goals.

3
00:00:11.000 --> 00:00:15.830
First, they provide scalability
to store large volumes of data

4
00:00:15.830 --> 00:00:17.150
on commodity hardware.

5
00:00:18.760 --> 00:00:21.570
As the number of systems increase, so

6
00:00:21.570 --> 00:00:25.620
does the chance for
crashes and hardware failures.

7
00:00:25.620 --> 00:00:30.390
They handle fault tolerance to
gracefully recover from these problems.

8
00:00:32.650 --> 00:00:37.595
In addition, they are designed to handle
big data capacity and compressing text

9
00:00:37.595 --> 00:00:43.185
files, graphs of social networks,
streaming sensor data and raster images.

10
00:00:43.185 --> 00:00:45.635
We can add more data
types to this variety.

11
00:00:46.695 --> 00:00:48.355
For any given data type,

12
00:00:49.515 --> 00:00:53.635
you can find several projects in
the ecosystem that support it.

13
00:00:55.250 --> 00:00:58.890
Finally, they facilitate
a shared environment,

14
00:00:58.890 --> 00:01:02.050
allow multiple jobs to
execute simultaneously.

15
00:01:04.350 --> 00:01:08.930
Additionally, the Hadoop ecosystem
includes a wide range of open source

16
00:01:08.930 --> 00:01:14.170
projects backed by a large and
active community.

17
00:01:14.170 --> 00:01:18.480
These projects are free to use and
easy to find support for.

18
00:01:20.060 --> 00:01:25.690
Today, there are over 100
Big Data open source projects,

19
00:01:25.690 --> 00:01:31.250
and this continues to grow, many rely
on Hadoop, but some are independent.

20
00:01:33.960 --> 00:01:39.230
Here is, one way of looking at a subset
of tools in the Hadoop Ecosystem.

21
00:01:40.360 --> 00:01:45.240
This layer diagram is organized
vertically based on the interface.

22
00:01:46.390 --> 00:01:52.720
Lower level interfaces to storage and
scheduling on the bottom and

23
00:01:52.720 --> 00:01:56.520
high level languages and
interactivity at the top.

24
00:01:59.080 --> 00:02:05.112
The Hadoop distributed file system,
or HDFS, is the foundation for

25
00:02:05.112 --> 00:02:11.777
many big data frameworks since it
provides scalable and reliable storage.

26
00:02:11.777 --> 00:02:16.915
As the size of your data increases,
you can add commodity

27
00:02:16.915 --> 00:02:21.314
hardware to HDFS to
increase storage capacity.

28
00:02:21.314 --> 00:02:27.107
So it enables what we call
scaling out of your resources.

29
00:02:29.267 --> 00:02:32.960
Hadoop YARN provide
flexible scheduling and

30
00:02:32.960 --> 00:02:36.660
resource management over the HTFS storage.

31
00:02:37.660 --> 00:02:43.955
Yarn is use at Yahoo to schedule
jobs across 40,000 servers.

32
00:02:45.105 --> 00:02:50.045
MapReduce is a programming model
that simplifies parallel computing.

33
00:02:50.045 --> 00:02:54.725
Instead of dealing with the complexities
of synchronization and scheduling you only

34
00:02:54.725 --> 00:03:00.325
need to give MapReduce two
functions map and reduce.

35
00:03:00.325 --> 00:03:02.643
This programming model is so

36
00:03:02.643 --> 00:03:07.962
powerful that Google previously
used it for indexing websites.

37
00:03:07.962 --> 00:03:12.820
MapReduce, only assumes
a limited model to express data.

38
00:03:12.820 --> 00:03:16.429
Hive, and
Pig are two additional programming models,

39
00:03:16.429 --> 00:03:20.421
on top of MapReduce,
to augment data modeling of MapReduce,

40
00:03:20.421 --> 00:03:24.744
with relational algebra and
data flow modeling, respectively.

41
00:03:26.869 --> 00:03:31.682
Hive was created at Facebook to
issue SQL-like queries using

42
00:03:31.682 --> 00:03:34.280
MapReduce on their data in HDFS.

43
00:03:35.410 --> 00:03:40.773
Pig was created at Yahoo to model
dataflow based programs using MapReduce.

44
00:03:40.773 --> 00:03:45.345
Thanks to YARNs ability to
manage resources, not just for

45
00:03:45.345 --> 00:03:48.715
MapReduce but other programming models.

46
00:03:48.715 --> 00:03:52.955
Giraph was built for
processing large scale graphs efficiently.

47
00:03:54.335 --> 00:04:00.676
For example, Facebook uses Giraph to
analyze the social graphs of its users.

48
00:04:00.676 --> 00:04:05.181
Similarly, Storm, Spark and
Flink were built for

49
00:04:05.181 --> 00:04:09.595
real time and
In-memory processing of big data.

50
00:04:09.595 --> 00:04:14.013
On top of the YARN resource scheduler and
HDFS.

51
00:04:14.013 --> 00:04:20.555
In-memory processing is a powerful
way of running big data applications,

52
00:04:20.555 --> 00:04:26.598
even faster, achieving 100x better
performance for some tasks.

53
00:04:26.598 --> 00:04:30.845
Sometimes your data processing or
tasks are not easily or

54
00:04:30.845 --> 00:04:36.332
efficiently represented using the file and
directory model of storage,

55
00:04:36.332 --> 00:04:42.010
examples of this include collections
of key values or large sparse tables.

56
00:04:43.282 --> 00:04:48.859
NoSQL projects such as
Cassandra MongoDB and

57
00:04:48.859 --> 00:04:52.681
HBase handle all these cases.

58
00:04:52.681 --> 00:04:57.396
Cassandra was created at Facebook and
Facebook also use HBase for

59
00:04:57.396 --> 00:04:59.290
its messaging platform.

60
00:05:01.934 --> 00:05:07.562
Finally, running all this tools requires
a centralized management system for

61
00:05:07.562 --> 00:05:12.520
synchronization, configuration And
to ensure high availability.

62
00:05:13.660 --> 00:05:18.828
Zookeeper, created by Yahoo to
wrangle services named after animals,

63
00:05:18.828 --> 00:05:20.614
performs these duties.

64
00:05:23.214 --> 00:05:27.023
Just looking at the small number
of Hadoop stack components,

65
00:05:27.023 --> 00:05:31.700
we can already see that most of them
are dedicated to data modeling.

66
00:05:31.700 --> 00:05:36.010
Management, and
efficient processing of the data.

67
00:05:36.010 --> 00:05:40.610
In the rest of this course,
we will give you fundamental knowledge and

68
00:05:40.610 --> 00:05:46.580
some practical skills on how to start
modeling and managing your data, and

69
00:05:46.580 --> 00:05:52.310
picking the right tools for this activity
from a plethora of big data tools.