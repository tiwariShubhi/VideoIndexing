WEBVTT

1
00:00:02.850 --> 00:00:07.270
In the previous modules, we talked
about data variety and streaming data.

2
00:00:08.410 --> 00:00:13.310
In this module, we'll focus on a central
issue in large scale data processing and

3
00:00:13.310 --> 00:00:19.540
management and that is when should
we use Hadoop or Yarn style system?

4
00:00:19.540 --> 00:00:24.230
And when should we use a database system
that can perform parallel operations?

5
00:00:24.230 --> 00:00:28.840
And then we'll explore how the state
of the art big data management systems

6
00:00:28.840 --> 00:00:31.370
address these issues of volume and
variety.

7
00:00:32.930 --> 00:00:36.510
We start with the problem
of high volume data and

8
00:00:36.510 --> 00:00:38.699
two contrasting approaches for
handling them.

9
00:00:40.150 --> 00:00:42.590
So after this video, you'll be able to

10
00:00:43.650 --> 00:00:47.270
explain the various advantages of
using a DBMS over a file system.

11
00:00:48.560 --> 00:00:52.500
Specify the differences between
parallel and distributed DBMS.

12
00:00:52.500 --> 00:00:57.362
And briefly describe
a MapReduce-style DBMS and

13
00:00:57.362 --> 00:01:01.537
its relationship with the current DBMSs.

14
00:01:01.537 --> 00:01:06.005
In the early days,
when database systems weren't around or

15
00:01:06.005 --> 00:01:11.450
just came in, databases were designed
as a set of application programs.

16
00:01:12.600 --> 00:01:17.390
They were written to handle data that
resided in files in a file system.

17
00:01:18.450 --> 00:01:21.310
However, soon this
approach led to problems.

18
00:01:22.520 --> 00:01:26.560
First, there are multiple file formats.

19
00:01:26.560 --> 00:01:32.370
And often, there was a duplication
of information in different files.

20
00:01:32.370 --> 00:01:36.940
Or the files simply had inconsistent
information that was very hard to

21
00:01:36.940 --> 00:01:41.150
determine, especially when the data was
large and the file content was complex.

22
00:01:43.550 --> 00:01:47.050
Secondly, there wasn't
a uniform way to access data.

23
00:01:48.090 --> 00:01:52.120
Each data access task, like finding
employees in a department sorted by their

24
00:01:52.120 --> 00:01:55.990
salary versus finding
employees in all departments

25
00:01:55.990 --> 00:02:00.060
sorted by their start date needed to
be written as a separate program.

26
00:02:01.160 --> 00:02:04.040
So people ended up writing
different programs for

27
00:02:04.040 --> 00:02:06.200
data access, as well as data update.

28
00:02:08.350 --> 00:02:11.890
A third problem was rooted to
the enforcement of constraints,

29
00:02:11.890 --> 00:02:14.050
often called integrity constraints.

30
00:02:14.050 --> 00:02:20.000
For example, to say something like every
employee has exactly one job title.

31
00:02:20.000 --> 00:02:23.530
One had arrived that condition,
as part of an application program called.

32
00:02:24.700 --> 00:02:27.670
So if you want to change the constraint,
you need to look for

33
00:02:27.670 --> 00:02:29.620
the programs where such
a rule is hard coded.

34
00:02:31.590 --> 00:02:35.300
The fourth problem has to
do with system failures.

35
00:02:35.300 --> 00:02:39.420
Supposed Joe, an employee becomes
the leader of a group and moves in to

36
00:02:39.420 --> 00:02:44.840
the office of the old leader, Ben who has
now become the director of the division.

37
00:02:44.840 --> 00:02:49.788
So we update Joe's details and
move on to update, Ben's new office for

38
00:02:49.788 --> 00:02:51.280
the system crashes.

39
00:02:51.280 --> 00:02:54.501
So the files are incompletely updated and

40
00:02:54.501 --> 00:02:58.096
there is no way to go back,
and start all over.

41
00:02:58.096 --> 00:03:04.138
The term atomicity means that all of
the changes that we need to do for

42
00:03:04.138 --> 00:03:09.657
these promotions must happen altogether,
as a single unit.

43
00:03:09.657 --> 00:03:14.874
They should either fully go through or
not go through at all.

44
00:03:14.874 --> 00:03:21.038
This atomicity is very difficult to handle
when the data reside in one or more files.

45
00:03:23.936 --> 00:03:29.173
So, a prime reason for the transition
to a DBMS is to alleviate these and

46
00:03:29.173 --> 00:03:30.842
other difficulties.

47
00:03:30.842 --> 00:03:34.507
If we look at the current DBMS,
especially relational DBMS,

48
00:03:34.507 --> 00:03:36.836
we will notice a number of advantages.

49
00:03:39.165 --> 00:03:42.710
DBMSs offer query languages,
which are declarative.

50
00:03:44.060 --> 00:03:48.200
Declarative means that we
state what we want to retrieve

51
00:03:48.200 --> 00:03:51.060
without telling the DBMS
how exactly to retrieve it.

52
00:03:52.220 --> 00:03:56.682
In a relational DBMS, we can say,
find the average set of salary of

53
00:03:56.682 --> 00:04:01.800
employees in the R&D division for
every job title and sort from high to low.

54
00:04:01.800 --> 00:04:05.779
We don't have to tell the system how
to group these records by job title or

55
00:04:05.779 --> 00:04:07.879
how to extract just the salary field.

56
00:04:10.073 --> 00:04:14.404
A typical user of a DBMS who issues
queries does not worry about how

57
00:04:14.404 --> 00:04:19.604
the relations are structured or whether
they are located in the same machine,

58
00:04:19.604 --> 00:04:21.835
or spread across five machines.

59
00:04:21.835 --> 00:04:25.835
The goal of data independence is to
isolate the users from the record

60
00:04:25.835 --> 00:04:28.993
layout so long as the logical
definition of the data,

61
00:04:28.993 --> 00:04:33.083
which means the tables and
their attributes are clearly specified.

62
00:04:35.195 --> 00:04:39.754
Now most importantly, relational DBMSs
have developed a very mature and

63
00:04:39.754 --> 00:04:44.389
continually improving methodology of
how to answer a query efficiently,

64
00:04:44.389 --> 00:04:47.255
even when there are a large
number of cables and

65
00:04:47.255 --> 00:04:50.590
the number of records
exceeds hundreds of millions.

66
00:04:51.800 --> 00:04:56.880
From a 2009 account,
EB uses the tera data system with

67
00:04:56.880 --> 00:05:02.280
72 machines to manage approximately
2.4 terabytes of relational data.

68
00:05:03.840 --> 00:05:08.496
These systems have built powerful
data structures, algorithms and

69
00:05:08.496 --> 00:05:12.991
sound principles to determine how
a specific array should be onset

70
00:05:12.991 --> 00:05:18.070
efficiently despite the size of the data
and the complexity of the tables.

71
00:05:18.070 --> 00:05:22.790
Now with any system,
bad things can happen.

72
00:05:22.790 --> 00:05:25.266
Systems fail in the middle
of an operation.

73
00:05:25.266 --> 00:05:29.489
Malicious processes try to get
unauthorized access to data.

74
00:05:29.489 --> 00:05:33.405
One large can often underappreciated
aspect of a DBMS is

75
00:05:33.405 --> 00:05:39.060
the implementation of transaction
safety and failure recovery.

76
00:05:39.060 --> 00:05:40.950
Now, recall our discussion of atomicity.

77
00:05:42.110 --> 00:05:47.082
In databases, a single logical operation
on the data is called a transaction.

78
00:05:47.082 --> 00:05:51.314
For example, a transfer of funds
from one bank account to another,

79
00:05:51.314 --> 00:05:55.253
even involving multiple changes
like debiting one account and

80
00:05:55.253 --> 00:05:58.091
crediting another is a single transaction.

81
00:05:58.091 --> 00:06:02.894
Now, atomicity is one of the four
properties that a transaction should

82
00:06:02.894 --> 00:06:04.350
provide.

83
00:06:04.350 --> 00:06:09.740
The four properties,
collectively called ACID are atomicity,

84
00:06:09.740 --> 00:06:13.370
consistency, isolation and durability.

85
00:06:14.730 --> 00:06:19.370
Consistency means any data
written to the database must be

86
00:06:19.370 --> 00:06:24.301
valid according to all defined
rules including constrains.

87
00:06:24.301 --> 00:06:29.747
The durability property ensures that
once a transaction has been committed,

88
00:06:29.747 --> 00:06:34.709
it will remain so, even in the event
of power loss, crashes or errors.

89
00:06:36.816 --> 00:06:40.573
The isolation property comes
in the context of concurrency,

90
00:06:40.573 --> 00:06:44.920
which refers to multiple people
updating a database simultaneously.

91
00:06:45.970 --> 00:06:50.640
To understand concurrency, think of
an airline or a railway reservation system

92
00:06:50.640 --> 00:06:54.330
where hundreds and thousands of
people are buying, cancelling and

93
00:06:54.330 --> 00:06:57.110
changing their reservations and
tickets all at the same time.

94
00:06:58.360 --> 00:07:02.710
The DBMS must be sure that
a ticket should no be sold twice.

95
00:07:02.710 --> 00:07:06.920
Or if one person is in the middle
of buying the last ticket,

96
00:07:06.920 --> 00:07:09.840
another person does not see
that ticket as available.

97
00:07:11.380 --> 00:07:15.480
These are guaranteed by
the isolation property that says,

98
00:07:15.480 --> 00:07:19.340
not withstanding the number of people
accessing the system at the same time.

99
00:07:19.340 --> 00:07:26.310
The transactions must happen as if they're
done serially, that is one after another.

100
00:07:26.310 --> 00:07:33.949
Providing these capabilities is
an important part of the M in DBMS.

101
00:07:33.949 --> 00:07:38.390
So next, we consider how traditional
databases handle large data volumes.

102
00:07:39.920 --> 00:07:44.093
The classical way in which DBMSs
have handled the issue of large

103
00:07:44.093 --> 00:07:48.120
volumes is by created parallel and
distributed databases.

104
00:07:48.120 --> 00:07:52.244
In a parallel database, for
example, parallel Oracle,

105
00:07:52.244 --> 00:07:54.357
parallel DB2 or post SQL XE.

106
00:07:54.357 --> 00:07:59.966
The tables are spread across multiple
machines and operations like selection,

107
00:07:59.966 --> 00:08:03.853
and join use parallel algorithms
to be more efficient.

108
00:08:03.853 --> 00:08:07.905
These systems also allow a user
to create a replication.

109
00:08:07.905 --> 00:08:10.400
That is multiple copies of tables.

110
00:08:10.400 --> 00:08:13.420
Thus, introducing data redundancy, so

111
00:08:13.420 --> 00:08:18.180
that failure on replica can be
compensated for by using another.

112
00:08:19.740 --> 00:08:23.678
Further, it replicates in
sync with each other and

113
00:08:23.678 --> 00:08:27.260
a query can result into
any of the replicates.

114
00:08:27.260 --> 00:08:32.021
This increases the number of simultaneous
that is conquer into queries

115
00:08:32.021 --> 00:08:34.332
that can be handled by the system.

116
00:08:34.332 --> 00:08:39.942
In contrast, a distributed DBMS, which
we'll not discuss in detail in this course

117
00:08:39.942 --> 00:08:46.200
is a network of independently running
DBMSs that communicate with each other.

118
00:08:46.200 --> 00:08:51.275
In this case, one component knows some
part of the schema of it is neighboring

119
00:08:51.275 --> 00:08:55.670
DBMS and can pass a query or part of
a query to the neighbor when needed.

120
00:08:57.520 --> 00:09:01.360
So the important takeaway issue here is,

121
00:09:01.360 --> 00:09:04.890
are all of these facilities
offered by a DBMS important for

122
00:09:04.890 --> 00:09:07.430
the big data application that
you are planning to build?

123
00:09:08.590 --> 00:09:11.131
And the answer in many
cases can be negative.

124
00:09:11.131 --> 00:09:16.304
However, if these issues are important,
then the database management

125
00:09:16.304 --> 00:09:20.726
systems may offer a viable option for
a big data application.

126
00:09:23.034 --> 00:09:27.726
Now, let's take a little more time to
address an issue that's often discussed in

127
00:09:27.726 --> 00:09:28.819
the big data word.

128
00:09:28.819 --> 00:09:32.300
The question is if DBMSs are so powerful,

129
00:09:32.300 --> 00:09:37.178
why do we see the emergence
of MapReduce-style Systems?

130
00:09:37.178 --> 00:09:41.800
Unfortunately, the answer to this
question is not straightforward.

131
00:09:43.660 --> 00:09:49.180
For a long while now, DBMSs have
effectively used parallelism, specifically

132
00:09:49.180 --> 00:09:54.030
parallel databases in addition to
replication would also create partitions.

133
00:09:55.220 --> 00:10:00.017
So that different parts of a logical
table can physically reside on different

134
00:10:00.017 --> 00:10:05.403
machines,, then different parts of a query
can access the partitions in parallel and

135
00:10:05.403 --> 00:10:07.501
speed up creative performance.

136
00:10:07.501 --> 00:10:11.753
Now these algorithms not only improve
the operating efficiency, but

137
00:10:11.753 --> 00:10:16.882
simultaneously optimize algorithms to
take into account the communication cost.

138
00:10:16.882 --> 00:10:21.511
That is the time needed to
exchange data between machines.

139
00:10:21.511 --> 00:10:28.390
However, classical parallel DBMSs did
not take into account machine failure.

140
00:10:29.700 --> 00:10:35.070
And in contrast, MapReduce was
originally developed not for storage and

141
00:10:35.070 --> 00:10:38.850
retrieval, but for distributive
processing of large amounts of data.

142
00:10:39.890 --> 00:10:44.783
Specifically, its goal was to support
complex custom computations that could

143
00:10:44.783 --> 00:10:47.569
be performed efficiently on many machines.

144
00:10:47.569 --> 00:10:50.259
So in a MapReduce or MR setting,

145
00:10:50.259 --> 00:10:54.357
the number of machines
could go up to thousands.

146
00:10:54.357 --> 00:10:58.710
Now since MR implementations were
done over Hadoop file systems,

147
00:10:58.710 --> 00:11:02.760
issues like node failure were
automatically accounted for.

148
00:11:04.492 --> 00:11:09.017
So MR effectively used in complex
applications like data mining or

149
00:11:09.017 --> 00:11:13.703
data clustering, and these algorithms
are often very complex, and

150
00:11:13.703 --> 00:11:17.202
typically require problem
specific techniques.

151
00:11:17.202 --> 00:11:20.554
Very often,
these algorithms have multiple stages.

152
00:11:20.554 --> 00:11:25.891
That is the output from one processing
stage is the input to the next.

153
00:11:25.891 --> 00:11:29.519
It is difficult to develop these
multistage algorithms in a standard

154
00:11:29.519 --> 00:11:30.650
relational system.

155
00:11:31.990 --> 00:11:36.080
But since these were genetic operations,
many of them were designed to work

156
00:11:36.080 --> 00:11:39.880
with unstructured data like text and
nonstandard custom data formats.

157
00:11:41.730 --> 00:11:46.139
Now, it's now amply clear that this
mixture of data management requirements

158
00:11:46.139 --> 00:11:50.818
and data processing analysis requirements
have created an interesting tension in

159
00:11:50.818 --> 00:11:52.439
the data management world.

160
00:11:52.439 --> 00:11:56.030
Just look at a few of
these tension points.

161
00:11:56.030 --> 00:12:00.990
Now, DBMSs perform storage and
retrieval operations very efficiently.

162
00:12:02.210 --> 00:12:05.170
But first,
the data must be loaded into the DBMS.

163
00:12:06.230 --> 00:12:07.490
So, how much time does loading take?

164
00:12:08.690 --> 00:12:12.900
In one study,
scientists use two CVS files.

165
00:12:12.900 --> 00:12:17.850
One had 92 attributes with
about 165 million tuples for

166
00:12:17.850 --> 00:12:19.890
a total size of 85 gigabytes.

167
00:12:21.190 --> 00:12:25.710
And the other had 227 attributes
with 5 million tuples for

168
00:12:25.710 --> 00:12:27.390
a total size of 5 gigabytes.

169
00:12:28.850 --> 00:12:35.610
The time to load and index this data in
MySQL and PostgreSQL, took 15 hours each.

170
00:12:36.690 --> 00:12:41.240
In a commercial database running on
three machines, it took two hours.

171
00:12:41.240 --> 00:12:44.980
Now there are applications like
the quantities in the case we discussed

172
00:12:44.980 --> 00:12:49.180
earlier where this kind of loading
time is simply not acceptable,

173
00:12:49.180 --> 00:12:51.880
because the analysis on
the data must be performed

174
00:12:51.880 --> 00:12:54.070
within a given time limit
after it's arrival.

175
00:12:56.600 --> 00:12:59.780
A second problem faced by
some application is that for

176
00:12:59.780 --> 00:13:03.140
them, the DBMSs offer
too much functionality.

177
00:13:04.160 --> 00:13:08.460
For example, think of an application
that only looks at the price of an item

178
00:13:08.460 --> 00:13:10.700
if you provide it with a product name or
product code.

179
00:13:12.210 --> 00:13:14.850
The number of products it serves
is let's say, 250 million.

180
00:13:16.170 --> 00:13:19.899
This lookup operation happens
only on a single table and

181
00:13:19.899 --> 00:13:23.164
does not mean anything
more complex like a join.

182
00:13:23.164 --> 00:13:27.963
Further, consider that while there
are several hundred thousand customers

183
00:13:27.963 --> 00:13:31.810
who access this data,
none of them really update the tables.

184
00:13:31.810 --> 00:13:36.685
So, do we need a full function DBMS for
this read-only application?

185
00:13:36.685 --> 00:13:41.175
Or can we get a simpler solution which
can use a cluster of machines, but

186
00:13:41.175 --> 00:13:44.275
does not provide all the wonderful
guarantees that a DBMS provides?

187
00:13:46.600 --> 00:13:51.940
At the other end of the spectrum,
there is an emerging class of optimization

188
00:13:51.940 --> 00:13:56.590
that meets all the nice transactional
guarantees that a DBMS provides.

189
00:13:56.590 --> 00:14:00.500
And at the same time, meets the support
for efficient analytical operations.

190
00:14:01.720 --> 00:14:05.741
These are often required for
systems like Real-Time Decision Support.

191
00:14:05.741 --> 00:14:10.141
That will accept real-time data like
customer purchases on a newly released

192
00:14:10.141 --> 00:14:13.188
product will perform some
statistical analysis, so

193
00:14:13.188 --> 00:14:15.365
that it can determine buying trends.

194
00:14:15.365 --> 00:14:17.969
And then decide whether in real-time,

195
00:14:17.969 --> 00:14:21.040
a discount can be offered
to this customer now.

196
00:14:23.860 --> 00:14:28.368
It turns out that the combination
of traditional requirements and

197
00:14:28.368 --> 00:14:32.072
new requirements is leading
to new capabilities, and

198
00:14:32.072 --> 00:14:35.465
products in the big data
management technology.

199
00:14:35.465 --> 00:14:40.283
On the one hand, DBMS technologies
are creating new techniques that make

200
00:14:40.283 --> 00:14:43.164
use of MapReduce-style data processing.

201
00:14:43.164 --> 00:14:46.473
Many of them are being
developed to run on HDFS and

202
00:14:46.473 --> 00:14:50.279
take advantage of his data
replication capabilities.

203
00:14:50.279 --> 00:14:54.423
More strikingly, DBMSs are beginning
to have a side door for

204
00:14:54.423 --> 00:14:58.405
a user to perform and
MR-style operation on HDFS files and

205
00:14:58.405 --> 00:15:02.399
exchange data between the Hadoop
subsystem and the DBMS.

206
00:15:02.399 --> 00:15:06.862
Thus, giving the user the flexibility
to use both forms of data processing.

207
00:15:09.674 --> 00:15:12.562
It has now been recognized
that a simple map and

208
00:15:12.562 --> 00:15:17.155
reduce operations are not sufficient for
many data operations leading to

209
00:15:17.155 --> 00:15:22.740
a significant expansion in the number
of operations in the MR ecosystems.

210
00:15:22.740 --> 00:15:26.510
For example,
Spark has several kinds of join and

211
00:15:26.510 --> 00:15:29.350
data grouping operations in
addition to map and reduce.

212
00:15:31.650 --> 00:15:34.900
Sound DBMSs are making use of large

213
00:15:34.900 --> 00:15:38.190
distributed memory management
operations to accept streaming data.

214
00:15:39.400 --> 00:15:43.510
These systems are designed with the idea
that the analysis they need to perform on

215
00:15:43.510 --> 00:15:45.400
the data are known before.

216
00:15:45.400 --> 00:15:49.860
And as new data records arrive,
they keep a record of the data

217
00:15:49.860 --> 00:15:53.820
in the memory long enough to finish
the computation needed on that record.

218
00:15:55.310 --> 00:15:57.610
And finally, computer scientists and

219
00:15:57.610 --> 00:16:00.770
data scientists are working
towards new solutions

220
00:16:00.770 --> 00:16:05.770
where large scale distributed algorithms
are beginning to emerge to solve different

221
00:16:05.770 --> 00:16:09.280
kinds of analytics problems like
finding dense regions of a graph.

222
00:16:10.610 --> 00:16:14.210
These algorithms use
a MR-style computing and

223
00:16:14.210 --> 00:16:17.880
are becoming a part of a new
generation of DBMS products

224
00:16:17.880 --> 00:16:20.860
that invoke these algorithms
from inside the database system.

225
00:16:21.890 --> 00:16:25.720
In the next video, we'll take a look at
some of the modern day data management

226
00:16:25.720 --> 00:16:28.030
systems that have some
of these capabilities.