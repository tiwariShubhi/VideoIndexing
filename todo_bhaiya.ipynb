{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"todo_bhaiya.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python2","display_name":"Python 2"}},"cells":[{"metadata":{"id":"CRbd_zVFNrTJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":201},"outputId":"0906aa6a-cbb0-4d23-e5d1-1d89bb7e5ac0","executionInfo":{"status":"ok","timestamp":1542823364850,"user_tz":-330,"elapsed":6953,"user":{"displayName":"KUNAL","photoUrl":"","userId":"14770010390584988954"}}},"cell_type":"code","source":["!pip install sklearn\n"],"execution_count":39,"outputs":[{"output_type":"stream","text":["Collecting sklearn\n","  Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python2.7/dist-packages (from sklearn) (0.19.2)\n","Building wheels for collected packages: sklearn\n","  Running setup.py bdist_wheel for sklearn ... \u001b[?25l-\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/76/03/bb/589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074\n","Successfully built sklearn\n","Installing collected packages: sklearn\n","Successfully installed sklearn-0.0\n"],"name":"stdout"}]},{"metadata":{"id":"HCJApSJuTJ-P","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"90839e59-5704-4b7e-fca0-8ddef5df5509","executionInfo":{"status":"ok","timestamp":1542826412966,"user_tz":-330,"elapsed":1177,"user":{"displayName":"KUNAL","photoUrl":"","userId":"14770010390584988954"}}},"cell_type":"code","source":["import nltk\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn.decomposition import NMF, LatentDirichletAllocation\n","nltk.download('wordnet')\n","from nltk.corpus import wordnet as wn\n","def get_lemma(word):\n","    lemma = wn.morphy(word)\n","    if lemma is None:\n","        return word\n","    else:\n","        return lemma\n","    \n","from nltk.stem.wordnet import WordNetLemmatizer\n","def get_lemma2(word):\n","    return WordNetLemmatizer().lemmatize(word)\n","\n","n_samples = 2000\n","n_features = 1000\n","n_components = 3\n","n_top_words = 10"],"execution_count":55,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"}]},{"metadata":{"id":"WGkOPIWqTNcf","colab_type":"code","colab":{}},"cell_type":"code","source":["nltk.download('stopwords')\n","en_stop = set(nltk.corpus.stopwords.words('english'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tPFvif_mTTl-","colab_type":"code","colab":{}},"cell_type":"code","source":["def prepare_text_for_lda(text):\n","    tokens=text.split()\n","    tokens = [token for token in tokens if len(token) > 4]\n","    tokens = [token for token in tokens if token not in en_stop]\n","    tokens = [get_lemma(token) for token in tokens]\n","    return tokens"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0ZtghWOEcRxP","colab_type":"code","colab":{}},"cell_type":"code","source":["def print_top_words(model, feature_names, n_top_words):\n","    for topic_idx, topic in enumerate(model.components_):\n","        message = \"Topic #%d: \" % topic_idx\n","        message += \" \".join([feature_names[i]\n","                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n","        print(message)\n","        \n","    print()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"J7dd4375Yo8v","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"vTAITK4pOPm-","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"uFsAJe8EOOz_","colab_type":"code","colab":{}},"cell_type":"code","source":["import pysrt\n","\n","subs = pysrt.open('BigData1.srt')\n","subs1= pysrt.open('002_1.1-en.srt')\n","subtitles=[]\n","\n","mega_subs=[]\n","count=0\n","for i in subs:\n","  subtitles.append(i.text)\n","  count=count+1\n","  if(count%50==0):\n","    mega_subs.append(subtitles)\n","    subtitles=[]\n","    count=0\n","if(count!=0):\n","  mega_subs.append(subtitles)\n","    \n"," \n","s = ' '.join(subtitles)\n","\n","\n","\n","subtitles1=[]\n","for i in subs1:\n","  subtitles1.append(i.text)\n"," \n","s1 = ' '.join(subtitles1)\n","\n","\n","\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hz04pwLEUSlA","colab_type":"code","colab":{}},"cell_type":"code","source":["tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n","                                   max_features=n_features,\n","                                   stop_words='english')\n","\n","tfidf = tfidf_vectorizer.fit_transform(mega_subs[0])\n","\n","\n","# Use tf (raw term count) features for LDA.\n","print(\"Extracting tf features for LDA...\")\n","tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n","                                max_features=n_features,\n","                                stop_words='english')\n","\n","tf = tf_vectorizer.fit_transform(mega_subs[0])\n","\n","print()\n","\n","# Fit the NMF model\n","print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n","      \"n_samples=%d and n_features=%d...\"\n","      % (n_samples, n_features))\n","\n","nmf = NMF(n_components=n_components, random_state=1,\n","          alpha=.1, l1_ratio=.5).fit(tfidf)\n","\n","\n","print(\"\\nTopics in NMF model (Frobenius norm):\")\n","tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n","print_top_words(nmf, tfidf_feature_names, n_top_words)\n","\n","# Fit the NMF model\n","print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n","      \"tf-idf features, n_samples=%d and n_features=%d...\"\n","      % (n_samples, n_features))\n","\n","nmf = NMF(n_components=n_components, random_state=1,\n","          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n","          l1_ratio=.5).fit(tfidf)\n","\n","\n","print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n","tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n","print_top_words(nmf, tfidf_feature_names, n_top_words)\n","\n","print(\"Fitting LDA models with tf features, \"\n","      \"n_samples=%d and n_features=%d...\"\n","      % (n_samples, n_features))\n","lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n","                                learning_method='online',\n","                                learning_offset=50.,\n","                                random_state=0)\n","\n","lda.fit(tf)\n","\n","\n","print(\"\\nTopics in LDA model:\")\n","tf_feature_names = tf_vectorizer.get_feature_names()\n","print_top_words(lda, tf_feature_names, n_top_words)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"knzdNoVTiC-K","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}