hi in this segment I'm going to

introduce the machine learning sequence

model approach to named entity

recognition and other kinds of

information extraction tasks I'm going

to say a little bit about the structure

of how you approach things and the

features they use for that task and then

in the next segment I'm going to talk

about the details of using Mac cementery

models as sequence classifiers so if

we're going to use a sequence model for

named entity recognition we need

supervised training data what that means

is we have examples of training

documents where the words are labeled

for what their entity class is so the

steps that we're going to have to go

through is first of all collecting a

representative set of training documents

that contain entities that were

interested in and the context we're

interest in them and then we're going to

go through each word and label each

token for its entity class or if it's

not if any entity class you'll be

labeled other which is normally denoted

O then on the machine learning

classifier side we're going to design

appropriate feature extractors for

identifying words of the classes and

then we're going to train a sequence

classifier whose job is that do the best

job possible it can of labeling each

token with this entity class or other

and so this is the part we'll talk about

in the next chunk um when we then want

to run the classifier on actual

documents to do stuff that's often

referred to as testing but maybe we

should just call it classifying we then

have the Train model so we get a set of

testing documents we have model and we

just run the sequence model in France

each document and it will be able to

tell us the highest probability label

for each token and we use those labels

to output recognized entities this all

probably becomes more concrete if I show

you an example

okay so here is our document which is a

sequence of words and so the labeling

which is done by hand for the training

documents and automatically by the train

model at classification time is putting

on each word a label which is

representing either its entity type or

its another so in this column I'm

showing what gets called IO encoding

which is inside short for inside outside

and it's the most obvious and natural

thing for you for someone to come up

with for doing named entity recognition

or sequence labeling so we're taking

Fred and labeling him as a person we're

then taking showed and labeling it as an

other then Sue is a person named monk U

is a person named Huang it's part of a

person name and then these next three

tokens are all other of other but

there's a catch in that labeling scheme

which is actually this is one person's

name and then this is another person's

name and we can't represent that in io

encoding we can only say that will take

maximal sequences of entities of the

same class and call them the name of an

entity so to recognize two entities here

whereas really there are three and so

there's a technical way to fix that

problem and that's known as IOB encoding

and the way that you do IOB encoding is

you're prefixing each class with a b if

it's the beginning of an entity of that

class or an i if it's a continuation of

an entity of that class so then we can

see here's one person named here's a

second person named and we know it stops

here because we have another be right

there and then we have a third person

named which is two tokens long so the

IOP encoding

efficient and solves this problem and

comes in a bit of a cost because if we

suppose that we have C entity classes

for IO encoding you need to have C plus

one labels

whereas for IO be encoding you have to

have two C plus one labels and the plus

ones coming from the other for which you

don't need to distinguish the B I even

our IOB encoding well that seems a

fairly small difference but as well see

when we look at sequence models if you

have any sequence information you're

raising this to the order of the

sequence model so you're at a minimum

squaring this and so that means that you

are ending up with things having a

considerably slower runtime with the IOB

encoding and so well in some sense this

is clearly the right thing to do since

it's not a deficient representation and

a lot of people actually do do this I'll

reveal a little secret here which is in

the Stanford name dirty recognizer we

actually use IO encoding and the reasons

why we do that is it runs a lot faster

and it turns out the slight limits in

the representation aren't really a

problem in practice and there are two

reasons that it's not a problem in

practice one is situations like this

very very rarely occur while you quite

often get entities next to each other

they're most commonly entities of

different classes but the IO encoding

has no problem if it's a person followed

by an organization then it can see the

boundary perfectly well you only have a

problem when you have two entities of

the same class and that happens pretty

rarely

now it does happen occasionally but it

turns out that in practice systems

trained with IO be encoding very rarely

yet

this right even though they are capable

of representing it that because of the

fact that having more classes worsens

the sparseness and because of the fact

that's simply hard to tell where one

name ends and the next one begins what

you find is in practice i OB train

systems given data like this to tag will

nearly always tag them as one person

name of three tokens which is exactly

the same classification we extract and

practice with the io classification and

so in practice using this works fine

despite a slight ugliness and so we use

it ok let me now just say a moment about

what features we put in to sequence

labeling for information extraction or

named ad recognition problems the

obvious starting point is we put in

features for the words so we put in a

feature for the current word in each

class and it essentially works like a

learn dictionary of words in each class

and then we also put in features for the

previous and next words which give us

some context features we know that for

things like words after a tour tour

might be more likely to be locations if

we have used other kinds of linguistic

processing and we know part of speech

tags they'll often also be useful

features and we might also just throw

them in for the current words part of

speech tag the next words part of speech

tag and the previous words part of

speech tag now all of these features are

just looking at the observed data and

they could be just done in a straight

classifier built for each word you only

have a sequence model when you also put

in the label context and so that's when

you're saying that John Smith if you

think that John Smith if you think that

John is a person name then it's quite

likely that the next token is also a

person named because person names are

commonly more than one token long and so

you can have features that model this

label sequence and as having features of

this tie

that are definitional for making

something a sequence model but before we

get into the details of sequence models

I'd just like to mention a couple of

other kinds of features that are back at

this level that are a little bit more

interesting than just using the words as

they are and these features are really

useful for having the models generalized

better and work better on rare and

unseen words so one of those kinds of

features is character sub sequences so

character sub sequences of word can be

very useful class of acute ory features

and I'm just going to show a neat

example of this that was done by a

student of mine Joseph Smarr years ago

so it was classifying entities as one of

these five classes with drug company

movie place person and what he asked was

how indicative our particular character

sub sequences and here are just a couple

of examples from his data so here are

some of the words he was trying to

classify just to give you an idea well

they weren't only words actually they

could be multi word sequences like this

one here but now it's asking questions

about particular character sub sequences

suppose you know that the term to be

classified has oxa in it well it turns

out this X letter is a really strong

marker of drug names right that's all of

the drug names like xanax and things

like that the people have these kind of

particular semantic sound patterns that

they name drugs after and in this data

at least if you saw the letter O X a in

a term 100 percent of the time it was a

drug name so that's why it's all

powerful here so that was a categorical

indicator feature so that's an extreme

case and most of them aren't like that

but there are lots of other good

features here's another very good

feature from his data and for these

terms if you saw a colon in the term

that was pretty much a giveaway that it

was a movie name there are only a few

exceptions up here so that one's an

almost

categorical feature he is perhaps a more

typical example but a place where

character sub sequences are still very

useful so that if a word ends and filled

I mean it could be anything just about

it couldn't be a drug name in this data

but it could be a place it could be a

person so David Copperfield it could be

the name of a movie because there's a

movie David Copperfield it could be the

name of a company but because of the

semantic origins of this field ending if

it ends and field is overwhelmingly a

location over two-thirds of the time so

ending and field is still a very good

indicative feature and so in this way

character sub sing string features are

very useful here's one other kind of

feature that turns out to be quite

complimentary to character sub sequences

so this is what I call words shape

sequences and it's an idea that first

first suggested by Michael Collins as

far as I'm aware and the idea was that

you map words onto a clip equivalence

classes which are a simplified

representation that encodes attributes

such as something about the length of

the words something about its

capitalization use of numerals internal

punctuation and things like that there

are very many particular ways that you

can do it but this gives you an example

of the general idea of how to do it done

on biological entities so precisely in

this system the way it was defined was

that any capital letter a B etc were

being mapped to a capital X any little

wet and letter was being mapped to a

little X any number was being mapped to

a little D and symbols like a hyphen or

a colon or a period were being mapped to

themselves and then there was one more

trick that was being used here which was

a way of shortening adjacent letters so

the idea was that the beginnings and

ends of words are important but maybe we

can pay less attention to stuff in the

middle so how these

were worked out is the first two letters

and the last two letters are being

encoded according to this system that

I've drawn over here and so that means

if the word is four characters or less

so that's picking up this length idea

you can see its length in the encoding

so if we had another example and

something was just XP that have been

being coded as xx but if a word is

longer than four characters

you're not encoding everything about the

remaining characters for all the

characters in the middle you're just

saying what is the set of these

character types that occur so in this

set here there's a lowercase letter and

there's a dash symbol and then that set

is being written out in a canonicalized

order which is always the same so

actually the dash comes before the X and

that's giving this form a lot of details

there a lot of different ways you could

imagine doing it but the important part

is that in this way we're defining this

word shape equivalence class for each

word and so these are much denser than

the rare individual words but can be

kind of good predictors of their

behavior because they are according

important attributes like is there a

digit is it capitalized is that all caps

or having funny capitals at the end of

it and so those can be useful features

for classification okay so I hope that

introduced the problem of sequence

models and some of the features we use

for NER and now we'll get back into a

bit of the details of building a maxim

entry sequence model
