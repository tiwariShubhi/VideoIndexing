hello again in this segment we're going

to talk about the inverted index and how

it's constructed an inverted index is

the key data structure that underlies

all modern information retrieval systems

from systems running on a single laptop

to those running in the biggest

commercial search engines an inverted

index is a data structure that exploits

the sparsity of the term document matrix

that we talked about in the preceding

segment and allows for very inefficient

sorry and allows for very efficient

retrieval it's essentially without peer

as the data structure used in

information retrieval systems so let me

go through what's in an inverted index

so for each term each word we must store

a list of all the documents that contain

the word let's identify each document by

a doc ID which is just a document serial

number so you would think of us starting

with the first document called 1 then 2

3 etc and then the question is what data

structure should we use I mean one idea

might be to use fixed arrays like the

vectors there in the term document

matrix but that's very inefficient

because while for some words they'll

appear in a lot of documents other words

will appear in very few documents

moreover there are perhaps other

problems if we think about a dynamic

index where some documents are added

later on that then we have or documents

are changed then we'll have difficult

things in adjusting our vector sizes for

these reasons one way or another we need

to use variable size lists the store the

documents and which a word occurs and in

standard information retrieval

terminology these lists are called

postings lists postings lists

traditionally we usually stored on disk

but that may not be the case for now for

big search engines and if your story

puts things listed on disk the right way

to store them is as one continuous run

of postings because that gives the most

efficient method of then being able to

load them off disk back into memory when

you're interested in the post

from a particular word in memory

postings lists can be represented as a

data structure like a linked list or

variable lengths arrays with some

obvious trade-offs in the size versus

the ease of insertion so the data

structure we end up with friend inferred

index is like the one that I'm showing

here so we have the terms that are in

any of our documents and then for each

term we've got then got a pointer to a

postings list that is then giving the

different documents which are described

by their document ID in which it occurs

okay so one occurrence of a word

document pair is referred to as a

posting and the sum of all of the

postings lists are then referred to as

the postings and so overall then we have

the parts of on the left-hand side we

have the dictionary and then on the

right-hand side we have the postings and

these and a property of the postings is

that they're sorted by document ID and

very soon now we'll explain why that's

essential so these two data structures

the dictionary and the postings have

somewhat different statuses because in

global size the dictionary is relatively

small but it's normally essential that

it's in memory whereas the postings are

large but at least for something like a

small-scale enterprise search engine

these will normally be stored on disk

let me move now to how an inverted index

is constructed so the starting off point

is we have a bunch of documents to be

indexed each of those documents we'll

think of as being a sequence of

characters more assuming that we've

already dealt with pets by someone

else's software conversion from PDF and

Microsoft Word documents and things like

that so then we're going to go through

first some pre-processing steps so we

need a tokenizer

that turns the document into a sequence

of word total

which are the basic units of indexing

but we often don't index exactly the

words they're contained in the document

there might be various linguistic

modules than some way modify the tokens

to put them into some kind of canonical

form so for instance here we're saying

that for friends here it's being both

lower cased and as being stemmed to

remove the s plural ending ok so then

it's those modified tokens which will be

fed to the indexer which is the thing

that builds the inverted index that I

was just talking about so here's the

inversion index and it's this step here

of the index so that is the main thing

that I want to talk about but let me

first just briefly mention those initial

stages of text processing so in just a

fraction more detail the things that

happen in those initial stages is

firstly tokenization so that's just how

we decide to cut the character sequence

into word tokens and there are various

issues there there are punctuation that

come up against words how to treat

possessives

hyphenated terms and all that kind of

stuff that we can talk about in more

detail then normalization is this issue

that while certain things like USA with

and without the dots you probably want

to treat us the same term and map both

the text and the query terms to the same

form so that they will match you might

want to do other kinds of mapping such

as stemming so that authorized and

authorization are both being mapped to

the same stem so that they

straightforwardly match in a query and

finally you may not want to index at all

the most common words traditionally many

search engines have left out very common

words like that are too enough from the

indexing it's not clear that in the

modern world when our amount of storage

is so vast that that's such a good idea

because there are queries that you might

like to do such as for the song to be or

not to be where you really need the stop

words and it turns out that with modern

indexes it's not that inefficient to

store them

okay now let's go through in detail how

the indexer goes from the sequence of

paps normalized tokens to building an

inverted index so for this example we're

assuming we have two documents doc one

and doc two here so there are key

sequence of steps that we go through so

our import is that we have the sequence

of tokens of the first document in the

order that they come in the text and the

sequence of dot tokens of the second

document in the order in which they come

in the text so the first step is that we

do a sort and restore as the primary key

by the terms putting them in

alphabetical order so here we have this

alphabetical list of terms and if we

have the same term appearing in multiple

documents we do a secondary sort by the

document ID so the word Caesar appears

twice once in document ID one and twice

in document ID 2 and we're sorting it

secondarily by document ID and so that's

a core and expensive indexing step once

we've got that for her what we then do

is essentially a consolidation of what

we found over here on the right so we

take that here it is again and multiple

entries in a single document are merged

so that's the two instances of Caesar

and they're just treated as one and then

we also merge all instances of a

particular term and so then we represent

that as over here so we say we have the

dictionary entry Caesar we record its

total frequency in the collection I'll

come back to that a bit later and then

we build for it the postings list which

is the list of documents in which it

occurs and straightforwardly because of

a consequence of our sort in the

previous step that this postings list is

itself going to be sorted by the

document ID

so I'm thinking about the size of an

inverted index we can think for a minute

about where do we pay in storage so we

pay some amount for the list of terms

and there counts but the number of terms

will be relatively modest now example

before hand there are 500,000 terms we

pay for a pointer tooth that identifies

where the postings lists are but again

that's of the order of 500,000 things

and then we pay for the actual postings

list themselves and these postings lists

by far the biggest part but even then

they're bounded by the size of the

number of tokens in the collection so an

example in our example before of the

million documents of average length a

thousand words we're still less than 1

billion items there and so storage is

manageable so when we are actually

building an efficient ir system

implementation we think further about

these questions we think about how can

we make the indexes of sufficient as

possible for retrieval and how can we

minimize the storage on both sides of

this both on this side and this side in

terms of various sorts of compression

we're not going to get into the details

of that now but what I hope you can

start to see is the beam verted index

gives an efficient basis on which to do

retrieval operations and that's

something that we'll talk about in more

detail in the next segment but at any

rate now you know the underlying data

structure it's really not that complex

that underlies all modern information

retrieval systems
