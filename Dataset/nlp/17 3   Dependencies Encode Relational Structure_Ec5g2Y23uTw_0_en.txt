in this segment I'm going to show you

that dependency syntax is a very natural

representation for relation extraction

applications one domain in which a lot

of work has been done in relation

extraction is in the biomedical text

domain so here for example we have the

sentence the results demonstrated that

Casey interacts with mclee with SAS a ka

and KB and what we'd like to get out of

that is a protein interaction event so

here's the interacts that is sort of

indicates the relation and these are the

proteins involved and there are a bunch

of other proteins involved as as well

well the point to get out of here is

that if we can have this kind of

dependency syntax then it's very easy

starting from here to follow along the

arguments of the subject and the

preposition with and to easily see the

relation that we'd like to get out and

if we were just a little bit cleverer we

can then also follow along the

conjunction relations and see that KC is

also interacting with these other two

proteins and that's something that a lot

of people have worked on in particular

one representation that's been widely

used for relation extraction

applications in biomedicine is the

Stanford dependencies representation so

the basic form of this representation is

as a projective dependency tree and it's

designed that way so it could be easily

generated by post-processing of phrase

structure trees so if you have a notion

of headedness in a free structure tree

the Stanford dependencies software

provides a set of matching pattern rules

that will then type the dependency

relations and give you out a Stanford

dependency tree but Stanford

dependencies can also be and now

increasingly our general generated

directly by dependency parsers such as

the Mont parser that we looked at

recently okay so this is roughly what

the representation looks like so it's

just as we saw before with

the words connected by type dependency

arcs but something that has been

explored in the Stanford dependencies

framework is starting from that basic

dependencies representation let's make

some changes to it

to facilitate relation extraction

applications and the idea here is to

emphasize the relationships between

content words they're useful for

relation extraction applications let me

give a couple of examples so one example

is that commonly you'll have a Content

word like based and where the company

here is based in Los Angeles and it's

separated by this preposition in a

function word and you can think of these

function words is really functioning

like case markers in a lot of other

languages so it seemed more useful if we

directly connected based LA and we

introduced a relationship of prep in and

so that's what we do and we simplify the

structure but there are some other

places too in which we can do a better

job and representing the semantics with

some modifications of the graph

structure and so a particular place of

that is these coordination relationships

so we very directly got here that bill

makes products but we'd also like to get

out the bill distributes products when

one way we could do that is by

recognizing this and relationship and

saying okay well that means that bill

should also be the subject of

distributing and what they distribute is

products and similarly down here we can

recognize that they're computer products

as well as electronic products so we can

make those changes to to the graph and

get a kind of reduced graph

representation

now once you do this there are some

things that are not as simple in

particular if you look at this structure

it's no longer a dependency tree because

we have multiple arcs pointing at this

node and multiple arcs pointing at this

node but on the other hand the relations

that we would like to extract and

represented much more directly and let

me just show you one graph that gives an

indication of this so this was a graph

that was originally put together by

Gerry Boehner at al who were the team

that won the bio NLP 2009 shared task on

relation extraction using as the

representational substrate stanford

dependencies and what we wanted to

illustrate with this graph is how much

more effective dependency structures

were at linking up the words that you

wanted to extract in a relation than

simply looking for words in the linear

context so here what we have is that

this is the distance which can be

measured either by just counting words

to the left or right or by counting the

number of dependency arcs that you have

to follow and this is the percent of

time that it occurred and so what you

see is if you just look at linear

distance there are lots of times that

there are arguments of relations that

you want to connect out that are 4 5 6 7

8 words away in fact there's even a

pretty large residue here well over 10%

where the linear distance away in words

is greater than 10 words if on the other

hand though you try and identify relate

the arguments of relations by looking at

the tendency distance then what you

discover is that the vast majority of

the arguments very close by neighbors in

terms of dependency distance so about

40% 7 percent of them are direct

dependencies and another 30 percent of

this

- so take those together and that's

greater than three-quarters of the

dependencies that you want to find and

then this number trails away quickly so

there are virtually no arguments of

relations that aren't fairly close

together in dependency distance and it's

precisely because of this reason that

you can get a lot of mileage in doing

relation extraction by having a

representation like dependency syntax

okay I hope that's given you some idea

of why knowing about syntax is useful

when you want to do various semantic

tasks in natural language processing
