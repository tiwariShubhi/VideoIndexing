finally let's talk about some practical

issues in text classification now that

we've seen the math of naive Bayes we

can turn to some real-world questions

how in practice should we build our

classifiers in the real world what

classifier you build and what you do

depends a lot on what kind of data you

have let's suppose you have no training

data well in that case the right thing

to do is to use Manly written rules so

here's a rule for deciding if a document

is about grain let's say we might say if

the word wheat or the word grain is

there and doesn't have the word whole or

the word bread so it's not a recipe then

we say this is a grain document now

manually written rules are difficult

they need careful crafting they have to

be tuned done development data and it's

very time-consuming can take days to

write the rules for each class but if we

have no training data this may be the

right approach what if you have very

little data well if you have very little

data the naive Bayes is just the

algorithm for you naive Bayes is what's

called a high bias algorithm and machine

learning a high bias algorithm is one

that doesn't tend to overfit training

data too badly it sort of trades off

variance or generalization to a new to a

new test set so it doesn't over fit too

much on a small amount of data so that's

the advantage of naive Bayes but it's

also important to get more data and you

can often find clever ways to get humans

to label data for you and that's an

important thing if you don't have enough

data get more data there's also various

ways that we're not going to talk about

so much in this class of semi supervised

training find some way to use a small

amount of data to help train a larger

amount of data and that's called

bootstrapping and another thing that you

might do if you had very little data if

you have a reasonable amount of data now

you might try all the clever classifiers

we'll talk about later in the quarter

classifiers like regular eyes logistic

regression or support vector machines in

fact you can even use decision trees

decision trees have advantages and

disadvantages but a big advantage of

decision trees is their user

interpretable and that's helpful because

people like to be able to modify a rule

or

Hakan the classifier and it's very easy

to modify a decision much easier to

modify your decision tree add a rule

change a threshold by hand that's much

harder to do that with an SVM or

logistic regression now if you have a

huge amount of data well now you can

achieve high accuracy although there is

a cost many classifiers just take a long

time SVM's especially K nearest

neighbors can be very slow to train a

classifier logistic regression can be

somewhat better but really if you have a

huge amount of data then it may just be

efficient to Train naive Bayes which is

quite fast actually if you have a very

huge amount of data here's it may turn

out that the classifier may not matter

here's a result from Berlin Banco on

spelling correction comparing the

performance of three four different

machine learning algorithms a memory

based Lerner winnow a perceptron and

naive Bayes with on a spelling

correction task with a million words

ten million words 100 million and so on

so a log scale and where we're measuring

how accurate the classifiers are and you

can see that as you that the difference

between the classifiers is much smaller

than the then the difference you get by

just adding more data and in fact things

depending on how much data you have the

classifiers cross over in their

performance curve so with enough data it

may not matter what classifier you have

so a real-world system in general will

combine this kind of automatic

classification whether whether it's from

some rules or supervised machine

learning with manual review of uncertain

or difficult or new cases there are some

important details for the computation in

naive Bayes one is under flow prevention

so it turns out that multiplying lots of

probabilities can result in floating

point under flow we talked about this

for a language modeling so since we by

the definition of logarithm the log of

XY is log of X plus log Y in general we

keep store our probabilities in the form

of logs and we add them instead of

multiply them so we still have the same

and formula here's the the naive Bayes

formula expressed now in terms of log

probabilities instead of probabilities

it's still an Arg max but now instead of

multiplying a probability and a product

of likelihoods we're adding a log

probability with a sum of log

likelihoods so now the model is just

maximizing choosing the class that

maximizes over sum sum of weights very

simple model finally we're going to want

to tweak performance and domain specific

features for your particular task domain

specific weights are very important in

the performance of real systems so for

example sometimes we're going to want to

collapse terms let's say we're dealing

with part numbers and some inventory

tasks now you might want to collapse all

the part numbers together into a part

number class and part number kind of

word or chemical formula you might want

to have just one named entity called

chemical formula but other kinds of

collapsing such as stemming generally

doesn't help so you have to know about

whether you need to collapse terms or

not it's also very important to up wait

up waiting is counting a word as if it

occurs twice and so often we up weight

title words or we went up weight the

first sentence of each paragraph or

sentences that have words that occurred

in the title went up with all the other

words in that sentence and so on so

small ways that can help in tweaking

performance so we've seen a number of

practical things that we can do in

building up a real-world text

classification system
