hi again let's turn to how to compute

whether two words are similar or not and

we'll start in this section with methods

that use a thesaurus to compute a word

similarity so where synonymous with a

binary relation two words are either

synonymous or they're not we often want

a looser definition of similarity or

distance that says two words are more

similar if they share more features of

meaning without requirement they'd be

absolute synonyms and similarity just

like synonymy is a relationship between

senses not between words so we don't say

that the word Bank is similar to the

word slope we say that one sense one of

bank is similar to the word fund

actually sends three of fund but it's

actually since two of bank is similar to

slope member we had two senses of bank

so even though technically similarities

a property of senses will find ways to

compute similarly over words as well

either by taking the Mac similarity

between senses or something or various

ways now word similarity plays a role in

lots of applications you might want to

know if two words are similar to grab a

set of synonyms or similar words for the

query if someone asks information

retrieval or for the answer in question

answering lots of times the translation

of a word you'll have to look for

similar words to help find a translation

essay grading where students write an

essay and you need to know what the

similar words are to the correct word

for the concept and so on this comes up

a lot and in all of our applications for

word similarity we often distinguish

technically word similarity from word

relatedness so a similar word is a near

synonym but a related word can be

related in any way so car and bicycle

might be similar maybe they're not

synonyms because car and automobile are

synonyms car and bicycle are not quite

so close so they're similar but car and

gasoline clearly related gasoline is

something that goes with cars but it's

not similar to cars in some way so we're

generally here looking for similarity

but occasionally some of the algorithms

will give you instead words that are

related and that might or may not be

useful depending upon the application

there are two classes of similarity

algorithms the

based algorithms we'll talk about in

this section are words nearby in the

hyper M hierarchy or do words have

similar glosses will use the hierarchy

or the glosses as ways of defining

similarity and distributional algorithms

that we'll talk about in the next

section do words have similar

distributional contexts so the simplest

of the thesaurus based similarity

algorithms is called path based

similarity and it give you a little

picture here of the wordnet hierarchy

and here we say two concepts two senses

or since Epps will call them concepts

for now are similar if they're near each

other in this thesaurus hierarchy maya

near each other we mean have a short

path in between them and we'll define

path lengths somewhat unusually we'll

say a concept has a path length one two

themselves and two to their nearest

neighbor and so on so the word the

concept nickel has a path length one

tentacle to two coin and three two dime

because it goes one for nickel two to

coin three to dime and the path length

between nickel and coinage is similarly

3 and 2 all the way up to money is 6 to

coin to coinage to currency to medium

exchange down to money and so on nickels

even further from Richter scale goes all

the way up to standard and then down to

Richter scale so the path length

formally is 1 plus the number of edges

in the shortest path in the hyper name

graph between the sense note C 1 and C 2

now we can use we can turn path length

which is a distance metric into sympathy

similarity metric simply by taking 1

over the distance so we'll take the path

length and invert it and we get a

similarity metric and we can turn the

sense base metrics empath as a metric of

similarity between two senses or

concepts c1 or c2 and turn it into a

metric between words by taking the

maximum similarity among pairs of senses

so for all senses of word 1 and all

senses award to I take the similarity

put in each of those word 1 senses in

each of those were two senses and I take

the maximum similarity

between those pairs and that's the

similarity between the words so

returning to sense base similarity we've

got our metric now for path based

similarity 1 over the path length so

between nickel and coin we have a path

length of 1 over 1 plus 1 then we're

adding 1 to the number of edges so it's

1/2 or 0.5 between funded and budget

similarly 1 over 2 or 0.5 between nickel

and currency we have 1 2 3 edges

so 1 over 3 plus 1 or 4 is 0.25 between

nickel and money now we have 5 plus 1 or

6 distance 1 over 6

similarily between coinage and um

Richter scale or OS similarity of 1 over

6 or for that matter between nickel and

standard also also 1 over 6 or 0.1 7 now

there's a problem with this basic path

based similarity which is that we assume

that every link represented a uniform

distance nickel to money somehow seems

to us they ought to be closer than

nickel 2 standard and that's because

nodes that are very high in the

hierarchy are very abstract we'd like a

metric that says that nodes whose only

link is going all the way up to the top

of the hierarchy those are probably not

very similar words and to do that we'd

like to represent the cost of each edge

independently the most common solution

to this problem is the use of

information content similarity metrics

first proposed by Resnick in 1995 and

these define a concept PFC a probability

of a concept see as the probability that

a randomly selected word in a corpus is

an instance of that concept informally

what I mean is there's a distinct random

variable that ranges over words

associated with each concept so every

node in a hierarchy has this random

variable and for any for each note for

that concept every observed noun is

either a member of that concept with

probability P of C or not a member of

that concept with probability 1 minus P

of C so every word is a number of the

root node which might be called entity

it might be called something else in

different versions of ordinate or your

own hierarchy and so that means that the

probability of whatever the root node is

1 and the probability of nodes right

below the root node are going to be very

high and the lower you get in the

hierarchy the lower the probability

let's see how that works so here's a

little piece of a hierarchy we've got

entity here there's actually something

above this hierarchy in the top and then

we have entity then we have down to

geological formation and then some leaf

nodes Hill Ridge grotto coast and so on

and we're going to train information

content similarity first by training a

probability P of C and every instance of

a word like Hill counts toward the

frequency of all of its parents of

itself obviously but also natural

elevation geological formation all the

way up to entity so if we define the

concept words of C words of C is the set

of all words that are children of nodes

C I should say plus C itself as well so

the words of natural elevation are Hill

Ridge and natural elevation itself and

the words of geological formation are

Hill Ridge grotto coast Shore cave

natural elevation and geological

formation itself and now we can take for

any concept we sum over all the words of

that concept so itself and all of its

children some the counts of all those

words and then normalized by the total

number of words in the corpus and that

tells us the probability of the concept

so the probability that a random word

will be an instance of that concept once

we have computed these probabilities we

can associate them with a hierarchy so

here's probabilities computed by takong

Lin and so now we can say that the

concept coast has probability point 0 0

0 to 1 6 while the further up we go in

the hierarchy up to entity we have a

probability 0.395 and whatever in this

particular version of wordnet was above

entity will have probability of 1

now that we have probability we just

need two more things we'll define the

information content of a concept as the

negative log probability of that concept

so we're just following the information

theoretic definition of information

there and we'll define the lowest common

sub sumer of a node as the lowest note

in the hierarchy that subsumes both of

them very naturally so the lowest common

sub sumer of hill and coast think about

that geological formation the lowest

common substan were of coasts and shore

shore now how are we going to use this

information content as a similarity

metric there are a number of methods the

Resnick method we say that the

similarity between two words is related

to how much information they have in

common the more they have in common the

more similar they are for Reznick we

just say what's in common between two

words is the information content of

their lowest common sub sumer if I have

two concepts what's in common between

them is the thing that they share is

their inherited thing in common so if I

just measure the amount of information

in that that is in fact what they have

in common so the negative log

probability of that least common sub

sumer that's their similarity so we'll

define that metric the Resnick

similarity metric this way

an alternative metric for dealing with

information theoretic similarity is the

Lin metric as with Resnick the more two

things have in common the more similar

they are but now the new intuition the

more differences between a and B the

less similar they are and we measure

commonality by introducing the predicate

common which is a proposition stating

the commonalities between a and B and I

see the information of information

contained in that proposition and to

measure the difference between a and B

we say that the total description of a

and B the sum of everything we know

about them is the sum of the

commonalities

plus the differences so to get the

differences we can take the description

and subtract out the commonalities so

roughly speaking a and B are more

similar if I see of common is high and I

see AB description is low so the Lin

similarity between two concepts a and B

is higher when they have more

and less when there's a lot of other

things about them they don't have in

common and Lynne modifies Resnick in

defining the information content of the

commonality of the two as twice the

information of their lowest common sub

Sumer and so given two concepts in a

hierarchy the Lin similarity is two

times the log probability of their least

common sub Sumer over the sum of the log

probabilities of the two concepts the

total of the description that we know

about the two concepts let's look at an

example in our small sample hierarchy we

want to know the Lin similarity between

the concepts Hill and Coast and we look

at their least lowest common sub Sumer

geological formation we take twice the

log probability of that divided by the

sum of the log probabilities of the two

items hill and coast and that gives us

for the Lin similarity between hill and

coast as 0.59 now a final thesaurus base

similarity metric is called the Lesk

algorithm after Michael s who invented

it it's often called Lascaux extended

Lesk and this method instead of using

the hierarchy looks at the glasses of

the words in the dictionary or thesaurus

and the intuition is that two concepts

are similar if they're glosses contain

similar words

so the two-word net words drawing paper

and decal have lots of similar words

paper that especially prepared for use

in drafting transferring designs from a

specially prepared paper a blah blah

blah and we have here the words

specially prepared are in common and

paper in both definitions and so for

each N word phrase that's in both

glosses the less Cal government adds a

score of N squared so papers in both

glosses that's a length one so we'll add

a score of one especially prepared as of

length two so we'll add a score of two

square root of four so this the total

Lesk similarity between drawing paper

and decal is 5 and in fact with most

versions of less similarity we don't

just look at the glosses of the two

words we look at the glasses of the

words they're hyper names they're high

poems and so on we add all that up so

it's a sum over all the words or

sometimes up max over all the words

of their similarity so in summary we've

seen three classes of thesaurus base

similarity path length similarity where

we two words are similar if there's a

short path between them in the hierarchy

information theoretic similarity we've

seen two methods Resnick and Lin I mean

there's a third one called Jian Konrath

so these are the information theoretic

similarity what we're looking at the

least common sub Sumer of a no measuring

its probability turning that into a into

an information measure and we've seen

less similarity we're given two concepts

we take the gloss of them and compute

the overlap in words with this kind of

waiting that we talked about or we might

just look at the glosses not just of the

words but of some words in relation to

those concepts like they're high

panama's or high poems and so on when we

sum all that up over a specified set of

relations and that gives us the less

similarity or extended less similarity

there are lots of libraries for

computing these various thesaurus base

methods in Python and ltk has methods

and there are Python based tools like

word net similarity and there's even a

nice web-based interface so that you can

check out the similarity between two

words based on different method methods

and you can go take a look at all of

these we evaluate similarity like many

other NLP algorithms in two different

ways we can do intrinsic evaluation

where we look at the metric itself and

say how similar is the the numbers this

metric gives to what humans would give

on some similar tasks so I get a

similarity metric for two words and then

I get humans to give me a number how

similar are these two words and I

compare those so that's intrinsic

evaluation more more functional is

entrance extrinsic or task-based

end-to-end evaluations where I have some

application I put my similarity metric

in the application and I see how well it

improves the application and that

application could be word sense

disambiguation or spelling error

correction or essay grading a common

simple extrinsic test that's used is

taking the test of English as a foreign

language or TOEFL multiple-choice

vocabulary tests so here we have

questions like levied is closest in

meaning to which of these four words and

we can

simply take our similarity metric

compute the similarity between levied

and imposed levied and believed levied

and requested levied and correlated and

see if our metric returns the right

answer the most similar word aware the

correct answer is imposed so thesaurus

based methods forward similarity are a

useful way of telling if two words are

similar

they're very functional in languages

like English where we have lots of

thesauruses either for general text in

word net or for medical text in mesh

there they'll work less well when we're

working in particular genres where we

might not have the right information in

the thesaurus or in languages for which

the source sources are not as available

and so for those applications will turn

to the distributional methods that we'll

see next
