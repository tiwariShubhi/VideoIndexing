it turns out that naivebayes has a very

close relationship to language modeling

let's see how that is we'll start by

looking at the generative model for

multinomial naive bayes so imagine I

have a class let's say it's China and

we're imagine that we were randomly

generating a document about China so we

might start by saying well the first

word X sub 1 is Shanghai and the second

word is and and the third word is

Shenyang and the fourth word is issue

and the fifth word is bonds and so when

we've generated a little document random

a little document about China so what

this generative model shows you is that

each word is a independently generated

word from a class generated with a

certain probability we have a little set

of probabilities we're keeping for each

word let's think about that now in

general naive Bayes classifiers can use

all sorts of features URLs email

addresses we'll talk about that for spam

detection but if in the previous slide

we just use the word features and if we

use all the words in the text then it

turns out that this generative model for

naive Bayes gives it an important

similarity to language modeling my base

turns out to be a kind of language model

and in particular each naive Bay each

class in a naive Bayes classifier each

class is a unigram language model and

the way we can think about that is each

word in a naive Bayes classifier the

likelihood term assigns a word the

probability of the word given the class

and a sentence in a naive Bayes

classifier since we're multiplying

together sentence or even a whole

document since we're multiplying

together the probabilities of all the

words we compute the probability of a

sentence given the class we're just

multiplying together all the words the

likelihoods of all the words in the

class so let's see how that works

imagine that we have the class positive

and we have our likelihoods the

likelihood of I given positive that's P

of I given positive

and we have P of love given positive and

we have P of this given positive and so

on and P of I given positive is point

one and P of love giving positive his

point one and so on okay so here's our

naive Bayes classifier well we can think

of that exactly as a language model we

have a sequence of words I generate some

words I love this fun film and and naive

Bayes is assigning that sequence of

words a set of probabilities one for

each word from the class point one point

one point oh five and so on so that if

we multiplied all these together we can

get a probability of the sentence so

naive Bayes each class is just a unigram

language model conditioned on the class

so when we ask the question which class

assigns a higher probability to a

document it's like we're running two

separate language models so here I've

shown you two separate language models

the positive class and the negative

class and each one has separate

probabilities so here's probability of

Pi given negative and here's probability

of AI given positive I guess people are

more likely to use the word I when they

don't like something and now if we take

a particular sentence I love this fun

film and we say what probabilities does

this sequence what probably what's the

probability of this sequence according

to our first model and what's the

probability according to our second

model each one assigns a probability for

each word that's the naive Bayes

likelihoods we can multiply them all

together and we can show that if we

multiply them all together you can sort

of see from inspection that the positive

probabilities multiplied together are

going to be I mean because of this one

and this one are going to be much higher

than the negative probabilities so you

can think of naive Bayes as each class

is a is a separate class conditioned

language model we're going to run each

language model to compute the likelihood

of a test sentence and we'll just pick

whichever language model

has the higher probability as the class

which is the more likely class so that's

the close relationship between naive

Bayes and language modeling
