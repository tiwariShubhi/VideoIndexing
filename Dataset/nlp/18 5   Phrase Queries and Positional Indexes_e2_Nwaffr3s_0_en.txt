in this segment I'm going to introduce

phrase queries which in practice have

been the most important kind of extended

boolean query and then examine how you

need to extend our inverted index data

structure to be able to handle phrase

queries so very often we'd like to query

not only for individual words but also

for multi word units many useful things

that we want to query on a multi-word

unit so information retrieval as a multi

word unit or many many other things like

organization names Stanford University

so what we want to do is have a

mechanism where we can see say let's

let's match these pair of words as a

unit and the syntax of standardly used

in modern web search engines is to put

it in inverted quotes like that and so

if we have that kind of phrase as a

query then what we're going to say is a

document that says I went to University

at Stanford doesn't match that phrase

query so this notion of putting things

in inverted commas has proven to be very

easily understood by users something

that information retrieval system

designers generally lament is that when

people design advanced search

functionality for their systems that

almost nobody uses it just a few people

like me and other academics and people

like that but regularly users just don't

use it and really this notion of phrase

querying is sort of the nearest thing to

an exception that it is fairly widely

understood and used but I'll point out

that as well as overt phrase queries

like this there's actually another

interesting phenomenon is that many

other queries are implicit phrase

queries so someone will give us their

query San Francisco or something like

that and well really what they want to

do is have it interpreted as a phrase

but they just didn't follow or no to put

the inverted quotes around it so in

modern web search engines an active area

of research has been how to identify

these implicit phrases

and take advantage of its knowing it's

an implicit phrase to alter what

documents get returned but we're gonna

leave that aside for the moment and

we're just going to deal with these

explicit phrases and work out how we can

do a good job at matching them with our

information retrieval system okay well

first what should be clear is that no

longer suffice as the store just a term

and a postings list of documents for

each term because if we have only that

we can easily find documents that

contain two terms but we have no idea

whether the terms are adjacent to each

other as a phrase in that document the

only way we could do it is by exhaustive

post-processing of candidate documents

to see if they actually did contain the

phrase and that would be extremely slow

because we'd be back to linear scanning

of documents one way to solve this

problem is the idea of a by word index

so for by word index what we do is we

index every consecutive pair of terms as

a phrase so for example if we have this

text here friends Romans countrymen we

generate the by words of adjacent words

of friends and Romans and then we

generate the by word of Romans

countrymen and for each of these by

words is now a dictionary term and so

what does that mean well that means that

for each of these we'd say that friends

Romans occurs in let's say this is

document 17 and maybe it also occurs in

documents 33 and Romans countrymen

occurs in document 17 and it might occur

in some other document well if we build

that two-word phrase queering is now

immediate because if we want to find

documents that contain friends Romans in

them we can just look up this dictionary

entry and then grab the postings list

that was returned for it if we consider

more calm

cases we can even basically handle those

so we can handle longer phrase queries

by breaking them down so if our phrase

query is Stanford University Palo Alto

we can break it up into an and query of

Stanford University and University Paulo

and Palo Alto and so we can use our by

word index to find documents that

contain all three of these by grams now

that's not going to be perfect without

doing a linear scan through the docs we

can't be sure that the documents that

were returning actually have this

continuous phrase Stanford University

Palo Alto but it seems highly likely

that they will because we have checked

for each of these diagrams occurring but

so there's a small chance of false

positives but it seems like we're in a

pretty good state so one of the issues

with using a byword index well as we

noted before there can be false

positives in what's returned but maybe

that's not such a big problem the big

problem is that there's this enormous

blow-up of the index because our

dictionary has gotten much more massive

so that means that you know it's not

practical to have try word and quadword

indices to actually exactly match long

queries but even for by words we've been

going to have the sort of potentially

have the space of words squared possible

dictionary entries so because of that by

word indices and not the standard

solution for handling phrase searching

but as I'll show it wasn't useless that

I explained them to you because as I'll

show at the end they can be an important

component of a solution so what is the

standard solution the standard solution

is to move to having a positional index

so the idea of the positional index is

that in the postings we in store not

only the documents that contain a term

but also the position in each document

where the term

years so the organization of what we

have now is in the dictionary we have a

term and the document frequency of the

term and then when we point off to the

postings lists then instead of just

having a list of document IDs we then

have a list of document IDs where for

each document we then have a list of

positions where the term occurs let's

look at that concretely with an example

so here we have the word B which occurs

in almost a million documents and then

we have where so in document one it

occurs at word 7 18 33 etc document 2 it

occurs in these two word positions it

doesn't occur in document 3 document 4

it occurs in a bunch of places and so on

and so with these we can then be able to

check when the Frasers occur by then

seeing where the words occur adjacent to

each other to get the idea of that you

could consider this little question here

so which of these four documents could

contain to be or not to be based on

those document positions now obviously

we haven't seen the postings list for

the other words but just looking at

where B occurs which document is

candidate so with this kind of data

structure we can handle phrase queries

using a merge algorithm much as we

showed before it's just a little bit

hairier because rather than just doing

an intersection of the document IDs we

have to do this sort of two-level

algorithm where we both intersect the

document IDs and then also check that

there are compatible positions for the

words occurring in a phrase and so that

means that we need to deal with a bit

more than just equality so for example

if we're wanting to find instances of

the phrase information retrieval

what we want is that if the word

information occurs at position 37 in a

certain document you want to tree vault

retrieval appearing at position 38 in

the document so going through them

slightly more detail to process a phrase

query what we do is so let's assume our

phrase is to be or not to be that we

want to find in inverted commas so we

find we get the postings lists of each

of the individual terms and then what

we're going to do is we're going to

progressively intersect them so if we

start off with the to B we're going to

start at the beginning

doing postings merge and we're saying

well document 1 can't be a candidate

because it doesn't appear on the other

postings list document 2 can't be a

candidate because it doesn't appear on

the other postings list and then at this

point we've got a document 4 and the doc

IDs match but then at this point with a

positional query we have to recurse and

then do another merge algorithm for the

positions within the postings list so we

start here and here but this time rather

than a quality check what we wanting to

see is can we find a place where B

occurs with a token number 1 larger than

2 so we'll again step along and we'll

say well here's one candidate 190 and

191 and here's a second candidate 429

and 430 so those will be two candidate

matches inside document 4 and so we'll

be returning both of those candidate

matches separately because we're

actually finding the positions in

documents where our phrase query is

matching and that we'll need to refine

with the other query words coming up

ahead at that point we then revert back

oh wait no sorry I'm wrong there's one

more candidate sorry there are three

candidates here right but once we've

exhausted

those the positions and one document we

then revert up to the higher level of

our postings merge and then we say five

is less than seven so this gets crossed

off and then we advance that pointer and

proceed along and I hope you can see

that actually this method works not only

for phrase queries where the words are

offset by one that you can actually use

exactly the same method for the

proximity queries that we saw earlier

with the Westlaw service where you could

ask for one word being within three

words of another word or something like

that right so that was the examples like

this so here we had limit within three

words of statute which is within three

words of federal within two words of

tort so we could use the same kind of

techniques of optimizing the query but

if we started with this one within

having a condition on how close together

the token position indices have to be to

count as a match within a document and

so clearly positional indices can be

used for these kind of proximity queries

whereby word indices do nothing for you

so I've sort of said the algorithm here

but it's something that you might want

to work through very concretely is how

you can change the linear postings merge

algorithms to handle proximity queries

in particular you can think about how to

get it to work for any value of K it's

actually a little bit tricky to do this

correctly and efficiently because you

can have the width in keywords matching

on either side of the word and you have

to keep track of things for the right

number of words on each side if is be

good to try and work out by yourself but

you can also see one example of an

answer in the bigger 212 of our

introduction to information retrieval

book which you can find free on the web

to look at so a positional index is

great because it allows us to answer

phrase queries and proximity queries

but there's a cost to a positional index

which is that our postings list just got

a lot larger because rather than only

storing document IDs they're storing

document IDs and the offsets of tokens

within the document and that's a major

factor even though indices can be

compressed nevertheless is now

completely standard to use a positional

index because of the power and

flexibility you can get from handling

phrase and proximity queries whether

they're being used explicitly in terms

of having these kind of phrase queries

or within three queries or whether

they're just being exploited to improve

the ranking of a system when it's

looking from licit phrases but as I said

the positional index gets much larger

and so when we're estimating the size of

a positional index we note that there

has to be an entry for each occurrence

of a word not just once a document so

what that means is that the size of the

index depends on the average length of

the documents now so if we have fairly

short documents it's not such a big deal

because actually most of the words the

cur in a document occur only once or

twice but if we have very long documents

then that blows out the size of

positional index rather more so for

example you can consider a word with a

common word with frequency 0.1% so this

word occurs one word in a thousand on

average and so well if you have a

document size of average length 1,000

then we're going to gain no blow out

really from going to a positional index

because they'll only be one position

being recorded but if we have a document

the documents that are really long then

we might begin a hundred times blowout

in the size of the positional index so

everything that depends but you know

just to give you some very rough rules

of thumb for you know what is in some

sense typical documents like webpages

that you might expect a position

next to be somewhere around two to four

times as large as a non positional index

and in particular the size of a

positional index remains smaller than

but starts to approach the volume of the

size of the original text so heading to

a sort of a third or half the original

size of the text that's being indexed

which is much larger than in the case of

a non positional index where it might be

down to something like just 10% so

having an IR system doesn't actually

take a lot more space than storing the

text in the first place I mentioned when

I mentioned by word indices that they

aren't a useless idea even though

they're not normally the solution by

themselves and so let me just come back

to that these two approaches can be very

profitably combined so if you look at

the query stream of high query volume

services like web search engines there

tend to be some queries that keep on

turning up again and again and again so

things like person names of popular

people well if we just treat those as

you know just regular postings list

intersection or the positional phrase

query posting list intersection what

we'd have to do is keep on doing this

intersection over and over again every

time that someone sends that question

and it's bad for cases like Michael

Jackson it's less bad when you've got

rare names like Britney Spears because

presumably they're posting lists are

much shorter and the intersection is

roughly the same as each individual

postings list this in doing this

intersection every time is especially

bad when you have common words like if

you're searching for the banned by the

WHO then what's going to happen is

you're going to retrieve two enormous

postings lists do the intersection of

them and end up with a very short

postings list for this phrase query

so in a paper from a group in Melbourne

that's of well known information

retrieval group in 2004 they

investigated a more sophisticated moost

indexing system so in this what happened

was that for common phrases you know

like these examples they did build by

words and index the by words where for

rarer phrases they were done by a

positional index and so what they were

able to show was with a typical web

query mixture you could execute it in

one quarter of the time the positional

index alone by making use of it also

having indexed some by words but at the

cost of only taking 26% more space than

having the positional index alone so

that makes that it look a fairly

appealing trade-off to augment a

positional index with common by grams

and well one model of doing that is to

do it as here

where you work out in advance the common

by grams and then index those in your

standard inverted index in practice what

happens a lot in modern systems is that

people try to do this a bit more

dynamically so that they keep a cache of

commonly being queried phrase queries

and that what the result of the postings

list intersection is for each of those

and so that's like having added those by

words to the inverted index but done a

bit more dynamically okay so that's

introduced what's the most extension to

the classic all-in retrieval model which

is having support for phrase queries and

we introduced a method for handling

those more two methods but in particular

we looked at positional indices which

can handle phrase queries but also the

proximity queries that we saw earlier
