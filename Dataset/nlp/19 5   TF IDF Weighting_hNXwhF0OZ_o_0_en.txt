we've now introduced two weights for

terms and documents to use in our

information retrieval making process

term frequency and inverse document

frequency in this segment we're going to

put them together to give the tf-idf

weight of terms the tf-idf weight of a

term and the document right here is

simply the product of its TF weight

scaled with a long term as we discussed

before times its inverse document

frequency weight this is the best-known

weighting scheme for terms and

information retrieval there's been a lot

of research and there are many others

but if you only know one it's the one to

know note in particular one fine point

so this little - or - here and this

tf-idf waiting that's a - it's not a

minus sign that we're taking a product

so sometimes people indicate that more

explicitly by using a dot or using a

multiplication sign so one of the

features of tf-idf weighting EF IDF

weighting increases with the number of

times the term occurs in the document so

that the tf-idf wait for a query term

depends on the document it's not

independent of the document and then the

tf-idf wait for a term also goes up with

the rarity of the term in the collection

that's from the IDF waiting here so

using this to find the ranking of

documents for a query what we're doing

to work out the score of the query in

the document is we're taking the terms

that appear in both the query in the

document the rest of them have no

waiting and then we're working out this

tf-idf weight for each of those terms

and then we're summing them to give the

score of the document with respect to

the query so what have we done here what

we've done is we've gradually moved from

first binary vectors

in the original model of doing boolean

information retrieval to count vectors

which we used when we just had an

unskilled term frequency so now we have

weight vectors for a document and hence

a weight matrix between terms and

documents and that's now what we see

here

so each document is now being

represented by a real valued vector so

for example the document Julius Caesar

is being represented by this vector so

that for each document it's in the

vector space of real valued numbers

where the dimensionality is the number

of different terms in our collection

again okay and then when we have a bunch

of these vectors for each document in

our collection we have a term a term

document matrix which is now a real

valued matrix well say a little bit more

later

about what some of the properties of

this is but hopefully seeing this kind

of term document matrix of real numbers

there's enough to see how we can do a

ranking of documents according to some

query according to these tf-idf scores

that we've assigned that each term and

each document

so that's tf-idf weighting one of the

most central concepts in information

retrieval systems
