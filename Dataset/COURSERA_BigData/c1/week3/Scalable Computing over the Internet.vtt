WEBVTT

1
00:00:01.110 --> 00:00:03.128
Scalable computing over the internet.

2
00:00:19.074 --> 00:00:23.040
Most computing is done on
a single compute node.

3
00:00:24.900 --> 00:00:29.960
If the computation needs more than
a node or parallel processing,

4
00:00:29.960 --> 00:00:34.070
like many scientific computing problems,
we use parallel computers.

5
00:00:35.070 --> 00:00:40.420
Simply put, a parallel computer
is a very large number of

6
00:00:40.420 --> 00:00:46.041
single computing nodes with specialized
capabilities connected to other network.

7
00:00:47.130 --> 00:00:54.525
For example, the Gordon Supercomputer here
at The San Diego Supercomputer Center,

8
00:00:54.525 --> 00:01:03.170
has 1,024 compute nodes with 16 cores each
equalling 16,384 compute cores in total.

9
00:01:04.230 --> 00:01:07.890
This type of specialized
computer is pretty costly

10
00:01:07.890 --> 00:01:13.180
compared to its most recent cousin,
the commodity cluster.

11
00:01:13.180 --> 00:01:17.920
The term, commodity cluster,
is often heard in big data conversations.

12
00:01:19.280 --> 00:01:21.560
Have you ever wondered
what it exactly means?

13
00:01:22.960 --> 00:01:27.740
Commodity clusters are affordable
parallel computers

14
00:01:27.740 --> 00:01:29.780
with an average number of computing nodes.

15
00:01:31.070 --> 00:01:35.650
They are not as powerful as
traditional parallel computers and

16
00:01:35.650 --> 00:01:38.280
are often built out of
less specialized nodes.

17
00:01:39.400 --> 00:01:42.660
In fact,
the nodes in the commodity cluster

18
00:01:42.660 --> 00:01:45.640
are more generic in their
computing capabilities.

19
00:01:46.960 --> 00:01:51.320
The service-oriented computing community
over the internet have pushed for

20
00:01:51.320 --> 00:01:56.800
computing to be done on commodity
clusters as distributed computations.

21
00:01:56.800 --> 00:02:02.030
And in turn, reducing the cost
of computing over the Internet.

22
00:02:03.680 --> 00:02:09.390
In commodity clusters,
the computing nodes are clustered in racks

23
00:02:11.400 --> 00:02:15.480
connected to each other
via a fast network.

24
00:02:16.860 --> 00:02:20.150
There might be many of such
racks in extensible amounts.

25
00:02:21.600 --> 00:02:26.410
Computing in one or
more of these clusters across

26
00:02:26.410 --> 00:02:31.910
a local area network or the internet
is called distributed computing.

27
00:02:31.910 --> 00:02:36.860
Such architectures enable what
we call data-parallelism.

28
00:02:36.860 --> 00:02:41.416
In data-parallelism many jobs
that share nothing can work on

29
00:02:41.416 --> 00:02:44.580
different data sets or
parts of a data set.

30
00:02:45.930 --> 00:02:51.060
This type of parallelism sometimes
gets called as job level parallelism.

31
00:02:51.060 --> 00:02:55.950
But in this specialization,
we will refer to it as data-parallelism

32
00:02:55.950 --> 00:03:00.020
in the context of Big-data computing.

33
00:03:00.020 --> 00:03:05.020
Large volumes and varieties of big
data can be analyzed using this mode

34
00:03:05.020 --> 00:03:11.540
of parallelism, achieving scalability,
performance and cost reduction.

35
00:03:11.540 --> 00:03:16.070
As you can imagine, there are many
points of failure inside systems.

36
00:03:17.500 --> 00:03:23.200
A node, or
an entire rack can fail at any given time.

37
00:03:23.200 --> 00:03:27.850
The connectivity of a rack
to the network can stop or

38
00:03:27.850 --> 00:03:31.300
the connections between
individual nodes can break.

39
00:03:32.820 --> 00:03:37.370
It is not practical to restart everything
every time, if failure happens.

40
00:03:38.500 --> 00:03:44.069
The ability to recover from such
failures is called Fault-tolerance.

41
00:03:44.069 --> 00:03:49.245
For Fault-tolerance of such systems,
two neat solutions emerged.

42
00:03:49.245 --> 00:03:54.230
Namely, Redundant data storage and

43
00:03:54.230 --> 00:03:57.480
restart of failed
individual parallel jobs.

44
00:03:58.700 --> 00:04:00.780
We will explain these two solutions next.

45
00:04:02.100 --> 00:04:06.970
As a summary the commodity
clusters are a cost effective way

46
00:04:06.970 --> 00:04:11.360
of achieving data parallel scalability for
big data applications.

47
00:04:11.360 --> 00:04:16.750
These type of systems have a higher
potential for partial failures.

48
00:04:16.750 --> 00:04:19.850
It is this type of distributed
computing that pushed for

49
00:04:19.850 --> 00:04:23.385
a change towards cost
effective reliable and

50
00:04:23.385 --> 00:04:27.550
Fault-tolerant systems for
management and analysis of big data.