WEBVTT

1
00:00:00.830 --> 00:00:03.232
Characteristics of Big Data- Volume.

2
00:00:13.965 --> 00:00:19.190
Volume is the big data dimension that
relates to the sheer size of big data.

3
00:00:20.410 --> 00:00:24.580
This volume can come from
large datasets being shared or

4
00:00:24.580 --> 00:00:30.240
many small data pieces and
events being collected over time.

5
00:00:31.460 --> 00:00:35.335
Every minute 204 million emails are sent,

6
00:00:35.335 --> 00:00:41.308
200,000 photos are uploaded, and 1.8
million likes are generated on Facebook.

7
00:00:41.308 --> 00:00:48.710
On YouTube, 1.3 million videos are viewed
and 72 hours of video are uploaded.

8
00:00:51.160 --> 00:00:54.120
But how much data are we talking about?

9
00:00:54.120 --> 00:00:59.170
The size and the scale of storage for
big data can be massive.

10
00:00:59.170 --> 00:01:03.310
You heard me say words that
start with peta, exa and

11
00:01:03.310 --> 00:01:09.220
yotta, to define size, but
what does all that really mean?

12
00:01:09.220 --> 00:01:15.210
For comparison, 100 megabytes will
hold a couple of encyclopedias.

13
00:01:16.460 --> 00:01:20.340
A DVD is around 5 GBs, and

14
00:01:20.340 --> 00:01:25.837
1 TB would hold around 300
hours of good quality video.

15
00:01:25.837 --> 00:01:32.530
A data-oriented business currently
collects data in the order of terabytes,

16
00:01:32.530 --> 00:01:35.880
but petabytes are becoming more
common to our daily lives.

17
00:01:36.930 --> 00:01:42.290
CERN's large hadron collider
generates 15 petabytes a year.

18
00:01:42.290 --> 00:01:47.340
According to predictions by an IDC report
sponsored by a big data company called

19
00:01:47.340 --> 00:01:54.150
EMC, digital data, will grow by
a factor of 44 until the year 2020.

20
00:01:54.150 --> 00:01:58.190
This is a growth from 0.8 zetabytes,

21
00:01:59.750 --> 00:02:04.280
In 2009 to 35.2 zettabytes in 2020.

22
00:02:04.280 --> 00:02:11.090
A zettabyte is 1 trillion gigabytes,
that's 10 to the power of 21.

23
00:02:11.090 --> 00:02:14.140
The effects of it will be huge!

24
00:02:15.160 --> 00:02:20.420
Think of all the time, cost,
energy that will be used to store and

25
00:02:20.420 --> 00:02:23.860
make sense of such an amount of data.

26
00:02:23.860 --> 00:02:26.430
The next era will be yottabytes.

27
00:02:26.430 --> 00:02:31.050
Ten to the power of 24 and
brontobytes, ten to the power of 27.

28
00:02:31.050 --> 00:02:36.410
Which is really hard to imagine for
most of us at this time.

29
00:02:36.410 --> 00:02:41.740
This is also what we call data
at an astronomical scale.

30
00:02:41.740 --> 00:02:44.280
The choice of putting the Milky Way Galaxy

31
00:02:45.540 --> 00:02:49.560
in the middle of the circle
is not just for aesthetics.

32
00:02:50.660 --> 00:02:54.520
This is what we would see if
we were to scale up 10 to

33
00:02:54.520 --> 00:02:56.780
the 21 times into the universe.

34
00:02:56.780 --> 00:02:57.510
Cool, isn't it?

35
00:02:58.750 --> 00:03:01.760
Please refer to the reading
in this module called,

36
00:03:01.760 --> 00:03:07.220
what does astronomical scale mean,
for a nice video on the powers of ten.

37
00:03:07.220 --> 00:03:14.400
All of these point to an exponential
growth in data volume and storage.

38
00:03:14.400 --> 00:03:17.200
What is the relevance of
this much data in our world?

39
00:03:18.260 --> 00:03:20.210
Remember the planes collecting big data?

40
00:03:21.550 --> 00:03:25.170
Our hope, as passengers,
is data means better flight safety.

41
00:03:26.540 --> 00:03:32.070
The idea is to understand that businesses
and organizations are collecting and

42
00:03:32.070 --> 00:03:36.520
leveraging large volumes of data
to improve their end products,

43
00:03:36.520 --> 00:03:40.366
whether it is safety, reliability,
healthcare, or governance.

44
00:03:40.366 --> 00:03:45.300
In general, in business the goal

45
00:03:45.300 --> 00:03:50.660
is to turn this much data into
some form of business advantage.

46
00:03:50.660 --> 00:03:55.140
The question is how do we
utilize larger volumes of data

47
00:03:55.140 --> 00:03:57.920
to improve our end product's quality?

48
00:03:57.920 --> 00:04:00.790
Despite a number of
challenges related to it.

49
00:04:01.840 --> 00:04:06.650
There are a number of challenges related
to the massive volumes of big data.

50
00:04:08.210 --> 00:04:11.970
The most obvious one is of course storage.

51
00:04:11.970 --> 00:04:14.800
As the size of the data increases so

52
00:04:14.800 --> 00:04:18.740
does the amount of storage space
required to store that data efficiently.

53
00:04:19.810 --> 00:04:24.340
However, we also need to be able
to retrieve that large amount of

54
00:04:24.340 --> 00:04:26.030
data fast enough, and

55
00:04:26.030 --> 00:04:32.140
move it to processing units in a timely
fashion to get results when we need them.

56
00:04:32.140 --> 00:04:35.900
This brings additional challenges
such as networking, bandwidth,

57
00:04:35.900 --> 00:04:37.205
cost of storing data.

58
00:04:37.205 --> 00:04:41.670
In-house versus cloud storage and
things like that.

59
00:04:41.670 --> 00:04:46.530
Additional challenges arise during
processing of such large data.

60
00:04:46.530 --> 00:04:50.450
Most existing analytical methods
won't scale to such sums of

61
00:04:50.450 --> 00:04:53.500
data in terms of memory,
processing, or IO needs.

62
00:04:55.150 --> 00:04:56.980
This means their performance will drop.

63
00:04:58.060 --> 00:05:02.220
You might be able to get good performance
for data from hundreds of customers.

64
00:05:02.220 --> 00:05:09.314
But how about scaling your solution
to 1,000 or 10,000 customers?

65
00:05:09.314 --> 00:05:15.270
As the volume increases performance and
cost start becoming a challenge.

66
00:05:16.880 --> 00:05:21.530
Businesses need a holistic
strategy to handle processing of

67
00:05:21.530 --> 00:05:26.090
large scale data to their benefit
in the most cost effective manner.

68
00:05:26.090 --> 00:05:29.690
Evaluating the options across
the dimensions mentioned here,

69
00:05:29.690 --> 00:05:33.580
is the first step when it comes to
continuously increasing data size.

70
00:05:33.580 --> 00:05:38.290
We will revisit this topic
later on in this course.

71
00:05:38.290 --> 00:05:44.330
As a summary volume is the dimension
of big data related to its size and

72
00:05:44.330 --> 00:05:45.540
its exponential growth.

73
00:05:46.770 --> 00:05:52.126
The challenges with working with volumes
of big data include cost, scalability,

74
00:05:52.126 --> 00:05:56.725
and performance related to their storage,
access, and processing.