WEBVTT

1
00:00:01.260 --> 00:00:04.590
Linear regression is a very common
algorithm to build regression models.

2
00:00:06.000 --> 00:00:11.150
After this video you will be able to
describe how linear regression works,

3
00:00:11.150 --> 00:00:16.120
discuss how least squares is used in
linear regression, define simple and

4
00:00:16.120 --> 00:00:17.260
multiple linear regression.

5
00:00:19.070 --> 00:00:24.440
A linear regression model captures the
relationship between a numerical output

6
00:00:24.440 --> 00:00:26.440
and the input variables.

7
00:00:26.440 --> 00:00:30.520
The relationship is modeled as
a linear relationship hence the linear

8
00:00:30.520 --> 00:00:31.410
in linear regression.

9
00:00:32.620 --> 00:00:37.250
To see how linear regression works, let's
take a look at an example from the Iris

10
00:00:37.250 --> 00:00:41.792
flower dataset, which is a commonly
used dataset for machine learning.

11
00:00:41.792 --> 00:00:46.060
This dataset has samples of
different species of iris flowers

12
00:00:46.060 --> 00:00:49.630
along with measurements such as
petal width and petal length.

13
00:00:49.630 --> 00:00:54.600
Here we have a plot with petal width
measurements in centimeters on the x axis,

14
00:00:54.600 --> 00:00:57.610
and petal length
measurements on the y axis.

15
00:00:57.610 --> 00:01:01.589
Let's say that we want to predict
petal length based on petal width.

16
00:01:02.680 --> 00:01:06.240
Then the regression task is this,
given a measurement for

17
00:01:06.240 --> 00:01:08.350
petal width predict the petal length.

18
00:01:09.620 --> 00:01:13.650
We can build a linear regression model to
capture this linear relationship between

19
00:01:13.650 --> 00:01:17.330
the input petal width and
the output petal length.

20
00:01:17.330 --> 00:01:21.510
The linear relationship for this samples
is shown as the red line on the plot.

21
00:01:22.910 --> 00:01:26.880
From this example we see that linear
regression works by finding the best

22
00:01:26.880 --> 00:01:29.740
fitting straight line,
through the samples.

23
00:01:29.740 --> 00:01:31.300
This is called the regression line.

24
00:01:32.450 --> 00:01:35.360
In the simple case with
just one input variable,

25
00:01:35.360 --> 00:01:38.000
the regression line is simply a line.

26
00:01:38.000 --> 00:01:43.290
The equation for a line is y = mx + b,

27
00:01:43.290 --> 00:01:47.530
where m determines
the slope of the line and

28
00:01:47.530 --> 00:01:51.480
b is the y intercept or
where the line crosses the y axis.

29
00:01:52.790 --> 00:01:54.960
M and b are the parameters of the model.

30
00:01:56.130 --> 00:01:59.850
Training a linear regression model
means adjusting these parameters to

31
00:01:59.850 --> 00:02:02.770
fit the regression line to the samples.

32
00:02:02.770 --> 00:02:05.890
The regression line can be
determined using what's referred to

33
00:02:05.890 --> 00:02:07.380
as the least squares method.

34
00:02:08.540 --> 00:02:12.100
This plot illustrates how
the least squares method works.

35
00:02:12.100 --> 00:02:13.980
The yellow dots are the data samples.

36
00:02:15.320 --> 00:02:17.690
The red line is the regression line,

37
00:02:17.690 --> 00:02:20.150
the straight line that
goes through the samples.

38
00:02:20.150 --> 00:02:24.210
This line represents the model's
prediction of the output given the input.

39
00:02:25.380 --> 00:02:30.010
Each green line indicates the distance
of each sample from the regression line.

40
00:02:30.010 --> 00:02:33.630
So the green line represents
the error between the prediction,

41
00:02:33.630 --> 00:02:37.870
which is the value of the red regression
line and the actual value of the sample.

42
00:02:38.990 --> 00:02:43.080
The square of this distance is
referred to as the residual

43
00:02:43.080 --> 00:02:45.390
associated with that sample.

44
00:02:45.390 --> 00:02:47.970
The least squares method
finds the regression line

45
00:02:47.970 --> 00:02:52.430
that makes the sum of
the residuals as small as possible.

46
00:02:52.430 --> 00:02:55.920
In other words, we want to find
the line that minimizes the sum

47
00:02:55.920 --> 00:02:57.800
of the squared errors of prediction.

48
00:02:59.040 --> 00:03:03.160
The goal of linear regression then is
to find the best fitting straight line

49
00:03:03.160 --> 00:03:05.490
through the samples using
the least squares method.

50
00:03:06.820 --> 00:03:10.550
Once the regression model is built,
we can use it to make predictions.

51
00:03:10.550 --> 00:03:14.200
For example,
given a measurement of 1.5 centimeters for

52
00:03:14.200 --> 00:03:18.920
petal width, the model will predict
a value of 4.5 centimeters for

53
00:03:18.920 --> 00:03:22.250
petal length base on the regression
line that it has constructed.

54
00:03:23.450 --> 00:03:27.808
In linear regression, if there is only
one input variable then the task is

55
00:03:27.808 --> 00:03:30.350
referred to as simple linear regression.

56
00:03:31.490 --> 00:03:34.210
In cases with more than
one input variables,

57
00:03:34.210 --> 00:03:37.850
then it is referred to as
multiple linear regression.

58
00:03:37.850 --> 00:03:41.640
To summarize, linear regression
captures the linear relationship

59
00:03:41.640 --> 00:03:45.328
between a numerical output and
the input variables.

60
00:03:45.328 --> 00:03:49.240
The least squares method can be used
to build a linear regression model

61
00:03:49.240 --> 00:03:51.780
by finding the best fitting
line through the samples.