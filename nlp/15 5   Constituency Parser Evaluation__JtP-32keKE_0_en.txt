in this segment I briefly discussed how

we evaluate pausing results so how do we

go about evaluating whether our parsers

are doing a good job or not

one measure would just be to say is the

parse that we produce exactly the right

pass according to the tree bank that's a

possible standard and actually close to

what the probabilistic models do but

it's a rather tough job to get the

entire structure of a sentence right and

so the most-used evaluation measures

have divided up the pieces of a pars so

you can get partial credit for being

right so let's go through how that works

so the idea is we start with a gold

standard parse tree this is the correct

pass of this sentence and so then we run

our parser and our parser is going to

choose some parse tree for the sentence

and return it so in the example here the

parser is mostly right it's got the noun

phrase Sales Executives

it's got this verb structure of we're

examining and a noun phrase the figures

right it's actually made one little

mistake at the end so yesterday should

be this kind of weird kind of temporal

noun phrase in English hares where you

have be a noun phrase like yes yesterday

or next week to express a time and

otherwise if it expresses a

prepositional phrase like on Sunday in

the spring but the parser is wrongly

just stuck it on as an extra noun at the

end of the noun phrase great care and so

that's an error so how are we going to

go about measuring that well the way we

gonna do it is we're going to look at

individual constituency claims so here

is a noun phrase and we're going to say

that it goes from position 0 to 2 in the

sentence where I'm putting these fence

post marker numbers in between the words

of the sentence and so then we look down

at our guest pars and say well it also

has a noun phrase that goes from 0 to 2

in the sentence and so that particular

constituency claim in the gold tree is

correctly

in the past tree and so the way we do

that for all the cases is we conferred

the gold tree into a set of constituency

claims where we leave out the root node

so I've written claims across the root

node so these are our constituency

claims so we have a sentence that's

going from 0 to 11 and then the various

other constituency claims then we do

exactly the same thing for the candidate

pars produced suggested by the parser

and so it's also suggested that the

whole thing as a sentence from 0 to 11

it's also suggested the NP from 0 to 2

and so on so it's got a state of

constituency claims and then what we do

is we simply treat each of these as a

unit as an atom that you either get

right or wrong and then we use exactly

the same precision recall F measure that

you've already seen several times before

so here are the gold standard labeled

reckonings

here are the candidate labeled

bracketing 's and I've shown in bold the

three that the candidate agrees with the

correct pars on and so in total there

are eight bracketing 's in the gold pars

and seven in the proposed pars so the

label precision is 3/7 42.9% me call

37.5%

and the F 1 that combines those in the

usual way comes out as 40% now what that

partly means is when before we had this

there were these extra nodes down here

for the parts of speech that we talked

about before we don't in the parsing

measure of value evaluate the part of

speech taking even though most parsers

do part of speech taking as part of

their work so that's it reported

separately and in the example I've got

here the tagging is completely correct

and so that's a hundred percent

something that you might notice here is

that these scores are actually pretty

low and it's worth thinking a bit

about why they're so low and what we

find is that even though the candidate

pars was mostly correct we actually get

a lot of brackets wrong so what happened

the only thing that was wrong the

candidate pars was that this now

yesterday was wrongly fitting to this

noun phrase rather than being a

sentential modifier as a temple NP but

there's a consequence of making that

mistake the parser is scored as doing a

lot of other things wrong so that this

verb phrase is scored as wrong because

it's wrongly extending the cover

yesterday whereas it should end this

verb phrase is wrong for exactly the

same reason this prepositional phrase is

wrong for the same reason and so is this

noun phrase so this label precision

recall measure suffers from these

cascading errors whenever you attempt

something very low that should be high

or vice versa so many people think this

is actually not such a good measure of

parser performance and people including

me have argued instead we should use

dependency measures of parsing

performance even for constituency

parsers but that just isn't the current

practice of what you see everywhere in

research the measure that you see is

this labeled precision labeled recall F

1 and so that's a measure that we've

presented here I'll just briefly mention

you can also do an unlabeled version of

these measures where you simply look at

the constituency claim as a sequence of

words without worrying about the label

but that's much less used ok so now that

we know how to evaluate pcfg s how well

do they do well if you're just training

a pcfg off the pin treebank and run it

on some pin treebank test data and

evaluate with this measure the answer is

you get about 73% if one measure so

that's not terrible you get quite a lot

of the brackets right but you

it is counting each bracket separately

so if you're getting over a quarter of

the brackets wrong that means that

you'll make tending to make several bad

attachment errors in every sentence so

it's not a great result and we'll talk

about how to improve it

I'm very soon so let's just summarize

though a little bit the properties of

the pcfg s that we've seen up until now

so the good things about pcfg s is that

they're very robust you normally when

estimating a pcfg of a tree bank you get

the result that though with smooth

probabilities that every possible string

of words is included in the grammar so

there's no categorical grammar

constraint whatsoever anymore all the

action is in the probability is what's

high probability and low probability and

that's useful because the parser is

robust you can give it anything at all

and all do its best job to give you the

most likely sentence pars for that

another good thing about pcfg s is that

they give us at least part of a solution

to grammar ambiguity so like in the

example of people fishing that I showed

earlier that the pcfg tells you which

parts for a sentence to choose and at

least some time to chooses the correct

one so it gives some ideas of the

portability for pars but as we'll see

more later and you already got a hint of

that because of the strong and

dependence assumptions of pcfg it's not

actually very good at doing that

a third good property is that pcfg s

give us a probabilistic language model

so we say how likely or unlikely

sentences are in a language it's

important to point out that if you

shouldn't just now go and run off and

think aha I've got a grammar I use a

pcfg

as a better language model than the

migrants and trigrams we saw earlier

because actually a pcfg

as we've shown it so far doesn't work as

well as a bigram or trigram as a

language model for tasks like spelling

correction or speech recognition

and the reason that seems to be is that

PCF Gees lack the lexicalization of a

trigram model so what do we mean by that

well what we meant by there is that if

you're looking at this pcfg most of the

pcfg

is of the nature vp rewrites as BB d vp

that rules that are expanding without

any reference to what words are actually

used in the sentence in fact the only

rewrite rules that consider words are

the ones that rewrite from a pre

terminal to the word itself and so this

limits the ability of a plane pcfg to

act as a very effective language model

that's a problem we'll address again

very soon but for the moment you now

know how to go about evaluating

constituency parsers
