some of the most popular recent

algorithms for relation extraction are

semi-supervised they're unsupervised

algorithm let's look at them what

happens if you don't have a large

training set but what you have instead

or maybe just a couple of seed examples

or maybe you have a couple of high

precision patterns can you use those

seeds to do something useful in the seed

based or bootstrapping approaches to

relation extraction we use the seeds to

directly learn how to populate a

relation the intuition of the seed based

method from hearst 1992 is to gather a

set of seed pairs that have a relation

and then iterate the following find

sentences with these pairs we look at

the context between or around the pair's

look at the words before or after or in

the relation itself and we generalize

this context to create some patterns we

use the patterns to search for more

pairs if we're on the web we're

searching the web if we know a large

corpus we look in this corpus we find

more pairs and now we just iterate we we

take from these pairs we find sentences

that have them we find more patterns we

generalize the patterns and so on and

this iterative loop so suppose we're

looking for where famous authors are

buried and we know that Mark Twain is

buried in Elmira New York so he might

start with that single seed tuple we

might grab our Google on the web for all

environments of that seed tuple so we

might find sentences like these Mark

Twain is buried in Elmira the grave of

Mark Twain is in Elmira

elmyra's Mark Twain's final resting

place and then we take out the actual

entities and create little variables so

we learn X is buried in Y is a pattern

or the grave of X is in Y is a pattern

or Y is X's final resting place so we

learn these kind of patterns we take

those patterns rep form or tuples and

then we iterate and this approach was

first applied by certainly Brin in 1998

who looked at the task of extracting

author book pairs so we might have

authors like Isaac Asimov and the robots

of dawn or William Shakespeare in the

comedy of errors so first we might find

instances so imagine for comedy of

errors

we happen to find these for instance

the comedy of errors by William

Shakespeare was the comedy of errors by

William Shakespeare is the comedy of

errors one of William Shakespeare's

earliest attempts and so on and now we

extract patterns from these and one way

that to do this is to group all these

patterns by what's in the middle so

these two both have comma by in the

middle and now we take the longest

common prefix or suffix of the of the

before and after parts so both of these

have nothing before and both of these

has a comma after so we'll extract a

pattern saying we have an X followed by

comma by and then a Y followed by a

comma or in these two patterns they both

have comma one live in the middle and

they both have nothing before and they

both have an apostrophe s afterwards so

we can extract the pattern X and then

comma one of and then Y and then

apostrophe s and again we iterate now

that we've got some new patterns and we

find new seeds that match that pattern

and we continually iterate the brain

approach was improved in the snowball

algorithm a similar iterative algorithm

and similar kinds of grouped instances

to extract patterns and the extra

intuition of snowball was the

requirement that X and y be named

entities so in the in the deep ray

algorithm x and y could be any string

now we're gonna add the intuition that

each of those things have to be a

particular named entity that's going to

help us because we know that we have a

relation between organization and

location

and again we extract words in or in

between or before or after the two

patterns and the snowball algorithm also

computed a confidence value for each

pattern so what's kind of a new

intuition another semi-supervised

algorithm extends both of these ideas by

combining the bootstrapping algorithms

that we saw with the seed based methods

with the supervised learning algorithms

we saw in the previous lecture so

instead of five seeds in a distance

supervision algorithm we take a large

database and we get a huge number of

seed examples now from those huge number

of seeds we have a big database we could

have hundreds of thousands of examples

we create lots and lots of features

and now instead of iterating we simply

take all of those features and build a

big supervised classifier supervised by

all these facts that we know are true in

our large database so like supervised

classification in distance supervision

we are learning a classifier with lots

of features that's supervised by the

detailed hand created knowledge in some

database but we're not having to do the

complex iteratively expanding at

patterns that we saw in the C based

method but like unsupervised

classification distant supervision lets

you use lots and lots of unlabeled data

and it's not sensitive to the genre

issues that you might have in supervised

classification so let's see how it works

for each relation let's say we're trying

to extract the born in relation we go

through each tuple in some big database

of the born in relation so we have lots

of born in relations Edwin Hubble born

in Marshfield Albert Einstein born in

ohm we find sentences in some large

corpus let's say with using the web that

have both entities so here's a bunch of

sentences we might find Hubble was born

in Marshfield Einstein born 1879 comma

ohm Hubble's birthplace in Marshfield

and so on lots of sentences and now from

all of those sentences for all those

different entities we extract frequent

features so we might parse the sentences

we might just use the words in between

they might have some named entities we

might have parts of speech tags all

sorts of things we extract lots and lots

of features and now we take all those

features and do exactly what we did for

supervised classification we have a ton

of features and we have a large training

set and now we just train a supervised

classifier we'll need like any

supervised classifier we need examples

in the training set of positive and

negative instances we've fact our

positive instances from what we've seen

in the database so person a particular

person Albert Einstein being born in ohm

is a positive instance so from our

supervised classifier we can get a

probability of the born in relation for

a particular data point and now what we

can condition that on all sorts of

features we can extract from each

sentence it's a huge number of features

most recently there's been a number of

algorithms for doing unsupervised

relation

extraction often called open information

extraction where the goal is to extract

relations from the web with no training

data and no realistic relations we just

go to the web and pull information out

and here's the the banker with all

algorithm called the text runner

algorithm they first use parse data to

train a classifier to decide if a

particular relationship to click tuple

is trustworthy or not and so they have a

small amount of parse data where they

can use very expensive parse features to

decide that a subject and a verb and an

object or in a likely to be in a

relation between a classifier that can

do that any relation and now they walk

through the very large corporates let's

say it's the web in a single pass and

they just extract any relation between

NPS and we keep them if the trustworthy

classifier says this is likely to be a

relation between the two entities and

then we rank these relations based on

redundancy if we see a relation

occurring a lot of times between two

entities on different websites then we

guess this is a real relation and this

open information extraction algorithm

extracts relations like FCI specializes

in software development or Tesla

invented coil transformer and so on

where we can extract a virtually

infinite number of possible relations

between any entities all we have to do

is see them often enough times on the

web how do we evaluate the

semi-supervised in the unsupervised

relation extraction algorithms so since

all of these are extracting totally new

relations from the web or from some

large corpus

there's no gold set of correct instances

we can't pre label the web with all the

relations that are on it if we could do

that we'd be done and that means that we

can't compute precision because we don't

know which of the new relations we've

extracted are correct now we can't

complete compute recall because we don't

know which ones we missed so what do we

do we can compute an approximate

precision and we do this by drawing a

random sample of relations from the

output of the system and we just check

the precision of those manually so we

take some random sample we pull out some

the relations in that sample and we

measure how many of those were correct

and that will tell us

an estimate of the precision of the

system and we can also do that at

different levels of recall so we could

take let's say our relations are ranked

and we got a probability there's

probably a ranking we could take the top

thousand by that ranking compute the

precision for some sample of a hundred

of those or take the top ten thousand

compute the precision for some sample of

a hundred of those and so on and get the

precision at different levels of recall

so in each case we're taking a random

sample but there's no way to evaluate

the actual recall the complete recall of

the system and without labeling the

entire database which of course we can't

do so send me supervised and

unsupervised algorithms for named entity

extraction are two of the most exciting

areas in modern research and extracting

relations and in general the use of

rules with named entities to extract

relations the use of supervised machine

learning and these new unsupervised

approaches are all ways to solve the

very important task of relation

extraction one of the core parts of

information extraction
