in this segment I'm going to talk about

why NLP researchers adopted an empirical

approach using data and statistics for

making progress on the problem of

parsing the structure of human language

sentences what was the situation that

existed before that well the classic way

of parsing by which I mean before 1990

was that people would write a grammar

which might be a phrase structure

grammar or context-free grammar those

two terms are equivalent or might be

even a more complex format and the

lexicon so something like this baby

example that I've shown here and once

you had a grammar of that sort you could

then use a grammar parsing system or a

proof system to find aloud parsers for a

string of words the problem with this

approach is that it's scaled very badly

to the kind of sentences that people

typically write and say and didn't give

a suitable solution to that problem and

let me try and give some indication of

the problem so consider the full version

of this sentence that we use as an

example before Fed raises interest rates

0.5% an effort to control inflation if I

as a linguist write a grammar that's not

completely crazy but is the smallest

possible grammar that can parse that

sentence and then say Pazza parse this

sentence what I find is that the grandma

already allows 36 parcels for that

sentence if I write a slightly more

general grammar which will allow other

things that commonly happen in that

drawing which sentences

well then suddenly my grammar allows 592

parsers for this sentence and if we then

look at the kind of broad coverage

grammars that are actually used in the

statistical parcels we'll talk about

soon and I try and pass the sentence I

find that this sentence has millions of

parses so you can see that we've got a

problem

that's kind of out of control so

classical parsing was stuck between two

problems on the one hand you could try

and put a lot of constraint on grammars

to limit their ability

trove generate weird and unlikely

parcels for sentences but the problem

was that to the extent that you did that

the grammar became very non robust

somewhere is quite common in traditional

systems that even if you gave them well

edited text like news wire articles that

you know something like one third of the

sentences would have no paths whatsoever

but on the other hand if you made the

grammar looser so it could pass more

sentences what you found is that even

extremely simple sentences started

getting more and more unlikely and weird

parsers and you didn't have any good way

to choose between them so what we need

is a system of grammar that's flexible

enough that it can deal with the

flexible ways in which humans use

language in their daily life but is

predictive enough that will it will

allow us to choose the correct or likely

parse for a sentence and so that's

precisely what the statistical parsing

systems that we'll look at in these

lectures allow us to do how can you

build a statistical parsing system

that's where the topic of data comes in

and so the the huge enabling factor for

being able to build statistical parsing

systems was the development of tree

banks and here I'm showing an example

sentence from the penn treebank which

was the earliest widely available tree

bank and still the most famous so what

we've shown examples of tree make

sentences before but what you actually

get in the tree bank is these kind of

structures so the sentence tree

structure is being indicated by these

parentheses which are nested the show

constituency if you're an old-time Lisp

programmer this should look to you like

as and indeed is Lisp s expressions so

this is giving the structure of a

sentence with noun phrases and verb

phrases and various other things so this

markings also will indicate empty

elements of various kinds and also sort

of various functional annotations so

this says that this is the subject of

the sentence and so once we had this

kind of annotated data within and

position in which we can use machine

learning techniques to train parsers to

say what is the most likely parse for a

sentence we're also in a much better

position to build robust parsers because

we have a lot of sentences that give us

some kind of indication of what kind of

flexibility of structure we need to

admit into our languages to be able to

pass typical language use and so this

was a revelation that if you're starting

off it actually seems like building a

tree bank is a lot of work with very low

payoff because the whole power of

writing rules of grammar is you can

write one rule and it generalizes to

thousands millions in fact an infinite

number of possible sentences but as

building a tree bank you're taking one

sentence at a time and parsing it so

that seems slower and less useful but

actually a tree bank gives us many many

useful things it gives us reusability

normally the parser of one person was

completely not used in developing the

positive another person was done

completely independently but now we've

been able to build many many parsers

part of speech takers etc off pin

treebank data and it also has other

wider user users so for example many

linguists now use the pin treebank as

the source for testing out and

developing hypotheses about language the

pin treebank gives us something that is

broad coverage it gives us the

statistics that allow us to choose

between parsers and it gives us another

thing that's improved to be incredibly

important it gives us a way to evaluate

parsers on a common testbed so that we

have good data on which parsers are

better or worse than other parsers now

of course these days the pin treebank

isn't the only parser there are now

dozens of tree banks they're both tree

banks for different languages and tree

banks for different genres the original

penn treebank was just newswire it's

been extended to some other genres but

now there are other tree banks that

cover things like well biomedical

which is one popular domain but also

other things like well here in various

kinds of informal texts and more

specialized kinds of English usage such

as tree banks of questions ok so I hope

that's motivated for you why it's been

important to take an empirical approach

and developing parsing systems for human

languages and the great value that would

be able to get from having data

resources like tree banks
