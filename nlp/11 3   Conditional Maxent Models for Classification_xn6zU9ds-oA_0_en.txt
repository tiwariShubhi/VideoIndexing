the models that we've been looking at

and to motivate maximum entropy models

have been joint models we've just had

some data and we've been putting a

probability distribution over that

observed data so what do these joint

models of the probability of X have to

do with a conditional model of the form

probability of C given D that we were

building beforehand the answer to this

question is that we can think of the C

cross d space as a complex X and in

particular for those applications we're

working with the set of classes is

generally small you know maybe there's

something like 2 4 100 topic classes

part of speech tags named entity labels

whereas on the other hand D is generally

huge

so D is the space of all possible

documents which is minimally humongous

and possibly infinite in principle we

can build models over the joint space of

CD this will involve calculating

expectations of features over CD such as

is shown in the equation here but in

this equation we're having to sum over

the joint space and in general that's

impractical because we cut it enumerate

all the members of X effectively it's

just far to bigger space so D may be

huge or infinite but only a few D occur

in our actual training data if at the

end of the day we're training on a

million words of training data or a

million documents in a document

classification system then we have at

most 1 million different D and often in

practice we'll have quite a few less

because of repeats so something we could

try doing is adding an extra feature to

our model for each D and constrain its

expectation to match our empirical data

so we're saying that the probability of

each D is its observed probability and

what that we'll end up doing is giving

all of the probable

ms2 the observed documents and saying

that all the rest of the entries of PCD

will be 0 and we're showing that here so

now all probability mass is going to the

observed D and all of the rest of them a

given probability 0

that seems a slightly crude thing to do

but it has a clear practical benefit

because now we have a much easier sum

because now we only have to sum over the

cases which we saw in the data and so

then we have to iterate over the

different possible classes but we don't

have to iterate anymore over all

possible data items so if we've

constrained the D marginals in this way

then an estimating our model the only

thing that can vary is the conditional

distributions so we've rewriting it as

this form and then we're saying that the

probability of D has to be the observed

probabilities of D and so to maximize

the likelihood that the model gives to

the data the only degree of freedom

we're left with is adjusting this

conditional probability distribution

here this is the connection between

joint and conditional maximum entropy or

exponential models conditional models

can be thought of as joint models with

marginal constraints where we exactly

match the distribution of the observed

data in this constricted model form

maximizing joint likelihood and

conditional likelihood of the data are

actually equivalent because the joint

likelihood with the constraint of

matching the marginals is the same as

maximizing the conditional likelihood

okay so I hope that's enough of an

introduction to have made sense of how

you can view exponential or log linear

models as models that maximize entropy
