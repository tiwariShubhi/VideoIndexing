in this segment I'm going to return to

dependency parsing right in the first

segment I introduced the idea of

dependency syntax now let's look again

and how that worked so the idea of

dependency syntax is you connect up the

words of a sentence by putting arrows

between them that show relationships are

being modifiers or arguments of other

words so here in this example we've got

the head of the whole sentence submitted

and it's got its dependents so it's got

bills submitted by somebody and then

also the axillary verb were now not

necessarily but quite commonly the

arrows are typed by the name of some

grammatical relation and so we can see

that here we've got the subject of a

passive the verbal exhilaration the

prepositional relationship the other

things that you should know about just a

bit of the terminology so firstly when

we have an arrow we always have the

thing that is the head or the governor

here submitted and the thing that is the

dependent modifier and FIRREA various

words are used as you can see over here

here bills so that they're the two ends

though the governor and the governing of

a dependency now Oh beyond that there's

actually some inconsistency about how

things are done

so in these slides and in the original

dependency grammar work of kaznia the

arrows run from the governor to the

dependent but you can absolutely also

find other work that points the arrows

the other way and actually if like here

you're using height in the tree to show

what's dependent of what you actually

don't need to have any arrows at all you

could just draw these as lines without

any arrow head on them okay so as in

this example normally what you find is

the dependencies form a tree so that

there's a root node and then from

they're everything heads down with words

having a single head and in a nice

cyclic manner I should mention also

there's actually quite common to add

sort of one pseudo node at the top often

called root or wall which points at the

head of the sentence and that actually

makes things a lot cleaner both in terms

of the parsing algorithms but also in

terms of things like evaluation and

representation because then you get the

property that every word of the sentence

including the root is the dependent of

one thing and so you can think of it as

doing an assignment process of working

out what is the governor of each word of

the sentence

[Music]

how does dependency grammar relate to

the kind of free structure grammar that

we've concentrated on so far well the

syndrome innovation is really that

dependency grammar is built around the

notion of having heads and dependents

whereas the basic case of a context-free

grammar there's no notion of a hint

whatsoever but actually things are moved

on from there if you look at modern X

modern linguistic theory that means

things like x-bar grammar - linguists or

i'm one of the statistical parcels went

to China at Collins or Stanford partha

all of them have a notion of head and

use it extensively so for example and

all these parsers there's a notion of

head rules where to identify some

category as the head of a larger

category and as soon as you have head

rules of the kind that we discussed

before well then you can act as

straightforwardly get the dependencies

out of a phrase structure representation

so basically you kind of have a spine of

head chains and then everywhere you have

something coming off that that's a

dependent so we have a dependency from

walk to sue and a dependency from walked

into this is another hid chain and then

we've got another dependency from in to

store head chain and

the store though so we have the basis

for a dependency representation inside a

very structure tree if and only if we

have heads represented what about if we

go on the opposite direction if you try

and go from dependencies to phrase

structure you can reconstruct a phrase

structure tree by taking the closure of

the dependencies of a word and saying

that those represent a constituent but

it slightly changes the representation

from what we normally see in phrase

structure trees in particular in a

situation like this you can't have a VP

node because actually both Sue and into

our dependence of walked and therefore

all three of those must have a flat

phrase structure representation where

you have the three the head and it's too

dependent suing into how do people go

about doing dependency parsing a whole

variety of methods have been used for

dependency parsing one method to do it

is with a dynamic programming algorithm

like the CKY algorithm that we saw for

free structure parsing now if you do

this naively by adding in heads you end

up with something similar to the

lexicalized probabilistic context-free

grammars we saw earlier and end up with

an O begin to v algorithm but there's a

clever reformulation of what the parse

items are due to JSON eyes in the 1996

and which makes the complexity of doing

dependency parsing also in cubed which

is kind of what you'd hope it to be I'm

just thinking about the nature of the

operation but there are a whole bunch of

other methods

so people have directly used graph

algorithms to do dependency parsing so

one idea from the algorithms literature

is that you can construct a maximum

spanning tree for a sentence because

since you want all words connected

together to be the dependent of

something that means you have to build a

tree that spans all the words in the

sentence and that's the idea that's used

in the well-known MST parser I'm their

other idea

years of constraint satisfaction where

you start off with an inset of edges

between all words and then eliminate

ones that don't satisfy hard constraints

but a final trend in dependency parsing

and actually what we're going to focus

on here is a way of doing dependency

parsing where you've had left to right

through the sentence and make greedy

decisions based on machine learning

classifiers as to which words to connect

to other words as dependents and so the

most well-known example of this

framework is moult parser and I'm going

to concentrate on this just partly

because it's very different of what we

did for our approach to free structure

parsing we looked at in depth but also

because it's been shown that this kind

of method of doing dependency parsing

actually works extremely well it can

work accurately and exceedingly quickly

so it's just a good thing to know about

as a different point in the space no

matter how we do dependency parsing we

need some sources of information to let

us choose between possible analyses and

which words to take as dependents as

other of other words so here's a list of

the main sources of information people

use so the most obvious source of

information is by lexical dependencies

so if we have something like a

dependency between issues and the well

we can look at the word that's the head

and look at the word of the dependent

and say is that likely that's similar to

the by lexical dependencies of our

earlier lexicalized pcfg s but we don't

want to use that as our only source of

information partly because lexical

information is so sparse and there are

several other good sources of

information so let's just go through

those so one is the distance between the

head and the dependent and if you look

at this picture what you'll see is that

most dependencies are short there with

nearby words there are a couple of

exceptions so this dependency here is a

pretty

on one but most of them are pretty short

other sources of information

what is the intervening material so in

general dependencies don't cross over

verbs and commonly they don't cross over

punctuation some exceptions commas are

quite often crossed over so looking at

the words in between can give

information about whether a dependency

is likely or not a final source of

information is looking at the valency of

heads and that's saying for a particular

word what kind of dependence does it

typically take so a word like that

typically takes no dependence on the

left and no dependence on the right as

in this case here on the other hand if

you have a word that is say a noun it

will take dependence like adjectives and

articles on the left but it won't take

those kind of dependence on the right

but it can take other kinds of words as

dependence on the right for example take

prepositional phrase modifiers or

relative clauses as dependent on the

right so you can develop a quite rich

typology of what kind of dependence

words take okay that should give you a

better sense of what dependency

representations look like and the big

picture of how we go about parsing with

them

in the next segment we'll introduce a

concrete algorithm for dependency

parsing
