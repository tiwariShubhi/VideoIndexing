while it's great that there are

sentiment lexicons available for lots of

other purposes we'd like to build our

own sentiment lexicon and the way we do

this is often by met by semi-supervised

learning and the idea of semi-supervised

learning is we have some small amount of

information maybe we have a few labelled

examples or maybe we have a few

hand-built patterns and from that set of

data

we'd like to bootstrap a complete

lexicon and we might want to do learning

of lexicons instead of taking an online

lexicon if we're looking at a particular

domain that maybe that doesn't match the

domain of the lexicon that was built or

we're trying to do a particular task or

maybe we just think that our online

lexicons might not have enough words

that are relevant to the topic we're

looking at one of the earliest ways of

inducing this kind of sentiment lexicon

was proposed in 1997 by hot savate Salo

glue and McEwen and I'm going to show

you this because the intuitive oh the

paper is old this intuition is included

in almost all modern ways of doing this

semi-supervised learning of getting a

lexicon and their intuition very simple

is that two adjectives if they're

conjoined by the word and they probably

have the same polarity and if they're

conjoined by the word but they don't so

if you see on a very high frequency of

fair and legitimate or corrupt and

brutal probably fair and legitimate are

both on the same polarity and and so we

might suspect that fair and brutal are

just less likely to occur on the web or

in some corpus and so if we see a lot of

something occurring with and maybe the

two are likely to have the same

sentiment but but if we see two words

linked by but there probably have

different sentiments of fair but brutal

more likely to occur than fair and

brutal and here's how they use this

intuition they first labeled by hand a

seed set of 1,300 adjectives and so they

had about a similar number of positive

and negative adjectives so sensual or

clever or famous or thriving and

negative adjectives like ignorant or or

listless or unresolved

and now from the seed set they expand

that to any adjective that's conjoined

with a word in their seed set so for

example we can go to Google and we can

type in was nice and and look at what

words occur next and what do we see

after was nice and what we see was nice

and helpful so that tells us that nice

and helpful are likely to have the same

sentiment and here we see nice and

classy so that tells us that nice and

classy might have the same sentiment and

we can do that

for for all sorts of words that occur

conjoined with our seed set and we take

any word that occurs frequently enough

in the right conjunction with our seed

set words and now we can build from that

a classifier and the job of the

classifier is just to assign for each

pair of words how similar the two words

are and so we can give the classifier

the count of the two words occurring

with an and in between them and a count

of the two words occurring with a but in

between them and it the classifier can

learn that nice and helpful are somewhat

similar nice and fair very similar fair

and corrupt are very dissimilar marked

them in red with a dotted line because

maybe but occurs much more often between

these two and and occurs more often

between these two and so on so we can

get between any pair of words a number

that indicates its similarity and now we

can just cluster the graph and use any

kind of clustering algorithm and in and

we get for the words that tend to be

linked together helpful and nice and

fair and classy we'll get that these are

linked together and brutal and corrupt

and irrational linked together and we

can get our our output polarity lexicon

and here I've shown you an output

polarity lexicon and of course it'll

have some mistakes in it so see if you

can find the mistakes in this lexicon

here are some of the mistakes so we have

disturbing as a positive word or strange

as a positive word or pleasant as a

negative word

so of course there's going to be errors

in any of these kind of automatic

semi-supervised to algorithms so while

the hot tsubasa low glue and McEwan

algorithm automatically finds the

sentiment or the polarity of individual

words it doesn't do well with phrases

and we'd like often to get phrases as

well as words so the tourney algorithm

is another way to bootstrap in a

semi-supervised Dawei lexicon and what

this does is extract a bunch of phrases

from reviews learn the player either

reviews and then take all the phrases

that occur in a review take their

average polarity and use that to rate

the review so let's look at these stages

in the turning algorithm first we're

going to extract every two-word phrase

that has a particular set of part of

speech tag and we haven't talked about

parts of speech yet but so this is

adjectives the tag for adjectives so if

the first word is an adjective and the

second word this means noun and this

means plural noun so if the first word

is an adjective so an adjective followed

by a noun or plural noun will extract

that phrase whatever the third word is

or if the first word is an adverb Arby

means adverb and the second word is an

adjective and the last word is not a

noun we'll extract that phrase so adverb

adjective adjective adjective noun

adjective adverb verb so these are

particular phrases so we're gonna run

our part of speech tagger I'll talk

about that in a in a few lectures and a

sign for each word a part of speech

adjective noun and so on and then we'll

extract two-word phrases that meet these

criterion the first word is this tag

second word is this tag the third word

has some constraints so that we don't

extract it and now we look at all those

phrases and our goal is for each of

those phrases we've extracted to measure

its polarity so how positive or negative

is a particular phrase and the intuition

of the turning algorithm is just like

the hots of a solo gluon queue an

algorithm to think about co-occurrence

so we ask well a phrase is positive if

that Acco occurs a lot

by let's say on the web or some large

corpus with the word excellent a

negative phrase is likely to co-occur

more often with a word like poor so how

are we going to measure this

co-occurrence the standard way to

measure this kind of co-occurrence is by

point wise mutual information point wise

mutual information and that's a variant

of a standard information theoretic

measure called mutual information the

mutual information between two random

variables is the sum over all the values

the two variables can take of the joint

probability of the two times the log of

the joint over the individual

probabilities so point wise mutual

information takes this intuition for

mutual information and just asks a very

simple computes a very simple ratio the

probability of two events x and y

divided by the product of the individual

probabilities so what point wise mutual

information is asking it's a ratio is

how much more to - events x and y

co-occur than if they were independent

if they were independent they would have

these independent multiplied

probabilities and and how much does the

joint occur more often than we'd expect

from independence it's the intuition

that point wise mutual information so

looking at point wise mutual information

I've just repeated the equation here um

we can look at it with between two words

we say how much more do these two words

word one or to co-occur than if they

were independent by taking the ratio the

probability of the two words occurring

together divided by the probability of

each word multiple separately and

multiplied and we take the log of that

how do we estimate this well the way

attorney did it originally is by using

the AltaVista search engine and we're

gonna estimate the probability of word

just by how many times how many hits we

see for that word and we're gonna

estimate the probability of two words by

how often word one occurs near word two

so although Vista has the near operator

that that lets us check if a word is

near another word and in each case we're

gonna want to normalize to get a real

probability from these counts and by our

definition from the previous slide point

wise mutual information it's

the probability of the joint so--that's

hits of word one near word - over N

squared / hits of word 1 / n / hits of

word and 2 over n and these ends are

going to cancel and we're going to end

up simply with with this equation so how

many times do the words occur near each

other how many times do they curve

individually multiplied together so once

we have a measure of co-occurrence of

how how much a phrase co-occurs with the

word excellent other word good we can

now determine its polarity so the

equation for polarity in the turning

algorithm is just how much is the mutual

information the point-wise mutual

information of the of any phrase with

the word excellent and we subtract its

mutual information with the phrase poor

so how much more does the phrase appear

with excellent than poor or vice versa

um and just doing a little algebra so

that's the the number of here's the

definition of point whence mutual

information of a phrase with excellent

how often it occurs near the word

excellent divided by the individual

number of hits of the phrase itself an

excellent and then subtract the same

thing for poor and of course by the

definition of logs we can bring things

inside the log and turn the minus into a

divide and then we can do some some

messing with terms so we have hits a

phrase here and we have hits a phrase

there and so in the end we can get our

formula for deciding the polarity of a

phrase it's just how many times does the

phrase occur near excellent times how

many times it occurs with the word poor

occurs over how many times the phrase

occurs in your pour over how many times

excellent occurs maybe sir these are

constants that we can get once and then

for each phrase we can compute these two

quantities and using the Turney

algorithm here's a positive review

obviously of a bank and so here's a

phrase like online service adjective

noun phrase and here's the polarity

assigned by it 2.8

another one direct deposit 1.3 um and

here's some negative phrases with

negative polarity so the phrases occur

more often near the word poor than near

the word excellent so in Kamini located

has a negative polarity so on average

though more I've just shown you a subset

more of the positive phrases occur in

this review than the negative phrases

and the average polarity is positive so

this is a thumbs-up review a thumbs down

review you can see that phrases like

virtual monopoly negative polarity

lesser evil negative polarity other

problems negative polarity all occur

more frequently and on average have a

more negative polarity than the positive

polarity words that occur in this review

and we end up with a review it has an

average negative polarity so the turning

algorithm um was evaluated on various

kinds of review sites and does a better

job than just the majority class

baseline at predicting the polarity of a

review so what's important about the

turning algorithm is that it lets us

look at phrases learn phrases rather

than just words and what's true about in

general about these unsupervised

algorithms is we can learn

domain-specific information that might

not be in some online dictionary so if

we're doing if I go back to the previous

slide if we're doing banking a word like

direct deposit or virtual monopoly might

simply might not be in an online

polarity dictionary or online web and so

we need to use some of these

semi-supervised delegate UM's for

learning the polarity of these kind of

words finally I mentioned briefly a

third algorithm for learning polarity

and this uses word net again that's the

online thesaurus that we'll talk about

in detail later and the intuition of

using these online thesauruses is as

similar to what we've seen in the in the

first 270 supervised algorithm we'll

start with a positive seed set and a

negative seed set so we might have words

like good and the positive seed set and

terrible in the negative seed set and

now we use the thesaurus just

simply to find synonyms and antonyms so

we add synonyms a positive words like

well and antonyms of negative words to

the positive set and to the negative set

which we started out with words like

terrible now we add synonyms of all

those words so maybe awful as a synonym

of terrible and antonyms of our positive

words to the negative set and we just

repeat and the sets grow as we keep

adding synonyms and antonyms to it and

then each of the algorithms that make

use of ordinate for learning player do

will have various ways of filtering out

bad examples or problems with word

senses and things like that so in

summary we learning lexicons can help us

deal with domain specific issues in

banking we might have a word like direct

deposit that's just not going to be in a

standard online polarity lexicon and we

can be more robust in general as new

names or people start using new names or

a new company name it might not be in

some training data but but we might be

able to learn it from the web or from

the data we're looking at and again the

intuition of all these algorithms is

really the same we start with some seed

set we find other words that have

similar polarity to that seed set in

some semi-automatic way and the ways

we've seen are using whether the cook

words are conjoined with and or but

using words that just occur near by two

words like poor or excellent so that's

our seed set there or using word net

synonyms or antonyms
