up until now I've referred to the models

that were using as maximum entropy

models but actually what we've defined

is these likelihood maximizing models

which are defined within a certain

exponential model form in this part I

want to show why these models are also

referred to as maximum entry models and

what the motivating intuition is for the

maximum entropy idea the motivating

intuition for maximum entropy models is

that in general there are tons of

probability distributions out there most

of which are very spiked and specific

and would tend to over fit on particular

data items what we want to do is to find

a distribution that's as uniform as

possible except in places where we know

that there's some reason to believe that

the probability distribution isn't

uniform uniformity can be thought of as

high entropy we search for distributions

which have the properties we desire but

which also have high entropy so this is

kind of embodying a statement of Thomas

Jefferson's ignorance is preferable to

error and he is less remote from the

truth who believe nothing than he here

believes what is wrong so what we're

wanting to do is not have any beliefs in

our model that we haven't particularly

stated to the extent that we haven't

stated any constraints in our model we

want the probabilities to be as uniform

as possible so entropy is a quantity

that measures the uncertainty of a

distribution so if we have an event X

which has some probability we can work

out the surprise of that event by taking

the log of the inverse of the

probability so if you see an event with

a very which you think has a very small

probability of happening your surprise

is then great if you thought it had zero

probability of happening your surprise

is infinite on the other hand if the

event had a very high probability of

happening your surprise is small in

particular if you think it had

probability one then your surprise is

zero we can then work out the entropy of

a distribution by taking the expectation

over the surprise which can be written

out in this

form here and this gives us the equation

for entropy so here's an example of that

with a simple case of flipping a

possibly weighted coin so if the coin

always comes down heads or always comes

down tails then there's no entropy in

the distribution and the entropy is

maximized when the coin is equally

likely to come down heads or tails let

me go through some concrete examples

that show what happens when we try and

maximize the entropy of a distribution

ie to minimize the commitment so we're

starting off with no beliefs and then in

some ways we're going to reward a

probability distribution that resembles

some reference distribution which for us

we're going to be taking straight from

the observed data so what we're going to

say in our model is that we want to

maximize entropy subject to feature

based constraints and precisely what our

feature based constraints are is to say

that the expectations of the values of

features in the model should be the same

as the empirical expectations of those

features over our observed data so every

time that we add features that that puts

constraints into the model and therefore

it lowers the maximum entropy but on the

other hand it raises them likelihood of

the observed data it takes the

distribution further from uniform but it

brings the distribution closer to the

data so here's a very simple example of

that so here was our unconstrained

entropy distribution which has its

maximum right here if we put in the

constraint that said the probability

beheads was 0.3 then we've constrained

the distribution to just a single point

and so at this point the constrained

distribution has a lower maximum entropy

than we had before that's example in one

dimension is so simple it's hard to see

much so let's do a slightly more complex

example in two dimension

so here we have two two probabilities

the probability of heads and the

probability of tails so let's just

assume those are two numbers between 0 &

1 and we haven't even modelled the fact

that heads and tails are in

complimentary distribution well then if

we model this entropy surface that we

find that the maximum entropy comes

about when the probability of P of X the

P of H and P of T is around there and

why is that well that's because for the

components of the entropy distribution

minus X log X they have their optimum at

the value of 1 on each edge is about

0.38 point 4 or something like that so

that's not what we normally see when we

see the entropy picture for a coin and

that's because normally we immediately

put in this constraint saying that heads

and tails are in complementary

distribution the sum of their

probabilities have to add up to 1 and so

then once we do that we've constrained

the space by saying we have to be are

somewhere along this line and then we're

in the situation that entropy is

maximized by having the probability of

heads equaling the probability of tails

equaling 1/2 but again we could sort of

stick in one more constraint and say the

probability of heads is 0.3 and then

again we've constrained the distribution

down to a single feasible point and the

point to notice is that with each of

these constraints the maximum entropy of

the model has gone lower so here at the

true maximum of the function now we're

sort of away from the true maximum of

the function but still at a reasonably

high entropy point and here we've

wandered even further from the maximum

of the function so our maximum entropy

is going down but we're modeling facts

about the situation in the world that we

want to model

let's look at a concrete example that's

a little bit closer to a natural

language problem now let's suppose we

have this event space so we have parts

of speech where here we just have six

parts of speech our nouns plural nouns

proper nouns proper plural nouns and two

verb parts of speech and this was our

empirical data so that we saw 36

different events and of those the most

common things we saw were proper nouns

which were actually two-thirds of the

data and then we saw some regular nouns

and we saw a few verbs right so if we

have a probability that each of these

events and we just say maxima and they

sort of P 1 P 2 P 3 and we say maximize

those probabilities again the maximum

entropy distribution is by sending each

of them to the value 1 on e but that's

not what we want we want to say that

this is a set of categorical

probabilities and so then the maximum

entropy distribution is to say that each

of those probabilities is 1/6 uniformity

is the maxim entropy distribution in

categorical distributions and that's too

uniform given what happened in the data

that nouns are much more common than

verbs in our data so we're going to add

a feature that has very one if they take

is a noun and zero otherwise

well the expected value of that feature

is 32 over 36 on 8/9 that's just using

the data from the previous slide so if

we add that constraint and then say

what's the maximum Trippi distribution

it's going to set the probabilities so

that this constraint is satisfied the

sum of these probabilities equals 32

over 36 but within that category

probability mass is still going to be

distributed uniformly because that

that's the maximum entropy distribution

similarly within the verb classes you're

going to get the remaining probability

mass distributed uniformly at that point

we might notice that

proper nouns are much more frequent the

common nouns so we can add a second

constraint that is a feature that has

value one when the tag is a proper noun

and the expectation of that feature is

2/3 2/3 of the data of proper nouns as

we noted before if we put that into the

model we then get the expectation of

that feature observed that two-thirds of

them probability mass goes to proper

nouns which is again distributed

uniformly we still have the feature that

32 36th of the problem or the mass goes

to some kind of noun so the remainder is

distributed evenly among the other nouns

and this is as before now of course we

could keep on refining our model we

could for example say add a feature to

say that singular nouns comprised a

certain amount of the data so singular

nouns comprised eighteen thirty sixth of

the data we could add that constraint in

and eventually if we added in enough

constraints we'd force the distribution

to be exactly the same as the empirical

distribution it's very easy to see that

maximum entropy models are convex models

so what's the idea of a convex function

the idea of a convex function is here F

is that if you take the function value

at the weighted mean of any set of

points then that function value is

greater than what you do if you take the

what function value at those points and

then weight those function values that

you've found that we kind of above it up

here

and so that's distinguished from

something like a non convex function

where you can have these local minima

and so then the function value at this

average point is beneath what the

function value is if you just take the

average of the values of the two red

points convexity guarantees that a

function has a single global maximum

because any higher points of the

function are greedily reachable in the

maximum entropy formulation it's easy

to see that we have a convex function so

we can start off by showing that the

entropy function is convex and so we

have that minus X log X is a convex

function and so therefore if we take a

sum and we drew it over here and that's

a convex function if we then take a sum

of convex functions that's always convex

then after that what we're doing is

adding constraints to the function and

so the feasible region of a constrained

entropy function is a linear subspace of

it which then also has to be convex so

like when we put a linear subspace

constraint right here through our

entropy surface that we've still got a

convex function coming out of it

and so I'm not going to show it here but

the same is true for our regional

maximum likelihood presentation that we

get a convex function okay well I hope

you now understand where the name MaxEnt

models comes from and what the key idea

of the maxim entropy principle is
