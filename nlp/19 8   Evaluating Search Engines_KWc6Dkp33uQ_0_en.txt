in this section I'll tell you a little

bit more about evaluating the quality of

a search engines

there are many measures for the quality

of a search engine there are technical

ones such as how fast does it index and

how fast does it search we can look at

things like the expressiveness of the

query language whether they're able to

express complex informational needs with

things like phrase queries negations

disjunctions people have other desires

like having an uncluttered UI and a

system that doesn't cost a lot to use

all of these are measures that are

measurable that we can quantify them and

we can get some kind of score of what is

their goodness

but in practice all of those measures

fall important tend to be dominated by

another measure of user happiness that

is the user happy and using this search

engine and speed a response and size of

the index has certainly factors but by

themselves blindingly fast useless

answers won't make a use a happy so a

huge part of user happiness is are the

results return results that they want

and so that's the metric of relevance of

results to a users information need

I mentioned this right at the beginning

but just to reiterate once more when

evaluating the IR system that we

evaluate with respect to an information

need so an information need is translate

into a query and that's what the IR

system actually runs but relevance is

assessed relative to the information

need not the query so for example if the

information need is the person is

looking for information on whether

drinking red wine is more effective than

white wine for reducing your risk of

heart attacks now come up with some

query for example it might be wine red

white heart attack effective and that'll

be submitted to a search engine and in

evaluating the effectiveness of the

search engine and returning relevant

results we're not asking are the results

that the search engine returns documents

that simply have those words on the page

rather we saying do these

commence address the users information

need okay well how can we go about doing

that well if the search engine returns a

set of results well then what we can do

for evaluation is if we start off with

three things if we have some benchmark

collection of documents that we can use

for evaluation and we have some

benchmark set of queries which are in

some sense representative of information

needs of interest and then we've

gathered this third thing which is

Assessor judgments of where the

particular documents are relevant the

particular queries commonly in practice

this can't be a verb assembled

exhaustively certainly not if the

document collection is large but at

least for a particular set of documents

that return by particular search engines

we can get the Assessor to judge whether

those documents are relevant to the

queries well if we have a results set

with just these three things we're in

business because we can use exactly the

same measures that we looked at

previously a precision recall on the F

measure that combines them and these are

suitable good measures for exactly the

same reason that there are good measures

from when we were talking about things

like named entity recognition that

normally only a few documents will be

relevant to a particular query and so we

can measure that much better by looking

at these measures of precision and

recall but what if we've now moved on to

a search engine that returns ranked

results we can't just totally

straightforwardly use these measures of

precision recall an F measure because

the system can return any number of

results in fact the number it returns

normally depends on how often we keep on

clicking asking for more but if we sort

of look at any initial subset of the

results we can then work out the

precision and recall for that subset and

then by putting them together we can

come up with a precision recall curve

let's look at how that works so here are

the first ten results for a search

engine where we've labeled each result

is either relevant

or not relevant according to an

Assessors judgment and so then we can

take any initial subsequence of these

documents and work out a recall on

precision so for the first document the

system got it right it's a relevant

document and let's assume that overall

there are ten relevant documents in the

collection so it's gotten one out of the

ten relevant documents and so it's

recall is 0.1 and well since that

document was relevant the system was

right on the first answer its precision

is one at this point well the next

document was not relevant so the recall

of the first two documents we're down to

here now is 0.1 and the precision is now

0.5 another not relevant document so the

precision is sorry the recall is still

0.1 and the precision is now dropped to

0.33 and the if we look at the set of

the top four documents we've now found

two of the ten relevant one so recall is

zero point two and our precision has

gone back up again to zero point five

the fifth one is also relevant so now

our recall is up to zero point three and

our recall is up to three out of five

zero point six and we can keep on going

down maybe you guys could work out what

some of these entries are down here the

other measure I want to mention one of

the most recently most used recent

measures is mean average precision if we

look at the rate retrieval results

ordered this way to give me a bit more

room and so the first document return is

irrelevant the second one is not

relevant say the third one is not

relevant in a relevant one and another

relevant one not relevant relevant

relevant let's say those are our top

eight results what you're doing for mean

average precision for first of all

you're working on average precision for

one query and so the way you do that is

by saying let's work out the procedure

at each point that a relevant document

is returned because that's when you're

increasing recall so here the precision

is one here that there are now four

documents of the precision as a half

here there are five documents of the

precision is 0.6 here there are seven

documents of which four of them are

relevant so that's this is around zero

point five eight you guys can correct my

arithmetic here we now have eight

documents of which five are relevant and

that's zero point six two five and then

what we're doing to work out the mean

average precision is we're kind of

keeping on calculating those numbers in

practice normally they aren't calculator

that exhaustively but they're calculated

up to some point let's say a hundred and

then you're calculating an average

function of all those numbers and that's

then the average precision for one query

you then calculate the same average

precision for all the other queries in

your benchmark query collection and you

again take the average of that and that

then gives you mean average precision so

in particular this is what's referred to

as macro averaging it's each query that

counts equally in the calculation of

mean average precision so this is a good

measure that evaluates to some extent

precision at different recall levels

while still being weighted most to what

the precision is like for the top few

returned documents and end at the level

of across different queries it's giving

equal weight to different queries which

tend to be a useful thing to do because

you're always interested in how systems

return work on queries of rare terms as

well as queries of common terms so this

is a pretty good measure to think about

using for evaluating information

retrieval systems okay

there's even more methods that I could

talk about that

probably a good sense of how about how

to go about evaluating the performance

of a rent retrieval system
