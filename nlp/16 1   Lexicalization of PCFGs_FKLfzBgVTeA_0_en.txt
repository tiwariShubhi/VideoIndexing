in this segment I'm going to introduce

the idea of lexical izing pcfg s so

let's look at our basic pcfg here and

see how the probabilities of rules

actually work so what we have in the

pcfg

is local trees corresponding to rules

such as verb phrase rewrites as vb d PP

and that has some probability attached

to it like maybe that probability would

be 0-3 about 3% on verb phrases expand

as a past tense verb and a prepositional

phrase or similarly we have other rules

so that we have up here a symptoms goes

to in PvP rule and that would have a

much higher probability maybe it's

probably the audio 0.4 or something like

that but the really important thing to

notice is that these rules make no

reference whatsoever to actual words so

this is saying that overall about 3

percent of verb phrases consist of a

past tense verb in a prepositional

phrase but whether that's likely or not

depends an awful lot on which verb we're

dealing with so in this example here we

have the word verb walked and walked is

the kind of motion verb that isn't

really really likely to have a PP

following after ed whereas if we had

another verb for example if we had the

verbs sore and there would be a past

tense verb but would be really unlikely

to have a prepositional phrase coming

after it's something like he saw in the

mirror or something like that you'd

normally get an a noun phrase object

after so first so it seems like we can

only really come up with good

probability estimates if we know more

about the words in the sentence and so

that's precisely what the idea of

lexicalization is the idea of

lexicalization is let's define for each

category a way of finding its head so

for a noun phrase we'll say that the

last noun inert

a proper noun or a common noun will be

declared its head and so this noun

phrase will say that its head is sue and

for this noun phrase here we'll say that

its head is the word store because this

is the last noun and the noun phrase and

we'll apply that to other ideas so for a

verb phrase the head of a verb phrase

will be the verb inside it so the head

of the verb phrase is walked and for

prepositional phrase the head will be

the preposition inside it into and will

say the head of a sentence is the head

of the verb phrase so we put in this way

lexical items that represent the head of

each phrase next to each non-terminal in

the grammar

let me give my more neatly printed

version of that here okay well what

happens if we do that well what we then

find is that we've now got these

categories like s warped which are a

combination of our old non terminals

plus some lexical item and so if we do

that we've enormous lis enormous lis

expanded our effective space of non

terminals because if we had something

like 20 non terminals before but we had

something like 30 thousand words in our

vocabulary well now we've got six

hundred thousand non terminals in our

grammar and so that suggests we need to

start doing some more special

engineering to be able to do that let's

not worry about that for a moment and

let's just think in terms of

probabilities what this will allow us to

do well anything that this will allow us

to do is now if we're looking at what's

the probability of this subtree we won't

just be saying what's the probability of

a verb expanding to a past it verb

phrase expanding into a past tense verb

in a PP will be saying what's the

probability of a verb phrase headed by

walk taking a PP and in particular a PP

that's headed by into and so we're now

going to be capturing inside the rule

two things

we'll be capturing both that a VP walk

is likely to take a pee-pee afterward

and we will be capturing the

relationship between the heads here so

we'll be Capulin capturing the

relationship that is reasonable to have

someone walk into something and so we

have a lot of much richer probabilistic

conditioning being captured by our

grammar this extra information will be

really really useful for resolving

various kinds of ambiguities so a

classic example that is prepositional

phrase attachment so if we want to work

out for a prepositional phrase whether

it is modifying an alum of precedes or

modifying a verb that perceives well we

can capture quite a lot of that inside a

pcfg

once we lexical eyes it because now

we'll have a rule where an NP rates is

taking on its right-hand side a PP 4 and

we can ask whether that is a likely

thing to happen or not or we can say we

have a VP announce is taking expanding

on its right-hand side to a PP in is

that a reasonable thing to happen or not

and so we can use this information to

better model PP attachments than we

could in a vanilla pcfg that doesn't

mean we can capture everything about PP

attachments in this model though and

actually if you want to you could think

a little about what other things that

you'd like to know about attaching a

prepositional phrase which aren't yet

captured in this model where we have a

head word pinned onto every phrase like

this but nevertheless this kind of head

lexicalization captures most of the

additional things that you'd like to

know to make the various parsing

decisions of the sentence so it's also

useful for things like coordination

scope knowing about the complementation

patterns of verbs and so on in

it was the case that doing this

lexicalization of pcfg s was seen as the

pausing breakthrough of the late 1990s

so that's a really useful notion to know

about is this idea of lexical izing pcfg

s to capture more of the necessary

promise take conditioning information to

make parsing decisions
