how do we deal with buy grams with zero

probability the simplest idea is called

add one smoothing and let's look at a

picture that gives us the intuition of

smoothing in general from Dan Klein so

suppose in our training data we saw

denied the allegations denied the

reports denied the claim denied the

request and so we computed probabilities

there were seven total things following

denied the and we can get our

probabilities of everything of each of

these things but we would like to say

well maybe denied the report might occur

I'm sorry denied the report was in the

training data denied the effort might

occur deny the outcome might occur so

we'd like to steal some probability mass

and save it for things we might not see

later so this is our training data and

this is the maximum likelihood counts so

these things occurred after denied that

and he's never occurred we'd like to

steal a little trip a little probability

mass from each of these words and put

that probability mass onto all other

possible words or some set of words so

that the zeros go away and the simplest

way of doing this is called add one

estimation or Laplace smoothing and the

idea is very simple we pretend we saw

each word one more time than we actually

did we just add one to all the counts so

if our maximum likelihood estimate is

the count of the of the bigram divided

by the count of the unigram our add one

estimate is the count of the bigram plus

one over the count of the unigram plus V

we have to add V here in the denominator

because we're adding one to every word

that follows word I minus one so our

denominator is increased not just by the

the total count of times that something

happened a word I minus one wasn't just

all the previous things that followed it

but each one of those got incremented by

one and there were V of them so we have

to add V to the denominator this is the

add one estimator and probability

estimator I keep using the term maximum

likelihood estimate

let's just remind you what that means

the maximum likelihood estimate of some

parameter of some model from a training

set is the one that maximizes the

likelihood of the training set given the

model so we have some training set and

we're gonna a maximum likelihood

estimator that lets us learn a model

from a training set is the one that

makes that training set most likely what

do we mean by this

suppose the word bagel occurs 400 times

in a corpus of a million words and I ask

what's the probability that a random

word from some other text will be bagel

well the maximum likelihood estimator

from our corpus is 400 over a million or

0.04 now this could be a bad fit for

that other corpus who knows whether the

other corpus bagel occurs four hundred

times per million or some other

probability but this estimate is the one

that makes it most likely that bagel

will occur four hundred times in a

million word corpus which is what it did

occur in our training corpus so we're

maximizing the likelihood of our

training data so an ad one smoothing and

any kind of smoothing is a non maximum

likelihood estimator because we're

changing the counts from what they

occurred in our training data to hope to

generalize better so if we go back to

our Berkeley restaurant project and we

add one to all of our accounts here's

our laplace smooth bigram accounts and

now all those zeros that we had have

become ones and everything else gets one

added to it so now we can compute the

bigram probabilities from those counts

and just using the laplace Laplace add

one smoothing equation that we saw

earlier and now we've got all of our

Laplace there are add one smooth by

Graham's so we have again the

probability of two given want is 0.26

and now all of those zeroes have turned

into other things point oh four 2.0 to 6

and so on now we can also take those

probabilities and reconstitute the

counts as if we had seen things the

number of times that we would have to

see to get

add one probabilities naturally so we

take our probabilities and we re

estimate the original counts as if they

were the numbers that would have given

us these probabilities and we ask one of

those reconstituted counts look like how

much of our add one smoothing changed

our probabilities so here's

reconstituted counts so we have I is

followed by want five hundred and twenty

seven times or Chinese is followed by

food eight point two times these are

reconstituted counts and let's compare

them to the original counts so um up

here here on the top we have the

original counts and here we have our

reconstituted counts and I want you to

notice that there is a huge change so an

original account um too far would want

six hundred and eight times in our

smooth counts two fouls want only two

hundred and thirty eight times so it's

it's almost a third some a third this

smaller three times smaller or

chinese-food occurs eighty two times in

our original counts and only eight point

two you know our reconstituted counts so

the the add one smoothing has made

massive changes to our counts and some

sometimes changing a factor of ten the

original counts in order to steal that

probability Menace to give to all those

massive numbers of zeros that had to be

assigned probabilities in other words

add one estimation is a very blunt

instrument it's it makes very big

changes in the counts in order to get

these probability mass to assign to this

massive number of zeros and so in

practice we don't actually use add one

smoothing for engrams will have better

methods if we do use add one smoothing

for other kinds of natural language

processing models so add one smoothing

for example is used in text

classification or in similar kinds of

domains where the number of zeros isn't

so enormous
