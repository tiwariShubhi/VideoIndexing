{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import nltk\n",
    "import pysrt\n",
    "import sklearn\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter youtube video link:https://www.youtube.com/watch?v=LPIQ_gyiHag\n"
     ]
    }
   ],
   "source": [
    "video_url = input(\"Please enter youtube video link:\")\n",
    "n_id = video_url.find(\"&\")\n",
    "if n_id != -1:\n",
    "    video_url = video_url[:n_id]\n",
    "\n",
    "cmd = [\"youtube-dl\",\n",
    "       \"--skip-download\",\n",
    "       \"--write-sub\",\n",
    "       \"--sub-lang\",\n",
    "       \"en\",\n",
    "       video_url\n",
    "    ]\n",
    "\n",
    "op_log = os.system(\" \".join(cmd))\n",
    "if op_log != 0:\n",
    "    print(\"Please enter a valid Youtube video link, which has english subtitle.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "files = os.listdir('./')\n",
    "for file in files:\n",
    "    if file.endswith('.en.vtt'):\n",
    "        file_path = file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wLrIejC4gOV5"
   },
   "outputs": [],
   "source": [
    "def text_to_word_list(text):\n",
    "    ''' Pre process and convert texts to a list of words '''\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "HCJApSJuTJ-P",
    "outputId": "3871082d-5a96-4a29-c136-902e089cc4ba"
   },
   "outputs": [],
   "source": [
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 1\n",
    "n_top_words = 25\n",
    "n_slice = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tPFvif_mTTl-"
   },
   "outputs": [],
   "source": [
    "def prepare_text_for_lda(text):\n",
    "    tokens=text.split()\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ZtghWOEcRxP"
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    mess=[]\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \" \".join([feature_names[i]\n",
    "                            for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        mess.append(message)\n",
    "    return mess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "id": "uFsAJe8EOOz_",
    "outputId": "8e605e0d-0b18-4dde-be60-ee586ea7ad68"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Introduction-T3PsRW6wZSY.en.vtt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-ad00c07833cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msubs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpysrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msubtitles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pysrt/srtfile.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(cls, path, encoding, error_handling)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mcontain\u001b[0m \u001b[0ma\u001b[0m \u001b[0mbit\u001b[0m \u001b[0morder\u001b[0m \u001b[0mmark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munless\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mset\u001b[0m \u001b[0mto\u001b[0m \u001b[0mutf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m8\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \"\"\"\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0msource_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_unicode_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclaimed_encoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0mnew_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mnew_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_handling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_handling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pysrt/srtfile.py\u001b[0m in \u001b[0;36m_open_unicode_file\u001b[0;34m(cls, path, claimed_encoding)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_open_unicode_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclaimed_encoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclaimed_encoding\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0msource_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rU'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pysrt/srtfile.py\u001b[0m in \u001b[0;36m_detect_encoding\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_detect_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mfile_descriptor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0mfirst_chars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_descriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBIGGER_BOM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0mfile_descriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Introduction-T3PsRW6wZSY.en.vtt'"
     ]
    }
   ],
   "source": [
    "subs = pysrt.open(file_path)\n",
    "os.remove(file_path)\n",
    "\n",
    "subtitles=[]\n",
    "\n",
    "mega_subs=[] \n",
    "count=0\n",
    "time=\"\"\n",
    "# print(len(subs))\n",
    "for i in subs:\n",
    "  \n",
    "    subtitles.append(text_to_word_list((i.text)))\n",
    "    if(count==0):\n",
    "        temp=str(i.start.hours)+\":\"+str(i.start.minutes)+\":\"+str(i.start.seconds)\n",
    "    \n",
    "    if(count==n_slice-1):\n",
    "        temp=temp+\",\"+str(i.end.hours)+\":\"+str(i.end.minutes)+\":\"+str(i.end.seconds)\n",
    "    \n",
    "    count=count+1\n",
    "    if(count%n_slice==0):\n",
    "        hi=[]\n",
    "        hi.append(temp)\n",
    "        merge=[subtitles,hi]\n",
    "        mega_subs.append(merge)\n",
    "        temp=\"\"\n",
    "        subtitles=[]\n",
    "        count=0\n",
    "\n",
    "if(count!=0):\n",
    "    mega_subs.append(subtitles)\n",
    "\n",
    "# print(len(mega_subs))\n",
    "# print(text_to_word_list(i.text))\n",
    "# print(mega_subs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 900
    },
    "colab_type": "code",
    "id": "hz04pwLEUSlA",
    "outputId": "050e1b92-641f-45dd-93d9-8e1ac33c4b0b"
   },
   "outputs": [],
   "source": [
    "id = 0\n",
    "result = {}\n",
    "result_NMF=[]\n",
    "result_LDA=[]\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=1.0, min_df=1,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "\n",
    "for i in range(len(mega_subs)-1):\n",
    "    tfidf = tfidf_vectorizer.fit_transform(mega_subs[i][0])\n",
    "    tf_vectorizer = CountVectorizer(max_df=1.0, min_df=1,\n",
    "                                  max_features=n_features,\n",
    "                                  stop_words='english')\n",
    "  \n",
    "    tf = tf_vectorizer.fit_transform(mega_subs[i][0])\n",
    "\n",
    "# Fit the NMF model\n",
    "    nmf = NMF(n_components=n_components, random_state=1,\n",
    "            alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "# Fit the NMF model\n",
    "\n",
    "    nmf = NMF(n_components=n_components, random_state=1,\n",
    "            beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "            l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                  learning_method='online',\n",
    "                                  learning_offset=50.,\n",
    "                                  random_state=0)\n",
    "\n",
    "    lda.fit(tf)\n",
    "\n",
    "#   print(\"\\nTopics in LDA model:\")\n",
    "    tf_vectorizer._validate_vocabulary()\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    result_LDA.append(print_top_words(lda, tf_feature_names, n_top_words))\n",
    "\n",
    "mega_sub_res=[]\n",
    "\n",
    "count=0\n",
    "for item in (result_LDA):\n",
    "    temp1=[]\n",
    "    temp1.append(item)\n",
    "    temp1.append(mega_subs[count][1])\n",
    "    mega_sub_res.append(temp1)\n",
    "    count=count+1\n",
    "\n",
    "# print(i, len(mega_subs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5mUTBdmfAYOI"
   },
   "outputs": [],
   "source": [
    "my_big_list = []\n",
    "\n",
    "my_set = set()\n",
    "for i in range(0, len(mega_sub_res)-1):\n",
    "    my_set = set()\n",
    "    row = mega_sub_res[i]\n",
    "    comp0 = row[0]\n",
    "    comp0 = comp0[0].split(' ')\n",
    "    comp1 = row[1][0]\n",
    "    for item in comp0:\n",
    "        my_set.add(item)\n",
    "    my_sub_list = [my_set, comp1]\n",
    "    my_big_list.append(my_sub_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JYeRkwMgD28Q"
   },
   "outputs": [],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / float(union)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kpDzJyp7D5oN"
   },
   "outputs": [],
   "source": [
    "threshold = 0.0638298\n",
    "\n",
    "merged_big_list = []\n",
    "merged_big_content = []\n",
    "merged_big_list.append(my_big_list[0])\n",
    "merged_big_content.append(mega_subs[0][0])\n",
    "i = 1\n",
    "while i < len(my_big_list):\n",
    "    similarity = jaccard_similarity(merged_big_list[-1][0], my_big_list[i][0])\n",
    "    if similarity < threshold:\n",
    "        merged_big_list[-1][0].union(my_big_list[i][0])\n",
    "        start = merged_big_list[-1][1].split(',')[0]\n",
    "        finish = my_big_list[i][1].split(',')[1]\n",
    "        merged_big_list[-1][1] = start+','+finish\n",
    "        merged_big_content[-1] += mega_subs[i][0]\n",
    "    else:\n",
    "        merged_big_list.append(my_big_list[i])\n",
    "        merged_big_content.append(mega_subs[i][0])\n",
    "    i += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "taYdUBLDKpc8"
   },
   "outputs": [],
   "source": [
    "result_LDA = []\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=1.0, min_df=1,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "\n",
    "for i in range(len(merged_big_content)):\n",
    "  \n",
    "    tfidf = tfidf_vectorizer.fit_transform(merged_big_content[i])\n",
    "\n",
    "    tf_vectorizer = CountVectorizer(max_df=1.0, min_df=1,\n",
    "                                  max_features=n_features,\n",
    "                                  stop_words='english')\n",
    "\n",
    "    tf = tf_vectorizer.fit_transform(merged_big_content[i])\n",
    "\n",
    "    nmf = NMF(n_components=n_components, random_state=1,\n",
    "            alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "    nmf = NMF(n_components=n_components, random_state=1,\n",
    "            beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "            l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                  learning_method='online',\n",
    "                                  learning_offset=50.,\n",
    "                                  random_state=0)\n",
    "\n",
    "    lda.fit(tf)\n",
    "\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    tokens = print_top_words(lda, tf_feature_names, 5)[0].split()\n",
    "    result_LDA.append(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H8VTQIaD1fle"
   },
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['timings'] = [i[1] for i in merged_big_list]\n",
    "data['result'] = result_LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mvP1TiwEbta6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:0:18 --> 0:2:42 {'module', 'introduction', 'machine', 'learning', 'discuss'}\n",
      "0:2:42 --> 0:5:17 {'following', 'machine', 'came', 'learning', 'perceptron'}\n",
      "0:5:17 --> 0:7:44 {'network', 'neural', 'popular', 'machine', 'learning'}\n",
      "0:7:44 --> 0:10:33 {'task', 'machine', 'experience', 'learning', 'data'}\n",
      "0:10:33 --> 0:14:23 {'machine', 'learning', 'data', 'program', 'computer'}\n",
      "0:14:23 --> 0:18:43 {'task', 'performance', 'tasks', 'data', 'experience'}\n",
      "0:18:47 --> 0:21:59 {'task', 'machine', 'learner', 'learning', 'solution'}\n",
      "0:22:0 --> 0:24:57 {'machine', 'product', 'learning', 'applications', 'want'}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data['result'])):\n",
    "    start = data['timings'][i].split(',')[0]\n",
    "    end = data['timings'][i].split(',')[1]\n",
    "    text = data['result'][i]\n",
    "    print(start,'-->', end, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "VideoIndexing.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
