WEBVTT

1
00:00:00.870 --> 00:00:04.540
In this short video we’ll
talk about how Meltwater

2
00:00:04.540 --> 00:00:09.240
helped Danone using sentiment analysis.

3
00:00:09.240 --> 00:00:12.600
Meltwater is a company
that helps other companies

4
00:00:12.600 --> 00:00:17.100
analyze what people are saying about
them and manage their online reputation.

5
00:00:18.700 --> 00:00:22.980
One of the case studies on their
website is about Danone baby nutrition.

6
00:00:24.180 --> 00:00:29.358
Meltwater helped Danone to monitor
the opinions through social media for

7
00:00:29.358 --> 00:00:31.873
one of their marketing campaigns.

8
00:00:31.873 --> 00:00:35.204
They were able to measure what
was impactful and what was not,

9
00:00:35.204 --> 00:00:36.690
through such monitoring.

10
00:00:37.890 --> 00:00:42.610
Meltwater also helped Danone manage
a potential reputation issue.

11
00:00:42.610 --> 00:00:45.380
When a crisis occurred
related to horse DNA

12
00:00:45.380 --> 00:00:48.320
being in some meat products across Europe.

13
00:00:48.320 --> 00:00:52.830
While Danone was confident that they
didn't have an issue with their products,

14
00:00:52.830 --> 00:00:57.730
having the information a couple of
hours before news hit the UK press.

15
00:00:57.730 --> 00:00:59.690
Allowed them to check and

16
00:00:59.690 --> 00:01:04.580
reassure their customers that their
products were safe to consume.

17
00:01:04.580 --> 00:01:09.040
You can imagine millions of mothers
having been reassured and happy for

18
00:01:09.040 --> 00:01:11.180
Danone's efforts on this.

19
00:01:11.180 --> 00:01:16.375
This is an excellent story about how
big data helped manage public opinion.

20
00:01:16.375 --> 00:01:21.136
And I'm sure Meltwater was able to help
them to measure the opinion impact through

21
00:01:21.136 --> 00:01:22.448
social media as well.
WEBVTT

1
00:00:01.240 --> 00:00:04.508
Big data is now being
generated all around us.

2
00:00:04.508 --> 00:00:05.750
So what?

3
00:00:05.750 --> 00:00:07.450
It's the applications.

4
00:00:07.450 --> 00:00:12.170
It is the way in which big data can
serve human needs that makes it valued.

5
00:00:13.430 --> 00:00:17.971
Let's look at a few examples of the
applications big data is allowing us to

6
00:00:17.971 --> 00:00:19.226
imagine and build.

7
00:00:37.796 --> 00:00:43.420
Big data allows us to build better models,
which produce higher precision results.

8
00:00:44.470 --> 00:00:49.021
We are witnessing hugely innovative
approaches in how companies

9
00:00:49.021 --> 00:00:51.758
market themselves and sell products.

10
00:00:51.758 --> 00:00:53.958
How human resources are managed.

11
00:00:53.958 --> 00:00:56.178
How disasters are responded to.

12
00:00:56.178 --> 00:01:00.389
And many other applications that
evidenced based data is being

13
00:01:00.389 --> 00:01:02.460
used to influence decisions.

14
00:01:04.660 --> 00:01:06.670
What exactly does that mean?

15
00:01:06.670 --> 00:01:08.150
Here is one example.

16
00:01:08.150 --> 00:01:10.640
Many of you might have experienced it,
I do.

17
00:01:12.540 --> 00:01:16.140
Data, Amazon keeps some
things I've been looking at

18
00:01:16.140 --> 00:01:19.610
allows them to personalize
what they show me.

19
00:01:19.610 --> 00:01:24.300
Which hopefully helps narrow down
the huge raft of options I might get

20
00:01:24.300 --> 00:01:27.380
than just searching on dinner plates.

21
00:01:27.380 --> 00:01:32.307
Now, businesses can leverage technology
to make better informed decisions

22
00:01:32.307 --> 00:01:37.246
that are actually based on signals
generated by actual consumers, like me.

23
00:01:39.454 --> 00:01:43.980
Big data enables you to hear
the voice of each consumer as

24
00:01:43.980 --> 00:01:46.590
opposed to consumers at large.

25
00:01:47.920 --> 00:01:51.500
Now, many companies,
including Walmart and Target,

26
00:01:51.500 --> 00:01:57.160
use this information to personalize their
communications with their costumers, which

27
00:01:57.160 --> 00:02:01.240
in turns leads to better met consumer
expectations and happier customers.

28
00:02:03.550 --> 00:02:09.628
Which basically is to say, big data
has enabled personalized marketing.

29
00:02:09.628 --> 00:02:14.250
Consumers are copiously generating
publicly accessible data through

30
00:02:14.250 --> 00:02:16.560
social media sites,
like Twitter or Facebook.

31
00:02:17.690 --> 00:02:22.130
Through such data, the companies
are able to see their purchase history,

32
00:02:22.130 --> 00:02:26.970
what they searched for, what they watched,
where they have been, and

33
00:02:26.970 --> 00:02:29.650
what they're interested in
through their likes and shares.

34
00:02:30.920 --> 00:02:35.717
Let's look at some examples of how
companies are putting this information to

35
00:02:35.717 --> 00:02:39.868
build better marketing campaigns and
reach the right customers.

36
00:02:42.718 --> 00:02:46.985
One area we are all familiar with
are the recommendation engines.

37
00:02:46.985 --> 00:02:52.190
These engines leverage user patterns and
product features

38
00:02:52.190 --> 00:02:58.028
to predict best match product for
enriching the user experience.

39
00:02:58.028 --> 00:03:00.303
If you ever shopped on Amazon,

40
00:03:00.303 --> 00:03:04.596
you know you get recommendations
based on your purchase.

41
00:03:04.596 --> 00:03:07.861
Similarly, Netflix would
recommend you to watch

42
00:03:07.861 --> 00:03:10.590
new shows based on your viewing history.

43
00:03:12.610 --> 00:03:18.550
Another technique that companies use is
sentiment analysis, or in simple terms,

44
00:03:18.550 --> 00:03:22.550
analysis of the feelings around events and
products.

45
00:03:23.880 --> 00:03:27.900
Remember the blue plates I
purchased on Amazon.com?

46
00:03:27.900 --> 00:03:31.680
I not only can read the reviews
before purchasing them,

47
00:03:31.680 --> 00:03:35.460
I can also write a product
review once I receive my plates.

48
00:03:37.280 --> 00:03:40.500
This way, other customers can be informed.

49
00:03:41.880 --> 00:03:47.110
But more importantly, Amazon can keep
a watch on the product reviews and

50
00:03:47.110 --> 00:03:49.220
trends for a particular product.

51
00:03:49.220 --> 00:03:51.951
In this case, blue plates.

52
00:03:51.951 --> 00:03:58.300
For example, they can judge if a product
review is positive or negative.

53
00:03:59.940 --> 00:04:03.140
In this case,
while the first review is negative,

54
00:04:05.140 --> 00:04:07.530
the next two reviews are positive.

55
00:04:09.180 --> 00:04:13.130
Since these reviews are written in
English using a technique called natural

56
00:04:13.130 --> 00:04:16.780
language processing, and
other analytical methods,

57
00:04:16.780 --> 00:04:22.120
Amazon can analyze the general opinion of
a person or public about such a product.

58
00:04:24.130 --> 00:04:29.128
This is why sentiment analysis often
gets referred to as opinion mining.

59
00:04:31.298 --> 00:04:35.729
News channels are filled with
Twitter feed analysis every time

60
00:04:35.729 --> 00:04:39.420
an event of importance occurs,
such as elections.

61
00:04:40.810 --> 00:04:46.162
Brands utilize sentiment analysis
to understand how customers

62
00:04:46.162 --> 00:04:51.630
relate to their product,
positively, negatively, neutral.

63
00:04:51.630 --> 00:04:54.448
This depends heavily on use of
natural language processing.

64
00:04:57.183 --> 00:04:59.289
Mobile devices are ubiquitous and

65
00:04:59.289 --> 00:05:02.780
people almost always carry
their cellphones with them.

66
00:05:03.810 --> 00:05:07.200
Mobile advertising is a huge market for
businesses.

67
00:05:08.760 --> 00:05:13.581
Platforms utilize the sensors
in mobile devices,

68
00:05:13.581 --> 00:05:18.847
such as GPS, and
provide real time location based ads,

69
00:05:18.847 --> 00:05:23.456
offer discounts,
based on this deluge of data.

70
00:05:23.456 --> 00:05:28.063
This time, let's imagine that
I bought a new house and

71
00:05:28.063 --> 00:05:32.178
I happen to be in a few
miles range of a Home Depot.

72
00:05:32.178 --> 00:05:35.444
Sending me mobile coupons about paint,
shelves, and

73
00:05:35.444 --> 00:05:39.290
other new home related purchases
would remind me of Home Depot.

74
00:05:40.550 --> 00:05:43.590
There's a big chance I
would stop by Home Depot.

75
00:05:43.590 --> 00:05:44.790
Bingo!

76
00:05:44.790 --> 00:05:49.470
Now I would like to take a moment to
analyze what kinds of big data are needed

77
00:05:49.470 --> 00:05:50.270
to make this happen.

78
00:05:51.495 --> 00:05:55.990
There's definitely the integration
of my consumer information and

79
00:05:55.990 --> 00:06:00.710
the online and offline databases
that include my recent purchases.

80
00:06:00.710 --> 00:06:05.569
But more importantly,
the geolocation data that falls under

81
00:06:05.569 --> 00:06:09.128
a larger type of big data,
spacial big data.

82
00:06:09.128 --> 00:06:11.780
We will talk about spacial
data later in this class.

83
00:06:13.190 --> 00:06:17.770
Let's now talk about how the global
consumer behavior can be used for

84
00:06:17.770 --> 00:06:18.360
product growth.

85
00:06:20.170 --> 00:06:23.760
We are now moving from
personalize marketing

86
00:06:23.760 --> 00:06:26.230
to the consumer behavior as a whole.

87
00:06:27.920 --> 00:06:32.758
Every business wants to understand
their consumer’s collective

88
00:06:32.758 --> 00:06:37.258
behavior in order to capture
the ever-changing landscape.

89
00:06:37.258 --> 00:06:42.809
Several big data products enable this
by developing models to capture user

90
00:06:42.809 --> 00:06:48.730
behavior and allow businesses to target
the right audience for their product.

91
00:06:50.340 --> 00:06:53.250
Or, develop new products for
uncharted territories.

92
00:06:55.570 --> 00:06:57.550
Let's look at this example.

93
00:06:57.550 --> 00:07:00.797
After an analysis of their sales for
weekdays,

94
00:07:00.797 --> 00:07:06.319
an airline company might notice that their
morning flights are always sold out,

95
00:07:06.319 --> 00:07:09.908
while their afternoon
flights run below capacity.

96
00:07:09.908 --> 00:07:15.545
This company might decide to add more
morning flights based on such analysis.

97
00:07:16.900 --> 00:07:21.550
Notice that they are not using
individual consumer choices, but

98
00:07:21.550 --> 00:07:26.760
using all the flights purchased without
consideration to who purchased them.

99
00:07:28.200 --> 00:07:29.150
They might, however,

100
00:07:29.150 --> 00:07:34.400
decide to pay closer attention to
the demographic of these consumers

101
00:07:34.400 --> 00:07:38.850
using big data to also add similar
flights in other geographical regions.

102
00:07:40.350 --> 00:07:45.240
With rapid advances in genome
sequencing technology,

103
00:07:45.240 --> 00:07:51.580
the life sciences industry is experiencing
an enormous draw in biomedical big data.

104
00:07:53.120 --> 00:07:59.168
This biomedical data is being used
by many applications in research and

105
00:07:59.168 --> 00:08:01.398
personalized medicine.

106
00:08:01.398 --> 00:08:06.400
Did you know genomics data is one of
the largest growing big data types?

107
00:08:06.400 --> 00:08:13.940
Between 100 million and 2 billion human
genomes could be sequenced by year 2025.

108
00:08:13.940 --> 00:08:14.520
Impressive.

109
00:08:16.700 --> 00:08:19.080
This [INAUDIBLE] sequence data demands for

110
00:08:19.080 --> 00:08:24.580
between 2 exabytes and
40 exabytes in data storage.

111
00:08:24.580 --> 00:08:30.190
In comparison, all of YouTube only
requires 1 to 2 exabytes a year.

112
00:08:32.530 --> 00:08:35.886
An exabyte is 10 to the power 18 bites.

113
00:08:35.886 --> 00:08:41.818
That is, 18 zeros after 40.

114
00:08:41.818 --> 00:08:48.270
Of course, analysis of such massive
volumes of sequence data is expensive.

115
00:08:48.270 --> 00:08:50.680
It could take up to 10,000
trillion CPU hours.

116
00:08:54.580 --> 00:08:59.040
One of the biomedical applications
that this much data is enabling

117
00:08:59.040 --> 00:09:00.450
is personalized medicine.

118
00:09:02.060 --> 00:09:06.879
Before personalized medicine,
most patients without a specific type and

119
00:09:06.879 --> 00:09:09.862
stage of cancer received
the same treatment,

120
00:09:09.862 --> 00:09:12.858
which worked better for
some than the others.

121
00:09:14.968 --> 00:09:20.086
Research in this area is enabling
development of methods to analyze

122
00:09:20.086 --> 00:09:25.472
large scale data to develop solutions
that tailor to each individual,

123
00:09:25.472 --> 00:09:28.900
and hence hypothesize
to be more effective.

124
00:09:30.450 --> 00:09:36.260
A person with cancer may now still receive
a treatment plan that is standard,

125
00:09:36.260 --> 00:09:38.620
such as surgery to remove a tumor.

126
00:09:39.870 --> 00:09:43.750
However, the doctor may
also be able to recommend

127
00:09:43.750 --> 00:09:45.460
some type of personalized
cancer treatment.

128
00:09:47.150 --> 00:09:51.900
A big challenge in biomedical big data
applications, like many other fields,

129
00:09:51.900 --> 00:09:56.860
is how we can integrate many types of data
sources to gain further insight problem.

130
00:09:58.300 --> 00:10:01.648
In one of our future lectures,
my colleagues here at

131
00:10:01.648 --> 00:10:06.670
the Supercomputer Center,
will explain how he and his colleague have

132
00:10:06.670 --> 00:10:11.940
used big data from a variety of sources
for personalized patient interventions.

133
00:10:13.840 --> 00:10:19.010
Another application of big data comes
from interconnected mesh of large

134
00:10:19.010 --> 00:10:24.490
number of sensors implanted
across smart cities.

135
00:10:24.490 --> 00:10:28.790
Analysis of data generated
from sensors in real time

136
00:10:28.790 --> 00:10:33.140
allows cities to deliver better
service quality to inhabitants.

137
00:10:33.140 --> 00:10:38.340
And reduce unwanted affect such
as pollution, traffic congestion,

138
00:10:38.340 --> 00:10:41.470
higher than optimal cost on
delivering urban services.

139
00:10:42.910 --> 00:10:44.750
Let's take our city, San Diego.

140
00:10:46.370 --> 00:10:51.794
San Diego generates a huge volumes
of data from many sources.

141
00:10:51.794 --> 00:10:57.676
Traffic sensors, satellites,
camera networks, and more.

142
00:10:57.676 --> 00:10:59.694
What if we could integrate and

143
00:10:59.694 --> 00:11:04.140
synthesize these data streams to
do even more for our community?

144
00:11:05.260 --> 00:11:07.000
Using such big data,

145
00:11:07.000 --> 00:11:12.170
we can work toward making San Diego
the prototype digital city.

146
00:11:12.170 --> 00:11:15.020
Not only for life-threatening hazards, but

147
00:11:15.020 --> 00:11:20.280
making our daily lives better, such as
managing traffic flow more efficiently or

148
00:11:20.280 --> 00:11:24.870
maximizing energy savings,
even as we'll see next, wildfires.

149
00:11:26.450 --> 00:11:31.390
If you want to read more,
here's a link to the AT Kearney report,

150
00:11:31.390 --> 00:11:34.120
where they talk about other
areas using big data.

151
00:11:35.580 --> 00:11:39.880
As a summary,
big data has a huge potential

152
00:11:39.880 --> 00:11:44.140
to enable models with higher
precision in many application areas.

153
00:11:45.230 --> 00:11:50.175
And these highly precise models
are influencing and transforming business.
WEBVTT

1
00:00:01.140 --> 00:00:04.170
Big Data Generated By People,
how is it being used?

2
00:00:05.740 --> 00:00:08.370
We listed a number of challenges for

3
00:00:08.370 --> 00:00:11.400
using unstructured data
generated by human activities.

4
00:00:13.080 --> 00:00:17.730
Now let's look at some of the emerging
technologies to tackle these challenges.

5
00:00:17.730 --> 00:00:22.930
And see some examples that turn
unstructured data into valuable insights.

6
00:00:41.968 --> 00:00:46.780
Although unstructured data specially
the kind generated by people has

7
00:00:46.780 --> 00:00:49.320
a number of challenges.

8
00:00:49.320 --> 00:00:53.300
The good news is that the business
culture of today is shifting

9
00:00:53.300 --> 00:00:57.070
to tackle these challenges and
take full advantage of such data.

10
00:00:58.150 --> 00:01:02.120
As it is often said,
a challenge is a perfect opportunity.

11
00:01:03.130 --> 00:01:06.150
This is certainly the case for
big data and

12
00:01:06.150 --> 00:01:09.680
these challenges have created
a tech industry of it's own.

13
00:01:10.860 --> 00:01:15.730
This industry is mostly centered or
as we would say, layered or

14
00:01:15.730 --> 00:01:21.493
stacked, around a few fundamental
open source big data frameworks.

15
00:01:21.493 --> 00:01:25.320
Need big data tools
are designed from scratch

16
00:01:25.320 --> 00:01:28.990
to manage unstructured information and
analyze it.

17
00:01:28.990 --> 00:01:32.860
A majority of these tools
are based on an open source

18
00:01:32.860 --> 00:01:34.620
big data framework called Hadoop.

19
00:01:35.840 --> 00:01:40.010
Hadoop is designed to support
the processing of large data sets

20
00:01:40.010 --> 00:01:42.960
in a distributed computing environment.

21
00:01:42.960 --> 00:01:47.510
This definition would already give you a
hint that it tackles the first challenge.

22
00:01:47.510 --> 00:01:51.140
Namely, the volume of
unstructured information.

23
00:01:52.150 --> 00:01:55.740
Hadoop can handle big batches
of distributed information but

24
00:01:55.740 --> 00:01:58.250
most often there's a need for

25
00:01:58.250 --> 00:02:03.650
a real time processing of people generated
data like Twitter or Facebook updates.

26
00:02:05.220 --> 00:02:09.950
Financial compliance monitoring is another
area of our central time processing is

27
00:02:09.950 --> 00:02:13.380
needed, in particular
to reduce market data.

28
00:02:14.750 --> 00:02:20.780
Social media and market data are two
types of what we call high velocity data.

29
00:02:21.840 --> 00:02:25.720
Storm and
Spark are two other open source frameworks

30
00:02:25.720 --> 00:02:29.480
that handle such real time
data generated at a fast rate.

31
00:02:30.490 --> 00:02:31.490
Both Storm and

32
00:02:31.490 --> 00:02:36.230
Spark can integrate data with any
database or data storage technology.

33
00:02:37.560 --> 00:02:41.810
As we have emphasized
before unstructured data

34
00:02:41.810 --> 00:02:45.740
does not have a relational data model so
it doesn't generally

35
00:02:45.740 --> 00:02:50.150
fit into the traditional data warehouse
model based on relational databases.

36
00:02:51.350 --> 00:02:55.870
Data warehouses are central repositories
of integrated data from one or

37
00:02:55.870 --> 00:02:56.490
more sources.

38
00:02:58.300 --> 00:03:04.260
The data that gets stored in warehouses,
gets extracted from multiple sources.

39
00:03:05.460 --> 00:03:09.850
It gets transformed into
a common structured form and

40
00:03:09.850 --> 00:03:13.120
it can slow that into
the central database for

41
00:03:13.120 --> 00:03:17.400
use by workers creating analytical
reports throughout an enterprise.

42
00:03:18.650 --> 00:03:25.613
This Exact Transform Load
process is commonly called ETL.

43
00:03:25.613 --> 00:03:29.480
This approach was fairly standard in
enterprise data systems until recently.

44
00:03:30.560 --> 00:03:34.530
As you probably noticed,
it is fairly static and

45
00:03:34.530 --> 00:03:37.350
does not fit well with today's
dynamic big data world.

46
00:03:38.550 --> 00:03:43.073
So how do today's businesses
get around this problem?

47
00:03:43.073 --> 00:03:47.834
Many businesses today are using a hybrid
approach in which their smaller

48
00:03:47.834 --> 00:03:51.894
structured data remains in
their relational databases, and

49
00:03:51.894 --> 00:03:56.750
large unstructured datasets get stored
in NoSQL databases in the cloud.

50
00:03:58.200 --> 00:04:03.840
NoSQL Data technologies are based
on non-relational concepts and

51
00:04:03.840 --> 00:04:08.310
provide data storage options
typically on computing clouds

52
00:04:08.310 --> 00:04:12.180
beyond the traditional relational
databases centered rate houses.

53
00:04:14.170 --> 00:04:19.640
The main advantage of using
NoSQL solutions is their ability

54
00:04:19.640 --> 00:04:24.430
to organize the data for
scalable access to fit the problem and

55
00:04:24.430 --> 00:04:27.330
objectives pertaining to
how the data will be used.

56
00:04:29.130 --> 00:04:34.990
For example, if the data will be used
in an analysis to find connections

57
00:04:34.990 --> 00:04:40.130
between data sets, then the best
solution is a graph database.

58
00:04:41.960 --> 00:04:44.800
Neo4j is an example of a graph database.

59
00:04:45.910 --> 00:04:49.630
Graph networks is a topic that
the graph analytics course

60
00:04:49.630 --> 00:04:53.140
later in this specialization,
we'll explain in depth.

61
00:04:53.140 --> 00:04:59.350
If the data will be best accessed using
key value pairs like a search engine

62
00:04:59.350 --> 00:05:05.490
scenario, the best solution is probably
a dedicated key value paired database.

63
00:05:08.710 --> 00:05:11.660
Cassandra is an example
of a key value database.

64
00:05:13.060 --> 00:05:14.010
These, and

65
00:05:14.010 --> 00:05:18.710
many other types of NoSQL systems will
be explained further in course two.

66
00:05:20.190 --> 00:05:24.160
So we are now confident that there
are emerging technologies for

67
00:05:24.160 --> 00:05:28.410
individual challenges to manage
people generated unstructured data.

68
00:05:29.610 --> 00:05:33.800
But how does one take advantage
of these to generate value?

69
00:05:35.710 --> 00:05:43.150
As we saw big data must pass through a
series of steps before it generates value.

70
00:05:43.150 --> 00:05:47.520
Namely data access, storage,
cleaning, and analysis.

71
00:05:49.220 --> 00:05:55.210
One approach to solve this problem is
to run each stage as a different layer.

72
00:05:56.400 --> 00:06:00.210
And use tools available to
fit the problem at hand, and

73
00:06:00.210 --> 00:06:03.730
scale analytical solutions to big data.

74
00:06:03.730 --> 00:06:08.110
In coming lectures, we will see
important tools that you can use

75
00:06:08.110 --> 00:06:11.840
to solve your big data problems in
addition to the ones you have seen today.

76
00:06:13.070 --> 00:06:17.960
Now let's take a step back and remind
ourselves what some of the value was.

77
00:06:19.220 --> 00:06:23.920
Remember how companies can listen to the
real voice of customers using big data?

78
00:06:25.540 --> 00:06:29.150
It is this type of generated
data that enabled it.

79
00:06:30.190 --> 00:06:35.150
Sentiment analysis analyzes social
media and other data to find

80
00:06:35.150 --> 00:06:40.750
whether people associate positively or
negatively with you business.

81
00:06:40.750 --> 00:06:45.210
Organizations are utilizing
processing of personal data to

82
00:06:45.210 --> 00:06:47.980
understand the true
preferences of their customers.

83
00:06:49.130 --> 00:06:54.110
Now let's take a fun quiz to guess how
much Twitter data companies analyze

84
00:06:54.110 --> 00:06:57.510
every day to measure sentiment
around their product.

85
00:06:59.080 --> 00:07:01.900
The answer is 12 terabytes a day.

86
00:07:03.040 --> 00:07:08.160
For comparison,
you would need to listen continuously for

87
00:07:08.160 --> 00:07:11.620
two years to finish listening
to 1 terabyte of music.

88
00:07:13.000 --> 00:07:15.332
Another example application area for

89
00:07:15.332 --> 00:07:18.960
people generated data is customer
behavior modeling and prediction.

90
00:07:20.100 --> 00:07:24.730
Amazon, Netflix and
a lot of other organizations,

91
00:07:24.730 --> 00:07:28.400
use analytics to analyze
preferences of their customers.

92
00:07:29.710 --> 00:07:35.260
Based on consumer behavior, organizations
suggest better products to customers,

93
00:07:36.350 --> 00:07:40.430
and in turn have happier customers and
higher profits.

94
00:07:41.610 --> 00:07:47.250
Another application area where the value
comes in the form of societal impact and

95
00:07:47.250 --> 00:07:50.540
social welfare, is disaster management.

96
00:07:51.720 --> 00:07:54.550
As you have seen in my wildfire example,

97
00:07:54.550 --> 00:07:58.030
there are many types of big data that
can help with disaster response.

98
00:07:59.450 --> 00:08:03.420
Data in the form of pictures and
tweets, helps facilitate

99
00:08:03.420 --> 00:08:08.380
a collective response to disaster
situations, such as evacuations through

100
00:08:08.380 --> 00:08:12.310
the safest route based on community
feedback through social media.

101
00:08:13.340 --> 00:08:16.820
There are also networks that
turn crowd sourcing and

102
00:08:16.820 --> 00:08:20.650
big data analytics into collective
disaster response tools.

103
00:08:21.920 --> 00:08:24.990
The International Network
of Crisis Mappers,

104
00:08:24.990 --> 00:08:29.950
also called Crisis Mappers Net,
is the largest of such networks and

105
00:08:29.950 --> 00:08:34.360
includes an active international
community of volunteers.

106
00:08:34.360 --> 00:08:40.190
Crisis Mappers use big data in the form
of aerial and satellite imagery,

107
00:08:40.190 --> 00:08:45.440
participatory maps and
live Twitter updates to analyze

108
00:08:45.440 --> 00:08:51.611
the data using geospatial platforms,
advanced visualization,

109
00:08:51.611 --> 00:08:56.300
live simulation and computational and
statistical models.

110
00:08:57.780 --> 00:09:03.220
Once analyzed the results get reported to
rapid response and humanitarian agencies

111
00:09:04.220 --> 00:09:10.020
in the form of mobile and
web applications.

112
00:09:10.020 --> 00:09:16.510
In 2015, right after the Nepal earthquake
Crises Mappers crowd source the analysis

113
00:09:16.510 --> 00:09:21.990
of tweets and mainstream media to
rapidly access disaster damage and

114
00:09:21.990 --> 00:09:27.210
needs and to identify where
humanitarian help is needed.

115
00:09:27.210 --> 00:09:32.663
This example is amazing and shows how
big data can have huge impacts for

116
00:09:32.663 --> 00:09:35.263
social welfare in times of need.

117
00:09:35.263 --> 00:09:38.400
You can learn more about this
story at the following link.

118
00:09:40.790 --> 00:09:45.679
As a summary, although there are
challenges in working with unstructured

119
00:09:45.679 --> 00:09:50.270
people generated data at a scale and
speed that applications demand.

120
00:09:50.270 --> 00:09:54.124
There are also emerging technologies and
solutions that are being

121
00:09:54.124 --> 00:09:58.890
used by many applications to generate
value from the rich source of information.
WEBVTT

1
00:00:00.920 --> 00:00:05.485
Big Data Generated By People,
The Unstructured Challenge.

2
00:00:21.404 --> 00:00:26.471
People are generating massive amounts of
data every day through their activities on

3
00:00:26.471 --> 00:00:31.405
various social media networking sites
like Facebook, Twitter, and LinkedIn.

4
00:00:31.405 --> 00:00:36.655
Or online photo sharing sites like
Instagram, Flickr, or Picasa.

5
00:00:38.540 --> 00:00:40.620
And video sharing websites like YouTube.

6
00:00:42.020 --> 00:00:47.100
In addition an enormous amount of
information gets generated via

7
00:00:47.100 --> 00:00:52.250
blogging and commenting,
internet searches, more via text messages,

8
00:00:53.320 --> 00:00:56.460
email, and through personal documents.

9
00:00:57.920 --> 00:01:02.711
Most of this data is text-heavy and
unstructured,

10
00:01:02.711 --> 00:01:08.780
that is non-conforming to
a well-defined data model.

11
00:01:08.780 --> 00:01:12.690
We can also consider this
data to be content with

12
00:01:12.690 --> 00:01:15.630
occasionally some
description attached to it.

13
00:01:15.630 --> 00:01:20.360
This much activity Leads
to a huge growth in data.

14
00:01:21.480 --> 00:01:25.980
Did you know that in a single day,
Facebook users produce

15
00:01:25.980 --> 00:01:30.550
more data than combined US
academic research libraries?

16
00:01:32.100 --> 00:01:35.130
Let's look at some similar
daily data volume numbers

17
00:01:36.130 --> 00:01:38.460
from some of the biggest online platforms.

18
00:01:39.720 --> 00:01:43.790
It is amazing that some of these
numbers are in the petabyte range for

19
00:01:43.790 --> 00:01:44.700
daily activity.

20
00:01:45.840 --> 00:01:48.930
A petabyte is a thousand terabytes.

21
00:01:50.470 --> 00:01:55.360
The sheer size of mostly unstructured
data generated by humans

22
00:01:55.360 --> 00:01:56.950
brings a lot of challenges.

23
00:01:58.730 --> 00:02:05.250
Unstructured data refers to data that does
not conform to a predefined data model.

24
00:02:07.200 --> 00:02:10.380
So no relation model and no SQL.

25
00:02:11.980 --> 00:02:16.190
It is mostly anything that we
don't store in a traditional

26
00:02:16.190 --> 00:02:17.640
Relational database management system.

27
00:02:19.170 --> 00:02:22.480
Consider a sales receipt that
you get from a grocery store.

28
00:02:23.580 --> 00:02:26.790
It has a section for a date, a section for

29
00:02:26.790 --> 00:02:30.690
store name, and
a section for total amount.

30
00:02:32.200 --> 00:02:34.310
This is an example of structure.

31
00:02:35.340 --> 00:02:40.550
Humans generate a lot of
unstructured data in form of text.

32
00:02:40.550 --> 00:02:42.960
There's no given format to that.

33
00:02:42.960 --> 00:02:46.180
Look at all the documents that you
have written with your hand so far.

34
00:02:47.220 --> 00:02:52.550
Collectively, it is a bank of unstructured
data you have personally generated.

35
00:02:53.580 --> 00:02:58.160
In fact, 80 to 90% of all data in

36
00:02:58.160 --> 00:03:02.980
the world is unstructured and
this number is rapidly growing.

37
00:03:04.490 --> 00:03:09.531
Examples of unstructured data generated
by people includes texts, images,

38
00:03:09.531 --> 00:03:16.190
videos, audio,
internet searches, and emails.

39
00:03:16.190 --> 00:03:21.580
In addition to it's rapid growth
major challenges of unstructured data

40
00:03:21.580 --> 00:03:26.768
include multiple data formats,
like webpages, images, PDFs,

41
00:03:26.768 --> 00:03:34.170
power point, XML, and other formats that
were mainly built for human consumption.

42
00:03:34.170 --> 00:03:41.030
Think of it, although I can sort my
email with date, sender and subject.

43
00:03:41.030 --> 00:03:44.690
It would be really difficult
to write a program,

44
00:03:44.690 --> 00:03:49.430
to categorize all my email messages
based on their content and

45
00:03:49.430 --> 00:03:55.460
organize them for me accordingly another
challenge of human generated data

46
00:03:55.460 --> 00:04:01.560
is the volume and fast generation of data,
which is what we call velocity.

47
00:04:01.560 --> 00:04:07.504
Just take a moment to study this info
graphic, and observe what happens in one

48
00:04:07.504 --> 00:04:12.728
minute on the internet, and
consider how much to contribute to it.

49
00:04:15.088 --> 00:04:22.586
Moreover, confirmation of unstructured
data is often time consuming and costly.

50
00:04:22.586 --> 00:04:28.130
The costs and
time of the process of acquiring, storing,

51
00:04:28.130 --> 00:04:34.110
cleaning, retrieving, and processing
unstructured data can add up to quite and

52
00:04:34.110 --> 00:04:37.220
investment before we can start
reaping value from this process.

53
00:04:39.230 --> 00:04:41.790
It can be pretty hard
to find the tools and

54
00:04:41.790 --> 00:04:45.810
people to implement such a process and
reap value in the end.

55
00:04:47.030 --> 00:04:50.570
As a summary,
although there is an enormous amount of

56
00:04:50.570 --> 00:04:54.780
data generated by people,
most of this data is unstructured.

57
00:04:55.890 --> 00:04:59.230
The challenges of working with
unstructured data should not

58
00:04:59.230 --> 00:05:01.170
be taken lightly.

59
00:05:01.170 --> 00:05:07.310
Next, we'll look at how businesses are
tackling these challenges to gain insight.

60
00:05:07.310 --> 00:05:10.770
And thus, value out of working
with people generated data.
WEBVTT

1
00:00:01.120 --> 00:00:05.970
As we have already seen, there are many
different exciting applications

2
00:00:05.970 --> 00:00:08.060
that are being enabled
by the Big Data era.

3
00:00:09.160 --> 00:00:13.500
As part of my core research here at
the San Diego Supercomputer Center,

4
00:00:13.500 --> 00:00:15.420
I work on building methodologies and

5
00:00:15.420 --> 00:00:20.150
tools to make Big Data useful to dynamic
data driven scientific applications.

6
00:00:21.170 --> 00:00:25.600
My colleagues and I work on many grand
challenge data science applications,

7
00:00:25.600 --> 00:00:30.903
in all areas of science and engineering,
including genomics, geoinformatics,

8
00:00:30.903 --> 00:00:36.050
metro science, energy management,
biomedicine, and personalized health.

9
00:00:37.660 --> 00:00:42.492
What is common to all these applications
is their unique way of bringing

10
00:00:42.492 --> 00:00:46.046
together new modes of data and
computing research.

11
00:00:46.046 --> 00:00:52.195
Let me tell you the one I'm passionate

12
00:00:52.195 --> 00:00:57.192
about, Wildfire Analytics,

13
00:00:57.192 --> 00:01:03.342
which breaks up into two components,

14
00:01:03.342 --> 00:01:07.779
prediction and response.

15
00:01:18.713 --> 00:01:21.629
Why is this so important?

16
00:01:21.629 --> 00:01:25.608
On May 2014, in San Diego County where

17
00:01:25.608 --> 00:01:30.804
the instructors of this
specialization live and work,

18
00:01:30.804 --> 00:01:36.330
there were 14 fires burning,
as many as nine at one time,

19
00:01:36.330 --> 00:01:42.964
which burned a total of 26,000 acres,
11,000 hectares,

20
00:01:42.964 --> 00:01:48.520
an area just less than the size
of the City of San Francisco.

21
00:01:50.710 --> 00:01:56.087
Six people were injured and
one person died.

22
00:01:56.087 --> 00:02:00.093
And these wildfires
resulted in a total cost of

23
00:02:00.093 --> 00:02:04.620
over $60 million US in damage and
firefighting.

24
00:02:05.870 --> 00:02:10.925
These wildfires can become so severe
that we actually call them firestorms.

25
00:02:12.150 --> 00:02:15.170
Although we cannot
control such fire storms,

26
00:02:15.170 --> 00:02:19.070
something we can do is to get ahead
of them by predicting their behavior.

27
00:02:20.790 --> 00:02:25.670
This is why disaster management
of ongoing wildfires relies

28
00:02:25.670 --> 00:02:29.470
heavily on understanding their
direction and rate of spread.

29
00:02:30.560 --> 00:02:33.000
As these fires are a part of our lives,

30
00:02:33.000 --> 00:02:38.410
we wanted to see if we can use Big Data to
monitor, predict and manage a firestorm.

31
00:02:40.400 --> 00:02:42.337
Why can Big Data help?

32
00:02:42.337 --> 00:02:46.685
As we will see in this video,
indeed, wildfire prevention and

33
00:02:46.685 --> 00:02:50.879
response can benefit from many
streams in our data torrent.

34
00:02:50.879 --> 00:02:56.087
Some streams are generated by
people through devices they carry.

35
00:02:56.087 --> 00:03:01.713
A lot come from sensors and satellites,
things that measure environmental factors.

36
00:03:01.713 --> 00:03:05.855
And some come from organizational data,
including area maps,

37
00:03:05.855 --> 00:03:10.839
better service updates and field content
databases, which archive how much

38
00:03:10.839 --> 00:03:15.991
registers vegetation and other types of
fuel are in the way of a potential fire.

39
00:03:17.840 --> 00:03:20.337
What makes this a Big Data problem?

40
00:03:20.337 --> 00:03:25.233
Because novel approaches and
responses can be taken if

41
00:03:25.233 --> 00:03:29.712
we can integrate this many
diverse data streams.

42
00:03:29.712 --> 00:03:34.504
Many such data sources have already
existed for quite some time.

43
00:03:34.504 --> 00:03:39.835
But what is lacking in disaster
management today is a dynamic system

44
00:03:39.835 --> 00:03:44.979
integration of real time sensor networks,
satellite imagery,

45
00:03:44.979 --> 00:03:50.405
near real time data management tools,
wildfire simulation tools,

46
00:03:50.405 --> 00:03:54.334
connectivity to emergency command centers,
and

47
00:03:54.334 --> 00:03:58.470
all these before, during,
and after a firestorm.

48
00:03:59.800 --> 00:04:03.450
As you will see,
the integration of diverse streams and

49
00:04:03.450 --> 00:04:07.760
novel ways is really what's driving
our ability to see new things and

50
00:04:07.760 --> 00:04:11.270
develop predictive analytics,
which may help improve our world.

51
00:04:12.770 --> 00:04:14.140
What are these diverse sources?

52
00:04:15.940 --> 00:04:20.870
One of the most important data sources
is sensor data streaming in from weather

53
00:04:20.870 --> 00:04:26.540
stations and satellites,
such sensed data include temperature,

54
00:04:26.540 --> 00:04:27.960
humidity, air pressure.

55
00:04:29.280 --> 00:04:33.346
We can also include image
data streaming from

56
00:04:33.346 --> 00:04:38.379
mountaintop cameras and
satellites in this category.

57
00:04:38.379 --> 00:04:41.967
Another important data source
comes from institutions

58
00:04:41.967 --> 00:04:44.867
such as
the San Diego Supercomputer Center,

59
00:04:44.867 --> 00:04:48.250
which generate data related
to wildfire modeling.

60
00:04:48.250 --> 00:04:53.585
These include past and current fire
perimeter maps put together by

61
00:04:53.585 --> 00:04:58.824
the authorities and fuel maps that
tell us about the vegetation,

62
00:04:58.824 --> 00:05:02.171
and other types of fuel in a fire's path.

63
00:05:02.171 --> 00:05:08.171
These types of data sources are often
static or updated at a slow rate,

64
00:05:08.171 --> 00:05:14.083
but they provide valuable data
that is well-curated and verified.

65
00:05:15.560 --> 00:05:20.450
A huge part of data on fires is
actually generated by the public

66
00:05:20.450 --> 00:05:24.820
on social media sites such as Twitter,
which support photo sharing resources.

67
00:05:26.650 --> 00:05:32.050
These are the hardest data sources to
streamline during an existing fire, but

68
00:05:32.050 --> 00:05:36.340
they can be very valuable once
integrated with other data sources.

69
00:05:38.820 --> 00:05:44.380
Imagine synthesizing all the pictures
on Twitter about an ongoing fire or

70
00:05:44.380 --> 00:05:47.850
checking the public sentiment
around the boundaries of a fire.

71
00:05:49.960 --> 00:05:53.710
Once you have access to such
information at your fingertips,

72
00:05:53.710 --> 00:05:56.610
there are many things you
can do with such data.

73
00:05:56.610 --> 00:06:01.300
You can simply monitor it, or
maybe you can visualize it.

74
00:06:03.310 --> 00:06:08.370
But it's not until you bring all these
different types of data sources together

75
00:06:08.370 --> 00:06:12.910
and integrate them with real time analysis
and predictive modeling that you can

76
00:06:12.910 --> 00:06:17.570
really make contributions in predicting
and responding to wildfire emergencies.

77
00:06:19.000 --> 00:06:22.221
So now,
I will like you to take a moment and

78
00:06:22.221 --> 00:06:27.296
imagine how Big Data might help
with firefighting in the future.

79
00:06:27.296 --> 00:06:32.280
All these streams of data will come
together in 3D displays that can show

80
00:06:32.280 --> 00:06:37.182
all the related information along
with weather and fire predictions,

81
00:06:37.182 --> 00:06:40.462
just like the way tornadoes
are managed today.
WEBVTT

1
00:00:01.270 --> 00:00:06.840
Let's look at a 2nd example where big data
can have a big impact on saving lives.

2
00:00:06.840 --> 00:00:10.428
I mean,
literally saving lives one life at a time.

3
00:00:10.428 --> 00:00:14.508
I collaborated with a number of
world-class researchers in San Diego, and

4
00:00:14.508 --> 00:00:18.911
an industrial group who are dedicated to
improving human health through research

5
00:00:18.911 --> 00:00:20.970
and practice of precision medicine.

6
00:00:22.400 --> 00:00:24.250
What is precision medicine?

7
00:00:24.250 --> 00:00:28.950
It is an emerging area of medicine
targeted toward an individual person.

8
00:00:28.950 --> 00:00:33.910
Analysing her genetics, her environment,
her daily activities so

9
00:00:33.910 --> 00:00:37.880
that one can detect or
predict a health problem early,

10
00:00:37.880 --> 00:00:42.570
help prevent disease and
in case of illness provide the right drug

11
00:00:42.570 --> 00:00:46.690
at the right dose that is
suitable just for her.

12
00:00:46.690 --> 00:00:51.587
Very recently the White House and
the National Institute of Health here

13
00:00:51.587 --> 00:00:56.401
in the U.S. have declared it to be
a top priority area for research and

14
00:00:56.401 --> 00:00:58.779
development for the next decade.

15
00:00:58.779 --> 00:01:01.689
The expected learning
outcome of this video is for

16
00:01:01.689 --> 00:01:04.668
you to give example of sensor,
organizational and

17
00:01:04.668 --> 00:01:07.870
people-generated data used
in precision medicine.

18
00:01:09.050 --> 00:01:13.650
And, explain why the integration of
different kinds of data is critical

19
00:01:13.650 --> 00:01:14.940
in advancing healthcare.

20
00:01:16.150 --> 00:01:20.000
For any technology to
succeed in real life we need

21
00:01:20.000 --> 00:01:24.940
not only a certain level of maturity of
the technology itself, but a number of

22
00:01:24.940 --> 00:01:30.050
enabling factors including social
economic environment, market demands,

23
00:01:30.050 --> 00:01:34.860
consumer readiness, cost effectiveness,
all of which must work together.

24
00:01:36.430 --> 00:01:39.790
Why is big data for
precision medicine important now?

25
00:01:39.790 --> 00:01:40.290
Let's see.

26
00:01:41.480 --> 00:01:44.900
An important aspect of precision
medicine is to utilize

27
00:01:44.900 --> 00:01:49.570
an individual's genetic profile for
his or her own diagnoses and treatment.

28
00:01:50.910 --> 00:01:52.630
Analyzing the human genome,

29
00:01:52.630 --> 00:01:56.670
which holds the key to human health
is rapidly becoming more affordable.

30
00:01:57.950 --> 00:02:01.110
Today's cost to sequence a genome is less

31
00:02:01.110 --> 00:02:03.760
than 10% of what it
cost just back in 2008.

32
00:02:03.760 --> 00:02:09.070
But, human genomic data is big.

33
00:02:09.070 --> 00:02:10.390
How big?

34
00:02:10.390 --> 00:02:14.660
In a perfect world, just the three
billion letters of your genome

35
00:02:14.660 --> 00:02:17.220
would require about 700
megabytes to store.

36
00:02:18.300 --> 00:02:23.280
In the real world, meaning the kind of
data generated from genome sequencing

37
00:02:23.280 --> 00:02:27.540
machines, we need 200GB to store a genome.

38
00:02:28.620 --> 00:02:32.700
And it takes now about
a day to sequence a genome.

39
00:02:32.700 --> 00:02:37.520
We are finally beginning to create more
electronic records that can be stored and

40
00:02:37.520 --> 00:02:39.130
manipulated in digital media.

41
00:02:40.270 --> 00:02:45.180
Most doctors offices and hospitals now
use electronic health record systems

42
00:02:45.180 --> 00:02:49.130
which contain all details of
a patient's visit and lab test.

43
00:02:50.410 --> 00:02:51.290
How big is this data?

44
00:02:52.710 --> 00:02:57.710
As a quick example The Samaritan Medical
Center Watertown New York at 294 that

45
00:02:57.710 --> 00:03:03.690
Community Hospital reported
120 terabytes as of 2013.

46
00:03:03.690 --> 00:03:07.490
The data value more than double
in just the last two years.

47
00:03:08.860 --> 00:03:13.860
So clearly just in a past two years
dramatic changes have prepared the health

48
00:03:13.860 --> 00:03:18.920
care industry to produce and analyze
larger mounts of complex patient data.

49
00:03:20.170 --> 00:03:25.570
To summarize what we have seen so far
the key components of these changes are:

50
00:03:25.570 --> 00:03:31.460
Reduced cost of data generation and
analysis, increased availability of cheap

51
00:03:31.460 --> 00:03:37.430
large data storage, and they increased
digitization of previously paper records.

52
00:03:38.430 --> 00:03:43.560
But we need one more capability to advance
toward the promised land of individualized

53
00:03:43.560 --> 00:03:44.410
health care practices.

54
00:03:45.950 --> 00:03:50.420
We need to combine various types of
data produce by different groups in

55
00:03:50.420 --> 00:03:51.130
a meaningful way.

56
00:03:52.580 --> 00:03:56.150
Let's look at this issue from
the same point of view as Ilka did.

57
00:03:56.150 --> 00:03:59.760
With her discussion of how big data
can help with wildfire analytics.

58
00:04:01.040 --> 00:04:04.980
The key is the integration of
multiple types of data sources.

59
00:04:04.980 --> 00:04:09.130
Data from sensors,
organizations and people.

60
00:04:09.130 --> 00:04:12.200
In the next few slides,
we look at each of these, and

61
00:04:12.200 --> 00:04:17.820
then I'll share a story about some of the
new and really exciting ways people data

62
00:04:17.820 --> 00:04:22.660
especially has the potential to
change healthcare big data landscape.

63
00:04:24.240 --> 00:04:25.320
Let's start with sensor data.

64
00:04:26.600 --> 00:04:31.000
Sure, digital hospital equipment
have been producing sensor data for

65
00:04:31.000 --> 00:04:35.720
years, but it was unlikely that
the data was ever stored or

66
00:04:35.720 --> 00:04:39.790
shared, let alone
analyzed retrospectively.

67
00:04:39.790 --> 00:04:41.025
These were intended for

68
00:04:41.025 --> 00:04:45.880
real-time use, to inform healthcare
professionals, and then got discarded.

69
00:04:47.110 --> 00:04:50.320
Now we have many more sensors and
deployment.

70
00:04:50.320 --> 00:04:52.850
And many more places
that are capturing and

71
00:04:52.850 --> 00:04:55.970
explicitly gathering information
to be stored and analyzed.

72
00:04:57.190 --> 00:04:59.970
Let's just take a new kind of data

73
00:04:59.970 --> 00:05:02.720
that's increasingly becoming
common in our daily lives.

74
00:05:04.960 --> 00:05:06.600
Fitness devices are everywhere now

75
00:05:07.770 --> 00:05:11.320
their sales have skyrocketed
in the last few years.

76
00:05:11.320 --> 00:05:16.360
They are in wristbands, watches, shoes and
vests, directly communicating with your

77
00:05:16.360 --> 00:05:21.160
personal mobile device, tracking several
activity variables like blood pressure,

78
00:05:21.160 --> 00:05:26.080
different types of activities,
blood glucose levels, etc at every moment.

79
00:05:26.080 --> 00:05:28.850
Their goal is to improve wellness.

80
00:05:28.850 --> 00:05:32.080
By having you monitor
your daily status and

81
00:05:32.080 --> 00:05:35.410
hopefully improve your
lifestyle to stay healthy.

82
00:05:35.410 --> 00:05:40.080
But the data they generate can be
very useful medical information

83
00:05:40.080 --> 00:05:44.100
because this data is about what
happens in your normal life and

84
00:05:44.100 --> 00:05:45.700
not just when you go to the doctor.

85
00:05:47.450 --> 00:05:49.660
How much data do they generate?

86
00:05:49.660 --> 00:05:53.890
The device called FitBit can
produce several gigabytes a day.

87
00:05:53.890 --> 00:05:58.900
Could this data be used to save healthcare
costs, effect a healthier lifestyle?

88
00:05:58.900 --> 00:05:59.660
That's a question mark.

89
00:06:01.090 --> 00:06:05.660
It's safe to guess that this data
alone wouldn't drive the dream of

90
00:06:05.660 --> 00:06:07.350
precision medicine.

91
00:06:07.350 --> 00:06:11.290
But what if we consider integrating
it with other sources of data

92
00:06:11.290 --> 00:06:14.060
like electronic health records or
a genomic profile?

93
00:06:15.150 --> 00:06:17.310
This remains an open question.

94
00:06:18.320 --> 00:06:22.460
This is an open arena for research that
my colleagues at scripts are doing.

95
00:06:22.460 --> 00:06:27.090
It's also a potentially significant area
for product and business development.

96
00:06:27.090 --> 00:06:30.480
Let's look at some examples of health
related data being generated by

97
00:06:30.480 --> 00:06:31.120
organizations.

98
00:06:32.630 --> 00:06:35.380
Many public databases
including those curated and

99
00:06:35.380 --> 00:06:39.690
managed by NCBI, the National Center for
Biotechnology Information,

100
00:06:39.690 --> 00:06:43.180
had been created to capture the basic
scientific data and knowledge for

101
00:06:43.180 --> 00:06:48.090
humans and other model organisms at
the different building blocks of life.

102
00:06:48.090 --> 00:06:53.220
These databases carry both experimental
and computed data that are necessary to

103
00:06:53.220 --> 00:06:56.193
observations for
unconquered diseases like cancer.

104
00:06:56.193 --> 00:07:00.730
In addition,
many have created knoweledge-bases

105
00:07:00.730 --> 00:07:04.500
like the Geneontology and
The Unified Medical Language System

106
00:07:04.500 --> 00:07:08.050
to assemble human knowledge in
a machine processable form.

107
00:07:08.050 --> 00:07:11.660
These are just a few examples of
organizational data sources and

108
00:07:11.660 --> 00:07:15.040
governmental data gathered by health
care systems around the world

109
00:07:15.040 --> 00:07:18.050
could also be used as a massive
source of information.

110
00:07:19.380 --> 00:07:22.210
But really some of
the most interesting and

111
00:07:22.210 --> 00:07:27.040
novel opportunities seem likely to come
from the area of people generated data.

112
00:07:28.410 --> 00:07:32.450
Mobile healths apps is an area
that is growing significantly.

113
00:07:32.450 --> 00:07:35.720
There are apps now to monitor heart rates,
blood pressure, and

114
00:07:35.720 --> 00:07:37.310
test oxygen saturation levels.

115
00:07:38.860 --> 00:07:42.630
Apps, we might say,
record data from sensors but

116
00:07:42.630 --> 00:07:45.040
are also obviously generated from people.

117
00:07:46.230 --> 00:07:50.160
But there is more people generated
data that's interesting beyond censure

118
00:07:51.530 --> 00:07:52.120
measurements.

119
00:07:52.120 --> 00:07:55.280
In 2015 the Webby People's Voice Award

120
00:07:55.280 --> 00:07:58.310
went to an app which supports
meditation and mindfulness.

121
00:07:59.410 --> 00:08:02.400
Rather than an electronic sensing device,

122
00:08:02.400 --> 00:08:07.259
a human would indicate how many
minutes per day they spent meditating.

123
00:08:07.259 --> 00:08:11.078
If they interact with the app
which reminds them to be mindful,

124
00:08:11.078 --> 00:08:15.630
then we have human generated behavior
that we couldn't get from a sensor.

125
00:08:16.840 --> 00:08:22.510
There are well over 100,000 health apps
today in either iTunes or Google Play.

126
00:08:22.510 --> 00:08:25.710
And by some estimates
the Mobile Health App market

127
00:08:25.710 --> 00:08:29.064
may be worth 27 billion dollars by 2017.

128
00:08:30.070 --> 00:08:35.500
So really we are just seen the beginning
of what data might be generated here

129
00:08:35.500 --> 00:08:41.490
from what's being called human sensors,
but to really understand where the power

130
00:08:41.490 --> 00:08:45.980
of people generated data might take us
in the era of big data for healthcare.

131
00:08:45.980 --> 00:08:48.360
Let's imagine how things stand now.

132
00:08:48.360 --> 00:08:51.660
In general,
a patient goes to see their doctor and

133
00:08:51.660 --> 00:08:55.960
maybe their doctor asks if they have had
any side effects from their medications.

134
00:08:57.060 --> 00:08:58.430
The accuracy and

135
00:08:58.430 --> 00:09:02.470
hence the quality of data patients provide
in this kind of setting is very low.

136
00:09:03.620 --> 00:09:05.170
Not that it's really the patients fault.

137
00:09:06.240 --> 00:09:09.930
It might have been days or
weeks ago that they experienced something.

138
00:09:09.930 --> 00:09:13.760
They may be unsure whether something
they experienced was actually a reaction

139
00:09:13.760 --> 00:09:15.380
to then report it.

140
00:09:15.380 --> 00:09:18.590
And there might be details about exactly
when they took a medication that

141
00:09:18.590 --> 00:09:20.790
are meaningful, but
they've forgotten it after the fact.

142
00:09:22.520 --> 00:09:28.120
Today, people are self reporting reactions
and experiences they are having.

143
00:09:28.120 --> 00:09:32.540
We're on Twitter, on blog sites,
online support groups,

144
00:09:32.540 --> 00:09:37.030
online data sharing services: these
are sources of data that we've

145
00:09:37.030 --> 00:09:41.660
never had before that can be used to
understand in a far more detailed and

146
00:09:41.660 --> 00:09:46.360
personal rate the impact of drug
integrations are responses to certain

147
00:09:47.640 --> 00:09:50.430
If applications were designed
to integrate doctor and

148
00:09:50.430 --> 00:09:55.040
hospital records with information
on when drugs were taken and

149
00:09:55.040 --> 00:10:00.000
then to further mine social media or
collect self reports from patients.

150
00:10:00.000 --> 00:10:03.230
Who knows what kinds of questions
we will be able to answer?

151
00:10:03.230 --> 00:10:05.008
Or new questions we may be able to ask?
WEBVTT

1
00:00:01.690 --> 00:00:03.830
Where does big data come from?

2
00:00:03.830 --> 00:00:07.720
The first thing I would like to
say before talking about big data

3
00:00:07.720 --> 00:00:09.010
is that it is not new.

4
00:00:10.040 --> 00:00:14.600
Most of the big data sources existed
before, but the scale we use and

5
00:00:14.600 --> 00:00:17.020
apply them today has changed.

6
00:00:17.020 --> 00:00:21.590
Just look at this image of open
link data on the Internet.

7
00:00:21.590 --> 00:00:23.740
I thought this image was so cool.

8
00:00:23.740 --> 00:00:27.350
It shows not only there are so
many sources of data, but

9
00:00:27.350 --> 00:00:28.490
they're also connected.

10
00:00:29.690 --> 00:00:32.147
If you want to check it out yourself,

11
00:00:32.147 --> 00:00:35.408
we'll give you the link
at the end of this video.

12
00:00:35.408 --> 00:00:40.859
Big data is often boiled down to a few
varieties of data generated by machines,

13
00:00:40.859 --> 00:00:44.010
people, and organizations.

14
00:00:44.010 --> 00:00:49.057
With machine generated data we refer to
data generated from real time sensors in

15
00:00:49.057 --> 00:00:54.029
industrial machinery or vehicles that
logs that track user behavior online,

16
00:00:54.029 --> 00:00:55.687
environmental sensors or

17
00:00:55.687 --> 00:00:59.930
personal health trackers, and
many other sense data resources.

18
00:01:01.080 --> 00:01:05.560
The Large Hadron Collider
generates 40 terabytes of data

19
00:01:05.560 --> 00:01:08.560
every second during experiments.

20
00:01:08.560 --> 00:01:14.078
But human generated data, we refer to
the vast amount of social media data,

21
00:01:14.078 --> 00:01:18.420
status updates, tweets,
photos, and medias.

22
00:01:18.420 --> 00:01:23.943
With organizational generated data we
refer to more traditional types of data,

23
00:01:23.943 --> 00:01:27.598
including transaction
information in databases and

24
00:01:27.598 --> 00:01:31.026
structured data open
stored in data warehouses.

25
00:01:31.026 --> 00:01:35.437
Note that big data can be either
structured, semi-structured, or

26
00:01:35.437 --> 00:01:40.390
unstructured, which is a topic we will
talk about more later in this course.

27
00:01:41.530 --> 00:01:47.180
In most business use cases, any single
source of data on its own is not useful.

28
00:01:48.490 --> 00:01:53.880
Real value often comes from combining
these streams of big data sources

29
00:01:53.880 --> 00:01:58.210
with each other and
analyzing them to generate new insights,

30
00:01:58.210 --> 00:02:00.430
which then goes back into
being big data themselves.

31
00:02:01.890 --> 00:02:03.870
Once you have such insights,

32
00:02:03.870 --> 00:02:08.320
it then enables what we call data
enabled decisions and actions.

33
00:02:09.670 --> 00:02:13.858
Let's now look into these different
types of big data in more detail.
WEBVTT

1
00:00:01.081 --> 00:00:04.312
Why is Big Data generated
by machines useful?

2
00:00:28.325 --> 00:00:31.997
Let's go back for
a second to our first example for

3
00:00:31.997 --> 00:00:34.715
machine generated big data planes.

4
00:00:35.790 --> 00:00:38.940
What is producing all
that data on the plane?

5
00:00:40.430 --> 00:00:43.180
If you look at some of
the sensors that contribute

6
00:00:43.180 --> 00:00:46.600
to the half terabyte of
data generated on a plane,

7
00:00:46.600 --> 00:00:52.040
we will find that some of it comes from
accelerometers that measure turbulence.

8
00:00:53.350 --> 00:00:58.393
There are also sensors built into
the engines for temperature, pressure,

9
00:00:58.393 --> 00:01:02.666
many other measurable factors
to detect engine malfunctions.

10
00:01:02.666 --> 00:01:07.487
Constant real-time analysis of all
the data collected provides help

11
00:01:07.487 --> 00:01:11.496
monitoring and
problem detection at 40,000 feet.

12
00:01:11.496 --> 00:01:15.622
That's approximately 12,000
meters above ground.

13
00:01:17.800 --> 00:01:22.210
We call this type of
analytical processing in-situ.

14
00:01:22.210 --> 00:01:27.940
Previously, in traditional relational
database management systems,

15
00:01:27.940 --> 00:01:32.160
data was often moved to
computational space for processing.

16
00:01:32.160 --> 00:01:37.000
In Big Data space In-Situ
means bringing the computation

17
00:01:37.000 --> 00:01:40.865
to where data is located or,
in this case, generated.

18
00:01:43.200 --> 00:01:47.936
A key feature of these types
of real-time notifications is

19
00:01:47.936 --> 00:01:51.090
that they enable real-time actions.

20
00:01:51.090 --> 00:01:55.756
However, using such a capability
would require you to approach your

21
00:01:55.756 --> 00:01:58.580
application and your work differently.

22
00:02:00.000 --> 00:02:05.090
If you are using an activity tracker, you
should probably come up with a strategy

23
00:02:05.090 --> 00:02:09.540
for how you will incorporate the usage of
these useful gadgets into your lifestyle.

24
00:02:10.780 --> 00:02:15.520
Just like that, if you're planning to
incorporate Big Data driven insights into

25
00:02:15.520 --> 00:02:21.610
your organization, you need to define
a new strategy, and a new way of working.

26
00:02:23.570 --> 00:02:29.184
Most Big Data centric businesses have
updated their culture to be more real-time

27
00:02:29.184 --> 00:02:34.160
action oriented, refining real-time
processes to handle anything from customer

28
00:02:34.160 --> 00:02:38.080
relations and fraud detection,
to system monitoring and control.

29
00:02:39.200 --> 00:02:44.150
In addition, such volumes of real-time
data and analytical operations that need

30
00:02:44.150 --> 00:02:49.880
to take place requires an increased
use of scalable computing systems,

31
00:02:49.880 --> 00:02:53.870
which need to be a part of the planning
for an organizational Big Data strategy.

32
00:02:55.700 --> 00:02:59.580
They see affects of such
changes also in SCADA system.

33
00:03:00.920 --> 00:03:04.630
SCADA stands for Supervisory Control and
Data Acquisition.

34
00:03:06.580 --> 00:03:11.710
SCADA is a type of industrial control
system for remote monitoring and

35
00:03:11.710 --> 00:03:17.190
control of industrial processes
that exists in the physical world,

36
00:03:17.190 --> 00:03:21.020
potentially including multiple sites,
many types of sensors.

37
00:03:23.080 --> 00:03:28.410
In addition to monitoring and control,
SCADA system can be used to define

38
00:03:28.410 --> 00:03:33.390
actions for reduced waste and improved
efficiency in industrial processes,

39
00:03:33.390 --> 00:03:39.100
including those of manufacturing and
power generation, public or

40
00:03:39.100 --> 00:03:43.820
private infrastructure processes,
including water treatment, oil, and

41
00:03:43.820 --> 00:03:49.040
gas pipelines, and
electrical power transmission, and

42
00:03:49.040 --> 00:03:54.300
facility processes including buildings,
airports, ships, and space stations.

43
00:03:55.550 --> 00:04:00.240
They can even be used in smart
building applications to monitor and

44
00:04:00.240 --> 00:04:05.910
control heating, ventilation,
air conditioning systems like HVAC,

45
00:04:05.910 --> 00:04:08.030
access, and energy consumption.

46
00:04:09.330 --> 00:04:14.040
Again, the management of these processes
once the trends, patterns, and

47
00:04:14.040 --> 00:04:20.460
anomalies are identified in real-time
needs to be decided in the Big Data case.

48
00:04:20.460 --> 00:04:26.960
As a summary, as the largest and fastest
type of Big Data, machine generated

49
00:04:26.960 --> 00:04:31.690
data can uniquely enable real-time
actions in many systems and processes.

50
00:04:32.740 --> 00:04:40.198
However, a culture shift is needed for
its computing and real-time action.
WEBVTT

1
00:00:00.910 --> 00:00:02.630
Big data generated by machines.

2
00:00:03.680 --> 00:00:05.508
It's everywhere and there's a lot.

3
00:00:16.258 --> 00:00:19.120
Do big planes require big data?

4
00:00:19.120 --> 00:00:20.650
Absolutely!

5
00:00:20.650 --> 00:00:24.770
Did you know that a Boeing 787 produces

6
00:00:24.770 --> 00:00:27.450
half a terabyte of data
every time it flies?

7
00:00:28.640 --> 00:00:33.680
Really, almost every part of
the plane updates both the flight and

8
00:00:33.680 --> 00:00:35.990
the ground team about
its status constantly.

9
00:00:37.105 --> 00:00:38.530
Where's all this data coming from?

10
00:00:39.560 --> 00:00:43.360
This is an example of machine-generated
data coming from sensors.

11
00:00:44.730 --> 00:00:47.730
If you look at all sources of big data,

12
00:00:47.730 --> 00:00:51.470
machine data is the largest
source of big data.

13
00:00:51.470 --> 00:00:54.850
Additionally, it is very complex.

14
00:00:54.850 --> 00:01:00.960
In general, we call machines that provide
some type of sensing capability smart.

15
00:01:00.960 --> 00:01:04.760
Have you ever wondered why you
call your cell phone a smartphone?

16
00:01:05.890 --> 00:01:08.790
Because it gives you a way
to track many things,

17
00:01:08.790 --> 00:01:12.740
including your geolocation, and
connect you to other things.

18
00:01:13.930 --> 00:01:16.716
So what makes a smart device smart, smart?

19
00:01:16.716 --> 00:01:22.110
Generally speaking, There are three
main properties of smart devices

20
00:01:22.110 --> 00:01:27.200
based on what they do with sensors and
things they encapsulate.

21
00:01:27.200 --> 00:01:29.480
They can connect to other devices or

22
00:01:29.480 --> 00:01:35.490
networks, they can execute services and
collect data autonomously,

23
00:01:35.490 --> 00:01:40.230
that means on their own, they have
some knowledge of the environment.

24
00:01:41.550 --> 00:01:46.940
The widespread availability of the smart
devices and their interconnectivity

25
00:01:46.940 --> 00:01:53.080
led to a new term being coined,
The Internet of Things.

26
00:01:53.080 --> 00:01:58.450
Think of a world of smart devices at home,
in your car, in the office,

27
00:01:58.450 --> 00:02:04.110
city, remote rural areas,
the sky, even the ocean,

28
00:02:04.110 --> 00:02:07.340
all connected and all generating data.

29
00:02:08.360 --> 00:02:12.590
Let's look at an example of a device
that has some of these things in it.

30
00:02:14.350 --> 00:02:19.930
An activity tracker is a device or
application for monitoring and

31
00:02:19.930 --> 00:02:25.790
tracking fitness-related metrics
such as distance walked or run,

32
00:02:25.790 --> 00:02:32.670
calorie consumption, and in some cases,
heartbeat and quality of sleep.

33
00:02:32.670 --> 00:02:37.140
What if everyone in New York City
wore an activity tracker?

34
00:02:37.140 --> 00:02:40.350
What if everyone wore several?

35
00:02:40.350 --> 00:02:45.300
I personally have three activity
trackers that I use on a daily basis.

36
00:02:45.300 --> 00:02:50.200
One to track my sleep, another one
to track my physical activity, and

37
00:02:50.200 --> 00:02:54.250
a third, my smartphone,
that goes everywhere with me.

38
00:02:54.250 --> 00:03:00.030
So it is not that unusual to imagine that
this will be the case for many people.

39
00:03:00.030 --> 00:03:05.320
As you have already heard from in
a previous lecture on personalized data,

40
00:03:05.320 --> 00:03:08.760
such activity trackers
have enabled a new way

41
00:03:08.760 --> 00:03:12.210
of doing patient intervention
via personalized medicine.

42
00:03:13.670 --> 00:03:18.440
Similarly, the sensors in planes have
generated a new way of looking at

43
00:03:18.440 --> 00:03:21.370
fleet management and flight safety.

44
00:03:21.370 --> 00:03:26.055
As a summary,
machines collect data 24/7 via

45
00:03:26.055 --> 00:03:31.980
their built-in sensors,
both at personal and industrial scales.

46
00:03:31.980 --> 00:03:35.308
And thus, they are the largest
of all the big data sources.
WEBVTT

1
00:00:01.300 --> 00:00:02.963
Organization-Generated Data.

2
00:00:02.963 --> 00:00:09.580
Benefits come from combining
with other data types.

3
00:00:09.580 --> 00:00:12.380
How are some organizations
benefiting from big data?

4
00:00:29.278 --> 00:00:31.844
Let's look at real world examples to
see the advantages these organizations

5
00:00:31.844 --> 00:00:32.800
are getting out of big data.

6
00:00:33.960 --> 00:00:36.120
One of these companies is UPS.

7
00:00:37.720 --> 00:00:41.650
UPS delivers 16 million shipments per day.

8
00:00:41.650 --> 00:00:46.360
They get around 40 million
tracking requests.

9
00:00:46.360 --> 00:00:46.900
That's huge.

10
00:00:48.270 --> 00:00:54.431
An estimate of how much data UPS has
on its operations is 16 petabytes.

11
00:00:55.880 --> 00:01:00.580
Can you guess how much money UPS can
save by reducing each driver's route

12
00:01:00.580 --> 00:01:01.620
by just one mile?

13
00:01:03.410 --> 00:01:09.113
If they can reduce distance traveled
by each truck by even one mile,

14
00:01:09.113 --> 00:01:13.830
UPS can save a whopping $50 million U.S.
per year.

15
00:01:15.490 --> 00:01:19.010
This is where big data steps in.

16
00:01:19.010 --> 00:01:22.610
Utilizing complex optimization
over large datasets

17
00:01:22.610 --> 00:01:27.250
can lead to route optimizations that were
previously not visible to the company.

18
00:01:29.110 --> 00:01:32.900
Big data, together with smart processing,

19
00:01:32.900 --> 00:01:36.720
enables UPS to manage thousands
of route optimizations.

20
00:01:37.900 --> 00:01:43.310
Let's travel from package
delivery to the retail domain.

21
00:01:43.310 --> 00:01:46.870
An organization from
the retail shopping domain

22
00:01:46.870 --> 00:01:49.760
that heavily utilizes big data is Walmart.

23
00:01:51.590 --> 00:01:58.220
Walmart is a big organization that gets
250 million customers in 10,000 stores.

24
00:02:00.820 --> 00:02:07.570
Did you know they collect 2.5
petabytes of data per hour?

25
00:02:09.280 --> 00:02:14.699
They collect data on Twitter tweets,
local events, local weather,

26
00:02:14.699 --> 00:02:19.640
in-store purchases, online clicks and

27
00:02:19.640 --> 00:02:23.530
many other sales, customer and
product related data.

28
00:02:25.070 --> 00:02:31.080
They use this data to find patterns
such as which products are frequently

29
00:02:31.080 --> 00:02:36.878
purchased together, and what is the best
new product to introduce in their stores,

30
00:02:36.878 --> 00:02:41.530
to predict demand at
the particular location,

31
00:02:43.670 --> 00:02:47.898
and to customize customer recommendations.

32
00:02:47.898 --> 00:02:50.430
Overall, by leveraging big data and

33
00:02:50.430 --> 00:02:55.390
analytics, Walmart has maintained
its position as a top retailer.

34
00:02:56.650 --> 00:03:02.130
UPS and Walmart examples were just two out
of a number of companies using big data.

35
00:03:03.380 --> 00:03:07.760
Big data is producing results for
companies in all sectors.

36
00:03:09.670 --> 00:03:14.370
Studies forecast spending on
big data technologies to go up

37
00:03:14.370 --> 00:03:16.630
drastically in the next five years.

38
00:03:18.320 --> 00:03:25.280
A study by Bane and Company
suggests that early adopters of big

39
00:03:25.280 --> 00:03:30.650
data analytics have gained a significant
lead over the rest of the corporate world.

40
00:03:32.620 --> 00:03:34.430
For graphics referenced here,

41
00:03:35.450 --> 00:03:40.700
we see that companies that use
analytics are twice as likely

42
00:03:40.700 --> 00:03:45.160
to be in the top quartile of financial
performance within their industries.

43
00:03:46.330 --> 00:03:51.368
Five times as likely to make decisions
much faster than market peers.

44
00:03:52.440 --> 00:03:56.180
Three times as likely to
execute decisions as intended.

45
00:03:56.180 --> 00:04:02.880
And twice as likely to use data very
frequently when making decisions.

46
00:04:04.040 --> 00:04:06.610
This points to the growth and
demand of people and

47
00:04:06.610 --> 00:04:12.450
technology centered around or
specializing in big data applications.

48
00:04:12.450 --> 00:04:17.870
As a summary, organizations are gaining
significant benefit from integrating

49
00:04:17.870 --> 00:04:22.920
big data practices into their culture and
breaking their silos.

50
00:04:22.920 --> 00:04:28.080
Some major benefits to organizations are
operational efficiency, improved marketing

51
00:04:28.080 --> 00:04:32.630
outcomes, higher profits, and
improved customer satisfaction.
WEBVTT

1
00:00:00.870 --> 00:00:06.650
Big data, generated by organizations,
structured but often siloed.

2
00:00:06.650 --> 00:00:11.930
The last type of big data we will discuss
is big data generated by organizations.

3
00:00:13.260 --> 00:00:18.740
This type of data is the closest to
what most businesses currently have.

4
00:00:18.740 --> 00:00:21.550
But it's considered a bit out of fashion,
or

5
00:00:21.550 --> 00:00:25.180
traditional, compared to
other types of big data.

6
00:00:25.180 --> 00:00:29.760
However, it is at least as important
as other types of big data.

7
00:00:47.566 --> 00:00:51.350
So how do organizations produce data?

8
00:00:51.350 --> 00:00:54.940
The answer to how
an organization generates data

9
00:00:54.940 --> 00:00:57.260
is very unique to the organization and
context.

10
00:00:58.590 --> 00:01:02.540
Each organization has distinct
operation practices and

11
00:01:02.540 --> 00:01:07.860
business models, which result in
a variety of data generation platforms.

12
00:01:07.860 --> 00:01:12.697
For example, the type and source of data
that a bank gets is very different from

13
00:01:12.697 --> 00:01:15.650
what a hardware equipment
manufacturer gets.

14
00:01:17.610 --> 00:01:23.808
Some common types of organizational big
data come from commercial transactions,

15
00:01:23.808 --> 00:01:29.033
credit cards, government institutions,
e-commerce, banking or

16
00:01:29.033 --> 00:01:35.070
stock records, medical records, sensors,
transactions, clicks and so on.

17
00:01:36.350 --> 00:01:39.930
Almost every event can
be potentially stored.

18
00:01:41.500 --> 00:01:45.320
Organizations store this data for
current and

19
00:01:45.320 --> 00:01:49.040
future use, as well as for
analysis of the past.

20
00:01:50.380 --> 00:01:54.970
Let's say you're an organization
that collects sales transactions.

21
00:01:54.970 --> 00:01:59.500
You can use this data for pattern
recognition to detect correlated products,

22
00:02:00.530 --> 00:02:05.990
to estimate demand for
products likely to go up in sales, and

23
00:02:05.990 --> 00:02:07.780
capture fraudulent activity.

24
00:02:08.930 --> 00:02:14.560
Moreover, when you know your
sales record and can correlate it

25
00:02:14.560 --> 00:02:20.370
with your marketing records, you can find
which campaigns really made an impact.

26
00:02:20.370 --> 00:02:24.340
You are already becoming
a data savvy organization.

27
00:02:24.340 --> 00:02:29.180
Now think of bringing your
sales data together with other

28
00:02:29.180 --> 00:02:34.680
external open public data,
such as major world events in the news.

29
00:02:34.680 --> 00:02:38.420
You can ask, was it savvy marketing or

30
00:02:38.420 --> 00:02:43.310
a consequence of external events
that triggered your sales?

31
00:02:43.310 --> 00:02:45.037
Using proper analytics,

32
00:02:45.037 --> 00:02:50.800
you can now build inventories to match
your predicted growth and demand.

33
00:02:50.800 --> 00:02:57.060
In addition, organizations build and
apply processes to record and

34
00:02:57.060 --> 00:03:02.310
monitor business events of interest,
such as registering a customer,

35
00:03:02.310 --> 00:03:05.116
manufacturing a product or
taking an order.

36
00:03:05.116 --> 00:03:10.930
These processes collect
highly structured data

37
00:03:10.930 --> 00:03:14.460
that include transactions,
reference tables, and

38
00:03:14.460 --> 00:03:19.810
relationships, as well as
the metadata that sets its context.

39
00:03:21.130 --> 00:03:26.580
Usually, structured data is stored in
relational database management systems.

40
00:03:28.030 --> 00:03:31.805
However, we call any data
that is in the form of

41
00:03:31.805 --> 00:03:36.708
a record located in a fixed field or
file structured data.

42
00:03:36.708 --> 00:03:40.300
This definition also
includes spreadsheets.

43
00:03:41.750 --> 00:03:46.490
As I have mentioned before,
traditionally this type of

44
00:03:46.490 --> 00:03:51.070
highly structured data is the vast
majority of what IT managed and

45
00:03:51.070 --> 00:03:55.420
processed in both operational and
business intelligence systems.

46
00:03:56.750 --> 00:04:00.410
Let's look at the sales transaction
data in our previous example.

47
00:04:01.890 --> 00:04:05.575
If you look at the data in
the relational table on the right.

48
00:04:05.575 --> 00:04:09.687
As the name structured suggests,

49
00:04:09.687 --> 00:04:14.649
the table is organized to store data using

50
00:04:14.649 --> 00:04:18.493
a structure defined by a model.

51
00:04:18.493 --> 00:04:24.450
Each column is tagged to tell us what
data that column intends to store.

52
00:04:24.450 --> 00:04:26.950
This is what we call a data model.

53
00:04:28.310 --> 00:04:31.680
A data model defines
each of these columns and

54
00:04:31.680 --> 00:04:37.290
fields in the table, and
defines relationships between them.

55
00:04:37.290 --> 00:04:43.960
If you look at the product ID column,
you will see that it includes only

56
00:04:43.960 --> 00:04:48.940
identifiers that can potentially be linked
to another table defining these products.

57
00:04:50.350 --> 00:04:55.990
The ability to define such relationships,
easily make structured data,

58
00:04:55.990 --> 00:05:01.220
or in this case relational databases,
highly adopted by many organizations.

59
00:05:02.460 --> 00:05:06.330
There are commonly used
languages like SQL,

60
00:05:06.330 --> 00:05:11.600
the Structured Query Language, to extract
data of interest from such tables.

61
00:05:12.690 --> 00:05:15.800
This is referred to as querying the data.

62
00:05:17.160 --> 00:05:23.060
However, it could even be a challenge
to integrate such structured data.

63
00:05:23.060 --> 00:05:28.460
This image shows us a continuum
of technologies to model,

64
00:05:28.460 --> 00:05:32.880
collect and query unstructured
data coming from software and

65
00:05:32.880 --> 00:05:35.180
hardware components
within an organization.

66
00:05:36.400 --> 00:05:41.150
In the past, such challenges
led to information being stored

67
00:05:41.150 --> 00:05:45.050
in what we call silos,
even within an organization.

68
00:05:46.400 --> 00:05:51.700
Many organizations have traditionally
captured data at the department level,

69
00:05:51.700 --> 00:05:56.950
without proper infrastructure and
policy to share and integrate this data.

70
00:05:56.950 --> 00:06:01.550
This has hindered the growth of
scalable pattern recognition

71
00:06:01.550 --> 00:06:04.510
to the benefits of
the entire organization.

72
00:06:04.510 --> 00:06:08.700
Because no one system has access to
all data that the organization owns.

73
00:06:10.480 --> 00:06:14.080
Each data set is compartmentalized.

74
00:06:14.080 --> 00:06:20.320
If such silos are left untouched,
organizations risk having outdated,

75
00:06:20.320 --> 00:06:24.320
unsynchronized, and
even invisible data sets.

76
00:06:25.570 --> 00:06:29.260
Organizations are realizing
the detrimental outcomes of this rigid

77
00:06:29.260 --> 00:06:30.820
structure.

78
00:06:30.820 --> 00:06:36.270
And changing policies and infrastructure
to enable integrated processing of

79
00:06:36.270 --> 00:06:39.025
all data to the entire
organization's benefit.

80
00:06:39.025 --> 00:06:44.090
Cloud-based solutions
are seen as agile and

81
00:06:44.090 --> 00:06:47.550
low capital intensive
solutions in this area.

82
00:06:47.550 --> 00:06:53.309
As a summary, while highly structured
organizational data is very useful and

83
00:06:53.309 --> 00:06:57.671
trustworthy, and
thus a valuable source of information,

84
00:06:57.671 --> 00:07:01.860
organizations must pay special
attention to breaking up

85
00:07:01.860 --> 00:07:06.243
the silos of information to
make full use of its potential.
WEBVTT

1
00:00:00.660 --> 00:00:03.542
The key, integrating diverse data.

2
00:00:13.178 --> 00:00:16.267
Whatever your big data application is, and

3
00:00:16.267 --> 00:00:21.527
the types of big data you are using
the real value will come from integrating

4
00:00:21.527 --> 00:00:26.056
different types of data sources,
and analyzing them at scale.

5
00:00:26.056 --> 00:00:28.947
So how do we start getting this value?

6
00:00:28.947 --> 00:00:30.706
Sometimes, all it takes,

7
00:00:30.706 --> 00:00:35.620
is looking at the data you already
collect in a different way.

8
00:00:35.620 --> 00:00:39.060
And it can mean a big difference
in your return on investment.

9
00:00:40.590 --> 00:00:45.970
This new story from June 2015
mentions that Carnival Cruises

10
00:00:45.970 --> 00:00:50.760
is using structured and unstructured
data from a variety of sources.

11
00:00:52.420 --> 00:00:55.200
Carnival turns it into profit

12
00:00:55.200 --> 00:00:58.640
using price optimization
techniques on the integrated data.

13
00:00:59.640 --> 00:01:02.460
For you to achieve such a success story,

14
00:01:02.460 --> 00:01:07.030
you will need to include data
integration into your big data practice.

15
00:01:07.030 --> 00:01:11.950
However, there are some unique challenges
when attempting to integrate these

16
00:01:11.950 --> 00:01:16.120
diverse data sources and
scaling the solutions.

17
00:01:16.120 --> 00:01:20.220
Course Two on on the specialization,
we'll teach you more about these issues.

18
00:01:21.260 --> 00:01:26.010
But let's take a moment to define why
effect of data integration is useful?

19
00:01:27.060 --> 00:01:33.040
Data integration means bringing
together data from diverse sources and

20
00:01:33.040 --> 00:01:37.170
turning them into coherent and
more useful information.

21
00:01:38.440 --> 00:01:40.390
We also call this knowledge.

22
00:01:41.740 --> 00:01:47.970
The main objective here is taming or
more technically managing data and

23
00:01:47.970 --> 00:01:52.520
turning it into something you can
make use of programmatically.

24
00:01:52.520 --> 00:01:57.460
A data integration process
involves many parts.

25
00:01:57.460 --> 00:02:03.250
It starts with discovering,
accessing, and monitoring data and

26
00:02:03.250 --> 00:02:09.340
continues with modeling and transforming
data from a variety of sources.

27
00:02:09.340 --> 00:02:13.160
But why do we need data
integration in the first place?

28
00:02:13.160 --> 00:02:16.900
Let's start by focusing on
differences between big data sets

29
00:02:16.900 --> 00:02:18.140
coming from different sources.

30
00:02:19.500 --> 00:02:24.640
You might have flat file formatted data,
relational database data,

31
00:02:24.640 --> 00:02:31.470
data encoded in XML or JSON,
both common for internet generated data.

32
00:02:31.470 --> 00:02:35.170
These different formats and
models are useful

33
00:02:35.170 --> 00:02:38.690
because they are designed to express
different data in unique ways.

34
00:02:39.880 --> 00:02:42.560
In a way, different data formats and

35
00:02:42.560 --> 00:02:49.100
models make big data more useful and
more challenging all at the same time.

36
00:02:49.100 --> 00:02:53.700
When you integrate data in different
formats, you make the final

37
00:02:53.700 --> 00:02:58.030
product richer in the number of
features you describe the data with.

38
00:02:58.030 --> 00:03:03.300
For example,
by integrating environmental sensor and

39
00:03:03.300 --> 00:03:08.300
camera data with geographical
information system data, such as in

40
00:03:08.300 --> 00:03:14.178
my wildfire prediction application,
I can use the spacial data capabilities

41
00:03:14.178 --> 00:03:19.970
with non-spacial data to more
accurately run fire simulations.

42
00:03:21.230 --> 00:03:25.920
In the past, although we were able
to see the images of the fire

43
00:03:27.280 --> 00:03:31.630
from mountain top cameras,
just like this image, we were still

44
00:03:31.630 --> 00:03:35.880
not able to tell what the exact
location of the fire is automatically.

45
00:03:37.460 --> 00:03:41.000
Now, when a fire is detected
from a mountain top camera,

46
00:03:42.180 --> 00:03:47.640
viewsheds are used to estimate
the location of the fire.

47
00:03:49.490 --> 00:03:54.760
This location information can
be fed into the fire simulator

48
00:03:54.760 --> 00:03:58.070
as early as it's detected
to predict the size and

49
00:03:58.070 --> 00:04:02.090
location of the fire in the next
hour more accurately and faster.

50
00:04:03.730 --> 00:04:08.920
Similarly, I can use real time
data with eye curve data sets, and

51
00:04:08.920 --> 00:04:09.940
use them all together.

52
00:04:10.980 --> 00:04:15.520
Additionally, by bringing
the data together, and

53
00:04:15.520 --> 00:04:22.100
providing programmable access to it, I'm
now making each data set more accessible.

54
00:04:23.200 --> 00:04:28.020
Moreover, integration of
diverse datasets significantly

55
00:04:28.020 --> 00:04:32.220
reduces the overall data complexity
in my data-driven product.

56
00:04:33.320 --> 00:04:39.610
The data becomes more available for
use and unified as a system of its own.

57
00:04:40.860 --> 00:04:45.010
One advantage of such an integration
is not often mentioned.

58
00:04:46.500 --> 00:04:49.830
Such a streamlined and
integrated data system

59
00:04:49.830 --> 00:04:53.550
can increase the collaboration between
different parts of your data systems.

60
00:04:54.930 --> 00:04:58.430
Each part can now clearly see

61
00:04:58.430 --> 00:05:02.000
how their data is integrated
into the overall system.

62
00:05:02.000 --> 00:05:07.860
Including the user scenarios and the
security and privacy processes around it.

63
00:05:09.080 --> 00:05:16.320
Overall by integrating diverse data
streams you add value to your big data and

64
00:05:16.320 --> 00:05:20.320
improve your business even
before you start analyzing it.

65
00:05:21.640 --> 00:05:25.783
Next, we will focus on the dimensions
of the scalability and

66
00:05:25.783 --> 00:05:29.937
discuss how we can start tackling
some of these challenges.
WEBVTT

1
00:00:01.310 --> 00:00:03.380
What launched the big data era?

2
00:00:04.730 --> 00:00:09.092
In this video, you will learn about
two new opportunities that have

3
00:00:09.092 --> 00:00:12.088
contributed to the launch
of the big data era.

4
00:00:27.737 --> 00:00:32.070
Opportunities are often
a signal of changing times.

5
00:00:34.110 --> 00:00:40.100
In 2013, an influential report by
a company called McKinsey claimed that

6
00:00:40.100 --> 00:00:45.900
the area of data science will be the
number one catalyst for economic growth.

7
00:00:47.380 --> 00:00:50.320
McKinsey identified one of our new

8
00:00:50.320 --> 00:00:54.366
opportunities that contributed to
the launch of the big data era.

9
00:00:54.366 --> 00:00:59.190
A growing torrent of data.

10
00:00:59.190 --> 00:01:05.150
This refers to the idea that data seems to
be coming continuously and at a fast rate.

11
00:01:06.340 --> 00:01:11.330
Think about this,
today you can buy a hard drive to store

12
00:01:11.330 --> 00:01:15.624
all the music in the world for only $600.

13
00:01:16.710 --> 00:01:24.600
That's an amazing storage capability over
any previous forms of music storage.

14
00:01:24.600 --> 00:01:29.286
In 2010 there were 5 billion
mobile phones in use.

15
00:01:29.286 --> 00:01:33.018
You can be sure that
there are more today and

16
00:01:33.018 --> 00:01:37.476
as I'm sure you will understand,
these phones and

17
00:01:37.476 --> 00:01:42.244
the apps we install on them
are a big source of big data,

18
00:01:42.244 --> 00:01:47.342
which all the time, every day,
contributes to our core.

19
00:01:47.342 --> 00:01:50.444
And Facebook, which recently just set

20
00:01:50.444 --> 00:01:55.144
a record of having one billion
people login in a single day,

21
00:01:55.144 --> 00:02:00.320
has more that 30 billion pieces
of content shared every month.

22
00:02:01.430 --> 00:02:03.620
Well, that number's from 2013.

23
00:02:03.620 --> 00:02:07.310
So I'm sure that it's much
higher than that now.

24
00:02:08.720 --> 00:02:11.950
Does it make you think how many
Facebook share's you made last month?

25
00:02:13.760 --> 00:02:17.105
All this leads to projections
of serious growth.

26
00:02:17.105 --> 00:02:24.860
40% in global data per year,
and 5% in global IT spending.

27
00:02:24.860 --> 00:02:29.080
This much data has sure
pushed the data science field

28
00:02:29.080 --> 00:02:32.420
to start remaining itself and
the business world of today.

29
00:02:33.450 --> 00:02:38.970
But, there's something else contributing
to the catalyzing power of data science.

30
00:02:38.970 --> 00:02:41.820
It is called cloud computing.

31
00:02:41.820 --> 00:02:44.600
We call this on demand computing.

32
00:02:44.600 --> 00:02:48.480
Cloud computing is one of
the ways in which computing

33
00:02:48.480 --> 00:02:53.220
has now become something that
we ca do anytime, and anywhere.

34
00:02:53.220 --> 00:02:56.170
You may be surprised to know
that some of your favorite

35
00:02:56.170 --> 00:02:59.900
apps are from businesses
being run from coffee shops.

36
00:02:59.900 --> 00:03:04.420
This new ability,
combined with our torrent of data,

37
00:03:04.420 --> 00:03:10.220
gives us the opportunity to perform novel,
dynamic and scalable data analysis,

38
00:03:10.220 --> 00:03:13.824
to tell us new things about our world and
ourself.

39
00:03:13.824 --> 00:03:20.037
To summarize, a new torrent of big data
combined with computing capability

40
00:03:20.037 --> 00:03:25.968
anytime, anywhere has been at the core
of the launch of the big data era.
WEBVTT

1
00:00:01.080 --> 00:00:02.360
Asking the Right Questions.

2
00:00:17.893 --> 00:00:23.810
The first step in any process is to define
what it is you are trying to tackle.

3
00:00:25.030 --> 00:00:27.975
What is the problem that
needs to be addressed, or

4
00:00:27.975 --> 00:00:30.240
the opportunity that
needs to be ascertained.

5
00:00:31.280 --> 00:00:35.223
Without this,
you won't have a clear goal in mind, or

6
00:00:35.223 --> 00:00:38.153
know when you've solved your problem.

7
00:00:38.153 --> 00:00:42.833
An example question is,
how can sales figures and

8
00:00:42.833 --> 00:00:48.072
call center logs be combined
to evaluate a new product,

9
00:00:48.072 --> 00:00:53.534
or in a manufacturing process,
how can data from multiple

10
00:00:53.534 --> 00:00:59.573
sensors in an instrument be used
to detect instrument failure?

11
00:00:59.573 --> 00:01:02.745
How can we understand our customers and

12
00:01:02.745 --> 00:01:07.273
market better to achieve
effective target marketing?

13
00:01:07.273 --> 00:01:11.320
Next you need to assess the situation
with respect to the problem or

14
00:01:11.320 --> 00:01:14.730
the opportunity you have defined.

15
00:01:14.730 --> 00:01:20.011
This is a step where you need to
exercise caution analyzing risks,

16
00:01:20.011 --> 00:01:23.609
costs, benefits, contingencies,

17
00:01:23.609 --> 00:01:28.650
regulations, resources and
requirements of the situation.

18
00:01:28.650 --> 00:01:30.910
What are the requirements of the problem?

19
00:01:30.910 --> 00:01:33.850
What are the assumptions and constraints?

20
00:01:33.850 --> 00:01:35.450
What resources are available?

21
00:01:35.450 --> 00:01:39.614
This is in terms of both personnel and
capital,

22
00:01:39.614 --> 00:01:43.685
such as computer systems, instruments etc.

23
00:01:43.685 --> 00:01:47.425
What are the main costs
associated with this project?

24
00:01:47.425 --> 00:01:49.335
What are the potential benefits?

25
00:01:49.335 --> 00:01:52.625
What risks are there in
pursuing the project?

26
00:01:52.625 --> 00:01:57.125
What are the contingencies to
potential risks, and so on?

27
00:01:57.125 --> 00:02:02.570
Answers to these questions will help you
get a better overview of the situation.

28
00:02:02.570 --> 00:02:05.700
And better understanding of
what the project involves.

29
00:02:05.700 --> 00:02:10.172
Then you need to define your goals and
objectives,

30
00:02:10.172 --> 00:02:13.813
based on the answers to these questions.

31
00:02:13.813 --> 00:02:17.850
Defining success criteria
is also very important.

32
00:02:17.850 --> 00:02:20.770
What do you hope to achieve
by the end of this project?

33
00:02:20.770 --> 00:02:22.368
Having clear goals and and

34
00:02:22.368 --> 00:02:28.280
success criteria will help you to assess
the project throughout its life cycle.

35
00:02:28.280 --> 00:02:32.780
Once you know the problem you want to
address and understand the constraints and

36
00:02:32.780 --> 00:02:37.990
goals, then you can formulate
the plan to come up with the answer,

37
00:02:37.990 --> 00:02:41.330
that is the solution to
your business problem.

38
00:02:41.330 --> 00:02:46.920
As a summary, defining the questions
you're looking to find answers for

39
00:02:46.920 --> 00:02:51.250
is a huge factor contributing to
the success of a data science project.

40
00:02:52.260 --> 00:02:56.830
By following the explained set of steps,
you can formulate better questions

41
00:02:56.830 --> 00:03:01.500
to solve using analytical skills and
link them to business value.
WEBVTT

1
00:00:03.028 --> 00:00:04.788
Building a Big Data Strategy.

2
00:00:23.698 --> 00:00:29.760
Before we focus on big data strategy,
let's look at what strategy means.

3
00:00:30.790 --> 00:00:33.090
Although it is associated
with a military term,

4
00:00:34.600 --> 00:00:39.490
a dictionary search on strategy shows
the meaning as a plan of action or

5
00:00:39.490 --> 00:00:43.820
policy designed to achieve a major or
overall aim.

6
00:00:46.200 --> 00:00:51.990
This definition calls out the four major
parts that need to be in any strategy.

7
00:00:51.990 --> 00:00:58.110
Namely, aim, policy, plan, and action.

8
00:00:59.280 --> 00:01:02.620
Now, we are talking about
a big data strategy.

9
00:01:02.620 --> 00:01:05.770
So what do these four terms mean for us?

10
00:01:05.770 --> 00:01:10.382
When building our big data strategy,
we look at what we have,

11
00:01:10.382 --> 00:01:15.793
what high level goals we want to achieve,
what we need to do to get there,

12
00:01:15.793 --> 00:01:20.878
and what are the policies around
data from the beginning to the end.

13
00:01:24.568 --> 00:01:29.210
A big data strategy starts
with big objectives.

14
00:01:29.210 --> 00:01:33.939
Notice that I didn't say it
starts with collecting data

15
00:01:33.939 --> 00:01:39.171
because in this activity we
are really trying to identify what

16
00:01:39.171 --> 00:01:44.118
data is useful and
why by focusing on what data to collect.

17
00:01:44.118 --> 00:01:48.410
Every organization or team is unique.

18
00:01:48.410 --> 00:01:51.120
Different projects have
different objectives.

19
00:01:51.120 --> 00:01:55.960
Hence, it's important to first
define what your team's goals are.

20
00:01:55.960 --> 00:02:00.570
Have you ever had the scenario where you
see the temperature on the weather report

21
00:02:00.570 --> 00:02:03.350
and someone else highlights
the humidity instead?

22
00:02:04.500 --> 00:02:07.820
To find problems relevant to solve and

23
00:02:07.820 --> 00:02:13.290
data related to it, it might be
useful to start with your objectives.

24
00:02:13.290 --> 00:02:15.500
Once you define these objectives, or

25
00:02:15.500 --> 00:02:21.030
more generally speaking, questions to turn
big data into advantage for your business,

26
00:02:21.030 --> 00:02:25.450
you can look at what you have and
analyze the gaps and actions to get there.

27
00:02:26.940 --> 00:02:30.192
It is important to focus
on both short term and

28
00:02:30.192 --> 00:02:33.108
long term objectives in this activity.

29
00:02:33.108 --> 00:02:37.791
These objectives should also be linked
to big data analytics with business

30
00:02:37.791 --> 00:02:38.720
objectives.

31
00:02:38.720 --> 00:02:43.620
To make the best use of big data,
each company needs to evaluate how

32
00:02:43.620 --> 00:02:48.220
data science or big data analytics would
add value to their business objectives.

33
00:02:50.010 --> 00:02:53.730
Once you have established that
analytics can help your business,

34
00:02:53.730 --> 00:02:57.550
you need to create
a culture to embrace it.

35
00:02:57.550 --> 00:02:59.820
The first and foremost ingredient for

36
00:02:59.820 --> 00:03:04.790
a successful data science program
is organizational buy-in.

37
00:03:04.790 --> 00:03:07.810
A big data strategy must
have commitment and

38
00:03:07.810 --> 00:03:11.010
sponsorship from the company's leadership.

39
00:03:11.010 --> 00:03:16.930
Goals for using big data analytics should
be developed with all stakeholders and

40
00:03:16.930 --> 00:03:20.640
clearly communicated to
everyone in the organization.

41
00:03:20.640 --> 00:03:24.740
So that its value is understood and
appreciated by all.

42
00:03:27.350 --> 00:03:30.790
The next step is to build
your data science team.

43
00:03:32.160 --> 00:03:36.930
A diverse team with data scientists,
information technologists,

44
00:03:36.930 --> 00:03:42.520
application developers, and business
owners is necessary to be effective.

45
00:03:42.520 --> 00:03:46.830
As well as the mentality that everyone
works together as partners with

46
00:03:46.830 --> 00:03:47.950
common goals.

47
00:03:47.950 --> 00:03:48.970
Remember, one for all.

48
00:03:50.690 --> 00:03:54.960
No one is a customer or
service provider of another.

49
00:03:54.960 --> 00:03:59.360
Rather, everyone works together and
delivers as a team.

50
00:04:02.470 --> 00:04:05.930
Since big data is a team game,
and multi-disciplinary,

51
00:04:05.930 --> 00:04:10.180
a big part of a big data
strategy is constant training

52
00:04:10.180 --> 00:04:13.400
of team members on new big data tools and
analytics.

53
00:04:13.400 --> 00:04:17.050
As well as business practices and
objectives.

54
00:04:17.050 --> 00:04:22.095
This becomes even more critical if your
business depends on deep expertise

55
00:04:22.095 --> 00:04:27.381
on one or more subject areas with subject
matter experts working on problems,

56
00:04:27.381 --> 00:04:28.838
utilizing big data.

57
00:04:31.947 --> 00:04:37.138
Such businesses might have subject matter
experts who can be trained to add big

58
00:04:37.138 --> 00:04:42.730
data skills, and provide more value added
support than a newcomer would have.

59
00:04:42.730 --> 00:04:47.024
Similarly, any project member would
be trained to understand what

60
00:04:47.024 --> 00:04:50.564
the business objectives and
products are, and how he or

61
00:04:50.564 --> 00:04:55.478
she can utilize big data to improve those
objectives using his or her skills.

62
00:04:58.218 --> 00:05:04.186
Many organizations might benefit by having
a small data science team whose main job

63
00:05:04.186 --> 00:05:10.000
is do data experiments and test new ideas
before they get deployed at full scale.

64
00:05:12.740 --> 00:05:15.200
They might come up with
a new idea themselves

65
00:05:15.200 --> 00:05:16.990
based on the analysis they perform.

66
00:05:18.190 --> 00:05:20.220
They take more research level role.

67
00:05:21.240 --> 00:05:26.010
However, their findings can drastically
shape your business strategy

68
00:05:26.010 --> 00:05:27.540
almost on a daily basis.

69
00:05:29.060 --> 00:05:32.620
The impact of such teams
becomes evident over time

70
00:05:32.620 --> 00:05:36.950
as other parts of your organization starts
to see the results of their finding and

71
00:05:36.950 --> 00:05:38.790
analysis affecting their strategies.

72
00:05:40.110 --> 00:05:45.550
They become strategic partners of
all verticals in your business.

73
00:05:45.550 --> 00:05:47.820
Once you see that something works,

74
00:05:47.820 --> 00:05:52.210
you can start collecting more data to see
similar results at organizational scale.

75
00:05:55.270 --> 00:06:00.330
Since data is key to any big
data initiative, it is essential

76
00:06:00.330 --> 00:06:04.870
that data across the organization
is easily accessed and integrated.

77
00:06:06.280 --> 00:06:10.700
Data silos as you know, are like
a death knell on effective analytics.

78
00:06:12.820 --> 00:06:16.470
So barriers to data
access must be removed.

79
00:06:16.470 --> 00:06:21.430
Opening up the silos must be encouraged
and supported from the organization's

80
00:06:21.430 --> 00:06:26.020
leaders in order to promote a data
sharing mindset for the company.

81
00:06:28.360 --> 00:06:31.900
Another aspect of defining
your big data strategy

82
00:06:31.900 --> 00:06:34.700
is defining the policies around big data.

83
00:06:35.810 --> 00:06:40.190
Although it has an amazing amount
of potential for your business,

84
00:06:40.190 --> 00:06:44.540
using big data should also raise some
concerns in long term planning for data.

85
00:06:45.870 --> 00:06:49.270
Although this is a very complex issue,

86
00:06:49.270 --> 00:06:53.230
here are some questions you should
think of addressing around policy.

87
00:06:53.230 --> 00:06:55.350
What are the privacy concerns?

88
00:06:55.350 --> 00:06:58.770
Who should have access to,
or control data?

89
00:06:58.770 --> 00:07:04.190
What is the lifetime of data,
which is sometimes defined as volatility,

90
00:07:04.190 --> 00:07:05.290
anatomy of big data?

91
00:07:06.970 --> 00:07:09.890
How does data get curated and cleaned up?

92
00:07:11.780 --> 00:07:14.320
What ensures data quality
in the long term?

93
00:07:15.570 --> 00:07:19.530
How do different parts of your
organization communicate or

94
00:07:19.530 --> 00:07:21.400
interoperate using this data?

95
00:07:22.440 --> 00:07:26.070
Are there any legal and
regulatory standards in place?

96
00:07:28.295 --> 00:07:32.270
Cultivating an analytics
driven culture is crucial

97
00:07:32.270 --> 00:07:34.600
to the success of a big data strategy.

98
00:07:36.090 --> 00:07:39.690
The mindset that you want to
establish is that analytics

99
00:07:39.690 --> 00:07:44.200
is an integral part of doing business,
not a separate afterthought.

100
00:07:45.550 --> 00:07:50.170
Analytics activities must be tied
to your business objectives, and

101
00:07:50.170 --> 00:07:53.480
you must be willing to use analytics
in driving business decisions.

102
00:07:54.790 --> 00:07:59.598
Analytics and business together bring
about exciting opportunities and

103
00:07:59.598 --> 00:08:01.750
growth to your big data strategy.

104
00:08:04.980 --> 00:08:08.570
Finally, one size does not fit all.

105
00:08:08.570 --> 00:08:10.780
Hence, big data technologies and

106
00:08:10.780 --> 00:08:15.490
analytics is growing rapidly as your
business is an evolving entity.

107
00:08:16.740 --> 00:08:21.180
You have to iterate your strategy to
take advantage of new advances and

108
00:08:21.180 --> 00:08:24.310
also make your business more
dynamic in the face of change.

109
00:08:27.300 --> 00:08:31.380
As a summary,
when building a big data strategy,

110
00:08:31.380 --> 00:08:35.130
it is important to integrate big data
analytics with business objectives.

111
00:08:36.590 --> 00:08:40.450
Communicate goals and
provide organizational buy-in for

112
00:08:40.450 --> 00:08:42.420
analytics projects.

113
00:08:42.420 --> 00:08:46.990
Build teams with diverse talents,
and establish a teamwork mindset.

114
00:08:48.000 --> 00:08:52.370
Remove barriers to data access and
integration.

115
00:08:52.370 --> 00:08:57.160
Finally, these activities need to be
iterated to respond to new business

116
00:08:57.160 --> 00:08:59.568
goals and technological advances.
WEBVTT

1
00:00:01.150 --> 00:00:05.590
In this video, we'll talk about a new
that is usually not covered much.

2
00:00:06.820 --> 00:00:07.510
It's called valence.

3
00:00:15.960 --> 00:00:19.440
Simply put Valence
refers to Connectedness.

4
00:00:20.440 --> 00:00:23.660
The more connected data is,
the higher it's valences.

5
00:00:24.830 --> 00:00:27.000
The term valence comes from chemistry.

6
00:00:28.030 --> 00:00:32.800
In chemistry, we talk about core electrons
and valence electrons of an atom.

7
00:00:33.910 --> 00:00:38.810
Valence electrons are in the outer most
shell, have the highest energy level and

8
00:00:38.810 --> 00:00:41.960
are responsible for
bonding with other atoms.

9
00:00:41.960 --> 00:00:46.560
That higher valence results in greater
boding, that is greater connectedness.

10
00:00:47.740 --> 00:00:51.670
This idea is carried over into our
definition of the term valence

11
00:00:51.670 --> 00:00:53.180
in the context of big data.

12
00:00:55.010 --> 00:00:58.750
Data items are often directly
connected to one another.

13
00:00:58.750 --> 00:01:01.490
A city is connected to
the country it belongs to.

14
00:01:02.530 --> 00:01:06.030
Two Facebook users are connected
because they are friends.

15
00:01:06.030 --> 00:01:08.340
An employee is connected
to his work place.

16
00:01:09.640 --> 00:01:11.470
Data could also be indirectly connected.

17
00:01:12.710 --> 00:01:16.150
Two scientists are connected,
because they are both physicists.

18
00:01:17.530 --> 00:01:23.520
For a data collection valence measures
the ratio of actually connected data items

19
00:01:23.520 --> 00:01:27.390
to the possible number of connections
that could occur within the collection.

20
00:01:28.460 --> 00:01:31.100
The most important aspect of valence

21
00:01:31.100 --> 00:01:33.870
is that the data connectivity
increases over time.

22
00:01:34.910 --> 00:01:39.620
The series of network graphs comes from
a social experiment where scientists

23
00:01:39.620 --> 00:01:44.230
attending a conference were asked to meet
other scientists they did not know before.

24
00:01:44.230 --> 00:01:45.830
After several rounds of meetings,

25
00:01:45.830 --> 00:01:49.840
they found new connections
shown by their red edges.

26
00:01:49.840 --> 00:01:55.500
Increase in valence can lead to emergent
group behavior in people networks,

27
00:01:55.500 --> 00:02:00.310
like creation of new groups and coalitions
that have shared values and goals.

28
00:02:02.760 --> 00:02:04.770
A high valence data set is denser.

29
00:02:06.060 --> 00:02:09.670
This makes many regular,
analytic critiques very inefficient.

30
00:02:10.930 --> 00:02:14.350
More complex analytical methods
must be adopted to account for

31
00:02:14.350 --> 00:02:15.490
the increasing density.

32
00:02:16.780 --> 00:02:20.760
More interesting challenges arise due
to the dynamic behavior of the data.

33
00:02:22.030 --> 00:02:24.170
Now there is a need to model and

34
00:02:24.170 --> 00:02:28.480
predict how valence of a connected data
set may change with time and volume.

35
00:02:29.890 --> 00:02:34.660
The dynamic behavior also leads to
the problem of event detection,

36
00:02:34.660 --> 00:02:38.510
such as bursts in the local
cohesion in parts of the data.

37
00:02:38.510 --> 00:02:40.760
And emergent behavior
in the whole data set,

38
00:02:40.760 --> 00:02:43.570
such as increased
polarization in a community.
WEBVTT

1
00:00:01.450 --> 00:00:04.620
Now we'll talk about a form of
scalability called variety.

2
00:00:06.040 --> 00:00:10.500
In this case, scale does not
refer to the largeness of data.

3
00:00:10.500 --> 00:00:12.398
It refers to increased diversity.

4
00:00:21.985 --> 00:00:24.450
Here is an important mantra
you need to think about.

5
00:00:25.660 --> 00:00:30.530
When we, as data scientists, think of
data variety, we think of the additional

6
00:00:30.530 --> 00:00:35.450
complexity that results from more kinds
of data that we need to store, process,

7
00:00:35.450 --> 00:00:37.010
and combine.

8
00:00:37.010 --> 00:00:39.970
Now, many years ago when I
started studying data management,

9
00:00:40.980 --> 00:00:43.030
we always thought of data as tables.

10
00:00:44.310 --> 00:00:50.060
These tables could be in spreadsheets or
databases or just files, but somehow

11
00:00:50.060 --> 00:00:53.290
they will be modeled and manipulated
as rows and columns of of tables.

12
00:00:54.610 --> 00:01:00.620
Now, tables are still really important and
dominant, however today a much wider

13
00:01:00.620 --> 00:01:04.800
variety of data are collected, stored, and
analyzed to solve real world problems.

14
00:01:05.990 --> 00:01:10.610
Image data, text data, network data,
geographic maps, computer

15
00:01:10.610 --> 00:01:15.390
generated simulations are only a few of
the types of data we encounter everyday.

16
00:01:16.460 --> 00:01:21.010
The heterogeneity of data can be
characterized along several dimensions.

17
00:01:21.010 --> 00:01:22.990
We mentioned four such axes here.

18
00:01:24.910 --> 00:01:27.630
Structural variety refers
to the difference in

19
00:01:27.630 --> 00:01:29.870
the representation of the data.

20
00:01:29.870 --> 00:01:33.600
For example, an EKG signal is very
different from a newspaper article.

21
00:01:34.690 --> 00:01:39.630
A satellite image of wildfires from
NASA is very different from tweets

22
00:01:39.630 --> 00:01:42.110
sent out by people who
are seeing the fire spread.

23
00:01:43.230 --> 00:01:48.100
Media variety refers to the medium
in which the data gets delivered.

24
00:01:48.100 --> 00:01:52.150
The audio of a speech versus
the transcript of the speech

25
00:01:52.150 --> 00:01:55.300
may represent the same information
in two different media.

26
00:01:56.460 --> 00:01:59.872
Data objects like news video
may have multiple media.

27
00:01:59.872 --> 00:02:03.436
An image sequence, an audio,
and closed captioned text,

28
00:02:03.436 --> 00:02:05.730
all time synchronized to each other.

29
00:02:06.850 --> 00:02:11.160
Semantic variety is best
described two examples.

30
00:02:11.160 --> 00:02:15.480
We often use different units for
quantities we measure.

31
00:02:15.480 --> 00:02:19.870
Sometimes we also use qualitative
versus quantitative measures.

32
00:02:19.870 --> 00:02:23.230
For example, age can be a number or

33
00:02:23.230 --> 00:02:26.440
we represent it by terms like infant,
juvenile, or adult.

34
00:02:28.140 --> 00:02:31.030
Another kind of semantic
variety comes from different

35
00:02:31.030 --> 00:02:33.450
assumptions of conditions on the data.

36
00:02:33.450 --> 00:02:39.510
For example, if we conduct two income
surveys on two different groups of people,

37
00:02:39.510 --> 00:02:40.970
we may not be able to compare or

38
00:02:40.970 --> 00:02:44.020
combine them without knowing more
about the populations themselves.

39
00:02:45.300 --> 00:02:47.620
The variation and
availability takes many forms.

40
00:02:48.730 --> 00:02:52.510
For one, data can be available real time,

41
00:02:52.510 --> 00:02:56.110
like sensor data, or it can be stored,
like patient records.

42
00:02:57.350 --> 00:03:00.430
Similarly data can be
accessible continuously, for

43
00:03:00.430 --> 00:03:01.630
example from a traffic cam.

44
00:03:02.640 --> 00:03:04.490
Versus intermittently, for

45
00:03:04.490 --> 00:03:07.750
example, only when the satellite
is over the region of interest.

46
00:03:08.780 --> 00:03:12.650
This makes a difference between what
operations one can do with data,

47
00:03:12.650 --> 00:03:15.020
especially if the volume
of the data is large.

48
00:03:16.260 --> 00:03:19.700
We'll cover this in more
detail in course two

49
00:03:19.700 --> 00:03:24.049
when we explore the different genres
of data and how we model them.

50
00:03:25.620 --> 00:03:28.470
We should not think that
a single data object, or

51
00:03:28.470 --> 00:03:32.880
a collection of similar data objects,
will be all uniform in themselves.

52
00:03:32.880 --> 00:03:35.040
Emails, for example, is a hybrid entity.

53
00:03:36.480 --> 00:03:40.110
Some of this information can be a table,
like shown here.

54
00:03:41.310 --> 00:03:43.940
Now, the body of the email
usually has text in it.

55
00:03:45.040 --> 00:03:48.360
However, some of the text may
have ornaments around them.

56
00:03:48.360 --> 00:03:49.370
For example,

57
00:03:49.370 --> 00:03:54.320
the part highlighted in yellow represents
something called a markup on text.

58
00:03:55.420 --> 00:03:58.600
We'll get to markups later in the course.

59
00:03:58.600 --> 00:04:00.500
Emails contain attachments.

60
00:04:00.500 --> 00:04:02.920
These are files, or embedded images,

61
00:04:02.920 --> 00:04:06.140
or other multimedia objects
that the mailer allows.

62
00:04:06.140 --> 00:04:10.150
This screenshot from my Outlook
shows the image of a scanned image

63
00:04:10.150 --> 00:04:11.470
of a handwritten note.

64
00:04:12.540 --> 00:04:16.230
When you take a collection of
all emails from your mailbox, or

65
00:04:16.230 --> 00:04:20.360
that from an organization,
you will see that senders and

66
00:04:20.360 --> 00:04:22.490
receivers form a communication network.

67
00:04:24.130 --> 00:04:28.917
In 2001, there was a famous scandal around
a company called Enron that engaged in

68
00:04:28.917 --> 00:04:32.390
fraudulent financial reporting practices.

69
00:04:32.390 --> 00:04:37.450
Their email network, partly shown here,
has been studied by data scientist to find

70
00:04:37.450 --> 00:04:42.330
usual and unusual patterns of connections
among the people in the organization.

71
00:04:43.810 --> 00:04:47.080
An email collection can also
have it's own semantics.

72
00:04:47.080 --> 00:04:51.340
For example, an email cannot refer to,
that means cannot copy or

73
00:04:51.340 --> 00:04:52.640
forward, a previous email.

74
00:04:53.920 --> 00:04:58.170
Finally, an email server
is a real-time data source.

75
00:04:58.170 --> 00:05:00.160
But an email repository is not.

76
00:05:01.190 --> 00:05:05.610
Does email, and email collections,
demonstrate significant

77
00:05:05.610 --> 00:05:10.270
internal variation in structure,
media, semantics, and availability?
WEBVTT

1
00:00:00.090 --> 00:00:03.478
Characteristics of Big Data- Velocity.

2
00:00:19.075 --> 00:00:23.612
Velocity refers to the increasing
speed at which big data is created and

3
00:00:23.612 --> 00:00:28.160
the increasing speed at which the data
needs to be stored and analyzed.

4
00:00:29.250 --> 00:00:34.310
Processing of data in real-time to match
its production rate as it gets generated

5
00:00:34.310 --> 00:00:37.810
is a particular goal
of big data analytics.

6
00:00:37.810 --> 00:00:41.090
For example,
this type of capability allows for

7
00:00:41.090 --> 00:00:45.110
personalization of advertisement
on the web pages you visit

8
00:00:45.110 --> 00:00:49.610
based on your recent search,
viewing, and purchase history.

9
00:00:49.610 --> 00:00:55.350
If a business cannot take advantage
of the data as it gets generated, or

10
00:00:55.350 --> 00:00:59.540
at the speed analysis of it is needed,
they often miss opportunities.

11
00:01:00.620 --> 00:01:05.540
In order to build a case for the
importance of this dimension of big data,

12
00:01:05.540 --> 00:01:07.290
let's imagine we are taking a road trip.

13
00:01:08.850 --> 00:01:12.610
You're looking for
some better information to start packing.

14
00:01:12.610 --> 00:01:14.730
In this case, the newer the information,

15
00:01:14.730 --> 00:01:19.140
the higher its relevance
in deciding what to pack.

16
00:01:19.140 --> 00:01:21.420
Would you use last month's
weather information or

17
00:01:21.420 --> 00:01:24.310
data from last year at this time?

18
00:01:24.310 --> 00:01:29.550
Or, would you use the weather
information from this week, yesterday or

19
00:01:29.550 --> 00:01:30.630
better, today?

20
00:01:31.820 --> 00:01:35.750
It makes sense to obtain the latest
information about weather and

21
00:01:35.750 --> 00:01:38.830
process it in a way that
makes your decisions easier.

22
00:01:38.830 --> 00:01:44.600
If the information is old,
it doesn't matter how accurate it is.

23
00:01:45.950 --> 00:01:49.190
Being able to catch up with
the velocity of big data and

24
00:01:49.190 --> 00:01:54.640
analyzing it as it gets generated can
even impact the quality of human life.

25
00:01:54.640 --> 00:02:00.700
Sensors and smart devices monitoring
the human body can detect abnormalities

26
00:02:00.700 --> 00:02:06.510
in real time and trigger immediate action,
potentially saving lives.

27
00:02:06.510 --> 00:02:10.950
This type of processing is what
we call real time processing.

28
00:02:10.950 --> 00:02:15.210
Real-time processing is quite
different from its remote relative,

29
00:02:15.210 --> 00:02:15.800
batch processing.

30
00:02:18.270 --> 00:02:22.320
Batch processing was the norm
until a couple of years ago.

31
00:02:22.320 --> 00:02:26.160
Large amounts of data would be
fed into large machines and

32
00:02:26.160 --> 00:02:27.870
processed for days at a time.

33
00:02:29.230 --> 00:02:33.870
While this type of processing is still
very common today, decisions based on

34
00:02:33.870 --> 00:02:39.490
information that is even few days old
can be catastrophic to some businesses.

35
00:02:41.620 --> 00:02:45.320
Organizations which make
decisions on latest data

36
00:02:45.320 --> 00:02:46.960
are more likely to hit the target.

37
00:02:48.860 --> 00:02:52.780
For this reason it's important
to match the speed of processing

38
00:02:52.780 --> 00:02:57.345
with the speed of information generation,
and get real time decision making power.

39
00:02:57.345 --> 00:03:01.497
In addition, today's sensor-powered

40
00:03:01.497 --> 00:03:06.843
socioeconomic climate
requires faster decisions.

41
00:03:06.843 --> 00:03:11.332
Hence, we can not wait for
all the data to be first produced,

42
00:03:11.332 --> 00:03:13.190
then fed into a machine.

43
00:03:14.480 --> 00:03:18.830
There are many applications where
new information is streaming and

44
00:03:18.830 --> 00:03:22.120
needs to be integrated
with existing data to

45
00:03:22.120 --> 00:03:26.825
produce decisions such as emergency
response planning in a tornado, or

46
00:03:26.825 --> 00:03:32.500
deciding trading strategies in real time,
or getting estimates in advertising.

47
00:03:34.520 --> 00:03:40.340
We have to digest chunks of data as they
are produced and give meaningful results.

48
00:03:42.900 --> 00:03:44.385
As more data comes in,

49
00:03:44.385 --> 00:03:48.930
your results will need to adapt to
reflect this change in the input.

50
00:03:50.060 --> 00:03:56.160
Decisions based on processing of already
acquired data such as batch processing,

51
00:03:56.160 --> 00:03:58.660
may give an incomplete picture.

52
00:03:58.660 --> 00:04:04.550
And hence, the applications need real
time status of the context at hand.

53
00:04:04.550 --> 00:04:05.910
That is, streaming analysis.

54
00:04:07.280 --> 00:04:12.420
Fortunately, with the event
of cheap sensors technology,

55
00:04:12.420 --> 00:04:17.520
mobile phones, and social media,
we can obtain the latest information

56
00:04:17.520 --> 00:04:21.940
at a much rapid rate and
in real time in comparison with the past.

57
00:04:22.990 --> 00:04:26.650
So how do you make sure we match
the velocity of the expectations

58
00:04:26.650 --> 00:04:28.910
to gain insights from big data?

59
00:04:28.910 --> 00:04:31.900
With the velocity of the big data.

60
00:04:31.900 --> 00:04:34.050
Rate of generation, retrieval,

61
00:04:34.050 --> 00:04:37.650
or processing of data is
application specific.

62
00:04:38.760 --> 00:04:43.660
The need for real time data-driven
actions within a business case is

63
00:04:43.660 --> 00:04:48.620
what in the end dictates the velocity
of analytics over big data.

64
00:04:49.920 --> 00:04:53.620
Sometimes precision of a minute is needed.

65
00:04:53.620 --> 00:04:55.750
Sometimes half a day.

66
00:04:55.750 --> 00:05:00.710
Let's look at these four paths and
discuss when to pick the right one for

67
00:05:00.710 --> 00:05:02.270
your analysis.

68
00:05:02.270 --> 00:05:06.937
The dollar signs next to the numbers
in this example indicate how costly

69
00:05:06.937 --> 00:05:09.000
the operation is.

70
00:05:09.000 --> 00:05:11.160
The more dollars, the higher the cost.

71
00:05:12.600 --> 00:05:16.750
When the timeliness of processed
information plays no role in decision

72
00:05:16.750 --> 00:05:21.950
making, the speed at which data
is generated becomes irrelevant.

73
00:05:21.950 --> 00:05:27.220
In other words, you can wait for
as long as it takes to process data.

74
00:05:27.220 --> 00:05:29.730
Days, months, weeks.

75
00:05:29.730 --> 00:05:33.120
And once processing is over,
you will look at the results and

76
00:05:33.120 --> 00:05:34.550
probably share them with someone.

77
00:05:35.680 --> 00:05:40.640
When timeliness is not an issue,
you can choose any of the four paths.

78
00:05:41.800 --> 00:05:44.150
You will likely pick the cheapest one.

79
00:05:45.290 --> 00:05:48.720
When timeliness of end result is an issue

80
00:05:48.720 --> 00:05:52.810
deciding which of the four paths
to choose is not so simple.

81
00:05:52.810 --> 00:05:56.620
You will have to make a decision
based on cost of hardware,

82
00:05:56.620 --> 00:06:01.110
time sensitivity of information,
future scenarios.

83
00:06:01.110 --> 00:06:06.480
In other words,
this becomes a business driven question.

84
00:06:06.480 --> 00:06:12.930
For example, if speed is really important
at all costs, you will pick path four.

85
00:06:12.930 --> 00:06:19.130
As a summary, we need to pay attention
to the velocity of big data.

86
00:06:19.130 --> 00:06:23.460
Streaming data gives information
on what's going on right now.

87
00:06:23.460 --> 00:06:28.670
Streaming data has velocity, meaning
it gets generated at various rates.

88
00:06:28.670 --> 00:06:33.370
And analysis of such data in
real time gives agility and

89
00:06:33.370 --> 00:06:37.620
adaptability to maximize
benefits you want to extract.
WEBVTT

1
00:00:01.640 --> 00:00:04.377
Characteristics of Big Data, Veracity.

2
00:00:20.366 --> 00:00:23.980
Veracity of Big Data refers
to the quality of the data.

3
00:00:25.010 --> 00:00:29.760
It sometimes gets referred
to as validity or

4
00:00:29.760 --> 00:00:33.250
volatility referring to
the lifetime of the data.

5
00:00:34.840 --> 00:00:38.360
Veracity is very important for
making big data operational.

6
00:00:39.810 --> 00:00:43.900
Because big data can be noisy and
uncertain.

7
00:00:43.900 --> 00:00:50.885
It can be full of biases,
abnormalities and it can be imprecise.

8
00:00:50.885 --> 00:00:54.741
Data is of no value if it's not accurate,

9
00:00:54.741 --> 00:00:59.900
the results of big data analysis are only
as good as the data being analyzed.

10
00:01:01.480 --> 00:01:06.840
This is often described in analytics
as junk in equals junk out.

11
00:01:08.040 --> 00:01:12.760
So we can say although big data
provides many opportunities to make

12
00:01:12.760 --> 00:01:18.100
data enabled decisions,
the evidence provided by data

13
00:01:18.100 --> 00:01:22.710
is only valuable if the data
is of a satisfactory quality.

14
00:01:23.880 --> 00:01:26.870
There are many different
ways to define data quality.

15
00:01:27.890 --> 00:01:30.200
In the context of big data,

16
00:01:30.200 --> 00:01:34.040
quality can be defined as a function
of a couple of different variables.

17
00:01:35.230 --> 00:01:41.310
Accuracy of the data, the trustworthiness
or reliability of the data source.

18
00:01:41.310 --> 00:01:44.940
And how the data was generated
are all important factors

19
00:01:44.940 --> 00:01:46.720
that affect the quality of data.

20
00:01:48.100 --> 00:01:53.910
Additionally how meaningful the data
is with respect to the program that

21
00:01:53.910 --> 00:02:00.360
analyzes it, is an important factor, and
makes context a part of the quality.

22
00:02:02.040 --> 00:02:07.850
In this chart from 2015,
we see the volumes of data increasing,

23
00:02:07.850 --> 00:02:12.380
starting with small amounts
of enterprise data to larger,

24
00:02:12.380 --> 00:02:17.480
people generated voice over IP and
social media data and

25
00:02:17.480 --> 00:02:21.010
even larger machine generated sensor data.

26
00:02:21.010 --> 00:02:26.040
We also see that the uncertainty of
the data increases as we go from

27
00:02:26.040 --> 00:02:28.840
enterprise data to sensor data.

28
00:02:28.840 --> 00:02:31.320
This is as we would expect it to be.

29
00:02:31.320 --> 00:02:35.980
Traditional enterprise
data in warehouses have

30
00:02:35.980 --> 00:02:41.080
standardized quality solutions
like master processes for extract,

31
00:02:41.080 --> 00:02:46.310
transform and load of the data which
we referred to as before as ETL.

32
00:02:46.310 --> 00:02:51.080
As enterprises started incorporating less
structured and unstructured people and

33
00:02:51.080 --> 00:02:54.870
machine data into their
big data solutions,

34
00:02:54.870 --> 00:02:58.670
the data become messier and
more uncertain.

35
00:02:58.670 --> 00:03:00.210
There are many reasons for this.

36
00:03:01.220 --> 00:03:07.620
First, unstructured data on the internet
is imprecise and uncertain.

37
00:03:07.620 --> 00:03:13.730
In addition, high velocity big data
leaves very little or no time for

38
00:03:13.730 --> 00:03:19.810
ETL, and in turn hindering the quality
assurance processes of the data.

39
00:03:19.810 --> 00:03:24.090
Let's look at these product reviews for
a banana slicer on amazon.com.

40
00:03:24.090 --> 00:03:28.960
One of the five star reviews say that

41
00:03:28.960 --> 00:03:34.190
it saved her marriage and compared it
to the greatest inventions in history.

42
00:03:34.190 --> 00:03:38.900
Another five star reviewer said
that his parole officer recommended

43
00:03:38.900 --> 00:03:42.470
the slicer as he is not
allowed to be around knives.

44
00:03:42.470 --> 00:03:44.400
These are obviously fake reviewers.

45
00:03:45.410 --> 00:03:49.070
Now think of an automated product
assessment going through such

46
00:03:49.070 --> 00:03:53.930
splendid reviews and estimating lots
of sales for the banana slicer and

47
00:03:53.930 --> 00:03:58.390
in turn suggesting stocking more
of the slicer in the inventory.

48
00:03:58.390 --> 00:03:59.670
Amazon will have problems.

49
00:04:00.760 --> 00:04:06.420
For a more serious case let's look at
the Google flu trends case from 2013.

50
00:04:06.420 --> 00:04:11.788
For January 2013,
the Google Friends actually

51
00:04:11.788 --> 00:04:17.280
estimated almost twice as many
flu cases as was reported by CDC,

52
00:04:17.280 --> 00:04:20.870
the Centers for
Disease Control and Prevention.

53
00:04:22.390 --> 00:04:26.960
The primary reason behind this was that
Google Flu Trends used a big data on

54
00:04:26.960 --> 00:04:32.070
the internet and did not account properly
for uncertainties about the data.

55
00:04:33.210 --> 00:04:37.780
Maybe the news and social media
attention paid to the particularly

56
00:04:37.780 --> 00:04:42.450
serious level of flu that
year effected the estimate.

57
00:04:42.450 --> 00:04:46.840
And resulted in what we
call an over estimation.

58
00:04:46.840 --> 00:04:49.050
This is a perfect example for

59
00:04:49.050 --> 00:04:55.470
how inaccurate the results can be if
only big data is used in the analysis.

60
00:04:55.470 --> 00:04:59.820
Imagine the economical impact of
making health care preparations for

61
00:04:59.820 --> 00:05:01.920
twice the amount of flu cases.

62
00:05:01.920 --> 00:05:03.610
That would be huge.

63
00:05:03.610 --> 00:05:07.850
The Google flu trends example
also brings up the need for

64
00:05:07.850 --> 00:05:13.350
being able to identify where exactly
the big data they used comes from.

65
00:05:13.350 --> 00:05:16.300
What transformation did
big data go through up

66
00:05:16.300 --> 00:05:19.420
until the moment it was used for
a estimate?

67
00:05:19.420 --> 00:05:23.460
This is what we refer
to as data providence.

68
00:05:23.460 --> 00:05:27.630
Just like we refer to
an artifacts provenance.

69
00:05:27.630 --> 00:05:33.180
As a summary, the growing
torrents of big data pushes for

70
00:05:33.180 --> 00:05:36.590
fast solutions to utilize
it in analytical solutions.

71
00:05:37.630 --> 00:05:42.000
This creates challenges on
keeping track of data quality.

72
00:05:42.000 --> 00:05:46.510
What has been collected,
where it came from, and

73
00:05:46.510 --> 00:05:48.940
how it was analyzed prior to its use.

74
00:05:50.150 --> 00:05:52.910
This is akin to an art artifact

75
00:05:52.910 --> 00:05:55.490
having providence of everything
it has gone through.

76
00:05:56.540 --> 00:06:01.560
But even more complicated to achieve
with large volumes of data coming

77
00:06:01.560 --> 00:06:04.130
in varieties and velocities.
WEBVTT

1
00:00:00.830 --> 00:00:03.232
Characteristics of Big Data- Volume.

2
00:00:13.965 --> 00:00:19.190
Volume is the big data dimension that
relates to the sheer size of big data.

3
00:00:20.410 --> 00:00:24.580
This volume can come from
large datasets being shared or

4
00:00:24.580 --> 00:00:30.240
many small data pieces and
events being collected over time.

5
00:00:31.460 --> 00:00:35.335
Every minute 204 million emails are sent,

6
00:00:35.335 --> 00:00:41.308
200,000 photos are uploaded, and 1.8
million likes are generated on Facebook.

7
00:00:41.308 --> 00:00:48.710
On YouTube, 1.3 million videos are viewed
and 72 hours of video are uploaded.

8
00:00:51.160 --> 00:00:54.120
But how much data are we talking about?

9
00:00:54.120 --> 00:00:59.170
The size and the scale of storage for
big data can be massive.

10
00:00:59.170 --> 00:01:03.310
You heard me say words that
start with peta, exa and

11
00:01:03.310 --> 00:01:09.220
yotta, to define size, but
what does all that really mean?

12
00:01:09.220 --> 00:01:15.210
For comparison, 100 megabytes will
hold a couple of encyclopedias.

13
00:01:16.460 --> 00:01:20.340
A DVD is around 5 GBs, and

14
00:01:20.340 --> 00:01:25.837
1 TB would hold around 300
hours of good quality video.

15
00:01:25.837 --> 00:01:32.530
A data-oriented business currently
collects data in the order of terabytes,

16
00:01:32.530 --> 00:01:35.880
but petabytes are becoming more
common to our daily lives.

17
00:01:36.930 --> 00:01:42.290
CERN's large hadron collider
generates 15 petabytes a year.

18
00:01:42.290 --> 00:01:47.340
According to predictions by an IDC report
sponsored by a big data company called

19
00:01:47.340 --> 00:01:54.150
EMC, digital data, will grow by
a factor of 44 until the year 2020.

20
00:01:54.150 --> 00:01:58.190
This is a growth from 0.8 zetabytes,

21
00:01:59.750 --> 00:02:04.280
In 2009 to 35.2 zettabytes in 2020.

22
00:02:04.280 --> 00:02:11.090
A zettabyte is 1 trillion gigabytes,
that's 10 to the power of 21.

23
00:02:11.090 --> 00:02:14.140
The effects of it will be huge!

24
00:02:15.160 --> 00:02:20.420
Think of all the time, cost,
energy that will be used to store and

25
00:02:20.420 --> 00:02:23.860
make sense of such an amount of data.

26
00:02:23.860 --> 00:02:26.430
The next era will be yottabytes.

27
00:02:26.430 --> 00:02:31.050
Ten to the power of 24 and
brontobytes, ten to the power of 27.

28
00:02:31.050 --> 00:02:36.410
Which is really hard to imagine for
most of us at this time.

29
00:02:36.410 --> 00:02:41.740
This is also what we call data
at an astronomical scale.

30
00:02:41.740 --> 00:02:44.280
The choice of putting the Milky Way Galaxy

31
00:02:45.540 --> 00:02:49.560
in the middle of the circle
is not just for aesthetics.

32
00:02:50.660 --> 00:02:54.520
This is what we would see if
we were to scale up 10 to

33
00:02:54.520 --> 00:02:56.780
the 21 times into the universe.

34
00:02:56.780 --> 00:02:57.510
Cool, isn't it?

35
00:02:58.750 --> 00:03:01.760
Please refer to the reading
in this module called,

36
00:03:01.760 --> 00:03:07.220
what does astronomical scale mean,
for a nice video on the powers of ten.

37
00:03:07.220 --> 00:03:14.400
All of these point to an exponential
growth in data volume and storage.

38
00:03:14.400 --> 00:03:17.200
What is the relevance of
this much data in our world?

39
00:03:18.260 --> 00:03:20.210
Remember the planes collecting big data?

40
00:03:21.550 --> 00:03:25.170
Our hope, as passengers,
is data means better flight safety.

41
00:03:26.540 --> 00:03:32.070
The idea is to understand that businesses
and organizations are collecting and

42
00:03:32.070 --> 00:03:36.520
leveraging large volumes of data
to improve their end products,

43
00:03:36.520 --> 00:03:40.366
whether it is safety, reliability,
healthcare, or governance.

44
00:03:40.366 --> 00:03:45.300
In general, in business the goal

45
00:03:45.300 --> 00:03:50.660
is to turn this much data into
some form of business advantage.

46
00:03:50.660 --> 00:03:55.140
The question is how do we
utilize larger volumes of data

47
00:03:55.140 --> 00:03:57.920
to improve our end product's quality?

48
00:03:57.920 --> 00:04:00.790
Despite a number of
challenges related to it.

49
00:04:01.840 --> 00:04:06.650
There are a number of challenges related
to the massive volumes of big data.

50
00:04:08.210 --> 00:04:11.970
The most obvious one is of course storage.

51
00:04:11.970 --> 00:04:14.800
As the size of the data increases so

52
00:04:14.800 --> 00:04:18.740
does the amount of storage space
required to store that data efficiently.

53
00:04:19.810 --> 00:04:24.340
However, we also need to be able
to retrieve that large amount of

54
00:04:24.340 --> 00:04:26.030
data fast enough, and

55
00:04:26.030 --> 00:04:32.140
move it to processing units in a timely
fashion to get results when we need them.

56
00:04:32.140 --> 00:04:35.900
This brings additional challenges
such as networking, bandwidth,

57
00:04:35.900 --> 00:04:37.205
cost of storing data.

58
00:04:37.205 --> 00:04:41.670
In-house versus cloud storage and
things like that.

59
00:04:41.670 --> 00:04:46.530
Additional challenges arise during
processing of such large data.

60
00:04:46.530 --> 00:04:50.450
Most existing analytical methods
won't scale to such sums of

61
00:04:50.450 --> 00:04:53.500
data in terms of memory,
processing, or IO needs.

62
00:04:55.150 --> 00:04:56.980
This means their performance will drop.

63
00:04:58.060 --> 00:05:02.220
You might be able to get good performance
for data from hundreds of customers.

64
00:05:02.220 --> 00:05:09.314
But how about scaling your solution
to 1,000 or 10,000 customers?

65
00:05:09.314 --> 00:05:15.270
As the volume increases performance and
cost start becoming a challenge.

66
00:05:16.880 --> 00:05:21.530
Businesses need a holistic
strategy to handle processing of

67
00:05:21.530 --> 00:05:26.090
large scale data to their benefit
in the most cost effective manner.

68
00:05:26.090 --> 00:05:29.690
Evaluating the options across
the dimensions mentioned here,

69
00:05:29.690 --> 00:05:33.580
is the first step when it comes to
continuously increasing data size.

70
00:05:33.580 --> 00:05:38.290
We will revisit this topic
later on in this course.

71
00:05:38.290 --> 00:05:44.330
As a summary volume is the dimension
of big data related to its size and

72
00:05:44.330 --> 00:05:45.540
its exponential growth.

73
00:05:46.770 --> 00:05:52.126
The challenges with working with volumes
of big data include cost, scalability,

74
00:05:52.126 --> 00:05:56.725
and performance related to their storage,
access, and processing.
WEBVTT

1
00:00:02.826 --> 00:00:04.162
Data Science.

2
00:00:04.162 --> 00:00:06.298
Getting value out of big data.

3
00:00:26.242 --> 00:00:31.860
We have all heard data science turned
data into insights or even actions.

4
00:00:31.860 --> 00:00:34.250
But what does that really mean?

5
00:00:34.250 --> 00:00:36.990
Data science can be
thought of as a basis for

6
00:00:36.990 --> 00:00:41.440
empirical research where data is used
to induce information for observations.

7
00:00:42.490 --> 00:00:46.310
These observations are mainly data,
in our case,

8
00:00:46.310 --> 00:00:49.985
big data, related to a business or
scientific case.

9
00:00:49.985 --> 00:00:54.430
Insight is a term

10
00:00:54.430 --> 00:00:59.050
we use to refer to the data
products of data science.

11
00:00:59.050 --> 00:01:03.523
It is extracted from a diverse amount
of data through a combination of

12
00:01:03.523 --> 00:01:07.686
exploratory data analysis and modeling.

13
00:01:07.686 --> 00:01:13.440
The questions are sometimes more specific,
and sometimes it requires

14
00:01:13.440 --> 00:01:18.170
looking at the data and patterns in it
to come up with the specific question.

15
00:01:20.420 --> 00:01:25.550
Another important point to recognize
is that data science is not static.

16
00:01:25.550 --> 00:01:28.020
It is not one time analysis.

17
00:01:28.020 --> 00:01:32.920
It involves a process where models
generated to lead to insights

18
00:01:32.920 --> 00:01:37.630
are constantly improved through further
empirical evidence, or simply, data.

19
00:01:39.640 --> 00:01:45.060
For example, a book retailer like
Amazon.com can constantly improve

20
00:01:45.060 --> 00:01:49.560
the model of a customer's book preferences
using the customer demographic,

21
00:01:50.580 --> 00:01:55.630
his or her previous purchases and
the book reviews of the customer.

22
00:01:57.920 --> 00:02:02.350
The book retailer can also
uses information to predict

23
00:02:02.350 --> 00:02:07.219
which customers are likely
to like any book,

24
00:02:07.219 --> 00:02:11.560
and take action to market
the book to those customers.

25
00:02:13.840 --> 00:02:16.955
This is where we see insights
being turned into action.

26
00:02:19.834 --> 00:02:24.718
As we have seen in the book marketing
example, using data science and analysis

27
00:02:24.718 --> 00:02:29.240
of the past and current information,
data science generates actions.

28
00:02:30.350 --> 00:02:33.200
This is not just an analysis of the past,
but

29
00:02:33.200 --> 00:02:36.980
rather generation of actionable
information for the future.

30
00:02:38.370 --> 00:02:42.070
This is what we can call a prediction,
like the weather forecast.

31
00:02:43.680 --> 00:02:49.140
When you decide what to wear for
the day based on the forecast of the day,

32
00:02:49.140 --> 00:02:53.880
you are taking action based
on insight delivered to you.

33
00:02:53.880 --> 00:02:58.110
Just like this, business leaders and
decision makers

34
00:02:58.110 --> 00:03:02.170
take action based on the evidence
provided by their data science teams.

35
00:03:04.140 --> 00:03:05.870
We set data science teams.

36
00:03:06.980 --> 00:03:11.290
This comes from the breadth of information
and skill that it takes to make it happen.

37
00:03:12.590 --> 00:03:17.010
You have probably seen diagrams like
this one that describe data science.

38
00:03:18.090 --> 00:03:22.220
Data science happens at
the intersection of computer science,

39
00:03:22.220 --> 00:03:24.820
mathematics and business expertise.

40
00:03:27.170 --> 00:03:29.750
If we zoom deeper into this diagram and

41
00:03:29.750 --> 00:03:35.030
open the sets of expertise we will
see a variation of this figure.

42
00:03:36.410 --> 00:03:42.690
Even at this level, all of these circles
require deeper knowledge and skills

43
00:03:42.690 --> 00:03:48.140
in areas like domain expertise, data
engineering, statistics and computing.

44
00:03:51.185 --> 00:03:55.790
An even deeper analysis of these
skills will lead you to skills like

45
00:03:55.790 --> 00:04:00.110
machine learning, statistical modeling,
relational algebra,

46
00:04:00.110 --> 00:04:04.640
business passion, problem solving and
data visualization.

47
00:04:04.640 --> 00:04:07.582
That's a lot of skills to have for
a single person.

48
00:04:12.922 --> 00:04:17.862
These wide range of skills and
definitions of data scientists having them

49
00:04:17.862 --> 00:04:21.855
all led to discussions like
are data scientists unicorns?

50
00:04:22.895 --> 00:04:24.115
Meaning they don't exist.

51
00:04:25.375 --> 00:04:29.795
There are data science experts who have
expertise in more than one of these

52
00:04:29.795 --> 00:04:31.380
skills, for sure.

53
00:04:31.380 --> 00:04:33.795
But they're relatively rare, and

54
00:04:33.795 --> 00:04:37.470
still would probably need help from
an expert on some of these areas.

55
00:04:38.730 --> 00:04:44.550
So, in reality, data scientists
are teams of people who act like one.

56
00:04:45.560 --> 00:04:48.860
They are passionate about the story and
the meaning behind data.

57
00:04:50.310 --> 00:04:54.700
They understand they problem
they are trying to solve, and

58
00:04:54.700 --> 00:04:57.910
aim to find the right analytical
methods to solve this problem.

59
00:04:57.910 --> 00:05:04.350
And they all have an interest in
engineering solutions to solve problems.

60
00:05:06.230 --> 00:05:11.360
They also have curiosity about each
others work, and have communication

61
00:05:11.360 --> 00:05:15.940
skills to interact with the team and
present their ideas and results to others.

62
00:05:17.900 --> 00:05:23.460
As a summary, a data science team often
comes together to analyze situations,

63
00:05:23.460 --> 00:05:29.370
business or scientific cases, which none
of the individuals can solve on their own.

64
00:05:29.370 --> 00:05:32.090
There are lots of moving
parts to the solution.

65
00:05:32.090 --> 00:05:36.370
But in the end, all these parts
should come together to provide

66
00:05:36.370 --> 00:05:39.055
actionable insight based on big data.

67
00:05:41.807 --> 00:05:46.732
Being able to use evidence-based
insight in business decisions is more

68
00:05:46.732 --> 00:05:48.517
important now than ever.

69
00:05:48.517 --> 00:05:52.594
Data scientists have a combination
of technical, business and

70
00:05:52.594 --> 00:05:54.716
soft skills to make this happen.
WEBVTT

1
00:00:00.930 --> 00:00:03.310
Getting started,
characteristics of big data.

2
00:00:21.143 --> 00:00:25.786
By now you have seen that big data is a
blanket term that is used to refer to any

3
00:00:25.786 --> 00:00:30.140
collection of data so large and
complex that it exceeds the processing

4
00:00:30.140 --> 00:00:35.530
capability of conventional data
management systems and techniques.

5
00:00:35.530 --> 00:00:39.040
The applications of big data are endless.

6
00:00:39.040 --> 00:00:44.000
Every part of business and society
are changing in front our eyes due to that

7
00:00:44.000 --> 00:00:48.870
fact that we now have so much more
data and the ability for analyzing.

8
00:00:50.220 --> 00:00:52.120
But how can we characterize big data?

9
00:00:53.310 --> 00:00:56.630
You can say, I know it when i see it.

10
00:00:56.630 --> 00:00:59.440
But there are easier ways to do it.

11
00:00:59.440 --> 00:01:03.030
Big data is commonly characterized
using a number of V's.

12
00:01:04.040 --> 00:01:09.720
The first three are volume,
velocity, and variety.

13
00:01:09.720 --> 00:01:15.580
Volume refers to the vast amounts of
data that is generated every second,

14
00:01:15.580 --> 00:01:19.470
mInutes, hour, and
day in our digitized world.

15
00:01:21.130 --> 00:01:27.480
Variety refers to the ever increasing
different forms that data can come in

16
00:01:27.480 --> 00:01:32.090
such as text, images,
voice, and geospatial data.

17
00:01:34.100 --> 00:01:39.440
Velocity refers to the speed at
which data is being generated and

18
00:01:39.440 --> 00:01:42.980
the pace at which data moves
from one point to the next.

19
00:01:44.290 --> 00:01:46.510
Volume, variety, and

20
00:01:46.510 --> 00:01:52.430
velocity are the three main dimensions
that characterize big data.

21
00:01:52.430 --> 00:01:53.790
And describe its challenges.

22
00:01:55.180 --> 00:02:00.410
We have huge amounts of data
in different formats, and

23
00:02:00.410 --> 00:02:04.160
varying quality which must
be processed quickly.

24
00:02:05.200 --> 00:02:08.470
More Vs have been introduced
to the big data community

25
00:02:08.470 --> 00:02:12.070
as we discover new challenges and
ways to define big data.

26
00:02:13.310 --> 00:02:17.830
Veracity and
valence are two of these additional V's we

27
00:02:17.830 --> 00:02:21.720
will pay special attention to as
a part of this specialization.

28
00:02:22.860 --> 00:02:29.360
Veracity refers to the biases,
noise, and abnormality in data.

29
00:02:29.360 --> 00:02:34.680
Or, better yet, It refers to the often
unmeasurable uncertainties and

30
00:02:34.680 --> 00:02:38.540
truthfulness and trustworthiness of data.

31
00:02:38.540 --> 00:02:44.150
Valence refers to the connectedness
of big data in the form of graphs,

32
00:02:44.150 --> 00:02:45.180
just like atoms.

33
00:02:46.220 --> 00:02:52.180
Moreover, we must be sure to
never forget our sixth V, value.

34
00:02:53.220 --> 00:02:55.890
How do big data benefit you and
your organization?

35
00:02:57.560 --> 00:02:59.640
Without a clear strategy and

36
00:02:59.640 --> 00:03:03.700
an objective with the value
they are getting from big data.

37
00:03:03.700 --> 00:03:07.800
It is easy to imagine that organizations
will be sidetracked by all these

38
00:03:07.800 --> 00:03:12.630
challenges of big data, and not be
able to turn them into opportunities.

39
00:03:13.990 --> 00:03:17.993
Now let's start looking into the first
five of these V's in detail.
WEBVTT

1
00:00:02.060 --> 00:00:04.400
How does data science happen?

2
00:00:04.400 --> 00:00:05.850
Five P's of data science.

3
00:00:07.810 --> 00:00:13.220
Now that we identified what data science
is and how companies can strategize around

4
00:00:13.220 --> 00:00:18.660
big data to start building a purpose,
let's come back to using data science

5
00:00:18.660 --> 00:00:23.870
to get value out of big data around
the purpose or questions they defined.

6
00:00:41.348 --> 00:00:43.591
Our experience with building and

7
00:00:43.591 --> 00:00:49.040
observing successful data science projects
led to a method around the craft with

8
00:00:49.040 --> 00:00:55.255
five distinct components that can be
defined as components of data science.

9
00:00:55.255 --> 00:00:59.859
Here we define data science
as a multi-disciplinary

10
00:00:59.859 --> 00:01:03.972
craft that combines
people teaming up around

11
00:01:03.972 --> 00:01:08.910
application-specific purpose that
can be achieved through a process,

12
00:01:10.420 --> 00:01:14.100
big data computing platforms,
and programmability.

13
00:01:17.260 --> 00:01:22.460
All of these should lead to
products where the focus really

14
00:01:22.460 --> 00:01:26.760
is on the questions or purpose that are
defined by your big data strategy ideas.

15
00:01:28.960 --> 00:01:33.540
There are many technology,
data and analytical research, and

16
00:01:33.540 --> 00:01:37.110
development related activities
around the questions.

17
00:01:37.110 --> 00:01:41.490
But in the end, everything we do
in this phase is to reach to that

18
00:01:41.490 --> 00:01:43.820
final product based on our purposes.

19
00:01:44.950 --> 00:01:47.830
So, it makes sense to start with it and

20
00:01:47.830 --> 00:01:50.860
build a process around how
we make this product happen.

21
00:01:52.740 --> 00:01:55.520
Remember the wild fire
prediction project I described?

22
00:01:56.830 --> 00:02:01.070
One of the products we described
there was the rate of spread and

23
00:02:01.070 --> 00:02:02.870
direction of an ongoing fire.

24
00:02:04.460 --> 00:02:06.780
We have identified questions and

25
00:02:06.780 --> 00:02:11.060
the process that led us to
the product in the end to solve it.

26
00:02:12.960 --> 00:02:17.280
We brought together experts around
the table for fire modeling,

27
00:02:17.280 --> 00:02:21.620
data management, time series analysis,
scalable computing,

28
00:02:21.620 --> 00:02:25.060
Geographical Information Systems,
and emergency response.

29
00:02:28.050 --> 00:02:31.770
I asked them,
let's not dive into the techniques yet.

30
00:02:31.770 --> 00:02:33.080
What is the problem at large?

31
00:02:34.100 --> 00:02:40.380
How do we see ourselves solving it?

32
00:02:40.380 --> 00:02:44.780
A typical conversation around
the process starts with this question.

33
00:02:46.230 --> 00:02:51.600
Then from then on,
drilling down to many areas of expertise,

34
00:02:51.600 --> 00:02:53.830
often we blur lines between the steps.

35
00:02:55.510 --> 00:03:00.280
My wildfire team would start listing
things like, we don't have an integrated

36
00:03:00.280 --> 00:03:05.400
system or we don't have real-time
access to data programmatically,

37
00:03:05.400 --> 00:03:08.210
so we can't analyze fires on the fly.

38
00:03:08.210 --> 00:03:12.950
Or they can say, I can't integrate
sensor data with satellite data.

39
00:03:14.370 --> 00:03:20.080
All of this leads me to challenges
I can then use to define problems.

40
00:03:21.300 --> 00:03:25.560
There are many dimensions of data science
to think about within this discussion.

41
00:03:26.570 --> 00:03:31.700
Let's start with the obvious ones,
people and purpose.

42
00:03:34.535 --> 00:03:39.800
People refers to a data science team or
the projects stakeholders.

43
00:03:39.800 --> 00:03:42.700
As you know by now,
they're expert in data and

44
00:03:42.700 --> 00:03:47.028
analytics, business, computing,
science, or big data management,

45
00:03:47.028 --> 00:03:53.154
like all the set of experts I
listed in my wildfire scenario.

46
00:03:53.154 --> 00:03:59.560
The purpose refers to the challenge or
set of challenges defined by your

47
00:03:59.560 --> 00:04:04.940
big data strategy, like solving the
question related to the rate of spread and

48
00:04:04.940 --> 00:04:08.170
direction of the fire perimeter
in the wildfire case.

49
00:04:12.003 --> 00:04:17.099
Since there's a predefined team
with a purpose, a great place for

50
00:04:17.099 --> 00:04:22.190
this team to start with is
a process they could iterate on.

51
00:04:22.190 --> 00:04:27.470
We can simply say, people with purpose
will define a process to collaborate and

52
00:04:27.470 --> 00:04:28.360
communicate around.

53
00:04:30.010 --> 00:04:33.500
The process is conceptual
in the beginning and

54
00:04:33.500 --> 00:04:37.390
defines the set of steps an how
everyone can contribute to it.

55
00:04:41.060 --> 00:04:43.230
There are many ways to
look at the process.

56
00:04:44.890 --> 00:04:50.250
One way of looking at it is
as two distinct activities,

57
00:04:50.250 --> 00:04:53.130
mainly big data engineering and

58
00:04:53.130 --> 00:04:57.500
big data analytics, or
computational big data science,

59
00:04:57.500 --> 00:05:01.910
as I like to call it, as more than simple
analytics is being performed here.

60
00:05:03.840 --> 00:05:10.180
A more detailed way of looking at
the process reveals five distinct steps or

61
00:05:10.180 --> 00:05:15.214
activities of this data science process,

62
00:05:15.214 --> 00:05:21.018
namely acquire, prepare,

63
00:05:21.018 --> 00:05:26.540
analyze, report, and act.

64
00:05:26.540 --> 00:05:29.190
We can simply say that
data science happens

65
00:05:29.190 --> 00:05:31.950
at the boundary of all these steps.

66
00:05:31.950 --> 00:05:36.510
Ideally, this process should
support experimental work and

67
00:05:36.510 --> 00:05:40.980
dynamic scalability on the big data and
computing platforms.

68
00:05:44.220 --> 00:05:49.050
This five step process can be used in
alternative ways in real life big data

69
00:05:49.050 --> 00:05:53.580
applications, if we add the dependencies
of different tools to each other.

70
00:05:54.930 --> 00:05:58.490
The influence of big data pushes for

71
00:05:58.490 --> 00:06:02.930
alternative scalability approaches
at each step of the process.

72
00:06:04.010 --> 00:06:07.080
Just like you would scale
each step on its own,

73
00:06:07.080 --> 00:06:10.780
you can scale the whole
process as a whole in the end.

74
00:06:14.613 --> 00:06:20.900
One can simply say, all of these steps
have reporting needs in different forms.

75
00:06:23.280 --> 00:06:30.290
Or there is a need to draw all these
activities as an iterating process,

76
00:06:30.290 --> 00:06:35.710
including build, explore, and
scale for big data as steps.

77
00:06:38.550 --> 00:06:42.550
Big data analysis needs alternative
data management techniques and

78
00:06:42.550 --> 00:06:47.290
systems, as well as analytical tools and
methods.

79
00:06:49.160 --> 00:06:54.730
Multiple modes of scalability is needed
based on dynamic data and computing loads.

80
00:06:55.750 --> 00:06:59.800
In addition,
change in physical infrastructure,

81
00:06:59.800 --> 00:07:04.320
streaming data specific urgencies
arising from special events

82
00:07:04.320 --> 00:07:07.780
can also require multiple
modes of scalability.

83
00:07:09.230 --> 00:07:11.490
In this intro course, for simplicity,

84
00:07:11.490 --> 00:07:17.470
we will refer to the process as a set of
five sequential activities that iterate.

85
00:07:18.500 --> 00:07:23.550
However, we'll touch on scalability as
needed in our example applications.

86
00:07:26.840 --> 00:07:30.190
As a part of building
your big data process,

87
00:07:30.190 --> 00:07:34.230
it's important to simply
mention two other P's.

88
00:07:34.230 --> 00:07:39.620
The first one is big data platforms,
like the ones in the Hadoop framework,

89
00:07:39.620 --> 00:07:43.610
or other computing platforms
to scale different steps.

90
00:07:43.610 --> 00:07:47.060
The scalability should be in
the mind of all team members and

91
00:07:47.060 --> 00:07:49.030
get communicated as an expectation.

92
00:07:51.390 --> 00:07:55.970
In addition, the scalable process
should be programmable through

93
00:07:55.970 --> 00:08:01.830
utilization of reusable and
reproducible programming interfaces

94
00:08:01.830 --> 00:08:06.308
to libraries, like systems middleware,
analytical tools,

95
00:08:06.308 --> 00:08:10.395
visualization environments, and
end user reporting environments.

96
00:08:14.010 --> 00:08:16.980
Thinking of big data
applications as a process,

97
00:08:16.980 --> 00:08:22.150
including a set of activities that
the team members can collaborate over,

98
00:08:22.150 --> 00:08:26.720
also helps to build metrics for
accountability to be built into it.

99
00:08:26.720 --> 00:08:31.000
This way, expectations on cost, time,

100
00:08:31.000 --> 00:08:34.610
optimization of deliverables,
and time lines can be discussed

101
00:08:34.610 --> 00:08:39.210
between the the members starting with
the beginning of the data science process.

102
00:08:42.325 --> 00:08:45.890
Sometimes we may not be able
to do this in one step.

103
00:08:47.550 --> 00:08:52.420
And joint explorations like statistical
evaluations of intermediate results or

104
00:08:52.420 --> 00:08:55.060
accuracy of sample data
sets become important.

105
00:08:57.150 --> 00:09:02.430
As a summary, data science can be
defined as a craft of using the five P's

106
00:09:02.430 --> 00:09:07.380
identified in this lecture,
leading to a sixth P, the data product.

107
00:09:08.580 --> 00:09:12.870
Having a process within the more
business-driven Ps, like people and

108
00:09:12.870 --> 00:09:17.120
purpose, and the more technically
driven P's, like platforms and

109
00:09:17.120 --> 00:09:22.220
programmability, leads to
a streamlined approach that starts and

110
00:09:22.220 --> 00:09:26.630
ends with the product, team
accountability, and collaboration in mind.

111
00:09:27.740 --> 00:09:31.311
Data science process
provides guidelines for

112
00:09:31.311 --> 00:09:36.621
implementing big data solution,
as it helps to organize efforts and

113
00:09:36.621 --> 00:09:43.137
ensures all critical steps taken conforms
to pre-define and agreed upon metrics.
WEBVTT

1
00:00:03.008 --> 00:00:05.098
Step one, acquiring data.

2
00:00:19.898 --> 00:00:24.420
The first step in the data science
process is to acquire the data.

3
00:00:25.660 --> 00:00:29.860
You need to obtain the source material
before analyzing or acting on it.

4
00:00:31.950 --> 00:00:37.690
The first step in acquiring data is
to determine what data is available.

5
00:00:37.690 --> 00:00:41.680
Leave no stone unturned when it comes
to finding the right data sources.

6
00:00:42.770 --> 00:00:46.150
You want to identify suitable
data related to your problem and

7
00:00:47.200 --> 00:00:51.540
make use of all data that is relevant
to your problem for analysis.

8
00:00:52.720 --> 00:00:56.580
Leaving out even a small
amount of important data

9
00:00:56.580 --> 00:00:58.570
can lead to incorrect conclusions.

10
00:01:01.190 --> 00:01:04.460
Data, comes from, many places, local and

11
00:01:04.460 --> 00:01:09.280
remote, in many varieties,
structured and un-structured.

12
00:01:09.280 --> 00:01:12.250
And, with different velocities.

13
00:01:12.250 --> 00:01:17.780
There are many techniques and technologies
to access these different types of data.

14
00:01:17.780 --> 00:01:19.420
Let's discuss a few examples.

15
00:01:21.360 --> 00:01:26.020
A lot of data exists in
conventional relational databases,

16
00:01:26.020 --> 00:01:28.560
like structure big data
from organizations.

17
00:01:29.600 --> 00:01:35.270
The tool of choice to access data from
databases is structured query language or

18
00:01:35.270 --> 00:01:40.360
SQL, which is supported by all
relational databases management systems.

19
00:01:41.680 --> 00:01:47.260
Additionally, most data base systems
come with a graphical application

20
00:01:47.260 --> 00:01:52.250
environment that allows you to query and
explore the data sets in the database.

21
00:01:54.990 --> 00:02:01.870
Data can also exist in files such as
text files and Excel spreadsheets.

22
00:02:01.870 --> 00:02:05.638
Scripting languages are generally
used to get data from files.

23
00:02:05.638 --> 00:02:11.200
A scripting language is a high
level programming language

24
00:02:11.200 --> 00:02:16.020
that can be either general purpose or
specialized for specific functions.

25
00:02:17.960 --> 00:02:24.010
Common scripting languages with support
for processing files are Java Script,

26
00:02:24.010 --> 00:02:29.300
Python, PHP, Perl, R, and
MATLAB, and are many others.

27
00:02:31.780 --> 00:02:36.230
An increasingly popular way
to get data is from websites.

28
00:02:36.230 --> 00:02:40.956
Web pages are written using
a set of standards approved by

29
00:02:40.956 --> 00:02:44.906
a world wide web consortium or
shortly, W3C.

30
00:02:44.906 --> 00:02:48.650
This includes a variety of formats and
services.

31
00:02:49.920 --> 00:02:55.860
One common format is
the Extensible Markup Language, or XML,

32
00:02:55.860 --> 00:03:00.640
which uses markup symbols or tabs to
describe the contents on a webpage.

33
00:03:02.250 --> 00:03:07.920
Many websites also host web services which
produce program access to their data.

34
00:03:10.310 --> 00:03:13.110
There are several types of web services.

35
00:03:13.110 --> 00:03:16.990
The most popular is REST because it's so
easy to use.

36
00:03:18.080 --> 00:03:22.280
REST stand for
Representational State Transfer.

37
00:03:22.280 --> 00:03:26.640
And it is an approach to implementing
web services with performance,

38
00:03:26.640 --> 00:03:29.200
scalability and maintainability in mind.

39
00:03:30.940 --> 00:03:34.400
Web socket services are also
becoming more popular

40
00:03:34.400 --> 00:03:37.440
since they allow real time
modifications from web sites.

41
00:03:40.040 --> 00:03:44.840
NoSQL storage systems are increasingly
used to manage a variety of data

42
00:03:44.840 --> 00:03:45.770
types in big data.

43
00:03:46.950 --> 00:03:50.710
These data stores are databases
that do not represent data

44
00:03:50.710 --> 00:03:55.580
in a table format with columns and rows as
with conventional relational databases.

45
00:03:56.910 --> 00:04:02.180
Examples of these data stores include
Cassandra, MongoDB and HBASE.

46
00:04:03.815 --> 00:04:08.350
NoSQL data stores provide APIs
to allow users to access data.

47
00:04:09.460 --> 00:04:14.940
These APIs can be used directly or in an
application that needs to access the data.

48
00:04:16.660 --> 00:04:21.240
Additionally, most NoSQL
systems provide data access

49
00:04:21.240 --> 00:04:23.960
via a web service interface, such a REST.

50
00:04:26.780 --> 00:04:29.860
Now, let's discuss our wildfire case study

51
00:04:29.860 --> 00:04:34.350
as a real project that acquires data
using several different mechanisms.

52
00:04:35.780 --> 00:04:42.020
The WIFIRE project stores sensor data from
weather stations in a relational database.

53
00:04:43.140 --> 00:04:48.110
We use SQL to retrieve this data
from the database to create

54
00:04:48.110 --> 00:04:52.800
models to identify weather patterns
associated with Santa Anna conditions.

55
00:04:54.450 --> 00:04:58.910
To determine whether a particular weather
station is currently experiencing

56
00:04:58.910 --> 00:05:05.340
Santa Anna conditions, we access real
time data using a web socket service.

57
00:05:06.610 --> 00:05:10.170
Once we start listening to this service,

58
00:05:10.170 --> 00:05:12.980
we receive weather station
measurements as they occur.

59
00:05:14.330 --> 00:05:19.450
This data is then processed and
compared to patterns found by our models

60
00:05:19.450 --> 00:05:23.390
to determine if a weather station is
experiencing Santa Ana conditions.

61
00:05:25.060 --> 00:05:28.130
At the same time Tweets are retrieved

62
00:05:28.130 --> 00:05:32.870
using hashtags related to any fire
that is occurring in the region.

63
00:05:33.950 --> 00:05:38.080
The Tweet messages are retrieves
using the Twitter REST service.

64
00:05:38.080 --> 00:05:43.260
The idea is to determine the sentiment
of these tweets to see if people

65
00:05:43.260 --> 00:05:50.540
are expressing fear, anger or are simply
nonchalant about the nearby fire.

66
00:05:50.540 --> 00:05:55.617
The combination of sensor data and
tweet sentiments helps

67
00:05:55.617 --> 00:06:00.389
to give us a sense of the urgency
of the fire situation.

68
00:06:00.389 --> 00:06:04.570
As a summary,
big data comes from many places.

69
00:06:05.570 --> 00:06:08.000
Finding and evaluating data

70
00:06:08.000 --> 00:06:12.490
useful to your big data analytics is
important before you start acquiring data.

71
00:06:13.740 --> 00:06:15.080
Depending on the source and

72
00:06:15.080 --> 00:06:19.240
structure of data,
there are alternative ways to access it.
WEBVTT

1
00:00:02.992 --> 00:00:05.893
Step 2-A: Exploring Data

2
00:00:18.798 --> 00:00:23.194
After you've put together the data
that you need for your application,

3
00:00:23.194 --> 00:00:27.460
you might be tempted to immediately
build models to analyze the data.

4
00:00:28.520 --> 00:00:29.830
Resist this temptation.

5
00:00:30.890 --> 00:00:35.370
The first step after getting
your data is to explore it.

6
00:00:35.370 --> 00:00:39.340
Exploring data is a part of
the two-step data preparation process.

7
00:00:41.450 --> 00:00:44.560
You want to do some
preliminary investigation

8
00:00:44.560 --> 00:00:50.080
in order to gain a better understanding of
the specific characteristics of your data.

9
00:00:50.080 --> 00:00:52.620
In this step, you'll be looking for

10
00:00:52.620 --> 00:00:56.350
things like correlations,
general trends, and outliers.

11
00:00:57.530 --> 00:01:01.700
Without this step, you will not be
able to use the data effectively.

12
00:01:04.100 --> 00:01:07.520
Correlation graphs can be used
to explore the dependencies

13
00:01:07.520 --> 00:01:09.630
between different variables in the data.

14
00:01:11.210 --> 00:01:15.910
Graphing the general trends of
variables will show you if there is

15
00:01:15.910 --> 00:01:21.120
a consistent direction in which the values
of these variables are moving towards,

16
00:01:22.150 --> 00:01:24.320
like sales prices going up or down.

17
00:01:26.200 --> 00:01:34.320
In statistics, an outlier is a data point
that's distant from other data points.

18
00:01:34.320 --> 00:01:37.256
Plotting outliers will
help you double check for

19
00:01:37.256 --> 00:01:39.698
errors in the data due to measurements.

20
00:01:39.698 --> 00:01:45.390
In some cases, outliers that are not
errors might make you find a rare event.

21
00:01:47.670 --> 00:01:53.860
Additionally, summary statistics provide
numerical values to describe your data.

22
00:01:55.040 --> 00:02:00.195
Summary statistics are quantities
that capture various characteristics

23
00:02:00.195 --> 00:02:04.540
of a set of values with a single number or
a small set of numbers.

24
00:02:06.540 --> 00:02:10.674
Some basic summary statistics
that you should compute for

25
00:02:10.674 --> 00:02:15.490
your data set are mean, median,
range, and standard deviation.

26
00:02:17.000 --> 00:02:22.510
Mean and median are measures of
the location of a set of values.

27
00:02:22.510 --> 00:02:26.590
Mode is the value that occurs
most frequently in your data set.

28
00:02:27.920 --> 00:02:32.220
And range and standard deviation
are measures of spread in your data.

29
00:02:33.640 --> 00:02:38.810
Looking at these measures will give you
an idea of the nature of your data.

30
00:02:40.630 --> 00:02:43.340
They can tell you if there's
something wrong with your data.

31
00:02:44.340 --> 00:02:48.957
For example, if the range of the values
for age in your data includes

32
00:02:48.957 --> 00:02:52.926
negative numbers, or
a number much greater than 100,

33
00:02:52.926 --> 00:02:57.708
there's something suspicious in
the data that needs to be examined.

34
00:03:00.629 --> 00:03:05.900
Visualization techniques also
provide a quick and effective, and

35
00:03:05.900 --> 00:03:11.830
overall a very useful way to look at
data in this preliminary analysis step.

36
00:03:11.830 --> 00:03:15.250
A heat map, such as the one shown here,

37
00:03:15.250 --> 00:03:19.530
can quickly give you the idea
of where the hotspots are.

38
00:03:19.530 --> 00:03:22.020
Many other different types
of graphs can be used.

39
00:03:23.340 --> 00:03:27.320
Histograms show that
the distribution of the data and

40
00:03:27.320 --> 00:03:30.520
can show skewness or unusual dispersion.

41
00:03:31.920 --> 00:03:36.168
Boxplots are another type of plot for
showing data distribution.

42
00:03:38.448 --> 00:03:44.410
Line graphs are useful for seeing how
values in your data change over time.

43
00:03:44.410 --> 00:03:47.140
Spikes in the data are also easy to spot.

44
00:03:49.550 --> 00:03:54.070
Scatter plots can show you
correlation between two variables.

45
00:03:54.070 --> 00:03:57.900
Overall, there are many types
of graph to visualize data.

46
00:03:58.935 --> 00:04:02.030
They are very useful in helping
you understand the data you have.

47
00:04:04.120 --> 00:04:08.720
In summary, what you get by
exploring your data is a better

48
00:04:08.720 --> 00:04:12.510
understanding of the complexity of
the data you have to work with.

49
00:04:13.780 --> 00:04:16.418
This, in turn,
will guide the rest of your process.
WEBVTT

1
00:00:00.005 --> 00:00:06.278
Step 2-B: Pre-processing Data

2
00:00:20.283 --> 00:00:24.904
The raw data that you get directly from
your sources are never in the format that

3
00:00:24.904 --> 00:00:26.875
you need to perform analysis on.

4
00:00:27.995 --> 00:00:31.885
There are two main goals in
the data pre-processing step.

5
00:00:31.885 --> 00:00:36.455
The first is to clean the data to
address data quality issues, and

6
00:00:36.455 --> 00:00:41.320
the second is to transform the raw
data to make it suitable for analysis.

7
00:00:43.850 --> 00:00:47.050
A very important part of data preparation

8
00:00:47.050 --> 00:00:49.606
is to address quality
of issues in your data.

9
00:00:49.606 --> 00:00:54.000
Real-world data is messy.

10
00:00:54.000 --> 00:01:00.160
There are many examples of quality issues
with data from real applications including

11
00:01:00.160 --> 00:01:05.990
inconsistent data like a customer with two
different addresses, duplicate customer

12
00:01:05.990 --> 00:01:11.460
records, for example, customers address
recorded at two different sales locations.

13
00:01:12.490 --> 00:01:14.600
And the two recordings don't agree.

14
00:01:16.190 --> 00:01:19.160
Missing customer agent demographics or
studies.

15
00:01:21.010 --> 00:01:25.750
Missing values like missing a customer
age in the demographic studies.

16
00:01:27.000 --> 00:01:32.160
invalid data like an invalid zip code for
example, a six digit code.

17
00:01:33.350 --> 00:01:39.060
And outliers like a sense of failure
causing values to be much higher or

18
00:01:39.060 --> 00:01:41.480
lower than expected for a period of time.

19
00:01:43.340 --> 00:01:45.860
Since we get the data downstream

20
00:01:45.860 --> 00:01:48.960
we usually have little control
over how the data is collected.

21
00:01:50.390 --> 00:01:55.120
Preventing data quality problems as
the data is being collected is not

22
00:01:55.120 --> 00:01:56.070
often an option.

23
00:01:57.740 --> 00:01:59.950
So we have the data that we get and

24
00:01:59.950 --> 00:02:03.770
we have to address quality issues
by detecting and correcting them.

25
00:02:05.710 --> 00:02:09.170
Here are some approaches we can take
to address this quality issues.

26
00:02:11.780 --> 00:02:14.630
We can remove data records
with missing values.

27
00:02:16.330 --> 00:02:19.290
We can merge duplicate records.

28
00:02:19.290 --> 00:02:23.430
This will require a way to determine
how to resolve conflicting values.

29
00:02:24.780 --> 00:02:29.320
Perhaps it makes sense to retain the newer
value whenever there's a conflict.

30
00:02:31.030 --> 00:02:34.500
For invalid values, the best estimate for

31
00:02:34.500 --> 00:02:38.330
a reasonable value can be
used as a replacement.

32
00:02:38.330 --> 00:02:42.800
For example, for
a missing age value for an employee,

33
00:02:42.800 --> 00:02:47.320
a reasonable value can be estimated based
on the employee's length of employment.

34
00:02:49.820 --> 00:02:54.340
Outliers can also be removed if
they are not important to the task.

35
00:02:56.590 --> 00:03:00.370
In order to address data
quality issues effectively,

36
00:03:00.370 --> 00:03:04.590
knowledge about the application,
such as how the data was collected,

37
00:03:04.590 --> 00:03:09.880
the user population, and the intended
uses of the application is important.

38
00:03:11.680 --> 00:03:15.280
This domain knowledge is
essential to making informed

39
00:03:15.280 --> 00:03:19.070
decisions on how to handle incomplete or
incorrect data.

40
00:03:22.740 --> 00:03:25.480
The second part of preparing data

41
00:03:25.480 --> 00:03:29.299
is to manipulate the clean data into
the format needed for analysis.

42
00:03:30.500 --> 00:03:32.910
The step is known by many names.

43
00:03:34.940 --> 00:03:39.894
Data manipulation,
data preprocessing, data wrangling,

44
00:03:39.894 --> 00:03:43.419
and even data munging, some operations for

45
00:03:43.419 --> 00:03:49.420
this type of operation I mean data
munging, wrangling, preprocessing,

46
00:03:49.420 --> 00:03:54.373
include, scaling, transformation,
feature selection,

47
00:03:54.373 --> 00:03:58.778
dimensionality reduction,
and data manipulation.

48
00:04:01.628 --> 00:04:08.840
Scaling involves changing the range of
values to be between a specified range.

49
00:04:08.840 --> 00:04:10.970
Such as from zero to one.

50
00:04:12.220 --> 00:04:16.460
This is done to avoid having certain
features that large values from

51
00:04:16.460 --> 00:04:18.570
dominating the results.

52
00:04:18.570 --> 00:04:22.990
For example,
in analyzing data with height and weight.

53
00:04:22.990 --> 00:04:27.350
To magnitude of weight values is much
greater than of the height values.

54
00:04:29.220 --> 00:04:32.880
So scaling all values
to be between zero and

55
00:04:32.880 --> 00:04:37.670
one will equalize contributions from
both height and weight features.

56
00:04:40.900 --> 00:04:46.000
Various transformations can be performed
on the data to reduce noise and

57
00:04:46.000 --> 00:04:46.720
variability.

58
00:04:48.380 --> 00:04:50.970
One such transformation is aggregation.

59
00:04:52.430 --> 00:04:57.260
Aggregate data generally results
in data with less variability,

60
00:04:57.260 --> 00:04:58.890
which may help with your analysis.

61
00:05:00.190 --> 00:05:05.720
For example, daily sales figures
may have many serious changes.

62
00:05:06.730 --> 00:05:11.880
Aggregating values to weekly or monthly
sales figures will result in similar data.

63
00:05:14.720 --> 00:05:19.340
Other filtering techniques can also be
used to remove variability in the data.

64
00:05:19.340 --> 00:05:23.530
Of course, this comes at
the cost of less detailed data.

65
00:05:23.530 --> 00:05:27.760
So these factors must be weighed for
the specific application.

66
00:05:30.570 --> 00:05:36.400
Future selection can involve removing
redundant or irrelevant features,

67
00:05:36.400 --> 00:05:40.300
combining features, and
creating new features.

68
00:05:41.460 --> 00:05:44.070
During the exploring data step,

69
00:05:44.070 --> 00:05:47.860
you might have discovered that
two features are correlated.

70
00:05:49.030 --> 00:05:52.350
In that case one of these
features can be removed

71
00:05:52.350 --> 00:05:55.090
without negatively affecting
the analysis results.

72
00:05:56.170 --> 00:05:59.870
For example,
the purchase price of a product and

73
00:05:59.870 --> 00:06:03.360
the amount of sales tax paid,
are likely to be correlated.

74
00:06:04.800 --> 00:06:08.550
Eliminating the sales tax amount,
then will be beneficial.

75
00:06:10.860 --> 00:06:12.810
Removing redundant or

76
00:06:12.810 --> 00:06:17.010
irrelevant features will make
the subsequent analysis much simpler.

77
00:06:19.320 --> 00:06:25.220
In other cases, you may want to combine
features or create new ones.

78
00:06:25.220 --> 00:06:29.540
For example,
adding the applicant's education level

79
00:06:29.540 --> 00:06:32.820
as a feature to a loan approval
application would make sense.

80
00:06:34.940 --> 00:06:39.790
There are also algorithms to automatically
determine the most relevant features,

81
00:06:39.790 --> 00:06:42.130
based on various mathematical properties.

82
00:06:45.440 --> 00:06:50.200
Dimensionality reduction is useful when
the data set has a large number of

83
00:06:50.200 --> 00:06:50.750
dimensions.

84
00:06:52.020 --> 00:06:55.830
It involves finding a smaller
subset of dimensions that

85
00:06:55.830 --> 00:06:58.350
captures most of
the variation in the data.

86
00:07:00.030 --> 00:07:03.600
This reduces the dimensions of the data

87
00:07:03.600 --> 00:07:08.400
while eliminating irrelevant features and
makes analysis simpler.

88
00:07:10.130 --> 00:07:12.089
A technique commonly used for

89
00:07:12.089 --> 00:07:16.878
dimensional reduction is called
principle component analysis or PCA.

90
00:07:20.528 --> 00:07:26.120
Raw data often has to be manipulated to
be in the correct format for analysis.

91
00:07:27.140 --> 00:07:32.890
For example, from samples recording
daily changes in stock prices,

92
00:07:32.890 --> 00:07:35.520
we may want the capture price changes for

93
00:07:35.520 --> 00:07:38.990
a particular market segments
like real estate or health care.

94
00:07:40.230 --> 00:07:45.150
This would require determining which
stocks belong to which market segment.

95
00:07:45.150 --> 00:07:49.540
Grouping them together, and
perhaps computing the mean, range,

96
00:07:49.540 --> 00:07:51.530
standard deviation for each group.

97
00:07:54.120 --> 00:07:54.760
In summary,

98
00:07:55.910 --> 00:08:00.470
data preparation is a very important
part of the data science process.

99
00:08:00.470 --> 00:08:05.530
In fact, this is where you will spend most
of your time on any data science effort.

100
00:08:06.710 --> 00:08:11.680
It can be a tedious process,
but it is a crucial step.

101
00:08:11.680 --> 00:08:15.200
Always remember, garbage in, garbage out.

102
00:08:15.200 --> 00:08:16.987
If you don't spend the time and

103
00:08:16.987 --> 00:08:21.250
effort to create good data for the
analysis, you will not get good results

104
00:08:21.250 --> 00:08:25.399
no matter how sophisticated
the analysis technique you're using is.
WEBVTT

1
00:00:00.005 --> 00:00:03.004
Step 3: Analyzing Data.

2
00:00:12.608 --> 00:00:15.650
Now that you have your
data nicely prepared,

3
00:00:15.650 --> 00:00:18.140
the next step is to analyze the data.

4
00:00:19.350 --> 00:00:23.620
Data analysis involves building
a model from your data,

5
00:00:23.620 --> 00:00:25.350
which is called input data.

6
00:00:26.620 --> 00:00:32.370
The input data is used by the analysis
technique to build a model.

7
00:00:34.010 --> 00:00:38.340
What your model generates
is the output data.

8
00:00:38.340 --> 00:00:41.140
There are different types of problems, and

9
00:00:41.140 --> 00:00:43.890
so there are different types
of analysis techniques.

10
00:00:45.370 --> 00:00:50.424
The main categories of analysis techniques
are classification, regression,

11
00:00:50.424 --> 00:00:56.330
clustering, association analysis,
and graph analysis.

12
00:00:56.330 --> 00:00:58.520
We will describe each one.

13
00:00:58.520 --> 00:01:03.650
In classification, the goal is to
predict the category of the input data.

14
00:01:04.740 --> 00:01:10.102
An example of this is predicting
the weather as being sunny,

15
00:01:10.102 --> 00:01:13.850
rainy, windy, or cloudy in this case.

16
00:01:13.850 --> 00:01:19.820
Another example is to classify
a tumor as either benign or malignant.

17
00:01:21.170 --> 00:01:27.270
In this case, the classification is
referred to as binary classification,

18
00:01:27.270 --> 00:01:29.830
since there are only two categories.

19
00:01:29.830 --> 00:01:32.660
But you can have many categories as well,

20
00:01:32.660 --> 00:01:37.530
as the weather prediction problem
shown here having four categories.

21
00:01:37.530 --> 00:01:41.780
Another example is to identify
handwritten digits as

22
00:01:41.780 --> 00:01:45.860
being in one of the ten
categories from zero to nine.

23
00:01:48.160 --> 00:01:53.310
When your model has to predict
a numeric value instead of a category,

24
00:01:53.310 --> 00:01:56.080
then the task becomes
a regression problem.

25
00:01:57.380 --> 00:02:01.210
An example of regression is to
predict the price of a stock.

26
00:02:02.410 --> 00:02:06.250
The stock price is a numeric value,
not a category.

27
00:02:06.250 --> 00:02:09.560
So this is a regression task
instead of a classification task.

28
00:02:11.300 --> 00:02:15.020
Other examples of regression
are estimating the weekly sales of a new

29
00:02:15.020 --> 00:02:19.740
product and
predicting the score on a test.

30
00:02:19.740 --> 00:02:25.910
In clustering, the goal is to
organize similar items into groups.

31
00:02:25.910 --> 00:02:31.390
An example is grouping a company's
customer base into distinct segments for

32
00:02:31.390 --> 00:02:36.360
more effective targeted marketing
like seniors, adults and

33
00:02:36.360 --> 00:02:38.420
teenagers, as we see here.

34
00:02:38.420 --> 00:02:43.050
Another such example is identifying
areas of similar topography,

35
00:02:43.050 --> 00:02:47.420
like mountains, deserts,
plains for land use application.

36
00:02:47.420 --> 00:02:52.300
Yet another example is determining
different groups of weather patterns,

37
00:02:52.300 --> 00:02:55.150
like rainy, cold or snowy.

38
00:02:55.150 --> 00:02:58.740
The goal in association analysis
is to come up with a set

39
00:02:58.740 --> 00:03:03.330
of rules to capture associations
within items or events.

40
00:03:03.330 --> 00:03:07.935
The rules are used to determine when
items or events occur together.

41
00:03:07.935 --> 00:03:12.300
A common application of association
analysis is known as market

42
00:03:12.300 --> 00:03:17.710
basket analysis, which is used to
understand customer purchasing behavior.

43
00:03:17.710 --> 00:03:22.660
For example, association analysis
can reveal that banking customers

44
00:03:22.660 --> 00:03:27.040
who have certificate of deposit accounts,
surety CDs, also

45
00:03:27.040 --> 00:03:31.790
tend to be interested in other investment
vehicles, such as money market accounts.

46
00:03:31.790 --> 00:03:35.000
This information can be used for
cross-selling.

47
00:03:35.000 --> 00:03:39.140
If you advertise money market
accounts to your customers with CDs,

48
00:03:39.140 --> 00:03:42.100
they're likely to open such an account.

49
00:03:42.100 --> 00:03:47.150
According to data mining folklore,
a supermarket chain used association

50
00:03:47.150 --> 00:03:51.990
analysis to discover a connection between
two seemingly unrelated products.

51
00:03:51.990 --> 00:03:56.940
They discovered that many customers who go
to the supermarket late on Sunday night

52
00:03:56.940 --> 00:04:02.330
to buy diapers also tend to buy beer,
who are likely to be fathers.

53
00:04:02.330 --> 00:04:05.590
This information was then
used to place beer and

54
00:04:05.590 --> 00:04:10.720
diapers close together and
they saw a jump in sales of both items.

55
00:04:10.720 --> 00:04:13.760
This is the famous diaper beer connection.

56
00:04:13.760 --> 00:04:18.090
When your data can be transformed into
a graph representation with nodes and

57
00:04:18.090 --> 00:04:22.330
links, then you want to use graph
analytics to analyze your data.

58
00:04:22.330 --> 00:04:26.220
This kind of data comes about when
you have a lot of entities and

59
00:04:26.220 --> 00:04:30.150
connections between those entities,
like social networks.

60
00:04:30.150 --> 00:04:35.111
Some examples where graph analytics can
be useful are exploring the spread of

61
00:04:35.111 --> 00:04:39.780
a disease or epidemic by analyzing
hospitals' and doctors' records.

62
00:04:39.780 --> 00:04:44.849
Identification of security threats
by monitoring social media,

63
00:04:44.849 --> 00:04:46.483
email and text data.

64
00:04:46.483 --> 00:04:51.015
And optimization of mobile
communications network traffic.

65
00:04:51.015 --> 00:04:56.110
And optimization of mobile
telecommunications network traffic,

66
00:04:56.110 --> 00:04:59.690
to ensure call quality and
reduce dropped calls.

67
00:04:59.690 --> 00:05:03.850
Modeling starts with selecting,
one of the techniques we listed

68
00:05:03.850 --> 00:05:08.680
as the appropriate analysis technique,
depending on the type of problem you have.

69
00:05:08.680 --> 00:05:12.690
Then you construct the model
using the data you've prepared.

70
00:05:12.690 --> 00:05:17.210
To validate the model,
you apply it to new data samples.

71
00:05:17.210 --> 00:05:19.960
This is to evaluate how well the model

72
00:05:19.960 --> 00:05:22.540
does on data that was
used to construct it.

73
00:05:22.540 --> 00:05:26.870
The common practice is to divide
the prepared data into a set of data for

74
00:05:26.870 --> 00:05:30.170
constructing the model and
reserving some of the data for

75
00:05:30.170 --> 00:05:33.330
evaluating the model after
it has been constructed.

76
00:05:33.330 --> 00:05:37.740
You can also use new data prepared the
same way as with the data that was used to

77
00:05:37.740 --> 00:05:39.060
construct model.

78
00:05:39.060 --> 00:05:43.950
Evaluating the model depends on the type
of analysis techniques you used.

79
00:05:43.950 --> 00:05:47.595
Let's briefly look at how
to evaluate each technique.

80
00:05:47.595 --> 00:05:52.595
For classification and regression,
you will have the correct output for

81
00:05:52.595 --> 00:05:54.655
each sample in your input data.

82
00:05:54.655 --> 00:05:56.945
Comparing the correct output and

83
00:05:56.945 --> 00:06:01.880
the output predicted by the model,
provides a way to evaluate the model.

84
00:06:01.880 --> 00:06:05.150
For clustering,
the groups resulting from clustering

85
00:06:05.150 --> 00:06:08.850
should be examined to see if they
make sense for your application.

86
00:06:08.850 --> 00:06:13.250
For example, do the customer
segments reflect your customer base?

87
00:06:13.250 --> 00:06:17.310
Are they helpful for
use in your targeted marketing campaigns?

88
00:06:17.310 --> 00:06:19.170
For association analysis and

89
00:06:19.170 --> 00:06:24.940
graph analysis, some investigation will be
needed to see if the results are correct.

90
00:06:24.940 --> 00:06:27.920
For example, network traffic delays

91
00:06:27.920 --> 00:06:33.220
need to be investigated to see what your
model predicts is actually happening.

92
00:06:33.220 --> 00:06:36.830
And whether the sources of the delays
are where they are predicted

93
00:06:36.830 --> 00:06:38.050
to be in the real system.

94
00:06:39.160 --> 00:06:44.450
After you have evaluated your model to get
a sense of its performance on your data,

95
00:06:44.450 --> 00:06:47.510
you will be able to
determine the next steps.

96
00:06:47.510 --> 00:06:51.000
Some questions to consider are,
should the analysis be

97
00:06:51.000 --> 00:06:55.340
performed with more data in order
to get a better model performance?

98
00:06:55.340 --> 00:06:57.760
Would using different data types help?

99
00:06:57.760 --> 00:06:59.990
For example, in your clustering results,

100
00:06:59.990 --> 00:07:04.070
is it difficult to distinguish
customers from distinct regions?

101
00:07:04.070 --> 00:07:07.050
Would adding zip code
to your input data help

102
00:07:07.050 --> 00:07:10.150
to generate finer grained
customer segments?

103
00:07:10.150 --> 00:07:14.060
Do the analysis results suggest
a more detailed look at

104
00:07:14.060 --> 00:07:15.900
some aspect of the problem?

105
00:07:15.900 --> 00:07:19.690
For example, predicting sunny
weather gives very good results,

106
00:07:19.690 --> 00:07:22.390
but rainy weather
predictions are just so-so.

107
00:07:22.390 --> 00:07:27.840
This means that you should take a closer
look at your examples for rainy weather.

108
00:07:27.840 --> 00:07:31.520
Perhaps you just need more
samples of rainy weather, or

109
00:07:31.520 --> 00:07:34.845
perhaps there are some
anomalies in those samples.

110
00:07:34.845 --> 00:07:40.380
Or maybe there are some missing data
that needs to be included in order

111
00:07:40.380 --> 00:07:42.440
to completely capture rainy weather.

112
00:07:42.440 --> 00:07:47.037
The ideal situation would be that your
model platforms very well with respect to

113
00:07:47.037 --> 00:07:49.620
the success criteria that were determined

114
00:07:49.620 --> 00:07:53.380
when you defined the problem at
the beginning of the project.

115
00:07:53.380 --> 00:07:56.920
In that case, you're ready to
move on to communicating and

116
00:07:56.920 --> 00:08:00.870
acting on the results that you
obtained from your analysis.

117
00:08:00.870 --> 00:08:05.670
As a summary, data analysis involves
selecting the appropriate technique for

118
00:08:05.670 --> 00:08:09.940
your problem, building the model,
then evaluating the results.

119
00:08:11.150 --> 00:08:13.516
As there are different types of problems,

120
00:08:13.516 --> 00:08:16.678
there are also different
types of analysis techniques.
WEBVTT

1
00:00:02.925 --> 00:00:05.646
Step four, reporting insights.

2
00:00:15.628 --> 00:00:20.498
The fourth step in our data science
process is reporting the insights gained

3
00:00:20.498 --> 00:00:21.800
from our analysis.

4
00:00:23.660 --> 00:00:27.840
This is a very important step
to communicate your insights and

5
00:00:27.840 --> 00:00:30.320
make a case for
what actions should follow.

6
00:00:32.040 --> 00:00:37.380
It can change shape based on your
audience and should not be taken lightly.

7
00:00:38.510 --> 00:00:39.760
So how do you get started?

8
00:00:43.150 --> 00:00:48.040
The first thing to do is to look
at your analysis results and

9
00:00:48.040 --> 00:00:53.720
decide what to present or report as the
biggest value or biggest set of values.

10
00:00:54.990 --> 00:00:59.530
In deciding what to present you
should ask yourself these questions.

11
00:01:01.260 --> 00:01:02.740
What is the punchline?

12
00:01:02.740 --> 00:01:05.330
In other words, what are the main results?

13
00:01:07.930 --> 00:01:12.590
What added value do
these results provide or

14
00:01:12.590 --> 00:01:14.870
how can the model add to the application?

15
00:01:17.120 --> 00:01:21.840
How do the results compare to
the success criteria determined at

16
00:01:21.840 --> 00:01:23.260
the beginning of the project?

17
00:01:26.030 --> 00:01:30.220
Answers to these questions are the items
you need to include in your report or

18
00:01:30.220 --> 00:01:30.990
presentation.

19
00:01:32.120 --> 00:01:36.336
So make them the main topics and
gather facts to back them up.

20
00:01:39.198 --> 00:01:44.330
Keep in mind that not all of
your results may be rosy.

21
00:01:44.330 --> 00:01:49.650
Your analysis may show results that are
counter to what you were hoping to find,

22
00:01:49.650 --> 00:01:53.410
or results that are inconclusive or
puzzling.

23
00:01:54.600 --> 00:01:56.530
You need to show these results as well.

24
00:01:58.620 --> 00:02:02.950
Domain experts may find some of
these results to be puzzling, and

25
00:02:02.950 --> 00:02:06.932
inconclusive findings may
lead to additional analysis.

26
00:02:08.020 --> 00:02:11.920
Remember the point of
reporting your findings

27
00:02:11.920 --> 00:02:14.410
is to determine what
the next step should be.

28
00:02:17.070 --> 00:02:21.538
All findings must be presented so
that informed decisions can be made.

29
00:02:24.818 --> 00:02:29.180
Visualization is an important
tool in presenting your results.

30
00:02:30.220 --> 00:02:35.400
The techniques that we discuss and
explore in data can be used here as well.

31
00:02:35.400 --> 00:02:36.760
What were they?

32
00:02:36.760 --> 00:02:41.690
Scatter plots, line graphs,
heat maps, and other types of graphs

33
00:02:43.040 --> 00:02:46.390
are effective ways to present
your results visually.

34
00:02:48.330 --> 00:02:52.150
This time you're not
plotting the input data, but

35
00:02:52.150 --> 00:02:55.250
you're plotting the output
data with similar tools.

36
00:02:57.120 --> 00:03:01.238
You should also have tables with
details from your analysis as backups,

37
00:03:01.238 --> 00:03:04.418
if someone wants to take
a deeper dive into the results.

38
00:03:07.398 --> 00:03:10.080
There are many visualization
tools that are available.

39
00:03:11.660 --> 00:03:14.750
Some of the most popular open
source ones are listed here.

40
00:03:16.205 --> 00:03:20.370
R is a software package for
general data analysis.

41
00:03:21.990 --> 00:03:24.830
It has powerful visualization
capabilities as well.

42
00:03:26.300 --> 00:03:30.800
Python is a general purpose
programming language

43
00:03:30.800 --> 00:03:35.090
that also has a number of packages to
support data analysis and graphics.

44
00:03:36.510 --> 00:03:39.690
D3 is a JavaScript library for

45
00:03:39.690 --> 00:03:44.820
producing interactive web based
visualizations and data driven documents.

46
00:03:46.500 --> 00:03:51.325
Leaflet is a lightweight mobile
friendly JavaScript library

47
00:03:51.325 --> 00:03:53.650
to create interactive maps.

48
00:03:55.360 --> 00:04:00.656
Tableau Public Allows you
to create visualizations,

49
00:04:00.656 --> 00:04:07.916
in your public profile, and share them,
or put them, on a site, or blog.

50
00:04:07.916 --> 00:04:12.925
Google Charts provides
cross-browser compatibility,

51
00:04:12.925 --> 00:04:17.940
and closed platform portability
to iPhones and Android.

52
00:04:19.430 --> 00:04:27.865
Timeline is a JavaScript library
that allows you to create timelines.

53
00:04:27.865 --> 00:04:33.787
In summary, you want to report your
findings by presenting your results and

54
00:04:33.787 --> 00:04:37.780
value add with graphs
using visualization tools.
WEBVTT

1
00:00:02.010 --> 00:00:05.922
Step 5, Act, turning insights into action.

2
00:00:15.758 --> 00:00:20.048
Now that you have evaluated
the results from your analysis and

3
00:00:20.048 --> 00:00:25.329
generated reports on the potential value
of the results, the next step is to

4
00:00:25.329 --> 00:00:31.075
determine what action or actions should
be taken, based on the insights gained?

5
00:00:31.075 --> 00:00:34.290
Remember why we started
bringing together the data and

6
00:00:34.290 --> 00:00:36.570
analyzing it in the first place?

7
00:00:36.570 --> 00:00:41.100
To find actionable insights
within all these data sets,

8
00:00:41.100 --> 00:00:44.970
to answer questions, or for
improving business processes.

9
00:00:46.120 --> 00:00:47.470
For example,

10
00:00:47.470 --> 00:00:51.440
is there something in your process that
should change to remove bottle necks?

11
00:00:52.580 --> 00:00:56.640
Is there data that should be added to your
application to make it more accurate?

12
00:00:57.810 --> 00:01:02.440
Should you segment your population
into more well defined groups for

13
00:01:02.440 --> 00:01:04.240
more effective targeted marketing?

14
00:01:05.510 --> 00:01:09.508
This is the first step in
turning insights into action.

15
00:01:09.508 --> 00:01:12.733
Now that you've determined
what action to take,

16
00:01:12.733 --> 00:01:17.450
the next step is figuring out
how to implement the action.

17
00:01:17.450 --> 00:01:21.610
What is necessary to add this action
into your process or application?

18
00:01:22.820 --> 00:01:24.080
How should it be automated?

19
00:01:25.440 --> 00:01:31.110
The stakeholders need to be identified and
become involved in this change.

20
00:01:31.110 --> 00:01:36.090
Just as with any process improvement
changes, we need to monitor and

21
00:01:36.090 --> 00:01:40.020
measure the impact of the action
on the process or application.

22
00:01:41.300 --> 00:01:45.600
Assessing the impact
leads to an evaluation.

23
00:01:45.600 --> 00:01:49.710
Evaluating results from the implemented
action will determine your next steps.

24
00:01:50.740 --> 00:01:55.640
Is there additional analysis that need
to be performed in order to yield

25
00:01:55.640 --> 00:01:56.730
even better results?

26
00:01:58.100 --> 00:01:59.740
What data should be revisited?

27
00:02:00.950 --> 00:02:04.980
Are there additional opportunities
that should be explored?

28
00:02:04.980 --> 00:02:09.685
For example, let's not forget
what big data enables us to do.

29
00:02:09.685 --> 00:02:16.553
Real-time actions based on high
velocity streaming information.

30
00:02:16.553 --> 00:02:21.067
We need to define what part of our
business needs real-time action to be

31
00:02:21.067 --> 00:02:25.670
able to influence the operations or
the interaction with the customer.

32
00:02:27.450 --> 00:02:32.430
Once we define these real time actions,
we need to make sure that

33
00:02:32.430 --> 00:02:37.080
there are automated systems, or
processes to perform such actions, and

34
00:02:37.080 --> 00:02:41.320
provide failure recovery
in case of problems.

35
00:02:41.320 --> 00:02:43.547
As a summary, big data and

36
00:02:43.547 --> 00:02:49.320
data science are only useful if
the insights can be turned into action,

37
00:02:49.320 --> 00:02:54.198
and if the actions are carefully
defined and evaluated.
WEBVTT

1
00:00:01.360 --> 00:00:03.545
Steps in the Data Science Process.

2
00:00:11.405 --> 00:00:16.739
We have already seen a simple linear
form of data science process,

3
00:00:16.739 --> 00:00:21.900
including five distinct activities
that depend on each other.

4
00:00:23.140 --> 00:00:28.095
Let's summarize each activity further
before we go into the details of each.

5
00:00:28.095 --> 00:00:34.617
Acquire includes anything that makes
us retrieve data including; finding,

6
00:00:34.617 --> 00:00:39.210
accessing, acquiring, and moving data.

7
00:00:39.210 --> 00:00:45.640
It includes identification of and
authenticated access to all related data.

8
00:00:45.640 --> 00:00:50.000
And transportation of data from
sources to distributed files systems.

9
00:00:51.720 --> 00:00:57.340
It includes way to subset and match
the data to regions or times of interest.

10
00:00:57.340 --> 00:01:00.830
As we sometimes refer to
it as geo-spacial query.

11
00:01:02.150 --> 00:01:08.010
The next activity is prepare data,
we divide the pre-data activity.

12
00:01:08.010 --> 00:01:11.574
Into two steps based on
the nature of the activity.

13
00:01:11.574 --> 00:01:16.990
Namely, explore data and pre-process data.

14
00:01:16.990 --> 00:01:21.890
The first step in data preparation
involves literally looking

15
00:01:21.890 --> 00:01:26.750
at the data to understand its nature,
what it means, its quality and format.

16
00:01:28.130 --> 00:01:32.320
It often takes a preliminary
analysis of data, or

17
00:01:32.320 --> 00:01:34.230
samples of data, to understand it.

18
00:01:35.440 --> 00:01:38.230
This is why this step is called explore.

19
00:01:39.350 --> 00:01:43.100
Once we know more about the data
through exploratory analysis,

20
00:01:43.100 --> 00:01:46.625
the next step is pre-processing
of data for analysis.

21
00:01:46.625 --> 00:01:52.180
Pre-processing includes cleaning data,
sub-setting or

22
00:01:52.180 --> 00:01:57.660
filtering data, creating data,
which programs can read and

23
00:01:57.660 --> 00:02:03.660
understand, such as modeling raw
data into a more defined data model,

24
00:02:03.660 --> 00:02:07.260
or packaging it using
a specific data format.

25
00:02:08.810 --> 00:02:11.630
If there are multiple data sets involved,

26
00:02:11.630 --> 00:02:17.300
this step also includes integration
of multiple data sources, or streams.

27
00:02:17.300 --> 00:02:22.070
The prepared data then would be
passed onto the analysis step,

28
00:02:22.070 --> 00:02:24.590
which involves selection of
analytical techniques to use,

29
00:02:25.800 --> 00:02:29.190
building a model of the data,
and analyzing results.

30
00:02:30.330 --> 00:02:34.120
This step can take a couple
of iterations on its own or

31
00:02:34.120 --> 00:02:37.180
might require data scientists
to go back to steps one and

32
00:02:37.180 --> 00:02:42.350
two to get more data or
package data in a different way.

33
00:02:42.350 --> 00:02:48.070
Step four for communicating results
includes evaluation of analytical results.

34
00:02:48.070 --> 00:02:52.400
Presenting them in a visual way,
creating reports that include

35
00:02:52.400 --> 00:02:56.080
an assessment of results with
respect to success criteria.

36
00:02:57.100 --> 00:03:01.690
Activities in this step can often be
referred to with terms like interpret,

37
00:03:01.690 --> 00:03:04.780
summarize, visualize, or post process.

38
00:03:05.900 --> 00:03:10.390
The last step brings us back to the very
first reason we do data science,

39
00:03:11.680 --> 00:03:12.240
the purpose.

40
00:03:13.340 --> 00:03:18.470
Reporting insights from analysis and
determining actions from insights based

41
00:03:18.470 --> 00:03:23.300
on the purpose you initially defined
is what we refer to as the act step.

42
00:03:24.700 --> 00:03:29.010
We have now seen all the steps in
a typical data science process.

43
00:03:30.220 --> 00:03:35.320
Please note that this is an iterative
process and findings from one step

44
00:03:35.320 --> 00:03:38.970
may require the previous step to
be repeated with new information.
WEBVTT

1
00:00:02.391 --> 00:00:06.640
The Sixth V, Value.

2
00:00:06.640 --> 00:00:07.970
In this module,

3
00:00:07.970 --> 00:00:13.160
we described the five ways which are
considered to be dimensions of big data.

4
00:00:14.190 --> 00:00:19.947
Each way presented a challenging
dimension of big data namely,

5
00:00:19.947 --> 00:00:25.500
size, complexity, speed,
quality, and connectedness.

6
00:00:27.480 --> 00:00:32.850
Although we can list some other rays base
on the context, we prefer to list these

7
00:00:32.850 --> 00:00:38.490
five s fundamental dimensions that this
big data specialization helps you work on.

8
00:00:39.570 --> 00:00:43.940
However, at the heart of
the big data challenge

9
00:00:43.940 --> 00:00:48.860
is turning all of the other dimensions
into truly useful business value.

10
00:00:50.110 --> 00:00:54.640
The idea behind processing all
this big data in the first place

11
00:00:54.640 --> 00:00:56.640
is to bring value to the problem at hand.

12
00:00:57.830 --> 00:01:02.230
In week two we will explore how
to take the first steps into

13
00:01:02.230 --> 00:01:05.270
starting to generate
value out of big data.

14
00:01:06.430 --> 00:01:12.465
Now that we saw all the ways, let's focus
on an example of a big data challenge.

15
00:01:12.465 --> 00:01:16.940
Let's imagine now that you're part
of a company called Eglence Inc.

16
00:01:18.160 --> 00:01:22.992
One of the products of Eglence Inc
is a highly popular mobile game

17
00:01:22.992 --> 00:01:25.506
called Catch the Pink Flamingo.

18
00:01:25.506 --> 00:01:30.046
It's a multi-user game where
the users have to catch special types

19
00:01:30.046 --> 00:01:34.188
of pink flamingos that randomly
pop up on the world map on their

20
00:01:34.188 --> 00:01:38.670
screens based on the mission
that gets updated randomly.

21
00:01:38.670 --> 00:01:43.120
The game is played by millions of
people online throughout the world.

22
00:01:43.120 --> 00:01:47.110
One of the goals of the game is to form
a network of players to collectively

23
00:01:47.110 --> 00:01:52.470
cover the world map with pink flamingo
sightings and compete other groups.

24
00:01:52.470 --> 00:01:55.670
Users can pick their groups
based on player stats.

25
00:01:56.780 --> 00:02:01.530
The game's website sends free
cool stuff to registered users.

26
00:02:01.530 --> 00:02:07.483
Registration requires users to enter
demographic information such gender,

27
00:02:07.483 --> 00:02:12.542
year of birth, city, highest education,
and things like that.

28
00:02:12.542 --> 00:02:17.675
However, most of the users enter
inaccurate information about themselves,

29
00:02:17.675 --> 00:02:20.140
just like most of us do.

30
00:02:20.140 --> 00:02:24.980
To help improve the game,
the game collects realtime usage activity

31
00:02:24.980 --> 00:02:29.740
data from each player and
feeds them to it's data servers.

32
00:02:29.740 --> 00:02:35.290
The players of this game are
enthusiastically active on social media,

33
00:02:35.290 --> 00:02:38.480
and have strong
associations with the game.

34
00:02:38.480 --> 00:02:43.424
A popular Twitter hashtag for
this game is, CatchThePinkFlamingo,

35
00:02:43.424 --> 00:02:48.780
which gets more than 200,000
mentions worldwide per day.

36
00:02:48.780 --> 00:02:53.695
There are strong communities of
users who meet via social media and

37
00:02:53.695 --> 00:02:55.984
get together to play the game.

38
00:02:55.984 --> 00:03:02.960
Now, imagine yourself as the big data
solutions architect for Fun Games Inc.

39
00:03:02.960 --> 00:03:07.640
There are definitely examples of all three
types of data sources in this example.

40
00:03:08.920 --> 00:03:13.510
The mobile app generates data for
the analysis of user activity.

41
00:03:13.510 --> 00:03:18.407
Twitter conversations of players form
a rich source of unstructured data from

42
00:03:18.407 --> 00:03:19.005
people.

43
00:03:19.005 --> 00:03:20.191
And the customer and

44
00:03:20.191 --> 00:03:24.250
game records are examples of data
that this organization collects.

45
00:03:26.160 --> 00:03:29.010
This is a challenging big data example

46
00:03:29.010 --> 00:03:32.680
where all characteristics of
big data are represented.

47
00:03:32.680 --> 00:03:35.240
There are high volumes of player, game and

48
00:03:35.240 --> 00:03:39.560
Twitter data,
which also speaks to the variety of data.

49
00:03:39.560 --> 00:03:43.465
The data streams from the mobile app,
website, and

50
00:03:43.465 --> 00:03:48.979
social media in real-time, which can
be defined as high velocity data.

51
00:03:48.979 --> 00:03:53.203
The quality of demographic data
users enter is not clear, and

52
00:03:53.203 --> 00:03:58.334
there are networks of players which
are related to the balance of big data.
WEBVTT

1
00:00:02.240 --> 00:00:05.918
Cloud Computing: An Important
Big Data Enabler.

2
00:00:20.598 --> 00:00:25.586
In our first lecture in this course,
we mentioned the cloud as one of

3
00:00:25.586 --> 00:00:29.450
the two influences of
the launch of the big data era.

4
00:00:30.930 --> 00:00:34.050
We called it on-demand computing, and

5
00:00:34.050 --> 00:00:38.390
we said that it enables us to
compute any time any anywhere.

6
00:00:38.390 --> 00:00:40.110
Simply, whenever we demand it.

7
00:00:41.170 --> 00:00:42.540
In this video,

8
00:00:42.540 --> 00:00:47.260
we will see how we deploy the cloud to
our benefit in our big data applications.

9
00:00:48.470 --> 00:00:50.990
The main idea behind cloud computing

10
00:00:50.990 --> 00:00:55.340
is to transform computing
infrastructure into a commodity.

11
00:00:55.340 --> 00:01:00.220
So application developers can focus on
solving application-specific challenges

12
00:01:00.220 --> 00:01:03.880
instead of trying to build
infrastructure to run on.

13
00:01:03.880 --> 00:01:05.760
So how does this happen?

14
00:01:05.760 --> 00:01:08.840
We can simply define
a cloud computing service,

15
00:01:08.840 --> 00:01:11.390
as a rental service for computing.

16
00:01:11.390 --> 00:01:14.700
You rent what you want,
and return upon usage.

17
00:01:15.710 --> 00:01:18.650
Think about this, you wouldn't buy, or

18
00:01:18.650 --> 00:01:23.950
even build, a truck every time you
have to move a piece of furniture.

19
00:01:23.950 --> 00:01:24.880
You would simply rent.

20
00:01:26.560 --> 00:01:29.750
Why build a computing
cluster when you can rent?

21
00:01:29.750 --> 00:01:32.930
Especially if you are not
using it all the time.

22
00:01:35.110 --> 00:01:39.900
Similarly, you can rent a car or
a bike when you are on vacation.

23
00:01:39.900 --> 00:01:43.035
So you can bike anytime, anywhere.

24
00:01:44.660 --> 00:01:46.970
Let's dig into this question.

25
00:01:46.970 --> 00:01:51.240
What factors do you consider when you're
developing a solution for your yourself or

26
00:01:51.240 --> 00:01:51.800
your client?

27
00:01:52.860 --> 00:01:55.880
Should you build a hardware and
software resources yourself?

28
00:01:56.930 --> 00:02:01.090
Or should you rent these
resources from the cloud?

29
00:02:02.860 --> 00:02:06.390
Let's look at in-house hardware and
software resource building first.

30
00:02:08.450 --> 00:02:12.975
If you choose to develop in-house
capabilities, you have to hire people and

31
00:02:12.975 --> 00:02:15.630
buy hardware that suits your requirements.

32
00:02:16.660 --> 00:02:22.750
These includes, but not limited to,
buying networking hardware,

33
00:02:22.750 --> 00:02:29.040
storage disks, upgrading hardware
when it becomes obsolete, and so on.

34
00:02:29.040 --> 00:02:32.360
Not to forget, the real estate
cost of keeping the hardware.

35
00:02:33.370 --> 00:02:36.190
How do you estimate the size
of your hardware needs?

36
00:02:36.190 --> 00:02:39.910
Do you make a five year estimate,
or ten year?

37
00:02:41.920 --> 00:02:44.110
In today's fast changing world,

38
00:02:44.110 --> 00:02:46.620
it is becoming harder to
estimate future demands.

39
00:02:48.070 --> 00:02:52.220
Getting the software that fits
your needs is equally challenging.

40
00:02:52.220 --> 00:02:55.700
Most software installations
require a lot of tweaking and

41
00:02:55.700 --> 00:02:58.030
manual intervention that
require a lot of skills.

42
00:02:59.220 --> 00:03:02.025
You will need your engineers to do this.

43
00:03:02.025 --> 00:03:05.300
Compatibility issues bring
problems that are hard to foresee.

44
00:03:06.340 --> 00:03:09.750
Most software is updated on a daily basis.

45
00:03:09.750 --> 00:03:12.210
You must ensure you're updated.

46
00:03:12.210 --> 00:03:16.870
This insures you avoid security risks and
get the best.

47
00:03:18.670 --> 00:03:25.770
Over all, building your own data center or
computing power house can be expensive.

48
00:03:25.770 --> 00:03:32.200
And it can be time consuming,
maintaining it is a task by itself.

49
00:03:32.200 --> 00:03:35.760
This requires high initial
capital investments and

50
00:03:35.760 --> 00:03:39.380
efficient operation of several
departments in your business,

51
00:03:39.380 --> 00:03:42.540
which you might not have if
you are a startup company.

52
00:03:42.540 --> 00:03:46.800
Most people forget to include
the cost of disposing old hardware.

53
00:03:46.800 --> 00:03:48.788
Now lets see what the cloud can do for us.

54
00:03:50.896 --> 00:03:55.660
Cloud's benefits are similar to what you
would get from a rental car company.

55
00:03:57.190 --> 00:04:01.720
You pay for what you use,
which means a low capital investment.

56
00:04:03.160 --> 00:04:07.680
You don't need to go to the dealership,
do a negotiation, get a bank loan,

57
00:04:07.680 --> 00:04:08.420
get insurance.

58
00:04:09.450 --> 00:04:12.680
That means quick implementation
of your projects.

59
00:04:13.930 --> 00:04:17.350
Just like you don't need to buy
a car if you only need a car for

60
00:04:17.350 --> 00:04:22.200
a limited use, deploying your application
on a server that is geographically

61
00:04:22.200 --> 00:04:27.200
closer to your client can give you
fast service and happy customers.

62
00:04:29.060 --> 00:04:34.430
For startup and small business,
it can be challenging to do so.

63
00:04:34.430 --> 00:04:36.620
Cloud lets you do this with a click.

64
00:04:38.030 --> 00:04:42.950
You can be sitting at a coffeeshop or your
home and starting your Internet business,

65
00:04:42.950 --> 00:04:46.700
without a huge capital investment,
thanks to the cloud.

66
00:04:46.700 --> 00:04:51.230
And you don't need to have a five or
ten year resource estimation plan.

67
00:04:51.230 --> 00:04:55.527
Adapt to your requirements faster if
your business is growing faster than you

68
00:04:55.527 --> 00:04:56.130
thought.

69
00:04:57.440 --> 00:05:01.330
Cloud lets you forget about
the resource management problems and

70
00:05:01.330 --> 00:05:06.310
lets you focus on your business's products
or domain expertise with minimal cost.

71
00:05:07.600 --> 00:05:12.500
Just as you can rent a truck or
a convertible at a rental car company,

72
00:05:12.500 --> 00:05:15.520
you can build your own
custom machine on cloud.

73
00:05:15.520 --> 00:05:19.430
With a custom machine,
we mean a commodity cluster

74
00:05:19.430 --> 00:05:22.940
made out of the right type of
computing nodes for your application.

75
00:05:24.020 --> 00:05:27.930
You pick not only a CPU or a GPU, but

76
00:05:27.930 --> 00:05:32.420
pick from a whole menu of compute,
memory and storage choices.

77
00:05:32.420 --> 00:05:34.590
It's a buffet on the cloud.

78
00:05:34.590 --> 00:05:39.900
Design machines to suit your application
requirements, data size and analytics.

79
00:05:41.330 --> 00:05:43.740
Get what you want, and
pay for what you use.

80
00:05:45.130 --> 00:05:47.380
Compare this with buying and

81
00:05:47.380 --> 00:05:51.980
maintaining all combinations of
hardware that you possibly would use.

82
00:05:51.980 --> 00:05:55.860
That is so costly and
not possible at all times.

83
00:05:57.770 --> 00:06:02.710
Thanks to all these advantages, there
are many cloud server providers today.

84
00:06:02.710 --> 00:06:03.990
And the numbers are growing.

85
00:06:05.240 --> 00:06:08.820
Here we list some of the players
in the cloud computing market.

86
00:06:10.620 --> 00:06:12.940
Take a moment to look at them.

87
00:06:12.940 --> 00:06:15.990
You will probably recognize
some big names and

88
00:06:15.990 --> 00:06:18.730
some others you have not
even heard of before.

89
00:06:20.630 --> 00:06:25.670
As a summary, cloud does the heavy
lifting, so your team can extract

90
00:06:25.670 --> 00:06:30.300
value from data with getting bogged
down in the infrastructure details.

91
00:06:31.880 --> 00:06:33.680
Cloud provides convenient and

92
00:06:33.680 --> 00:06:38.485
viable solutions for scaling your
prototype to a full fledged application.

93
00:06:38.485 --> 00:06:43.400
You can leverage the experts
to handle security,

94
00:06:43.400 --> 00:06:46.450
robustness, and
let them handle the technical issues.

95
00:06:47.480 --> 00:06:51.560
Your team can work on
utilizing your strengths

96
00:06:51.560 --> 00:06:54.320
to solve your domain specific problem.
WEBVTT

1
00:00:02.690 --> 00:00:05.528
Cloud Service Models: Exploration
of Choices.

2
00:00:20.538 --> 00:00:24.709
There are many levels of services that
you can get from cloud providers.

3
00:00:25.860 --> 00:00:29.880
Any cloud computing discussion will
involve terms like application

4
00:00:29.880 --> 00:00:34.800
as a service, platform as a service,
and infrastructure as a service.

5
00:00:36.450 --> 00:00:40.660
All of these refer to business
models around using the cloud

6
00:00:40.660 --> 00:00:45.460
with different levels of engagement and
servicing similar to rental agreements.

7
00:00:47.860 --> 00:00:55.470
IaaS, infrastructure as a service, can be
defined as a bare minimum rental service.

8
00:00:57.030 --> 00:01:02.630
This is like renting a truck from
a company that you can assume has hardware

9
00:01:02.630 --> 00:01:07.978
and you do the packing of your furniture,
and drive to your new house.

10
00:01:11.118 --> 00:01:15.430
You as the user of the service install and
maintain an operating system,

11
00:01:15.430 --> 00:01:19.400
and other applications in
the infrastructure as a service model.

12
00:01:20.810 --> 00:01:24.180
The Amazon EC2 cloud is a good example for
this model.

13
00:01:26.470 --> 00:01:29.940
PaaS, platform as a service,

14
00:01:29.940 --> 00:01:35.840
is the model where a user is provided
with an entire computing platform.

15
00:01:35.840 --> 00:01:40.130
This could include the operating system
and programming languages that you need.

16
00:01:42.310 --> 00:01:47.440
It could extend to include the database
of your choice, or even a web server.

17
00:01:47.440 --> 00:01:48.920
You can develop, and

18
00:01:48.920 --> 00:01:52.150
run your own application software,
on top of these layers.

19
00:01:54.120 --> 00:01:59.510
The Google App engine and Microsoft Azure
are two examples of this model.

20
00:02:01.460 --> 00:02:07.770
SaaS, the software as a service model,
is the model

21
00:02:07.770 --> 00:02:13.090
in which the cloud service provider takes
the responsibilities for the hardware and

22
00:02:13.090 --> 00:02:18.710
software environment such as the operating
system and the application software.

23
00:02:19.940 --> 00:02:25.720
This means you can work on using
the application to solve your problem.

24
00:02:28.180 --> 00:02:31.848
Dropbox is a very popular
software as a service platform.

25
00:02:34.638 --> 00:02:39.205
Ultimately, the decision of which service
you want to explore is a function of

26
00:02:39.205 --> 00:02:40.460
several variables.

27
00:02:41.470 --> 00:02:45.760
It depends on the skill level of your
team to handle computing environment,

28
00:02:45.760 --> 00:02:48.080
development and maintenance.

29
00:02:48.080 --> 00:02:51.010
It also depends on how you
might need to use the service.

30
00:02:53.170 --> 00:02:55.990
You need to pick the right
service model that

31
00:02:55.990 --> 00:02:58.520
best fits you in terms of long term goals.

32
00:02:59.590 --> 00:03:04.610
Finally, when you're deploying a cloud
service you also have to understand

33
00:03:04.610 --> 00:03:09.480
all the security risks, since your
data resides on third party service.

34
00:03:11.730 --> 00:03:16.026
Security is a very important
aspect in today's world of growing

35
00:03:16.026 --> 00:03:18.207
digitization of information.

36
00:03:18.207 --> 00:03:21.990
You must make your client's
data safety a top priority,

37
00:03:21.990 --> 00:03:25.859
and hence this should be an important
criteria in your decision.

38
00:03:27.790 --> 00:03:31.870
All the security risks
must be understood and

39
00:03:31.870 --> 00:03:35.000
evaluated as your data resides
on third party servers.

40
00:03:37.670 --> 00:03:40.370
We are seeing other forms
of services being added

41
00:03:40.370 --> 00:03:42.080
to the family of cloud services.

42
00:03:43.260 --> 00:03:45.710
The logic of infrastructure, platform, and

43
00:03:45.710 --> 00:03:48.550
Software as a Service is
getting extended further.

44
00:03:50.040 --> 00:03:53.580
XaaS is an umbrella term that signifies

45
00:03:53.580 --> 00:03:57.980
even finer-grain control over computing
resources that you want to rent.

46
00:03:57.980 --> 00:04:03.520
For example, storage as a service,
communication as a service,

47
00:04:03.520 --> 00:04:05.410
marketing as a service, and so on.

48
00:04:06.990 --> 00:04:14.370
As a summary, infrastructure as a service,
platform as a service, and application

49
00:04:14.370 --> 00:04:18.800
as a service are three main class service
models that are being used with success.

50
00:04:19.960 --> 00:04:25.788
Picking one will depend on the number
of variables which are company's goals.

51
00:04:25.788 --> 00:04:28.840
These three models have
inspired many similar models

52
00:04:28.840 --> 00:04:30.880
to emerge around cloud computing.
WEBVTT

1
00:00:00.210 --> 00:00:04.331
In this lecture we'll be learning about
the basic file manipulation commands for

2
00:00:04.331 --> 00:00:05.953
the Hadoop File System or HDFS.

3
00:00:05.953 --> 00:00:09.440
We'll first start by downloading
a text file of words.

4
00:00:09.440 --> 00:00:15.040
We'll use this text file to copy to and
from the local file system in HDFS and

5
00:00:15.040 --> 00:00:17.730
later on we'll use it
to run word count on.

6
00:00:18.770 --> 00:00:21.960
After we download the text file,
we'll open a terminal shell and

7
00:00:21.960 --> 00:00:25.220
copy the text file from
the local file system to HDFS.

8
00:00:26.310 --> 00:00:29.870
Next, we'll copy the file within HDFS and

9
00:00:29.870 --> 00:00:34.880
also see how to copy file from
HDFS to the local file system.

10
00:00:34.880 --> 00:00:38.223
Finally, we'll see how to
delete a file in HDFS.

11
00:00:38.223 --> 00:00:39.410
Let's start.

12
00:00:39.410 --> 00:00:42.390
We're going to download text
file to copy into HDFS.

13
00:00:42.390 --> 00:00:46.610
It doesn't matter what the contents of
the text file is, so we'll download

14
00:00:46.610 --> 00:00:50.380
the complete works of Shakespeare
since it contains interesting text.

15
00:00:53.343 --> 00:00:56.840
First, click on the icon here
to launch a web browser.

16
00:01:00.860 --> 00:01:03.780
Now we'll search Google for
the complete works of Shakespeare.

17
00:01:17.633 --> 00:01:19.880
I'm going to be using
this first link here.

18
00:01:19.880 --> 00:01:23.500
And we'll provide this link
in the reading section.

19
00:01:26.500 --> 00:01:30.080
So this is the complete
works of Shakespeare and

20
00:01:30.080 --> 00:01:35.700
we'll save it to a text file in the local
file system by clicking on the icon here,

21
00:01:35.700 --> 00:01:40.790
the Open Menu and selecting save page.

22
00:01:43.690 --> 00:01:47.105
So we'll call it words.txt.

23
00:01:47.105 --> 00:01:52.090
And the Save in folder, it's going to
save it into the Downloads directory.

24
00:01:57.733 --> 00:02:02.800
Once that completes, we'll open a terminal
window by clicking on the icon here.

25
00:02:06.183 --> 00:02:13.480
So if we go into the downloads
Ddrectory by typing a cd Downloads and

26
00:02:13.480 --> 00:02:21.320
running ls, we can see that words.txt
was successfully downloaded.

27
00:02:24.120 --> 00:02:31.100
Moving on, let's copy words.txt from the
local file system to the HDFS file system.

28
00:02:31.100 --> 00:02:38.711
The command to do this is hadoop
fs- copyFromLocal words.txt.

29
00:02:40.850 --> 00:02:43.330
When I run this,
it'll copy it from the local directory and

30
00:02:43.330 --> 00:02:45.595
local file system to HDFS.

31
00:02:47.710 --> 00:02:52.681
We can see that the file was
copied by running hadoop fs -ls.

32
00:02:52.681 --> 00:02:57.630
You can see that the file
was successfully copied.

33
00:03:00.270 --> 00:03:05.673
Next, we can copy this file
to another file within HDFS.

34
00:03:05.673 --> 00:03:14.170
We can do this by running hadoop
fs -cp words.txt words2.txt.

35
00:03:14.170 --> 00:03:20.500
The first words.txt is the file
that already exists in HDFS.

36
00:03:22.160 --> 00:03:24.760
The second words 2.txt

37
00:03:24.760 --> 00:03:27.230
is the new file that we're going to
create when we run this command.

38
00:03:28.340 --> 00:03:33.182
Let's run it, and again we're can run

39
00:03:33.182 --> 00:03:38.033
hadoopfs-ls to see the files in HDFS.

40
00:03:38.033 --> 00:03:44.364
We can see the original file words.txt,
and the copy that was made, words2.txt.

41
00:03:46.723 --> 00:03:52.680
Let's copy words2.txt from
HDFS to the local filesystem.

42
00:03:52.680 --> 00:04:00.909
We can do this by running hadoop
fs -copyToLocal words2.txt.

43
00:04:03.813 --> 00:04:09.210
After I run this command, I can call ls to
see the contents of the local file system.

44
00:04:11.570 --> 00:04:16.455
So now we have the new file words2.txt
which we've just copied from HDFS.

45
00:04:19.640 --> 00:04:22.980
The last step in this lecture
is to delete a file in HDFS.

46
00:04:22.980 --> 00:04:28.881
We can delete words2.txt
by running hadoop fs,

47
00:04:28.881 --> 00:04:32.404
but is rn words2.txt.

48
00:04:35.330 --> 00:04:37.930
As you can see,
it printed that it deleted the file.

49
00:04:37.930 --> 00:04:41.806
We can also run hadoop fs- ls,
to verify that the file is deleted.

50
00:04:45.246 --> 00:04:50.872
You can see that there's only the original
words.txt, and words2.txt was deleted.
WEBVTT

1
00:00:00.710 --> 00:00:02.260
Getting started.

2
00:00:02.260 --> 00:00:04.310
Why do we worry about foundations?

3
00:00:14.575 --> 00:00:19.079
Starting next week you'll start
diving into the details of the Hadoop

4
00:00:19.079 --> 00:00:21.650
framework for big data.

5
00:00:21.650 --> 00:00:26.460
Before you start, taking a little
bit of time to understand some core

6
00:00:26.460 --> 00:00:31.580
concepts will help you to digest the
information on Hadoop better and faster.

7
00:00:32.950 --> 00:00:36.670
Imagine yourself attending
a chemistry lab.

8
00:00:36.670 --> 00:00:40.700
Before you start hearing about
the tubes and mixtures, you really need

9
00:00:40.700 --> 00:00:45.650
to understand the chemistry or theory
of the practical concepts in the lab.

10
00:00:45.650 --> 00:00:49.650
Similarly, learning these
concepts now will help

11
00:00:49.650 --> 00:00:53.880
you with your understanding of the
practical concepts in the Hadoop lectures.

12
00:00:55.000 --> 00:01:00.290
In addition, we want to prepare you to
understand the tools beyond Hadoop.

13
00:01:00.290 --> 00:01:06.410
Any big data system that you find
will be built on these core concepts.

14
00:01:06.410 --> 00:01:09.200
So these foundations will
help you beyond this course.

15
00:01:10.280 --> 00:01:11.460
Now, let's get started.
WEBVTT

1
00:00:02.022 --> 00:00:03.322
Getting started.

2
00:00:03.322 --> 00:00:05.063
Why Hadoop?

3
00:00:05.063 --> 00:00:07.254
We have all heard that Hadoop and

4
00:00:07.254 --> 00:00:12.150
related projects in this
ecosystem are great for big data.

5
00:00:12.150 --> 00:00:18.332
This module will answer the four Ws and
an H about why this statement is true.

6
00:00:30.473 --> 00:00:33.788
Before we dive further into
the details of Hadoop,

7
00:00:33.788 --> 00:00:38.890
let's take a moment to analyze the
characteristics of the Hadoop ecosystem.

8
00:00:40.560 --> 00:00:42.680
What's in the ecosystem?

9
00:00:42.680 --> 00:00:43.890
Why is it beneficial?

10
00:00:45.150 --> 00:00:46.040
Where is it used?

11
00:00:47.420 --> 00:00:48.130
Who uses it?

12
00:00:49.910 --> 00:00:51.740
And how do these tools work?

13
00:00:54.830 --> 00:00:59.642
The Hadoop ecosystem frameworks and
applications that we will describe in this

14
00:00:59.642 --> 00:01:03.670
module have several overarching themes and
goals.

15
00:01:03.670 --> 00:01:08.930
First, they provide scalability
to store large volumes of data

16
00:01:08.930 --> 00:01:10.180
on commodity hardware.

17
00:01:12.160 --> 00:01:16.820
As the number of systems increases,
so does the chance for crashes and

18
00:01:16.820 --> 00:01:18.510
hardware failures.

19
00:01:18.510 --> 00:01:23.570
A second goal, supported by most
frameworks in the Hadoop ecosystem,

20
00:01:23.570 --> 00:01:26.590
is the ability to gracefully
recover from these problems.

21
00:01:28.730 --> 00:01:34.070
In addition, as we have mentioned before,
big data comes in

22
00:01:34.070 --> 00:01:40.560
a variety of flavors, such as text files,
graph of social networks,

23
00:01:40.560 --> 00:01:46.681
streaming sensor data and raster images.

24
00:01:46.681 --> 00:01:50.250
A third goal for
the Hadoop ecosystem then,

25
00:01:50.250 --> 00:01:55.920
is the ability to handle these different
data types for any given type of data.

26
00:01:57.040 --> 00:02:00.590
You can find several projects in
the ecosystem that support it.

27
00:02:02.350 --> 00:02:05.890
A fourth goal of the Hadoop ecosystem

28
00:02:05.890 --> 00:02:08.998
is the ability to facilitate
a shared environment.

29
00:02:08.998 --> 00:02:15.030
Since even modest-sized
clusters can have many cores,

30
00:02:15.030 --> 00:02:18.640
it is important to allow multiple
jobs to execute simultaneously.

31
00:02:20.070 --> 00:02:23.450
Why buy servers only to let them sit idle?

32
00:02:25.670 --> 00:02:30.750
Another goal of the Hadoop ecosystem
is providing value for your enterprise.

33
00:02:32.910 --> 00:02:37.630
The ecosystem includes a wide
range of open source projects

34
00:02:37.630 --> 00:02:40.270
backed by a large active community.

35
00:02:41.640 --> 00:02:46.500
These projects are free to use and
easy to find support for.

36
00:02:48.640 --> 00:02:50.750
In the following lectures in this module,

37
00:02:50.750 --> 00:02:55.260
we will take a more detailed
look at the Hadoop ecosystem.

38
00:02:55.260 --> 00:02:59.590
First, we will explore the kinds
of projects available and

39
00:02:59.590 --> 00:03:01.760
the types of capabilities they provide.

40
00:03:03.100 --> 00:03:09.300
Next we will take a deeper look at
the three main parts of Hadoop.

41
00:03:09.300 --> 00:03:12.130
The Hadoop distributed file system,
or HDFS.

42
00:03:13.490 --> 00:03:15.910
YARN, the scheduler and resource manager.

43
00:03:17.170 --> 00:03:21.400
And MapReduce, a programming model for
processing big data.

44
00:03:23.210 --> 00:03:28.060
We will then discuss cloud computing, and
the types of service models it provides.

45
00:03:29.980 --> 00:03:34.865
We will also describe situations in
which Hadoop is not the best solution.

46
00:03:36.935 --> 00:03:40.945
This module then concludes with
two readings involving hands-on

47
00:03:40.945 --> 00:03:44.245
experience with HDFS and MapReduce.

48
00:03:44.245 --> 00:03:45.325
So let's get started.
WEBVTT

1
00:00:01.873 --> 00:00:05.670
MapReduce, Simple Programming for
Big Results.

2
00:00:21.723 --> 00:00:26.650
MapReduce is a programming model for
the Hadoop ecosystem.

3
00:00:26.650 --> 00:00:29.020
It relies on YARN to schedule and

4
00:00:29.020 --> 00:00:33.430
execute parallel processing over
the distributed file blocks in HDFS.

5
00:00:34.600 --> 00:00:39.292
There are several tools that use the
MapReduce model to provide a higher level

6
00:00:39.292 --> 00:00:41.829
interface to other programming models.

7
00:00:41.829 --> 00:00:46.747
Hive has a SQL-like interface that
adds capabilities that help with

8
00:00:46.747 --> 00:00:49.220
relational data modeling.

9
00:00:49.220 --> 00:00:52.460
And Pig is a high level data flow language

10
00:00:52.460 --> 00:00:55.470
that adds capabilities that
help with process map modeling.

11
00:00:57.070 --> 00:01:02.200
Traditional parallel programming requires
expertise on a number of computing and

12
00:01:02.200 --> 00:01:04.160
systems concepts.

13
00:01:04.160 --> 00:01:09.170
For example, synchronization
mechanisms like locks, semaphores,

14
00:01:09.170 --> 00:01:11.460
and monitors are essential.

15
00:01:11.460 --> 00:01:15.450
And incorrectly using them can
either crash your program, or

16
00:01:15.450 --> 00:01:17.460
severely impact performance.

17
00:01:19.080 --> 00:01:21.600
This high learning curve
makes it difficult.

18
00:01:22.930 --> 00:01:27.465
It is also error prone,
since your code can run on hundreds, or

19
00:01:27.465 --> 00:01:30.813
thousands of nodes,
each having many cores.

20
00:01:30.813 --> 00:01:33.685
And any problem related to
these parallel processes,

21
00:01:33.685 --> 00:01:36.130
needs to be handled by
your parallel program.

22
00:01:37.510 --> 00:01:43.200
The MapReduce programming model greatly
simplifies running code in parallel

23
00:01:43.200 --> 00:01:46.250
since you don't have to deal
with any of these issues.

24
00:01:46.250 --> 00:01:52.450
Instead, you only need to create and
map and reduce tasks, and you don't

25
00:01:52.450 --> 00:01:56.670
have to worry about multiple threads,
synchronization, or concurrency issues.

26
00:01:58.900 --> 00:02:01.110
So, what is a map and reduce?

27
00:02:02.760 --> 00:02:07.522
Map and reduce are two concepts
based on functional programming

28
00:02:07.522 --> 00:02:12.160
where the output the function
is based solely on the input.

29
00:02:14.010 --> 00:02:20.060
Just like in a mathematical function,
f (x) = y, y depends on x.

30
00:02:21.480 --> 00:02:25.360
You provide a function, or
operation for a map, and reduce.

31
00:02:26.890 --> 00:02:30.570
And the runtime executes it over the data.

32
00:02:30.570 --> 00:02:35.890
For map, the operation is
applied on each data element.

33
00:02:35.890 --> 00:02:40.110
And in reduce, the operation
summarizes elements in some manner.

34
00:02:41.510 --> 00:02:47.180
An example, using map and
reduce will make this concepts more clear.

35
00:02:49.230 --> 00:02:52.180
Hello word is a traditional
first program you code

36
00:02:52.180 --> 00:02:54.310
when you start to learning
programming languages.

37
00:02:55.730 --> 00:03:01.661
The first program to learn, or hello
word of map reduce, is often WordCount.

38
00:03:03.735 --> 00:03:08.128
WordCount reads one or
more text files, and

39
00:03:08.128 --> 00:03:14.313
counts the number of occurrences
of each word in these files.

40
00:03:14.313 --> 00:03:17.654
The output will be a text
file with a list of words and

41
00:03:17.654 --> 00:03:20.940
their occurrence frequencies
in the input data.

42
00:03:22.320 --> 00:03:25.560
Let's examine each step of WordCount.

43
00:03:26.820 --> 00:03:31.790
For simplification we are assuming
we have one big file as an input.

44
00:03:33.200 --> 00:03:39.040
Before WordCount runs,
the input file is stored in HDFS.

45
00:03:39.040 --> 00:03:40.880
As you know now,

46
00:03:40.880 --> 00:03:45.200
HDFS partitions the blocks across
multiple nodes in the cluster.

47
00:03:46.470 --> 00:03:53.503
In this case, four partitions labeled,
A, B, C, and D.

48
00:03:53.503 --> 00:03:58.550
The first step in MapReduce is to
run a map operation on each node.

49
00:04:00.100 --> 00:04:05.190
As the input partitions are read
from HTFS, map is called for

50
00:04:05.190 --> 00:04:06.760
each line in the input.

51
00:04:08.020 --> 00:04:12.330
Let's look at the first lines
of the input partitions, A and

52
00:04:12.330 --> 00:04:14.810
B, and start counting the words.

53
00:04:15.920 --> 00:04:21.162
The first line,
in the partition on node A,

54
00:04:21.162 --> 00:04:26.133
says, My apple is red and my rose is blue.

55
00:04:26.133 --> 00:04:32.990
Similarly, the first line, on partition B,
says, You are the apple of my eye.

56
00:04:34.370 --> 00:04:38.170
Let's now see what happens in
the first map node for partition A.

57
00:04:40.138 --> 00:04:42.910
Map creates a key value for

58
00:04:42.910 --> 00:04:49.510
each word on the line containing
the word as the key, and 1 as the value.

59
00:04:49.510 --> 00:04:55.470
In this example, the word apple is
read from the line in partition A.

60
00:04:56.550 --> 00:05:01.447
Map produces a key value of (apple, 1).

61
00:05:01.447 --> 00:05:08.273
Similarly, the word my is seen
on the first line of A twice.

62
00:05:08.273 --> 00:05:14.759
So, the key values of (my,
1), are created.

63
00:05:14.759 --> 00:05:20.040
Note that map goes to each node
containing a data block for

64
00:05:20.040 --> 00:05:25.050
the file,
instead of the data moving to map.

65
00:05:25.050 --> 00:05:27.410
This is moving computation to data.

66
00:05:28.790 --> 00:05:33.863
Let's now see what the same map
operation generates for partition B.

67
00:05:33.863 --> 00:05:37.221
Since each word only
happens to occur once,

68
00:05:37.221 --> 00:05:42.770
a list of all the words with one
key-value pairing each gets generated.

69
00:05:43.950 --> 00:05:48.170
Please take a moment to
observe the outputs of map and

70
00:05:48.170 --> 00:05:52.480
each key-value pair associated to a word.

71
00:06:02.003 --> 00:06:09.570
Next, all the key-values that were output
from map are sorted based on their key.

72
00:06:09.570 --> 00:06:16.810
And the key values, with the same word,
are moved, or shuffled, to the same node.

73
00:06:17.980 --> 00:06:23.850
To simplify this figure, each node only
has a single word, in orange boxes.

74
00:06:24.970 --> 00:06:29.010
But in general,
a node will have many different words.

75
00:06:29.010 --> 00:06:32.890
Just like our example from the two
lines in A and B partitions.

76
00:06:33.960 --> 00:06:39.480
Here we see that, you and apple,
are assigned to the first node.

77
00:06:39.480 --> 00:06:43.543
The word is, to the second node.

78
00:06:43.543 --> 00:06:47.710
And the words, rose and red, to the third.

79
00:06:48.900 --> 00:06:54.960
Although, for simplicity, we drew four
map nodes and three shuffle nodes.

80
00:06:54.960 --> 00:06:58.760
The number of nodes can be extended
as much as the application demands.

81
00:07:01.903 --> 00:07:07.083
Next, the reduce operation
executes on these nodes to

82
00:07:07.083 --> 00:07:12.045
add values for
key-value pairs with the same keys.

83
00:07:12.045 --> 00:07:16.875
For example, (apple, 1), and

84
00:07:16.875 --> 00:07:23.959
another (apple, 1), becomes (apple, 2).

85
00:07:23.959 --> 00:07:28.161
The result of reduce is
a single key pair for

86
00:07:28.161 --> 00:07:32.263
each word that was read in the input file.

87
00:07:32.263 --> 00:07:36.160
The key is the word, and
the value is the number of occurrences.

88
00:07:38.790 --> 00:07:42.650
If we look back at our WordCount example,

89
00:07:42.650 --> 00:07:45.640
we see that there were
three distinct steps.

90
00:07:45.640 --> 00:07:52.818
Namely, the map step, the shuffle and
sort step, and the reduce step.

91
00:07:52.818 --> 00:07:58.840
Although, the WordCount
example is pretty simple,

92
00:07:58.840 --> 00:08:03.320
it represents a large number of
applications to which these three steps

93
00:08:03.320 --> 00:08:07.000
can be applied in order to achieve
data parallel scalability.

94
00:08:08.540 --> 00:08:14.213
For example, now that you have
seen the WordCount application,

95
00:08:14.213 --> 00:08:19.063
consider changing the WordCount
algorithm to index all

96
00:08:19.063 --> 00:08:22.170
the URLs by words after a web crawl.

97
00:08:23.320 --> 00:08:29.225
This means, instead of pointing to
a number, the keys would refer to URLs.

98
00:08:31.100 --> 00:08:36.350
After the map, with this new function,
which by the way is called a user

99
00:08:36.350 --> 00:08:41.760
defined function, the output of
shuffle and sort would look like this.

100
00:08:46.333 --> 00:08:53.450
Now, when we reduce the URLs, all the URLs
that mention Apple would look like this.

101
00:08:55.943 --> 00:09:01.010
This is, in fact, one of the ways
a search engine like Google works.

102
00:09:02.900 --> 00:09:08.150
So now, if somebody came to the interface
built for this application,

103
00:09:08.150 --> 00:09:12.078
to search for the word apple,
and entered apple,

104
00:09:12.078 --> 00:09:17.490
it would be easy to get all
the URLs as the word itself.

105
00:09:18.840 --> 00:09:22.943
No wonder the first MapReduce
paper was produced by Google.

106
00:09:22.943 --> 00:09:27.760
We will give you a link to the original
Google paper on MapReduce from

107
00:09:27.760 --> 00:09:30.390
2004 at the end of this lecture.

108
00:09:32.030 --> 00:09:34.080
It is pretty technical, but

109
00:09:34.080 --> 00:09:38.040
it gives you a simple overview without
the current system implementations.

110
00:09:39.720 --> 00:09:43.440
We just saw how MapReduce can
be used in search engines

111
00:09:43.440 --> 00:09:45.800
in addition to counting the words and
documents.

112
00:09:47.180 --> 00:09:51.900
Although it's possible to add many
more applications, let's stop here for

113
00:09:51.900 --> 00:09:55.150
a general discussion on how the points of

114
00:09:55.150 --> 00:09:59.670
data parallelism can be used in
search in this three step pattern.

115
00:10:01.480 --> 00:10:05.780
There is definitely parallelization
during the map step.

116
00:10:06.810 --> 00:10:09.953
This parallelization is over the input,

117
00:10:09.953 --> 00:10:13.918
as each partition gets
processed one line at a time.

118
00:10:13.918 --> 00:10:18.440
To achieve this type of data
parallelism we must decide on

119
00:10:18.440 --> 00:10:23.430
the data granularity of
each parallel competition.

120
00:10:23.430 --> 00:10:25.540
In this case, it will be a line.

121
00:10:26.860 --> 00:10:33.410
We also see parallel grouping of
data in the shuffle and sort phase.

122
00:10:33.410 --> 00:10:37.710
This time, the parallelization is
over the intermediate products.

123
00:10:38.740 --> 00:10:41.570
That is, the individual key-value pairs.

124
00:10:42.940 --> 00:10:46.400
And after the grouping of
the intermediate products,

125
00:10:46.400 --> 00:10:51.630
the reduce step gets parallelized
to construct one output file.

126
00:10:53.090 --> 00:10:56.340
You have probably noticed
that the data gets reduced

127
00:10:56.340 --> 00:10:58.140
to a smaller set at each step.

128
00:10:59.840 --> 00:11:06.208
This overview gave us an idea of what
kinds of tasks that MapReduce is good for.

129
00:11:06.208 --> 00:11:11.604
While MapReduce excels at independent
batch tasks similar to our applications,

130
00:11:11.604 --> 00:11:16.700
there are certain kinds of tasks that
you would not want to use MapReduce for.

131
00:11:17.965 --> 00:11:22.300
For example,
if your data is frequently changing,

132
00:11:22.300 --> 00:11:27.120
MapReduce is slow since it reads
the entire input data set each time.

133
00:11:28.350 --> 00:11:31.540
The MapReduce model requires that maps and

134
00:11:31.540 --> 00:11:35.320
reduces execute
independently of each other.

135
00:11:35.320 --> 00:11:37.920
This greatly simplifies
your job as a designer,

136
00:11:37.920 --> 00:11:41.910
since you do not have to deal
with synchronization issues.

137
00:11:41.910 --> 00:11:46.060
However, it means that computations
that do have dependencies,

138
00:11:46.060 --> 00:11:47.880
cannot be expressed with MapReduce.

139
00:11:49.600 --> 00:11:53.760
Finally, MapReduce does
not return any results

140
00:11:53.760 --> 00:11:56.490
until the entire process is finished.

141
00:11:56.490 --> 00:11:59.060
It must read the entire input data set.

142
00:12:00.130 --> 00:12:04.240
This makes it unsuitable for interactive
applications where the results must be

143
00:12:04.240 --> 00:12:09.200
presented to the user very quickly,
expecting a return from the user.

144
00:12:10.880 --> 00:12:15.830
As a summary, MapReduce hides
complexities of parallel programming and

145
00:12:15.830 --> 00:12:18.450
greatly simplifies building
parallel applications.

146
00:12:19.530 --> 00:12:21.996
Many types of tasks suitable for

147
00:12:21.996 --> 00:12:27.130
MapReduce include search engine
page ranking and topic mapping.

148
00:12:27.130 --> 00:12:30.929
Please see the reading after
this lecture on making Pasta Sauce

149
00:12:32.382 --> 00:12:38.320
with MapReduce for another fun application
using the MapReduce programming model.
WEBVTT

1
00:00:01.058 --> 00:00:03.042
Programming Models for Big Data.

2
00:00:22.264 --> 00:00:26.664
We have seen that scalable computing
over the internet to achieve

3
00:00:26.664 --> 00:00:32.375
data-parallel scalability for big data
applications is now a possibility.

4
00:00:32.375 --> 00:00:34.625
Thanks to commodity clusters.

5
00:00:34.625 --> 00:00:38.970
Cost-effective commodity clusters
together with advances in distributed

6
00:00:38.970 --> 00:00:41.670
file systems to move computation to data,

7
00:00:41.670 --> 00:00:46.280
provide a potential to conduct
scalable big data analytics.

8
00:00:46.280 --> 00:00:49.050
The next thing we will
talk about is how to take

9
00:00:49.050 --> 00:00:51.050
advantage of these
infrastructure advances.

10
00:00:52.100 --> 00:00:53.560
What are the right programming models?

11
00:00:54.680 --> 00:01:00.460
A programming model is an abstraction or
existing machinery or infrastructure.

12
00:01:00.460 --> 00:01:03.970
It is a set of abstract
runtime libraries and

13
00:01:03.970 --> 00:01:08.310
programming languages that
form a model of computation.

14
00:01:08.310 --> 00:01:14.110
This abstraction level can be low-level
as in machine language in computers.

15
00:01:14.110 --> 00:01:20.030
Or very high as in high-level programming
languages, for example, Java.

16
00:01:20.030 --> 00:01:23.470
So we can say,
if the enabling infrastructure for

17
00:01:23.470 --> 00:01:28.320
big data analysis is distributed
file systems as we mentioned,

18
00:01:28.320 --> 00:01:33.120
then the programming model for
big data should enable the programmability

19
00:01:33.120 --> 00:01:37.100
of the operations within
distributed file systems.

20
00:01:37.100 --> 00:01:41.840
What we mean by this being able to
write computer programs that work

21
00:01:41.840 --> 00:01:46.410
efficiently on top of distributed
file systems using big data and

22
00:01:46.410 --> 00:01:50.430
making it easy to cope with
all the potential issues.

23
00:01:50.430 --> 00:01:52.320
Based on everything we discussed so

24
00:01:52.320 --> 00:01:57.680
far, let's describe the requirements for
big data programming models.

25
00:01:57.680 --> 00:02:02.480
First of all, such a programming model for
big data should support

26
00:02:02.480 --> 00:02:07.280
common big data operations like
splitting large volumes of data.

27
00:02:08.420 --> 00:02:12.460
This means for partitioning and
placement of data in and

28
00:02:12.460 --> 00:02:18.220
out of computer memory along with a model
to synchronize the datasets later on.

29
00:02:18.220 --> 00:02:21.463
The access to data should
be achieved in a fast way.

30
00:02:21.463 --> 00:02:26.291
It should allow fast distribution to nodes
within a rack and these are potentially,

31
00:02:26.291 --> 00:02:28.815
the data nodes we moved
the computation to.

32
00:02:28.815 --> 00:02:34.020
This means scheduling of
many parallel tasks at once.

33
00:02:34.020 --> 00:02:37.950
It should also enable
reliability of the computing and

34
00:02:37.950 --> 00:02:40.100
full tolerance from failures.

35
00:02:40.100 --> 00:02:43.768
This means it should enable
programmable replications and

36
00:02:43.768 --> 00:02:45.758
recovery of files when needed.

37
00:02:45.758 --> 00:02:50.518
It should be easily scalable to
the distributed notes where the data gets

38
00:02:50.518 --> 00:02:51.305
produced.

39
00:02:51.305 --> 00:02:56.647
It should also enable adding new resources
to take advantage of distributive

40
00:02:56.647 --> 00:03:01.670
computers and scale to more or
faster data without losing performance.

41
00:03:01.670 --> 00:03:05.280
This is called scaling out if needed.

42
00:03:05.280 --> 00:03:08.220
Since there are a variety
of different types of data,

43
00:03:08.220 --> 00:03:13.140
such as documents, graphs,
tables, key values, etc.

44
00:03:13.140 --> 00:03:17.100
A programming model should enable
operations over a particular set

45
00:03:17.100 --> 00:03:18.680
of these types.

46
00:03:18.680 --> 00:03:23.261
Not every type of data may be
supported by a particular model, but

47
00:03:23.261 --> 00:03:27.020
the models should be optimized for
at least one type.

48
00:03:27.020 --> 00:03:29.540
Is it getting a little complicated?

49
00:03:29.540 --> 00:03:31.153
It doesn't have to have to be.

50
00:03:31.153 --> 00:03:36.696
In fact, we apply similar models in
our daily lives for everyday tasks.

51
00:03:36.696 --> 00:03:41.520
Let's look at the scenario where you
might unknowingly apply this model.

52
00:03:41.520 --> 00:03:45.020
Imagine a peaceful Saturday afternoon.

53
00:03:45.020 --> 00:03:47.840
You receive a phone call from a friend and
she says,

54
00:03:47.840 --> 00:03:50.050
she they will be at your
house in an hour for dinner.

55
00:03:51.290 --> 00:03:56.157
It seems like you completely forgot that
you had invited your friends for dinner.

56
00:03:56.157 --> 00:03:59.309
So you say, you are looking forward
to it and head to the kitchen.

57
00:03:59.309 --> 00:04:05.035
As a quick solution, you decide to
cook pasta with some tomato sauce.

58
00:04:05.035 --> 00:04:07.487
You need to take advantage
of parallelization, so

59
00:04:07.487 --> 00:04:11.315
that the dinner is ready by the time your
guest arrive, that's within an hour.

60
00:04:11.315 --> 00:04:15.593
You call your spouse and your teenage
kids to action in the kitchen.

61
00:04:15.593 --> 00:04:20.464
Now, you need to give them directions to
start dicing the ingredients for you.

62
00:04:20.464 --> 00:04:27.010
But in the heat of the moment, you end up
mixing the onions, tomatoes and peppers.

63
00:04:27.010 --> 00:04:28.890
Instead of sorting them first,

64
00:04:28.890 --> 00:04:33.380
you give everyone a randomly mixed
batch of different types of vegetables.

65
00:04:33.380 --> 00:04:38.360
They are required to use their computer
powers to chop the vegetables.

66
00:04:38.360 --> 00:04:42.210
They need to ensure not mix
different types of veggies.

67
00:04:42.210 --> 00:04:47.562
When everyone is done chopping, you want
to group the veggies by their types.

68
00:04:47.562 --> 00:04:52.877
You ask each helper to collect items
of the same type, put them in a large

69
00:04:52.877 --> 00:04:58.016
bowl and label this large bowl with
the sum of individual bowl weights

70
00:04:58.016 --> 00:05:03.975
like tomatoes in one bowl, peppers in
another and the onions in the third bowl.

71
00:05:03.975 --> 00:05:04.781
In the end,

72
00:05:04.781 --> 00:05:10.209
you have nice large bowls with the total
weight of each vegetable labeled on it.

73
00:05:10.209 --> 00:05:15.378
Your helpers are soon done with their work
while you're focused on coordinating their

74
00:05:15.378 --> 00:05:20.284
actions and other dinner tasks in the
kitchen, you can start cooking your pasta.

75
00:05:20.284 --> 00:05:25.230
What you have just seen is an excellent
example of big data modeling in action.

76
00:05:25.230 --> 00:05:30.439
Only it is really the data
processed by human processors.

77
00:05:30.439 --> 00:05:34.600
This scenario can be modeled by a common
programming model for big data.

78
00:05:34.600 --> 00:05:36.609
Namely MapReduce.

79
00:05:36.609 --> 00:05:40.625
MapReduce is a big data programming
model that supports all

80
00:05:40.625 --> 00:05:44.249
the requirements of big
data modeling we mentioned.

81
00:05:44.249 --> 00:05:47.093
It can model processing large data,

82
00:05:47.093 --> 00:05:51.406
split complications into
different parallel tasks and

83
00:05:51.406 --> 00:05:57.562
make efficient use of large commodity
clusters and distributed file systems.

84
00:05:57.562 --> 00:06:01.542
In addition, it abstracts out
the details of parallelzation,

85
00:06:01.542 --> 00:06:06.050
full tolerance, data distribution,
monitoring and load balancing.

86
00:06:07.140 --> 00:06:08.700
As a programming model,

87
00:06:08.700 --> 00:06:12.590
it has been implemented in a few
different big data frameworks.

88
00:06:12.590 --> 00:06:16.123
Next week,
we will see more details on MapReduce and

89
00:06:16.123 --> 00:06:18.788
how its Hadoop implementation works.

90
00:06:18.788 --> 00:06:21.362
To summarize, programming models for

91
00:06:21.362 --> 00:06:25.267
big data are abstractions over
distributed file systems.

92
00:06:25.267 --> 00:06:30.022
The desired programming models for
big data should handle large volumes and

93
00:06:30.022 --> 00:06:31.289
varieties of data.

94
00:06:31.289 --> 00:06:35.492
Support full tolerance and
provide scale out functionality.

95
00:06:35.492 --> 00:06:37.535
MapReduce is one of these models,

96
00:06:37.535 --> 00:06:41.063
implemented in a variety of
frameworks including Hadoop.

97
00:06:41.063 --> 00:06:45.360
We will summarize the inner workings
of the Hadoop implementation next week.
WEBVTT

1
00:00:00.230 --> 00:00:03.230
In this lecture we will use
Hadoop to run WordCount.

2
00:00:03.230 --> 00:00:05.487
First we will open a terminal shell and

3
00:00:05.487 --> 00:00:08.680
explore the Hadoop-provided MapReduce
programs.

4
00:00:08.680 --> 00:00:12.657
Next we will verify the input
file exists in HDFS.

5
00:00:12.657 --> 00:00:17.130
We will then run WordCount and
explore the WordCount output directory.

6
00:00:17.130 --> 00:00:19.560
After that we will copy
the WordCount results

7
00:00:19.560 --> 00:00:22.556
from HDFS to the local file system and
view them.

8
00:00:22.556 --> 00:00:24.493
Let's begin.

9
00:00:24.493 --> 00:00:27.930
First we'll open a terminal shell by

10
00:00:27.930 --> 00:00:30.320
clicking on the icon at
top of the window here.

11
00:00:33.430 --> 00:00:36.811
Next, we'll look at the map produced
programs that come with Hadoop.

12
00:00:36.811 --> 00:00:42.961
We can do this by running Hadoop,
jars user jars, Hadoop examples .jar.

13
00:00:46.666 --> 00:00:51.616
This command says we're going to use
the jar command to run a program

14
00:00:51.616 --> 00:00:54.030
in Hadoop from a jar file.

15
00:00:54.030 --> 00:00:59.730
And the jar file that we're running from
is in /usr/jars/hadoop-examples.jar.

16
00:00:59.730 --> 00:01:03.510
Many programs written in Java
are distributed via jar files.

17
00:01:03.510 --> 00:01:10.520
If we run this command We'll see a list of
different programs that come with Hadoop.

18
00:01:10.520 --> 00:01:13.090
So for example, wordcount.

19
00:01:13.090 --> 00:01:14.780
Count the words in a text file.

20
00:01:15.990 --> 00:01:19.350
Wordmean, count the average
length of words.

21
00:01:19.350 --> 00:01:25.220
And other programs, such as sorting and
calculating the length of pi.

22
00:01:27.748 --> 00:01:31.558
In the previous lecture we downloaded
the Works of Shakespeare and

23
00:01:31.558 --> 00:01:32.721
saved it into HDFS.

24
00:01:32.721 --> 00:01:37.822
Let's make sure that file is still
there by running hadoop fs -ls.

25
00:01:40.387 --> 00:01:44.734
We can see that the file is still there,
and it's called words.txt.

26
00:01:46.900 --> 00:01:51.390
We can run wordcount by running hadoop jar

27
00:01:51.390 --> 00:01:57.180
/usr/jars/hadoop-examples.jar wordcount.

28
00:01:57.180 --> 00:01:59.090
This command says that
we're going to run a jar,

29
00:01:59.090 --> 00:02:02.690
and this is the name of the jar
containing the program.

30
00:02:02.690 --> 00:02:05.690
And the program we're
going to run is wordcount.

31
00:02:05.690 --> 00:02:08.450
When we run it, we see that it
prints the command line usage for

32
00:02:08.450 --> 00:02:10.050
how to run wordcount.

33
00:02:10.050 --> 00:02:17.310
This says that wordcount takes one or
more input files and an output name.

34
00:02:17.310 --> 00:02:21.837
Now, both the input and
the output are located in HDFS.

35
00:02:21.837 --> 00:02:27.010
So we have the input file that we
just listed, words.txt, in HDFS.

36
00:02:27.010 --> 00:02:27.940
We can run wordcount.

37
00:02:29.400 --> 00:02:36.148
So we'll run hadoop
jar/usr/jars/hadoop-examples.jar

38
00:02:36.148 --> 00:02:39.351
wordcount words.txt out.

39
00:02:40.390 --> 00:02:45.250
This is saying we're going to run
the WordCount program using words.txt

40
00:02:45.250 --> 00:02:49.170
as the input and
put the output in a directory called out.

41
00:02:50.470 --> 00:02:51.251
So we'll run it.

42
00:02:58.381 --> 00:03:01.190
As wordcount is running,
your prints progress to the screen.

43
00:03:03.247 --> 00:03:06.760
It'll print the percentage of map and
reduce completed.

44
00:03:06.760 --> 00:03:10.124
And when both of these reach 100%,
then the job is done.

45
00:03:14.664 --> 00:03:17.440
Now that the job is complete,
let's look at the results.

46
00:03:19.900 --> 00:03:23.220
We can run Hadoop fs
-ls to see the output.

47
00:03:26.220 --> 00:03:30.930
This shows that out was created and
this is where our results are stored.

48
00:03:30.930 --> 00:03:34.580
Notice that it's
a directory with a d here.

49
00:03:34.580 --> 00:03:39.870
So hadoop word count created
the directory to contain the output.

50
00:03:39.870 --> 00:03:44.412
Let's look inside that directory
by running Hadoop fs- ls out.

51
00:03:44.412 --> 00:03:49.580
[BLANK AUDIO] We can see that there
are two files in this directory.

52
00:03:49.580 --> 00:03:55.280
The first is _SUCCESS, this means that
the WordCount job completed successfully.

53
00:03:55.280 --> 00:03:59.279
The other file part-r-00000 is a text file

54
00:03:59.279 --> 00:04:03.284
containing the output from
the WordCount command

55
00:04:05.198 --> 00:04:11.260
Now let's copy this text file to the local
file system from HDFS and then view it.

56
00:04:11.260 --> 00:04:18.357
We could copy it by running
hadoop fs -copytolocal

57
00:04:18.357 --> 00:04:23.481
out/part-r-00000 local.

58
00:04:25.760 --> 00:04:28.520
And we'll say local.txt is the name.

59
00:04:28.520 --> 00:04:33.340
You can view the results of this.

60
00:04:33.340 --> 00:04:35.320
We're running more local.txt.

61
00:04:38.170 --> 00:04:39.740
This will view the contents of the file.

62
00:04:42.700 --> 00:04:44.310
We can hit spacebar to scroll down.

63
00:04:47.130 --> 00:04:50.450
We see the results of
WordCount in this file.

64
00:04:50.450 --> 00:04:55.980
Each line is a particular word and
the second column is the count

65
00:04:55.980 --> 00:05:00.020
of how many words of this particular
word was found in the input file.

66
00:05:02.820 --> 00:05:04.159
You can hit q to quit
WEBVTT

1
00:00:01.110 --> 00:00:03.128
Scalable computing over the internet.

2
00:00:19.074 --> 00:00:23.040
Most computing is done on
a single compute node.

3
00:00:24.900 --> 00:00:29.960
If the computation needs more than
a node or parallel processing,

4
00:00:29.960 --> 00:00:34.070
like many scientific computing problems,
we use parallel computers.

5
00:00:35.070 --> 00:00:40.420
Simply put, a parallel computer
is a very large number of

6
00:00:40.420 --> 00:00:46.041
single computing nodes with specialized
capabilities connected to other network.

7
00:00:47.130 --> 00:00:54.525
For example, the Gordon Supercomputer here
at The San Diego Supercomputer Center,

8
00:00:54.525 --> 00:01:03.170
has 1,024 compute nodes with 16 cores each
equalling 16,384 compute cores in total.

9
00:01:04.230 --> 00:01:07.890
This type of specialized
computer is pretty costly

10
00:01:07.890 --> 00:01:13.180
compared to its most recent cousin,
the commodity cluster.

11
00:01:13.180 --> 00:01:17.920
The term, commodity cluster,
is often heard in big data conversations.

12
00:01:19.280 --> 00:01:21.560
Have you ever wondered
what it exactly means?

13
00:01:22.960 --> 00:01:27.740
Commodity clusters are affordable
parallel computers

14
00:01:27.740 --> 00:01:29.780
with an average number of computing nodes.

15
00:01:31.070 --> 00:01:35.650
They are not as powerful as
traditional parallel computers and

16
00:01:35.650 --> 00:01:38.280
are often built out of
less specialized nodes.

17
00:01:39.400 --> 00:01:42.660
In fact,
the nodes in the commodity cluster

18
00:01:42.660 --> 00:01:45.640
are more generic in their
computing capabilities.

19
00:01:46.960 --> 00:01:51.320
The service-oriented computing community
over the internet have pushed for

20
00:01:51.320 --> 00:01:56.800
computing to be done on commodity
clusters as distributed computations.

21
00:01:56.800 --> 00:02:02.030
And in turn, reducing the cost
of computing over the Internet.

22
00:02:03.680 --> 00:02:09.390
In commodity clusters,
the computing nodes are clustered in racks

23
00:02:11.400 --> 00:02:15.480
connected to each other
via a fast network.

24
00:02:16.860 --> 00:02:20.150
There might be many of such
racks in extensible amounts.

25
00:02:21.600 --> 00:02:26.410
Computing in one or
more of these clusters across

26
00:02:26.410 --> 00:02:31.910
a local area network or the internet
is called distributed computing.

27
00:02:31.910 --> 00:02:36.860
Such architectures enable what
we call data-parallelism.

28
00:02:36.860 --> 00:02:41.416
In data-parallelism many jobs
that share nothing can work on

29
00:02:41.416 --> 00:02:44.580
different data sets or
parts of a data set.

30
00:02:45.930 --> 00:02:51.060
This type of parallelism sometimes
gets called as job level parallelism.

31
00:02:51.060 --> 00:02:55.950
But in this specialization,
we will refer to it as data-parallelism

32
00:02:55.950 --> 00:03:00.020
in the context of Big-data computing.

33
00:03:00.020 --> 00:03:05.020
Large volumes and varieties of big
data can be analyzed using this mode

34
00:03:05.020 --> 00:03:11.540
of parallelism, achieving scalability,
performance and cost reduction.

35
00:03:11.540 --> 00:03:16.070
As you can imagine, there are many
points of failure inside systems.

36
00:03:17.500 --> 00:03:23.200
A node, or
an entire rack can fail at any given time.

37
00:03:23.200 --> 00:03:27.850
The connectivity of a rack
to the network can stop or

38
00:03:27.850 --> 00:03:31.300
the connections between
individual nodes can break.

39
00:03:32.820 --> 00:03:37.370
It is not practical to restart everything
every time, if failure happens.

40
00:03:38.500 --> 00:03:44.069
The ability to recover from such
failures is called Fault-tolerance.

41
00:03:44.069 --> 00:03:49.245
For Fault-tolerance of such systems,
two neat solutions emerged.

42
00:03:49.245 --> 00:03:54.230
Namely, Redundant data storage and

43
00:03:54.230 --> 00:03:57.480
restart of failed
individual parallel jobs.

44
00:03:58.700 --> 00:04:00.780
We will explain these two solutions next.

45
00:04:02.100 --> 00:04:06.970
As a summary the commodity
clusters are a cost effective way

46
00:04:06.970 --> 00:04:11.360
of achieving data parallel scalability for
big data applications.

47
00:04:11.360 --> 00:04:16.750
These type of systems have a higher
potential for partial failures.

48
00:04:16.750 --> 00:04:19.850
It is this type of distributed
computing that pushed for

49
00:04:19.850 --> 00:04:23.385
a change towards cost
effective reliable and

50
00:04:23.385 --> 00:04:27.550
Fault-tolerant systems for
management and analysis of big data.
WEBVTT

1
00:00:02.950 --> 00:00:07.672
The Hadoop Distributed File System,
a storage system for big data.

2
00:00:21.994 --> 00:00:28.920
As a storage layer, the Hadoop distributed
file system, or the way we call it HDFS.

3
00:00:30.070 --> 00:00:33.890
Serves as the foundation for
most tools in the Hadoop ecosystem.

4
00:00:35.700 --> 00:00:39.900
It provides two capabilities that
are essential for managing big data.

5
00:00:40.900 --> 00:00:43.500
Scalability to large data sets.

6
00:00:43.500 --> 00:00:46.570
And reliability to cope
with hardware failures.

7
00:00:48.760 --> 00:00:53.060
HDFS allows you to store and
access large datasets.

8
00:00:53.060 --> 00:00:57.700
According to Hortonworks,
a leading vendor of Hadoop services,

9
00:00:57.700 --> 00:01:03.950
HDFS has shown production
scalability up to 200 petabytes and

10
00:01:03.950 --> 00:01:06.980
a single cluster of 4,500 servers.

11
00:01:06.980 --> 00:01:09.684
With close to a billion files and blocks.

12
00:01:11.504 --> 00:01:16.610
If you run out of space, you can simply
add more nodes to increase the space.

13
00:01:19.000 --> 00:01:22.470
HDFS achieves scalability
by partitioning or

14
00:01:22.470 --> 00:01:26.630
splitting large files
across multiple computers.

15
00:01:26.630 --> 00:01:31.040
This allows parallel access to very large
files since the computations run in

16
00:01:31.040 --> 00:01:34.300
parallel on each node
where the data is stored.

17
00:01:35.760 --> 00:01:38.920
Typical file size is
gigabytes to terabytes.

18
00:01:40.150 --> 00:01:47.900
The default chunk size, the size of
each piece of a file is 64 megabytes.

19
00:01:47.900 --> 00:01:50.390
But you can configure this to any size.

20
00:01:51.950 --> 00:01:55.180
By spreading the file across many nodes,

21
00:01:55.180 --> 00:01:59.740
the chances are increased that a node
storing one of the blocks will fail.

22
00:02:01.650 --> 00:02:03.440
What happens next?

23
00:02:03.440 --> 00:02:05.970
Do we lose the information
stored in block C?

24
00:02:09.560 --> 00:02:12.350
HDFS is designed for
full tolerance in such case.

25
00:02:13.640 --> 00:02:16.080
HDFS replicates, or

26
00:02:16.080 --> 00:02:21.530
makes a copy of, file blocks on
different nodes to prevent data loss.

27
00:02:23.820 --> 00:02:28.945
In this example,
the node that crashed stored block C.

28
00:02:28.945 --> 00:02:37.270
But block C was replicated on
two other nodes in the cluster.

29
00:02:39.820 --> 00:02:44.380
By default, HDFS maintains
three copies of every block.

30
00:02:45.760 --> 00:02:47.850
This is the default replication factor.

31
00:02:48.890 --> 00:02:52.300
But you can change it globally for
every file, or

32
00:02:52.300 --> 00:02:57.180
on a per file basis.

33
00:02:57.180 --> 00:03:01.370
HDFS is also designed to handle
a variety of data types aligned with

34
00:03:01.370 --> 00:03:02.450
big data variety.

35
00:03:04.370 --> 00:03:10.220
To read a file in HDFS you must
specify the input file format.

36
00:03:10.220 --> 00:03:14.610
Similarly to write the file you must
provide the output file format.

37
00:03:16.885 --> 00:03:21.040
HDFS provides a set of formats for
common data types.

38
00:03:21.040 --> 00:03:27.310
But this is extensible and you can provide
custom formats for your data types.

39
00:03:27.310 --> 00:03:30.830
For example text files can be read.

40
00:03:32.030 --> 00:03:34.620
Line by line or a word at a time.

41
00:03:36.950 --> 00:03:42.650
Geospatial data can be read as vectors or
rasters.

42
00:03:43.660 --> 00:03:49.210
Data formats specific to geospatial data,
or

43
00:03:49.210 --> 00:03:52.670
other domain specific data formats.

44
00:03:52.670 --> 00:03:57.588
Like FASTA, or FASTQ formats for
sequence data genomics.

45
00:03:59.660 --> 00:04:02.665
HDFS is comprised of two components.

46
00:04:02.665 --> 00:04:06.582
NameNode, and DataNode.

47
00:04:06.582 --> 00:04:11.180
These operate using a master
slave relationship.

48
00:04:12.350 --> 00:04:16.960
Where the NameNode issues comments
to DataNodes across the cluster.

49
00:04:18.360 --> 00:04:22.110
The NameNode is responsible for metadata.

50
00:04:22.110 --> 00:04:25.072
And DataNodes provide block storage.

51
00:04:27.174 --> 00:04:31.064
There is usually one NameNode per cluster,

52
00:04:31.064 --> 00:04:35.910
a DataNode however,
runs on each node in the cluster.

53
00:04:37.920 --> 00:04:42.280
In some sense the NameNode
is the administrator or

54
00:04:42.280 --> 00:04:44.850
the coordinator of the HDFS cluster.

55
00:04:46.030 --> 00:04:50.560
When the file is created,
the NameNode records the name,

56
00:04:50.560 --> 00:04:54.010
location in the directory hierarchy and
other metadata.

57
00:04:55.820 --> 00:05:01.730
The NameNode also decides which data nodes
to store the contents of the file and

58
00:05:01.730 --> 00:05:03.370
remembers this mapping.

59
00:05:05.420 --> 00:05:08.400
The DataNode runs on
each node in the cluster.

60
00:05:09.460 --> 00:05:12.880
And is responsible for
storing the file blocks.

61
00:05:14.600 --> 00:05:19.328
The data node listens to commands from
the name node for block creation,

62
00:05:19.328 --> 00:05:25.980
deletion, and replication.

63
00:05:25.980 --> 00:05:29.530
Replication provides two key capabilities.

64
00:05:29.530 --> 00:05:32.260
Fault tolerance and data locality.

65
00:05:33.390 --> 00:05:38.730
As discussed earlier, when a machine in
the cluster has a hardware failure there

66
00:05:38.730 --> 00:05:43.100
are two other copies of each block
that are stored on that node.

67
00:05:43.100 --> 00:05:44.470
So no data is lost.

68
00:05:45.660 --> 00:05:49.920
Replication also means that the same block
will be stored on different nodes on

69
00:05:49.920 --> 00:05:54.780
the system which are in different
geographical locations.

70
00:05:56.370 --> 00:06:01.280
A location may mean a specific rack or
a data center in a different town.

71
00:06:03.140 --> 00:06:07.950
The location is important since we
want to move computation to data and

72
00:06:07.950 --> 00:06:08.890
not the other way around.

73
00:06:11.100 --> 00:06:15.030
We'll talk about what moving computation
to data means later in this module.

74
00:06:16.650 --> 00:06:21.120
As I mentioned earlier, the default
replication factor is three, but

75
00:06:21.120 --> 00:06:23.080
you can change this.

76
00:06:23.080 --> 00:06:28.370
A high replication factor means more
protection against hardware failures,

77
00:06:28.370 --> 00:06:31.350
and better chances for data locality.

78
00:06:31.350 --> 00:06:34.660
But it also means increased
storage space is used.

79
00:06:37.190 --> 00:06:40.050
As a summary HDFS provides

80
00:06:40.050 --> 00:06:44.440
scalable big data storage by
partitioning files over multiple nodes.

81
00:06:45.950 --> 00:06:50.160
This helps to scale big data
analytics to large data volumes.

82
00:06:51.530 --> 00:06:54.730
The application protects
against hardware failures and

83
00:06:54.730 --> 00:06:59.420
provides data locality when we move
analytical complications to data.
WEBVTT

1
00:00:02.252 --> 00:00:05.592
The Hadoop Ecosystem: So Much free Stuff!

2
00:00:20.275 --> 00:00:24.046
How did the big data
open-source movement begin?

3
00:00:24.046 --> 00:00:28.492
In 2004 Google published
a paper about their

4
00:00:28.492 --> 00:00:33.811
in-house processing framework
they called MapReduce.

5
00:00:33.811 --> 00:00:37.699
The next year,
Yahoo released an open-source

6
00:00:37.699 --> 00:00:42.369
implementation based on this
framework called Hadoop.

7
00:00:42.369 --> 00:00:45.379
In the following years,
other frameworks and

8
00:00:45.379 --> 00:00:49.650
tools were released to the community
as open-source projects.

9
00:00:49.650 --> 00:00:53.764
These frameworks provided new
capabilities missing in Hadoop,

10
00:00:53.764 --> 00:00:56.990
such as SQL like querying or
high level scripting.

11
00:00:59.240 --> 00:01:03.220
Today, there are over 100
open-source projects for

12
00:01:03.220 --> 00:01:06.880
big data and
this number continues to grow.

13
00:01:08.160 --> 00:01:11.181
Many rely on Hadoop, but
some are independent.

14
00:01:13.711 --> 00:01:18.420
With so many frameworks and tools
available, how do we learn what they do?

15
00:01:19.600 --> 00:01:26.590
We can organize them with a layer diagram
to understand their capabilities.

16
00:01:26.590 --> 00:01:30.830
Sometimes we also used the term
stack instead of a layer diagram.

17
00:01:32.560 --> 00:01:37.400
In a layer diagram,
a component uses the functionality or

18
00:01:37.400 --> 00:01:41.958
capabilities of the components
in the layer below it.

19
00:01:41.958 --> 00:01:47.470
Usually components at the same
layer do not communicate.

20
00:01:47.470 --> 00:01:54.669
And a component never assumes a specific
tool or component is above it.

21
00:01:54.669 --> 00:02:00.901
In this example,
component A is in the bottom layer,

22
00:02:00.901 --> 00:02:04.370
which components B and C use.

23
00:02:06.170 --> 00:02:09.390
Component D uses B, but not C.

24
00:02:11.240 --> 00:02:13.790
And D does not directly use A.

25
00:02:17.060 --> 00:02:21.630
Let's look at one set of tools in
the Hadoop ecosystem as a layer diagram.

26
00:02:23.920 --> 00:02:30.795
This layer diagram is organized
vertically based on the interface.

27
00:02:30.795 --> 00:02:35.054
Low level interfaces, so storage and
scheduling, on the bottom.

28
00:02:35.054 --> 00:02:39.315
And high level languages and
interactivity at the top.

29
00:02:41.915 --> 00:02:47.315
The Hadoop distributed file system,
or HDFS, is the foundation for

30
00:02:47.315 --> 00:02:53.570
many big data frameworks, since it
provides scaleable and reliable storage.

31
00:02:54.740 --> 00:03:00.380
As the size of your data increases,
you can add commodity hardware

32
00:03:00.380 --> 00:03:05.660
to HDFS to increase storage capacity so

33
00:03:05.660 --> 00:03:09.250
it enables scaling out of your resources.

34
00:03:11.460 --> 00:03:15.388
Hadoop YARN provides
flexible scheduling and

35
00:03:15.388 --> 00:03:19.220
resource management over the HDFS storage.

36
00:03:20.910 --> 00:03:25.988
YARN is used at Yahoo to schedule
jobs across 40,000 servers.

37
00:03:28.370 --> 00:03:33.430
MapReduce is a programming model
that simplifies parallel computing.

38
00:03:34.720 --> 00:03:39.300
Instead of dealing with the complexities
of synchronization and scheduling, you

39
00:03:39.300 --> 00:03:45.920
only need to give MapReduce two functions,
map and reduce, as you heard before.

40
00:03:48.020 --> 00:03:50.080
This programming model is so

41
00:03:50.080 --> 00:03:55.740
powerful that Google previously
used it for indexing websites.

42
00:03:59.450 --> 00:04:05.009
MapReduce only assume a limited
model to express data.

43
00:04:05.009 --> 00:04:10.094
Hive and Pig are two additional
programming models on

44
00:04:10.094 --> 00:04:15.292
top of MapReduce to augment
data modeling of MapReduce

45
00:04:15.292 --> 00:04:21.398
with relational algebra and
data flow modeling respectively.

46
00:04:21.398 --> 00:04:25.718
Hive was created at
Facebook to issue SQL-like

47
00:04:25.718 --> 00:04:30.038
queries using MapReduce
on their data in HDFS.

48
00:04:30.038 --> 00:04:37.951
Pig was created at Yahoo to model data
flow based programs using MapReduce.

49
00:04:37.951 --> 00:04:41.431
Thanks to YARN stability
to manage resources,

50
00:04:41.431 --> 00:04:45.959
not just for MapReduce but
other programming models as well.

51
00:04:45.959 --> 00:04:50.787
Giraph was built for
processing large-scale graphs efficiently.

52
00:04:50.787 --> 00:04:57.904
For example, Facebook uses Giraph to
analyze the social graphs of its users.

53
00:04:57.904 --> 00:05:03.196
Similarly, Storm, Spark,
and Flink were built for

54
00:05:03.196 --> 00:05:07.899
real time and
in memory processing of big data on

55
00:05:07.899 --> 00:05:12.381
top of the YARN resource scheduler and
HDFS.

56
00:05:12.381 --> 00:05:17.986
In-memory processing is a powerful way of
running big data applications even faster,

57
00:05:17.986 --> 00:05:21.787
achieving 100x's better performance for
some tasks.

58
00:05:24.446 --> 00:05:28.405
Sometimes, your data or
processing tasks are not easily or

59
00:05:28.405 --> 00:05:33.330
efficiently represented using the file and
directory model of storage.

60
00:05:34.620 --> 00:05:39.810
Examples of this include collections
of key-values or large sparse tables.

61
00:05:41.990 --> 00:05:49.381
NoSQL projects such as Cassandra,
MongoDB, and HBase handle these cases.

62
00:05:49.381 --> 00:05:54.180
Cassandra was created at Facebook,
but Facebook also used HBase for

63
00:05:54.180 --> 00:05:56.010
its messaging platform.

64
00:05:58.320 --> 00:06:03.560
Finally, running all of these tools
requires a centralized management system

65
00:06:03.560 --> 00:06:08.490
for synchronization, configuration and
to ensure high availability.

66
00:06:09.860 --> 00:06:13.210
Zookeeper performs these duties.

67
00:06:13.210 --> 00:06:17.700
It was created by Yahoo to wrangle
services named after animals.

68
00:06:20.060 --> 00:06:22.990
A major benefit of the Hadoop ecosystem

69
00:06:22.990 --> 00:06:25.340
is that all these tools
are open-source projects.

70
00:06:26.450 --> 00:06:28.920
You can download and use them for free.

71
00:06:30.220 --> 00:06:34.934
Each project has a community of users and
developers that

72
00:06:34.934 --> 00:06:39.756
answer questions, fix bugs and
implement new features.

73
00:06:39.756 --> 00:06:44.370
You can mix and match to get only
the tools you need to achieve your goals.

74
00:06:46.010 --> 00:06:50.330
Alternatively, there are several
pre-built stacks of these tools

75
00:06:50.330 --> 00:06:54.670
offered by companies such as Cloudera,
MAPR and Hortonworks.

76
00:06:56.090 --> 00:07:00.850
These companies provide the core
software stacks for free and

77
00:07:00.850 --> 00:07:03.740
offer commercial support for
production environments.

78
00:07:05.800 --> 00:07:09.110
As a summary, the Hadoop ecosystem

79
00:07:09.110 --> 00:07:13.672
consists of a growing number
of open-source tools.

80
00:07:13.672 --> 00:07:17.502
Providing opportunities to
pick the right tool for

81
00:07:17.502 --> 00:07:21.788
the right tasks for
better performance and lower costs.

82
00:07:21.788 --> 00:07:25.284
We will reveal some of these
tools in further detail and

83
00:07:25.284 --> 00:07:29.620
provide an analysis of when to use
which in the next set of lectures.
WEBVTT

1
00:00:00.610 --> 00:00:03.160
Generating value from Hadoop and

2
00:00:03.160 --> 00:00:07.996
Pre-Built Hadoop Images that
come as off the shelf products.

3
00:00:18.576 --> 00:00:23.922
Assembling your own software stack
from scratch can be messy and

4
00:00:23.922 --> 00:00:26.298
a lot of work for beginners.

5
00:00:26.298 --> 00:00:31.661
The task of setting up the whole stack
could consume a lot of project time and

6
00:00:31.661 --> 00:00:35.550
man power, reducing time to deployment.

7
00:00:35.550 --> 00:00:41.490
Getting pre-built images is similar
to buying pre-assembled furniture.

8
00:00:41.490 --> 00:00:46.701
You can obtain a ready to go software
stack which contains a pre-installed

9
00:00:46.701 --> 00:00:51.510
operating system, required libraries and
application software.

10
00:00:52.830 --> 00:00:55.380
It saves you from the trouble
of putting the different

11
00:00:55.380 --> 00:00:58.090
parts together in the right orientation.

12
00:00:58.090 --> 00:01:00.030
You can start using
the furniture right away.

13
00:01:01.600 --> 00:01:05.620
Packaging of these pre-built
software images is enabled

14
00:01:05.620 --> 00:01:08.420
by virtual machines using
virtualization software.

15
00:01:09.640 --> 00:01:14.380
Without going into too much detail,
one of the benefits of virtualization

16
00:01:14.380 --> 00:01:18.710
software is that it lets you run a ready
made software stack within minutes.

17
00:01:19.890 --> 00:01:23.390
Your software stack comes as a large file.

18
00:01:23.390 --> 00:01:26.800
Virtualization software provides
a platform where your stack can run.

19
00:01:28.160 --> 00:01:33.240
Many companies provide images for
their version of the Hadoop platform,

20
00:01:33.240 --> 00:01:36.100
including a number of
tools of their choice.

21
00:01:37.100 --> 00:01:41.570
Hortonworks is one of the companies that
provides a pre-built software stack for

22
00:01:41.570 --> 00:01:43.370
both Mac and Windows platforms.

23
00:01:44.520 --> 00:01:48.420
Cloudera is another company
that provides pre-installed and

24
00:01:48.420 --> 00:01:51.120
assembled software stack images.

25
00:01:51.120 --> 00:01:54.230
Cloudera image is what we will
be working with in this course.

26
00:01:55.480 --> 00:01:58.020
Many other companies
provide similar images.

27
00:01:59.100 --> 00:02:04.831
Additionally, lots of online tutorials for
beginners are on vendors websites for

28
00:02:04.831 --> 00:02:08.740
self-training of users
working with these images and

29
00:02:08.740 --> 00:02:10.580
the open source tools they include.

30
00:02:12.270 --> 00:02:16.200
Once you choose the vendor,
you can check out their website for

31
00:02:16.200 --> 00:02:18.550
tutorials on how to get started quickly.

32
00:02:19.590 --> 00:02:21.980
There are plenty of resources online for
that.

33
00:02:23.240 --> 00:02:26.080
You can deploy pre-built
images over the Cloud.

34
00:02:27.110 --> 00:02:30.490
This would further accelerate your
application deployment process.

35
00:02:31.850 --> 00:02:36.170
It is always best to evaluate which
approach is most cost effective for

36
00:02:36.170 --> 00:02:38.320
your business model and organization.

37
00:02:39.560 --> 00:02:43.485
Companies such as Cloudera,
Hortonworks and others,

38
00:02:43.485 --> 00:02:49.090
provide step-by-step guides on how to
set up pre-built images on the Cloud.

39
00:02:50.180 --> 00:02:55.460
As a summary, using pre-built software
packages have a number of benefits and

40
00:02:55.460 --> 00:02:58.280
can significantly accelerate
your big data projects.

41
00:02:59.500 --> 00:03:06.558
Even small teams can quickly prototype,
deploy and validate their project ideas.

42
00:03:06.558 --> 00:03:11.480
The developed analytical solutions
can be scaled to larger volumes and

43
00:03:11.480 --> 00:03:15.560
increase velocities of
data in a matter of hours.

44
00:03:15.560 --> 00:03:20.162
These companies also provide
Enterprise level solutions for large,

45
00:03:20.162 --> 00:03:21.837
full-fledged applications.

46
00:03:22.940 --> 00:03:27.395
An added benefit is that there
are plenty of companies which provide

47
00:03:27.395 --> 00:03:29.250
ready-made solutions.

48
00:03:29.250 --> 00:03:35.270
That means lots of choices for you to
pick the one most suited to your project.
WEBVTT

1
00:00:00.540 --> 00:00:02.593
What is a Distributed File System?

2
00:00:16.485 --> 00:00:19.336
Most of us have file
cabinets in our offices or

3
00:00:19.336 --> 00:00:22.420
homes that help us store
our printed documents.

4
00:00:24.170 --> 00:00:27.280
Everyone has their own
method of organizing files,

5
00:00:27.280 --> 00:00:31.410
including the way we bin similar
documents into one file, or

6
00:00:31.410 --> 00:00:35.210
the way we sort them in alphabetical or
date order.

7
00:00:35.210 --> 00:00:38.380
When computers first came out,
the information and

8
00:00:38.380 --> 00:00:40.970
programs were stored in punch cards.

9
00:00:42.280 --> 00:00:45.374
These punch cards were
stored in file cabinets,

10
00:00:45.374 --> 00:00:48.760
just like the physical
file cabinets today.

11
00:00:48.760 --> 00:00:51.940
This is where the name,
file system, comes from.

12
00:00:51.940 --> 00:00:55.970
The need to store information in
files comes from a larger need

13
00:00:55.970 --> 00:00:58.720
to store information in the long-term.

14
00:00:58.720 --> 00:01:03.360
This way the information lives
after the computer program, or

15
00:01:03.360 --> 00:01:07.350
what we call process,
that produced it terminates.

16
00:01:07.350 --> 00:01:12.450
If we don't have files, our access to
such information would not be possible

17
00:01:12.450 --> 00:01:15.560
once a program using or producing it.

18
00:01:15.560 --> 00:01:20.430
Even during the process, we might need
to store large amounts of information

19
00:01:20.430 --> 00:01:25.240
that we cannot store within the program
components or computer memory.

20
00:01:25.240 --> 00:01:28.690
In addition, once the data is in a file,

21
00:01:28.690 --> 00:01:33.120
multiple processes can access
the same information if needed.

22
00:01:33.120 --> 00:01:38.940
For all these reasons, we store
information in files on a hard disk.

23
00:01:38.940 --> 00:01:41.350
There are many of these files, and

24
00:01:41.350 --> 00:01:45.750
they get managed by your operating system,
like Windows or Linux.

25
00:01:45.750 --> 00:01:50.640
How the operating system manages
files is called a file system.

26
00:01:50.640 --> 00:01:55.930
How this information is stored
on disk drives has high impact

27
00:01:55.930 --> 00:02:02.090
on the efficiency and speed of access to
data, especially in the big data case.

28
00:02:02.090 --> 00:02:07.420
While the files have exact addresses for
their locations in the drive, referring

29
00:02:07.420 --> 00:02:13.274
to the data units of sequence of these
blocks, that's called the flat structure,

30
00:02:13.274 --> 00:02:18.510
or hierarchy construction of index
records, that's called the database.

31
00:02:18.510 --> 00:02:24.710
They also have human readable symbolic
names, generally followed by an extension.

32
00:02:25.750 --> 00:02:29.140
Extensions tell what kind of file it is,
in general.

33
00:02:29.140 --> 00:02:33.640
Programs and
users can access files with their names.

34
00:02:33.640 --> 00:02:39.010
The contents of a file can be numeric,
alphabetic, alphanumeric,

35
00:02:39.010 --> 00:02:40.160
or binary executables.

36
00:02:41.180 --> 00:02:45.250
Most computer users work
on personal laptops or

37
00:02:45.250 --> 00:02:48.050
desktop computers with
a single hard drive.

38
00:02:49.050 --> 00:02:53.780
In this model, the user is limited
to the capacity of their hard drive.

39
00:02:53.780 --> 00:02:57.060
The capacity of different devices vary.

40
00:02:57.060 --> 00:02:59.460
For example, while your phone or

41
00:02:59.460 --> 00:03:04.380
tablet might have a storage capacity
in the order of gigabytes, your

42
00:03:04.380 --> 00:03:10.210
laptop computer might have a terabyte of
storage, but what if you have more data?

43
00:03:10.210 --> 00:03:13.290
Some of you probably had
issues in the past with

44
00:03:13.290 --> 00:03:15.950
running out of space on your hard drive.

45
00:03:15.950 --> 00:03:19.250
A way to solve this is to have
an external hard drive and

46
00:03:19.250 --> 00:03:23.390
store your files there or,
you can buy a bigger disk.

47
00:03:23.390 --> 00:03:28.580
Both options are a bit of a hassle, to
copy the data to a new disk, aren't they?

48
00:03:28.580 --> 00:03:30.520
They might not even be
an option sometimes.

49
00:03:31.560 --> 00:03:35.620
Now imagine, you having two computers and

50
00:03:35.620 --> 00:03:41.050
storing some of your data in one and
the rest of your data in another.

51
00:03:41.050 --> 00:03:45.560
How you organize and partition your data
between these computers is up to you.

52
00:03:46.570 --> 00:03:50.400
You might want to store your
work data in one computer and

53
00:03:50.400 --> 00:03:53.030
your personal data in another.

54
00:03:53.030 --> 00:03:57.120
Distributing data on multiple
computers might be an option, but

55
00:03:57.120 --> 00:03:59.030
it raises new issues.

56
00:03:59.030 --> 00:04:04.800
In this situation, you need to know
where to find the files you need,

57
00:04:04.800 --> 00:04:06.630
depending on what you’re doing.

58
00:04:06.630 --> 00:04:10.080
You can find it manageable,
if it’s just your data.

59
00:04:10.080 --> 00:04:14.500
But now imagine having
thousands of computers

60
00:04:14.500 --> 00:04:18.373
to store your data with big volumes and
variety.

61
00:04:18.373 --> 00:04:23.240
Wouldn't it be good to have a system
that can handle the data access and

62
00:04:23.240 --> 00:04:24.330
do this for you?

63
00:04:24.330 --> 00:04:29.450
This is a case that can be handled
by a distributive file system.

64
00:04:29.450 --> 00:04:33.590
Now, let's assume that there
are racks of these computers,

65
00:04:33.590 --> 00:04:37.758
often even distributed across the local or

66
00:04:37.758 --> 00:04:43.400
wide area network, because such file
systems, distributed file systems.

67
00:04:44.910 --> 00:04:48.800
Data sets, or parts of a data set,

68
00:04:48.800 --> 00:04:52.900
can be replicated across the nodes
of a distributed file system.

69
00:04:53.900 --> 00:04:58.900
Since data is already on these nodes,
then analysis of parts of the data

70
00:04:58.900 --> 00:05:05.030
is needed in a data parallel fashion,
computation can be moved to these nodes.

71
00:05:06.500 --> 00:05:11.590
Additionally, distributed file
systems replicate the data

72
00:05:11.590 --> 00:05:16.720
between the racks, and also computers
distributed across geographical regions.

73
00:05:18.180 --> 00:05:22.160
Data replication makes
the system more fault tolerant.

74
00:05:23.250 --> 00:05:28.290
That means, if some nodes or
a rack goes down,

75
00:05:28.290 --> 00:05:33.764
there are other parts of the system,
the same data can be found and analyzed.

76
00:05:33.764 --> 00:05:40.080
Data replication also helps with scaling
the access to this data by many users.

77
00:05:41.170 --> 00:05:47.110
Often, if the data is popular, many
reader processes will want access to it.

78
00:05:48.310 --> 00:05:51.240
In a highly parallelized replication,

79
00:05:51.240 --> 00:05:55.680
each reader can get their own node
to access to and analyze data.

80
00:05:56.920 --> 00:05:59.450
This increases overall system performance.

81
00:06:00.935 --> 00:06:05.600
Note that a problem with having
such a distributive replication is,

82
00:06:05.600 --> 00:06:09.490
that it is hard to make
changes to data over time.

83
00:06:09.490 --> 00:06:15.090
However, in most big data systems,
the data is written once and

84
00:06:15.090 --> 00:06:20.570
the updates to data is maintained
as additional data sets over time.

85
00:06:20.570 --> 00:06:25.500
As a summary, a file system is
responsible from the organization of

86
00:06:25.500 --> 00:06:28.970
the long term information
storage in a computer.

87
00:06:28.970 --> 00:06:33.991
When many storage computers
are connected through the network,

88
00:06:33.991 --> 00:06:36.916
we call it a distributed file system.

89
00:06:36.916 --> 00:06:42.035
Distributed file systems provide data
scalability, fault tolerance, and

90
00:06:42.035 --> 00:06:47.321
high concurrency through partitioning and
replication of data on many nodes.
WEBVTT

1
00:00:03.148 --> 00:00:05.208
When to reconsider Hadoop?

2
00:00:19.098 --> 00:00:22.960
The Hadoop ecosystem is
growing at a fast pace.

3
00:00:23.970 --> 00:00:26.937
This means a lot of stuff
that was difficult, or

4
00:00:26.937 --> 00:00:30.180
not supportive, is becoming possible.

5
00:00:32.480 --> 00:00:33.990
In this lecture,

6
00:00:33.990 --> 00:00:37.730
we will look at some aspects that
clearly make a good match for Hadoop.

7
00:00:39.200 --> 00:00:43.220
We will also look at several
aspects that might motivate you

8
00:00:43.220 --> 00:00:45.510
to evaluate Hadoop at a deeper level.

9
00:00:47.060 --> 00:00:51.560
And does Hadoop really make sense for
your specific problem?

10
00:00:54.190 --> 00:00:58.790
First let's look at the key features
that make a problem Hadoop friendly.

11
00:00:59.950 --> 00:01:04.390
If you see a large scale growth in
amount of data you will tackle,

12
00:01:04.390 --> 00:01:06.310
probably it makes sense to use Hadoop.

13
00:01:07.610 --> 00:01:12.230
When you want quick access to your old
data which would otherwise go on tape

14
00:01:12.230 --> 00:01:17.180
drives for archival storage,
Hadoop might provide a good alternative.

15
00:01:20.080 --> 00:01:22.650
Other Hadoop friendly features include

16
00:01:22.650 --> 00:01:28.020
scenarios when you want to use multiple
applications over the same data store.

17
00:01:28.020 --> 00:01:31.683
High volume or
high variety are also great indicators for

18
00:01:31.683 --> 00:01:33.598
Hadoop as a platform choice.

19
00:01:37.148 --> 00:01:40.860
Small data set processing
should raise your eyebrows.

20
00:01:40.860 --> 00:01:42.410
Do you really need Hadoop for that?

21
00:01:43.560 --> 00:01:49.178
Dig deeper, and find out exactly why you
want to use Hadoop before going ahead.

22
00:01:53.238 --> 00:01:56.330
Hadoop is good for data parallelism.

23
00:01:56.330 --> 00:02:01.320
As you know, data parallelism is
the simultaneous execution of the same

24
00:02:01.320 --> 00:02:05.520
function on multiple nodes across
the elements of a dataset.

25
00:02:06.630 --> 00:02:11.060
On the other hand, task parallelism,
as you see in this graph,

26
00:02:12.640 --> 00:02:17.160
is the simultaneous execution
of many different functions

27
00:02:17.160 --> 00:02:21.260
on multiple nodes across the same or
different data sets.

28
00:02:22.930 --> 00:02:26.950
If your problem has task-level
parallelism, you must do further

29
00:02:26.950 --> 00:02:30.990
analysis as to which tools you plan
to deploy from the Hadoop ecosystem.

30
00:02:32.910 --> 00:02:36.260
What are the precise benefits
that these tools provide?

31
00:02:37.790 --> 00:02:38.920
Proceed with caution.

32
00:02:41.330 --> 00:02:44.330
Not all algorithms are scalable in Hadoop,
or

33
00:02:44.330 --> 00:02:48.030
reducible to one of the programming
models supported by YARN.

34
00:02:50.090 --> 00:02:55.590
Hence, if you are looking to deploy
highly coupled data processing algorithms

35
00:02:55.590 --> 00:02:56.860
proceed with caution.

36
00:02:58.300 --> 00:03:01.270
Do a thorough analysis
before using Hadoop.

37
00:03:01.270 --> 00:03:05.680
Are you thinking of

38
00:03:05.680 --> 00:03:10.350
throwing away your existing database
solutions and replacing them with Hadoop?

39
00:03:10.350 --> 00:03:10.850
Think again.

40
00:03:12.130 --> 00:03:17.100
Hadoop may be a good platform where
your diverse data sets can land and

41
00:03:17.100 --> 00:03:21.210
get processed into a form
digestible with your database.

42
00:03:22.660 --> 00:03:27.070
Hadoop may not be the best data store
solution for your business case.

43
00:03:27.070 --> 00:03:29.470
Evaluate, and proceed with caution.

44
00:03:30.960 --> 00:03:35.560
HDFS stores data in blocks
of 64 megabytes or larger,

45
00:03:35.560 --> 00:03:40.940
so you may have to read an entire
file just to pick one data entry.

46
00:03:42.580 --> 00:03:45.858
That makes it a bit harder to
perform random data access.

47
00:03:48.078 --> 00:03:52.370
The Hadoop ecosystem is growing
at a faster pace than ever.

48
00:03:53.820 --> 00:03:57.530
This slide shows some of the moving
targets in the Hadoop ecosystem and

49
00:03:57.530 --> 00:04:00.650
the additional needs which
must be addressed by new tools

50
00:04:00.650 --> 00:04:02.470
to the Hadoop ecosystem.

51
00:04:02.470 --> 00:04:05.260
Mainly, advanced analytical queries,

52
00:04:06.270 --> 00:04:10.580
latency sensitive tasks, and
cyber security of sensitive data.

53
00:04:12.050 --> 00:04:16.510
Here, we give pointers to tools you
might want to look into further

54
00:04:16.510 --> 00:04:19.090
to understand the challenges
these need tools address.

55
00:04:21.000 --> 00:04:27.590
As a summary, although Hadoop is good with
scalability of many algorithms, it is just

56
00:04:27.590 --> 00:04:32.520
one model and does not solve all issues
in managing and processing big data.

57
00:04:33.630 --> 00:04:38.590
Although it would be possible to find
counterexamples, we can generally say

58
00:04:38.590 --> 00:04:43.110
that the Hadoop framework is not the best
for working with small data sets,

59
00:04:43.110 --> 00:04:47.120
advanced algorithms that require
a specific hardware type,

60
00:04:47.120 --> 00:04:52.190
task level parallelism, infrastructure
replacement, or random data access.
WEBVTT

1
00:00:02.028 --> 00:00:04.748
YARN, The Resource Manager for Hadoop.

2
00:00:21.825 --> 00:00:27.260
YARN is a resource manage layer that
sits just above the storage layer HDFS.

3
00:00:28.510 --> 00:00:33.210
YARN interacts with applications and
schedules resources for their use.

4
00:00:34.390 --> 00:00:39.390
YARN enables running multiple
applications over HDFC increases

5
00:00:39.390 --> 00:00:44.160
resource efficiency and
let's you go beyond the map reduce or

6
00:00:44.160 --> 00:00:46.720
even beyond the data
parallel programming model.

7
00:00:48.910 --> 00:00:52.630
When Hadoop was first created,
this wasn't the case.

8
00:00:52.630 --> 00:00:57.560
In fact, the original Hadoop
stack had no resource manager.

9
00:00:57.560 --> 00:01:02.480
These two stacked diagrams, show, some of
it's evolution over the last ten years.

10
00:01:03.980 --> 00:01:07.580
One of the biggest limitations
of Hadoop one point zero,

11
00:01:07.580 --> 00:01:11.318
was it's inability to support
non-mapreduce applications.

12
00:01:11.318 --> 00:01:15.200
It had several terrible
resource utilization.

13
00:01:15.200 --> 00:01:19.770
This meant that for advanced
applications such as graph analysis that

14
00:01:19.770 --> 00:01:22.020
required different ways of modelling and

15
00:01:22.020 --> 00:01:26.950
looking at data, you would need to
move your data to another platform.

16
00:01:26.950 --> 00:01:29.310
That's a lot of work if your data is big.

17
00:01:31.540 --> 00:01:36.510
Adding YARN in between HDFS and
the applications enabled

18
00:01:36.510 --> 00:01:41.510
new systems to be built, focusing on
different types of big data applications

19
00:01:41.510 --> 00:01:46.280
such as Giraph for
graph data analysis, Storm for

20
00:01:46.280 --> 00:01:50.190
streaming data analysis, and
Spark for in-memory analysis.

21
00:01:51.360 --> 00:01:55.150
YARN does so
by providing a standard framework

22
00:01:55.150 --> 00:01:59.530
that supports customized application
development in the HADOOP ecosystem.

23
00:02:00.990 --> 00:02:04.850
YARN lets you extract maximum
benefits from your data sets

24
00:02:04.850 --> 00:02:09.320
by letting you use the tools you
think are best for your big data.

25
00:02:10.860 --> 00:02:14.810
Let's take a peek into the architecture
of YARN without getting too technical.

26
00:02:16.430 --> 00:02:21.620
In this picture, notice the resource
manager in the center, and

27
00:02:21.620 --> 00:02:25.810
the node managers on each of
the three nodes on the right.

28
00:02:27.910 --> 00:02:32.450
The resource manager controls all
the resources, and decides who gets what.

29
00:02:34.290 --> 00:02:39.580
Node manager operates at machine level and
is in charge of a single machine.

30
00:02:41.490 --> 00:02:44.180
Together the resource manager and

31
00:02:44.180 --> 00:02:47.240
the node manager form the data
computation framework.

32
00:02:48.410 --> 00:02:51.530
Each application gets
an application master.

33
00:02:52.560 --> 00:02:56.190
It negotiates resource from
the Resource Manager and

34
00:02:56.190 --> 00:02:59.640
it talks to Node Manager
to get its tasks completed.

35
00:03:01.820 --> 00:03:06.900
Notice the ovals labeled
Container The container

36
00:03:06.900 --> 00:03:12.240
is an abstract Notions that
signifies a resource that is

37
00:03:12.240 --> 00:03:16.760
a collection of CPU
memory disk network and

38
00:03:16.760 --> 00:03:21.680
other resources within
the compute note to simplify and

39
00:03:21.680 --> 00:03:26.020
be less precise you can think
of a container and the Machine.

40
00:03:27.440 --> 00:03:30.460
We looked at the essential
gears of the YARN

41
00:03:30.460 --> 00:03:34.880
engine to give you an idea of
the key components of YARN.

42
00:03:34.880 --> 00:03:38.580
Now when you hear terms like
Resource Manager, Node Manager and

43
00:03:38.580 --> 00:03:43.670
Container, you will have an understanding
of what tasks they are responsible for.

44
00:03:46.150 --> 00:03:52.400
Here is a real life example to show
the strength Hadoop 2.0 over 1.0.

45
00:03:52.400 --> 00:03:55.940
Yahoo was able to run almost

46
00:03:55.940 --> 00:03:59.910
twice as many jobs per day with
Yarn than with Hadoop 1.0.

47
00:03:59.910 --> 00:04:07.100
They also experienced a substantial
increase in CPU utilization.

48
00:04:07.100 --> 00:04:08.130
Yahoo!

49
00:04:08.130 --> 00:04:11.710
even claimed that upgrading
to YARN was equal into

50
00:04:11.710 --> 00:04:15.850
adding 1000 machines to
their 2500 machine cluster.

51
00:04:15.850 --> 00:04:16.480
That's big.

52
00:04:18.450 --> 00:04:22.790
YARN success is evident
from an explosive growth of

53
00:04:22.790 --> 00:04:25.940
different application that
the Hadoop ecosystem now has.

54
00:04:27.330 --> 00:04:28.660
New to yarn?

55
00:04:28.660 --> 00:04:32.750
You can use the tool of your choice
over your big data without any hassle.

56
00:04:33.920 --> 00:04:39.170
Compare this with Hadoop 1.0 which
was limited to MapReduce alone.

57
00:04:41.260 --> 00:04:45.630
Let's review a summary of
the key take-aways about yarn.

58
00:04:45.630 --> 00:04:50.540
Yarn gives you many ways for
applications to extract value from data.

59
00:04:51.720 --> 00:04:56.490
It lets you run many distributed
applications over the same Hadoop cluster.

60
00:04:57.860 --> 00:05:02.630
In addition, YARN reduces
the need to move data around and

61
00:05:02.630 --> 00:05:07.110
supports higher resource utilization
resulting in lower costs.

62
00:05:08.690 --> 00:05:13.310
It's a scalable platform that has
enabled growth of several applications

63
00:05:13.310 --> 00:05:17.490
over the HDFS,
enriching the Hadoop ecosystem.
WEBVTT

1
00:00:00.880 --> 00:00:06.100
At this point, you hopefully have a good
general idea of what big data means and

2
00:00:06.100 --> 00:00:07.380
why big data is important.

3
00:00:08.590 --> 00:00:11.190
So now, we need to focus on what to do

4
00:00:11.190 --> 00:00:13.090
when we have an application
that uses big data.

5
00:00:14.980 --> 00:00:18.780
In this video, we focus on
the problem of managing big data.

6
00:00:19.780 --> 00:00:20.970
So at the end of the video,

7
00:00:20.970 --> 00:00:25.390
you should be able to describe what data
management means in general, and then

8
00:00:25.390 --> 00:00:29.180
specifically recognize the issues that are
involved in the management of big data.

9
00:00:31.760 --> 00:00:34.620
First, let's see what data
management means in general.

10
00:00:35.910 --> 00:00:39.120
Instead of giving you
definitions of data management,

11
00:00:39.120 --> 00:00:41.760
let's think of some questions
that must be asked and

12
00:00:41.760 --> 00:00:44.880
answered well if you're to manage
a reasonable amount of data.

13
00:00:46.250 --> 00:00:49.350
Now, we can not possibly cover
all questions one should ask for

14
00:00:49.350 --> 00:00:54.020
a data-centric application, but here are
some important ones, which range from how

15
00:00:54.020 --> 00:00:59.290
we get the data, to how we work with it,
to how we secure it from malicious users.

16
00:01:00.300 --> 00:01:02.386
We'll visit these issues one at a time.
WEBVTT

1
00:00:01.280 --> 00:00:03.990
Ingestion means the process
of getting the data

2
00:00:03.990 --> 00:00:07.040
into the data system that
we are building or using.

3
00:00:07.040 --> 00:00:10.440
Now you might think,
why is it worth talking about?

4
00:00:11.600 --> 00:00:14.560
We'll just read the data from somewhere,
like a file.

5
00:00:14.560 --> 00:00:17.320
And then using some command,
place it into the data system.

6
00:00:18.550 --> 00:00:22.180
Or we'll have have some
kind of a web form or

7
00:00:22.180 --> 00:00:25.870
other visual interface and just fill it
in so that the data goes into the system.

8
00:00:27.500 --> 00:00:31.180
Both of these ways of
data ingestion are valid.

9
00:00:31.180 --> 00:00:32.650
In fact, they're valid for

10
00:00:32.650 --> 00:00:35.420
some big data systems like your
airline reservation system.

11
00:00:36.470 --> 00:00:39.850
However when you think
of a large scale system

12
00:00:39.850 --> 00:00:43.990
you wold like to have more automation
in the data ingestion processes.

13
00:00:43.990 --> 00:00:48.060
And data ingestion then becomes a part of
the big data management infrastructure.

14
00:00:49.200 --> 00:00:53.920
So here are some questions you might want
to ask when you automate data ingestion.

15
00:00:55.140 --> 00:00:56.830
Now take a minute to read the questions.

16
00:00:59.140 --> 00:01:03.350
We'll look at two examples to
explore them in greater detail.

17
00:01:04.660 --> 00:01:08.380
The first example is that of a hospital
information system that we discussed in

18
00:01:08.380 --> 00:01:11.600
course one in the context
of precision medicine.

19
00:01:11.600 --> 00:01:15.650
We said that hospitals collect
terabytes of medical record

20
00:01:15.650 --> 00:01:18.780
from different departments and
be considered big data systems.

21
00:01:20.450 --> 00:01:24.630
The second example is a cloud based data
store where many people upload their

22
00:01:24.630 --> 00:01:29.030
messages, chats, pictures,
videos, music and so fourth.

23
00:01:29.030 --> 00:01:33.100
The cloud storage also supports active
communication between the members and

24
00:01:33.100 --> 00:01:34.920
store their communication in real time.

25
00:01:36.650 --> 00:01:41.060
So let's think of a hypothetical
hospital information information and

26
00:01:41.060 --> 00:01:43.970
the answer to depressions
that we are putting there.

27
00:01:43.970 --> 00:01:45.750
Now, do not take the numbers
to be very accurate.

28
00:01:45.750 --> 00:01:46.630
They are just examples.

29
00:01:47.630 --> 00:01:50.330
But it illustrates some important points.

30
00:01:50.330 --> 00:01:56.260
One, note that there are two kinds
of likeness associated with data.

31
00:01:56.260 --> 00:02:00.410
Some data like medical images
are large data objects by themselves.

32
00:02:02.300 --> 00:02:06.880
Secondly, the records
themselves are quite small but

33
00:02:06.880 --> 00:02:09.740
the size of the total collection
of records is very high.

34
00:02:11.580 --> 00:02:16.546
Two, while there is a lot of patient data,
the number of data sources that is

35
00:02:16.546 --> 00:02:21.517
the different departmental systems
contributing to the total information

36
00:02:21.517 --> 00:02:24.408
system does not change
very much over time.

37
00:02:24.408 --> 00:02:29.739
Three, the rate of data ingestion is
not enormous and is often proportional

38
00:02:29.739 --> 00:02:34.668
to the number of patient activities
that takes place at the hospital.

39
00:02:34.668 --> 00:02:37.993
Four, the system contains
medical records so

40
00:02:37.993 --> 00:02:42.778
data can never be discarded even
when there are errors in the data.

41
00:02:42.778 --> 00:02:48.020
The errors in this specific case
are flagged but the data is retained.

42
00:02:49.590 --> 00:02:53.740
Now this is the kind of rule
called an error handling policy.

43
00:02:53.740 --> 00:02:56.120
Which might be different for
different application problems.

44
00:02:57.960 --> 00:03:03.180
An air handling policy is part of
a larger scheme of policies called

45
00:03:03.180 --> 00:03:04.050
ingestion policies.

46
00:03:06.530 --> 00:03:10.630
Another kind of ingestion policy involves
decisions regarding what the system should

47
00:03:10.630 --> 00:03:14.480
do if the data rate suddenly increases or
becomes suspiciously low.

48
00:03:15.540 --> 00:03:19.420
In this example we have deliberately
decided not to include it in the design.

49
00:03:20.630 --> 00:03:24.150
Now compare the previous case with
the case of the online store of

50
00:03:24.150 --> 00:03:26.000
personal information.

51
00:03:26.000 --> 00:03:28.800
Again this is just an imaginary example.

52
00:03:28.800 --> 00:03:31.270
So don't think of all
the parameters to be exact.

53
00:03:32.670 --> 00:03:37.390
Now in this case one, the store will
have a fast growing membership.

54
00:03:38.680 --> 00:03:42.370
Each member will use multiple devices
to capture and ingest their data.

55
00:03:44.060 --> 00:03:50.510
Two, over all members together, the site
will be updated at a really fast rate,

56
00:03:50.510 --> 00:03:56.182
making this a large volume data
store with a fast ingest rate.

57
00:03:56.182 --> 00:04:01.150
Three, in this system, our primary
challenge is to keep up with the data

58
00:04:01.150 --> 00:04:06.780
rate, and hence, erroneous data will be
discarded after just one edit to reinvest.

59
00:04:08.760 --> 00:04:13.380
Four, now there is an actual policy for
handling data overflow,

60
00:04:13.380 --> 00:04:17.720
which essentially says,
keep the excess data in a site store.

61
00:04:17.720 --> 00:04:20.560
And ingest them when the data
rate becomes slower.

62
00:04:20.560 --> 00:04:24.660
But if the site store starts getting full

63
00:04:24.660 --> 00:04:28.980
start dropping some incoming data
at a rate of 0.1% at a time.

64
00:04:30.470 --> 00:04:33.220
Now we should see why data
ingestion together with it's

65
00:04:33.220 --> 00:04:36.320
policies should be an integral
part of a big data system.

66
00:04:36.320 --> 00:04:39.390
Especially when it involves
storing fast data.
WEBVTT

1
00:00:00.770 --> 00:00:05.720
A very significant aspect of data
management is to document, define,

2
00:00:05.720 --> 00:00:09.580
implement, and test the set of
operations that are required for

3
00:00:09.580 --> 00:00:10.610
a specific application.

4
00:00:11.910 --> 00:00:16.570
As we'll see later in the specialization,
some operations are independent of

5
00:00:16.570 --> 00:00:21.770
the type of data and some others would
require us to know the nature of the data

6
00:00:21.770 --> 00:00:24.890
because the operations make use
of a particular data model.

7
00:00:24.890 --> 00:00:26.180
That is the way that it is structured.

8
00:00:27.760 --> 00:00:31.130
In general, there are two
broad divisions of operations.

9
00:00:32.620 --> 00:00:35.950
Those that work on a singular object and

10
00:00:35.950 --> 00:00:38.590
those that work on
collections of data objects.

11
00:00:40.020 --> 00:00:44.520
In the first case,
an operation that crops an image,

12
00:00:44.520 --> 00:00:49.120
that means extracts a sub
area from an area of pixels,

13
00:00:49.120 --> 00:00:53.280
is a single object operation because we
consider the image as a single object.

14
00:00:55.250 --> 00:00:58.860
One can think of many subclasses
of the second category

15
00:00:58.860 --> 00:01:01.790
where the operations
are on data collections.

16
00:01:01.790 --> 00:01:06.300
We briefly referred to three very
common operations that can be done

17
00:01:06.300 --> 00:01:07.640
regardless of the nature of the data.

18
00:01:08.820 --> 00:01:13.610
The first is to take a collection and
filter out a subset of that collection.

19
00:01:13.610 --> 00:01:17.180
The most obvious case is
selecting a subset from a set.

20
00:01:17.180 --> 00:01:21.010
In this example, we select circles
whose number is greater than three.

21
00:01:22.050 --> 00:01:26.830
A second case is merging two collections
together to form a larger collection.

22
00:01:28.220 --> 00:01:29.780
In the example shown,

23
00:01:29.780 --> 00:01:35.380
two three structure data items are merged
by fusing the node with a common property.

24
00:01:35.380 --> 00:01:35.880
That is two.

25
00:01:37.520 --> 00:01:41.650
In the last case,
we compute a function on a collection and

26
00:01:41.650 --> 00:01:43.530
return the value of the function.

27
00:01:43.530 --> 00:01:46.120
So in this example,
the function is a simple count.

28
00:01:47.450 --> 00:01:51.230
In the real world, this kind of aggregate
function can be very complicated.

29
00:01:52.540 --> 00:01:57.470
We will come back to this issue when
we talk more about map readings, but

30
00:01:57.470 --> 00:02:00.670
in this course, we'll talk about
many different data operations.

31
00:02:01.750 --> 00:02:04.580
Every operator must be efficient.

32
00:02:04.580 --> 00:02:09.550
That means every operator must
perform its task as fast as possible

33
00:02:09.550 --> 00:02:13.400
by taking up as little memory,
or our disk, as possible.

34
00:02:14.580 --> 00:02:17.170
Obviously, the time to
perform an operation

35
00:02:17.170 --> 00:02:20.490
will depend on the size of the input and
the size of the output.

36
00:02:21.590 --> 00:02:26.300
So, if there is an opportunity to use
concurrency where the operator can split

37
00:02:26.300 --> 00:02:30.880
its data and have different threads
operate on the pieces at the same time,

38
00:02:30.880 --> 00:02:32.080
it should definitely do so.

39
00:02:33.820 --> 00:02:37.120
We present a simple example of
an operator we saw on the previous slide.

40
00:02:38.290 --> 00:02:40.600
So this operator, called selection,

41
00:02:40.600 --> 00:02:45.390
refers to choosing a subset of
a set based on some conditions.

42
00:02:45.390 --> 00:02:49.120
Here we are choosing a subset of
circles whose numbers are even.

43
00:02:50.460 --> 00:02:55.080
To make it more efficient,
we can take the input data and

44
00:02:55.080 --> 00:02:59.150
partition it randomly into two groups.

45
00:02:59.150 --> 00:03:01.040
Now, for each group,

46
00:03:01.040 --> 00:03:05.850
we can concurrently run the subset
algorithm and get the partial results.

47
00:03:07.020 --> 00:03:10.880
For this operation, the partial results
can be directly sent to the output

48
00:03:10.880 --> 00:03:12.880
without any additional processing step.
WEBVTT

1
00:00:00.790 --> 00:00:04.740
Okay we in essence store
the data efficiently.

2
00:00:04.740 --> 00:00:05.670
But is it any good?

3
00:00:06.930 --> 00:00:10.970
Are there ways of knowing if the data is
potentially error free and useful for

4
00:00:10.970 --> 00:00:11.770
the intended purpose?

5
00:00:12.940 --> 00:00:15.500
This is the issue of data quality.

6
00:00:15.500 --> 00:00:18.300
There are many reasons
why any data application,

7
00:00:18.300 --> 00:00:22.470
especially larger applications need
to be mindful of data quality.

8
00:00:23.490 --> 00:00:26.650
Here are three reasons, of course
there are more that we do not mention.

9
00:00:27.730 --> 00:00:32.480
The first reason emphasizes that
the ultimate use of big data

10
00:00:32.480 --> 00:00:35.040
is its ability to give
us actionable insight.

11
00:00:36.130 --> 00:00:40.030
Poor quality data leads to poor
analysis and hence to poor decisions.

12
00:00:41.970 --> 00:00:46.820
The second related data in regulated
industries in areas like clinical

13
00:00:46.820 --> 00:00:50.710
trials for pharmaceutical companies or
financial data like from banks.

14
00:00:51.970 --> 00:00:56.440
Errors in data in these industries
can regulate regulations leading to

15
00:00:56.440 --> 00:00:57.290
legal complications.

16
00:00:58.980 --> 00:01:02.230
The third factor is different
than the first two.

17
00:01:02.230 --> 00:01:06.620
It says if your big data should
be used by other people or

18
00:01:06.620 --> 00:01:09.290
a third party software
it's very important for

19
00:01:09.290 --> 00:01:13.059
the data to give good quality to
gain trust as a leader provider.

20
00:01:14.260 --> 00:01:18.270
A class of big data applications
is scientific, where large,

21
00:01:18.270 --> 00:01:22.180
integrated collections of data
are created by human experts to

22
00:01:22.180 --> 00:01:24.570
understand scientific questions.

23
00:01:24.570 --> 00:01:28.960
Ensuring accuracy of data will lead
to correct human engagement and

24
00:01:28.960 --> 00:01:30.220
interaction with the data system.

25
00:01:32.370 --> 00:01:35.170
Gartner, the well known
technology research and

26
00:01:35.170 --> 00:01:39.720
advising company created a 2015
industry report on big data qualities.

27
00:01:41.300 --> 00:01:44.240
In this report,
they identify the approaches to meeting

28
00:01:44.240 --> 00:01:46.210
the data quality requirements
in the industry.

29
00:01:47.240 --> 00:01:51.690
This methods include the adherence
to standards where applicable.

30
00:01:51.690 --> 00:01:55.150
It also refers to the need to
create the rules in the data system

31
00:01:55.150 --> 00:01:59.870
that can be use to check if the data
passes a set of correct this qualities.

32
00:01:59.870 --> 00:02:02.950
Like is even employed above 18.

33
00:02:02.950 --> 00:02:07.490
It also includes methods to clean
the data if it's found to have errors or

34
00:02:07.490 --> 00:02:08.260
inconsistencies.

35
00:02:09.720 --> 00:02:14.772
Further the data quality management should
include a well define work flow on how

36
00:02:14.772 --> 00:02:19.690
low quality data could be corrected to
bring it back to a high level of quality.
WEBVTT

1
00:00:01.920 --> 00:00:03.770
There are many ways of
looking at scalability.

2
00:00:04.880 --> 00:00:07.600
And we'll consider them as
we go forward in the course.

3
00:00:08.930 --> 00:00:12.870
One way is to consider scaling up and
scaling out.

4
00:00:14.360 --> 00:00:18.300
Simply put,
it is a decision between making a machine

5
00:00:18.300 --> 00:00:23.264
that makes a server more powerful
versus adding more machines.

6
00:00:23.264 --> 00:00:27.240
The first choice will
involve adding more memory,

7
00:00:27.240 --> 00:00:31.020
replacing processes with
process of more course and

8
00:00:31.020 --> 00:00:35.060
adding more processes within a system with
a very fast internet connection speed.

9
00:00:36.360 --> 00:00:41.520
The second choice will involve adding more
machines to a relatively slower network.

10
00:00:42.880 --> 00:00:44.290
Now, there are no absolutes here.

11
00:00:45.560 --> 00:00:49.490
In many cases, we'll choose the former
to get more performance for

12
00:00:49.490 --> 00:00:50.740
large leader systems.

13
00:00:52.920 --> 00:00:54.760
The general trend in the big data world,

14
00:00:54.760 --> 00:00:57.590
however, is to target
the scale out option.

15
00:00:59.330 --> 00:01:01.950
Most big data management systems today

16
00:01:01.950 --> 00:01:05.140
are designed to operate over
a cluster up machines and

17
00:01:05.140 --> 00:01:10.490
have the ability to adjust as more
machines are added and when machines fail.

18
00:01:11.740 --> 00:01:16.060
Cluster management and management
of data operations over a cluster

19
00:01:16.060 --> 00:01:19.930
is an important component in today's
big data management systems.

20
00:01:22.110 --> 00:01:25.990
Now we'll briefly touch upon
the complex issue of data security.

21
00:01:27.310 --> 00:01:31.700
It is obvious that having more sensitive
data implies the need for more security.

22
00:01:32.960 --> 00:01:35.650
If the data is within
the walls of an organization,

23
00:01:35.650 --> 00:01:38.180
we'll still need a security plan.

24
00:01:38.180 --> 00:01:41.700
However, if a big data system
is deployed in the cloud,

25
00:01:41.700 --> 00:01:45.900
over multiple machines, security of
data becomes an even bigger challenge.

26
00:01:47.200 --> 00:01:50.800
So now we need to ensure the security for
not only the machines,

27
00:01:50.800 --> 00:01:54.880
but also the network which will be
heavily used during data transfer

28
00:01:54.880 --> 00:01:57.610
across different phases
of data operations.

29
00:01:57.610 --> 00:02:03.200
For example, if the data store and
the data analysis are performed over

30
00:02:03.200 --> 00:02:08.760
different sets of servers that
every analysis operation gets to

31
00:02:08.760 --> 00:02:13.630
an additional overhead of encrypting the
data as the data gets to the network and

32
00:02:13.630 --> 00:02:16.030
decrypting when it gets
to the processing server.

33
00:02:17.190 --> 00:02:19.880
This effectively increases
operational cost.

34
00:02:21.310 --> 00:02:26.050
As of today, while there are many
security products, the methods for

35
00:02:26.050 --> 00:02:31.290
ensuring security and achieving data
processing efficiency at the same time

36
00:02:31.290 --> 00:02:33.620
remains a research issue
in big data management.
WEBVTT

1
00:00:00.530 --> 00:00:05.150
Now the goal of a storage infrastructure,
obviously, is to store data.

2
00:00:05.150 --> 00:00:07.330
There are two storage related
issues we consider here.

3
00:00:09.590 --> 00:00:12.920
The first is the issue of capacity.

4
00:00:12.920 --> 00:00:15.110
How much storage should we allocate?

5
00:00:15.110 --> 00:00:18.990
That means, what should be the size
of the memory, how large and

6
00:00:18.990 --> 00:00:21.280
how many disk units should we have,
and so forth.

7
00:00:22.700 --> 00:00:26.760
There is also the issue of scalability.

8
00:00:26.760 --> 00:00:30.150
Should the storage devices be
attached directly to the computers

9
00:00:30.150 --> 00:00:33.690
to make the direct IO fast but
less scalable?

10
00:00:33.690 --> 00:00:36.310
Or should the storage be
attached to the network

11
00:00:37.340 --> 00:00:39.820
that connect the computers in the cluster?

12
00:00:39.820 --> 00:00:42.690
This will make disk
access a bit slower but

13
00:00:42.690 --> 00:00:45.210
allows one to add more
storage to the system easily.

14
00:00:46.230 --> 00:00:49.060
Now these questions do
not have a simple answer.

15
00:00:49.060 --> 00:00:52.200
If you're interested, you may look up
a website given on your reading list.

16
00:00:53.470 --> 00:00:58.080
A different class of questions deals
with the speed of the IU operation.

17
00:00:59.090 --> 00:01:03.620
This question is often addressed with
this kind of diagram here called a memory

18
00:01:03.620 --> 00:01:07.480
hierarchy, or storage hierarchy, or
sometimes memory storage hierarchy.

19
00:01:09.270 --> 00:01:13.740
The top of the pyramid structure shows
a part of memory called cache memory,

20
00:01:14.840 --> 00:01:17.410
that lives inside the CPU and
is very fast.

21
00:01:17.410 --> 00:01:21.650
There are different levels of cache,
called L1, L2, L3,

22
00:01:21.650 --> 00:01:27.229
where L3 is the slowest but
still faster than what we call memory,

23
00:01:27.229 --> 00:01:30.110
shown here in orange near the middle.

24
00:01:31.400 --> 00:01:33.990
The figure shows their speed
in terms of response times.

25
00:01:35.070 --> 00:01:38.410
Notice the memory streamed here
is 65 nanoseconds per access.

26
00:01:39.930 --> 00:01:40.892
In contrast,

27
00:01:40.892 --> 00:01:46.030
the speed of the traditional hard disk
is of the order of 10 milliseconds.

28
00:01:47.510 --> 00:01:51.918
This gap has prompted the design
of many data structures and

29
00:01:51.918 --> 00:01:56.768
algorithms.that use a hard disk but
tries to minimize the cost of

30
00:01:56.768 --> 00:02:01.455
the IO operations between the fast
memory and the slower disk.

31
00:02:01.455 --> 00:02:04.543
But more recently,
a newer kind of storage.

32
00:02:07.329 --> 00:02:09.463
Very similar to the flash drives or

33
00:02:09.463 --> 00:02:13.738
USBs that we regularly use have made
an entry as a new storage medium.

34
00:02:13.738 --> 00:02:17.820
These devices are called SSDs or
Solid State Devices.

35
00:02:17.820 --> 00:02:19.780
They are much faster
than spinning hard disks.

36
00:02:20.990 --> 00:02:24.795
An even newer addition is
the method called NVMe,

37
00:02:24.795 --> 00:02:27.787
NVM stands for non-volatile memory,

38
00:02:27.787 --> 00:02:32.600
that makes data transfer between SSDs and
memory much faster.

39
00:02:33.940 --> 00:02:38.317
What all this means in a big data system
is that now we have the choice of

40
00:02:38.317 --> 00:02:43.077
architecting a storage infrastructure
by choosing how much of each type of

41
00:02:43.077 --> 00:02:44.684
storage we need to have.

42
00:02:44.684 --> 00:02:49.842
In my own research with large amounts
of data, I have found that using SSDs

43
00:02:49.842 --> 00:02:55.597
speed up all look up operations in data by
at least a factor of ten over hard drives.

44
00:02:55.597 --> 00:02:59.057
Of course the flip side of
this is the cost factor.

45
00:02:59.057 --> 00:03:03.305
The components become increasingly more
expensive as we go from the lower layers

46
00:03:03.305 --> 00:03:05.187
of the pyramid to the upper layers.

47
00:03:05.187 --> 00:03:08.972
So ultimately, it becomes an issue
of cost-benefit tradeoff.
WEBVTT

1
00:00:00.025 --> 00:00:05.173
[SOUND] Let us consider
a real life application to

2
00:00:05.173 --> 00:00:11.360
demonstrate the utility and
the challenges of big data.

3
00:00:11.360 --> 00:00:14.540
Many industries naturally deal
with large amounts of data.

4
00:00:15.540 --> 00:00:19.585
For our discussion, we consider
an energy company that provides gas and

5
00:00:19.585 --> 00:00:22.248
electricity to its
consumers in an urban area.

6
00:00:24.230 --> 00:00:28.354
In this news report, you can see
that Commonwealth Edison, or Con Ed,

7
00:00:28.354 --> 00:00:30.829
the gas and electric provider of New York,

8
00:00:30.829 --> 00:00:35.440
decided to place smart meters
all through its jurisdictions.

9
00:00:35.440 --> 00:00:38.000
That comes to 4.7 million smart meters.

10
00:00:39.540 --> 00:00:45.150
Now smart meters are smart because aside
from measuring energy consumption,

11
00:00:45.150 --> 00:00:48.730
they have a two way communication
capability between the meter and

12
00:00:48.730 --> 00:00:51.800
the central system at the gas and
electric company.

13
00:00:51.800 --> 00:00:56.590
In other words, they generate real time
data from the meters to be stored and

14
00:00:56.590 --> 00:00:58.060
processed at the central facility.

15
00:00:59.710 --> 00:01:00.280
How much data?

16
00:01:01.610 --> 00:01:03.070
According to this report,

17
00:01:03.070 --> 00:01:06.680
the number of data received at
the center is 1.5 billion per day.

18
00:01:06.680 --> 00:01:13.480
So the system will not only consume
this data, but process it and

19
00:01:14.650 --> 00:01:20.750
produce output at 15-minute intervals,
and sometimes, 5 minute intervals.

20
00:01:20.750 --> 00:01:22.270
Let's do the math.

21
00:01:22.270 --> 00:01:24.650
That comes to ingesting and processing

22
00:01:26.620 --> 00:01:30.360
about 10.5 million data
points per 15 minutes.

23
00:01:32.150 --> 00:01:36.830
So what kind of computation must
take place within these 15 minutes?

24
00:01:36.830 --> 00:01:41.760
Well, one obvious computation is billing,
where one needs to compute who, especially

25
00:01:41.760 --> 00:01:46.290
in the commercial sector, who actually
owns the meter and should be billed?

26
00:01:47.490 --> 00:01:49.980
This requires combining the meter data

27
00:01:49.980 --> 00:01:53.640
with the data in the customer
database maintained by the company.

28
00:01:53.640 --> 00:01:57.390
But let's just consider
computation related to analytics.

29
00:01:57.390 --> 00:02:01.180
We can list at least four
different kinds of computations.

30
00:02:02.610 --> 00:02:07.980
The first is computing the consumption
pattern per user, not per meter.

31
00:02:07.980 --> 00:02:12.840
Per user, where the output is
a histogram of hourly usage.

32
00:02:12.840 --> 00:02:16.270
So the x axis of the histogram
is hourly intervals And

33
00:02:16.270 --> 00:02:19.380
the y-axis is a number of units consumed.

34
00:02:20.950 --> 00:02:25.310
This leads to the computed both daily and
over larger time periods.

35
00:02:25.310 --> 00:02:28.560
To determine the hourly requirements for
this consumer.

36
00:02:30.430 --> 00:02:33.970
The second computation relates
to estimating the effects of

37
00:02:33.970 --> 00:02:37.160
outdoor temperature on the electricity
consumption of each consumer.

38
00:02:38.380 --> 00:02:40.890
For those you who
are statistically inclined,

39
00:02:40.890 --> 00:02:45.860
this often involves fitting a piece-wise
linear progression model to the data.

40
00:02:48.000 --> 00:02:53.080
The third task is to extract the daily
consumption trends that occur

41
00:02:53.080 --> 00:02:55.470
regardless of the outdoor temperature.

42
00:02:55.470 --> 00:02:59.590
This is again a statistical computation,
and may require something like

43
00:02:59.590 --> 00:03:03.570
a periodic alter regression algorithm,
for time series theta.

44
00:03:03.570 --> 00:03:05.290
The algorithm is not that important there.

45
00:03:06.410 --> 00:03:11.360
What's more important is the ability to
make make good prediction has a direct

46
00:03:11.360 --> 00:03:15.690
economic impact because the company
needs to buy energy from others.

47
00:03:15.690 --> 00:03:20.280
For example, an under-prediction
implies they'll end up paying more for

48
00:03:20.280 --> 00:03:23.940
buying energy at the last moment to
meet the consumer's requirements.

49
00:03:25.870 --> 00:03:30.590
The fourth task is to find groups of
similar consumers based on their usage

50
00:03:30.590 --> 00:03:34.860
pattern so that the company can determine
how many distinct groups of customers

51
00:03:34.860 --> 00:03:39.330
there are and design targeted energy
saving campaigns for each group.

52
00:03:40.370 --> 00:03:45.730
This requires finding similarities
over large number of time series data,

53
00:03:45.730 --> 00:03:47.140
which is a complex computation.

54
00:03:48.430 --> 00:03:52.110
Regardless of the number and
complexity of computation required,

55
00:03:52.110 --> 00:03:57.220
the company's constrained by the fact that
it has only 15 minutes to process the data

56
00:03:57.220 --> 00:04:02.942
before the next and
computation has to be performed.

57
00:04:02.942 --> 00:04:06.550
That issue is not just
the bigness of the data, but

58
00:04:06.550 --> 00:04:09.610
the strip and
strings of the arrival to output time.

59
00:04:11.520 --> 00:04:18.150
The analytics has value, only if it can be
completed within the life-cycle deadline.

60
00:04:18.150 --> 00:04:22.220
So if we were to design a big data system
for such a company, you would need to

61
00:04:22.220 --> 00:04:26.730
understand how much are the computation
can be executed in parallel And

62
00:04:26.730 --> 00:04:31.180
how many machines with what kind of
capability are required to handle the data

63
00:04:31.180 --> 00:04:35.510
rate and the number and complexity of
the analytical computations needed?
WEBVTT

1
00:00:07.370 --> 00:00:08.820
Hi, my name is Chad Berkley.

2
00:00:08.820 --> 00:00:13.750
I'm the CTO of FlightStats, and I'm here
today to talk to you a little bit about

3
00:00:13.750 --> 00:00:17.740
our platform and
how we acquire and process data.

4
00:00:17.740 --> 00:00:21.320
But first of all, I'd like to start by
just kind of introducing the company and

5
00:00:21.320 --> 00:00:26.540
telling you a little bit
about what we're all about.

6
00:00:26.540 --> 00:00:30.940
So FlightStats is a data company and

7
00:00:30.940 --> 00:00:36.340
we basically are the leading provider
of global real-time flight status data.

8
00:00:36.340 --> 00:00:41.110
We pull in data from over 500 sources and
we aggregate that data back together, and

9
00:00:41.110 --> 00:00:46.140
we sell it out to our customers which our
other businesses as well as consumers.

10
00:00:47.290 --> 00:00:50.920
So just to give you a little bit
of information about the scope and

11
00:00:50.920 --> 00:00:52.460
scale of what we do.

12
00:00:52.460 --> 00:00:55.210
Like I said,
we have over 500 sources of data.

13
00:00:55.210 --> 00:01:00.300
And on a daily basis,
we process about 15 million flight events,

14
00:01:00.300 --> 00:01:04.380
those that includes landings,
arrivals, departures.

15
00:01:05.510 --> 00:01:10.600
Any time the status of the flight changes,
we got some sort of message on that.

16
00:01:10.600 --> 00:01:15.790
We process about 260 million aircraft
positions per day, so we have an extensive

17
00:01:15.790 --> 00:01:21.980
network that monitors graph positions for
realtime flight tracking applications.

18
00:01:21.980 --> 00:01:26.020
And we also handle about One million PNRs,
or passenger name records,

19
00:01:26.020 --> 00:01:31.590
which are the actual data type
of an itenerary any time you

20
00:01:31.590 --> 00:01:36.520
book travel, a PNR is created for
you, for your travel.

21
00:01:36.520 --> 00:01:41.770
And it includes all of the segments,
like air travel, ferries,

22
00:01:41.770 --> 00:01:46.110
hotels, taxis, Anyhting that
can be scheduled on your trip.

23
00:01:47.390 --> 00:01:49.650
And we basically take in all that data and

24
00:01:49.650 --> 00:01:54.080
we aggregate it together and
we sell it back out.

25
00:01:54.080 --> 00:01:57.930
And how most people kind of know us for
FlightStats.com, that's our

26
00:01:57.930 --> 00:02:02.260
consumer site for business where we
handle about 2 million daily requests.

27
00:02:02.260 --> 00:02:04.550
And we handle about 1
million mobile app requests.

28
00:02:05.790 --> 00:02:12.710
That our b to b side, we cert out a lot
of data by APIs and real time data feeds.

29
00:02:12.710 --> 00:02:16.480
People make about 15 million
API requests to us everyday.

30
00:02:16.480 --> 00:02:20.180
And we also send out about one and half
million flight and trip notifications.

31
00:02:20.180 --> 00:02:24.310
So if you get a push
notification to your phone,

32
00:02:24.310 --> 00:02:26.930
telling you that your flight is delayed or
on time.

33
00:02:26.930 --> 00:02:29.010
That possibly has come from us.

34
00:02:29.010 --> 00:02:34.530
So a little bit about how the data
flows through our company.

35
00:02:34.530 --> 00:02:37.820
We bring in all these
different types of data and

36
00:02:37.820 --> 00:02:41.260
our sources and it flows through
our data acquisition team.

37
00:02:41.260 --> 00:02:47.380
We have a team whose primary purpose
is to pull in all sorts of raw data,

38
00:02:47.380 --> 00:02:49.520
a very heterogeneous datasets.

39
00:02:49.520 --> 00:02:55.150
And process that into a normalized form.

40
00:02:55.150 --> 00:02:58.420
So if you kind of follow the blue arrow
in this diagram you can see that it goes

41
00:02:58.420 --> 00:03:00.840
through this raw data
channel through the data hub.

42
00:03:00.840 --> 00:03:03.660
Which the data hub is a central
component of our system that I'll talk

43
00:03:03.660 --> 00:03:06.000
a little bit more about in a second.

44
00:03:06.000 --> 00:03:09.960
So the blue data, the blue line is
raw data coming in from the source

45
00:03:09.960 --> 00:03:12.300
It goes through our data
acquisition system,

46
00:03:12.300 --> 00:03:16.360
it turns into that purple line
which is a normalized form.

47
00:03:16.360 --> 00:03:20.640
It then goes back through our data hub
again and into our processing engine.

48
00:03:20.640 --> 00:03:24.210
Our processing engine is really where
most of the business logic happens.

49
00:03:24.210 --> 00:03:27.210
The first thing we have to do
is we have to match any piece of

50
00:03:27.210 --> 00:03:30.390
flight information against
a flight that we know about and

51
00:03:30.390 --> 00:03:33.220
primarily the way we know about
flights is through schedules.

52
00:03:33.220 --> 00:03:38.560
So we import schedules on a daily
basis from one of our partner

53
00:03:38.560 --> 00:03:39.510
schedule providers.

54
00:03:40.750 --> 00:03:45.065
That data,
once it's matched is then processed and

55
00:03:45.065 --> 00:03:49.635
the processing basically looks at each
message, and tries to determine if

56
00:03:49.635 --> 00:03:52.825
we think that that message needs
to be passed on to consumers.

57
00:03:52.825 --> 00:03:56.940
So you know it looks at things like,
have we seen that message before.

58
00:03:56.940 --> 00:03:58.560
Or is it a duplicate?

59
00:03:58.560 --> 00:04:00.530
Is it from a data source that we trust?

60
00:04:01.530 --> 00:04:05.120
Are there other things going on
that we need to know about that

61
00:04:05.120 --> 00:04:08.300
may impact whether that message is true or
not?

62
00:04:08.300 --> 00:04:11.370
And once we decide that a message
should be passed through,

63
00:04:11.370 --> 00:04:13.020
if you follow the green line.

64
00:04:13.020 --> 00:04:15.860
It goes into our process
data channel on our hub, and

65
00:04:15.860 --> 00:04:18.400
it's then pushed out to
a couple different places.

66
00:04:18.400 --> 00:04:19.080
So first of all,

67
00:04:19.080 --> 00:04:23.510
it goes into our production database which
is where all of our real time data lives.

68
00:04:23.510 --> 00:04:27.750
That database serves data to our websites,

69
00:04:27.750 --> 00:04:32.230
to our mobile apps, and
a variety of other places.

70
00:04:32.230 --> 00:04:34.060
It also goes into our
data warehouse which,

71
00:04:34.060 --> 00:04:37.330
is where our analytics products use it.

72
00:04:37.330 --> 00:04:39.670
I'll talk a bit more
about that in a minute.

73
00:04:39.670 --> 00:04:43.910
And then, the stream of data actually
goes up to so many of our customers.

74
00:04:43.910 --> 00:04:45.490
We don't need a database on our side.

75
00:04:45.490 --> 00:04:47.650
They would rather build
a database on their side so

76
00:04:47.650 --> 00:04:51.490
we actually just stream all of
the processed data directly to them.

77
00:04:51.490 --> 00:04:54.930
They then host it within
their own systems.

78
00:04:57.600 --> 00:05:04.920
So a little bit more about the hub, the
hub is central to how we move data around.

79
00:05:04.920 --> 00:05:07.960
It's a technology that
we developed in-house.

80
00:05:07.960 --> 00:05:11.125
And it's an object storage based,
scalable, highly available,

81
00:05:11.125 --> 00:05:15.810
multi-channel data queuing and
eventing system.

82
00:05:15.810 --> 00:05:21.690
The object storage part is,
we use Amazon S3 to store this data.

83
00:05:21.690 --> 00:05:24.310
So it's an object storage system.

84
00:05:24.310 --> 00:05:26.600
It's scalable,
we can scale it horizontally or

85
00:05:26.600 --> 00:05:32.140
vertically depending on, but the what
type of data is flowing through it.

86
00:05:32.140 --> 00:05:33.250
It's highly available meaning,

87
00:05:33.250 --> 00:05:36.350
that we have multiple instances
of it in different data centers.

88
00:05:36.350 --> 00:05:39.410
So if one that goes down we can
easily pull another one up or

89
00:05:39.410 --> 00:05:42.490
it's we're going to cross
multiple instances.

90
00:05:42.490 --> 00:05:44.600
And then it's multi channel.

91
00:05:44.600 --> 00:05:49.030
So it's got a rest interface and
any surface can create

92
00:05:49.030 --> 00:05:53.760
a new channel within the system and
start posting data to it.

93
00:05:53.760 --> 00:05:58.260
That data is then queued based on
the time that it comes in, and

94
00:05:58.260 --> 00:06:01.490
other services can be listening for
events on those channels.

95
00:06:01.490 --> 00:06:05.240
So, as soon as a new piece of data
comes into one of those channels,

96
00:06:05.240 --> 00:06:09.330
any service that's listening on that
channel, gets an event notification.

97
00:06:09.330 --> 00:06:12.470
They, that service can then
act upon that piece of data.

98
00:06:12.470 --> 00:06:16.700
And do whatever processing
it may need to do.

99
00:06:16.700 --> 00:06:21.690
This project is open source and
anybody can download it and use it.

100
00:06:24.490 --> 00:06:28.230
So a little bit about some of the data
that we collect and aggregate.

101
00:06:28.230 --> 00:06:33.560
And FLIFO is kind of the industry term for
flight information.

102
00:06:33.560 --> 00:06:39.360
And primarily we look at kind of
the five different parts of flight.

103
00:06:39.360 --> 00:06:43.870
So we pull in information on gate
departure, and then that becomes a runway

104
00:06:43.870 --> 00:06:47.855
departure, basically when the wheels
go up, that is a runway departure.

105
00:06:47.855 --> 00:06:54.080
We do in-flight positional tracking,
so when your flight is moving along,

106
00:06:54.080 --> 00:06:58.330
about once every ten seconds we get
notified of its latitude and longitude.

107
00:06:58.330 --> 00:07:00.530
And its heading and its speed and

108
00:07:00.530 --> 00:07:04.790
its vertical center descent rate and
several other variables.

109
00:07:05.960 --> 00:07:07.230
Then once it lands,

110
00:07:07.230 --> 00:07:11.300
as soon as the wheels touch the ground,
we're notified of a runway arrival and

111
00:07:11.300 --> 00:07:15.890
when the door is opened at the gate,
we have gate arrival information.

112
00:07:15.890 --> 00:07:21.480
All five of these data fields
come in three different forms.

113
00:07:21.480 --> 00:07:24.460
So we have a scheduled,
scheduled departure and arrival.

114
00:07:24.460 --> 00:07:29.310
We have estimated departure and arrival
which can come from a variety of sources,

115
00:07:29.310 --> 00:07:33.710
either airlines, airports,
positional data, et cetera.

116
00:07:33.710 --> 00:07:38.120
And then, we have actuals so
if we have an airport or

117
00:07:38.120 --> 00:07:42.460
an airline that's sending us data about
exactly when the wheels touch down, or

118
00:07:42.460 --> 00:07:46.770
exactly when that door opens on that
aircraft, we push that data as well.

119
00:07:46.770 --> 00:07:51.100
We also generate some
data at flight stops.

120
00:07:51.100 --> 00:07:56.850
So special incidents, if an aircraft
has an issue It's in the news.

121
00:07:56.850 --> 00:08:01.920
We do flag our content with
a message from our support staff.

122
00:08:01.920 --> 00:08:04.790
We do some prediction.

123
00:08:04.790 --> 00:08:08.030
Right now we're just starting to get into
that market, or we're actually trying to

124
00:08:08.030 --> 00:08:13.490
predict 24 hours out whether a flight
will be delayed, disrupted or on time.

125
00:08:14.500 --> 00:08:19.550
We do some synthetic positions,
so over oceans, primarily.

126
00:08:19.550 --> 00:08:24.310
We don't get tracking data
on aircraft over the oceans.

127
00:08:24.310 --> 00:08:28.480
There is currently no satellite-based
tracking system for aircraft.

128
00:08:28.480 --> 00:08:34.010
So we basically take the last
known position a heading, a speed.

129
00:08:34.010 --> 00:08:38.330
And if we have a flight plan, we'll use
a flight plan to synthesize the positions

130
00:08:38.330 --> 00:08:42.140
when we're not getting actual
positions over large bodies of water.

131
00:08:43.320 --> 00:08:47.560
We also generate notifications,
so the push alerts,

132
00:08:47.560 --> 00:08:52.680
the preflight emails,
delay notifications those types of things.

133
00:08:52.680 --> 00:08:56.570
We create those based on what we see
in the data that's coming in to us.

134
00:08:58.970 --> 00:09:02.630
We store all of our historical
data in a data warehouse,

135
00:09:02.630 --> 00:09:05.860
so right now we have six years
of historical flight data.

136
00:09:05.860 --> 00:09:08.080
And that powers our analytics products,

137
00:09:08.080 --> 00:09:12.570
so we allow airlines to do competitive
analysis, and route analysis.

138
00:09:13.690 --> 00:09:15.320
Routes are very important to airlines.

139
00:09:15.320 --> 00:09:17.220
That's how they compete
with each other and

140
00:09:17.220 --> 00:09:22.800
that's primarily how they
are judged by the FAA and

141
00:09:22.800 --> 00:09:27.429
other governmental organizations
on whether they're on-time or not.

142
00:09:27.429 --> 00:09:32.070
We also do airport operations analysis
things like taxi in and taxi out times.

143
00:09:32.070 --> 00:09:35.910
Very important for lots of airports,
runway utilization,

144
00:09:35.910 --> 00:09:40.230
hourly passenger flows through airports,
that type of information.

145
00:09:40.230 --> 00:09:42.830
And we do on-time performance metrics so.

146
00:09:42.830 --> 00:09:45.290
Airlines can look at how they're doing.

147
00:09:45.290 --> 00:09:47.020
How many flights did they complete?

148
00:09:47.020 --> 00:09:49.870
How many flights were on
time within 14 minutes?

149
00:09:51.640 --> 00:09:54.630
And they can compare themselves
to their competitors.

150
00:09:56.890 --> 00:10:01.310
So we host all of this in
a hybrid cloud architecture.

151
00:10:01.310 --> 00:10:05.560
Hybrid cloud basically means that we have
our own private datacenter resources and

152
00:10:05.560 --> 00:10:10.120
we also host resources in
the Amazon Web Services cloud.

153
00:10:11.870 --> 00:10:15.680
Most of our core data processing and
service layer is in our private data

154
00:10:15.680 --> 00:10:19.250
center and we're getting ready to spin
up a second private data center as well.

155
00:10:19.250 --> 00:10:24.270
Right now, our main data center is in
Portland, Oregon, and we're going to spin

156
00:10:24.270 --> 00:10:29.499
another one up on the east coast of
the United States probably in Q2 or Q3.

157
00:10:31.210 --> 00:10:35.050
For our API's we try to keep
those close to our customers so

158
00:10:35.050 --> 00:10:38.810
API end points and
web end points live in Amazon.

159
00:10:38.810 --> 00:10:44.170
And they are automatically routed to
whichever end point is closest to you,

160
00:10:44.170 --> 00:10:45.640
you will automatically be routed to them.

161
00:10:46.720 --> 00:10:50.700
All of our private infrastructure
is virtualized with VMWare.

162
00:10:50.700 --> 00:10:54.560
We pretty much have a fully
virtualized environment.

163
00:10:57.530 --> 00:11:04.870
And we're an Agile shop, so
we have six small, fast teams.

164
00:11:04.870 --> 00:11:09.300
Those are product centric teams, we
allow them to be as customer interactive

165
00:11:09.300 --> 00:11:12.610
as they need to be, and
we try to make our teams semi autonomous.

166
00:11:12.610 --> 00:11:16.300
So, teams get to choose their own tools,
they get to choose their

167
00:11:16.300 --> 00:11:21.340
own development methodologies,
they choose a variety of things.

168
00:11:21.340 --> 00:11:24.020
And We physically allow them

169
00:11:24.020 --> 00:11:26.770
to do what they need to do to get
their job done as quickly as possible.

170
00:11:28.030 --> 00:11:30.010
We try to automate everything.

171
00:11:30.010 --> 00:11:33.120
You do something once manually and
then the next time you write a script or

172
00:11:33.120 --> 00:11:34.480
program to do it.

173
00:11:34.480 --> 00:11:35.700
And we also measure everything.

174
00:11:35.700 --> 00:11:39.190
Right now, we're taking in about
2.5 billion metrics per month off

175
00:11:39.190 --> 00:11:40.480
of our systems.

176
00:11:40.480 --> 00:11:44.680
And we use those metrics to monitor
our application performance,

177
00:11:44.680 --> 00:11:48.890
to monitor revenue, to monitor pretty
much everything we do in the company.

178
00:11:48.890 --> 00:11:51.390
We really try to enable
total system awareness,

179
00:11:51.390 --> 00:11:56.200
everything from the hardware layer
up to the website is monitored.

180
00:11:57.240 --> 00:12:01.190
And we use industry best practices and
tools and of course, we try to recruit and

181
00:12:01.190 --> 00:12:05.740
hire the best talent possible a little
bit about our software stock.

182
00:12:05.740 --> 00:12:07.140
We're primarily a Java shop.

183
00:12:08.220 --> 00:12:11.830
Our core processing services
are all written in Java.

184
00:12:11.830 --> 00:12:15.870
We do use node JS in our
Microservice Edge layer.

185
00:12:15.870 --> 00:12:20.090
And node JS is actually starting to move
more down into the processing service

186
00:12:20.090 --> 00:12:21.750
layer as well.

187
00:12:21.750 --> 00:12:23.760
We use many different types of data bases.

188
00:12:23.760 --> 00:12:27.220
Our primary realtime
database is Post Press.

189
00:12:27.220 --> 00:12:31.140
And we use Mongo for
the backend of our API services.

190
00:12:32.280 --> 00:12:36.940
On the website, we're all HTML5 and
we're moving to React and Redux, and

191
00:12:36.940 --> 00:12:41.750
we're making use of Elasticsearch for
quick searching and indexing on our data.

192
00:12:41.750 --> 00:12:45.750
And, of course, we have iOS and
Android mobile applications.

193
00:12:45.750 --> 00:12:51.610
So you can find out more about
Flightstats on our website.

194
00:12:51.610 --> 00:12:53.950
If you need data for your applications,

195
00:12:53.950 --> 00:12:57.860
please go to the developer center
at developer.flightstats.com.

196
00:12:57.860 --> 00:13:00.790
You can sign up for
a free test account and

197
00:13:00.790 --> 00:13:03.980
be able to pull data
directly off of our APIs.

198
00:13:03.980 --> 00:13:06.660
If you're interested in the Hub,
like I said that's open source.

199
00:13:06.660 --> 00:13:10.540
Please check out the Git Hub page and
if you have any additional questions,

200
00:13:10.540 --> 00:13:14.270
feel free to contact
myself John Berkeley and

201
00:13:14.270 --> 00:13:18.360
I'd be happy to answer any
of your questions via email.

202
00:13:18.360 --> 00:13:20.690
Thanks for listening today and
hope you have a great day.

203
00:13:20.690 --> 00:13:21.190
Bye.
WEBVTT

1
00:00:10.512 --> 00:00:14.240
Different data sources in the game
industry include using your finger.

2
00:00:14.240 --> 00:00:16.408
What type of device is coming from?

3
00:00:16.408 --> 00:00:18.600
The amount of headsets.

4
00:00:18.600 --> 00:00:19.953
It's pretty much infinite,

5
00:00:19.953 --> 00:00:22.720
as far as the number of ways we
can bring data in from the game.

6
00:00:22.720 --> 00:00:27.602
And it could be joystick or mouse,
keyboards, there's lots of ways

7
00:00:27.602 --> 00:00:31.726
as well as what happens inside
the game itself as far cars or

8
00:00:31.726 --> 00:00:35.537
the driving, tires,
flying machines, anything.

9
00:00:42.179 --> 00:00:45.479
The volume of data,
it really depends on the type of game and

10
00:00:45.479 --> 00:00:47.592
how often they want to send the data in.

11
00:00:47.592 --> 00:00:51.260
How many types of events they've tagged
and how many users are playing the game.

12
00:00:51.260 --> 00:00:54.891
So if you have a user, you've 5 million
users that are playing your game and

13
00:00:54.891 --> 00:00:57.844
you're tapping and
you're tracking each tap of the screen,

14
00:00:57.844 --> 00:01:00.963
of where they went or each click of
the mouse as they were using it,

15
00:01:00.963 --> 00:01:02.982
you're going to get
a lot of volume of data.

16
00:01:02.982 --> 00:01:07.854
And so
you need to be prepared to bring in a lot

17
00:01:07.854 --> 00:01:13.140
of different data very, very quickly.

18
00:01:13.140 --> 00:01:16.322
As far as the variety of data, it depends.

19
00:01:16.322 --> 00:01:18.520
You have round pizzas and
you have round tires.

20
00:01:18.520 --> 00:01:19.513
I mean, they're completely different.

21
00:01:19.513 --> 00:01:23.816
They're both round, but there's different
ways that you're going to want to know how

22
00:01:23.816 --> 00:01:27.104
many pepperoni are on one pizza and
how many lug nuts go on a tire.

23
00:01:27.104 --> 00:01:29.510
So the variety is really unlimited,
as well.

24
00:01:29.510 --> 00:01:31.122
It really just depends on each game.

25
00:01:31.122 --> 00:01:34.430
So, you have to be prepared to
bring in all kinds of data.

26
00:01:34.430 --> 00:01:38.851
Touch data, wheel data,
track speeds, anything.

27
00:01:38.851 --> 00:01:43.628
And if you put taxonomy together that
you can define as an example of verb,

28
00:01:43.628 --> 00:01:49.021
object, location, value and any number
of other sources, then you can basically

29
00:01:49.021 --> 00:01:54.290
track anything you want as long as they
all fall into the same kind of buckets.

30
00:01:54.290 --> 00:01:56.705
And the buckets can be different
sizes based on the type of events.

31
00:01:56.705 --> 00:01:59.804
They don't all need to be 4,
some can be 2, some can be 20,

32
00:01:59.804 --> 00:02:01.218
it doesn't really matter.

33
00:02:08.280 --> 00:02:12.384
The modeling challenge has really come
down to who designs the structure at which

34
00:02:12.384 --> 00:02:15.355
you store your data and
how you want to retrieve that data.

35
00:02:15.355 --> 00:02:19.488
Those kind of of storage and
retrieval models are very, very important,

36
00:02:19.488 --> 00:02:22.098
because what it really
comes down to is speed.

37
00:02:22.098 --> 00:02:25.119
You can record a lot of data and
it can take you five years to query it,

38
00:02:25.119 --> 00:02:26.710
it doesn't really do you any good.

39
00:02:26.710 --> 00:02:31.347
So you need to make sure you plan for
reporting speed, because that's ultimately

40
00:02:31.347 --> 00:02:35.865
what within the organisation needs is
the ability to report on it very quickly.

41
00:02:35.865 --> 00:02:40.781
The management challenges really come
down to trying to figure out what data to

42
00:02:40.781 --> 00:02:41.306
store.

43
00:02:41.306 --> 00:02:44.657
A lot of times we go into various
companies and you've got producers sitting

44
00:02:44.657 --> 00:02:47.810
across the hall from designers, and
they don't even know each other.

45
00:02:47.810 --> 00:02:50.823
They don't realize what they want and
the programmer says,

46
00:02:50.823 --> 00:02:54.606
I'm going to put these events in and the
product manager who wants to figure out

47
00:02:54.606 --> 00:02:58.118
how many times somebody crashed says,
well, I need these events in.

48
00:02:58.118 --> 00:03:02.436
So unless they're communicating, you're
going to get the wrong type of data.

49
00:03:02.436 --> 00:03:06.241
So the management challenge is trying
to make sure everybody communicates,

50
00:03:06.241 --> 00:03:08.639
they decide on the taxonomy and
the structure and

51
00:03:08.639 --> 00:03:12.350
then we can go forward with tagging and
getting in the entire game working.

52
00:03:18.226 --> 00:03:20.469
We process, stayed in two main ways.

53
00:03:20.469 --> 00:03:22.107
One is streaming data.

54
00:03:22.107 --> 00:03:24.667
One is batched or scheduled data.

55
00:03:24.667 --> 00:03:28.653
Streaming data has scripts that run
instantly the minute the data arrives.

56
00:03:28.653 --> 00:03:31.222
And so as the data come in,
it gets processed and

57
00:03:31.222 --> 00:03:33.160
then stored In a reporting format.

58
00:03:33.160 --> 00:03:39.347
So they can easily generate reports
up to the second very, very quickly.

59
00:03:39.347 --> 00:03:42.671
Batch processing data really depends on
the type of data where it's coming from.

60
00:03:42.671 --> 00:03:46.689
Most of the time when we download
data from iTunes or YouTube or

61
00:03:46.689 --> 00:03:50.885
something like that, it comes in a CSV or
a very similar format.

62
00:03:50.885 --> 00:03:53.038
There's not really a lot of
processing we need to do.

63
00:03:53.038 --> 00:03:55.451
It's more of ingesting that data.

64
00:03:55.451 --> 00:03:59.788
There are processes we need to run
with some type of batch data, but

65
00:03:59.788 --> 00:04:03.040
most of the batch data we
receive comes in as a CSV or

66
00:04:03.040 --> 00:04:05.914
other similar already processed formats.

67
00:04:05.914 --> 00:04:10.228
So typically, while you can do processing
in both modes most processing typically

68
00:04:10.228 --> 00:04:14.497
happens with the streaming real-time data
than it does with offline batch data.

69
00:04:20.394 --> 00:04:26.260
We actually didn't use any
technology in the a big data space.

70
00:04:26.260 --> 00:04:28.684
We created our own, from scratch.

71
00:04:28.684 --> 00:04:32.280
What we did was we decided
what kind of model we wanted.

72
00:04:32.280 --> 00:04:33.475
How we were going to store data?

73
00:04:33.475 --> 00:04:35.112
How we were going to retrieve data?

74
00:04:35.112 --> 00:04:37.574
And ultimately,
how we were going to reduce the data?

75
00:04:37.574 --> 00:04:40.772
Because the more and more and more data
you have, the slower it is to actually do

76
00:04:40.772 --> 00:04:43.692
a query, because you have to look through
all the different pieces of data.

77
00:04:43.692 --> 00:04:47.148
A lot of databases solve this problem for
you, but they were really doing it in more

78
00:04:47.148 --> 00:04:50.170
generic way were we needed
something very specific.

79
00:04:50.170 --> 00:04:55.020
So we started from scratch building our
own data storage and retrieval, and

80
00:04:55.020 --> 00:04:57.003
reporting from the ground up.

81
00:04:57.003 --> 00:05:01.097
When it came to scalability,
it was really about designing the parts of

82
00:05:01.097 --> 00:05:03.906
the system that could be
independently scaled.

83
00:05:03.906 --> 00:05:07.773
So if data's coming in in real-time,
we send that into what we call a gateway.

84
00:05:07.773 --> 00:05:10.155
And the gateway could be three gateways,
let's say.

85
00:05:10.155 --> 00:05:11.922
But if the data starts
getting over loaded,

86
00:05:11.922 --> 00:05:13.362
I just have to add another gateway.

87
00:05:13.362 --> 00:05:16.383
And a gateway is just this little
light layer that just receives data,

88
00:05:16.383 --> 00:05:19.605
passes it on and goes back to it's job,
doesn't do anything else.

89
00:05:19.605 --> 00:05:22.296
So it can receive a lot
of data very quickly, but

90
00:05:22.296 --> 00:05:24.124
also I can just add another one.

91
00:05:24.124 --> 00:05:27.309
And it just automatically logs in and
adds itself to a list and

92
00:05:27.309 --> 00:05:30.813
now the data is being distributed
amongst four gateways instead.

93
00:05:30.813 --> 00:05:32.043
Query engine is the same way.

94
00:05:32.043 --> 00:05:34.720
When you're doing queries to try
to get data out of the system, so

95
00:05:34.720 --> 00:05:35.704
you can build reports.

96
00:05:35.704 --> 00:05:39.151
If I need more query engines,
because people are doing more reporting,

97
00:05:39.151 --> 00:05:40.733
we can add more query engines.

98
00:05:40.733 --> 00:05:44.722
So, the idea behind scalability is
trying to break the services up into

99
00:05:44.722 --> 00:05:47.390
the type of services that
they most make sense.

100
00:05:47.390 --> 00:05:48.466
So that if you need to,

101
00:05:48.466 --> 00:05:51.820
you can add just the service without
rebuilding the entire platform.

102
00:05:58.367 --> 00:06:01.825
My advice for people designing systems for
big data is to first,

103
00:06:01.825 --> 00:06:04.398
try to understand what
you want to accomplish.

104
00:06:04.398 --> 00:06:05.763
What's the goal?

105
00:06:05.763 --> 00:06:08.695
I mean, we're going to ingest everything
and we're going to report on everything.

106
00:06:08.695 --> 00:06:15.518
It's not really something that you can
achieve without some special thought.

107
00:06:15.518 --> 00:06:18.790
If you're going to focus on,
your area's going to be say gardening,

108
00:06:18.790 --> 00:06:22.419
then look at what kind of things you're
going to do in the gardening area and

109
00:06:22.419 --> 00:06:24.880
try to focus on what that
type of data is going to be.

110
00:06:24.880 --> 00:06:27.722
This isn't going to restrict you
to only being a gardener, but

111
00:06:27.722 --> 00:06:30.507
it is going to give you focus
on how to design your system, so

112
00:06:30.507 --> 00:06:32.701
that they're actually going to work for
you.

113
00:06:32.701 --> 00:06:36.912
You're going to continually evolve your
systems, add more things to them and

114
00:06:36.912 --> 00:06:37.580
grow them.

115
00:06:37.580 --> 00:06:40.804
I wouldn't suggest starting with
an unlimited variety of options and

116
00:06:40.804 --> 00:06:42.944
hoping you're going to
solve all the problems.

117
00:06:42.944 --> 00:06:49.185
Start with the goal of what your current
solution is and expand from there.

118
00:06:55.600 --> 00:06:58.971
Data can be fun, but
it can also be overwhelming.

119
00:06:58.971 --> 00:07:04.051
So, try to keep the data in mind
without keeping the world in mind and

120
00:07:04.051 --> 00:07:06.291
I think you'll be just fine.
WEBVTT

1
00:00:01.860 --> 00:00:06.740
In this video we will provide a quick
summary of the main points from our

2
00:00:06.740 --> 00:00:08.960
first course on introduction to big data.

3
00:00:10.310 --> 00:00:12.930
If you have just completed
our first course and

4
00:00:12.930 --> 00:00:16.810
do not need a refresher,
you may now skip to the next lecture.

5
00:00:18.480 --> 00:00:23.000
After this video,
you will be able to recall

6
00:00:23.000 --> 00:00:27.480
what started the big data era and
the three main big data sources.

7
00:00:29.310 --> 00:00:32.830
Summarize the volume,
variety, velocity and

8
00:00:32.830 --> 00:00:35.350
veracity issues related to each source.

9
00:00:36.790 --> 00:00:42.140
Explain the five step data science
process to gain value from big data.

10
00:00:43.510 --> 00:00:46.680
Remember the main elements
of the Hadoop Stack.

11
00:00:49.670 --> 00:00:54.950
We began our first course with
an explanation of how a lead torrent of

12
00:00:54.950 --> 00:01:00.690
big data combined with cloud computing
capabilities to process data anytime and

13
00:01:00.690 --> 00:01:05.450
anywhere has been at the core of
the launch of the Big Data Era.

14
00:01:08.470 --> 00:01:13.240
This big torrent of big data is often
boil down to a few varieties of data

15
00:01:13.240 --> 00:01:18.282
generated by machines, people and

16
00:01:18.282 --> 00:01:23.810
organizations with machine generated data.

17
00:01:23.810 --> 00:01:28.650
We refer to the data generated from real
time sensors and industrial machinery or

18
00:01:28.650 --> 00:01:30.020
vehicles.

19
00:01:30.020 --> 00:01:33.350
Web logs that track user behavior online.

20
00:01:33.350 --> 00:01:35.470
environmental sensors,

21
00:01:35.470 --> 00:01:38.680
personal health trackers among
many other sense data sources.

22
00:01:40.220 --> 00:01:46.050
With human generated data, we really refer
to the vast amount of social media data,

23
00:01:46.050 --> 00:01:49.760
status updates, tweets, photos and videos.

24
00:01:51.260 --> 00:01:56.600
With organization generated data,
we refer to more traditional types of data

25
00:01:56.600 --> 00:01:59.770
including transaction
information data bases and

26
00:01:59.770 --> 00:02:02.740
structure data often
stored in data warehouses.

27
00:02:03.960 --> 00:02:10.180
Note that big data can be structured,
semi-structured, and unstructured.

28
00:02:10.180 --> 00:02:15.030
Which is a topic we will talk about
more and in depth later in this course.

29
00:02:18.775 --> 00:02:23.975
Whatever your big data application is and
the types of big data you're using,

30
00:02:23.975 --> 00:02:29.905
the real value will come from integrating
different types of data sources and

31
00:02:29.905 --> 00:02:31.755
analyzing them at scale.

32
00:02:33.225 --> 00:02:36.355
Overall, by modeling, managing and

33
00:02:36.355 --> 00:02:40.700
integrating diverse streams
to improve our business and

34
00:02:40.700 --> 00:02:44.880
add value to our big data even
before we start analyzing it.

35
00:02:45.910 --> 00:02:47.591
As a part of modeling and

36
00:02:47.591 --> 00:02:52.804
managing big data is focusing on
the dimensions of scale availability and

37
00:02:52.804 --> 00:02:58.959
considering the challenges associated with
this dimensions to pick the right tools.

38
00:03:02.138 --> 00:03:07.460
Volume, variety and
velocity are the main dimensions which

39
00:03:07.460 --> 00:03:12.480
we characterized big data and
describe its challenges.

40
00:03:13.710 --> 00:03:18.550
We have huge amounts of data
in different formats and

41
00:03:18.550 --> 00:03:21.940
varying quality which
must be processed quickly

42
00:03:24.750 --> 00:03:30.810
veracity refers to the biases,
noise, and abnormality in data,

43
00:03:30.810 --> 00:03:36.190
or the unmeasurable certainty is in the
truthfulness and trustworthiness of data,

44
00:03:37.270 --> 00:03:41.470
and valence refers to
the connectedness of big data.

45
00:03:41.470 --> 00:03:43.790
Such as in the form of graph networks.

46
00:03:46.340 --> 00:03:52.500
Each V presents a challenging
dimension of big data mainly of size,

47
00:03:52.500 --> 00:03:57.160
complexity, speed, quality,
and consecutiveness.

48
00:03:57.160 --> 00:04:00.910
Although we can list some
other v' based on the context.

49
00:04:00.910 --> 00:04:05.390
We prefer to list these five as
fundamental dimensions which

50
00:04:05.390 --> 00:04:08.350
this big data specialization
helps you work on.

51
00:04:09.570 --> 00:04:15.130
Moreover, we must be sure to
never forget the sixth V: Value,

52
00:04:15.130 --> 00:04:18.220
at the heart of the big
data challenge is turning

53
00:04:18.220 --> 00:04:21.590
all of the other dimensions into
truly useful business value.

54
00:04:22.760 --> 00:04:26.490
How will Big Data benefit you and
your organization?

55
00:04:26.490 --> 00:04:30.960
The idea behind processing all
this Big Data in the first place

56
00:04:30.960 --> 00:04:33.100
is to bring value to the problem at hand.

57
00:04:34.360 --> 00:04:38.310
We need to take steps into
Big Data engineering and

58
00:04:38.310 --> 00:04:42.030
scalable data science to
generate value out of Big Data.

59
00:04:43.910 --> 00:04:44.900
We have all heard it.

60
00:04:44.900 --> 00:04:50.370
Data signs turns big data into insides,
or even actions.

61
00:04:51.490 --> 00:04:52.940
But what does that really mean?

62
00:04:54.340 --> 00:04:59.180
Data signs can be taught of as
the basis for empirical research.

63
00:04:59.180 --> 00:05:02.940
Like data is used to induce
information on the observations.

64
00:05:04.120 --> 00:05:06.830
These observations are mainly data.

65
00:05:06.830 --> 00:05:12.280
In our case, big data related to
a business or scientific use case.

66
00:05:14.550 --> 00:05:20.100
Inside is a term we use to refer to
the data products of data science.

67
00:05:20.100 --> 00:05:23.450
It is extracted from
a diverse amount of data

68
00:05:23.450 --> 00:05:27.620
through a combination of exploratory
data analysis and modeling.

69
00:05:28.830 --> 00:05:33.560
The questions are sometimes less
specific and it can require looking

70
00:05:33.560 --> 00:05:38.140
carefully at the data for patterns in
it to come up with a specific question.

71
00:05:40.400 --> 00:05:45.470
Another important point to recognize
is that data science is not static

72
00:05:45.470 --> 00:05:47.450
one time analysis.

73
00:05:47.450 --> 00:05:52.990
It involves a process where models
where you generate give us insights

74
00:05:52.990 --> 00:05:57.155
are constantly improve to a further and
prequel evidence and iterations.
WEBVTT

1
00:00:01.390 --> 00:00:03.650
There are many ways to
look at this process.

2
00:00:04.790 --> 00:00:11.120
One way of looking at it
as two distinct activities.

3
00:00:11.120 --> 00:00:17.000
Mainly, big data engineering and
big data analytics,

4
00:00:17.000 --> 00:00:21.940
or computational big data
science as I like to call it,

5
00:00:21.940 --> 00:00:24.890
since more than simple
analytics are being performed.

6
00:00:26.840 --> 00:00:34.053
A more detailed way of looking at
the process reveals five listing steps or

7
00:00:34.053 --> 00:00:38.125
activities of the data science process,

8
00:00:38.125 --> 00:00:43.850
namely acquire, prepare,
analyze, report, and act.

9
00:00:43.850 --> 00:00:48.900
We can simply say that data science
happens at the boundary of all the steps.

10
00:00:48.900 --> 00:00:52.950
Ideally, this process should
support experimental work,

11
00:00:52.950 --> 00:00:58.930
which is constantly iterated and
leads to more scientific exploration,

12
00:00:58.930 --> 00:01:04.130
as well as producing actionable
results during these explorations

13
00:01:04.130 --> 00:01:08.495
using dynamic scalability on big data and
cloud platforms.

14
00:01:11.095 --> 00:01:15.842
This five step process can be used in
alternative ways in real life big data

15
00:01:15.842 --> 00:01:20.685
applications if we add the dependencies
of different tools to each other.

16
00:01:22.325 --> 00:01:25.915
The influence of big data pushes for

17
00:01:25.915 --> 00:01:30.335
alternative scalability approaches
at each step of the process.

18
00:01:31.600 --> 00:01:35.740
Acquire includes anything
that helps us retrieve data,

19
00:01:35.740 --> 00:01:39.740
including finding, accessing,
acquiring, and moving data.

20
00:01:41.500 --> 00:01:49.170
It includes identification of and
authenticated access to all related data,

21
00:01:49.170 --> 00:01:53.490
as well as transportation of data
from sources to destinations.

22
00:01:54.890 --> 00:02:00.760
It includes ways to subset and
match the data to regions or

23
00:02:00.760 --> 00:02:05.350
times of interest, which we sometimes
refer to as geospatial querying.

24
00:02:07.130 --> 00:02:11.234
We divide the prepare data
step into two sub-steps,

25
00:02:11.234 --> 00:02:14.070
based on the nature of the activity.

26
00:02:15.630 --> 00:02:21.060
The first step in data preparation
involves exploring the data

27
00:02:21.060 --> 00:02:25.690
to understand its nature,
what it means, its quality, and format.

28
00:02:27.160 --> 00:02:30.550
It often takes a preliminary
analysis of data, or

29
00:02:30.550 --> 00:02:32.670
samples of data, to understand it.

30
00:02:33.700 --> 00:02:36.770
This is why this primary
step is called prepare.

31
00:02:39.030 --> 00:02:42.990
Once we know more about the data
through exploratory analysis,

32
00:02:42.990 --> 00:02:46.420
the next step is pre-processing
of data for analysis.

33
00:02:47.540 --> 00:02:52.690
It includes cleaning data,
subsetting, or filtering data, and

34
00:02:52.690 --> 00:02:58.600
creating data, which programs can read and
understand by modelling raw data

35
00:02:58.600 --> 00:03:04.710
into a more defined data model, or
packaging it using a specific data format.

36
00:03:05.850 --> 00:03:11.270
We will learn more about data models and
data formats later in this course.

37
00:03:12.940 --> 00:03:15.780
If there are multiple data sets involved,

38
00:03:15.780 --> 00:03:20.220
this step also includes integration
of different data sources or

39
00:03:20.220 --> 00:03:23.745
streams, which is a topic we will
explore in our course three.

40
00:03:26.980 --> 00:03:32.050
The prepared data then would be
passed on to the analysis step,

41
00:03:32.050 --> 00:03:35.670
which involves selection of
analytical techniques to use,

42
00:03:35.670 --> 00:03:38.840
building a model of the data,
and analyzing results.

43
00:03:39.920 --> 00:03:43.520
This step can take a couple
of iterations on its own or

44
00:03:43.520 --> 00:03:47.119
might require a data scientist
to go back to steps 1 and

45
00:03:47.119 --> 00:03:50.410
2 to get more data or
package data in a different way.

46
00:03:51.700 --> 00:03:53.520
So, exploration never ends.

47
00:03:56.050 --> 00:04:02.413
Step 4 for communicating results includes
evaluation of analytical results,

48
00:04:02.413 --> 00:04:07.464
presenting them in a visual way,
creating reports that include

49
00:04:07.464 --> 00:04:12.250
an assessment of results with
respect to success criteria.

50
00:04:12.250 --> 00:04:19.293
Activities in this step can often be
referred to with terms like interpret,

51
00:04:19.293 --> 00:04:23.720
summarize, visualize, and post-process.

52
00:04:23.720 --> 00:04:28.880
The last step brings us back to the very
first reason we do data science,

53
00:04:28.880 --> 00:04:29.550
for a purpose.

54
00:04:31.090 --> 00:04:36.140
Reporting insights from analysis and
determining actions from insights based

55
00:04:36.140 --> 00:04:41.267
on the purpose you initially defined
is what we refer to as the act step.

56
00:04:43.210 --> 00:04:48.140
We have now seen all of the steps
in a typical data science process.

57
00:04:48.140 --> 00:04:52.750
Please note that this is an iterative
process and findings from

58
00:04:52.750 --> 00:04:57.310
one step may require previous steps
to be repeated, but need information,

59
00:04:57.310 --> 00:05:01.250
leading for further exploration and
application of these steps.

60
00:05:02.650 --> 00:05:07.230
Scalability of this process to big
data analysis requires the use of

61
00:05:07.230 --> 00:05:10.099
big data platforms like Hadoop.
WEBVTT

1
00:00:00.450 --> 00:00:04.280
The Hadoop ecosystem frameworks and
applications

2
00:00:04.280 --> 00:00:08.870
provide such functionality through
several overarching themes and goals.

3
00:00:11.000 --> 00:00:15.830
First, they provide scalability
to store large volumes of data

4
00:00:15.830 --> 00:00:17.150
on commodity hardware.

5
00:00:18.760 --> 00:00:21.570
As the number of systems increase, so

6
00:00:21.570 --> 00:00:25.620
does the chance for
crashes and hardware failures.

7
00:00:25.620 --> 00:00:30.390
They handle fault tolerance to
gracefully recover from these problems.

8
00:00:32.650 --> 00:00:37.595
In addition, they are designed to handle
big data capacity and compressing text

9
00:00:37.595 --> 00:00:43.185
files, graphs of social networks,
streaming sensor data and raster images.

10
00:00:43.185 --> 00:00:45.635
We can add more data
types to this variety.

11
00:00:46.695 --> 00:00:48.355
For any given data type,

12
00:00:49.515 --> 00:00:53.635
you can find several projects in
the ecosystem that support it.

13
00:00:55.250 --> 00:00:58.890
Finally, they facilitate
a shared environment,

14
00:00:58.890 --> 00:01:02.050
allow multiple jobs to
execute simultaneously.

15
00:01:04.350 --> 00:01:08.930
Additionally, the Hadoop ecosystem
includes a wide range of open source

16
00:01:08.930 --> 00:01:14.170
projects backed by a large and
active community.

17
00:01:14.170 --> 00:01:18.480
These projects are free to use and
easy to find support for.

18
00:01:20.060 --> 00:01:25.690
Today, there are over 100
Big Data open source projects,

19
00:01:25.690 --> 00:01:31.250
and this continues to grow, many rely
on Hadoop, but some are independent.

20
00:01:33.960 --> 00:01:39.230
Here is, one way of looking at a subset
of tools in the Hadoop Ecosystem.

21
00:01:40.360 --> 00:01:45.240
This layer diagram is organized
vertically based on the interface.

22
00:01:46.390 --> 00:01:52.720
Lower level interfaces to storage and
scheduling on the bottom and

23
00:01:52.720 --> 00:01:56.520
high level languages and
interactivity at the top.

24
00:01:59.080 --> 00:02:05.112
The Hadoop distributed file system,
or HDFS, is the foundation for

25
00:02:05.112 --> 00:02:11.777
many big data frameworks since it
provides scalable and reliable storage.

26
00:02:11.777 --> 00:02:16.915
As the size of your data increases,
you can add commodity

27
00:02:16.915 --> 00:02:21.314
hardware to HDFS to
increase storage capacity.

28
00:02:21.314 --> 00:02:27.107
So it enables what we call
scaling out of your resources.

29
00:02:29.267 --> 00:02:32.960
Hadoop YARN provide
flexible scheduling and

30
00:02:32.960 --> 00:02:36.660
resource management over the HTFS storage.

31
00:02:37.660 --> 00:02:43.955
Yarn is use at Yahoo to schedule
jobs across 40,000 servers.

32
00:02:45.105 --> 00:02:50.045
MapReduce is a programming model
that simplifies parallel computing.

33
00:02:50.045 --> 00:02:54.725
Instead of dealing with the complexities
of synchronization and scheduling you only

34
00:02:54.725 --> 00:03:00.325
need to give MapReduce two
functions map and reduce.

35
00:03:00.325 --> 00:03:02.643
This programming model is so

36
00:03:02.643 --> 00:03:07.962
powerful that Google previously
used it for indexing websites.

37
00:03:07.962 --> 00:03:12.820
MapReduce, only assumes
a limited model to express data.

38
00:03:12.820 --> 00:03:16.429
Hive, and
Pig are two additional programming models,

39
00:03:16.429 --> 00:03:20.421
on top of MapReduce,
to augment data modeling of MapReduce,

40
00:03:20.421 --> 00:03:24.744
with relational algebra and
data flow modeling, respectively.

41
00:03:26.869 --> 00:03:31.682
Hive was created at Facebook to
issue SQL-like queries using

42
00:03:31.682 --> 00:03:34.280
MapReduce on their data in HDFS.

43
00:03:35.410 --> 00:03:40.773
Pig was created at Yahoo to model
dataflow based programs using MapReduce.

44
00:03:40.773 --> 00:03:45.345
Thanks to YARNs ability to
manage resources, not just for

45
00:03:45.345 --> 00:03:48.715
MapReduce but other programming models.

46
00:03:48.715 --> 00:03:52.955
Giraph was built for
processing large scale graphs efficiently.

47
00:03:54.335 --> 00:04:00.676
For example, Facebook uses Giraph to
analyze the social graphs of its users.

48
00:04:00.676 --> 00:04:05.181
Similarly, Storm, Spark and
Flink were built for

49
00:04:05.181 --> 00:04:09.595
real time and
In-memory processing of big data.

50
00:04:09.595 --> 00:04:14.013
On top of the YARN resource scheduler and
HDFS.

51
00:04:14.013 --> 00:04:20.555
In-memory processing is a powerful
way of running big data applications,

52
00:04:20.555 --> 00:04:26.598
even faster, achieving 100x better
performance for some tasks.

53
00:04:26.598 --> 00:04:30.845
Sometimes your data processing or
tasks are not easily or

54
00:04:30.845 --> 00:04:36.332
efficiently represented using the file and
directory model of storage,

55
00:04:36.332 --> 00:04:42.010
examples of this include collections
of key values or large sparse tables.

56
00:04:43.282 --> 00:04:48.859
NoSQL projects such as
Cassandra MongoDB and

57
00:04:48.859 --> 00:04:52.681
HBase handle all these cases.

58
00:04:52.681 --> 00:04:57.396
Cassandra was created at Facebook and
Facebook also use HBase for

59
00:04:57.396 --> 00:04:59.290
its messaging platform.

60
00:05:01.934 --> 00:05:07.562
Finally, running all this tools requires
a centralized management system for

61
00:05:07.562 --> 00:05:12.520
synchronization, configuration And
to ensure high availability.

62
00:05:13.660 --> 00:05:18.828
Zookeeper, created by Yahoo to
wrangle services named after animals,

63
00:05:18.828 --> 00:05:20.614
performs these duties.

64
00:05:23.214 --> 00:05:27.023
Just looking at the small number
of Hadoop stack components,

65
00:05:27.023 --> 00:05:31.700
we can already see that most of them
are dedicated to data modeling.

66
00:05:31.700 --> 00:05:36.010
Management, and
efficient processing of the data.

67
00:05:36.010 --> 00:05:40.610
In the rest of this course,
we will give you fundamental knowledge and

68
00:05:40.610 --> 00:05:46.580
some practical skills on how to start
modeling and managing your data, and

69
00:05:46.580 --> 00:05:52.310
picking the right tools for this activity
from a plethora of big data tools.
WEBVTT

1
00:00:01.170 --> 00:00:04.780
Welcome to course two of
the big data specialization.

2
00:00:04.780 --> 00:00:06.390
I'm Amarnath Gupta.

3
00:00:06.390 --> 00:00:07.270
>> And I'm Ilkay Altintas.

4
00:00:07.270 --> 00:00:10.830
We are really excited to work
with you in this course,

5
00:00:10.830 --> 00:00:14.820
to develop your understanding and skills
in big data modeling and management.

6
00:00:16.100 --> 00:00:20.470
You might have just finished our first
course, and see the potential and

7
00:00:20.470 --> 00:00:22.570
challenges of big data.

8
00:00:22.570 --> 00:00:25.630
If you haven't it's not required but for

9
00:00:25.630 --> 00:00:29.510
those with less background in
the area you might find it valuable.

10
00:00:30.890 --> 00:00:34.070
>> Let's explain what we mean by
big data modeling and management.

11
00:00:35.400 --> 00:00:39.470
Suppose you have an application
where the data is big in a sense and

12
00:00:39.470 --> 00:00:45.470
it has a large volume, or has high speed,
or comes with a lot of variations.

13
00:00:45.470 --> 00:00:49.220
Even before you think of
how to handle the bigness,

14
00:00:49.220 --> 00:00:51.450
you need to have a sense of
what the data looks like.

15
00:00:52.550 --> 00:00:57.860
The goal of data modeling is to
formally explore the nature of data, so

16
00:00:57.860 --> 00:01:00.870
that you can figure out what
kind of storage you need, and

17
00:01:00.870 --> 00:01:02.470
what kind of processing you can do on it.

18
00:01:04.010 --> 00:01:07.010
The goal of data management
is to figure out

19
00:01:07.010 --> 00:01:10.840
what kind of infrastructure support
you would need for the data.

20
00:01:10.840 --> 00:01:15.130
For example, does your environment need
to keep multiple replicas of the data?

21
00:01:15.130 --> 00:01:18.454
Do you need to do statistical
computation with the data?

22
00:01:18.454 --> 00:01:23.120
Once these operational requirements,
you'll

23
00:01:23.120 --> 00:01:27.110
be able to choose the right system that
will let you perform these operations.

24
00:01:28.990 --> 00:01:33.200
>> We will also introduce
management of big data as it is

25
00:01:33.200 --> 00:01:38.530
streaming from data sources and talk
about storage architectures for big data.

26
00:01:38.530 --> 00:01:42.790
For example,
how can high velocity data get ingested,

27
00:01:42.790 --> 00:01:48.360
managed, stored in order to enable
real time analytical capabilities.

28
00:01:48.360 --> 00:01:53.560
Or what is the difference between
data at rest and data in motion?

29
00:01:53.560 --> 00:01:56.230
And how can a data system enable both?

30
00:01:57.570 --> 00:02:00.680
>> Once you've understood the basic
concepts of data modeling,

31
00:02:00.680 --> 00:02:04.300
data management, and streaming data,
we will introduce you

32
00:02:04.300 --> 00:02:09.120
to the characteristics of large volume
data and how to think about that.

33
00:02:09.120 --> 00:02:13.080
Thus, we will transition from
classical database management systems,

34
00:02:13.080 --> 00:02:18.730
that is DBMSs,
to big data management systems, or BDMSs.

35
00:02:18.730 --> 00:02:23.000
We'll present brief overviews of
several big data management systems

36
00:02:23.000 --> 00:02:24.430
available in the marketplace today.

37
00:02:25.630 --> 00:02:31.310
We are so excited to show you examples
of archived and streaming big data sets.

38
00:02:31.310 --> 00:02:36.500
Our goal is to provide you with simple
hands on exercises that require

39
00:02:36.500 --> 00:02:41.290
no programming, but show you what
big data looks like, and why various

40
00:02:41.290 --> 00:02:47.030
big data management systems are suitable
for specific kinds of big data.

41
00:02:47.030 --> 00:02:50.510
In the end you will be
able to design a simple

42
00:02:50.510 --> 00:02:53.110
big data information system
using this knowledge.

43
00:02:54.110 --> 00:02:56.280
We wish you a fun time learning and

44
00:02:56.280 --> 00:03:00.540
hope to hear from you in the discussion
forums and learner stories.

45
00:03:00.540 --> 00:03:02.529
>> Happy learning and think big data.
WEBVTT

1
00:00:01.000 --> 00:00:06.820
This is the second course in the 2016
version of our big data specialization.

2
00:00:06.820 --> 00:00:10.820
>> After listening to learners like you,
we have changed some of the content and

3
00:00:10.820 --> 00:00:12.149
ordering in the specialization.

4
00:00:13.210 --> 00:00:17.200
This course takes you on the first
step of any big data project,

5
00:00:17.200 --> 00:00:19.470
its modeling and management.

6
00:00:19.470 --> 00:00:23.221
>> We hope that this background
on what big data looks like and

7
00:00:23.221 --> 00:00:27.662
how it is modeled and managed in
a large scale system will make you feel

8
00:00:27.662 --> 00:00:32.487
more prepared to take steps towards
retrieving and processing big data and

9
00:00:32.487 --> 00:00:37.338
perform big data analytics which
are topics coming up in the next courses.
WEBVTT

1
00:00:03.582 --> 00:00:06.800
The third component of
a data model is constraints.

2
00:00:08.080 --> 00:00:10.418
A constraint is a logical statement.

3
00:00:10.418 --> 00:00:15.770
That means one can compute and test
whether the statement is true or false.

4
00:00:16.980 --> 00:00:21.130
Constraints are part of the data model
because they can specify something about

5
00:00:21.130 --> 00:00:24.240
the semantics, that is,
the meaning of the data.

6
00:00:24.240 --> 00:00:27.705
For example,
the constraint that a week has seven and

7
00:00:27.705 --> 00:00:32.402
only seven days is something that a data
system would not know unless this

8
00:00:32.402 --> 00:00:36.025
knowledge is passed on to it
in the form of a constraint.

9
00:00:36.025 --> 00:00:39.531
Another constraint, shown here,

10
00:00:42.132 --> 00:00:46.690
Tells the system that the number of
titles for a movie is restricted to one.

11
00:00:49.740 --> 00:00:50.800
Different data models,

12
00:00:50.800 --> 00:00:54.260
as we'll see in the next module,
will have different kinds of constraints.

13
00:00:56.140 --> 00:00:58.760
There may be many different
kinds of constraints.

14
00:01:00.020 --> 00:01:04.039
A value constraint is a logical
statement about data values.

15
00:01:05.520 --> 00:01:09.600
On a previous slide we have said,
that the age, that is,

16
00:01:09.600 --> 00:01:13.540
the value of data elements representing
the age of an entity can not be negative.

17
00:01:16.310 --> 00:01:20.840
We also saw an example of
uniqueness constraint when we said

18
00:01:20.840 --> 00:01:23.880
every movie can have only one title.

19
00:01:25.200 --> 00:01:27.120
In the words of logic,

20
00:01:27.120 --> 00:01:32.460
there should exist no data object that's
a movie and has more than one title.

21
00:01:33.860 --> 00:01:37.719
It's easy to see that enforcing
these constraints requires us

22
00:01:37.719 --> 00:01:41.446
to count the number of titles and
then verify that it's one.

23
00:01:41.446 --> 00:01:46.490
Now, one can generalize this to count
the number of values associated with

24
00:01:46.490 --> 00:01:51.310
each object and check whether it lies
between an upper and lower bound.

25
00:01:52.380 --> 00:01:56.010
This is often called a cardinality
constraint of data property.

26
00:01:59.530 --> 00:02:04.388
In a medical example here,
the constraint has a lower limit of 0 and

27
00:02:04.388 --> 00:02:05.931
an upper limit of 3.

28
00:02:10.390 --> 00:02:15.253
A different kind of value constraint can
be enforced by restricting the type of

29
00:02:15.253 --> 00:02:17.030
the data allowed in a field.

30
00:02:18.620 --> 00:02:23.210
If we do not have such a constraint we
can put any type of data in the field.

31
00:02:23.210 --> 00:02:28.300
For example, you can have -99 as
the value of the last name of a person,

32
00:02:28.300 --> 00:02:30.680
of course that would be wrong.

33
00:02:30.680 --> 00:02:34.750
To ensure that this does not happen,
we can enforce the type of the last name

34
00:02:35.780 --> 00:02:40.300
to be a non-numeric alphabetic string.

35
00:02:40.300 --> 00:02:45.420
This example shows a logical
expression for this constraint.

36
00:02:45.420 --> 00:02:50.840
A type constraint is a special
kind of domain constraint.

37
00:02:50.840 --> 00:02:52.420
The domain of a data property or

38
00:02:52.420 --> 00:02:57.900
attribute is the possible set of values
that are allowed for that attribute.

39
00:02:57.900 --> 00:03:01.026
For example, the possible values for

40
00:03:01.026 --> 00:03:05.624
the day part of the date field
can be between 1 and 31.

41
00:03:05.624 --> 00:03:09.746
While a month may have
the value between 1 and 12.

42
00:03:09.746 --> 00:03:16.200
Or, alternately, a value from the set
January, February, ect till December.

43
00:03:17.870 --> 00:03:21.697
Now, one can devise a more complex
constraint, where the value of the date

44
00:03:21.697 --> 00:03:26.099
for April, June, and September and
November, are restricted between 1 and 30.

45
00:03:26.099 --> 00:03:30.609
And if you think about it,
all three constraints that we have

46
00:03:30.609 --> 00:03:35.420
described in the last slide
are value constraints.

47
00:03:35.420 --> 00:03:39.380
So they only state how to restrict
the values of some data property.

48
00:03:40.470 --> 00:03:45.392
In sharp contrast, structural properties
restrict the structure of the data.

49
00:03:45.392 --> 00:03:48.700
We'll choose a more complex example for
this.

50
00:03:50.320 --> 00:03:53.952
Suppose we are a matrix,
as shown in the example, and

51
00:03:53.952 --> 00:03:56.794
we've restricted to be a square matrix.

52
00:03:56.794 --> 00:04:00.919
So the number of columns is exactly
equal to the number of rows.

53
00:04:03.230 --> 00:04:06.570
We have not put any restriction
on the number of rows or columns.

54
00:04:06.570 --> 00:04:07.910
But just that they have to be the same.

55
00:04:09.280 --> 00:04:11.560
Now this constrains
the structure of the matrix and

56
00:04:11.560 --> 00:04:16.140
implies that the number of entries in
the structure will be a squared number.

57
00:04:17.480 --> 00:04:22.600
If we convert this matrix to a three
column table as shown, and impose

58
00:04:22.600 --> 00:04:28.000
the same squareness constraint, it will
translate to a more complex condition.

59
00:04:28.000 --> 00:04:31.540
That the number of data
rows will be the square of

60
00:04:31.540 --> 00:04:34.200
the number of unique values
in column one of the table.

61
00:04:35.900 --> 00:04:39.410
We'll encounter some more structural
constraints in the next module.
WEBVTT

1
00:00:03.208 --> 00:00:07.770
The second component of a data model is
a set of operations that can be performed

2
00:00:07.770 --> 00:00:09.390
on the data.

3
00:00:09.390 --> 00:00:10.250
And in this module,

4
00:00:10.250 --> 00:00:15.650
we'll discuss the operations without
considering the bigness aspect.

5
00:00:15.650 --> 00:00:17.110
In course three,

6
00:00:17.110 --> 00:00:20.710
we'll come back to the issue of performing
these operations when the data is large.

7
00:00:22.515 --> 00:00:25.830
Now,operations specified
the methods to manipulate the data.

8
00:00:26.840 --> 00:00:31.220
SInce different data models are typically
associated with different structures,

9
00:00:31.220 --> 00:00:34.210
the operations on them will be different.

10
00:00:34.210 --> 00:00:39.410
But some types of operations are usually
performed across all data models.

11
00:00:39.410 --> 00:00:41.030
We'll describe a few of them here.

12
00:00:43.320 --> 00:00:50.818
One common operation extract a part of
a collection based on the condition.

13
00:00:50.818 --> 00:00:54.422
In the example here,
we have a set of records and

14
00:00:54.422 --> 00:00:59.110
we're looking for a sub set that
satisfies the condition that

15
00:00:59.110 --> 00:01:03.548
the fifth field has a value
greater than 100,000.

16
00:01:03.548 --> 00:01:07.603
The only one record
satisfies this requirement.

17
00:01:07.603 --> 00:01:11.580
Note that we called this operation
subsetting rather loosely.

18
00:01:12.940 --> 00:01:17.210
Depending on the context,
it's also called selection or filtering.

19
00:01:20.410 --> 00:01:26.300
The next common operation is retrieving
a part of a structure that is specified.

20
00:01:26.300 --> 00:01:29.440
In this case,
we specify that we are interested

21
00:01:29.440 --> 00:01:33.130
in just the first two fields
of a collection off records.

22
00:01:34.880 --> 00:01:40.340
But this produces a new collection of
records which has only these fields.

23
00:01:41.350 --> 00:01:44.460
This operation like before has many names.

24
00:01:44.460 --> 00:01:46.910
The most dominant name is projection.

25
00:01:48.340 --> 00:01:52.204
In the next module, we'll see several
versions of this operation for

26
00:01:52.204 --> 00:01:53.588
different data models.

27
00:01:56.108 --> 00:02:00.176
The next two operations are about
combining two collections

28
00:02:00.176 --> 00:02:01.460
into a larger one.

29
00:02:02.540 --> 00:02:06.450
The term combine may be
interpreted in various ways.

30
00:02:06.450 --> 00:02:08.830
The most straightforward
of them is said union.

31
00:02:09.910 --> 00:02:12.500
The assumption behind the union operation

32
00:02:12.500 --> 00:02:15.930
is that the two collections
involved have the same structure.

33
00:02:15.930 --> 00:02:22.050
In other words, if one collection has
four fields and another has 14 fields, or

34
00:02:22.050 --> 00:02:27.230
if one has four fields on people and
the dates of birth, and the other has four

35
00:02:27.230 --> 00:02:31.350
things about countries and their capitols,
they cannot be combined through union.

36
00:02:33.650 --> 00:02:37.340
In the example here,
their two collections have three and

37
00:02:37.340 --> 00:02:42.620
two records, respectively, with one
record that's common between them.

38
00:02:42.620 --> 00:02:43.120
The green one.

39
00:02:45.100 --> 00:02:48.170
The result collection has four record,

40
00:02:48.170 --> 00:02:53.300
because duplicates are disallowed
because it's a set operation.

41
00:02:53.300 --> 00:02:58.110
There is indeed another version of
union where duplicates are allowed and

42
00:02:58.110 --> 00:03:00.540
will produce five records instead of four.

43
00:03:03.040 --> 00:03:08.020
The second kind of combining,
called a Join, can be done when

44
00:03:08.020 --> 00:03:12.460
the two collections have different data
content but have some common elements.

45
00:03:14.430 --> 00:03:16.290
In the example shown,

46
00:03:16.290 --> 00:03:19.920
the first field is the common element
between the two collections on the left.

47
00:03:21.040 --> 00:03:24.630
In this kind of data combination
there are two stages.

48
00:03:24.630 --> 00:03:30.380
First, for each data item think
of a record of collection one,

49
00:03:30.380 --> 00:03:33.490
one finds a set of matching
data items in collection two.

50
00:03:35.120 --> 00:03:38.960
Thus, the first records of
the two collections match

51
00:03:38.960 --> 00:03:40.060
based on the first field.

52
00:03:41.490 --> 00:03:43.750
In the second phase of the operation,

53
00:03:43.750 --> 00:03:46.710
all fields of the matching
record pairs are put together.

54
00:03:47.780 --> 00:03:51.170
In the first record of the result
collection shown on the right,

55
00:03:51.170 --> 00:03:54.890
one gets the first four fields
on the first collection and

56
00:03:54.890 --> 00:03:57.230
the remaining two fields
from the second collection.

57
00:03:58.500 --> 00:04:04.380
Now in this one example, we found one pair
of matching records from the collections.

58
00:04:04.380 --> 00:04:07.790
In general, one would find more
than one matching record pairs.

59
00:04:09.260 --> 00:04:12.415
As you can see,
this operation is more complex and

60
00:04:12.415 --> 00:04:16.863
can be very expensive when the size
of the true collections are large.
WEBVTT

1
00:00:00.800 --> 00:00:06.950
In the big data world, we often hear the
term structured data, that is, data having

2
00:00:06.950 --> 00:00:12.490
a structure which is quite different
from the so-called unstructured data.

3
00:00:12.490 --> 00:00:13.500
But what is a structure?

4
00:00:14.910 --> 00:00:16.040
Let's consider file 1.

5
00:00:17.310 --> 00:00:21.960
It's a typical CSV file that has
three lines with different content.

6
00:00:21.960 --> 00:00:26.260
But the file content is uniform
in the sense that each line,

7
00:00:26.260 --> 00:00:30.400
call it a record,
has exactly three fields,

8
00:00:30.400 --> 00:00:33.360
which we sometimes call data properties or
attributes.

9
00:00:34.720 --> 00:00:41.420
Further, the first two of these fields
are strings and the third one is a date.

10
00:00:41.420 --> 00:00:44.770
We can add more records, that's
lines with the same pattern of data,

11
00:00:44.770 --> 00:00:46.210
to the file in the same fashion.

12
00:00:47.240 --> 00:00:52.430
The content will grow, but the pattern of
data organization will remain identical.

13
00:00:53.440 --> 00:00:58.600
This repeatable pattern of data
organization makes the file structured.

14
00:00:58.600 --> 00:01:02.870
Now let's look at file 2,
which is four records of five fields each,

15
00:01:03.910 --> 00:01:07.680
except that the third record seems
to be missing the last entry.

16
00:01:08.710 --> 00:01:09.660
Is this file structured?

17
00:01:10.810 --> 00:01:15.383
We argue that it is, because the missing
value makes the third record incomplete,

18
00:01:15.383 --> 00:01:20.310
but it does not break the structure or
the pattern of the data organization.

19
00:01:20.310 --> 00:01:23.088
Let's looks at these
two files side by side.

20
00:01:23.088 --> 00:01:29.320
Clearly file 2 has more fields, and hence
is sort of wider than the first file.

21
00:01:29.320 --> 00:01:31.980
Would you say that they
have the same structure?

22
00:01:31.980 --> 00:01:33.990
Well, on the face of it they don't.

23
00:01:33.990 --> 00:01:35.740
But if you think more broadly,

24
00:01:35.740 --> 00:01:39.260
you would notice that they
are both collection of k fields.

25
00:01:40.330 --> 00:01:44.770
The size of the collection,
respectively three and four, differs.

26
00:01:44.770 --> 00:01:48.320
And k is 3 in the first case and
5 in the second.

27
00:01:48.320 --> 00:01:52.330
But we can think of 3 and 5 as parameters.

28
00:01:52.330 --> 00:01:57.280
In that case, we will say that these
files have been generated by a similar

29
00:01:57.280 --> 00:02:02.160
organizational structure, and
hence they have the same data model.

30
00:02:03.440 --> 00:02:06.190
Now in contrast, consider this file.

31
00:02:06.190 --> 00:02:10.480
Just looking at it, it's impossible to
figure out how the data is organized and

32
00:02:10.480 --> 00:02:12.420
how to identify subparts of the data.

33
00:02:13.440 --> 00:02:15.850
We would call this data unstructured.

34
00:02:16.860 --> 00:02:21.605
Often, compressed data like JPEG images,
MP3 audio files,

35
00:02:21.605 --> 00:02:26.450
MPEG3 video files, encrypted data,
are usually unstructured.

36
00:02:27.490 --> 00:02:32.098
In module two, we'll elaborate on data
models that are not fully structured or

37
00:02:32.098 --> 00:02:33.900
are structured differently.
WEBVTT

1
00:00:00.890 --> 00:00:03.790
This is the first of two
hands-on exercises involving

2
00:00:03.790 --> 00:00:06.170
sensor data from a weather station.

3
00:00:06.170 --> 00:00:09.530
In this one, we will look at
static data in a text file.

4
00:00:09.530 --> 00:00:12.900
The next one we will look at live data
streaming from the weather station

5
00:00:12.900 --> 00:00:13.490
in real time.

6
00:00:13.490 --> 00:00:17.820
In this exercise, we will begin
by opening a terminal window and

7
00:00:17.820 --> 00:00:21.330
changing into the directory
containing the station measurements.

8
00:00:21.330 --> 00:00:24.560
We will look at these measurements in
a text file and then look at the key for

9
00:00:24.560 --> 00:00:27.540
these measurements so
we can understand what the values mean.

10
00:00:27.540 --> 00:00:29.310
Finally, we will plot the measurements.

11
00:00:30.340 --> 00:00:30.840
Let's begin.

12
00:00:31.870 --> 00:00:35.023
First, we will open a terminal window by
clicking on the Terminal icon on the top

13
00:00:35.023 --> 00:00:35.692
of the toolbar.

14
00:00:38.179 --> 00:00:44.814
Next, we'll cd into the directory

15
00:00:44.814 --> 00:00:50.350
containing the sensor data.

16
00:00:51.380 --> 00:00:58.358
We'll run cd
Downloads/big-data-two/sensor.

17
00:01:00.680 --> 00:01:07.750
We can write ls to see
the contents of this directory.

18
00:01:12.274 --> 00:01:18.554
The data from the weather station is
in a text file called wx-data.txt.

19
00:01:21.370 --> 00:01:26.550
We can run more wx-data.txt to
see the contents of this file.

20
00:01:34.970 --> 00:01:37.990
Each line of this file is
a separate set of measurements.

21
00:01:37.990 --> 00:01:42.390
There are two columns in this file,
the first column is a time stamp and

22
00:01:42.390 --> 00:01:45.160
it's separated by
a second column by a tab.

23
00:01:45.160 --> 00:01:51.710
The second column itself has separate
columns and these are separated by commas.

24
00:01:51.710 --> 00:01:56.790
The time stamp is the number
of seconds since 1970.

25
00:01:56.790 --> 00:01:59.450
You'll notice that it increases for
each time stamp.

26
00:02:01.830 --> 00:02:04.520
You'll notice that it increases for
each measurement.

27
00:02:04.520 --> 00:02:06.550
But sometimes measurements
come in at the same time.

28
00:02:06.550 --> 00:02:10.028
For example this one at 006.

29
00:02:10.028 --> 00:02:15.053
The measurements,
we see that the prefix is 0R1 for

30
00:02:15.053 --> 00:02:18.188
most of them but some have 0R2.

31
00:02:18.188 --> 00:02:21.060
If we look at the other measurements,

32
00:02:21.060 --> 00:02:26.646
we see that all the 0R1 measurements
start with Dn, Dm, Dx, and so on.

33
00:02:26.646 --> 00:02:31.670
Whereas R2 begins with Ta, Ua, and Pa.

34
00:02:34.310 --> 00:02:37.419
If we scroll down in the text
file by hitting the space bar,

35
00:02:37.419 --> 00:02:40.728
we'll see there are other
measurements besides R1 and R2.

36
00:02:43.860 --> 00:02:49.360
For example, there's R5 that has Th,
Vh, Vs and so on.

37
00:02:49.360 --> 00:02:53.830
And there's R0 which has
all the measurements.

38
00:02:53.830 --> 00:02:57.510
So Dn, Dm, Dx, Ta, Ua, Pa.

39
00:03:00.550 --> 00:03:01.930
And the remaining ones.

40
00:03:04.160 --> 00:03:07.434
Next we'll open another internal
window and look at the key to this

41
00:03:07.434 --> 00:03:10.598
measurements click on the tool
bar to open the terminal window.

42
00:03:10.598 --> 00:03:15.900
cd into downloads big data two sensor

43
00:03:23.120 --> 00:03:31.131
And the key to these measurements is
in a file called wxt520format.txt.

44
00:03:31.131 --> 00:03:36.290
We can run more wxt520format.txt
to see this file.

45
00:03:44.190 --> 00:03:47.116
This file says where each
of the prefix is mean, for

46
00:03:47.116 --> 00:03:49.307
example Sn is the wind speed minimum.

47
00:03:49.307 --> 00:03:56.430
Sm is the wind speed average.

48
00:03:56.430 --> 00:04:00.700
And Ta is the air temperature.

49
00:04:00.700 --> 00:04:04.530
So if we go back to our sensor file,
we see here Ta equals 13.9c.

50
00:04:04.530 --> 00:04:10.187
That means the air temperature at this

51
00:04:10.187 --> 00:04:15.690
time was 13.9 degrees celsius.

52
00:04:15.690 --> 00:04:21.022
We can also create a plot of
the data in this text file

53
00:04:21.022 --> 00:04:25.994
by running plot-data.py wx-data.txt Ta.

54
00:04:32.727 --> 00:04:37.529
This says to plot the data in
the wx-data file and the measure that we

55
00:04:37.529 --> 00:04:42.680
want to apply is Ta, which according
to our key, is the air temperature.

56
00:04:46.880 --> 00:04:51.520
When we run it, it displays a plot of the
air temperature found in the text file.
WEBVTT

1
00:00:00.568 --> 00:00:02.240
In this hands-on exercise,

2
00:00:02.240 --> 00:00:05.870
we will be looking at an image file,
which uses an array data model.

3
00:00:07.050 --> 00:00:11.380
First, we will open a terminal window and
display an image file on the screen.

4
00:00:12.890 --> 00:00:16.630
Next, we will examine the structure
of the image, and finally,

5
00:00:16.630 --> 00:00:19.860
extract pixel values from
various locations in the image.

6
00:00:21.550 --> 00:00:22.660
Let's begin.

7
00:00:22.660 --> 00:00:25.954
First, we'll open a terminal window by
clicking on the terminal icon at the top

8
00:00:25.954 --> 00:00:26.650
of the toolbar.

9
00:00:33.101 --> 00:00:37.989
Next, we'll CDN2 the directory
containing the image,

10
00:00:37.989 --> 00:00:41.390
cdn2downloads/bigdata2/image.

11
00:00:48.410 --> 00:00:51.470
We can run ls to see the image
in different scripts.

12
00:00:54.050 --> 00:00:58.344
The file, Australia.jpg,
is an image that we want to view,

13
00:00:58.344 --> 00:01:00.090
we can use eog to view it.

14
00:01:00.090 --> 00:01:08.830
Run eog Australia.jpg Eog is
a common image viewer on Linux.

15
00:01:11.192 --> 00:01:15.870
Australia.jpg is a satellite image
of the Australian continent.

16
00:01:15.870 --> 00:01:17.560
Now let's look at the structure
of this image file.

17
00:01:21.320 --> 00:01:25.815
If we go back to our terminal window,
we can run the image

18
00:01:25.815 --> 00:01:30.611
viewer in the background by
hitting CTRL+Z, and then bg.

19
00:01:30.611 --> 00:01:35.319
We can view the dimensions or
structure of the array data model of this

20
00:01:35.319 --> 00:01:38.904
image by running
dimensions.py Australia.jpg.

21
00:01:43.740 --> 00:01:50.860
This says that the image has
5250 columns and 4320 rows.

22
00:01:50.860 --> 00:01:53.240
So it is a two-dimensional image.

23
00:01:53.240 --> 00:01:55.671
Additionally, each cell or

24
00:01:55.671 --> 00:02:01.470
pixel within this image is
composed of three 8-bit pixels.

25
00:02:01.470 --> 00:02:06.130
These pixels are composed of three
elements, red, green, and blue.

26
00:02:06.130 --> 00:02:09.410
We can extract or
view the individual pixel elements

27
00:02:09.410 --> 00:02:13.110
at a specific location in the image
by using the pixel.py script.

28
00:02:14.440 --> 00:02:21.240
We can run pixel.py Australia.jpg 0
0 to see the value at one location.

29
00:02:29.430 --> 00:02:33.430
The 0 0 location is
the corner of the image.

30
00:02:33.430 --> 00:02:39.281
If we go back to the image, the corners
are all the ocean, so they're dark blue.

31
00:02:39.281 --> 00:02:43.995
If we look at the value that was
extracted, we see that blue has a high

32
00:02:43.995 --> 00:02:48.490
value of 50, whereas red and
green are low with 11 and 10.

33
00:02:48.490 --> 00:02:51.991
If we view it at another corner,
by looking at 5000 0,

34
00:02:51.991 --> 00:02:53.610
we'll see the same value.

35
00:03:01.136 --> 00:03:05.679
If we go back to the image, the middle
of the image, which is the land,

36
00:03:05.679 --> 00:03:07.070
is orange or yellow.

37
00:03:07.070 --> 00:03:12.261
It's definitely not blue, so
let's take a look at a pixel value there.

38
00:03:12.261 --> 00:03:17.010
Okay, run pixel.py
Australia.jpg 2000 2000.

39
00:03:23.750 --> 00:03:31.070
This says that the red has a value of 118,
green is 89, and the blue is 57.

40
00:03:31.070 --> 00:03:34.130
So the red and green are higher than blue,
so it's not ocean.
WEBVTT

1
00:00:02.370 --> 00:00:05.969
This is the second hands on exercises for
CSV data.

2
00:00:05.969 --> 00:00:10.155
In the first, we saw how to import
a CSV file into a spreadsheet and

3
00:00:10.155 --> 00:00:12.430
make a simple plot.

4
00:00:12.430 --> 00:00:15.240
In this one,
we will learn how to filter data and

5
00:00:15.240 --> 00:00:16.830
perform some aggregate operations.

6
00:00:18.390 --> 00:00:21.659
We will begin by opening a terminal
window and starting a spreadsheet.

7
00:00:22.800 --> 00:00:26.440
Next, we will load the CSV
data into the spreadsheet and

8
00:00:26.440 --> 00:00:29.490
perform a filter over several columns.

9
00:00:29.490 --> 00:00:32.398
Finally, we will calculate an average and
sum from the data.

10
00:00:32.398 --> 00:00:35.313
Let's begin.

11
00:00:35.313 --> 00:00:39.470
First, we open a terminal window by
clicking on the terminal icon in

12
00:00:39.470 --> 00:00:40.290
the top toolbar.

13
00:00:42.770 --> 00:00:46.296
We can start the spreadsheet
by writing oocalc.

14
00:00:51.338 --> 00:00:54.590
Next, let's load our CSV
data into the spreadsheet.

15
00:00:54.590 --> 00:01:02.142
We click on File > Open, CENSUS.CSV.

16
00:01:05.719 --> 00:01:08.426
And click OK on this
dialog to load the data.

17
00:01:12.387 --> 00:01:17.344
Column F in the spreadsheet is the state,
and

18
00:01:17.344 --> 00:01:22.704
column H is the census population for
2010.

19
00:01:25.110 --> 00:01:28.480
Let's create a filter that
just shows the data for

20
00:01:28.480 --> 00:01:32.920
California, for
counties larger than one million people.

21
00:01:34.100 --> 00:01:39.030
We can create this filter by first
selecting both the state name column and

22
00:01:39.030 --> 00:01:41.210
the census 2000 population column.

23
00:01:42.790 --> 00:01:48.054
Next we go to data,
filter, standard filter.

24
00:01:52.184 --> 00:01:56.025
Here we change the field
name to be the state name,

25
00:01:56.025 --> 00:02:01.061
the condition we leave at equals and
the value we use California.

26
00:02:06.318 --> 00:02:11.055
This filters all the rows,
unless the state is California.

27
00:02:15.143 --> 00:02:16.498
We then want to filter for

28
00:02:16.498 --> 00:02:20.590
all the counties whose population
is greater than one million people.

29
00:02:20.590 --> 00:02:24.400
To do that, in this second line here,
we change the operator to AND.

30
00:02:26.180 --> 00:02:31.710
The field name should be
census 2010 population and

31
00:02:31.710 --> 00:02:35.040
the condition should be greater than.

32
00:02:35.040 --> 00:02:36.431
Then set the value to be one million.

33
00:02:41.923 --> 00:02:44.893
And click OK.

34
00:02:44.893 --> 00:02:49.603
We can see that all the data from the
spreadsheet has disappeared except where

35
00:02:49.603 --> 00:02:51.559
the state name is California and

36
00:02:51.559 --> 00:02:54.762
the population is greater
than one million people.

37
00:02:56.310 --> 00:03:00.110
We can reset or remove this
filter to see all the data again

38
00:03:00.110 --> 00:03:04.240
by going to Data > Filter.

39
00:03:06.260 --> 00:03:07.020
Reset filter.

40
00:03:08.390 --> 00:03:11.550
You can perform aggregate operations
on the data in a spreadsheet.

41
00:03:13.160 --> 00:03:16.830
Next, let's perform some aggregate
operations over the data.

42
00:03:16.830 --> 00:03:19.220
We can compute the average and
the sum of the data.

43
00:03:20.460 --> 00:03:23.980
To do this, let's run these
calculations in a separate sheet.

44
00:03:25.470 --> 00:03:28.181
Create a new sheet by clicking
on the green plus button.

45
00:03:31.854 --> 00:03:37.656
To compute the average we select
a cell and enter =Average( and

46
00:03:37.656 --> 00:03:43.470
then we select the data that we
want to compute the average from.

47
00:03:44.870 --> 00:03:46.150
If we go back to sheet one.

48
00:03:46.150 --> 00:03:50.190
We can select some of
the data from the H column.

49
00:03:50.190 --> 00:03:54.634
So let's just choose several
counties in Alabama.

50
00:03:54.634 --> 00:03:58.267
When we hit Enter,
it takes us back to sheet one and

51
00:03:58.267 --> 00:04:01.306
we can see that the average is computing.

52
00:04:03.513 --> 00:04:12.220
Similarly, we can compute the sum
by entering =sum, open parentheses.

53
00:04:12.220 --> 00:04:16.120
Going back to sheet one and
selecting the columns we want to sum.

54
00:04:16.120 --> 00:04:21.360
When we're done hit Enter and
the sum is computed.
WEBVTT

1
00:00:02.360 --> 00:00:05.130
In this hands on exercise,
we will be looking at JSON data.

2
00:00:07.040 --> 00:00:09.600
First, we will open a terminal window and

3
00:00:09.600 --> 00:00:13.300
then look at the contents of a JSON
file containing tweets from Twitter.

4
00:00:14.860 --> 00:00:17.800
Next, we will examine
the schema of this JSON file.

5
00:00:18.810 --> 00:00:21.920
Finally, we will extract different
fields from the JSON data.

6
00:00:24.340 --> 00:00:25.470
Let's begin.

7
00:00:25.470 --> 00:00:27.307
First, we'll open a terminal window,

8
00:00:27.307 --> 00:00:29.964
by clicking on the terminal
icon at the top of the toolbar.

9
00:00:37.240 --> 00:00:42.005
Next, we'll see cd into the directory
containing the json data,

10
00:00:42.005 --> 00:00:45.594
by running cddownload/big/data/2/json.

11
00:00:52.298 --> 00:00:54.520
We can run ls, to see the json file.

12
00:00:58.836 --> 00:01:04.030
The json file is called twitter.json.

13
00:01:04.030 --> 00:01:07.980
We can run more twitter.json to
view the contents of this file.

14
00:01:16.280 --> 00:01:20.990
The json data contains semi-structured
data, which is nested several levels.

15
00:01:20.990 --> 00:01:25.350
There are many tweets in this file, and
it's hard to read using the more command.

16
00:01:25.350 --> 00:01:29.690
You can use space to scroll down and
when we're done, hit q.

17
00:01:32.420 --> 00:01:37.358
We can run the jsonschema.pi command
to view the schema of this data.

18
00:01:37.358 --> 00:01:42.179
We run jsonschema.pitwitter.json

19
00:01:49.715 --> 00:01:52.670
And we'll add a pipe more at the end.

20
00:01:56.665 --> 00:02:00.572
This shows the nested
fields within this data,

21
00:02:00.572 --> 00:02:06.139
at the top level there are fields
like contributors text and id and so

22
00:02:06.139 --> 00:02:11.900
on, but there are also fields nested
within these top level fields for

23
00:02:11.900 --> 00:02:17.372
example entities also contains
called symbols media hashtags and

24
00:02:17.372 --> 00:02:20.691
so on If we scroll down by hitting space,

25
00:02:20.691 --> 00:02:25.270
we'll see that there's
several levels of nesting.

26
00:02:25.270 --> 00:02:32.320
For example, user also has
follow_request_sent, id, and so on.

27
00:02:36.105 --> 00:02:40.466
We can run the print json script to view
the contents of a particular tweet and

28
00:02:40.466 --> 00:02:42.746
a particular field within that tweet.

29
00:02:42.746 --> 00:02:47.018
Let's run print_json.py.

30
00:02:47.018 --> 00:02:50.812
It asks for the file name so

31
00:02:50.812 --> 00:02:55.300
we'll enter twitter.json.

32
00:02:58.407 --> 00:03:01.434
And we'll look at tweet 99.

33
00:03:03.645 --> 00:03:06.340
So let's look at the top
level field called text.

34
00:03:10.506 --> 00:03:14.884
So here we see the text for
note the 99th tweet in this file.

35
00:03:14.884 --> 00:03:19.110
We could also look at a nested field
within the file, by running print_json

36
00:03:19.110 --> 00:03:24.276
again The file name is twitter.json.

37
00:03:26.975 --> 00:03:28.833
We'll look at tweet 99 again.

38
00:03:32.097 --> 00:03:35.240
And we'll look at the field
entities hashtags.

39
00:03:35.240 --> 00:03:39.565
The hashtags that are embedded or
nested within the entities field.
WEBVTT

1
00:00:02.390 --> 00:00:08.427
This is the first of two hands
on exercises involving CSV Data.

2
00:00:08.427 --> 00:00:12.568
In this exercise, we will import
a CSV file into a spreadsheet and

3
00:00:12.568 --> 00:00:13.860
make a simple plot.

4
00:00:15.290 --> 00:00:20.350
We will begin by opening a terminal window
and looking at a CSV file in the terminal.

5
00:00:20.350 --> 00:00:23.330
Next, we will start
the spreadsheet application and

6
00:00:23.330 --> 00:00:25.410
import the CSV data into the spreadsheet.

7
00:00:25.410 --> 00:00:30.760
We can then look at the rows and columns
of the CSV file and make a simple plot.

8
00:00:33.088 --> 00:00:35.837
Let's begin, first,
open a terminal shell by

9
00:00:35.837 --> 00:00:39.423
clicking on the black terminal
icon at the top of the toolbar.

10
00:00:47.759 --> 00:00:52.914
Next, let's cd into the directory
containing the CSV data.

11
00:00:52.914 --> 00:00:58.587
We'll run cd space
Download/big-data-2/csv.

12
00:00:58.587 --> 00:01:04.042
We can run ls to

13
00:01:04.042 --> 00:01:11.325
see the CSV files.

14
00:01:11.325 --> 00:01:16.190
The file census.csv contains
census data for the United States.

15
00:01:16.190 --> 00:01:21.367
We can run the command more census.csv
to see the contents of this file.

16
00:01:29.262 --> 00:01:32.925
The first line of this file is the header
with the columns separated by commas.

17
00:01:32.925 --> 00:01:37.705
You can go down in
the file by hitting space.

18
00:01:41.295 --> 00:01:42.735
Hit Q to quit more.

19
00:01:45.570 --> 00:01:48.210
Next, let's start
a spreadsheet application.

20
00:01:48.210 --> 00:01:52.446
We run oocalc to start this.

21
00:02:00.318 --> 00:02:04.453
We can import the census
data CSV file into

22
00:02:04.453 --> 00:02:08.598
the spreadsheet by going to File > Open.

23
00:02:12.568 --> 00:02:16.067
Clicking on downloads.

24
00:02:16.067 --> 00:02:22.468
Big Data 2, CSV, census.csv.

25
00:02:27.531 --> 00:02:29.109
In this dialog click OK.

26
00:02:44.945 --> 00:02:47.355
You can see into this spreadsheet,

27
00:02:47.355 --> 00:02:50.900
import of the CSV data to
a bunch of rows and columns.

28
00:02:52.880 --> 00:02:56.460
Each column that was separated
by a comma in the CSV file,

29
00:02:56.460 --> 00:02:58.580
is a column in the spreadsheet.

30
00:02:59.700 --> 00:03:02.590
We can see that our CSV file
was successfully imported into

31
00:03:02.590 --> 00:03:03.250
the spreadsheet.

32
00:03:05.650 --> 00:03:07.892
If we scroll down to
the bottom of the spreadsheet,

33
00:03:07.892 --> 00:03:10.094
we can see how many rows
there were in the CSV file.

34
00:03:19.939 --> 00:03:25.372
There are 3194 rows in the CSV file.

35
00:03:25.372 --> 00:03:29.149
If this file instead had millions or
10 millions of rows,

36
00:03:29.149 --> 00:03:33.237
then we would have to use a big
data system such as Hadoop or HDFS.

37
00:03:36.882 --> 00:03:38.089
Let's scroll back to the top.

38
00:03:43.592 --> 00:03:49.351
Next, let's make a simple plot of
some of the data in the CSV file.

39
00:03:49.351 --> 00:03:54.993
Let's plot the population estimates for
several years for the state of Alabama.

40
00:03:54.993 --> 00:04:00.558
The state of Alabama is
given in the second row and

41
00:04:00.558 --> 00:04:06.813
the population estimates
are given in these columns.

42
00:04:06.813 --> 00:04:12.225
Let's select J through O,
so you get the population

43
00:04:12.225 --> 00:04:16.657
estimate for 2010 through 2015.

44
00:04:20.086 --> 00:04:24.331
We can create a plot of these values
by clicking on the Chart button.

45
00:04:29.393 --> 00:04:32.806
And clicking finish.

46
00:04:32.806 --> 00:04:36.442
In the second hands on for CSV data,
we'll perform some filtering and

47
00:04:36.442 --> 00:04:38.669
some aggregate operations over the data.
WEBVTT

1
00:00:02.170 --> 00:00:02.670
Welcome.

2
00:00:03.800 --> 00:00:06.200
In this module,
we'll talk about data models.

3
00:00:07.530 --> 00:00:10.770
If you completed the introductory
course of this specialization,

4
00:00:10.770 --> 00:00:12.940
you might recall our
video on data variety.

5
00:00:14.220 --> 00:00:16.840
One way to characterize data variety

6
00:00:16.840 --> 00:00:21.290
is to identify the different models of
data that are used in any application.

7
00:00:23.120 --> 00:00:24.480
So what is a data model?

8
00:00:24.480 --> 00:00:27.980
And why do we care about data
models in the context of big data?

9
00:00:27.980 --> 00:00:32.010
In this lesson, we'll introduce you to
three components of a data model and

10
00:00:32.010 --> 00:00:33.380
what they tell us about the data.

11
00:00:34.550 --> 00:00:39.620
So after this lesson, you'll be able
to distinguish between structured and

12
00:00:39.620 --> 00:00:41.270
unstructured data.

13
00:00:41.270 --> 00:00:46.880
Describe four basic data operations namely
selection, projection, union, and join.

14
00:00:46.880 --> 00:00:51.120
And enumerate different types of data
constraints like type, value and

15
00:00:51.120 --> 00:00:53.240
structural constraints.

16
00:00:53.240 --> 00:00:57.760
You'll also be able to explain why
constraints are useful to specify

17
00:00:57.760 --> 00:01:00.320
the semantics of data.

18
00:01:00.320 --> 00:01:04.980
Now, regardless of whether the data is big
or small, one needs to know or determine

19
00:01:04.980 --> 00:01:10.100
the characteristics of data before one can
manipulate or analyze them meaningfully.

20
00:01:10.100 --> 00:01:14.320
Let's use a simple example,
suppose you have data is

21
00:01:14.320 --> 00:01:19.330
a file of records with fields called first
name, last name and date of birth of

22
00:01:19.330 --> 00:01:24.840
the employees in the company that this
file consists of records with fields,

23
00:01:24.840 --> 00:01:29.290
and not for instance plain text,
gives us more insight

24
00:01:29.290 --> 00:01:34.770
into the organization of the data in the
file and hence is part of the data model.

25
00:01:34.770 --> 00:01:37.810
This aspect is called Structure.

26
00:01:37.810 --> 00:01:41.930
Similarly, the consideration
that we can perform

27
00:01:41.930 --> 00:01:44.980
data arithmetic with
the date of birth field, and

28
00:01:44.980 --> 00:01:50.340
not with the first name field, is also
part of our understanding of data model.

29
00:01:50.340 --> 00:01:52.620
These are called Operations.

30
00:01:52.620 --> 00:01:57.300
Finally, we may know that in
this company no one's age

31
00:01:57.300 --> 00:02:00.950
that is today's date minus the date
of birth, can't be less than 18.

32
00:02:00.950 --> 00:02:06.000
So it gives us a way to detect records
with blatantly erroneous dates of birth.

33
00:02:06.000 --> 00:02:07.940
In the following three videos,

34
00:02:07.940 --> 00:02:11.790
we'll look at these three aspects
of data models more carefully.
WEBVTT

1
00:00:01.120 --> 00:00:05.140
We mentioned before that a data model
is characterized by the structure of

2
00:00:05.140 --> 00:00:09.430
the data that it admits,
the operations on that structure, and

3
00:00:09.430 --> 00:00:11.090
a way to specify constraints.

4
00:00:12.290 --> 00:00:13.750
In this lesson,

5
00:00:13.750 --> 00:00:18.950
we'll present a more detailed description
of a number of common data models.

6
00:00:18.950 --> 00:00:20.800
We'll start with relational data.

7
00:00:22.460 --> 00:00:26.780
It is one of the simplest and most
frequently used data models today, and

8
00:00:26.780 --> 00:00:30.320
forms the basis of many other
traditional database management systems,

9
00:00:30.320 --> 00:00:33.930
like MySQL, Oracle,
Teradata, and so forth.

10
00:00:35.480 --> 00:00:38.940
So after this video you'll be able to

11
00:00:38.940 --> 00:00:41.680
describe the structural components
of a relational data model.

12
00:00:43.320 --> 00:00:46.430
Demonstrate which components
become a data model's schema.

13
00:00:48.760 --> 00:00:51.760
Explain the purpose of primary and
foreign keys.

14
00:00:52.770 --> 00:00:55.370
And describe Join and other operations.

15
00:00:58.070 --> 00:00:59.610
The primary data structure for

16
00:00:59.610 --> 00:01:03.380
a relational model is a table, like
the one shown here for a toy application.

17
00:01:05.030 --> 00:01:09.740
But we need to be careful about relational
tables, which are also called relations.

18
00:01:11.110 --> 00:01:15.695
This table actually
represents a set of tuples.

19
00:01:17.550 --> 00:01:22.390
This is a relational tuple,
represented as a row in the table.

20
00:01:23.500 --> 00:01:26.140
We were informally calling
this a record before.

21
00:01:27.350 --> 00:01:33.110
But a relational tuple implies that unless
otherwise stated, the elements of it

22
00:01:33.110 --> 00:01:37.600
like 203 or 204, Mary and
so forth, are atomic.

23
00:01:38.950 --> 00:01:43.300
That is,
they represent one unit of information and

24
00:01:43.300 --> 00:01:44.740
cannot be decomposed further.

25
00:01:45.830 --> 00:01:48.150
We'll return to this issue
in the next few slides.

26
00:01:49.490 --> 00:01:53.935
Thus, this is a relation of six tuples.

27
00:01:55.468 --> 00:01:58.490
Remember, the definition of sets,

28
00:01:58.490 --> 00:02:03.270
it's a collection of distinct
elements of the same type.

29
00:02:04.420 --> 00:02:10.920
That means I cannot add
this tuple to the solution.

30
00:02:10.920 --> 00:02:14.350
Because if I do,
it will be introducing a duplicate.

31
00:02:15.820 --> 00:02:21.330
Now in practice, many systems will allow
duplicate tuples in a relation, but

32
00:02:21.330 --> 00:02:26.450
mechanisms are provided to prevent
duplicate entries if the user so chooses.

33
00:02:26.450 --> 00:02:28.170
So I cannot add it.

34
00:02:28.170 --> 00:02:31.170
Here is another tuple I cannot add.

35
00:02:31.170 --> 00:02:33.680
It has all the right
pieces of information, but

36
00:02:33.680 --> 00:02:35.410
it's all in the wrong order.

37
00:02:35.410 --> 00:02:39.300
So it is a tuple dissimilar with
the other six tuples in the relation.

38
00:02:40.300 --> 00:02:43.690
Okay, so how does the system know
that this tuple is different?

39
00:02:45.390 --> 00:02:49.700
This brings our attention to the very
first row that is the header of this table

40
00:02:49.700 --> 00:02:50.710
painted in black.

41
00:02:52.680 --> 00:02:55.910
This row is part of
the scheme of the table.

42
00:02:55.910 --> 00:02:56.420
Lets look at it.

43
00:02:57.890 --> 00:03:01.190
It tells us the name of the table,
in this case employee.

44
00:03:02.580 --> 00:03:06.560
This also tells us the names of the six
columns called attributes of the relation.

45
00:03:07.740 --> 00:03:09.090
And for each column,

46
00:03:09.090 --> 00:03:13.370
it tells us the allowed data type, that
is the type constraint for each column.

47
00:03:14.610 --> 00:03:16.340
Given this schema,

48
00:03:16.340 --> 00:03:20.890
it should now be clear why the last
red row does not belong to this table.

49
00:03:21.940 --> 00:03:26.180
The schema in a relational table
can also specify constraints,

50
00:03:27.330 --> 00:03:30.150
shown in yellow in the third
line of the schema row.

51
00:03:31.720 --> 00:03:36.050
It says that the minimum salary of
a person has to be greater than 25k.

52
00:03:38.150 --> 00:03:44.670
Further, it states that every employee
must have a first and last name.

53
00:03:44.670 --> 00:03:46.660
They cannot be left null,
that means without a value.

54
00:03:48.840 --> 00:03:51.900
Why doesn't department or
title column have this constraint?

55
00:03:53.500 --> 00:03:57.990
One answer can be that a newly
hired employee may not be assigned

56
00:03:57.990 --> 00:04:01.350
a department or a title yet but
can still be an entry in the table.

57
00:04:02.560 --> 00:04:06.040
However, the department column
has another constraint.

58
00:04:07.130 --> 00:04:11.390
It restricts the possible values
that is the domain of the attribute

59
00:04:11.390 --> 00:04:14.060
to only four possibilities.

60
00:04:14.060 --> 00:04:17.900
HR, IT, research, and business.

61
00:04:19.620 --> 00:04:24.840
Finally, the first says
that ID is a primary key.

62
00:04:26.180 --> 00:04:29.960
This means it is unique for each employee.

63
00:04:29.960 --> 00:04:33.200
And for
every employee knowing the primary key for

64
00:04:33.200 --> 00:04:38.720
the employee will also uniquely know the
other five attributes of that employee.

65
00:04:40.180 --> 00:04:43.240
You should now see that
a table with a primary key

66
00:04:43.240 --> 00:04:48.590
logically implies that the table cannot
have a duplicate record because if we do,

67
00:04:48.590 --> 00:04:51.870
it will violate the uniqueness constraint
associated with the primary key.

68
00:04:53.330 --> 00:04:57.140
Let us introduce a new table containing
the salary history of employees.

69
00:04:58.410 --> 00:05:02.010
The employees are identified
with the column EmpID, but

70
00:05:02.010 --> 00:05:04.760
these are not new values that
this table happens to have.

71
00:05:05.900 --> 00:05:10.460
They are the same IDs that are present in
the ID column of the employee's table,

72
00:05:10.460 --> 00:05:11.140
presented earlier.

73
00:05:13.420 --> 00:05:15.570
This is reflected in
the statement made on the right.

74
00:05:16.760 --> 00:05:23.530
The term References means,
the values in this column can exist

75
00:05:23.530 --> 00:05:28.760
only if the same values if you are in
employees the table being referenced,

76
00:05:28.760 --> 00:05:30.700
also called the parent table.

77
00:05:32.150 --> 00:05:38.920
So in the terminology of the relational
model, the EmpID column of EmpSalaries

78
00:05:38.920 --> 00:05:44.660
table is called a foreign key that refers
to the primary key of the Employees table.

79
00:05:46.560 --> 00:05:52.650
Note that EmpID is not a primary
key in this EmpSalaries table.

80
00:05:52.650 --> 00:05:56.420
Because it is multiple to
post with the same EmpID

81
00:05:56.420 --> 00:05:58.920
reflecting the salary of
the employee at different times.

82
00:06:00.700 --> 00:06:04.070
You will remember join is a common
operation that we discussed before.

83
00:06:05.150 --> 00:06:09.200
So here is an example of
a relational join performed

84
00:06:09.200 --> 00:06:13.810
on the first three columns of employee and
EmpSalaries table.

85
00:06:13.810 --> 00:06:20.630
Where employees.ID, and EmpSalaries.EmpID
columns are matched for equality.

86
00:06:21.770 --> 00:06:24.130
The output table shows
all the columns involved.

87
00:06:25.360 --> 00:06:27.360
The common column is represented once.

88
00:06:28.680 --> 00:06:31.210
This form of join is
called a Natural Join.

89
00:06:32.690 --> 00:06:37.770
It is important to understand that
join is one of the most expensive

90
00:06:37.770 --> 00:06:41.570
that means time consuming and
space consuming operations.

91
00:06:42.770 --> 00:06:48.270
As data becomes larger, and tables contain
hundreds of millions of tuples, the join

92
00:06:48.270 --> 00:06:52.480
operation can easily become a bottleneck
in a larger analytic application.

93
00:06:53.880 --> 00:06:59.280
So for analytical big data application
that needs joins, it's very important to

94
00:06:59.280 --> 00:07:04.890
choose a suitable data management platform
that makes this operation efficient.

95
00:07:04.890 --> 00:07:07.090
We will return to this
issue in module four.

96
00:07:08.760 --> 00:07:10.600
We end this video on a practical note.

97
00:07:11.830 --> 00:07:13.060
In many scientific and

98
00:07:13.060 --> 00:07:16.990
business applications,
people start with CSV files,

99
00:07:16.990 --> 00:07:21.650
manipulate them with the spreadsheet,
then migrate their relational system only

100
00:07:21.650 --> 00:07:25.790
as an afterthought where the data becomes
too large to handle the spreadsheet.

101
00:07:27.490 --> 00:07:30.500
While the spreadsheet offers
many useful features.

102
00:07:30.500 --> 00:07:36.037
It does not conform and enforce many
principles of relational data models.

103
00:07:37.700 --> 00:07:42.210
Consequently, a large amount of time
may be spent in cleaning up and

104
00:07:42.210 --> 00:07:45.480
correcting data errors after
the migration actually happens.

105
00:07:46.670 --> 00:07:51.860
Let me show a few examples from
a spreadsheet that has 125,000 rows and

106
00:07:51.860 --> 00:07:52.430
over 100 columns.

107
00:07:53.950 --> 00:07:59.280
The spreadsheet here lists terrorism
attacks gathered from news media.

108
00:07:59.280 --> 00:08:02.030
So each row represents one attack.

109
00:08:03.080 --> 00:08:07.210
This is a valuable piece of data for
people who study terrorism.

110
00:08:07.210 --> 00:08:10.420
But we are going to look at it from
a relational data modelling viewpoint.

111
00:08:12.200 --> 00:08:15.160
First, notice the column marked in green.

112
00:08:16.750 --> 00:08:20.900
It lists two weapons used in
the attack separated by a semicolon.

113
00:08:22.330 --> 00:08:24.390
Why is this really common?

114
00:08:24.390 --> 00:08:26.950
It makes this column non-atomic.

115
00:08:26.950 --> 00:08:31.450
It means that this column actually
has two different values.

116
00:08:32.570 --> 00:08:37.140
In a relational design, this information
will be moved to another table

117
00:08:37.140 --> 00:08:40.870
just like the multiple salaries of
employees were placed in a separate table.

118
00:08:42.100 --> 00:08:45.740
Next, notice the column outlined in red.

119
00:08:47.290 --> 00:08:52.370
It describes the amount of property
damaged by a possible terrorist attack.

120
00:08:53.890 --> 00:08:59.880
In this column, the intended
legitimate values are unknown,

121
00:08:59.880 --> 00:09:01.540
minor, major and catastrophic.

122
00:09:02.670 --> 00:09:07.350
However, the value in the highlighted
part of the spreadsheet is minor and

123
00:09:07.350 --> 00:09:12.012
then within bracket,
likely less than $1 million.

124
00:09:12.012 --> 00:09:16.700
Which means a query like
find all attacks for

125
00:09:16.700 --> 00:09:20.390
which the property damage is equal to
minor cannot be answered directly.

126
00:09:21.680 --> 00:09:25.150
Instead, we need to perform
a substring search for

127
00:09:25.150 --> 00:09:27.630
minor in the beginning of the description.

128
00:09:27.630 --> 00:09:30.420
Which is doable, but
it's a more expensive operation.

129
00:09:31.960 --> 00:09:35.730
This shows the columns of the spreadsheet.

130
00:09:35.730 --> 00:09:37.950
So this is part of the schema of the data.

131
00:09:39.120 --> 00:09:42.310
If you observe carefully you
will see a recurring pattern.

132
00:09:44.120 --> 00:09:49.290
The designer of the data table determined
that there can be at most three types of

133
00:09:49.290 --> 00:09:54.380
attacks within a single encounter and
represented with three separate columns.

134
00:09:55.790 --> 00:10:01.220
Now in proper relational modeling,
one would say that there is a one to many

135
00:10:01.220 --> 00:10:06.650
relationship between the attack and
the number of attack types.

136
00:10:08.210 --> 00:10:10.980
In such a case, it would be more prudent

137
00:10:10.980 --> 00:10:15.030
to place these attack type
columns in a separate table and

138
00:10:15.030 --> 00:10:19.480
connect with the parent using a primary
key, foreign key relationship.

139
00:10:20.920 --> 00:10:24.240
Here's another block,
with a similar pattern,

140
00:10:25.490 --> 00:10:29.420
this time this is about the types and
subtypes of weapons used.

141
00:10:30.620 --> 00:10:33.690
Now can you determine how you might
be able to reorganize this block?

142
00:10:35.130 --> 00:10:36.761
We'll leave this as an exercise.
WEBVTT

1
00:00:03.030 --> 00:00:05.956
It can be said without a doubt,
and the Internet and

2
00:00:05.956 --> 00:00:08.960
the worldwide web changed
everything in our lives.

3
00:00:10.450 --> 00:00:15.100
The worldwide web is indeed the largest
information source there is today.

4
00:00:15.100 --> 00:00:16.630
But what's the data model behind the web?

5
00:00:17.800 --> 00:00:20.740
We will say that it is
the semi-structure data model.

6
00:00:22.310 --> 00:00:26.560
So after going through this video you
will be able to distinguish between

7
00:00:26.560 --> 00:00:29.870
the structured data model that we
talked about the last time and

8
00:00:29.870 --> 00:00:32.805
semi-structured data model.

9
00:00:32.805 --> 00:00:36.110
Further, you will recognize that the most

10
00:00:36.110 --> 00:00:40.680
times the semi-structured data
refers to tree structured data.

11
00:00:41.720 --> 00:00:47.160
And you can explain why tree
navigation operations are important for

12
00:00:47.160 --> 00:00:49.370
formats like XML and JSON.

13
00:00:51.050 --> 00:00:53.290
Let's a take a very simple web page.

14
00:00:55.090 --> 00:00:58.740
Now this page does not have
a lot of content or stylization.

15
00:00:58.740 --> 00:01:01.970
It doesn't even have links to other pages,
but

16
00:01:01.970 --> 00:01:04.390
let's look at the corresponding HTML code.

17
00:01:06.560 --> 00:01:11.340
This code is used by the browser so
that it can render the HTML, and

18
00:01:11.340 --> 00:01:13.580
notice a few things in this data.

19
00:01:13.580 --> 00:01:19.710
The entire data comes within the HTML and
slash HTML blocks.

20
00:01:21.760 --> 00:01:28.590
And we similarly have a body begin and
end, a header begin and

21
00:01:28.590 --> 00:01:33.950
end, a list begin and end and
a paragraph begin and end.

22
00:01:36.020 --> 00:01:40.330
Everywhere here a block is
nested within a larger block.

23
00:01:41.770 --> 00:01:46.350
The second item to notice is that
unlike a relational structure

24
00:01:46.350 --> 00:01:49.940
there are multiple list items and
multiple paragraphs.

25
00:01:49.940 --> 00:01:53.040
And any single document would
have a different number of them.

26
00:01:54.100 --> 00:01:59.250
This means while the date object has
some structure it is more flexible.

27
00:02:00.360 --> 00:02:05.930
So this is the hallmark office
semi structure date model.

28
00:02:05.930 --> 00:02:07.020
Now XML, or

29
00:02:07.020 --> 00:02:12.100
the extensible markup language, is another
well known standard to represent data.

30
00:02:12.100 --> 00:02:16.880
You can think of XML as a generalization
of HTML where the elements, that's

31
00:02:16.880 --> 00:02:21.270
the beginning and end markers within
the angular brackets, can be any string.

32
00:02:21.270 --> 00:02:24.240
And not like the ones
allowed by standard HTML.

33
00:02:25.530 --> 00:02:28.840
Let's see an example
from a biological case.

34
00:02:30.140 --> 00:02:34.770
As you can see, there are two
elements called sample attribute.

35
00:02:36.100 --> 00:02:39.130
They do structurally
different because they

36
00:02:39.130 --> 00:02:42.320
have different numbers of sub
elements called the value.

37
00:02:44.030 --> 00:02:46.960
Another interesting issue
about XML data processing

38
00:02:46.960 --> 00:02:50.530
is that you can actually credit for
the structure elements.

39
00:02:50.530 --> 00:02:55.560
For example, it is perfectly fine to ask,
what is the name of the element

40
00:02:55.560 --> 00:02:59.250
which contains a sub-element whose
textual content is cell type?

41
00:03:00.390 --> 00:03:06.990
As you can see, you'll get two results,
sample attribute.

42
00:03:06.990 --> 00:03:11.604
An experimental factor because sample
attribute has a sub-element called

43
00:03:11.604 --> 00:03:15.940
category and experimental factor
has a subelement called link and

44
00:03:15.940 --> 00:03:18.860
each of these subelements
have the value celltape.

45
00:03:20.090 --> 00:03:23.860
Now we cannot perform an operation
like this in a relational data model.

46
00:03:23.860 --> 00:03:28.090
For example, we cannot say which relation
has a column with a value, John.

47
00:03:30.730 --> 00:03:35.740
The same idea can also be seen in JSON or
the Java Script Object Notation, which

48
00:03:35.740 --> 00:03:41.610
is a very popular format used for many
different data like Twitter and Facebook.

49
00:03:41.610 --> 00:03:47.170
Consider the example here,
all of the format looks different.

50
00:03:47.170 --> 00:03:50.390
We have a similar nested
structure varies that is lists

51
00:03:50.390 --> 00:03:55.030
containing other lists which will contain
topples Which consists of p value ps.

52
00:03:57.080 --> 00:04:01.160
So the key value pairs at atomic
property names and their values.

53
00:04:02.660 --> 00:04:07.760
But one way to generalize about all these
different forms of semi structured data

54
00:04:07.760 --> 00:04:10.460
is to model them as trees.

55
00:04:10.460 --> 00:04:12.380
Let's go back to .xml.

56
00:04:12.380 --> 00:04:15.210
The left side shows an XML document, and

57
00:04:15.210 --> 00:04:17.440
the right side shows
the corresponding tree.

58
00:04:18.660 --> 00:04:22.030
Since the top object of
the root element is document,

59
00:04:22.030 --> 00:04:23.230
it is also the root of the tree.

60
00:04:24.550 --> 00:04:28.620
Now under document we have
a report element with author and

61
00:04:28.620 --> 00:04:34.450
date under it, and also a paper element
with title, author, and source under it.

62
00:04:34.450 --> 00:04:38.670
The actual values,
like is the textual content of an element.

63
00:04:40.480 --> 00:04:43.760
Since a text data item cannot
have any further components,

64
00:04:43.760 --> 00:04:46.590
these text values are always
the leaves of the tree.

65
00:04:48.760 --> 00:04:53.950
Now, modeling a document as a tree
has significant advantages.

66
00:04:53.950 --> 00:04:55.740
A tree is a well-known data structure,

67
00:04:55.740 --> 00:05:00.100
that allows what's called
a navigational access to data.

68
00:05:00.100 --> 00:05:02.150
Imagine you are standing
on the note paper.

69
00:05:03.460 --> 00:05:08.610
Now you can perform a getParent
operation and navigate the document.

70
00:05:08.610 --> 00:05:14.460
Or you can perform a getChildren operation
to get to the title, author and source.

71
00:05:14.460 --> 00:05:18.550
You can even perform a getSiblings
operation and get to the report.

72
00:05:19.810 --> 00:05:24.970
You can also ask a textual query like
which strings have the substring data and

73
00:05:24.970 --> 00:05:31.340
seek their root-to-node path to get to
the path from document to the text nodes.

74
00:05:31.340 --> 00:05:34.590
You can possibly see how queries
can be evaluated on the tree,

75
00:05:35.670 --> 00:05:37.250
now let us take the query.

76
00:05:37.250 --> 00:05:39.990
Who is the author of XML query data model.

77
00:05:41.900 --> 00:05:47.987
In one evaluation scheme we can navigate
up from the text note to title,

78
00:05:47.987 --> 00:05:53.580
to paper, and then navigate down
to author and then to Don Robie.

79
00:05:53.580 --> 00:05:56.990
Well how do we know that we have to get up
to paper before reversing the direction?

80
00:05:58.050 --> 00:06:01.470
Well, paper is the least,
that's the lowest in the tree,

81
00:06:01.470 --> 00:06:07.170
common ancestor of the author note,
and the XM query data model note.

82
00:06:07.170 --> 00:06:10.207
We will come back to semi
structure data in a later module.
WEBVTT

1
00:00:01.851 --> 00:00:05.973
In this hands-on activity,
we will be looking at graph data in Gephi.

2
00:00:05.973 --> 00:00:10.510
First, we will import data into Gephi and
then examine the properties of the graph.

3
00:00:11.720 --> 00:00:15.160
Next, we will perform some statistical
operations on the graph data,

4
00:00:15.160 --> 00:00:17.750
and then run some different
layout algorithms.

5
00:00:20.090 --> 00:00:21.585
Let's begin.

6
00:00:21.585 --> 00:00:26.370
We're not running Gephi in the Cloudera
virtual machine, on the Coursera website,

7
00:00:26.370 --> 00:00:30.450
there will be a reading with instructions
on how to download, install, and

8
00:00:30.450 --> 00:00:34.050
run Gephi on your native hardware,
instead of in the virtual machine.

9
00:00:35.960 --> 00:00:39.693
Once you have Gephi started,
let's import the data into Gephi.

10
00:00:39.693 --> 00:00:46.908
We'll go to File Import spreadsheet,
and in the CSV dialog,

11
00:00:46.908 --> 00:00:52.338
we'll click the button with dot, dot, dot.

12
00:00:52.338 --> 00:00:57.679
We'll choose diseasegraph.csv and
click open.

13
00:01:01.336 --> 00:01:06.420
Make sure that as table says
edges table Click next.

14
00:01:09.570 --> 00:01:13.535
And make sure Create
missing nodes is checked.

15
00:01:13.535 --> 00:01:17.215
We'll click finish to import
the CSV file as a graph.

16
00:01:17.215 --> 00:01:19.630
Gephi now shows the graph
in the center pane.

17
00:01:21.270 --> 00:01:23.930
The little black circles
are the nodes of the graph and

18
00:01:23.930 --> 00:01:26.500
the lines between them are the edges.

19
00:01:26.500 --> 00:01:32.707
In the top right we can see that
there are 777 nodes and 998 edges.

20
00:01:32.707 --> 00:01:36.380
Next let's perform some statistical
operations on this graph.

21
00:01:37.510 --> 00:01:42.145
In the statistics pane we
can see average degree.

22
00:01:42.145 --> 00:01:44.980
Let's compute the average degree
of the graph by clicking on Run.

23
00:01:49.320 --> 00:01:53.260
This says that the average
degree is 2.569.

24
00:01:53.260 --> 00:01:58.693
Let's close this,
let's compute the connected

25
00:01:58.693 --> 00:02:02.583
components, we'll click on Run.

26
00:02:02.583 --> 00:02:05.400
We'll leave this as a directed,
since the graph is directed.

27
00:02:07.140 --> 00:02:09.930
Click OK.
It says that there

28
00:02:09.930 --> 00:02:14.940
are 5 weakly connected components,
and 761 strongly connected components.

29
00:02:16.440 --> 00:02:17.400
Let's close this.

30
00:02:18.470 --> 00:02:21.710
Next, let's run some different
layout algorithms over the graph.

31
00:02:23.240 --> 00:02:25.760
The bottom left,
we'll go to choose layout.

32
00:02:27.550 --> 00:02:32.019
We'll choose Force Atlas and click run.

33
00:02:50.345 --> 00:02:52.080
Click stop to stop the layout.

34
00:02:53.655 --> 00:02:58.200
We can see that Gephi has grouped
strongly connected components together

35
00:02:58.200 --> 00:02:59.320
in different clusters.

36
00:02:59.320 --> 00:03:01.920
We can also see that they're parts
of the graph that are not connected.

37
00:03:03.440 --> 00:03:04.830
Let's run a different layout algorithm.

38
00:03:06.080 --> 00:03:11.395
The combo box,
choose Fruchterman Reingold, click run.

39
00:03:18.234 --> 00:03:23.125
After it runs for a few seconds, click
stop, then click the magnifying glass,

40
00:03:23.125 --> 00:03:25.680
center on graph, to see the whole graph.

41
00:03:28.320 --> 00:03:31.700
In this layout,
all the nodes appear to be equally spaced.

42
00:03:31.700 --> 00:03:34.537
But we can also see
the nodes with many edges.
WEBVTT

1
00:00:01.350 --> 00:00:04.350
In this hands on activity,
we will be working with Lucene,

2
00:00:04.350 --> 00:00:07.310
a search engine that uses a vector
space model to index data.

3
00:00:08.430 --> 00:00:10.580
First, we will open a terminal window and

4
00:00:10.580 --> 00:00:14.200
change into the directory
containing the data and scripts.

5
00:00:14.200 --> 00:00:18.700
Next, we will index some text
documents and query terms in Lucene.

6
00:00:19.960 --> 00:00:22.538
After that we'll query
using weighted terms or

7
00:00:22.538 --> 00:00:25.810
boosting to see how this
changes the rankings.

8
00:00:25.810 --> 00:00:30.045
Finally, we will show the term
frequency-inverse document frequency or

9
00:00:30.045 --> 00:00:32.988
TF-IDF in terms.

10
00:00:32.988 --> 00:00:35.300
Let's begin.

11
00:00:35.300 --> 00:00:39.139
First, let's open a terminal window by
clicking on the terminal icon at the top

12
00:00:39.139 --> 00:00:39.953
of the tool bar.

13
00:00:43.413 --> 00:00:47.090
Next, let's cd into the directory
containing the scripts and data.

14
00:00:47.090 --> 00:00:48.679
We'll run cd

15
00:00:48.679 --> 00:00:58.560
Downloads/big-data-2/vector/.

16
00:00:58.560 --> 00:01:02.201
We'll run ls to see the scripts.

17
00:01:06.543 --> 00:01:09.670
The data directory
contains three text files.

18
00:01:09.670 --> 00:01:13.310
Each of these text files contains
news data about elections.

19
00:01:15.730 --> 00:01:21.497
Let's index these files by
running runLuceneQuery.sh data.

20
00:01:28.974 --> 00:01:35.740
Next, let's query Lucene for some terms in
these text documents and see the rankings.

21
00:01:35.740 --> 00:01:37.970
Let's query for the term voters.

22
00:01:43.242 --> 00:01:47.973
You can see the rankings and
scores that news1.csv ranked first,

23
00:01:47.973 --> 00:01:52.900
news2.csv was the second ranking,
and the third was news3.csv.

24
00:01:52.900 --> 00:01:56.359
Let's query for delegates.

25
00:01:56.359 --> 00:02:01.324
For this term, we see that news2.csv

26
00:02:01.324 --> 00:02:06.288
was first, and news1 was second, and

27
00:02:06.288 --> 00:02:11.260
news3 did not contain the term at all.

28
00:02:13.710 --> 00:02:17.275
Now, let's query for both terms,
voters and delegates.

29
00:02:17.275 --> 00:02:24.935
In this result we see that
news2 was ranked first,

30
00:02:24.935 --> 00:02:29.566
news1 was ranked second, and

31
00:02:29.566 --> 00:02:33.500
news3 was ranked third.

32
00:02:35.230 --> 00:02:37.000
Now lets use query term waiting or

33
00:02:37.000 --> 00:02:40.920
boosting to increase
the relevance of voters.

34
00:02:40.920 --> 00:02:45.900
I can do this by ensuring
voters carat 5 delegates.

35
00:02:45.900 --> 00:02:53.524
The carat 5 notation is a syntax for

36
00:02:53.524 --> 00:02:58.360
Lucene for boosting.

37
00:02:59.890 --> 00:03:05.990
When we run this, we see that now,
news1 is ranked first,

38
00:03:05.990 --> 00:03:10.170
news2 is ranked second,
and news3 is ranked third.

39
00:03:10.170 --> 00:03:14.500
Notice this is different from the original
query with voters and delegates,

40
00:03:14.500 --> 00:03:18.100
where news2 is ranked first and
news1 was ranked second.

41
00:03:20.210 --> 00:03:23.890
Now let's look at the term frequency,
inverse document frequency or TF-IDF.

42
00:03:23.890 --> 00:03:28.465
We'll enter q to quit this and

43
00:03:28.465 --> 00:03:33.960
we'll run Lucene TF-IDF SH data.

44
00:03:45.380 --> 00:03:51.000
Let's look at the TF-IDF for voters.

45
00:03:51.000 --> 00:03:53.320
You can see that it ranked number 1 for
news1.

46
00:03:54.370 --> 00:03:58.160
Second news 2 and news 3 is last.

47
00:03:58.160 --> 00:03:59.170
Lets try delegates.

48
00:04:05.110 --> 00:04:09.040
Here we see that news 2 had
a higher score than news 1,

49
00:04:09.040 --> 00:04:12.850
and news 3 is not listed because
news 3 does not contain this term.

50
00:04:13.870 --> 00:04:14.590
Hit q to quit.
WEBVTT

1
00:00:00.800 --> 00:00:05.529
So the next category of data we discuss
has the form of graphs or networks,

2
00:00:05.529 --> 00:00:08.749
the most obvious example
being social networks.

3
00:00:08.749 --> 00:00:10.782
Now speaking of social networks,

4
00:00:10.782 --> 00:00:15.760
Tim Libzek created a social network
from the Lord of the Rings Trilogy.

5
00:00:15.760 --> 00:00:18.545
This graph represents
the characters' allegiances,

6
00:00:18.545 --> 00:00:20.744
that is who is faithful
to whom in the books.

7
00:00:20.744 --> 00:00:24.670
So the nodes are characters and
other entities, like cities, and

8
00:00:24.670 --> 00:00:29.070
the edges connecting pairs of
nodes represent allegiances.

9
00:00:29.070 --> 00:00:35.480
So after this video, you'll be able to
identify graph data in practical problems

10
00:00:35.480 --> 00:00:41.150
and describe path, neighborhood, and
connectivity operations in graphs.

11
00:00:41.150 --> 00:00:44.860
But this specialization includes
a separate course in graph analytics

12
00:00:44.860 --> 00:00:48.470
that provides a much more detailed
treatment on the subject.

13
00:00:48.470 --> 00:00:51.810
Now what distinguishes a graph
from other data models

14
00:00:51.810 --> 00:00:55.400
is that it bears two kinds of information.

15
00:00:55.400 --> 00:01:00.080
One, properties and attributes of
entities and relationships, and

16
00:01:00.080 --> 00:01:04.130
two, the connectivity structure that
constitutes the network itself.

17
00:01:05.480 --> 00:01:08.260
One way to look at this data
is shown in the figure,

18
00:01:08.260 --> 00:01:10.070
borrowed from the Apache Spark system.

19
00:01:11.130 --> 00:01:12.360
In this representation,

20
00:01:12.360 --> 00:01:16.390
the graph on the left is represented
by two tables on the right.

21
00:01:16.390 --> 00:01:22.095
The vertex, or node table, gives IDs
to nodes and lists their properties.

22
00:01:23.170 --> 00:01:25.720
The edge table has two parts.

23
00:01:25.720 --> 00:01:29.210
The colored part represents
the properties of the edge,

24
00:01:29.210 --> 00:01:33.600
whereas the white part contains just the
direction of the arrows in the network.

25
00:01:33.600 --> 00:01:39.280
Thus, since there is a directed
edge going from node 3 to node 7,

26
00:01:39.280 --> 00:01:43.830
there is a tupple 3,
7 in that part of the edge table.

27
00:01:43.830 --> 00:01:48.540
Now this form of the graph model is
called the property graph model,

28
00:01:48.540 --> 00:01:52.760
which we'll see many times in this
course and in the specialization.

29
00:01:52.760 --> 00:01:57.740
Now representing connectivity information
gives graph data a new kind of

30
00:01:57.740 --> 00:02:01.910
computing ability that's different from
other data models we have seen so far.

31
00:02:03.090 --> 00:02:07.450
Even without looking at the properties
of the nodes and edges, one can get very

32
00:02:07.450 --> 00:02:12.640
interesting information just by analyzing
or querying this connectivity structure.

33
00:02:13.800 --> 00:02:18.990
Consider a social network with three
types of nodes, user, city, and

34
00:02:18.990 --> 00:02:24.250
restaurant, and three types of edges,
friend, likes, and lives in.

35
00:02:25.300 --> 00:02:28.110
The leftmost node, AG, represents me.

36
00:02:28.110 --> 00:02:31.930
And I'm interested in finding a good
Italian restaurant in New York

37
00:02:31.930 --> 00:02:36.360
that my friends, or their friends,
who also live in New York, like.

38
00:02:36.360 --> 00:02:41.765
I shall possibly choose IT3 because
it has the highest number of

39
00:02:41.765 --> 00:02:47.700
like edges coming into it from people
who have a lives in edge to New York.

40
00:02:47.700 --> 00:02:49.008
And at the same time,

41
00:02:49.008 --> 00:02:52.869
can be reached by following
the friend edges going out from me.

42
00:02:52.869 --> 00:02:57.306
Now this shows a very important class
of operations and ground data, namely

43
00:02:57.306 --> 00:03:01.830
traversal, that involves edge following
based on some sort of conditions.

44
00:03:03.090 --> 00:03:07.660
A number of path operations
required some sort of optimization.

45
00:03:07.660 --> 00:03:12.885
The simplest among these is the well known
shortest path query, which is applied to

46
00:03:12.885 --> 00:03:17.750
node networks to find the best route from
a source location to a target location.

47
00:03:17.750 --> 00:03:21.215
The second class of optimization
operations is required to find

48
00:03:21.215 --> 00:03:26.050
an optimal path that must include
some user specified nodes, for

49
00:03:26.050 --> 00:03:30.420
the operation has to determine the order
in which the nodes once we visited.

50
00:03:30.420 --> 00:03:32.795
The classical application
is a trip planner,

51
00:03:32.795 --> 00:03:36.610
where the user specifies the cities
she wishes to visit, and

52
00:03:36.610 --> 00:03:40.740
the operation will optimize the criterion,
like the total distance covered.

53
00:03:40.740 --> 00:03:45.260
The third category is a case where
the system must find the best possible

54
00:03:45.260 --> 00:03:47.690
path in the network, given two or

55
00:03:47.690 --> 00:03:52.070
more optimization criteria,
which cannot be satisfied simultaneously.

56
00:03:52.070 --> 00:03:56.350
For example, if I want to travel
from my house to the airport

57
00:03:56.350 --> 00:04:01.150
using the shortest distance, but also
minimizing the amount of highway travel,

58
00:04:01.150 --> 00:04:03.710
the algorithm must find a best compromise.

59
00:04:03.710 --> 00:04:07.510
This is called a pareto-optimality
problem on graphs.

60
00:04:07.510 --> 00:04:13.120
The neighborhood of a node N in a graph is
a set of edges directly connected to it.

61
00:04:13.120 --> 00:04:18.360
A K neighborhood of N is a collection
of edges between nodes that are,

62
00:04:18.360 --> 00:04:21.160
at most, K steps away from N.

63
00:04:21.160 --> 00:04:24.710
So going back to our mini social
network graph, Bob, Jill,

64
00:04:24.710 --> 00:04:29.630
and Sarah are the first neighbors of AG,
while Max, Tim and

65
00:04:29.630 --> 00:04:34.570
Pam belong to the second neighborhood and
not the first neighborhood of AG.

66
00:04:34.570 --> 00:04:37.910
Finally, Jen is a third level neighbor.

67
00:04:38.910 --> 00:04:43.790
An important class of analysis to perform
with neighborhoods is community finding.

68
00:04:43.790 --> 00:04:47.740
A community and a social network can
be a very close group of friends.

69
00:04:47.740 --> 00:04:51.340
So the graph shown in this
figure has four communities.

70
00:04:51.340 --> 00:04:56.327
One can see in the figure that each
community has a higher density of edges

71
00:04:56.327 --> 00:05:02.480
within the community and a lower density
across two different communities.

72
00:05:02.480 --> 00:05:05.440
Finding densely connected parts of a graph

73
00:05:05.440 --> 00:05:09.860
helps identify neighborhoods that
can be recognized as communities.

74
00:05:09.860 --> 00:05:15.032
A more complex class of operations include
finding the best possible clusters,

75
00:05:15.032 --> 00:05:17.620
which is another name for
communities in a graph, so

76
00:05:17.620 --> 00:05:22.030
that any other grouping of nodes into
communities will be less effective.

77
00:05:22.030 --> 00:05:27.430
Now, as graphs become bigger and denser,
these methods become harder to compute.

78
00:05:27.430 --> 00:05:32.240
Thus, neighborhood-based
optimization operation present

79
00:05:32.240 --> 00:05:34.520
significant scalability challenges.

80
00:05:34.520 --> 00:05:38.650
If we inspect the neighborhood of
every node in a graph, sometimes,

81
00:05:38.650 --> 00:05:42.650
we'll find neighborhoods that
are different from all others.

82
00:05:42.650 --> 00:05:45.390
These neighborhoods are called anomalous.

83
00:05:45.390 --> 00:05:49.720
Consider the following four graphs and
on the central red node.

84
00:05:49.720 --> 00:05:54.177
The first graph is odd because
it's almost perfectly star shaped.

85
00:05:54.177 --> 00:05:58.751
That is, the nodes that the red node
is connected to are almost unconnected

86
00:05:58.751 --> 00:06:00.137
amongst themselves.

87
00:06:00.137 --> 00:06:04.000
That's really odd because it
doesn't happen in reality much.

88
00:06:04.000 --> 00:06:05.620
So it's an anomalous node.

89
00:06:05.620 --> 00:06:09.630
The second figure shows
a neighborhood to which

90
00:06:09.630 --> 00:06:13.770
a significantly large number of neighbors
has connected amongst themselves.

91
00:06:13.770 --> 00:06:18.860
This makes the graph very cliquish,
where a clique refers to a neighborhood

92
00:06:18.860 --> 00:06:23.280
where each node is connected to all other
neighborhood nodes in the neighborhood.

93
00:06:23.280 --> 00:06:25.770
The third figure shows a neighborhood,

94
00:06:25.770 --> 00:06:31.040
where some edges have an unusually
heavy weight compared to the others.

95
00:06:31.040 --> 00:06:34.578
The fourth figure shows
a special case of the third,

96
00:06:34.578 --> 00:06:38.930
where one edge is predominantly high
rate compared to all the other edges.

97
00:06:40.120 --> 00:06:42.890
Connectedness is a fundamental
property of a graph.

98
00:06:43.960 --> 00:06:45.390
In a connected graph,

99
00:06:45.390 --> 00:06:49.570
each node is reachable from every
other node through some path.

100
00:06:49.570 --> 00:06:54.750
If a graph is not connected, but there
are subgraphs of it, which are connected,

101
00:06:54.750 --> 00:06:58.745
then these subgraphs are called connected
components of the original graph.

102
00:06:58.745 --> 00:07:01.799
In the figure on the right,
there are four connected components.

103
00:07:01.799 --> 00:07:05.805
A search gradient like
finding optimal paths

104
00:07:05.805 --> 00:07:09.075
should be performed only within
each component and not across them.

105
00:07:10.215 --> 00:07:14.325
For large graphs, there are several
new parallelized techniques for

106
00:07:14.325 --> 00:07:15.835
the detection of connected components.

107
00:07:17.070 --> 00:07:19.680
We will discuss a map
reduce based technique for

108
00:07:19.680 --> 00:07:21.830
connected components in a later course.
WEBVTT

1
00:00:03.200 --> 00:00:05.569
We have discussed quite a few data models,
but

2
00:00:05.569 --> 00:00:08.669
there are many other data models
that have been developed for

3
00:00:08.669 --> 00:00:13.500
various purposes, and we really cannot
cover all of them in a single course.

4
00:00:13.500 --> 00:00:17.360
We'll end these lectures on data models
with an example that may give you

5
00:00:17.360 --> 00:00:21.560
an insight into a class of objects that
define in many different applications.

6
00:00:22.850 --> 00:00:27.972
So after this video you'll be able
to describe how arrays can serve

7
00:00:27.972 --> 00:00:33.278
as a data model, explain why images
can be modeled as vector arrays,

8
00:00:33.278 --> 00:00:37.689
specify a set of operations on scalar and
vector arrays.

9
00:00:39.969 --> 00:00:42.690
Now, we have all seen the arrays.

10
00:00:42.690 --> 00:00:46.390
In the simplest case,
an array is a matrix like this.

11
00:00:46.390 --> 00:00:48.380
Let's call this array A.

12
00:00:50.960 --> 00:00:55.140
The top row in yellow,
gives the column numbers and

13
00:00:55.140 --> 00:00:57.890
the left column, also in yellow,
gives the row numbers.

14
00:00:58.970 --> 00:01:03.263
When we need to refer to a value
of the array as A(3, 2),

15
00:01:03.263 --> 00:01:07.530
we mean the value of the cell in row 3 and
column 2.

16
00:01:07.530 --> 00:01:12.270
This is called indexed structure,
where 3 and

17
00:01:12.270 --> 00:01:18.070
2 are the row and column indices that are
necessary to get the value of a data item.

18
00:01:19.430 --> 00:01:21.830
The area has two dimensions.

19
00:01:21.830 --> 00:01:24.050
So hence there are two indexes.

20
00:01:24.050 --> 00:01:27.650
If these were a three dimensional array,
we would have three indexes.

21
00:01:28.860 --> 00:01:29.390
Now earlier,

22
00:01:29.390 --> 00:01:34.360
we have seen that we can represent the two
dimensional array as a three column table.

23
00:01:34.360 --> 00:01:37.790
One column for the row index,
one column for the column index, and

24
00:01:37.790 --> 00:01:38.849
the last column for the value.

25
00:01:40.310 --> 00:01:45.310
Thus a k dimensional array can be
represented as a relation with k

26
00:01:45.310 --> 00:01:46.090
plus one columns.

27
00:01:47.690 --> 00:01:51.100
The number of tuples in this
representation will be the product of

28
00:01:51.100 --> 00:01:55.220
the size of the first dimension times the
size of the second dimension and so forth.

29
00:01:56.670 --> 00:02:01.000
Then in this case,
the size is five in each dimension.

30
00:02:01.000 --> 00:02:08.070
So there are 25 C column tuples in
a relation representing the array.

31
00:02:08.070 --> 00:02:11.890
A more useful situation occurs
when the cells of an array have

32
00:02:11.890 --> 00:02:13.490
a vectors as values.

33
00:02:14.620 --> 00:02:19.610
As you can see in the 2D vector array
here, each cell has a three vector.

34
00:02:19.610 --> 00:02:21.340
That is a vector with three elements.

35
00:02:22.400 --> 00:02:25.150
Therefore, if we want to
receive a cell value and

36
00:02:25.150 --> 00:02:27.930
treat it like before,
we'll get back the whole vector.

37
00:02:29.220 --> 00:02:31.680
Now, this type of data
should look familiar to you,

38
00:02:31.680 --> 00:02:35.840
because images often have a red,
green and blue channels per pixel.

39
00:02:37.030 --> 00:02:38.050
In other words,

40
00:02:38.050 --> 00:02:43.780
images of vector valued arrays where each
array cell has a three color vector.

41
00:02:45.270 --> 00:02:49.000
We can also think of the array model
in the context of satellite images.

42
00:02:49.000 --> 00:02:52.850
Where there are many more channels
depending on the range of wavelengths

43
00:02:52.850 --> 00:02:54.100
each channel catches.

44
00:02:55.360 --> 00:02:58.710
Let us consider the operations
on arrays of vectors.

45
00:02:58.710 --> 00:03:03.430
Because it is a combination of two models,
one can create different combinations of

46
00:03:03.430 --> 00:03:08.140
array operations, vector operations and
composite operations.

47
00:03:08.140 --> 00:03:08.650
Here are some.

48
00:03:09.810 --> 00:03:12.580
The dimension of the array here,
the first operation, is two.

49
00:03:13.940 --> 00:03:16.289
If we pick up any dimension, say one.

50
00:03:16.289 --> 00:03:20.741
The size of it is also two
because they're two elements,

51
00:03:20.741 --> 00:03:23.260
zero and one in each dimension.

52
00:03:23.260 --> 00:03:31.259
As we saw before, the value of the cell
(1,1) is a vector 16, 301, 74.

53
00:03:31.259 --> 00:03:37.979
While the value of A11 component 2 is 74.

54
00:03:37.979 --> 00:03:42.279
The length of the vector is a square root
of the sum of the elements of the vector.

55
00:03:42.279 --> 00:03:48.999
So length of A11 would come to 310.375.

56
00:03:48.999 --> 00:03:50.623
The distance function can be so

57
00:03:50.623 --> 00:03:54.184
simple like the Euclidean distance
function between two vectors or

58
00:03:54.184 --> 00:03:57.820
the cosine of an angle between them
as we saw in the previous lecture.

59
00:03:59.080 --> 00:04:02.580
But it can also be something more complex,
based on the needs of the application.

60
00:04:04.320 --> 00:04:08.390
Obviously one can also perform operations
like selection over indices so

61
00:04:08.390 --> 00:04:13.170
we can ask which cells had
the zero value greater than 25.

62
00:04:13.170 --> 00:04:16.060
Giving as the result zero one and
one zero.

63
00:04:17.190 --> 00:04:20.050
You will experience some of these
operations in your hands on session.
WEBVTT

1
00:00:02.265 --> 00:00:06.497
Next, we'll look at a data model
that has been successfully used to

2
00:00:06.497 --> 00:00:10.150
retrieve data from large
collections of text and images.

3
00:00:11.170 --> 00:00:12.230
Let's stay with text for now.

4
00:00:13.590 --> 00:00:17.490
Text is often thought of
as unstructured data.

5
00:00:17.490 --> 00:00:20.315
Primarily because it doesn't really
have attributes and relationships.

6
00:00:21.530 --> 00:00:25.740
Instead, it is a sequence of strings
punctuated by line and parent of breaks.

7
00:00:27.810 --> 00:00:34.370
So one is to think of a different
way to find and analyze text data.

8
00:00:34.370 --> 00:00:39.130
In this video, we'll describe that
finding text from a huge collection of

9
00:00:39.130 --> 00:00:42.880
text data is a little different from
the data modules we have seen so far.

10
00:00:44.080 --> 00:00:48.710
To find text,
we not only need the text data itself, but

11
00:00:48.710 --> 00:00:53.810
we need a different structure that
is computed from the text data.

12
00:00:53.810 --> 00:00:55.020
To create the structure,

13
00:00:55.020 --> 00:01:01.000
we'll introduce the notion of the document
vector model which we call a vector model.

14
00:01:02.760 --> 00:01:05.510
Further you will see
that finding a document

15
00:01:05.510 --> 00:01:08.090
is not really an exact search problem.

16
00:01:09.380 --> 00:01:13.090
Here, we'll give a query document and

17
00:01:13.090 --> 00:01:17.130
ask the system to find all
documents that are similar to it.

18
00:01:18.440 --> 00:01:20.990
After this video,
you'll be able to describe

19
00:01:20.990 --> 00:01:25.380
how the similarity is computed and
how it is used to search documents.

20
00:01:26.930 --> 00:01:31.770
Finally, you will see that search engines
use some form of vector models and

21
00:01:31.770 --> 00:01:33.850
similarity search to locate text data.

22
00:01:35.480 --> 00:01:39.970
And you will see that the same principle
can be use for finding similar images.

23
00:01:42.740 --> 00:01:45.410
Let us describe the concept
of a document to an example.

24
00:01:46.540 --> 00:01:49.170
So lets consider three types
of document shown here.

25
00:01:51.040 --> 00:01:52.470
Now we'll create a matrix.

26
00:01:56.040 --> 00:01:57.790
The rows of the matrix stand for

27
00:01:57.790 --> 00:02:02.950
the documents and columns represent
the words in the documents.

28
00:02:02.950 --> 00:02:05.770
We put the number of occurrences
of returning the document in

29
00:02:05.770 --> 00:02:07.400
the appropriate cell of the matrix.

30
00:02:08.630 --> 00:02:14.380
In this case, the count of each term
in each document happens to be one.

31
00:02:14.380 --> 00:02:16.700
This is called the term frequency matrix.

32
00:02:19.340 --> 00:02:22.620
So now that we have created
the term frequency matrix,

33
00:02:22.620 --> 00:02:24.540
which we call tf for short.

34
00:02:25.595 --> 00:02:30.350
We'll create a new vector called the
inverse document frequency for each term.

35
00:02:31.810 --> 00:02:34.990
We'll explain why we need this
vector on the next slide.

36
00:02:34.990 --> 00:02:37.160
First, let's see how it's computed.

37
00:02:39.280 --> 00:02:43.435
The number of documents n here is 3.

38
00:02:43.435 --> 00:02:46.270
The term new occurs
twice in the collection.

39
00:02:47.520 --> 00:02:52.950
So the inverse document frequency or
IDF of the term new

40
00:02:52.950 --> 00:02:57.810
is log to the base 2,
n divided by term count.

41
00:02:57.810 --> 00:03:04.464
That is, log to the base 2,
3 divided by 2, which is 0.584.

42
00:03:05.980 --> 00:03:09.210
We'll show the ideal score for
all six terms here.

43
00:03:10.770 --> 00:03:15.735
Now some of you may wonder why we use
log to the base 2 instead of let's

44
00:03:15.735 --> 00:03:17.540
say log to the base 10.

45
00:03:17.540 --> 00:03:20.350
There is no deep scientific reason for it.

46
00:03:20.350 --> 00:03:23.970
It's more of a convention in
many areas of Computer Science

47
00:03:23.970 --> 00:03:26.110
when many important
numbers are powers of two.

48
00:03:27.150 --> 00:03:32.153
In reality,
log to the base two of x is the same

49
00:03:32.153 --> 00:03:39.156
number as log to the base ten of x
times log to the base two of ten.

50
00:03:39.156 --> 00:03:44.340
The second number, that is log to
the base two of ten is a constant.

51
00:03:44.340 --> 00:03:50.080
So the relative score of IDF does not
change regardless of the base we use.

52
00:03:50.080 --> 00:03:52.170
Now let's understand this
number one more time.

53
00:03:53.400 --> 00:03:57.829
The document frequency of a term is
the count of that term in the whole

54
00:03:57.829 --> 00:04:01.029
collection divided by
the number of documents.

55
00:04:02.748 --> 00:04:07.168
Here, we take the inverse of
the document frequency, so

56
00:04:07.168 --> 00:04:11.510
that n, the number of documents,
is in the numerator.

57
00:04:13.580 --> 00:04:18.270
Now before we continue, let's understand
the intuition behind the IDF vector.

58
00:04:19.500 --> 00:04:25.210
Now suppose you have 100
random newspaper articles and

59
00:04:25.210 --> 00:04:28.110
let's say 10 of them cover elections.

60
00:04:29.210 --> 00:04:33.710
Which means all the others
cover all other subjects.

61
00:04:33.710 --> 00:04:39.730
Now in this article, let's say we'll find
the term election 50 times in total.

62
00:04:41.660 --> 00:04:46.000
How often do you think you'll find
the term is as in the verb is?

63
00:04:47.320 --> 00:04:50.440
You can imagine that it will
occur in all hundred of them and

64
00:04:50.440 --> 00:04:52.160
that too, multiple times.

65
00:04:53.370 --> 00:04:57.240
We can safely assume that
the number of occurrences of is

66
00:04:57.240 --> 00:04:59.700
will be 300 at the very least.

67
00:05:01.000 --> 00:05:06.940
Thus the document frequency of is is six
times the document frequency of election.

68
00:05:07.940 --> 00:05:10.460
But that doesn't sound right, does it?

69
00:05:10.460 --> 00:05:15.030
Is is such a common word that it's
prevalence has a negative impact

70
00:05:15.030 --> 00:05:17.240
on its informativeness.

71
00:05:17.240 --> 00:05:21.520
So, now if you want to
compute the IDF of is and

72
00:05:21.520 --> 00:05:24.990
election, the IDF of is will be far lower.

73
00:05:26.170 --> 00:05:29.930
So, IDF acts like a penalty factor for

74
00:05:29.930 --> 00:05:33.570
terms which are too widely used
to be considered informative.

75
00:05:36.340 --> 00:05:39.890
Now that we have understood
that IDF is a penalty factor,

76
00:05:39.890 --> 00:05:45.800
we will multiply the tf numbers through
the IDF numbers giving us this.

77
00:05:48.610 --> 00:05:53.550
This is a column-wise multiplication
of the tf numbers with the IDF

78
00:05:53.550 --> 00:05:57.677
numbers giving us what we
call the tf-idf matrix.

79
00:05:59.120 --> 00:06:06.070
Therefore for each document, we have
a vector represented here as a row.

80
00:06:06.070 --> 00:06:10.910
So that row represents the relative
importance of each term in the vocabulary.

81
00:06:10.910 --> 00:06:14.370
Vocabulary means the collection of all
words that appear in this collection.

82
00:06:16.160 --> 00:06:20.800
If the vocabulary has 3 million entries,
then this vector can get quite long.

83
00:06:22.150 --> 00:06:26.890
Also, if the number of document
grows let's just say to 1 billion,

84
00:06:26.890 --> 00:06:28.720
then it becomes a big data problem.

85
00:06:30.460 --> 00:06:33.660
Now the last column after
each document of vector here

86
00:06:33.660 --> 00:06:35.180
is the length of the document vector.

87
00:06:36.300 --> 00:06:41.060
Which is really the square root of the sum
of squares of the individual term scores

88
00:06:41.060 --> 00:06:42.060
as shown in the formula.

89
00:06:45.560 --> 00:06:48.170
To perform a search in the vector space,

90
00:06:48.170 --> 00:06:52.010
we write a query just like
we type terms in Google.

91
00:06:52.010 --> 00:06:55.170
Here, the number of terms is three.

92
00:06:55.170 --> 00:06:57.910
Out of which the term
new appears two times.

93
00:06:59.260 --> 00:07:03.049
In fact, this is the maximum frequency
out of all terms in the query.

94
00:07:04.220 --> 00:07:08.660
So we take the document vector of
the query and multiply each term

95
00:07:08.660 --> 00:07:14.570
by the number of occurrences divided by
two which is the maximum term frequency.

96
00:07:14.570 --> 00:07:18.090
Now in this case,
it gives us two non-zero terms.

97
00:07:18.090 --> 00:07:20.950
0.584 and

98
00:07:20.950 --> 00:07:26.460
0.292 for new and york.

99
00:07:26.460 --> 00:07:30.726
Then we compute the length of
the query vector just like we did for

100
00:07:30.726 --> 00:07:33.730
the document vectors
on the previous slide.

101
00:07:33.730 --> 00:07:39.443
Next, we will compute the similarity
between the query vector and each document

102
00:07:39.443 --> 00:07:45.170
with the idea that we'll measure how far
the query vector is from each document.

103
00:07:47.730 --> 00:07:53.150
Now there are many similar functions
defined and used for different things.

104
00:07:53.150 --> 00:07:56.940
A popular similarity measure
is the cosine function,

105
00:07:56.940 --> 00:08:03.200
which measures the cosine function of
the angle between these two vectors.

106
00:08:03.200 --> 00:08:07.080
The mathematical formula for
computing the function is given here.

107
00:08:07.080 --> 00:08:11.250
The intuition is that if
the vectors are identical,

108
00:08:11.250 --> 00:08:14.330
then the angle between them is zero.

109
00:08:14.330 --> 00:08:16.230
And therefore,
the cosine function evaluates to one.

110
00:08:18.250 --> 00:08:20.910
As the angle increases,

111
00:08:20.910 --> 00:08:25.450
the value of the cosine function
decreases to make them more dissimilar.

112
00:08:26.780 --> 00:08:30.694
The way to compute the function is to
multiply the corresponding elements of

113
00:08:30.694 --> 00:08:32.170
the two vectors.

114
00:08:32.170 --> 00:08:34.850
That is the first element
of one with the first

115
00:08:34.850 --> 00:08:37.340
element of the second one and so forth.

116
00:08:37.340 --> 00:08:38.790
And then sum of these products.

117
00:08:39.960 --> 00:08:45.610
Here, the only contributing
terms are from new and

118
00:08:45.610 --> 00:08:49.230
york because, these are the only two
non-zero terms in the query vector.

119
00:08:50.660 --> 00:08:55.630
This sum is then divided by
the product of the document length and

120
00:08:55.630 --> 00:08:58.040
the query length that we
have computed earlier.

121
00:08:59.310 --> 00:09:02.650
Look at the result of the distance
function and you will notice that

122
00:09:02.650 --> 00:09:07.570
the document 1 is much more similar
to the query than the other two.

123
00:09:08.810 --> 00:09:13.640
So while similarity scoring and document
ranking process working effectively,

124
00:09:13.640 --> 00:09:16.100
the method is a little cotton dry.

125
00:09:16.100 --> 00:09:19.920
More often than not users would
like a little more control

126
00:09:19.920 --> 00:09:20.910
over the ranking of terms.

127
00:09:22.270 --> 00:09:27.450
One way of accomplishing this is to put
different weights on each query term.

128
00:09:27.450 --> 00:09:33.460
And in this example, the query term
york has a default weight of one,

129
00:09:33.460 --> 00:09:36.530
times has a weight of two.

130
00:09:36.530 --> 00:09:40.340
And post has a weight of five
as specified by the user.

131
00:09:41.350 --> 00:09:43.662
So relatively speaking,

132
00:09:43.662 --> 00:09:49.765
york has a weight of 1 divided
by 1 + 5 + 2 is equal to 0.125.

133
00:09:49.765 --> 00:09:55.630
Times has a weight of 0.25 and
post has a weight of 0.625.

134
00:09:55.630 --> 00:09:59.280
Now the scoring method we showed
before will change a bit.

135
00:09:59.280 --> 00:10:04.091
The query of vector and its length
were exactly as computed before.

136
00:10:04.091 --> 00:10:08.777
However, now each term in the query
vector is further multiplied by these

137
00:10:08.777 --> 00:10:10.810
relative weights.

138
00:10:10.810 --> 00:10:14.490
In our case,
the term york now has a much higher rate.

139
00:10:15.500 --> 00:10:21.237
So as expected, this will change
the ranking of the documents and

140
00:10:21.237 --> 00:10:24.900
new york post will have the highest rank.

141
00:10:24.900 --> 00:10:30.580
Now similarity search is often used for
images using a vector space model.

142
00:10:30.580 --> 00:10:33.210
One can compute futures from images.

143
00:10:33.210 --> 00:10:36.180
And one common feature
is a scatter histogram.

144
00:10:36.180 --> 00:10:38.000
Consider the image here.

145
00:10:38.000 --> 00:10:40.820
One can create the histogram of the red,
green and

146
00:10:40.820 --> 00:10:46.170
blue channels where histogram is the count
of pixels having a certain density value.

147
00:10:47.360 --> 00:10:52.600
This picture is mostly bright, so the
count of dark pixels is relatively small.

148
00:10:53.690 --> 00:10:56.746
Now one can think of
histograms like a vector.

149
00:10:56.746 --> 00:11:01.430
Very often the pixel values will
be bend before creating a vector.

150
00:11:01.430 --> 00:11:06.424
The table shown is a feature vector
where the numbers for each row have been

151
00:11:06.424 --> 00:11:10.960
normalized with the size of the image
to make the row sum equal to one.

152
00:11:10.960 --> 00:11:15.320
Similar vectors can be computed of
the image texture, shapes of objects and

153
00:11:15.320 --> 00:11:16.790
any other properties.

154
00:11:16.790 --> 00:11:20.700
Thus making a vector space model
significant for unstructured data.
WEBVTT

1
00:00:01.646 --> 00:00:06.170
In our experience as educators
we have observed that

2
00:00:06.170 --> 00:00:10.940
learners often make the assumption
that the format of the data

3
00:00:10.940 --> 00:00:15.820
is the same as the logical model of the
data in the way that you operate on it.

4
00:00:15.820 --> 00:00:18.740
The goal of this very
short lecture is to ensure

5
00:00:18.740 --> 00:00:21.590
that we can clearly
distinguish between the two.

6
00:00:21.590 --> 00:00:26.550
So, after watching this video you will be
able to explain the difference between

7
00:00:26.550 --> 00:00:31.460
format, which is a serialized
representation of the data, as opposed to

8
00:00:31.460 --> 00:00:35.010
data model which we have discussed
at length in the previous months.

9
00:00:36.930 --> 00:00:41.050
Perhaps the simplest example of a data
format is a csv file, so here is

10
00:00:41.050 --> 00:00:45.359
a snippet of a csv file from the global
terrorism database we discussed earlier.

11
00:00:46.640 --> 00:00:50.520
We know that CSV or
common separative values means

12
00:00:50.520 --> 00:00:53.910
that the term between two commas
is the value of an attribute.

13
00:00:53.910 --> 00:00:54.740
But what is this value?

14
00:00:56.200 --> 00:01:00.110
The common notion is that it's
a content of a single relation

15
00:01:01.280 --> 00:01:04.870
where each line is a record
that's a tuple and

16
00:01:04.870 --> 00:01:09.030
the iod value in the CSV corresponds
to the iod attribute as shown here.

17
00:01:10.340 --> 00:01:14.850
Now that might very well be true but
let's look at a different example.

18
00:01:14.850 --> 00:01:17.440
Let's say this snippet
here is my CSV file.

19
00:01:18.790 --> 00:01:21.960
There is no difference between
the previous file and this one.

20
00:01:21.960 --> 00:01:24.780
However, here is how I
like to see the data.

21
00:01:26.630 --> 00:01:28.600
As you can see this is a graph,

22
00:01:28.600 --> 00:01:33.190
and the data model is the same
although the format is still CSV.
WEBVTT

1
00:00:00.870 --> 00:00:03.200
This is the second hands on exercise for
sensor data.

2
00:00:03.200 --> 00:00:06.220
In the first we looked at
static data in a text file.

3
00:00:06.220 --> 00:00:10.410
In this one we'll be looking at
real-time streaming measurements.

4
00:00:10.410 --> 00:00:12.675
First, we will open a terminal window, and

5
00:00:12.675 --> 00:00:15.770
cd into the directory containing
the data and the scripts.

6
00:00:15.770 --> 00:00:18.110
Next we'll connect to
the weather station and

7
00:00:18.110 --> 00:00:20.990
look at the real-time
data as it streams in.

8
00:00:20.990 --> 00:00:25.280
After that, we will look at the key to
remind ourselves what the fields mean.

9
00:00:25.280 --> 00:00:28.140
And finally, we will plot the data
streaming from the weather station.

10
00:00:29.400 --> 00:00:30.490
Let's begin.

11
00:00:30.490 --> 00:00:33.830
First, open a terminal window,
by clicking on the terminal icon.

12
00:00:33.830 --> 00:00:34.861
Top of the toolbar.

13
00:00:36.498 --> 00:00:42.919
[NOISE] Let's run cd
Downloads/big-data-2/sensor.

14
00:00:51.421 --> 00:00:54.021
You can run ls to see
the name of the scripts.

15
00:00:57.772 --> 00:01:02.114
Let's run stream-data.py
to see the real-time data.

16
00:01:10.240 --> 00:01:15.860
This shows us the real-time measurements
coming from the weather station.

17
00:01:15.860 --> 00:01:17.738
By looking at the time stamps,

18
00:01:17.738 --> 00:01:22.733
we can see that each measurement arrives
about one second after the previous one.

19
00:01:25.078 --> 00:01:28.439
Additionally, we can see
that R1 comes fairly often,

20
00:01:28.439 --> 00:01:32.172
whereas other measurements,
such as R2, are not as often.

21
00:01:37.361 --> 00:01:38.880
We can open another terminal and

22
00:01:38.880 --> 00:01:42.040
look at the key to remind ourselves
what these measurements mean.

23
00:01:53.639 --> 00:01:58.875
The key is in wxt-520-format.txt.

24
00:01:58.875 --> 00:02:03.693
We could run more
wxt-520-format.txt to view it.

25
00:02:14.792 --> 00:02:19.535
If we go back to our live data,
we can see that the 19th

26
00:02:19.535 --> 00:02:24.500
measurement here says Ta
was 22.5 degrees Celsius.

27
00:02:25.930 --> 00:02:30.116
And look up here,
see that Ta is the air temperature.

28
00:02:30.116 --> 00:02:34.737
The next measure we can see
that Dn was equal to 255D.

29
00:02:34.737 --> 00:02:39.340
According to our key,
Dn is the wind direction minimum, and

30
00:02:39.340 --> 00:02:41.157
the units are degrees.

31
00:02:46.417 --> 00:02:51.316
We can also plot specific measurements
streaming live from the weather station.

32
00:02:54.056 --> 00:02:55.740
Let's plot the wind speed average.

33
00:02:57.300 --> 00:03:02.819
If we look at our key,
we see that the wind speed average is Sm.

34
00:03:02.819 --> 00:03:08.554
So we can plot this by running
stream-plot-data.py sm.

35
00:03:20.501 --> 00:03:23.659
This plots the data as the weather
station sends it to us.

36
00:03:27.501 --> 00:03:28.843
If we look at the x-axis,

37
00:03:28.843 --> 00:03:31.960
we can see that one measurement
comes in about every second.

38
00:03:43.199 --> 00:03:46.620
We can plot other measurements by
choosing different fields from the key.

39
00:03:47.650 --> 00:03:56.350
For example, we can plot the air pressure
by running stream-plot-data.py Pa.

40
00:03:56.350 --> 00:03:58.289
Since Pa is the air pressure.

41
00:04:14.810 --> 00:04:18.720
First thing we notice is that there's
only one measurement so far in the graph.

42
00:04:20.950 --> 00:04:23.710
This means that the air pressure
measurements are not coming as

43
00:04:23.710 --> 00:04:25.840
fast as the wind measurements.

44
00:04:25.840 --> 00:04:27.127
In fact, we only got one.
WEBVTT

1
00:00:00.008 --> 00:00:01.736
In this hands-on activity,

2
00:00:01.736 --> 00:00:05.270
we'll be looking at real time
data streaming from Twitter.

3
00:00:06.360 --> 00:00:08.750
First, we'll open a terminal window and

4
00:00:08.750 --> 00:00:12.520
cd into the directory containing
Python scripts to access this data.

5
00:00:13.880 --> 00:00:17.750
Next, we'll look at the contents
of tweets streaming from Twitter

6
00:00:17.750 --> 00:00:19.220
containing specific words.

7
00:00:20.400 --> 00:00:23.230
Finally, we will plot the frequency
of these streaming tweets.

8
00:00:24.500 --> 00:00:25.810
Let's begin.

9
00:00:25.810 --> 00:00:29.259
First, click on the terminal
icon at the top of the toolbar.

10
00:00:33.485 --> 00:00:38.414
Let cd into the directory containing
the python scripts to access the real time

11
00:00:38.414 --> 00:00:40.400
data from Twitter.

12
00:00:40.400 --> 00:00:45.473
We'll run CD downloads big-data-2 json.

13
00:00:52.538 --> 00:00:55.828
We can run ls to see
the files in this directory.

14
00:01:01.678 --> 00:01:07.200
The file auth should be created containing
your Twitter authentication information.

15
00:01:08.200 --> 00:01:11.434
You could see the separate reading for
how to setup the Twitter app and

16
00:01:11.434 --> 00:01:12.617
how to create this file.

17
00:01:16.023 --> 00:01:20.958
Next, let's run the script live tweets
to view tweets in real time containing

18
00:01:20.958 --> 00:01:22.093
a specific word.

19
00:01:22.093 --> 00:01:27.819
Let's run LiveTweets.py president.

20
00:01:33.534 --> 00:01:36.267
This will show the real time tweets
containing the word president.

21
00:01:42.171 --> 00:01:45.280
[INAUDIBLE] Runs, in the first column,
you can see the time stamp of the tweet.

22
00:01:45.280 --> 00:01:48.500
In the second column,
you can see the text.

23
00:01:54.220 --> 00:01:56.471
When you're done, hit Ctrl + C.

24
00:02:01.208 --> 00:02:04.836
Let's run LiveTweet again using
a different keyword that appear more

25
00:02:04.836 --> 00:02:06.260
frequently.

26
00:02:06.260 --> 00:02:10.448
Let's use the word time,
we'll run LiveTweet time

27
00:02:24.118 --> 00:02:30.780
When we're done, hit Ctrl + C.

28
00:02:30.780 --> 00:02:34.900
We can plot the frequency of these Tweets
by running the script plot Tweets.

29
00:02:37.390 --> 00:02:41.373
Let's run plot, tweet, president to see
the frequency for the word president.

30
00:02:57.849 --> 00:03:01.466
As this runs, we could see
the frequency changes over time.

31
00:03:19.557 --> 00:03:21.960
I can see that the maximum was one.

32
00:03:23.130 --> 00:03:25.540
And a few times,
there were just one tweet in that second.

33
00:03:29.380 --> 00:03:31.180
When you're done looking at the graph,

34
00:03:31.180 --> 00:03:32.930
click in the terminal window and
hit Enter.

35
00:03:35.940 --> 00:03:38.010
Now, let's plot the frequency for
the word time.

36
00:03:39.040 --> 00:03:42.645
Run PlotTweets.py time.

37
00:03:58.468 --> 00:04:02.500
This plot shows that the word time appears
a lot more frequently than president.

38
00:04:03.800 --> 00:04:09.514
We can see spikes in the frequency
of 40 and a maximum of around 65.

39
00:04:09.514 --> 00:04:12.663
When you're done looking at the graph,

40
00:04:12.663 --> 00:04:16.840
click on the terminal window and
press enter to quit.
WEBVTT

1
00:00:01.000 --> 00:00:05.890
With big data streaming from different
sources in varying formats, models, and

2
00:00:05.890 --> 00:00:10.650
speeds it is no surprise that we
need to be able to ingest this data

3
00:00:10.650 --> 00:00:13.970
into a fast and scalable storage system

4
00:00:13.970 --> 00:00:18.900
that is flexible enough to serve many
current and future analytical processes.

5
00:00:20.410 --> 00:00:25.730
This is when traditional data warehouses
with strict data models and data

6
00:00:25.730 --> 00:00:31.160
formats don't fit the big data challenges
for streaming and batch applications.

7
00:00:32.760 --> 00:00:38.700
The concept of a data lake was created in
response of these data big storage and

8
00:00:38.700 --> 00:00:39.890
processing challenges.

9
00:00:41.140 --> 00:00:45.460
After this video you
will be able to describe

10
00:00:45.460 --> 00:00:48.880
how data lakes enable batch
processing of streaming data.

11
00:00:50.410 --> 00:00:55.390
Explain the difference between
schema on write and schema on read.

12
00:00:56.680 --> 00:01:00.320
Organize data streams and data lakes and

13
00:01:00.320 --> 00:01:04.720
data warehouses on a spectrum of
big data management and storage.

14
00:01:07.080 --> 00:01:08.080
What is a data lake?

15
00:01:09.140 --> 00:01:15.450
Simply speaking, a data lake is
a part of a big data infrastructure

16
00:01:15.450 --> 00:01:20.270
that many streams can flow into and

17
00:01:20.270 --> 00:01:23.680
get stored for
processing in their original form.

18
00:01:23.680 --> 00:01:28.140
We can think of it as a massive
storage depository with huge

19
00:01:28.140 --> 00:01:33.560
processing power and ability to handle
a very large number of concurrence,

20
00:01:33.560 --> 00:01:35.970
data management and analytical tasks.

21
00:01:37.370 --> 00:01:42.660
In 2010,
the Pentaho Corporation's CTO James Dixon

22
00:01:42.660 --> 00:01:44.510
defined a data link as follows.

23
00:01:45.930 --> 00:01:50.830
If you think of a datamart as a store
of bottled water, cleansed and

24
00:01:50.830 --> 00:01:54.090
packaged and structured for
easy consumption,

25
00:01:54.090 --> 00:01:59.320
the data lake is a large body of
water in a more natural state.

26
00:01:59.320 --> 00:02:04.940
The contents of the data lake stream
in from a source to fill the lake,

27
00:02:04.940 --> 00:02:11.980
and various users of the lake can come
to examine it, dive in, or take samples.

28
00:02:11.980 --> 00:02:14.000
A data lake works as follows.

29
00:02:15.140 --> 00:02:17.600
The data gets loaded from its source,

30
00:02:19.160 --> 00:02:22.140
stored in its native
format until it is needed

31
00:02:23.330 --> 00:02:28.240
at which time the applications can freely
read the data and add structure to it.

32
00:02:29.500 --> 00:02:33.250
This is what we call schema on read.

33
00:02:33.250 --> 00:02:38.070
In a traditional data warehouse,
the data is loaded into the warehouse

34
00:02:38.070 --> 00:02:42.670
after transforming it into a well
defined and structured format.

35
00:02:42.670 --> 00:02:45.398
This is what we call schema on write.

36
00:02:45.398 --> 00:02:51.078
Any application using the data needs to
know this format in order to retrieve and

37
00:02:51.078 --> 00:02:52.110
use the data.

38
00:02:53.330 --> 00:02:54.990
In this approach,

39
00:02:54.990 --> 00:02:58.882
data is not loaded into the warehouse
unless there is a use for it.

40
00:02:58.882 --> 00:03:04.030
However, schema on read
approach of data lakes ensures

41
00:03:04.030 --> 00:03:09.040
all data is stored for
a potentially unknown use at a later time.

42
00:03:09.040 --> 00:03:12.110
So how is a data lake
from a data warehouse?

43
00:03:13.540 --> 00:03:18.650
A traditional data warehouse
stores data in a hierarchical file

44
00:03:19.820 --> 00:03:22.973
system with a well-defined structure.

45
00:03:22.973 --> 00:03:29.590
However, a data lake stores data as
flat files with a unique identifier.

46
00:03:29.590 --> 00:03:33.550
This often gets referred to as
object storage in big data systems.

47
00:03:36.250 --> 00:03:42.330
In data lakes each data is stored
as a binary large object or

48
00:03:42.330 --> 00:03:45.460
BLOB and is assigned a unique identifier.

49
00:03:46.460 --> 00:03:53.620
In addition, each data object is
tagged with a number of metadata tags.

50
00:03:54.860 --> 00:03:58.230
The data can be searched
using these metadata tags

51
00:03:58.230 --> 00:04:00.400
to retrieve it when there
is a need to access it.

52
00:04:01.820 --> 00:04:03.405
From a users perspective,

53
00:04:03.405 --> 00:04:08.672
metadata is stored is not a problem as
long as it is accessible when needed.

54
00:04:08.672 --> 00:04:15.780
In Hadoop data architectures,
data is loaded into HDFS and processed

55
00:04:15.780 --> 00:04:20.410
using the appropriate data management and
analytical systems on commodity clusters.

56
00:04:21.860 --> 00:04:27.080
The selection of the tools is based on the
nature of the problem being solved, and

57
00:04:27.080 --> 00:04:28.540
the data format being accessed.

58
00:04:30.090 --> 00:04:32.920
We will talk more about
the processing of data streams and

59
00:04:32.920 --> 00:04:35.820
data lakes in the next course
in this specialization.

60
00:04:37.520 --> 00:04:43.250
To summarize a data lake is
a storage architecture for

61
00:04:43.250 --> 00:04:45.190
big data collection and processing.

62
00:04:46.670 --> 00:04:50.060
It enables collection of
all data suitable for

63
00:04:50.060 --> 00:04:53.060
analysis today and
potentially in the future.

64
00:04:54.410 --> 00:04:57.450
Regardless of the data source,
structure, and

65
00:04:57.450 --> 00:05:03.120
format it supports storage of data and
transforms it only when it is needed.

66
00:05:04.500 --> 00:05:09.940
A data lake ideally supports all parts
of the user base to benefit from

67
00:05:09.940 --> 00:05:15.790
this architecture, including business,
storage, analytics and computing experts.

68
00:05:17.240 --> 00:05:21.310
Finally, And perhaps most importantly,

69
00:05:21.310 --> 00:05:25.790
data lakes are infrastructure components
within a big data architecture

70
00:05:25.790 --> 00:05:29.980
that can evolve over time based
on application-specific needs.
WEBVTT

1
00:00:01.870 --> 00:00:04.473
What is a data stream?

2
00:00:04.473 --> 00:00:05.913
After this video,

3
00:00:05.913 --> 00:00:11.323
you will be able to summarize the key
characteristics of a data stream.

4
00:00:11.323 --> 00:00:16.263
Identify the requirements of
streaming data systems, and

5
00:00:16.263 --> 00:00:20.323
recognize the data streams
you use in your life.

6
00:00:20.323 --> 00:00:23.262
When we talked about how
big data is generated and

7
00:00:23.262 --> 00:00:26.640
the characteristics of the big
data using sound waves.

8
00:00:26.640 --> 00:00:32.460
One of the challenges we mentioned was the
velocity of data coming in varying rates.

9
00:00:34.560 --> 00:00:39.360
For some applications this
presents the need to process data

10
00:00:39.360 --> 00:00:43.540
as it is generated, or
in other words, as it streams.

11
00:00:44.760 --> 00:00:48.770
We call these types of applications
Streaming Data Processing Applications.

12
00:00:49.840 --> 00:00:55.630
This terminology refers to a constant
stream of data flowing from a source,

13
00:00:55.630 --> 00:01:00.380
for example data from a sensory machine or
data from social media.

14
00:01:02.753 --> 00:01:07.149
An example application would be making
data-driven marketing decisions in

15
00:01:07.149 --> 00:01:08.390
real time.

16
00:01:08.390 --> 00:01:12.790
Through the use of data from
real-time sales trends,

17
00:01:12.790 --> 00:01:15.670
social media analysis,
and sales distributions.

18
00:01:17.430 --> 00:01:23.070
Another example for streaming data
processing is monitoring of industrial or

19
00:01:23.070 --> 00:01:25.250
farming machinery in real time.

20
00:01:25.250 --> 00:01:30.075
For monitoring and
detection of potential system failures.

21
00:01:30.075 --> 00:01:32.837
In fact, any sensor network or

22
00:01:32.837 --> 00:01:38.576
internet of things environment
controlled by another entity,

23
00:01:38.576 --> 00:01:42.733
or set of entities falls
under this category.

24
00:01:42.733 --> 00:01:46.304
For example,
as you have seen in an earlier video,

25
00:01:46.304 --> 00:01:48.640
FlightStats is an application.

26
00:01:48.640 --> 00:01:52.942
That processes about 60
million weekly flight events

27
00:01:52.942 --> 00:01:56.610
that come into their
data acquisition system.

28
00:01:56.610 --> 00:02:00.978
And turns it into real-time
intelligence for airlines and

29
00:02:00.978 --> 00:02:04.480
millions of travelers
around the world daily.

30
00:02:07.520 --> 00:02:10.180
So, how then do we define a data stream?

31
00:02:12.300 --> 00:02:17.268
A stream is defined as
a possibly unbounded sequence of

32
00:02:17.268 --> 00:02:19.433
data items or records.

33
00:02:19.433 --> 00:02:25.880
That may or may not be related to,
or correlated with each other.

34
00:02:25.880 --> 00:02:30.952
Each data is generally timestamped and

35
00:02:30.952 --> 00:02:34.670
in some cases geo-tagged.

36
00:02:34.670 --> 00:02:39.063
As you have seen in our examples,
the data can stream from many sources.

37
00:02:39.063 --> 00:02:44.403
Including instruments, and
many internet of things application areas,

38
00:02:44.403 --> 00:02:48.480
computer programs, websites,
or social media posts.

39
00:02:49.900 --> 00:02:54.360
Streaming data sometimes get
referred to as event data as

40
00:02:54.360 --> 00:02:58.940
each data item is treated as an individual
event in a synchronized sequence.

41
00:03:01.200 --> 00:03:04.070
Streams pose very difficult challenges for

42
00:03:04.070 --> 00:03:06.940
conventional data
management architectures.

43
00:03:06.940 --> 00:03:13.860
Which are built primarily on the concept
of persistence, static data collections.

44
00:03:13.860 --> 00:03:18.219
Due to the fact that most often we
have only one chance to look at and

45
00:03:18.219 --> 00:03:21.643
process streaming data
before more gets piled on.

46
00:03:21.643 --> 00:03:26.597
Streaming data management systems cannot
be separated from real-time processing

47
00:03:26.597 --> 00:03:27.160
of data.

48
00:03:29.130 --> 00:03:30.160
Managing and

49
00:03:30.160 --> 00:03:35.030
processing data in motion is a typical
capability of streaming data systems.

50
00:03:36.060 --> 00:03:38.791
However, the sheer size, variety and

51
00:03:38.791 --> 00:03:43.305
velocity of big data adds further
challenges to these systems.

52
00:03:43.305 --> 00:03:49.260
Such systems are designed to manage
relatively simple computations.

53
00:03:49.260 --> 00:03:52.640
Such as one record at a time or

54
00:03:52.640 --> 00:03:57.070
a set of objects in a short time
window of the most recent data.

55
00:03:58.460 --> 00:04:02.090
The computations are done
in near-real-time,

56
00:04:02.090 --> 00:04:06.910
sometimes in memory, and
as independent computations.

57
00:04:08.950 --> 00:04:14.099
The processing components
often subscribe to a system,

58
00:04:14.099 --> 00:04:18.074
or a stream source, non-interactively.

59
00:04:19.440 --> 00:04:24.580
This means they sent nothing
back to the source, nor

60
00:04:24.580 --> 00:04:26.710
did they establish
interaction with the source.

61
00:04:29.933 --> 00:04:35.133
The concept of dynamic steering involves
dynamically changing the next steps or

62
00:04:35.133 --> 00:04:38.345
direction of an application
through a continuous

63
00:04:38.345 --> 00:04:41.110
computational process using streaming.

64
00:04:42.460 --> 00:04:48.817
Dynamic steering is often a part of
streaming data management and processing.

65
00:04:48.817 --> 00:04:55.280
A self-driving car is a perfect example
of a dynamic steering application.

66
00:04:56.350 --> 00:05:00.330
But all streaming data applications
fall into this category.

67
00:05:00.330 --> 00:05:04.585
Such as the online gaming example we
discussed earlier in this course.

68
00:05:04.585 --> 00:05:08.893
Amazon Kinesis an other open-source Apache

69
00:05:08.893 --> 00:05:13.993
projects like Storm, Flink,
Spark Streaming, and

70
00:05:13.993 --> 00:05:18.880
Samza are examples of big
data streaming systems.

71
00:05:20.650 --> 00:05:23.770
Many other companies also
provide streaming systems for

72
00:05:23.770 --> 00:05:26.920
big data that are frequently
updated in response

73
00:05:26.920 --> 00:05:30.280
to the rapidly changing
nature of these technologies.

74
00:05:32.440 --> 00:05:38.300
As a summary, dynamic near-real-time
streaming data management,

75
00:05:38.300 --> 00:05:43.640
processing, and steering is an important
part of today's big data applications.

76
00:05:44.710 --> 00:05:47.590
Next, we will look at some
of the challenges for

77
00:05:47.590 --> 00:05:49.830
streaming data management and processing.
WEBVTT

1
00:00:01.280 --> 00:00:04.810
Now that we have seen what
streaming data means,

2
00:00:04.810 --> 00:00:08.690
let’s look at what makes streaming data
different and what some management and

3
00:00:08.690 --> 00:00:11.320
processing challenges for
streaming data are.

4
00:00:12.510 --> 00:00:16.130
After this video you will
be able to compare and

5
00:00:16.130 --> 00:00:18.980
contrast data in motion and data at rest.

6
00:00:20.450 --> 00:00:23.990
Differentiate between streaming and
batch data processing.

7
00:00:25.250 --> 00:00:31.070
And list management and
processing challenges for streaming data.

8
00:00:31.070 --> 00:00:33.560
We often hear the terms data addressed and

9
00:00:33.560 --> 00:00:36.665
data in motion,
when talking about big data management.

10
00:00:36.665 --> 00:00:43.260
Data-at-rest refers to mostly
static data collected from one or

11
00:00:43.260 --> 00:00:49.186
more data sources, and the analysis
happens after the data is collected.

12
00:00:49.186 --> 00:00:54.920
The term data-in-motion refers to a mode

13
00:00:54.920 --> 00:00:58.210
although similar data
collection methods apply,

14
00:00:58.210 --> 00:01:02.490
the data gets analyzed at the same
time it is being generated.

15
00:01:03.670 --> 00:01:09.300
Just like the sensor data processing
in a plane or a self-driving car.

16
00:01:09.300 --> 00:01:14.610
Analysis of data addressed is called
batch or static processing and

17
00:01:14.610 --> 00:01:18.080
the analysis of streaming data
is called stream processing.

18
00:01:19.640 --> 00:01:24.633
The run time and memory usage of most
algorithms that process static data,

19
00:01:24.633 --> 00:01:28.890
is usually dependent on the data size, and

20
00:01:28.890 --> 00:01:33.010
this size can easily be calculated
from files or databases.

21
00:01:34.670 --> 00:01:41.200
A key property, of streaming data
processing is the size of the data

22
00:01:41.200 --> 00:01:46.650
is unbounded and this changes the types
of algorithms that can be used.

23
00:01:48.050 --> 00:01:52.760
Algorithms that require iterating or
looping over the whole data set are not

24
00:01:52.760 --> 00:01:57.470
possible since with stream data,
you never get to the end.

25
00:01:58.730 --> 00:02:04.710
The modeling and management of streaming
data should enable computations on

26
00:02:04.710 --> 00:02:09.720
one data element or a small window
of group of recent data elements.

27
00:02:10.880 --> 00:02:14.820
These computations can update metrics,
monitor and

28
00:02:14.820 --> 00:02:18.050
plot statistics on the streaming data.

29
00:02:18.050 --> 00:02:22.120
Or apply analysis techniques
to the streaming data

30
00:02:22.120 --> 00:02:26.010
to learn about the dynamics of
the system as a time series.

31
00:02:27.120 --> 00:02:30.820
Since computations need to
be completed in real time,

32
00:02:30.820 --> 00:02:35.460
the analysis tasks processing
streaming data should be quicker or

33
00:02:35.460 --> 00:02:39.370
not much longer than
the streaming rate of the data.

34
00:02:39.370 --> 00:02:41.200
Which we define by it's velocity.

35
00:02:42.460 --> 00:02:45.970
In most streaming systems,
the management, and

36
00:02:45.970 --> 00:02:51.020
processing system subscribe to
the data source, but doesn't

37
00:02:51.020 --> 00:02:55.310
send anything back to the stream source
in terms of feedback or interactions.

38
00:02:57.070 --> 00:03:01.530
These requirements for streaming data
processing are quite different than batch

39
00:03:01.530 --> 00:03:06.330
processing where the analytical steps
have access to often, all data and

40
00:03:06.330 --> 00:03:11.890
can take more time to complete a complex
analytical task with less pressure

41
00:03:11.890 --> 00:03:16.360
on the completion time of individual
data management and processing tasks.

42
00:03:18.120 --> 00:03:21.756
Most organizations today
use a hybrid architecture.

43
00:03:21.756 --> 00:03:26.990
Sometimes get referred to as
the lambda architecture for

44
00:03:26.990 --> 00:03:31.490
processing streaming and
back jobs at the same time.

45
00:03:31.490 --> 00:03:38.130
In these systems, streaming wheel over
the real-time data is managed and

46
00:03:38.130 --> 00:03:43.230
kept until those data elements are pushed

47
00:03:43.230 --> 00:03:48.510
to a batch system and become available
to access and process as batch data.

48
00:03:49.870 --> 00:03:54.770
In such systems, a stream storage
layer is used to enable fast

49
00:03:54.770 --> 00:03:59.591
trees of streams and
ensure data ordering and consistency.

50
00:03:59.591 --> 00:04:01.398
And a processing layer for

51
00:04:01.398 --> 00:04:06.076
data is used to retrieve data from
the storage layer to analyze it and

52
00:04:06.076 --> 00:04:11.003
most probably little bit to a batch
data stream and notify the streaming

53
00:04:11.003 --> 00:04:16.810
storage that the data set does no
longer need to be in streaming storage.

54
00:04:16.810 --> 00:04:22.200
The big data challenges we discussed
were scalability, data replication,

55
00:04:22.200 --> 00:04:27.490
and durability, and fault tolerance arise
in this type of data very significantly.

56
00:04:28.690 --> 00:04:34.260
Among many there are two main
challenges that needs to be overcome

57
00:04:34.260 --> 00:04:39.820
to avoid data loss, and
enable real time analytical tasks.

58
00:04:39.820 --> 00:04:44.950
One challenge in streaming data
process is that the size and

59
00:04:44.950 --> 00:04:49.020
frequency of the mean data can
significantly change over time.

60
00:04:50.330 --> 00:04:55.910
These changes can be unpredictable and
may be driven by human behavior.

61
00:04:57.060 --> 00:05:02.340
For example, streaming data found on
social networks such as Facebook and

62
00:05:02.340 --> 00:05:05.025
Twitter can increase in
volume during holidays,

63
00:05:05.025 --> 00:05:08.160
sports matches, or major news events.

64
00:05:09.980 --> 00:05:15.910
These changes can be periodic and occur,
for example, in the evenings or weekends.

65
00:05:17.280 --> 00:05:22.470
For example, people may post messages
on Facebook more in the evening

66
00:05:22.470 --> 00:05:25.940
instead of during the day working hours.

67
00:05:25.940 --> 00:05:30.750
Streaming data changes may also
be unpredictable and sporadic.

68
00:05:30.750 --> 00:05:33.510
There can be an increase in data size and

69
00:05:33.510 --> 00:05:38.930
frequency during during major events,
sporting matches and things like that.

70
00:05:38.930 --> 00:05:44.950
Other changes include dropping or
missing data or even no data

71
00:05:44.950 --> 00:05:50.320
when there are network problems or device
generating the data has hardware problems.

72
00:05:50.320 --> 00:05:52.880
As an example of streaming
data fluctuation,

73
00:05:52.880 --> 00:05:54.964
consider the number of Tweets per second.

74
00:05:54.964 --> 00:06:01.050
On average,
there are 6,000 tweets sent every second.

75
00:06:01.050 --> 00:06:05.810
However, in August 2013,
the world record was

76
00:06:05.810 --> 00:06:10.980
set when over 144,000 tweets
were sent in a second.

77
00:06:10.980 --> 00:06:13.210
That's a factor of 24 increase.

78
00:06:15.320 --> 00:06:20.250
At the end of this lesson we will ask
you to focus on Twitter streams for

79
00:06:20.250 --> 00:06:23.160
trending topics and any other topic.

80
00:06:24.480 --> 00:06:27.810
You will notice how the rates
of Tweets streaming

81
00:06:27.810 --> 00:06:31.600
changes between different times and
different topics.

82
00:06:31.600 --> 00:06:38.010
To summarize, streaming data must be
handled differently than static data.

83
00:06:38.010 --> 00:06:43.150
Unlike static data, where you can
determine the size, streaming data is

84
00:06:43.150 --> 00:06:48.570
continually generated, and
you can not process it all at once.

85
00:06:50.250 --> 00:06:56.580
Streaming data can unpredictably
change in both size and frequency.

86
00:06:56.580 --> 00:06:58.330
This can be due to human behavior.

87
00:06:59.420 --> 00:07:01.910
Finally, algorithms for

88
00:07:01.910 --> 00:07:06.830
processing streaming data must
be relatively fast and simple.

89
00:07:06.830 --> 00:07:09.170
Since you don't know when
the next data arrives.
WEBVTT

1
00:00:02.380 --> 00:00:03.730
In a previous lecture,

2
00:00:03.730 --> 00:00:08.440
we said that new interesting solutions are
emerging in the big data product space.

3
00:00:09.780 --> 00:00:13.718
While these solutions do not have
the full fledged power of a DBMS,

4
00:00:13.718 --> 00:00:18.800
they offer novel feature combinations that
suit some application space just right.

5
00:00:19.970 --> 00:00:23.380
One of these products is Aerospike,

6
00:00:23.380 --> 00:00:28.040
which calls itself a distributed
NoSQL database and

7
00:00:28.040 --> 00:00:32.340
key value store, and
goes on to say that it is architected for

8
00:00:32.340 --> 00:00:36.700
the performance needs of
today's web scale applications.

9
00:00:36.700 --> 00:00:40.630
The diagram here is from
an Aerospike whitepaper.

10
00:00:41.920 --> 00:00:45.990
It shows how Aerospike relates to
the ecosystem for which it is designed.

11
00:00:47.340 --> 00:00:52.585
The top layer shows several applications
for real time consumer facing systems,

12
00:00:52.585 --> 00:00:57.115
such as travel recommendation systems,
pricing engines used for

13
00:00:57.115 --> 00:01:00.455
stock market applications,
real time decision systems that

14
00:01:00.455 --> 00:01:04.145
analyze data to figure out whether
an investment should be done and so forth.

15
00:01:05.405 --> 00:01:08.215
Now, all of these systems
have the common need

16
00:01:08.215 --> 00:01:12.385
that large amounts of data should be
accessible to them at any point of time.

17
00:01:14.090 --> 00:01:18.939
The Aerospike system can interoperate
with Hadoop-based systems, so

18
00:01:18.939 --> 00:01:23.170
Spark, or a Legacy database, or
even a real time data source.

19
00:01:23.170 --> 00:01:27.761
It can exchange large volumes
of data with any such source and

20
00:01:27.761 --> 00:01:32.274
serve fast lookups and
queries to the applications above.

21
00:01:32.274 --> 00:01:36.420
Now that translates to a very
high availability robust and

22
00:01:36.420 --> 00:01:38.460
strong consistency needs.

23
00:01:40.720 --> 00:01:46.600
The figure here presents a high level
architecture diagram of Aerospike.

24
00:01:46.600 --> 00:01:51.010
The first item to notice here
is what they call a fast bat,

25
00:01:52.410 --> 00:01:55.500
which essentially refers to
the left side of the architecture.

26
00:01:57.200 --> 00:02:00.700
The client system processes transactions.

27
00:02:00.700 --> 00:02:05.730
That is data that are primarily managed in
a primary index that is a key value store.

28
00:02:07.180 --> 00:02:11.950
This index stays in memory for
operational purposes.

29
00:02:11.950 --> 00:02:16.430
However, the server also interacts with
the storage layer for persistence.

30
00:02:17.910 --> 00:02:21.290
The storage layer uses three
kinds of storage systems,

31
00:02:22.770 --> 00:02:27.940
in memory with a dynamic RAM or
DRAM, a regular spinning disk,

32
00:02:29.040 --> 00:02:34.190
and flash/SSD, which is solid state device
for fast loading of data when needed.

33
00:02:35.410 --> 00:02:38.160
In fact the Aerospike system

34
00:02:38.160 --> 00:02:42.210
has optimized its performance with
characteristics of an SSD in mind.

35
00:02:44.320 --> 00:02:48.890
For those of who are not sure
what an SSD is, you can think of

36
00:02:48.890 --> 00:02:53.500
an SSD as a kind of storage device
whose random read performance is much

37
00:02:53.500 --> 00:02:57.610
faster than speeding hard disk and the
write performance is just a little slower.

38
00:02:59.150 --> 00:03:04.556
One vendor recently advertised its SSD
has sequence share read speeds of up to

39
00:03:04.556 --> 00:03:09.980
2,500 MBPS and the sequential write
speeds as fast as 1,500 MBPS.

40
00:03:12.435 --> 00:03:14.810
Now why is this important
in a big data discussion?

41
00:03:16.890 --> 00:03:21.300
When we speak of scalability, grade
efficiency, fast transactions, and so

42
00:03:21.300 --> 00:03:25.940
forth, we often do not mention
that part of the performance

43
00:03:25.940 --> 00:03:29.950
guarantee is governed by the combination
of hardware and software.

44
00:03:31.200 --> 00:03:36.200
So the ability to offer more efficient
persistent storage with fast IO

45
00:03:36.200 --> 00:03:41.540
implies that while a significant amount
of information can be stored on disk,

46
00:03:41.540 --> 00:03:45.714
it can be done without compromising
the overall system performance for

47
00:03:45.714 --> 00:03:48.819
an environment that
needs fast data loading.

48
00:03:51.490 --> 00:03:55.410
The second point of uniqueness
is a secondary index there.

49
00:03:56.760 --> 00:04:01.022
Aerospike built secondary index
fields that are non-primary keys.

50
00:04:01.022 --> 00:04:05.192
A non-primary key is a key attribute
that makes a tuple unique,

51
00:04:05.192 --> 00:04:07.560
but it has not been
chosen as a primary key.

52
00:04:08.940 --> 00:04:12.980
In Aerospike, secondary indices
are stored in main memory.

53
00:04:12.980 --> 00:04:17.770
They are built on every node in a cluster
and co-located with the primary index.

54
00:04:18.960 --> 00:04:23.224
Each secondary index entry
contains references to records,

55
00:04:23.224 --> 00:04:25.196
which are local to the node.

56
00:04:27.240 --> 00:04:32.414
As a key value store,
Aerospike uses standard database like

57
00:04:32.414 --> 00:04:39.450
scalar types like integer, string, and
so forth, as well as lists like Reddis.

58
00:04:39.450 --> 00:04:46.340
The map type is similar to the hashtag of
Reddis and contains attribute value pairs.

59
00:04:46.340 --> 00:04:52.390
Since it is focused on real time web
application, it supports geospatial data,

60
00:04:52.390 --> 00:04:56.930
like places with latitude and
longitude values or regency polygons.

61
00:04:59.020 --> 00:05:04.300
This allows them to perform KV store
operations, for example, like is

62
00:05:04.300 --> 00:05:09.640
the location of this in La Jolla,
which is a point-in-polygon query.

63
00:05:09.640 --> 00:05:14.120
Or a distance query, like find hotels
within three miles of my location.

64
00:05:15.700 --> 00:05:18.230
KV queries are constructed
programmatically.

65
00:05:19.750 --> 00:05:26.506
Now interestingly, Aerospike also provides
a more declarative language called AQL.

66
00:05:26.506 --> 00:05:31.840
AQL looks very similar to SQL,

67
00:05:31.840 --> 00:05:34.520
the Standard Query Language for
relational databases.

68
00:05:35.830 --> 00:05:37.360
A query like select name and

69
00:05:37.360 --> 00:05:43.460
age from user star profiles projects
out the name and age values

70
00:05:43.460 --> 00:05:48.250
from the profile's record set that
belongs to the ding space called users.

71
00:05:49.750 --> 00:05:53.080
The language also allows
advocate functions, like sum and

72
00:05:53.080 --> 00:05:56.800
average, and other user defined functions,

73
00:05:56.800 --> 00:06:00.379
which the system may evaluate through
a map produced time operation.

74
00:06:03.160 --> 00:06:06.880
We mentioned earlier that while most
medium assists today offer base

75
00:06:06.880 --> 00:06:11.850
guarantees, Aerospike, despite being
a distributor information system,

76
00:06:11.850 --> 00:06:13.740
actually offers ACID guarantees.

77
00:06:14.750 --> 00:06:17.250
This is accomplished using
a number of techniques.

78
00:06:19.338 --> 00:06:23.100
We'll consider a few of them to give you
a flavor of the mechanisms that current

79
00:06:23.100 --> 00:06:28.380
systems use to balance between
large scale data management and

80
00:06:28.380 --> 00:06:32.010
transaction management in a cluster
where nodes can join or leave.

81
00:06:33.800 --> 00:06:37.499
You may recall that consistency
means two different things,

82
00:06:38.640 --> 00:06:42.460
one is to ensure that all constraints,
like domain constraints, are satisfied.

83
00:06:43.940 --> 00:06:46.905
The second meaning is applied
to distributor systems and

84
00:06:46.905 --> 00:06:51.320
ensures all copies of a data
item in a cluster are in sync.

85
00:06:52.950 --> 00:06:56.040
For operations and
single keys with replication and

86
00:06:56.040 --> 00:07:01.070
secondary indices,
Aerospike provides immediate consistency

87
00:07:01.070 --> 00:07:04.720
using synchronous writes to
replicas within the cluster.

88
00:07:05.860 --> 00:07:10.083
Synchronous write means the write
process will be considered

89
00:07:10.083 --> 00:07:13.420
successful only if
the replica is all subdated.

90
00:07:14.768 --> 00:07:18.829
No other write is allowed on the record
while the object of replica is pending.

91
00:07:20.520 --> 00:07:24.949
So what happens if there is an increase
in the number of write operations due to

92
00:07:24.949 --> 00:07:26.800
an increase in ingestion rate?

93
00:07:27.890 --> 00:07:33.490
In Aerospike, it is possible to relax
this immediate consistency condition

94
00:07:33.490 --> 00:07:37.950
by bypassing some of
the consistency checks.

95
00:07:37.950 --> 00:07:42.670
But if this is done, eventual
consistency will still be enforced.

96
00:07:45.080 --> 00:07:50.890
Durability is achieved by storing data
in the flash SSD on every node and

97
00:07:50.890 --> 00:07:52.530
performing direct reads from the flash.

98
00:07:54.000 --> 00:07:57.920
Now durability is also maintained through
the process of replication because we have

99
00:07:57.920 --> 00:07:59.790
multiple copies of data.

100
00:07:59.790 --> 00:08:05.870
So even if one node fails, the latest copy
of the last data is available from one or

101
00:08:05.870 --> 00:08:08.190
more replica nodes in the same cluster,

102
00:08:08.190 --> 00:08:11.350
as well as in nodes residing
in remote clusters.

103
00:08:13.550 --> 00:08:16.250
But does that just
contradict the CAP theorem?

104
00:08:17.525 --> 00:08:22.700
The CAP theorem holds when
the network is partitioned.

105
00:08:24.220 --> 00:08:27.700
That means when nodes in
different parts of the network

106
00:08:27.700 --> 00:08:29.360
have different data content.

107
00:08:30.810 --> 00:08:35.750
Aerospike reduces and tries to completely
eliminate the situation by making

108
00:08:35.750 --> 00:08:41.250
sure that the master knows exactly
where all the other nodes are.

109
00:08:41.250 --> 00:08:44.260
And the replication is
happening properly even when

110
00:08:44.260 --> 00:08:46.300
the new nodes are joining the network.
WEBVTT

1
00:00:02.850 --> 00:00:07.270
In the previous modules, we talked
about data variety and streaming data.

2
00:00:08.410 --> 00:00:13.310
In this module, we'll focus on a central
issue in large scale data processing and

3
00:00:13.310 --> 00:00:19.540
management and that is when should
we use Hadoop or Yarn style system?

4
00:00:19.540 --> 00:00:24.230
And when should we use a database system
that can perform parallel operations?

5
00:00:24.230 --> 00:00:28.840
And then we'll explore how the state
of the art big data management systems

6
00:00:28.840 --> 00:00:31.370
address these issues of volume and
variety.

7
00:00:32.930 --> 00:00:36.510
We start with the problem
of high volume data and

8
00:00:36.510 --> 00:00:38.699
two contrasting approaches for
handling them.

9
00:00:40.150 --> 00:00:42.590
So after this video, you'll be able to

10
00:00:43.650 --> 00:00:47.270
explain the various advantages of
using a DBMS over a file system.

11
00:00:48.560 --> 00:00:52.500
Specify the differences between
parallel and distributed DBMS.

12
00:00:52.500 --> 00:00:57.362
And briefly describe
a MapReduce-style DBMS and

13
00:00:57.362 --> 00:01:01.537
its relationship with the current DBMSs.

14
00:01:01.537 --> 00:01:06.005
In the early days,
when database systems weren't around or

15
00:01:06.005 --> 00:01:11.450
just came in, databases were designed
as a set of application programs.

16
00:01:12.600 --> 00:01:17.390
They were written to handle data that
resided in files in a file system.

17
00:01:18.450 --> 00:01:21.310
However, soon this
approach led to problems.

18
00:01:22.520 --> 00:01:26.560
First, there are multiple file formats.

19
00:01:26.560 --> 00:01:32.370
And often, there was a duplication
of information in different files.

20
00:01:32.370 --> 00:01:36.940
Or the files simply had inconsistent
information that was very hard to

21
00:01:36.940 --> 00:01:41.150
determine, especially when the data was
large and the file content was complex.

22
00:01:43.550 --> 00:01:47.050
Secondly, there wasn't
a uniform way to access data.

23
00:01:48.090 --> 00:01:52.120
Each data access task, like finding
employees in a department sorted by their

24
00:01:52.120 --> 00:01:55.990
salary versus finding
employees in all departments

25
00:01:55.990 --> 00:02:00.060
sorted by their start date needed to
be written as a separate program.

26
00:02:01.160 --> 00:02:04.040
So people ended up writing
different programs for

27
00:02:04.040 --> 00:02:06.200
data access, as well as data update.

28
00:02:08.350 --> 00:02:11.890
A third problem was rooted to
the enforcement of constraints,

29
00:02:11.890 --> 00:02:14.050
often called integrity constraints.

30
00:02:14.050 --> 00:02:20.000
For example, to say something like every
employee has exactly one job title.

31
00:02:20.000 --> 00:02:23.530
One had arrived that condition,
as part of an application program called.

32
00:02:24.700 --> 00:02:27.670
So if you want to change the constraint,
you need to look for

33
00:02:27.670 --> 00:02:29.620
the programs where such
a rule is hard coded.

34
00:02:31.590 --> 00:02:35.300
The fourth problem has to
do with system failures.

35
00:02:35.300 --> 00:02:39.420
Supposed Joe, an employee becomes
the leader of a group and moves in to

36
00:02:39.420 --> 00:02:44.840
the office of the old leader, Ben who has
now become the director of the division.

37
00:02:44.840 --> 00:02:49.788
So we update Joe's details and
move on to update, Ben's new office for

38
00:02:49.788 --> 00:02:51.280
the system crashes.

39
00:02:51.280 --> 00:02:54.501
So the files are incompletely updated and

40
00:02:54.501 --> 00:02:58.096
there is no way to go back,
and start all over.

41
00:02:58.096 --> 00:03:04.138
The term atomicity means that all of
the changes that we need to do for

42
00:03:04.138 --> 00:03:09.657
these promotions must happen altogether,
as a single unit.

43
00:03:09.657 --> 00:03:14.874
They should either fully go through or
not go through at all.

44
00:03:14.874 --> 00:03:21.038
This atomicity is very difficult to handle
when the data reside in one or more files.

45
00:03:23.936 --> 00:03:29.173
So, a prime reason for the transition
to a DBMS is to alleviate these and

46
00:03:29.173 --> 00:03:30.842
other difficulties.

47
00:03:30.842 --> 00:03:34.507
If we look at the current DBMS,
especially relational DBMS,

48
00:03:34.507 --> 00:03:36.836
we will notice a number of advantages.

49
00:03:39.165 --> 00:03:42.710
DBMSs offer query languages,
which are declarative.

50
00:03:44.060 --> 00:03:48.200
Declarative means that we
state what we want to retrieve

51
00:03:48.200 --> 00:03:51.060
without telling the DBMS
how exactly to retrieve it.

52
00:03:52.220 --> 00:03:56.682
In a relational DBMS, we can say,
find the average set of salary of

53
00:03:56.682 --> 00:04:01.800
employees in the R&D division for
every job title and sort from high to low.

54
00:04:01.800 --> 00:04:05.779
We don't have to tell the system how
to group these records by job title or

55
00:04:05.779 --> 00:04:07.879
how to extract just the salary field.

56
00:04:10.073 --> 00:04:14.404
A typical user of a DBMS who issues
queries does not worry about how

57
00:04:14.404 --> 00:04:19.604
the relations are structured or whether
they are located in the same machine,

58
00:04:19.604 --> 00:04:21.835
or spread across five machines.

59
00:04:21.835 --> 00:04:25.835
The goal of data independence is to
isolate the users from the record

60
00:04:25.835 --> 00:04:28.993
layout so long as the logical
definition of the data,

61
00:04:28.993 --> 00:04:33.083
which means the tables and
their attributes are clearly specified.

62
00:04:35.195 --> 00:04:39.754
Now most importantly, relational DBMSs
have developed a very mature and

63
00:04:39.754 --> 00:04:44.389
continually improving methodology of
how to answer a query efficiently,

64
00:04:44.389 --> 00:04:47.255
even when there are a large
number of cables and

65
00:04:47.255 --> 00:04:50.590
the number of records
exceeds hundreds of millions.

66
00:04:51.800 --> 00:04:56.880
From a 2009 account,
EB uses the tera data system with

67
00:04:56.880 --> 00:05:02.280
72 machines to manage approximately
2.4 terabytes of relational data.

68
00:05:03.840 --> 00:05:08.496
These systems have built powerful
data structures, algorithms and

69
00:05:08.496 --> 00:05:12.991
sound principles to determine how
a specific array should be onset

70
00:05:12.991 --> 00:05:18.070
efficiently despite the size of the data
and the complexity of the tables.

71
00:05:18.070 --> 00:05:22.790
Now with any system,
bad things can happen.

72
00:05:22.790 --> 00:05:25.266
Systems fail in the middle
of an operation.

73
00:05:25.266 --> 00:05:29.489
Malicious processes try to get
unauthorized access to data.

74
00:05:29.489 --> 00:05:33.405
One large can often underappreciated
aspect of a DBMS is

75
00:05:33.405 --> 00:05:39.060
the implementation of transaction
safety and failure recovery.

76
00:05:39.060 --> 00:05:40.950
Now, recall our discussion of atomicity.

77
00:05:42.110 --> 00:05:47.082
In databases, a single logical operation
on the data is called a transaction.

78
00:05:47.082 --> 00:05:51.314
For example, a transfer of funds
from one bank account to another,

79
00:05:51.314 --> 00:05:55.253
even involving multiple changes
like debiting one account and

80
00:05:55.253 --> 00:05:58.091
crediting another is a single transaction.

81
00:05:58.091 --> 00:06:02.894
Now, atomicity is one of the four
properties that a transaction should

82
00:06:02.894 --> 00:06:04.350
provide.

83
00:06:04.350 --> 00:06:09.740
The four properties,
collectively called ACID are atomicity,

84
00:06:09.740 --> 00:06:13.370
consistency, isolation and durability.

85
00:06:14.730 --> 00:06:19.370
Consistency means any data
written to the database must be

86
00:06:19.370 --> 00:06:24.301
valid according to all defined
rules including constrains.

87
00:06:24.301 --> 00:06:29.747
The durability property ensures that
once a transaction has been committed,

88
00:06:29.747 --> 00:06:34.709
it will remain so, even in the event
of power loss, crashes or errors.

89
00:06:36.816 --> 00:06:40.573
The isolation property comes
in the context of concurrency,

90
00:06:40.573 --> 00:06:44.920
which refers to multiple people
updating a database simultaneously.

91
00:06:45.970 --> 00:06:50.640
To understand concurrency, think of
an airline or a railway reservation system

92
00:06:50.640 --> 00:06:54.330
where hundreds and thousands of
people are buying, cancelling and

93
00:06:54.330 --> 00:06:57.110
changing their reservations and
tickets all at the same time.

94
00:06:58.360 --> 00:07:02.710
The DBMS must be sure that
a ticket should no be sold twice.

95
00:07:02.710 --> 00:07:06.920
Or if one person is in the middle
of buying the last ticket,

96
00:07:06.920 --> 00:07:09.840
another person does not see
that ticket as available.

97
00:07:11.380 --> 00:07:15.480
These are guaranteed by
the isolation property that says,

98
00:07:15.480 --> 00:07:19.340
not withstanding the number of people
accessing the system at the same time.

99
00:07:19.340 --> 00:07:26.310
The transactions must happen as if they're
done serially, that is one after another.

100
00:07:26.310 --> 00:07:33.949
Providing these capabilities is
an important part of the M in DBMS.

101
00:07:33.949 --> 00:07:38.390
So next, we consider how traditional
databases handle large data volumes.

102
00:07:39.920 --> 00:07:44.093
The classical way in which DBMSs
have handled the issue of large

103
00:07:44.093 --> 00:07:48.120
volumes is by created parallel and
distributed databases.

104
00:07:48.120 --> 00:07:52.244
In a parallel database, for
example, parallel Oracle,

105
00:07:52.244 --> 00:07:54.357
parallel DB2 or post SQL XE.

106
00:07:54.357 --> 00:07:59.966
The tables are spread across multiple
machines and operations like selection,

107
00:07:59.966 --> 00:08:03.853
and join use parallel algorithms
to be more efficient.

108
00:08:03.853 --> 00:08:07.905
These systems also allow a user
to create a replication.

109
00:08:07.905 --> 00:08:10.400
That is multiple copies of tables.

110
00:08:10.400 --> 00:08:13.420
Thus, introducing data redundancy, so

111
00:08:13.420 --> 00:08:18.180
that failure on replica can be
compensated for by using another.

112
00:08:19.740 --> 00:08:23.678
Further, it replicates in
sync with each other and

113
00:08:23.678 --> 00:08:27.260
a query can result into
any of the replicates.

114
00:08:27.260 --> 00:08:32.021
This increases the number of simultaneous
that is conquer into queries

115
00:08:32.021 --> 00:08:34.332
that can be handled by the system.

116
00:08:34.332 --> 00:08:39.942
In contrast, a distributed DBMS, which
we'll not discuss in detail in this course

117
00:08:39.942 --> 00:08:46.200
is a network of independently running
DBMSs that communicate with each other.

118
00:08:46.200 --> 00:08:51.275
In this case, one component knows some
part of the schema of it is neighboring

119
00:08:51.275 --> 00:08:55.670
DBMS and can pass a query or part of
a query to the neighbor when needed.

120
00:08:57.520 --> 00:09:01.360
So the important takeaway issue here is,

121
00:09:01.360 --> 00:09:04.890
are all of these facilities
offered by a DBMS important for

122
00:09:04.890 --> 00:09:07.430
the big data application that
you are planning to build?

123
00:09:08.590 --> 00:09:11.131
And the answer in many
cases can be negative.

124
00:09:11.131 --> 00:09:16.304
However, if these issues are important,
then the database management

125
00:09:16.304 --> 00:09:20.726
systems may offer a viable option for
a big data application.

126
00:09:23.034 --> 00:09:27.726
Now, let's take a little more time to
address an issue that's often discussed in

127
00:09:27.726 --> 00:09:28.819
the big data word.

128
00:09:28.819 --> 00:09:32.300
The question is if DBMSs are so powerful,

129
00:09:32.300 --> 00:09:37.178
why do we see the emergence
of MapReduce-style Systems?

130
00:09:37.178 --> 00:09:41.800
Unfortunately, the answer to this
question is not straightforward.

131
00:09:43.660 --> 00:09:49.180
For a long while now, DBMSs have
effectively used parallelism, specifically

132
00:09:49.180 --> 00:09:54.030
parallel databases in addition to
replication would also create partitions.

133
00:09:55.220 --> 00:10:00.017
So that different parts of a logical
table can physically reside on different

134
00:10:00.017 --> 00:10:05.403
machines,, then different parts of a query
can access the partitions in parallel and

135
00:10:05.403 --> 00:10:07.501
speed up creative performance.

136
00:10:07.501 --> 00:10:11.753
Now these algorithms not only improve
the operating efficiency, but

137
00:10:11.753 --> 00:10:16.882
simultaneously optimize algorithms to
take into account the communication cost.

138
00:10:16.882 --> 00:10:21.511
That is the time needed to
exchange data between machines.

139
00:10:21.511 --> 00:10:28.390
However, classical parallel DBMSs did
not take into account machine failure.

140
00:10:29.700 --> 00:10:35.070
And in contrast, MapReduce was
originally developed not for storage and

141
00:10:35.070 --> 00:10:38.850
retrieval, but for distributive
processing of large amounts of data.

142
00:10:39.890 --> 00:10:44.783
Specifically, its goal was to support
complex custom computations that could

143
00:10:44.783 --> 00:10:47.569
be performed efficiently on many machines.

144
00:10:47.569 --> 00:10:50.259
So in a MapReduce or MR setting,

145
00:10:50.259 --> 00:10:54.357
the number of machines
could go up to thousands.

146
00:10:54.357 --> 00:10:58.710
Now since MR implementations were
done over Hadoop file systems,

147
00:10:58.710 --> 00:11:02.760
issues like node failure were
automatically accounted for.

148
00:11:04.492 --> 00:11:09.017
So MR effectively used in complex
applications like data mining or

149
00:11:09.017 --> 00:11:13.703
data clustering, and these algorithms
are often very complex, and

150
00:11:13.703 --> 00:11:17.202
typically require problem
specific techniques.

151
00:11:17.202 --> 00:11:20.554
Very often,
these algorithms have multiple stages.

152
00:11:20.554 --> 00:11:25.891
That is the output from one processing
stage is the input to the next.

153
00:11:25.891 --> 00:11:29.519
It is difficult to develop these
multistage algorithms in a standard

154
00:11:29.519 --> 00:11:30.650
relational system.

155
00:11:31.990 --> 00:11:36.080
But since these were genetic operations,
many of them were designed to work

156
00:11:36.080 --> 00:11:39.880
with unstructured data like text and
nonstandard custom data formats.

157
00:11:41.730 --> 00:11:46.139
Now, it's now amply clear that this
mixture of data management requirements

158
00:11:46.139 --> 00:11:50.818
and data processing analysis requirements
have created an interesting tension in

159
00:11:50.818 --> 00:11:52.439
the data management world.

160
00:11:52.439 --> 00:11:56.030
Just look at a few of
these tension points.

161
00:11:56.030 --> 00:12:00.990
Now, DBMSs perform storage and
retrieval operations very efficiently.

162
00:12:02.210 --> 00:12:05.170
But first,
the data must be loaded into the DBMS.

163
00:12:06.230 --> 00:12:07.490
So, how much time does loading take?

164
00:12:08.690 --> 00:12:12.900
In one study,
scientists use two CVS files.

165
00:12:12.900 --> 00:12:17.850
One had 92 attributes with
about 165 million tuples for

166
00:12:17.850 --> 00:12:19.890
a total size of 85 gigabytes.

167
00:12:21.190 --> 00:12:25.710
And the other had 227 attributes
with 5 million tuples for

168
00:12:25.710 --> 00:12:27.390
a total size of 5 gigabytes.

169
00:12:28.850 --> 00:12:35.610
The time to load and index this data in
MySQL and PostgreSQL, took 15 hours each.

170
00:12:36.690 --> 00:12:41.240
In a commercial database running on
three machines, it took two hours.

171
00:12:41.240 --> 00:12:44.980
Now there are applications like
the quantities in the case we discussed

172
00:12:44.980 --> 00:12:49.180
earlier where this kind of loading
time is simply not acceptable,

173
00:12:49.180 --> 00:12:51.880
because the analysis on
the data must be performed

174
00:12:51.880 --> 00:12:54.070
within a given time limit
after it's arrival.

175
00:12:56.600 --> 00:12:59.780
A second problem faced by
some application is that for

176
00:12:59.780 --> 00:13:03.140
them, the DBMSs offer
too much functionality.

177
00:13:04.160 --> 00:13:08.460
For example, think of an application
that only looks at the price of an item

178
00:13:08.460 --> 00:13:10.700
if you provide it with a product name or
product code.

179
00:13:12.210 --> 00:13:14.850
The number of products it serves
is let's say, 250 million.

180
00:13:16.170 --> 00:13:19.899
This lookup operation happens
only on a single table and

181
00:13:19.899 --> 00:13:23.164
does not mean anything
more complex like a join.

182
00:13:23.164 --> 00:13:27.963
Further, consider that while there
are several hundred thousand customers

183
00:13:27.963 --> 00:13:31.810
who access this data,
none of them really update the tables.

184
00:13:31.810 --> 00:13:36.685
So, do we need a full function DBMS for
this read-only application?

185
00:13:36.685 --> 00:13:41.175
Or can we get a simpler solution which
can use a cluster of machines, but

186
00:13:41.175 --> 00:13:44.275
does not provide all the wonderful
guarantees that a DBMS provides?

187
00:13:46.600 --> 00:13:51.940
At the other end of the spectrum,
there is an emerging class of optimization

188
00:13:51.940 --> 00:13:56.590
that meets all the nice transactional
guarantees that a DBMS provides.

189
00:13:56.590 --> 00:14:00.500
And at the same time, meets the support
for efficient analytical operations.

190
00:14:01.720 --> 00:14:05.741
These are often required for
systems like Real-Time Decision Support.

191
00:14:05.741 --> 00:14:10.141
That will accept real-time data like
customer purchases on a newly released

192
00:14:10.141 --> 00:14:13.188
product will perform some
statistical analysis, so

193
00:14:13.188 --> 00:14:15.365
that it can determine buying trends.

194
00:14:15.365 --> 00:14:17.969
And then decide whether in real-time,

195
00:14:17.969 --> 00:14:21.040
a discount can be offered
to this customer now.

196
00:14:23.860 --> 00:14:28.368
It turns out that the combination
of traditional requirements and

197
00:14:28.368 --> 00:14:32.072
new requirements is leading
to new capabilities, and

198
00:14:32.072 --> 00:14:35.465
products in the big data
management technology.

199
00:14:35.465 --> 00:14:40.283
On the one hand, DBMS technologies
are creating new techniques that make

200
00:14:40.283 --> 00:14:43.164
use of MapReduce-style data processing.

201
00:14:43.164 --> 00:14:46.473
Many of them are being
developed to run on HDFS and

202
00:14:46.473 --> 00:14:50.279
take advantage of his data
replication capabilities.

203
00:14:50.279 --> 00:14:54.423
More strikingly, DBMSs are beginning
to have a side door for

204
00:14:54.423 --> 00:14:58.405
a user to perform and
MR-style operation on HDFS files and

205
00:14:58.405 --> 00:15:02.399
exchange data between the Hadoop
subsystem and the DBMS.

206
00:15:02.399 --> 00:15:06.862
Thus, giving the user the flexibility
to use both forms of data processing.

207
00:15:09.674 --> 00:15:12.562
It has now been recognized
that a simple map and

208
00:15:12.562 --> 00:15:17.155
reduce operations are not sufficient for
many data operations leading to

209
00:15:17.155 --> 00:15:22.740
a significant expansion in the number
of operations in the MR ecosystems.

210
00:15:22.740 --> 00:15:26.510
For example,
Spark has several kinds of join and

211
00:15:26.510 --> 00:15:29.350
data grouping operations in
addition to map and reduce.

212
00:15:31.650 --> 00:15:34.900
Sound DBMSs are making use of large

213
00:15:34.900 --> 00:15:38.190
distributed memory management
operations to accept streaming data.

214
00:15:39.400 --> 00:15:43.510
These systems are designed with the idea
that the analysis they need to perform on

215
00:15:43.510 --> 00:15:45.400
the data are known before.

216
00:15:45.400 --> 00:15:49.860
And as new data records arrive,
they keep a record of the data

217
00:15:49.860 --> 00:15:53.820
in the memory long enough to finish
the computation needed on that record.

218
00:15:55.310 --> 00:15:57.610
And finally, computer scientists and

219
00:15:57.610 --> 00:16:00.770
data scientists are working
towards new solutions

220
00:16:00.770 --> 00:16:05.770
where large scale distributed algorithms
are beginning to emerge to solve different

221
00:16:05.770 --> 00:16:09.280
kinds of analytics problems like
finding dense regions of a graph.

222
00:16:10.610 --> 00:16:14.210
These algorithms use
a MR-style computing and

223
00:16:14.210 --> 00:16:17.880
are becoming a part of a new
generation of DBMS products

224
00:16:17.880 --> 00:16:20.860
that invoke these algorithms
from inside the database system.

225
00:16:21.890 --> 00:16:25.720
In the next video, we'll take a look at
some of the modern day data management

226
00:16:25.720 --> 00:16:28.030
systems that have some
of these capabilities.
WEBVTT

1
00:00:02.600 --> 00:00:07.300
As we mentioned in the last lesson, there
is no single irricuteous solution for

2
00:00:07.300 --> 00:00:08.200
big data problems.

3
00:00:08.200 --> 00:00:10.960
So in this lesson,

4
00:00:10.960 --> 00:00:14.900
our goal will be to explore some existing
solutions in a little more depth.

5
00:00:16.780 --> 00:00:19.290
So after this lesson, you'll be able to.

6
00:00:20.540 --> 00:00:23.250
Explain the at least five
desirable characteristics of

7
00:00:23.250 --> 00:00:29.150
a Big Data Management System,
explain the differences between acid and

8
00:00:29.150 --> 00:00:32.070
base, and

9
00:00:32.070 --> 00:00:37.210
list examples of BDMSs and describe some
of their similarities and differences.

10
00:00:39.260 --> 00:00:44.890
So we start from high level, suppose there
were an ideal big data management system.

11
00:00:46.170 --> 00:00:48.610
What capabilities or
features should such a system have?

12
00:00:50.000 --> 00:00:53.700
Professor Michael Carey of
the University of California Irvine

13
00:00:53.700 --> 00:00:56.750
has described a number of characteristics.

14
00:00:56.750 --> 00:01:00.590
We'll go through them and
use them as an idealistic yardstick

15
00:01:00.590 --> 00:01:03.190
against which we compare
existing solutions.

16
00:01:04.500 --> 00:01:10.820
First, the ideal BDMS would allow for
a semi-structured data model.

17
00:01:10.820 --> 00:01:15.060
Now that does not mean it will only
support a specific format like XML.

18
00:01:16.170 --> 00:01:18.550
The operative word here is flexible.

19
00:01:19.660 --> 00:01:22.390
The flexibility can take many forms,

20
00:01:23.600 --> 00:01:27.920
one of which Is the degree to which
schemas should be supported by the system.

21
00:01:29.190 --> 00:01:31.810
In a perfect world, it should support

22
00:01:31.810 --> 00:01:35.550
a completely traditional application which
requires the development of a schema.

23
00:01:37.250 --> 00:01:41.840
At the same time, it should also support
applications which require no schema,

24
00:01:41.840 --> 00:01:45.420
because the data can vary in terms
of its attributes and relationships.

25
00:01:47.790 --> 00:01:53.290
A different axis of flexibility
is in the data types it supports.

26
00:01:53.290 --> 00:01:56.810
For example, it should support
operations on text and documents.

27
00:01:57.920 --> 00:02:03.010
It should also permit social media and
other data that have a time component and

28
00:02:03.010 --> 00:02:06.820
need temporal operations, like before,
after, during, and so on.

29
00:02:08.260 --> 00:02:11.340
Similarly, it should
allow spacial data and

30
00:02:11.340 --> 00:02:15.530
allow operations like find all data
within a five mile radius of a landmark.

31
00:02:17.960 --> 00:02:20.290
As we saw in a previous lesson,

32
00:02:20.290 --> 00:02:24.260
a big advantage of a DBMS is that
is provides a query language.

33
00:02:25.800 --> 00:02:30.360
There is a notion that query languages
present a steep learning curve for

34
00:02:30.360 --> 00:02:33.690
data science people with no
background in computer science.

35
00:02:33.690 --> 00:02:37.900
However, to effectively
manage large volumes of data,

36
00:02:37.900 --> 00:02:41.190
it's often more convenient
to use a query language and

37
00:02:41.190 --> 00:02:45.720
let the query processor automatically
determine optimal ways to receive data.

38
00:02:47.270 --> 00:02:50.440
Now this query language may or
may not look like SQL,

39
00:02:50.440 --> 00:02:55.120
which is the standard query language used
by the modern relational systems, but

40
00:02:55.120 --> 00:02:57.870
it should at least be equally powerful.

41
00:02:59.080 --> 00:03:04.770
Now this is not an unreasonable feature
given that most DBMS vendors offer their

42
00:03:04.770 --> 00:03:12.120
own extension of SQL Of course it's not
enough to just have a good query language.

43
00:03:13.430 --> 00:03:18.230
Today's big data systems must
have a parallel query engine

44
00:03:18.230 --> 00:03:20.080
which will run on multiple machines.

45
00:03:21.480 --> 00:03:25.240
The machines can be connected to
a shared nothing architecture, or

46
00:03:25.240 --> 00:03:28.760
shared memory architecture,
or a shared cluster.

47
00:03:28.760 --> 00:03:34.260
The shared nothing means two machines
do not share a disk or memory.

48
00:03:34.260 --> 00:03:36.750
But this is a critical requirement for

49
00:03:36.750 --> 00:03:41.850
any BDMS regardless of how complete
the supported query languages is for

50
00:03:41.850 --> 00:03:46.970
efficiency sake Continuing with our list,
the next

51
00:03:46.970 --> 00:03:51.620
capability of a BDMS is often
not emphasized as much as it should be.

52
00:03:52.890 --> 00:03:56.880
Some applications working on top
of a BDMS will issue queries

53
00:03:56.880 --> 00:04:01.200
which will only have a few conditions and
a few small objects to return.

54
00:04:01.200 --> 00:04:06.740
But some applications, especially those
generated by other software tools or

55
00:04:06.740 --> 00:04:10.380
machine learning algorithms can
have many many conditions and

56
00:04:10.380 --> 00:04:11.990
can return many large objects.

57
00:04:13.090 --> 00:04:18.320
In my own work, we have seen how internet
bots can blast an information system

58
00:04:18.320 --> 00:04:22.230
with really large queries that
can potentially choke the system.

59
00:04:22.230 --> 00:04:23.580
But that should not happen in a BDMS.

60
00:04:25.810 --> 00:04:28.080
Now, we discussed streaming
data in a previous lesson.

61
00:04:29.600 --> 00:04:33.830
In many cases,
a BDMS will have both streaming data,

62
00:04:33.830 --> 00:04:37.590
which adds to the volume,
as well as to the large data that

63
00:04:37.590 --> 00:04:40.380
need to be combined with the streaming
data to solve a problem.

64
00:04:41.430 --> 00:04:45.140
An example would be to combine
streaming data from weather stations

65
00:04:45.140 --> 00:04:48.870
with historical data to make
better predictions for wild fire.

66
00:04:51.610 --> 00:04:54.510
We have discussed the definition,
significance and

67
00:04:54.510 --> 00:04:57.140
importance of scalability before.

68
00:04:57.140 --> 00:05:00.130
However, what a BDMS needs to guarantee

69
00:05:00.130 --> 00:05:05.260
that it is designed to operate over a
cluster, possibly a cluster of hundred or

70
00:05:05.260 --> 00:05:09.450
thousand of machines and
that it knows how to handle a failure.

71
00:05:11.040 --> 00:05:15.200
Further the system should be able
to handling new machines joining or

72
00:05:15.200 --> 00:05:16.829
existing machines leaving the cluster.

73
00:05:18.910 --> 00:05:23.530
Finally, our BDMS must have
data management capabilities.

74
00:05:23.530 --> 00:05:29.080
It should be easy to install, restart and
configure, provide high availability and

75
00:05:29.080 --> 00:05:33.340
make operational management as
simple as possible even when

76
00:05:33.340 --> 00:05:37.880
a BDMS is declined across data centers
that are possibly geographically apart

77
00:05:40.770 --> 00:05:45.580
In a prior module, we discussed
the ACID properties of transactions and

78
00:05:45.580 --> 00:05:47.880
said that BDMSs guarantee them.

79
00:05:49.250 --> 00:05:53.270
For big data systems,
there is too much data and

80
00:05:53.270 --> 00:05:56.540
too many updates from too many users.

81
00:05:56.540 --> 00:05:59.730
So the effort to maintain ACID properties

82
00:05:59.730 --> 00:06:02.330
May lead to a significant
slowdown of the system.

83
00:06:03.550 --> 00:06:07.590
Now, this lead to the idea, that while
the ACID properties are still desirable,

84
00:06:08.840 --> 00:06:14.410
it might be more practical to relax
the ACID conditions and replace them

85
00:06:14.410 --> 00:06:19.770
with what's called the BASE properties,
beginning with Basic availability.

86
00:06:21.410 --> 00:06:24.060
This states that the system

87
00:06:24.060 --> 00:06:26.600
does guarantee the availability
of data in the following sense.

88
00:06:28.140 --> 00:06:32.710
If you make a request,
there will be a response to that request.

89
00:06:32.710 --> 00:06:36.310
But, the response could still
be failure to obtain data or

90
00:06:36.310 --> 00:06:39.800
the data isn't Inconsistent state or
changing state.

91
00:06:41.370 --> 00:06:44.970
Well this is not unusual because
it's much like waiting for

92
00:06:44.970 --> 00:06:46.540
a check to clear a bank account.

93
00:06:49.270 --> 00:06:51.580
Second, there is a soft state.

94
00:06:52.620 --> 00:06:57.650
Which means the state of the system
is very likely to change over time.

95
00:06:57.650 --> 00:07:00.030
So even during times without input,

96
00:07:00.030 --> 00:07:04.330
there may be changes going on through
the system due to eventual consistency.

97
00:07:05.680 --> 00:07:08.190
Thus the state of
the system is always soft.

98
00:07:09.640 --> 00:07:12.080
And finally there's eventual consistency.

99
00:07:13.330 --> 00:07:18.010
This means that the system will
eventually become consistent

100
00:07:18.010 --> 00:07:19.770
once it stops receiving input.

101
00:07:21.870 --> 00:07:23.730
When it stops receiving input,

102
00:07:23.730 --> 00:07:28.800
the data will propagate to everywhere
that it should sooner or later go to but

103
00:07:28.800 --> 00:07:32.840
in reality the system will
continue to receive input.

104
00:07:32.840 --> 00:07:36.840
And it's not checking the consistency
of every transaction at every moment

105
00:07:36.840 --> 00:07:40.610
because there's still lots
of transactions to process.

106
00:07:40.610 --> 00:07:45.904
So, if you make a new Facebook post,
your friend in Zambia, who is supposed

107
00:07:45.904 --> 00:07:51.200
to see your update but is served by a very
different data center in a different

108
00:07:51.200 --> 00:07:56.116
geographic region, will certainly
see it but may not see right away.

109
00:07:58.272 --> 00:08:02.066
Now for those of you who have a bit
of computer science background,

110
00:08:02.066 --> 00:08:06.744
we just want to mention in passing that
there is actually some theoretical results

111
00:08:06.744 --> 00:08:08.260
behind this relaxation.

112
00:08:09.490 --> 00:08:13.090
The result comes from what's
called the CAP Theorem.

113
00:08:13.090 --> 00:08:16.840
Also named Bauer's theorem after
the computer scientist Eric Bauer.

114
00:08:18.200 --> 00:08:23.410
He states, that it is impossible for
a distributed computer system

115
00:08:23.410 --> 00:08:27.420
to simultaneously provide all
three of the following guarantees.

116
00:08:28.680 --> 00:08:30.300
Consistency.

117
00:08:30.300 --> 00:08:34.970
It means all nodes see the
same data at any time.

118
00:08:37.080 --> 00:08:42.000
Availability, which is a guarantee
that every request receives a response

119
00:08:42.000 --> 00:08:43.948
about whether it succeeded or failed.

120
00:08:43.948 --> 00:08:48.150
And Partition Tolerance.

121
00:08:48.150 --> 00:08:51.270
Which means the system
continues to operate

122
00:08:51.270 --> 00:08:55.610
despite arbitrary partitioning
due to network failures.

123
00:08:55.610 --> 00:08:59.740
Now, most of the big data systems
available today will adhere

124
00:08:59.740 --> 00:09:04.130
to these BASE properties,
although several modern systems

125
00:09:04.130 --> 00:09:08.010
do offer the stricter ACID properties,
or at least several of them.

126
00:09:10.180 --> 00:09:13.060
Now given the idealistic background
of what is desirable and

127
00:09:13.060 --> 00:09:17.060
achievable in a big data system,
today's marketplace for

128
00:09:17.060 --> 00:09:20.100
big data related products
looks somewhat like this.

129
00:09:21.680 --> 00:09:26.350
Now this is Matt Turk's depiction of big
data products from a couple of years back.

130
00:09:27.750 --> 00:09:31.730
You would notice that the products
are grouped into categories, like no SQL,

131
00:09:31.730 --> 00:09:36.340
massively parallel databases, analytic
systems, real time systems, and so forth.

132
00:09:37.430 --> 00:09:42.050
In this lesson, we'll do a quick
tour through a few of these products

133
00:09:43.690 --> 00:09:45.470
from different areas of this landscape.

134
00:09:47.140 --> 00:09:53.070
In each case our goal will to be assess
what aspects of our ideal BDMS they cover,

135
00:09:53.070 --> 00:09:56.240
and whether they have obvious limitations.

136
00:09:56.240 --> 00:10:01.650
Now we will not cover all features
of every system, but will highlight

137
00:10:01.650 --> 00:10:05.130
those aspects of the system that
are relevant to our discussion on BDMS.
WEBVTT

1
00:00:02.542 --> 00:00:07.110
The first up in our rapid tour
of modern systems is Redis.

2
00:00:08.420 --> 00:00:12.690
Redis calls itself an in-memory
data structure store.

3
00:00:12.690 --> 00:00:18.540
In simple terms, Redis is not a full blown
DBMS, in the sense we discussed earlier.

4
00:00:19.620 --> 00:00:24.490
It can persist data on disks, and
does so to save its state, but

5
00:00:24.490 --> 00:00:29.840
it's intended use is to optimally
use memory and memory based methods

6
00:00:29.840 --> 00:00:33.840
to make a number of common data
structures very fast for lots of users.

7
00:00:35.000 --> 00:00:37.649
Here is a list of data
structures that Redis supports.

8
00:00:41.084 --> 00:00:46.410
A good way to think about them is
to think of a data lookup problem.

9
00:00:46.410 --> 00:00:50.160
Now, in the simplest case,
a lookup needs a key value pair

10
00:00:50.160 --> 00:00:54.650
where the key is a string and
the value is also a string.

11
00:00:54.650 --> 00:00:59.000
So for a lookup we provide the key and
get back the value.

12
00:00:59.000 --> 00:01:00.550
Simple, right?

13
00:01:00.550 --> 00:01:01.050
Let's see.

14
00:01:02.760 --> 00:01:05.510
I'm sure you've seen captures like this.

15
00:01:07.090 --> 00:01:11.770
These are small images used by websites
to ensure that the user is a human and

16
00:01:11.770 --> 00:01:12.370
not a robot.

17
00:01:13.470 --> 00:01:17.810
The images presented to the user,
who is supposed to write the text he or

18
00:01:17.810 --> 00:01:20.990
she sees in the image, into a text box.

19
00:01:20.990 --> 00:01:23.010
Upon success, the user is let in.

20
00:01:24.100 --> 00:01:28.900
To implement this,
one obviously needs a key value stored,

21
00:01:28.900 --> 00:01:31.500
where the key is the idea of the image.

22
00:01:31.500 --> 00:01:33.230
And the value is the desired text.

23
00:01:34.610 --> 00:01:39.900
Now what if we wanted to use
the image itself as the key,

24
00:01:39.900 --> 00:01:42.940
instead of an ID number that
you generate separately?

25
00:01:44.580 --> 00:01:46.160
The content of the image,

26
00:01:46.160 --> 00:01:51.400
which let's say is a .jpg file,
can be thought of as a binary string.

27
00:01:51.400 --> 00:01:52.706
But can it serve as a key then?

28
00:01:52.706 --> 00:01:56.050
According to the Redis specification,
it can.

29
00:01:57.360 --> 00:02:00.050
The Redis string can be binary and

30
00:02:00.050 --> 00:02:05.770
can have a size of up to 512 megabytes,
although its internal limit is higher.

31
00:02:06.840 --> 00:02:12.169
So, small images like this can indeed
be used as binary string keys.

32
00:02:14.040 --> 00:02:19.100
In some application scenarios,
keys may have an internal structure.

33
00:02:19.100 --> 00:02:23.090
For example,
product codes may have product family,

34
00:02:23.090 --> 00:02:28.040
manufacturing batch, and the actual
product ID strung together into one ID.

35
00:02:29.250 --> 00:02:33.710
The example shown here is a typical
Twitter style key to store the response

36
00:02:33.710 --> 00:02:35.500
to comment one, two, three, four.

37
00:02:37.240 --> 00:02:40.980
How long do we want to
keep the comment around,

38
00:02:40.980 --> 00:02:45.240
this is a standard big data issue
when it comes to streaming data

39
00:02:45.240 --> 00:02:48.680
whose data values have limited
utility beyond the certain period.

40
00:02:50.540 --> 00:02:52.000
One typically would not look for

41
00:02:52.000 --> 00:02:55.110
this response possibly three
months after the conversation.

42
00:02:56.410 --> 00:03:00.490
One would certainly not like to keep
such a value in memory for a long time.

43
00:03:00.490 --> 00:03:04.250
Because the memory is being used as
a cache for rapid access to current data.

44
00:03:05.510 --> 00:03:11.250
In fact Redis has the ability
to delete an expired key and

45
00:03:11.250 --> 00:03:14.230
can be made to call a function
to generate a new key.

46
00:03:16.210 --> 00:03:19.580
An interesting side
benefits to structured keys

47
00:03:19.580 --> 00:03:22.830
that it can encode
a hierarchy to the structure.

48
00:03:22.830 --> 00:03:23.490
In the example,

49
00:03:23.490 --> 00:03:28.449
we show keys that represent
increasingly finer subgroups of users.

50
00:03:30.290 --> 00:03:31.730
With this structure,

51
00:03:31.730 --> 00:03:36.433
a lookup on user.commercial.entertainment
will also retrieve values

52
00:03:36.433 --> 00:03:40.390
from user.commercial.entertainment.movie
industry.

53
00:03:43.330 --> 00:03:49.390
A slightly more complex case occurs
when the value is not atomic,

54
00:03:49.390 --> 00:03:54.300
but a collection object like a list
which by definition is an ordered set.

55
00:03:55.720 --> 00:03:58.218
An example of such a list
can come from Twitter,

56
00:03:58.218 --> 00:04:02.760
that uses Redis list to store timelines.

57
00:04:02.760 --> 00:04:05.860
Now borrowed from a Twitter presentation
about their timeline architecture

58
00:04:06.920 --> 00:04:10.880
our timeline service can
take a specific ID, and

59
00:04:10.880 --> 00:04:14.550
it quickly identifies all tweets of
this user that are in the cache.

60
00:04:16.010 --> 00:04:19.310
These tweets are then populated
by the content of the tweet,

61
00:04:19.310 --> 00:04:21.460
then returned as a result.

62
00:04:21.460 --> 00:04:26.000
This list can be long, but the insertion
and delete operations on the list

63
00:04:26.000 --> 00:04:28.970
can be performed in constant
time which is in milliseconds.

64
00:04:30.220 --> 00:04:35.110
If a tweet is retweeted,
the IDs of those tweets are also added

65
00:04:35.110 --> 00:04:38.820
to the list of the first tweet, as you can
see for the three cases in the figure.

66
00:04:41.099 --> 00:04:45.210
When the lists are long,
space saving becomes an important issue.

67
00:04:46.480 --> 00:04:50.370
So Redis employs a method called
Ziplists which essentially

68
00:04:50.370 --> 00:04:54.700
compacts the size of the list in
memory without changing the content.

69
00:04:54.700 --> 00:04:57.620
Often producing significant
reduction in memory used.

70
00:04:59.150 --> 00:05:01.890
Of course,
while Ziplists are very efficient for

71
00:05:01.890 --> 00:05:06.240
retrieval, they are a little more complex
for insertion and deletion operations.

72
00:05:08.100 --> 00:05:11.100
Since Redis is an open source system,

73
00:05:11.100 --> 00:05:15.070
Twitter made a few innovations
on the Redis data structures.

74
00:05:15.070 --> 00:05:20.470
One of these innovations is that
they created lists of Ziplists.

75
00:05:20.470 --> 00:05:24.480
This gave them the flexibility of
having constant timing insertions and

76
00:05:24.480 --> 00:05:28.970
deletions and at the same time used the
compressed representation to save space.

77
00:05:30.880 --> 00:05:35.830
In 2012, the timeline service has about

78
00:05:35.830 --> 00:05:41.220
40 terabytes of main memory, serving
30 million user queries per second.

79
00:05:41.220 --> 00:05:44.589
Running on over 6,000
machines in one data center.

80
00:05:45.870 --> 00:05:51.280
For those interested I would like to point
you to two wonderful Twitter presentation

81
00:05:51.280 --> 00:05:55.200
explaining Twitter's use of Redis
among other design issue for

82
00:05:55.200 --> 00:05:56.790
a realtime data system like Twitter.

83
00:05:58.740 --> 00:06:01.800
We also introduce links in
the supplemental readings for this lesson.

84
00:06:05.843 --> 00:06:10.949
Now the value to be looked up by the keys
can actually be more complicated and

85
00:06:10.949 --> 00:06:15.260
can be records containing
attribute value pairs themselves.

86
00:06:17.110 --> 00:06:18.939
Redis values can be hashes

87
00:06:20.010 --> 00:06:24.970
which are essentially named containers
of unique fields and their values.

88
00:06:26.010 --> 00:06:29.210
In the example, the key, std:101,

89
00:06:29.210 --> 00:06:33.960
is associated with five
attributed value pairs.

90
00:06:35.150 --> 00:06:38.330
The hashed attributes
are stored very efficiently.

91
00:06:38.330 --> 00:06:43.570
And even when the list of attributed
value pairs in a hash is really long,

92
00:06:43.570 --> 00:06:45.110
retrieval is efficient.

93
00:06:48.250 --> 00:06:50.240
Horizontal scalability or

94
00:06:50.240 --> 00:06:54.710
scale out capabilities refers
to the ability of a system

95
00:06:54.710 --> 00:06:59.170
to achieve scalability when the number
of machines it operates on is increased.

96
00:07:00.720 --> 00:07:06.380
Redis allows data partitioning through
range partitioning and hash partitioning.

97
00:07:07.600 --> 00:07:13.110
Rate partitioning takes a numeric key and
breaks up the range of keys into bins.

98
00:07:13.110 --> 00:07:18.610
In this case, by bins of 10,000
each of bin is assigned a machine.

99
00:07:20.730 --> 00:07:24.940
And ultimately, a partitioning is where
computing a hashing function on a key.

100
00:07:26.060 --> 00:07:28.360
Suppose we have 10 machines.

101
00:07:28.360 --> 00:07:31.690
We pick the key and use the hash
function to get back a number.

102
00:07:33.050 --> 00:07:37.560
We represent the number, modular 10,
and the result, in this case,

103
00:07:37.560 --> 00:07:40.920
2, is the machine to which
the record will be allocated.

104
00:07:43.979 --> 00:07:48.110
Replication is accomplished in
Redis through master-slave mode.

105
00:07:50.350 --> 00:07:54.110
The slaves have a copy of the master node.

106
00:07:54.110 --> 00:07:55.940
And can serve read queries.

107
00:07:58.090 --> 00:08:00.850
Clients write to the master node.

108
00:08:00.850 --> 00:08:03.280
And master node replicates to the slaves.

109
00:08:05.310 --> 00:08:08.090
Clients read from the slaves to
scale up the read performance.

110
00:08:09.190 --> 00:08:11.760
The replication processes are synchronous.

111
00:08:11.760 --> 00:08:16.040
That is that slaves do not get replicated
data, it locks them with each other.

112
00:08:17.360 --> 00:08:20.540
However, the replication
process does ensure

113
00:08:20.540 --> 00:08:22.120
that they're consistent with each other.
WEBVTT

1
00:00:01.310 --> 00:00:04.510
The next system we'll explore is Vertica.

2
00:00:04.510 --> 00:00:08.810
Which is the relational DBMS
designed to operate on top of HTFS.

3
00:00:10.450 --> 00:00:13.960
It belongs to a family of DBMS
architectures called column stores.

4
00:00:15.320 --> 00:00:18.410
Other products in
the same family are UCDV,

5
00:00:18.410 --> 00:00:21.540
Carrot Cell Xvelocity from Microsoft and
so forth.

6
00:00:23.070 --> 00:00:25.620
The primary difference
between a row store and

7
00:00:25.620 --> 00:00:28.010
a column store is shown
in the diagram here.

8
00:00:29.160 --> 00:00:32.094
Logically, this table has five columns.

9
00:00:32.094 --> 00:00:35.935
Emp number, department number,

10
00:00:35.935 --> 00:00:38.810
hire date, employee last name and
employee first name.

11
00:00:40.940 --> 00:00:45.760
In a row oriented design the database
internally organizes the record

12
00:00:45.760 --> 00:00:46.300
two four by two.

13
00:00:48.390 --> 00:00:52.030
In a column store,
the data is organized column wise.

14
00:00:53.500 --> 00:00:58.100
So the nth number column is stored
separately from the department id

15
00:00:58.100 --> 00:00:59.280
column and so forth.

16
00:01:00.740 --> 00:01:03.730
Now suppose a query
needs to find the ID and

17
00:01:03.730 --> 00:01:09.100
department ID of all employees who were
hired after first of January, 2001.

18
00:01:09.100 --> 00:01:12.990
The system only needs to look up

19
00:01:12.990 --> 00:01:16.720
the hire date column to figure
out which records qualify.

20
00:01:16.720 --> 00:01:18.670
And then pick up the values of the ID,

21
00:01:18.670 --> 00:01:22.450
and the department of ID columns for
the qualifying records.

22
00:01:22.450 --> 00:01:26.060
The other columns are not touched.

23
00:01:26.060 --> 00:01:31.270
So if a table has 30 to 50 columns, very
often only a few of them are needed for

24
00:01:31.270 --> 00:01:31.950
any single query.

25
00:01:33.730 --> 00:01:40.440
So for tables with 500 million rows and
40 columns a typical query is very fast,

26
00:01:40.440 --> 00:01:45.400
and uses much less memory because a full
record is not used most of the time.

27
00:01:46.990 --> 00:01:51.780
My own experience is that with
an application that needed a database

28
00:01:51.780 --> 00:01:53.830
with 150 billion tuples in a table.

29
00:01:54.870 --> 00:01:58.860
Accounting operation took a little
under three minutes to complete.

30
00:02:00.330 --> 00:02:03.890
A second advantage of the column store
comes from the nature of the data.

31
00:02:05.350 --> 00:02:09.220
In a column store,
data in every column is sorted.

32
00:02:10.400 --> 00:02:13.900
The figure on the right shows
three sorted columns of a table

33
00:02:13.900 --> 00:02:15.110
with three visible columns.

34
00:02:16.550 --> 00:02:18.940
The bottom of the first column shown here

35
00:02:18.940 --> 00:02:22.670
has many accurate sixteen
transactions on the same day.

36
00:02:24.340 --> 00:02:29.170
The second column has customer ids which
can be numerically close to each other.

37
00:02:29.170 --> 00:02:31.930
They are all within seventy
values of the first customer.

38
00:02:33.430 --> 00:02:38.020
So in the first case we can just
say that the value is one, one,

39
00:02:38.020 --> 00:02:41.730
2007, and
the next 16 records have this value.

40
00:02:42.800 --> 00:02:48.580
That means we do not have to store
the next 16 values thus saving space.

41
00:02:50.770 --> 00:02:54.280
Now this form of shortened
representation is called compression.

42
00:02:55.300 --> 00:03:00.070
And this specific variety is
called run-length encoding or RLE.

43
00:03:01.760 --> 00:03:05.010
Another form of encoding can
be seen in the second column.

44
00:03:07.030 --> 00:03:10.990
Here the customer ids a long integer,
but for

45
00:03:10.990 --> 00:03:14.460
the records shown there
are nearby numbers.

46
00:03:14.460 --> 00:03:18.420
So, if we pick a value in the column and
just put the difference between

47
00:03:18.420 --> 00:03:22.340
this value and
other values The difference will be small.

48
00:03:22.340 --> 00:03:25.380
It means,
we'll need fewer bites to represent them.

49
00:03:27.330 --> 00:03:30.800
This one of compression is called
Frame-of-reference encoding.

50
00:03:32.440 --> 00:03:36.520
The lesson to remember here is
that compressed data presentation

51
00:03:36.520 --> 00:03:41.760
can significantly reduce the total
size of a database and if BDMS.

52
00:03:41.760 --> 00:03:44.970
Should use all such tricks
to improve space efficiency.

53
00:03:46.730 --> 00:03:50.020
While the space efficiency and
performance of Vertica is impressive for

54
00:03:50.020 --> 00:03:54.490
large data, one has to be careful
about how to design a system with it.

55
00:03:55.620 --> 00:03:57.570
Like Google's Web Table, and

56
00:03:57.570 --> 00:04:02.844
Apache Cassandra, Vertica allows
the declaration of column-groups.

57
00:04:04.180 --> 00:04:05.830
These are columns like first name and

58
00:04:05.830 --> 00:04:08.730
last name which are very
often accessed together.

59
00:04:09.980 --> 00:04:13.040
In Vertica,
a column group is like a mini table

60
00:04:13.040 --> 00:04:15.640
which is treated like a little
row storied in the system.

61
00:04:16.750 --> 00:04:22.070
But because these groups represent
activists that are frequently co accessed

62
00:04:22.070 --> 00:04:24.500
the grouping actually
improves performance.

63
00:04:25.560 --> 00:04:29.660
So an application developer is
better off when the nature and

64
00:04:29.660 --> 00:04:33.150
the frequency of user queries
are known to some degree.

65
00:04:33.150 --> 00:04:37.190
And this knowledge is applies
to designing the column groups.

66
00:04:37.190 --> 00:04:41.200
An important side effect of column
structure organization of vertical

67
00:04:41.200 --> 00:04:44.390
Is that the writing of data into
Vertica is a little slower.

68
00:04:45.590 --> 00:04:50.110
When rows are added to a table,
Vertica initially places

69
00:04:50.110 --> 00:04:53.750
them in a row-wise data structure and

70
00:04:53.750 --> 00:04:58.029
then converts them into a column-wise
data structure, which is then compressed.

71
00:04:59.370 --> 00:05:02.920
This lowness can be perceptible for
large uploads or updates.

72
00:05:04.140 --> 00:05:08.050
Vertica belongs to a new breed of
database systems that call themselves

73
00:05:08.050 --> 00:05:09.220
analytical databases.

74
00:05:10.620 --> 00:05:14.780
This means two slightly different things.

75
00:05:14.780 --> 00:05:18.110
First, Vertica offers many more
statistical functions than in

76
00:05:18.110 --> 00:05:18.950
classical DBMS.

77
00:05:20.140 --> 00:05:24.500
For example one can perform
operations over a Window

78
00:05:25.728 --> 00:05:32.150
to see an example consider a table of
stock ticks having a time attribute,

79
00:05:32.150 --> 00:05:35.520
a stock name and
a value of the stock called bid here.

80
00:05:36.570 --> 00:05:41.610
This shows that data values in
the table now we would like to compute

81
00:05:41.610 --> 00:05:45.220
a moving average of
the bit every 40 seconds.

82
00:05:46.510 --> 00:05:48.770
We show the query in a frame below.

83
00:05:48.770 --> 00:05:53.450
Now we aren't going into the details of
the query just consider the blue part.

84
00:05:53.450 --> 00:05:57.930
It says the average,
which is the AVG function in yellow.

85
00:05:57.930 --> 00:06:00.760
It must be computed on
the last column which is bit.

86
00:06:01.920 --> 00:06:03.060
But, this computation

87
00:06:04.200 --> 00:06:08.270
must be over the range that is
computed on the timestamp call.

88
00:06:09.740 --> 00:06:15.877
So, the range is defined by a 40
second row before that current row so,

89
00:06:15.877 --> 00:06:21.404
here the computation of the average
advances for the stock abc.

90
00:06:21.404 --> 00:06:26.293
And for each computation,
the system only considers the rows whose

91
00:06:26.293 --> 00:06:30.510
timestamp is within 40 seconds
before the current row.

92
00:06:31.840 --> 00:06:35.630
The table on the right shows
the result of the query.

93
00:06:35.630 --> 00:06:38.130
The average value, 10.12.

94
00:06:38.130 --> 00:06:41.910
Is the same as the actual value, because
there are no other rows within 40 seconds.

95
00:06:41.910 --> 00:06:45.963
The next two result rows average
over the preceding rows,

96
00:06:45.963 --> 00:06:49.530
was times R within 40
seconds of the current row.

97
00:06:50.650 --> 00:06:51.670
When we get to the blue row,

98
00:06:51.670 --> 00:06:58.020
that we notice that it occurs 1 minute
16 seconds after the previous row.

99
00:06:58.020 --> 00:07:00.950
So we cannot consider the previous
sort in the computation.

100
00:07:00.950 --> 00:07:04.660
Instead, the result is just the value
of the bid in the current row.

101
00:07:06.190 --> 00:07:10.180
The takeaway from this example is
that analytical computations like

102
00:07:10.180 --> 00:07:14.260
this are happening inside the database and
not in an external application.

103
00:07:15.880 --> 00:07:19.850
This brings us to the second feature
of Vertica as an analytical database.

104
00:07:21.200 --> 00:07:25.710
R is a well known free statistics
package that's used by statisticians,

105
00:07:25.710 --> 00:07:28.050
data minors, predictive analytics experts.

106
00:07:28.050 --> 00:07:33.840
Today, R can not only
read data from files,

107
00:07:33.840 --> 00:07:37.660
but it can go to an SQL database and
grab data to perform statistical analysis.

108
00:07:39.300 --> 00:07:42.160
Over time, R has evolved and

109
00:07:42.160 --> 00:07:47.830
given rights to distributed R which
is a high performance platform for R.

110
00:07:49.430 --> 00:07:51.810
As expected in this distributed setting,

111
00:07:51.810 --> 00:07:55.050
the system operates in
a master slave mode.

112
00:07:55.050 --> 00:07:59.650
The master node coordinates computations
by sending commands to the workers.

113
00:07:59.650 --> 00:08:02.520
The worker nodes maintain
data partitions and

114
00:08:02.520 --> 00:08:05.470
apply the computation
functions to the data.

115
00:08:05.470 --> 00:08:09.070
Just getting the data parallelly,

116
00:08:09.070 --> 00:08:12.310
the essential data structure
is a distributed array.

117
00:08:12.310 --> 00:08:15.266
That is an array that is
partitioned as shown here.

118
00:08:15.266 --> 00:08:18.480
Now in this diagram
the partitions are equal, but

119
00:08:18.480 --> 00:08:20.960
in practice they may be
all different sizes.

120
00:08:20.960 --> 00:08:26.320
On which, one can compute a function for
each of these mini-arrays.

121
00:08:26.320 --> 00:08:30.200
The bottom diagram, shows a simple
work flow of constructing and

122
00:08:30.200 --> 00:08:31.690
deploying a predictive model.

123
00:08:32.730 --> 00:08:34.460
The role of Vertica here,

124
00:08:34.460 --> 00:08:38.620
is that it's a data supplier to the worker
nodes of R, and a model consumer.

125
00:08:39.880 --> 00:08:43.730
The data to be analyzed is
the output of the vertica query,

126
00:08:43.730 --> 00:08:48.010
which is transferred in memory through
a protocol called vertica fast transfer

127
00:08:48.010 --> 00:08:49.780
through distributed R as a dArray.

128
00:08:50.940 --> 00:08:52.120
When the model is created in R,

129
00:08:52.120 --> 00:08:56.900
it should come back as a code that
goes through vertica as a function.

130
00:08:58.070 --> 00:09:00.870
This function can be
called from inside Vertica

131
00:09:00.870 --> 00:09:03.310
as if it was a user defined function.

132
00:09:04.330 --> 00:09:07.690
Now in sophisticated applications,
the features of the data needed for

133
00:09:07.690 --> 00:09:13.670
predicted modeling will also be
computed inside the DBMS possibly

134
00:09:13.670 --> 00:09:16.389
using the new analytical operations
of Vertica that we've just shown.

135
00:09:17.770 --> 00:09:21.190
Now this will make future
computation much faster and

136
00:09:21.190 --> 00:09:24.930
improve the efficiency of
the entire analytics process.

137
00:09:26.240 --> 00:09:31.920
Going forward, we believe that most DBMS's
will want to play in the analytics field,

138
00:09:31.920 --> 00:09:33.290
will support similar functions.
WEBVTT

1
00:00:02.390 --> 00:00:06.010
Most of you have heard of
MongoDB as a dominant store for

2
00:00:06.010 --> 00:00:07.770
JSON style semi-structured data.

3
00:00:09.070 --> 00:00:11.200
MongoDB is very popular and

4
00:00:11.200 --> 00:00:13.860
there are a number of excellent
tutorials on it on the web.

5
00:00:15.460 --> 00:00:20.290
In this module we would like to discuss
a relatively new big data management

6
00:00:20.290 --> 00:00:25.910
system for semistructured data that's
currently being incubated by Apache.

7
00:00:25.910 --> 00:00:27.040
It's called AsterixDB.

8
00:00:28.260 --> 00:00:32.500
Originally, AsterixDB was conceived by
the University of California Irvine.

9
00:00:33.910 --> 00:00:36.800
Since it is a full fledged DBMS,

10
00:00:36.800 --> 00:00:42.310
it provides ACID guarantees to
understand the basic design of AsterixDB,

11
00:00:43.330 --> 00:00:48.140
let's consider this incomplete JSON
snippet taken from an actual tweet.

12
00:00:50.120 --> 00:00:51.900
We have seen the structure of JSON before.

13
00:00:53.000 --> 00:00:59.090
Here we point out that entities and
user, the two parts

14
00:00:59.090 --> 00:01:03.910
in blue are nested, that means embedded,
within the structure of the tweet.

15
00:01:05.640 --> 00:01:10.540
If we represent a part of the schema of
this abbreviated structure in AsterixDB,

16
00:01:12.010 --> 00:01:13.270
it will look like this.

17
00:01:14.890 --> 00:01:17.950
Here a dataverse is like a name space for
data.

18
00:01:19.610 --> 00:01:22.860
Data is declared in terms of data types.

19
00:01:22.860 --> 00:01:28.030
The top type, which looks like
a standard data with stable declaration,

20
00:01:28.030 --> 00:01:32.030
represents the user portion of the JSON
object that we highlighted before.

21
00:01:33.130 --> 00:01:35.520
The type below represents the message.

22
00:01:36.750 --> 00:01:39.820
Now, instead of nesting it like JSON.

23
00:01:39.820 --> 00:01:44.520
The user attribute highlighted in
blue is declared to have the type

24
00:01:44.520 --> 00:01:49.240
TwitterUserType, thus it captures
the hierarchical structure of JSON.

25
00:01:51.450 --> 00:01:55.560
We should also notice that
the first type is declared as open.

26
00:01:57.160 --> 00:02:02.100
It means that the actual data can have
more attributes than specified here.

27
00:02:03.640 --> 00:02:08.900
In contrast, the TweetMessage
type is declared as closed,

28
00:02:08.900 --> 00:02:13.130
meaning that the data instance must have
the same attributes as in the schema.

29
00:02:15.020 --> 00:02:19.890
AsterixDB can handle spatial data as given
by the point data types shown in green.

30
00:02:21.380 --> 00:02:27.120
The question mark at the end of the point
type says that this attribute is optional.

31
00:02:27.120 --> 00:02:29.960
That means all instances need not have it.

32
00:02:31.780 --> 00:02:37.810
Finally, the create dataset actually
asks the system to create a dataset

33
00:02:37.810 --> 00:02:43.010
called TweetMessages, whose type is
the just declared quick message type.

34
00:02:44.730 --> 00:02:50.060
AstrerixDB which runs on HDFS provides
several options for credit support.

35
00:02:53.350 --> 00:02:58.440
First it has its own query language
called the Asterix query language

36
00:02:58.440 --> 00:03:01.450
which resembles the XML
credit language query.

37
00:03:03.000 --> 00:03:05.340
The details of this query language
are not important right now.

38
00:03:06.720 --> 00:03:11.150
We are illustrating the structure of
a query just to show what it looks like.

39
00:03:12.330 --> 00:03:15.040
This particular query asks for

40
00:03:15.040 --> 00:03:20.020
all user objects from the dataset
TwitterUsers in descending order of their

41
00:03:20.020 --> 00:03:25.010
follower count and in alphabetical
order of the user's preferred language.

42
00:03:26.740 --> 00:03:30.650
What is more interesting and distinctive
is that AsterixDB has a creative

43
00:03:30.650 --> 00:03:36.409
processing engine that can process
queries in multiple languages.

44
00:03:37.750 --> 00:03:41.680
For its supported language
they've developed a way to

45
00:03:41.680 --> 00:03:46.550
transfer the query into a set of low
level operations like select and

46
00:03:46.550 --> 00:03:49.895
join which their query
exchange can support.

47
00:03:49.895 --> 00:03:54.520
Further, they've determined how
a record described in one of these

48
00:03:54.520 --> 00:03:58.280
languages can be transformed
into an Asterix.

49
00:03:58.280 --> 00:04:02.449
In this manner, the support hive queries,

50
00:04:02.449 --> 00:04:06.410
which is expressed in like this.

51
00:04:06.410 --> 00:04:11.630
Xquery, Hadoop map reduce,
as wall as a new language

52
00:04:11.630 --> 00:04:15.930
called SQL++ which extends SQL for JSON.

53
00:04:19.107 --> 00:04:20.781
Like a typical DB BDms,

54
00:04:20.781 --> 00:04:25.660
AsterixDB is designed to operate
on a cluster of machines.

55
00:04:25.660 --> 00:04:31.395
The basic idea, not surprisingly,
is to use partition data parallellism.

56
00:04:31.395 --> 00:04:35.415
Each data set is divided into
instances of various types

57
00:04:35.415 --> 00:04:39.685
which can be decomposed to different
machines by either range partitioning or

58
00:04:39.685 --> 00:04:41.565
hash partitioning like
we discussed earlier.

59
00:04:43.572 --> 00:04:47.292
A runtime distributed execution
engine called Hyracks is used for

60
00:04:47.292 --> 00:04:51.172
partitioned parallel
execution of query plans.

61
00:04:51.172 --> 00:04:54.762
For example, let's assume we have
two relations, customers and orders,

62
00:04:54.762 --> 00:04:55.522
as you can see here.

63
00:04:57.402 --> 00:05:00.249
Our query is find the number of orders for

64
00:05:00.249 --> 00:05:03.946
every market segment that
the customers belong to.

65
00:05:05.526 --> 00:05:10.936
Now this query need a join operation
between the two relations,

66
00:05:10.936 --> 00:05:16.260
using the O_CUSTKEY as a foreign
key of customer into orders.

67
00:05:17.450 --> 00:05:21.090
It also needs a grouping operation,
which for

68
00:05:21.090 --> 00:05:25.330
each market segment will pull together all
the orders which will then be counted.

69
00:05:26.530 --> 00:05:29.960
You don't have to understand the details
of this diagram at this point.

70
00:05:29.960 --> 00:05:33.580
We just want to point out that
the different parts of the query

71
00:05:33.580 --> 00:05:38.180
that are being marked,
the customer filed here has two partitions

72
00:05:38.180 --> 00:05:41.760
that reside on two nodes,
NC one and NC two respectively.

73
00:05:43.300 --> 00:05:47.200
The orders file also has two partitions.

74
00:05:47.200 --> 00:05:49.787
But each partition is dually replicated.

75
00:05:51.567 --> 00:05:58.660
One can be accessed either of nodes NC3 or
NC2 and the other on NC1 and NC5.

76
00:06:01.540 --> 00:06:06.970
Hyracks will also break up
the query into a number of jobs and

77
00:06:06.970 --> 00:06:10.740
then fill it out which tasks can
be performed in parallel and

78
00:06:10.740 --> 00:06:13.020
which ones must be
executed stage by stage.

79
00:06:14.220 --> 00:06:17.650
This whole thing will be managed
by the cluster controller.

80
00:06:19.080 --> 00:06:22.930
The cluster controller is also
responsible for replanning and

81
00:06:22.930 --> 00:06:27.050
reexecuting of a job
if there is a failure.

82
00:06:27.050 --> 00:06:32.300
AsteriskDB also has the provision

83
00:06:32.300 --> 00:06:35.990
to accept real time data from external
data sources at multiple rates.

84
00:06:37.610 --> 00:06:40.160
One way is from files in a directory path.

85
00:06:41.400 --> 00:06:43.040
Consider the example of tweets.

86
00:06:44.040 --> 00:06:46.480
As you have seen with the hands-on demo,

87
00:06:46.480 --> 00:06:52.350
usually people acquire tweets by accessing
data through an api that twitter provides.

88
00:06:52.350 --> 00:06:55.590
Very typically a certain volume of tweets,
lets say for

89
00:06:55.590 --> 00:07:00.010
every 5 minutes, is accumulated into
a .json file in a specific directory.

90
00:07:00.010 --> 00:07:02.861
The next 5 minutes,
in another .json file, and so forth.

91
00:07:04.891 --> 00:07:07.205
The way to get this
data into asterisks DB,

92
00:07:07.205 --> 00:07:10.240
is to first create an empty
data set called Tweets here.

93
00:07:11.490 --> 00:07:13.420
The next task is to create a feed.

94
00:07:14.470 --> 00:07:16.830
That is an externally resource.

95
00:07:16.830 --> 00:07:21.450
One has to specify that it's coming from
the local file system called local fs here

96
00:07:22.640 --> 00:07:25.390
and the location of the directory,
the format and

97
00:07:25.390 --> 00:07:27.850
the data type it's going to copy it.

98
00:07:27.850 --> 00:07:31.100
Next, the feed is connected
to the data set and

99
00:07:31.100 --> 00:07:33.770
the system starts reading unread
files from the directory.

100
00:07:35.960 --> 00:07:41.170
Another way for AsteriskDB to access
external data is directly from an API,

101
00:07:41.170 --> 00:07:43.020
such as the Twitter API.

102
00:07:43.020 --> 00:07:47.750
To do this,
one would create a dataset as before.

103
00:07:47.750 --> 00:07:51.460
But this time the data feed is
not on the local file system.

104
00:07:52.580 --> 00:07:57.960
Instead it uses the push Twitter
method which invokes the Twitter

105
00:07:57.960 --> 00:08:01.830
client with the four authentication
parameters required by the API.

106
00:08:03.360 --> 00:08:06.700
Once the feed is defined it is
connected to the data set as before.
WEBVTT

1
00:00:02.986 --> 00:00:05.186
Now we move to another Apache product for

2
00:00:05.186 --> 00:00:07.720
large scale text data
searching called Solr.

3
00:00:09.100 --> 00:00:10.430
Systems like Solr, and

4
00:00:10.430 --> 00:00:16.300
its underlying text indexing engine, are
typically designed for search problems.

5
00:00:16.300 --> 00:00:18.629
So they would typically be
part of a search engine.

6
00:00:19.860 --> 00:00:21.410
But before we talk about Solr or

7
00:00:21.410 --> 00:00:26.770
large scale text, we need to first
appreciate some fundamental challenges

8
00:00:26.770 --> 00:00:29.590
when it comes to storing,
indexing, and matching text data.

9
00:00:31.690 --> 00:00:36.030
The basic challenge comes from
the numerous ways in which

10
00:00:36.030 --> 00:00:40.660
a text string may vary, making it
hard to define what a good match is.

11
00:00:41.830 --> 00:00:43.560
Let us show some of these challenges.

12
00:00:44.750 --> 00:00:49.810
Remember in each case,
we're asking whether the strings shown on

13
00:00:49.810 --> 00:00:52.560
either side of the double
tilde sign should match.

14
00:00:55.100 --> 00:00:58.541
The first issue is about spelling
variations and capitalization.

15
00:01:02.049 --> 00:01:05.600
The second issue relates
structured strings.

16
00:01:05.600 --> 00:01:10.080
Where different parts of a string
represent different kind of information.

17
00:01:10.080 --> 00:01:16.420
We have seen this before for file paths,
URLs, and like in this case, product IDs.

18
00:01:16.420 --> 00:01:21.081
The problem is that the searcher may
not always know the structure or

19
00:01:21.081 --> 00:01:24.775
my job on misposition,
the internal punctuations.

20
00:01:28.014 --> 00:01:32.674
The next problem is very common with
proper nouns which are represented in

21
00:01:32.674 --> 00:01:38.248
various ways, including dropping a part of
the string, picking up only the initials.

22
00:01:38.248 --> 00:01:43.497
But consider the last variation, should
BH Obama really match the full name?

23
00:01:46.797 --> 00:01:50.120
The next example is about
frequently used synonyms.

24
00:01:51.210 --> 00:01:55.512
If the document has one of the synonyms,
while the query uses another,

25
00:01:55.512 --> 00:01:56.742
should they match?

26
00:01:59.679 --> 00:02:03.400
Example 5 illustrates
a very common problem.

27
00:02:03.400 --> 00:02:05.710
People use abbreviations all the time.

28
00:02:06.970 --> 00:02:09.160
If we look at text and social media and

29
00:02:09.160 --> 00:02:12.930
instant messaging, we see a much
wider variety of abbreviations.

30
00:02:14.270 --> 00:02:17.270
How well should they met with
the real correct version of the term?

31
00:02:19.580 --> 00:02:22.510
Now problem six is a special
case of problem five.

32
00:02:24.350 --> 00:02:26.690
Many long nouns are shortened

33
00:02:26.690 --> 00:02:30.440
because we take the first initial
letter of each significant word.

34
00:02:31.570 --> 00:02:37.580
We say significant because we
drop words like of as shown here.

35
00:02:37.580 --> 00:02:38.870
This is called initialism.

36
00:02:40.390 --> 00:02:44.490
Just so you know, when an initialism
can be said like a real word,

37
00:02:44.490 --> 00:02:45.480
it's called an acronym.

38
00:02:46.480 --> 00:02:51.620
Thus IBM is an initialization,
but NATO is an acronym.

39
00:02:53.160 --> 00:02:58.250
Now problem seven and
problem eight show two

40
00:02:58.250 --> 00:03:02.690
different situation where we first must
decide what to do with the period sign.

41
00:03:04.100 --> 00:03:09.322
In the first case, we should not
find a match because students and

42
00:03:09.322 --> 00:03:13.250
American are in two different
sentences punctuated by a period sign.

43
00:03:14.720 --> 00:03:18.330
But in the second case we should
find a match because the period

44
00:03:18.330 --> 00:03:20.310
does not designate a sentence boundary.

45
00:03:23.055 --> 00:03:28.510
Lucene, the engine on which Solr is
built is effectively not a database,

46
00:03:28.510 --> 00:03:30.030
but a modern inverted index.

47
00:03:31.040 --> 00:03:32.020
What's an inverted index?

48
00:03:33.620 --> 00:03:37.590
Let's first define a vocabulary
as a collection of terms.

49
00:03:37.590 --> 00:03:42.090
Where a term may be a single word or
it can be multiple words.

50
00:03:42.090 --> 00:03:45.030
It can a single term or
it can be a collection of synonyms.

51
00:03:46.510 --> 00:03:48.730
But how would a search
engine know what a term is?

52
00:03:49.795 --> 00:03:52.558
We'll revisit this question
in a couple of slides.

53
00:03:52.558 --> 00:03:57.390
For now, let's just say that if we
have a corpus of documents, we can

54
00:03:57.390 --> 00:04:01.690
extract most of the terms and construct
a vocabulary for that collection.

55
00:04:03.500 --> 00:04:08.080
We can then define occurrence, as a list

56
00:04:08.080 --> 00:04:12.270
containing all the information necessary
for each term in the vocabulary.

57
00:04:13.780 --> 00:04:17.790
It would include information like,
which documents have the term?

58
00:04:17.790 --> 00:04:21.050
The positions in the document
where the term occurs.

59
00:04:21.050 --> 00:04:25.404
We can then go on to compute the count of
the term in the document and the corpus.

60
00:04:26.540 --> 00:04:30.760
Referring back to a previous module
in this course, we can also compute

61
00:04:30.760 --> 00:04:34.370
the term frequency and inverse
document frequency for the collection.

62
00:04:36.470 --> 00:04:40.250
An inverted index is
essentially an index which for

63
00:04:40.250 --> 00:04:45.850
every term stores at least the ID of
the document where the term occurs.

64
00:04:45.850 --> 00:04:48.440
Practically, other computed numbers or

65
00:04:48.440 --> 00:04:52.060
properties associated with the terms
will also be included in the index.

66
00:04:54.570 --> 00:04:58.820
Solr is an open source
enterprise search platform.

67
00:04:58.820 --> 00:05:04.190
The heart of Solr is its search
functionality built for full text search.

68
00:05:04.190 --> 00:05:07.110
However, Solr provides much
more than tech search.

69
00:05:08.620 --> 00:05:14.780
It can take any structured document,
even CSV files

70
00:05:14.780 --> 00:05:19.490
which can be broken up into fields,
and can index each field separately.

71
00:05:20.850 --> 00:05:24.640
Full text indexes where text columns
are supplemented by indexes for

72
00:05:24.640 --> 00:05:29.735
other types of data, including numeric
data, dates, geographic coordinates, and

73
00:05:29.735 --> 00:05:33.390
fields where domains are limited
to an emergent set of values.

74
00:05:35.190 --> 00:05:40.230
Solr provides other facilities
like faceted search and

75
00:05:40.230 --> 00:05:43.440
highlighting of terms that match a query.

76
00:05:43.440 --> 00:05:46.970
Now if you're not familiar
with the term faceted search,

77
00:05:46.970 --> 00:05:50.550
let's look at the screenshot
from amazon.com.

78
00:05:50.550 --> 00:05:52.960
I performed a search on the string,
Dell laptop.

79
00:05:54.260 --> 00:05:56.680
Consider the highlighted
part of the image.

80
00:05:58.400 --> 00:06:02.230
Each laptop record carries a lot of
attributes like the display size,

81
00:06:02.230 --> 00:06:04.499
the processor speed,
the amount of memory, and so forth.

82
00:06:05.680 --> 00:06:10.480
These attributes can be put into builds
like processor type has Intel i5, i7, etc.

83
00:06:11.950 --> 00:06:17.098
Faceted search essentially extracts
the individual values of these fields and

84
00:06:17.098 --> 00:06:22.188
displays them back to the user, usually
with a count of the number of records.

85
00:06:22.188 --> 00:06:26.466
We see this in the upper
part of the marked portion,

86
00:06:26.466 --> 00:06:31.452
which says there are 5619
laptops per 411 tablets.

87
00:06:31.452 --> 00:06:32.600
These are called facets.

88
00:06:33.750 --> 00:06:37.810
If a user clicks on a facet,
documents with only those values,

89
00:06:37.810 --> 00:06:40.250
that's just the tablets,
will be presented back to the user.

90
00:06:42.380 --> 00:06:45.860
Now let's get back to the question
of what a term is, and

91
00:06:45.860 --> 00:06:47.550
how a Solr system should know it.

92
00:06:49.870 --> 00:06:55.730
Solr allows the system designer to
specify how to parse a document,

93
00:06:55.730 --> 00:07:01.770
by instructing how to tokenize
a document and how to filter it.

94
00:07:01.770 --> 00:07:06.720
Tokenization is the process of
breaking down the characters read.

95
00:07:06.720 --> 00:07:10.540
For example, one can break
the stream at white spaces, and

96
00:07:10.540 --> 00:07:13.170
get all the words as tokens.

97
00:07:13.170 --> 00:07:17.803
Then, it can filter out the punctuation
like the period, the apostrophe, and so

98
00:07:17.803 --> 00:07:19.631
on, just to get the pure words.

99
00:07:21.984 --> 00:07:24.410
The code snippet on the right
essentially achieves this.

100
00:07:25.510 --> 00:07:29.960
It uses a standard tokenizer that gets
the words with immediate punctuation.

101
00:07:31.100 --> 00:07:34.730
The first filter removes the punctuations.

102
00:07:34.730 --> 00:07:38.040
The second filter turns
everything into lowercase.

103
00:07:38.040 --> 00:07:41.960
And the third filter uses
a synonym file to ensure that

104
00:07:41.960 --> 00:07:46.140
all the synonyms get the same
token after ignoring the case.

105
00:07:46.140 --> 00:07:50.828
The last filter removes common
English words like a and the.

106
00:07:52.401 --> 00:07:56.330
While a similar process would need to
happen when we get the query string.

107
00:07:58.080 --> 00:08:01.245
It will also need to go through a
tokenization and token filtering process.

108
00:08:03.933 --> 00:08:07.918
In the query analyzer example, there
are six filters within the tokenizer.

109
00:08:09.060 --> 00:08:12.130
We use a pattern tokenizer which
will remove the white spaces and

110
00:08:12.130 --> 00:08:15.020
periods and semi-colon.

111
00:08:15.020 --> 00:08:19.710
The common grams filter creates
tokens out of pairs of terms, and

112
00:08:19.710 --> 00:08:23.470
in doing so, makes sure that words
in the stopword file are used.

113
00:08:23.470 --> 00:08:24.750
So if we have the string,

114
00:08:24.750 --> 00:08:29.560
the cat, the term the should
not be ignored in this filter.

115
00:08:31.240 --> 00:08:34.520
Now, the filters here
are executed in order.

116
00:08:36.180 --> 00:08:39.720
After the common grams
filter is already done,

117
00:08:39.720 --> 00:08:42.500
the next filter removes all
the stopwords in the file.

118
00:08:44.320 --> 00:08:49.190
The fifth filter makes sure that if
the queries coming from a web form,

119
00:08:49.190 --> 00:08:51.240
all the HTML characters are stripped off.

120
00:08:52.490 --> 00:08:57.360
Finally, the remaining words are stemmed,
so that runs and

121
00:08:57.360 --> 00:09:01.440
running in the query would match
the word run in the document.

122
00:09:04.170 --> 00:09:08.935
We'll end this section with a discussion
on Solr queries, that is searches.

123
00:09:12.370 --> 00:09:16.450
We present a CSV file with nine
records and seven attributes.

124
00:09:18.280 --> 00:09:23.900
We issue queries against
the system by posing a web query.

125
00:09:23.900 --> 00:09:27.850
In these examples, we show some of
the queries one can post to the system.

126
00:09:29.060 --> 00:09:32.187
This will be covered in more detail
during the hands-on session.

127
00:09:33.220 --> 00:09:39.200
Just notice that the q equal to is a query
and the fl is what you want to back.
WEBVTT

1
00:00:01.230 --> 00:00:06.440
Listens on activity who will be performing
queries assessing the Postgres database.

2
00:00:06.440 --> 00:00:10.650
First, will open a terminal window and
start the Postgres shell.

3
00:00:10.650 --> 00:00:14.430
Next, we will look at table and
column definitions in database.

4
00:00:15.670 --> 00:00:19.000
Who’s in query the content
of the buy-clicks table?

5
00:00:19.000 --> 00:00:24.170
And see how to do this query, but filter
specific rows and columns from the table.

6
00:00:24.170 --> 00:00:25.950
Next, we will perform average and

7
00:00:25.950 --> 00:00:28.780
some aggregation operations
on a specific column.

8
00:00:30.040 --> 00:00:30.590
And finally,

9
00:00:30.590 --> 00:00:33.960
we will see how to combine two tables
by joining them on a single column.

10
00:00:35.900 --> 00:00:37.130
Let's begin.

11
00:00:37.130 --> 00:00:40.300
First, click on the terminal
icon at the top of the toolbar

12
00:00:40.300 --> 00:00:41.390
to open a Terminal window.

13
00:00:43.930 --> 00:00:48.347
Next, let's start the Postgres
shell by running psql.

14
00:00:51.821 --> 00:00:56.410
The Postgres shell allows us to
enter queries and run commands for

15
00:00:56.410 --> 00:00:58.180
the postgres database.

16
00:00:58.180 --> 00:01:02.028
We can see what tables are in
the database by running \d.

17
00:01:06.453 --> 00:01:11.350
This says that there are three
tables in the database,

18
00:01:11.350 --> 00:01:14.980
adclicks, buyclicks and gameclicks.

19
00:01:14.980 --> 00:01:19.689
We could use \d table name to see
the definition of one of these tables.

20
00:01:20.830 --> 00:01:26.786
Let's look at the definition of buyclicks,

21
00:01:26.786 --> 00:01:30.084
we enter \d buyclicks.

22
00:01:32.395 --> 00:01:35.090
This shows that there's seven
columns in the database.

23
00:01:36.400 --> 00:01:41.490
These are the column names and here's
the data type for each of the columns.

24
00:01:44.240 --> 00:01:46.630
Now let's look at the contents
of the buyclicks table.

25
00:01:48.310 --> 00:01:56.718
We can query the contents by running
the command select * from buyclicks;.

26
00:01:56.718 --> 00:02:00.050
The select says that
we want to do a query.

27
00:02:00.050 --> 00:02:03.730
The star means we want to
retrieve all the columns and

28
00:02:03.730 --> 00:02:07.450
from buyclicks says,
which table to perform the query from.

29
00:02:08.660 --> 00:02:14.226
And finally, all commands in Postgres
shell need to end with a semicolon.

30
00:02:14.226 --> 00:02:19.299
When we run this command we see
the contents of the buyclicks table.

31
00:02:21.120 --> 00:02:24.760
Column header is at the top and
the contents are below.

32
00:02:26.530 --> 00:02:31.609
Again hit Space to scroll through
the contents, and hit Q when we're done.

33
00:02:34.341 --> 00:02:37.250
Now, let's view the contents
of only two of the columns.

34
00:02:38.370 --> 00:02:42.889
Let's query only the price and
the user id from the buyclicks table.

35
00:02:42.889 --> 00:02:49.180
To do this we run select price,
userid from buyclicks.

36
00:02:52.020 --> 00:02:55.245
This command says,
we want to query only the price and

37
00:02:55.245 --> 00:02:57.747
userid columns from the buyclicks table

38
00:03:00.568 --> 00:03:05.166
When we run this,
We only get those two columns.

39
00:03:07.416 --> 00:03:10.892
We can also perform queries that
just select certain rows and

40
00:03:10.892 --> 00:03:12.400
meet a certain criteria.

41
00:03:13.570 --> 00:03:14.630
For example,

42
00:03:14.630 --> 00:03:19.744
let's make a query that only shows
the rows containing the price over $10.

43
00:03:20.770 --> 00:03:25.538
You can do this by running select price,

44
00:03:25.538 --> 00:03:30.736
userid from buyclicks where price > 10.

45
00:03:30.736 --> 00:03:35.159
This query says, we only want to see
the price and userid columns from

46
00:03:35.159 --> 00:03:40.762
the buyclicks table where the value in the
price column has a value greater than 10.

47
00:03:43.008 --> 00:03:49.437
We run this, we see that we always
have price values greater than 10.

48
00:03:51.588 --> 00:03:55.650
The SQL language has a number of
aggregation operations built into it.

49
00:03:57.250 --> 00:04:04.277
For example,
we can take the average price by running,

50
00:04:04.277 --> 00:04:09.219
select avg(price) from buyclicks.

51
00:04:09.219 --> 00:04:17.925
This command will show the average price
from all the data in the buyclicks table.

52
00:04:17.925 --> 00:04:20.920
Another aggregate operation is sum.

53
00:04:20.920 --> 00:04:25.531
We can see the total price by running,

54
00:04:25.531 --> 00:04:30.297
select sum(price) from buyclicks.

55
00:04:34.474 --> 00:04:39.547
We can also combine two tables
by joining on a common column.

56
00:04:39.547 --> 00:04:44.050
If you recall we have three
tables in our database adclicks,

57
00:04:44.050 --> 00:04:46.180
buyclicks and gameclicks.

58
00:04:47.870 --> 00:04:55.594
We look at the description the definition
of adclicks buy running \d adclicks.

59
00:04:59.061 --> 00:05:04.810
You can see that it also
has a column called userid.

60
00:05:04.810 --> 00:05:09.030
Let's combine buyclicks and
adclicks based on this common column.

61
00:05:10.180 --> 00:05:16.252
You can do this by running, select adid,

62
00:05:16.252 --> 00:05:23.019
buyid, adclicks.userid from adclicks join

63
00:05:23.019 --> 00:05:31.011
buyclicks on adclicks.userid
= buyclicks.userid

64
00:05:33.704 --> 00:05:38.278
This query says,
we want to just see the adid, buyid and

65
00:05:38.278 --> 00:05:43.252
userid columns, and
we want to combine the adclicks table,

66
00:05:43.252 --> 00:05:47.729
the buyclicks table, and
we will be combining them on

67
00:05:47.729 --> 00:05:52.524
the userid column and
it's common in both those tables.

68
00:05:55.767 --> 00:05:58.809
When we run it,
you see just the three columns.
WEBVTT

1
00:00:02.160 --> 00:00:05.540
Next we'll consider queries
where two tables are used.

2
00:00:06.610 --> 00:00:08.540
Let's consider the query,

3
00:00:08.540 --> 00:00:13.320
find the beers liked by drinkers who
frequent The Great American Bar.

4
00:00:15.070 --> 00:00:21.540
For this query, we need
the relation's Frequents and Likes.

5
00:00:21.540 --> 00:00:24.330
Now look at the scheme of these
relations in the light blue box.

6
00:00:25.840 --> 00:00:28.390
They have a common
attribute called drinker.

7
00:00:29.940 --> 00:00:32.740
So if we use the attribute drinker,

8
00:00:32.740 --> 00:00:35.760
we need to tell the system
which one we are referring to.

9
00:00:37.980 --> 00:00:44.710
Now look at the SQL query, the FROM clause
in the query has these two relations.

10
00:00:44.710 --> 00:00:46.530
To handle a common attribute name issue,

11
00:00:46.530 --> 00:00:51.380
we need to give nicknames,
aliases to these relations.

12
00:00:51.380 --> 00:00:56.305
Therefore in the FROM clause we say,
Likes has the alias L and

13
00:00:56.305 --> 00:00:58.060
Frequents has the alias F.

14
00:00:59.950 --> 00:01:05.290
Since we want to find beers like before,
we use a SELECT DISTINCT clause for beer.

15
00:01:06.880 --> 00:01:11.740
As we saw before, using SELECT DISTINCT
avoids duplicates in the result.

16
00:01:13.300 --> 00:01:16.850
The WHERE clause has two
kinds of conditions,

17
00:01:17.870 --> 00:01:22.280
the first kind is
a single table condition.

18
00:01:22.280 --> 00:01:30.170
In this case, bar = The Great American Bar
on the Frequents relation.

19
00:01:31.820 --> 00:01:38.215
The second kind is a joined condition
which says that the drinker's attribute in

20
00:01:38.215 --> 00:01:45.090
the frequency relation is the same as the
drinker's attribute of the Likes relation.

21
00:01:46.248 --> 00:01:50.420
We encode this in SQL in the last
line of the query using aliases.

22
00:01:52.140 --> 00:01:57.780
Why did we not say L.beer in the SELECT
clause or F.bar in the first condition?

23
00:01:57.780 --> 00:02:00.410
We could have,
the query would have been equally right.

24
00:02:01.480 --> 00:02:06.520
But we are using a shortcut because we
know that these attributes are unique

25
00:02:06.520 --> 00:02:07.680
already in the query.

26
00:02:09.570 --> 00:02:11.690
Now let's look at the query again,

27
00:02:11.690 --> 00:02:14.980
this time from the viewpoint
of evaluating the query.

28
00:02:16.280 --> 00:02:19.080
There are many ways to evaluate the query,
but

29
00:02:19.080 --> 00:02:21.789
the way it's most likely
to be evaluated is this.

30
00:02:23.610 --> 00:02:28.110
The query will first look at the tables
that have single table conditions.

31
00:02:29.260 --> 00:02:32.640
So it would perform a select operation

32
00:02:33.980 --> 00:02:38.360
on the Frequents table to match
the records of the condition

33
00:02:38.360 --> 00:02:41.700
that The Great American Bar
equal to The Great American Bar.

34
00:02:42.950 --> 00:02:43.990
Why is this strategy good?

35
00:02:45.120 --> 00:02:48.590
It's because the selection operative
reduces the number of triples to consider.

36
00:02:49.720 --> 00:02:53.390
Thus, if there are thousand
triples in the relation frequents,

37
00:02:53.390 --> 00:02:55.760
maybe 60 of them matches the desired bar.

38
00:02:57.240 --> 00:03:03.010
So in the next step, we have to deal with
a fewer number of records than thousand.

39
00:03:04.170 --> 00:03:09.216
All right, the next step will
be a Join with a Likes relation.

40
00:03:09.216 --> 00:03:12.970
A Join requires two relations
in a Join condition,

41
00:03:14.070 --> 00:03:15.620
the Join condition comes from the query.

42
00:03:17.410 --> 00:03:21.102
The first relation shown with
an underscore symbol here

43
00:03:24.008 --> 00:03:27.665
is a result of the previous operation.

44
00:03:27.665 --> 00:03:31.610
Another way of saying this
is that the result of

45
00:03:31.610 --> 00:03:36.470
the selection is piped
into the Join operation.

46
00:03:36.470 --> 00:03:39.930
That means we do not create

47
00:03:39.930 --> 00:03:43.400
an intermediate table from
the result of the selection.

48
00:03:43.400 --> 00:03:49.060
The results are directly supplied to the
next operator, which in this case is Join.

49
00:03:51.340 --> 00:03:56.017
Now the result of the Join operator is an
intermediate structure with columns beer

50
00:03:56.017 --> 00:03:57.373
from Likes relation and

51
00:03:57.373 --> 00:04:01.116
the drinker from the Frequents
relation that we've processed.

52
00:04:03.510 --> 00:04:08.492
This intermediate set of triples is
piped to the Project operation that

53
00:04:08.492 --> 00:04:10.326
picks up the beer column.

54
00:04:12.846 --> 00:04:17.216
Now we need to process
the DISTINCT clause for

55
00:04:17.216 --> 00:04:22.970
Deduplicate elimination,
which then goes to the Output.

56
00:04:24.950 --> 00:04:28.614
We have already seen how the select
project queries on single tables

57
00:04:28.614 --> 00:04:32.610
are evaluated when the tables
are partitioned across several machines.

58
00:04:33.720 --> 00:04:37.070
We'll now see how we process Join
queries in the same setting.

59
00:04:38.780 --> 00:04:44.546
For our case, consider that the Likes and

60
00:04:44.546 --> 00:04:48.470
Frequents tables are on
two different machines.

61
00:04:49.830 --> 00:04:51.740
In the first part of the query,

62
00:04:51.740 --> 00:04:54.699
the selection happens on the machine
with the Frequents table.

63
00:04:55.870 --> 00:05:00.550
The output of the query is a smaller
table with the same scheme as Frequents,

64
00:05:00.550 --> 00:05:03.550
that is with drinkers and bars.

65
00:05:03.550 --> 00:05:05.480
Now we define an operation
called Semijoin,

66
00:05:07.420 --> 00:05:11.070
in which we need to move data
from one machine to another.

67
00:05:13.050 --> 00:05:17.400
The goal of the Semijoin operation is
to reduce the cost of data movement.

68
00:05:18.480 --> 00:05:22.420
That is to move data from
the machine which has

69
00:05:22.420 --> 00:05:27.196
the Frequents data to
the machine with the Likes data.

70
00:05:27.196 --> 00:05:30.670
The cost is reduced if we ship less data.

71
00:05:30.670 --> 00:05:38.090
The way to it is to first find which
data the join operation actually needs.

72
00:05:38.090 --> 00:05:43.570
Clearly, it needs only the drinkers
column and not the bars column.

73
00:05:43.570 --> 00:05:49.070
So the drinkers column is projected out,
then

74
00:05:49.070 --> 00:05:52.440
just this column is transmitted
to the second machine.

75
00:05:54.140 --> 00:05:59.130
Finally, the join is performed by looking
at the values in the Likes column

76
00:05:59.130 --> 00:06:02.080
that only matches the values
in the shipped data.

77
00:06:04.270 --> 00:06:09.720
That means only the data from Likes that
matches the drinkers that are chosen.

78
00:06:11.410 --> 00:06:15.090
These are then the join results which
would go to the output of the operation.

79
00:06:16.460 --> 00:06:19.680
Now here you can see the Semijoin
operation graphically.

80
00:06:20.990 --> 00:06:24.800
The red table on the left is the output
of the selection operations on the left.

81
00:06:27.520 --> 00:06:31.820
The white table on the right
is the table to be joined to.

82
00:06:33.260 --> 00:06:36.120
Since we need only the Drinkers column,

83
00:06:36.120 --> 00:06:38.870
it is projected to create
a one-column relation.

84
00:06:40.380 --> 00:06:46.490
Notice that the red table has two entries
for Pete, who frequented two bars.

85
00:06:46.490 --> 00:06:49.770
But the output of the project
is condensed in the yellow table

86
00:06:49.770 --> 00:06:52.490
to just show the Drinkers,
where Pete appears only once.

87
00:06:54.170 --> 00:06:57.940
For those of you with
a background in computer science,

88
00:06:57.940 --> 00:07:00.450
this can be done using a hash
map like data structure.

89
00:07:02.400 --> 00:07:06.180
This one-column table is now shipped
to Site2, which has the Likes relation.

90
00:07:07.290 --> 00:07:11.834
Now at Site2, the Shipped relation
is used to find matches from

91
00:07:11.834 --> 00:07:16.222
the Drinkers column and
it finds only one match called Sally.

92
00:07:16.222 --> 00:07:19.960
So the corresponding result triples,
in this case,

93
00:07:19.960 --> 00:07:23.956
only one triple is produced
at the end of this operation.

94
00:07:23.956 --> 00:07:29.414
Now, the original table and
the matching table are shipped

95
00:07:29.414 --> 00:07:35.350
to the last of the operation
to finish the Join operation.

96
00:07:35.350 --> 00:07:39.120
And more efficient version of
this is shown in the next slide.

97
00:07:40.120 --> 00:07:47.250
In this version, the first two
steps this and that are the same.

98
00:07:48.620 --> 00:07:52.938
Then the result of the reduce
is also shipped to Site1 to find

99
00:07:52.938 --> 00:07:55.490
the matches from the red relation.

100
00:07:56.510 --> 00:08:00.060
Another reduce operation
is performed on Site1 now

101
00:08:00.060 --> 00:08:03.020
to get the matching records
on the red relation.

102
00:08:03.020 --> 00:08:06.103
Finally, these two reduced
relations are shipped to

103
00:08:06.103 --> 00:08:09.200
the site where the final join happens.

104
00:08:09.200 --> 00:08:11.920
And all of this may seem
like a lot of detail.

105
00:08:13.070 --> 00:08:15.960
Let me repeat something I've said before.

106
00:08:17.600 --> 00:08:23.850
If we have a system like DB2 or Spark SQL
that implements multi-site joins, it

107
00:08:23.850 --> 00:08:27.200
will perform this kind of operation under
the hood, you don't have to know them.

108
00:08:28.460 --> 00:08:34.020
However, if we were to implement
a similar operation and all that you have

109
00:08:34.020 --> 00:08:38.240
is Hadoop, you may end up implementing
this kind of algorithm yourself.
WEBVTT

1
00:00:01.642 --> 00:00:06.010
The queries in real life are little more
complex than what we have seen before.

2
00:00:07.080 --> 00:00:09.180
So let's consider a more complex query.

3
00:00:11.010 --> 00:00:17.190
Let’s find bars where the price
of Miller is the same as or

4
00:00:17.190 --> 00:00:21.600
less than what the great American bar
called TGAB here charges for Bud.

5
00:00:23.090 --> 00:00:28.026
You may say, but we do not really
know what TGAB charges for Bud.

6
00:00:28.026 --> 00:00:34.600
That's correct, so
we can break up the query into two parts.

7
00:00:34.600 --> 00:00:42.060
First, we will find this unknown price,
and then we'll use that price

8
00:00:42.060 --> 00:00:47.160
to find the bars that would sell Miller
for the same price or better price.

9
00:00:47.160 --> 00:00:52.119
Now this is a classic situation where
the result from the first part of

10
00:00:52.119 --> 00:00:56.326
the query should be fed as
a parameter to the second query.

11
00:00:56.326 --> 00:00:59.670
Now this situation is called a subquery.

12
00:01:01.510 --> 00:01:04.090
We write this in SQL as
shown in the slide here.

13
00:01:05.730 --> 00:01:10.730
What makes this query different is that
the part where price is less than equal

14
00:01:10.730 --> 00:01:15.440
to, instead of specifying
a constant like $8,

15
00:01:15.440 --> 00:01:21.610
we actually place another query which
computes the price of a Bud at TGAB.

16
00:01:24.300 --> 00:01:28.600
The shaded part is called the inner query,
or the subquery.

17
00:01:29.870 --> 00:01:32.750
In this case, both the outer query and

18
00:01:32.750 --> 00:01:36.110
the inner query use the same relation,
which is Sells.

19
00:01:37.820 --> 00:01:43.100
Now in terms of evaluation,
the inner query is evaluated first,

20
00:01:43.100 --> 00:01:45.520
and the outer query uses its output.

21
00:01:46.750 --> 00:01:50.110
Now while it may not be
obvious at this time,

22
00:01:50.110 --> 00:01:53.930
notice that the inner query is
independent of the outer query.

23
00:01:54.960 --> 00:01:56.270
In other words,

24
00:01:56.270 --> 00:02:00.930
even if we did not have the outer query,
we can still evaluate the inner query.

25
00:02:02.270 --> 00:02:06.550
We say in this case that
the subquery is uncorrelated.

26
00:02:09.180 --> 00:02:11.260
Let's look at another
example of a subquery.

27
00:02:12.350 --> 00:02:15.504
In this example,
we want to find the name and

28
00:02:15.504 --> 00:02:19.185
manufacturer of each beer
that Fred didn't like.

29
00:02:19.185 --> 00:02:22.953
So how do we know what Fred didn't like?

30
00:02:22.953 --> 00:02:27.545
We do however know that the set of
beers that Fred likes because they

31
00:02:27.545 --> 00:02:29.925
are listed in the Likes relation.

32
00:02:29.925 --> 00:02:34.324
So we need to subtract this set from the
total set of beers that the company has

33
00:02:34.324 --> 00:02:35.030
recorded.

34
00:02:36.260 --> 00:02:40.180
This subtraction of sets can
be performed in several ways.

35
00:02:40.180 --> 00:02:43.660
One of them is to use
the NOT IN construct.

36
00:02:45.130 --> 00:02:50.270
So the query class's job is to take
every name from the Beers table and

37
00:02:50.270 --> 00:02:55.730
output it only if it does not appear
in the set produced by the inner query.

38
00:02:57.180 --> 00:03:01.990
Similar to the previous query,
the subquery here is also uncorrelated.

39
00:03:04.920 --> 00:03:06.640
Now this is a more sophisticated query.

40
00:03:07.745 --> 00:03:11.490
The intention is to find
beers that are more expensive

41
00:03:11.490 --> 00:03:13.820
than the average price of beer.

42
00:03:13.820 --> 00:03:17.250
But since beers have different
prices in different bars,

43
00:03:17.250 --> 00:03:19.520
we have to find the average for every bar.

44
00:03:20.720 --> 00:03:24.340
Therefore the idea is to find
the average price of beer for

45
00:03:24.340 --> 00:03:29.340
every bar and then compare the price of
each beer with respect to this average.

46
00:03:30.670 --> 00:03:32.580
Now look at the query and the table.

47
00:03:34.265 --> 00:03:37.110
Let's assume we are processing
the first table.

48
00:03:39.250 --> 00:03:43.134
The beer is Bud, and the price is $5.

49
00:03:43.134 --> 00:03:50.741
Now we need to know if $5 is greater than
the average price of beer sold at HGAT.

50
00:03:50.741 --> 00:03:55.120
To do this,
we need to compute the inner query, okay?

51
00:03:55.120 --> 00:03:57.410
So now let's look at the fourth row.

52
00:03:57.410 --> 00:04:03.490
The price of Guinness needs to be
compared to that average again for HGAT.

53
00:04:04.550 --> 00:04:07.869
In fact for
every table processed by the outer query,

54
00:04:07.869 --> 00:04:10.980
one needs to compute the inner query for
that bar.

55
00:04:12.240 --> 00:04:16.620
This makes the inner subquery
correlated with the outer query.

56
00:04:17.910 --> 00:04:22.013
Now a smart query processor will store
the average once it's computed and

57
00:04:22.013 --> 00:04:25.929
then reuse the stored value instead
of computing over and over again.

58
00:04:29.376 --> 00:04:31.521
What's an aggregate query?

59
00:04:31.521 --> 00:04:36.440
Let's use a simple example of
finding the average price of Bud.

60
00:04:37.980 --> 00:04:42.100
This is like a simple select project
query with the additional aspect

61
00:04:42.100 --> 00:04:46.010
that it takes a list of price values
of Bud from different bars and

62
00:04:46.010 --> 00:04:47.250
then computes an average.

63
00:04:48.280 --> 00:04:52.749
In the example shown,
the average of the five prices is 4.2.

64
00:04:52.749 --> 00:04:55.230
In other words the average function,

65
00:04:55.230 --> 00:05:00.750
the AVG function, takes a list of
values and produces a single value.

66
00:05:00.750 --> 00:05:03.700
Now there are many functions
that have this behavior.

67
00:05:03.700 --> 00:05:06.090
The SUM function takes
a list of values and

68
00:05:06.090 --> 00:05:08.280
adds them up to produce a single value.

69
00:05:08.280 --> 00:05:11.000
The COUNT function takes
a list of list of values and

70
00:05:11.000 --> 00:05:13.490
counts the number of items
in that list and so on.

71
00:05:16.020 --> 00:05:19.170
These are called aggregate functions.

72
00:05:21.160 --> 00:05:26.208
Now if we wanted to count only the price
values that are different, that is 3,

73
00:05:26.208 --> 00:05:31.690
4, and 5 just once, we can write
the SELECT clause a little differently.

74
00:05:33.350 --> 00:05:38.852
We would say that the average
is over distinct values

75
00:05:38.852 --> 00:05:43.612
of price which in this
case will result in 4.

76
00:05:43.612 --> 00:05:48.481
You should recognize that most analytical
operations need to use statistical

77
00:05:48.481 --> 00:05:50.630
functions which are aggregates.

78
00:05:52.150 --> 00:05:54.850
So another important
analytical requirement

79
00:05:54.850 --> 00:05:58.140
is computing the statistical
aggregate by groups.

80
00:05:58.140 --> 00:06:02.660
For example, we often compute the average
salaries of employees per department.

81
00:06:03.670 --> 00:06:09.070
Now back to our example here,
we want to find the average price paid for

82
00:06:09.070 --> 00:06:15.670
Bud per drinker, where we know
that a drinker visits many bars.

83
00:06:15.670 --> 00:06:18.520
So the grouping variable here is drinker.

84
00:06:19.740 --> 00:06:24.900
So we have three attributes at play,
price which we need to aggregate,

85
00:06:24.900 --> 00:06:29.230
drinker which we need to group by,
and bar which is a join attribute.

86
00:06:30.660 --> 00:06:34.380
The fourth attribute, namely beer,
is used for selection and

87
00:06:34.380 --> 00:06:37.100
does not participate in grouping.

88
00:06:37.100 --> 00:06:42.340
So after the selection we will
get an intermediate relation

89
00:06:42.340 --> 00:06:45.220
containing drinker, bar, and price.

90
00:06:46.420 --> 00:06:50.660
With this, the GROUP BY operation
will create one result row for

91
00:06:50.660 --> 00:06:55.040
each drinker and place the average
price over all such rows.

92
00:06:57.650 --> 00:06:59.090
Now how does GROUP BY and

93
00:06:59.090 --> 00:07:01.630
aggregate computation work
when the data is partitioned?

94
00:07:02.670 --> 00:07:03.570
Let's take the same query.

95
00:07:04.680 --> 00:07:09.071
We are looking for the average
price of Bud grouped by drinker.

96
00:07:11.859 --> 00:07:17.090
But this time the result of the selection
are in two different machines.

97
00:07:17.090 --> 00:07:20.270
Imagine that this time they are range
partitioned by row numbers,

98
00:07:20.270 --> 00:07:22.170
which we have not shown
to maintain clarity.

99
00:07:23.480 --> 00:07:27.374
Now with the GROUP BY operation the data
will get repartitioned by the grouping

100
00:07:27.374 --> 00:07:28.970
attribute, that's drinker.

101
00:07:31.730 --> 00:07:37.340
And then the aggregate
function is computed locally.

102
00:07:38.890 --> 00:07:44.453
To accomplish this repartitioning task,
each machine groups its own data locally,

103
00:07:44.453 --> 00:07:49.783
determines which portions of data should
be transmitted to a different machine,

104
00:07:49.783 --> 00:07:52.580
and accordingly ships it to that machine.

105
00:07:53.630 --> 00:07:56.390
Now there are several variants
of this general scheme

106
00:07:56.390 --> 00:07:58.680
which are even more efficient.

107
00:07:58.680 --> 00:08:02.870
Now if this reminds you of the map
operation you saw in your previous course,

108
00:08:02.870 --> 00:08:03.790
you are exactly right.

109
00:08:04.910 --> 00:08:10.164
This fundamental process of grouping,
partitioning, and redistribution of data

110
00:08:10.164 --> 00:08:15.500
is inherent in data-parallel computing and
implemented inside database systems.
WEBVTT

1
00:00:02.070 --> 00:00:02.600
Welcome back.

2
00:00:03.850 --> 00:00:08.705
In this video, we will provide you
a quick summary of the main points from

3
00:00:08.705 --> 00:00:12.303
our last course on big data modeling and
management.

4
00:00:12.303 --> 00:00:16.882
If you had just completed our second
course and do not need a refresher,

5
00:00:16.882 --> 00:00:19.261
you may now skip to the next lecture.

6
00:00:19.261 --> 00:00:24.311
After this video, you will be able
to recall why big data modeling and

7
00:00:24.311 --> 00:00:29.536
management is essential in preparing
to gain insights from your data,

8
00:00:29.536 --> 00:00:32.693
summarize different kids of data models.

9
00:00:32.693 --> 00:00:38.351
Describe streaming data and the different challenges
it presents, and explain the differences

10
00:00:38.351 --> 00:00:43.403
between a database management system and
a big data management system.

11
00:00:46.510 --> 00:00:51.140
In the second course, we described
a data model as a specification

12
00:00:51.140 --> 00:00:55.194
that precisely characterizes
the structure of the data,

13
00:00:55.194 --> 00:01:00.260
the operations on the data, and
the constraints that may apply on data.

14
00:01:01.360 --> 00:01:05.070
For example, a data model may state that

15
00:01:05.070 --> 00:01:09.030
a data is structured like
a two-dimensional array or a matrix.

16
00:01:10.760 --> 00:01:16.050
For this structure,
one may have a data access operation,

17
00:01:16.050 --> 00:01:21.720
which given an index of the array,
we use the cell of the array to refer to.

18
00:01:24.210 --> 00:01:28.820
A data model may also specify
constraints on the data.

19
00:01:28.820 --> 00:01:33.720
For example, while a total
data set may have many arrays,

20
00:01:33.720 --> 00:01:36.640
the name of each array must be unique and

21
00:01:36.640 --> 00:01:41.420
the values of a specific array
must always be greater than zero.

22
00:01:43.070 --> 00:01:48.335
Database management systems handle
low level data management operations,

23
00:01:48.335 --> 00:01:51.980
help organization of the data
using a data model, and

24
00:01:51.980 --> 00:01:55.147
provide an open programmable
access to data.

25
00:01:57.975 --> 00:02:01.346
We covered a number of data models.

26
00:02:01.346 --> 00:02:04.911
We showed four models that were
discussed in more details.

27
00:02:07.330 --> 00:02:10.810
The relational data to date
is the most used data model.

28
00:02:11.990 --> 00:02:17.189
Here, data is structured like tables
which are formally called relations.

29
00:02:18.260 --> 00:02:21.840
The relational data model has been
implemented in traditional database

30
00:02:21.840 --> 00:02:23.120
systems.

31
00:02:23.120 --> 00:02:28.240
But they are being refreshly implemented
in modern data systems over Hadoop and

32
00:02:28.240 --> 00:02:31.400
Spark and
are getting deployed on cloud platforms.

33
00:02:32.700 --> 00:02:37.930
The second category of data gaining
popularity is semi-structured data,

34
00:02:37.930 --> 00:02:42.840
which includes documents like HTML pages,
XML data and

35
00:02:42.840 --> 00:02:45.960
JSON data that are used by
many Internet applications.

36
00:02:47.470 --> 00:02:50.201
This data can have one element nested or

37
00:02:50.201 --> 00:02:55.674
embedded within another data element and
hence can often be modeled as a tree.

38
00:02:58.779 --> 00:03:03.480
The third category of data
models is called graph data.

39
00:03:03.480 --> 00:03:08.700
A graph is a network where
nodes represent entities and

40
00:03:08.700 --> 00:03:12.540
edges represent relationships
between pairs of such entities.

41
00:03:13.800 --> 00:03:19.675
For example, in a social network,
nodes may represent users and

42
00:03:19.675 --> 00:03:23.160
edges may represent their friendship.

43
00:03:23.160 --> 00:03:27.990
The operations performed on graph data
includes traversing the network so

44
00:03:27.990 --> 00:03:32.000
that one can find friend of
a friend of a friend if needed.

45
00:03:34.200 --> 00:03:39.890
In contrast to the previous three models,
that there is a structure to the data,

46
00:03:39.890 --> 00:03:45.280
the text data is much more unstructured
because an entire data item

47
00:03:45.280 --> 00:03:47.780
like a new article can
be just a text string.

48
00:03:49.180 --> 00:03:53.540
However, text is the primary form of data

49
00:03:53.540 --> 00:03:57.120
in information retrieval systems or
search engines like Google.

50
00:04:00.250 --> 00:04:05.290
We also discussed streaming data,
or data with velocity, as a special

51
00:04:05.290 --> 00:04:10.580
class of data that continually come
to the system at some data rate.

52
00:04:12.350 --> 00:04:17.308
Examples can be found in data coming
from road sensors that measure traffic

53
00:04:17.308 --> 00:04:21.636
patterns or stock price data from
the stock exchange that may come

54
00:04:21.636 --> 00:04:25.111
in volumes from stock
exchanges all over the world.

55
00:04:27.563 --> 00:04:35.090
Streaming data is special because a stream
is technically an infinite data source.

56
00:04:35.090 --> 00:04:38.050
And therefore,
we keep filling up memory and

57
00:04:38.050 --> 00:04:42.390
storage and will eventually go
beyond the capacity of any system.

58
00:04:43.930 --> 00:04:47.720
Streaming data, therefore, needs
a different kind of management system.

59
00:04:48.960 --> 00:04:54.135
For this reason,
streaming data is processed in memory,

60
00:04:54.135 --> 00:04:57.841
in chunks which are also called windows.

61
00:04:57.841 --> 00:05:01.226
Often only the necessary
part of the data stream or

62
00:05:01.226 --> 00:05:05.186
the results of queries against
the data stream is stored.

63
00:05:07.145 --> 00:05:13.180
A typical type of query against streaming
data are alerts or notifications.

64
00:05:13.180 --> 00:05:17.933
The system notices an event like multiple
stock price changing within a short time.

65
00:05:21.658 --> 00:05:24.160
Streaming data is also used for
prediction.

66
00:05:26.330 --> 00:05:31.287
For instance, based on wind direction and
temperature data streams,

67
00:05:31.287 --> 00:05:34.768
one can predict how a wildfire
is going to spread.

68
00:05:37.640 --> 00:05:42.522
In the last course, we also covered
a number of data systems that we called

69
00:05:42.522 --> 00:05:44.590
big data management systems.

70
00:05:46.510 --> 00:05:49.890
These systems use
different data models and

71
00:05:49.890 --> 00:05:55.030
have different capabilities, but
are characterized by some common features.

72
00:05:56.210 --> 00:06:00.430
They are also designed from the start for
parallel and distributed processing.

73
00:06:01.610 --> 00:06:07.100
Most of them implement data partition
parallelism, which, if you can recall,

74
00:06:07.100 --> 00:06:11.890
refers to the process of segmenting
the data into multiple machines so

75
00:06:11.890 --> 00:06:16.839
data retrieval and manipulations can be
performed in parallel on these machines.

76
00:06:18.880 --> 00:06:24.360
Many of these systems allow a large
number of users who constantly update and

77
00:06:24.360 --> 00:06:25.510
query the system.

78
00:06:27.590 --> 00:06:32.190
Some of the systems do not maintain
transaction consistency with every update.

79
00:06:33.300 --> 00:06:34.224
That means,

80
00:06:34.224 --> 00:06:39.437
not all the machines may have all
the updates guaranteed at every moment.

81
00:06:41.091 --> 00:06:46.460
However, most of them provide
a guarantee of eventual consistency,

82
00:06:46.460 --> 00:06:51.780
which means all the machines will
get all updates sooner or later.

83
00:06:51.780 --> 00:06:54.920
Therefore, providing better accuracy and
time.

84
00:06:57.610 --> 00:07:02.740
The third common characteristic
of big data management systems is

85
00:07:02.740 --> 00:07:06.750
that they are often built on top of
a Hadoop-like platform that provides

86
00:07:06.750 --> 00:07:11.040
automatic replication and
a map-reduce style processing ability.

87
00:07:12.270 --> 00:07:16.440
Some of the data operations performed
within these systems make use of these

88
00:07:16.440 --> 00:07:17.829
lower level capabilities.

89
00:07:20.200 --> 00:07:22.520
After this refresher on data modeling and

90
00:07:22.520 --> 00:07:26.650
management, let's start big data
integration and processing.
WEBVTT

1
00:00:02.188 --> 00:00:03.689
So, hi.

2
00:00:03.689 --> 00:00:08.521
In the previous course, we saw
examples of different data models and

3
00:00:08.521 --> 00:00:12.290
talked about a few current
data management systems.

4
00:00:12.290 --> 00:00:15.617
In this module,
we'll focus on data retrieval.

5
00:00:35.163 --> 00:00:40.012
Data retrieval refers to
the way in which data desired

6
00:00:40.012 --> 00:00:44.980
by a user is specified and
retrieved from a data store.

7
00:00:46.280 --> 00:00:50.720
Note that in this course, we are using
the term data retrieval in two ways.

8
00:00:51.760 --> 00:00:56.430
Assume that your data is stored in a data
store that follows a specific data model,

9
00:00:56.430 --> 00:00:58.190
like for
example the relational data model.

10
00:00:59.340 --> 00:01:03.300
By data retrieval, we will refer to, one,

11
00:01:03.300 --> 00:01:07.950
the way you specify how to get
the desired data out of the system,

12
00:01:07.950 --> 00:01:12.910
this is called the query
specification method, and two,

13
00:01:12.910 --> 00:01:18.300
the internal processing that occurs within
the data management system to compute or

14
00:01:18.300 --> 00:01:21.520
evaluate that specified retrieval request.

15
00:01:23.170 --> 00:01:28.780
While query specification can apply to
small data stores or large data stores,

16
00:01:28.780 --> 00:01:32.600
we'll keep an eye on the nature of
query evaluation when the data is big.

17
00:01:34.230 --> 00:01:38.370
Further, we'll consider how
the query specification changes

18
00:01:38.370 --> 00:01:40.770
when we deal with faster streaming data.

19
00:01:43.380 --> 00:01:48.590
A query language is a language in which
a retrieval request is specified.

20
00:01:50.730 --> 00:01:56.232
A query language is often called
declarative, which means it lets you

21
00:01:56.232 --> 00:02:01.600
specify what you want to retrieve without
having to tell the system how to retrieve.

22
00:02:02.670 --> 00:02:04.660
For example, you can say,

23
00:02:04.660 --> 00:02:10.550
find all data from relation employee
where the salary is more than 50k.

24
00:02:10.550 --> 00:02:14.970
Now, you don't have to write a program
which will tell the system to open a file,

25
00:02:14.970 --> 00:02:20.006
skip the first 250 bytes,
then in a loop pick the next 1024 bytes,

26
00:02:20.006 --> 00:02:24.340
probe into the 600th byte and
read an integer, and so forth.

27
00:02:25.480 --> 00:02:28.160
Instead of writing such
a complicated procedure,

28
00:02:28.160 --> 00:02:32.450
you just specify the data items that
you need and the system does the rest.

29
00:02:33.840 --> 00:02:38.167
For example, SQL,
structured query language,

30
00:02:38.167 --> 00:02:42.810
is the most used query language for
relational data.

31
00:02:42.810 --> 00:02:47.156
Now, in contrast to a query language,
a database programming

32
00:02:47.156 --> 00:02:52.076
language like Oracle's PL/SQL or
Postgres's PgSQL are high-level

33
00:02:52.076 --> 00:02:56.680
procedural programming languages
that embed query operations.

34
00:02:57.870 --> 00:03:00.648
We will look at some query
languages in detail and

35
00:03:00.648 --> 00:03:03.649
show examples of database
programming languages.

36
00:03:06.040 --> 00:03:08.881
The first query language
we'll look at is SQL,

37
00:03:08.881 --> 00:03:13.002
which is the ubiquitous query
language when the data is structured,

38
00:03:13.002 --> 00:03:18.120
but has been extended in many ways
to accommodate other types of data.

39
00:03:18.120 --> 00:03:22.329
For this course, we'll stick to
the structured aspect of the language.

40
00:03:22.329 --> 00:03:26.939
Now, you should know that SQL is used for
classical database management systems

41
00:03:26.939 --> 00:03:31.440
like Oracle as well as modern Hadoop
style distributed systems such as Spark.

42
00:03:32.650 --> 00:03:35.220
Now, we will work with
an illustrative example.

43
00:03:37.140 --> 00:03:40.360
First, we need to define
the schema of the database.

44
00:03:40.360 --> 00:03:45.640
Now, think of a business called the
Beer Drinkers Club that owns many bars,

45
00:03:45.640 --> 00:03:46.990
and each bar sells beer.

46
00:03:48.060 --> 00:03:52.646
Our schema for
this business has six relations of tables.

47
00:03:52.646 --> 00:03:57.448
The first table lists these bars,
the names, addresses, and

48
00:03:57.448 --> 00:03:59.814
the license number of the bar.

49
00:03:59.814 --> 00:04:04.469
Notice that the attribute name is
underlined because it is the primary key

50
00:04:04.469 --> 00:04:05.930
of the bars relation.

51
00:04:05.930 --> 00:04:09.800
Recall that the primary key
refers to a set of attributes,

52
00:04:09.800 --> 00:04:13.350
in this case just the name,
that makes a record unique.

53
00:04:15.060 --> 00:04:17.720
Note that the relation bars
with the attribute name

54
00:04:19.080 --> 00:04:22.241
within parenthesis is the same
as the table shown on the right.

55
00:04:23.280 --> 00:04:27.147
We will use both representations
as we go forward.

56
00:04:27.147 --> 00:04:32.030
The second table called Beers, this is
the names and manufacturers of beer.

57
00:04:32.030 --> 00:04:36.266
Now, not every bar sells the same
brands of beer, and even when they do,

58
00:04:36.266 --> 00:04:38.280
they may have different prices for

59
00:04:38.280 --> 00:04:42.390
the same product because of differences
in the establishment costs.

60
00:04:43.550 --> 00:04:48.960
So the Sells table records which
bar sells which beer at what price.

61
00:04:50.970 --> 00:04:53.268
Now, our business is special.

62
00:04:53.268 --> 00:04:57.358
It also keeps information about
the regular member customers.

63
00:04:57.358 --> 00:05:02.750
So the Drinkers relation has the name,
address, and phone of these customers.

64
00:05:02.750 --> 00:05:07.032
Well, not only that,
it knows which member visits

65
00:05:07.032 --> 00:05:11.720
which bars and
which beer each member likes.

66
00:05:11.720 --> 00:05:14.980
Clearly, the Beer Drinkers Club
knows its customers.

67
00:05:17.670 --> 00:05:24.490
The most basic structure of an SQL
query is a SELECT-FROM-WHERE clause.

68
00:05:25.500 --> 00:05:28.920
In this example, we're looking for
beer names that are made by Heineken.

69
00:05:30.060 --> 00:05:33.160
So we need to specify
our output attribute,

70
00:05:33.160 --> 00:05:35.520
in this case the name of the beer.

71
00:05:35.520 --> 00:05:41.040
The logical table which will be used to
answer the query, in this case, Beers.

72
00:05:42.370 --> 00:05:46.972
And the condition that all the desired
data items should satisfy,

73
00:05:46.972 --> 00:05:51.260
namely, the value of the attribute
called manf is Heineken.

74
00:05:52.600 --> 00:05:55.570
Now, there are few things to notice here.

75
00:05:55.570 --> 00:05:59.311
First, the literal Heineken
is put within quotes,

76
00:05:59.311 --> 00:06:02.210
because it's a single string literal.

77
00:06:03.470 --> 00:06:04.550
Remember that in this case,

78
00:06:04.550 --> 00:06:08.830
the string is supposed to match exactly,
including the case.

79
00:06:10.910 --> 00:06:16.590
Secondly, if you go back to the data
operations discussed in course two,

80
00:06:16.590 --> 00:06:21.610
you will recognize that this form
of query can also be represented

81
00:06:21.610 --> 00:06:27.070
as a selection operation on the relation
Beers with a condition on the manf

82
00:06:27.070 --> 00:06:30.800
attribute, followed by
a projection operation

83
00:06:30.800 --> 00:06:34.349
that outputs the name attribute from
the result of the selection operation.

84
00:06:35.680 --> 00:06:40.340
So the selection operation finds all
tuples of beer for which the manufacturer

85
00:06:40.340 --> 00:06:45.710
is Heineken, and from those tuples
it projects only the name column.

86
00:06:47.300 --> 00:06:51.580
The result of the query is a table
with one single attribute called name.

87
00:06:54.250 --> 00:06:58.883
We illustrate some more features of SQL,
using two example queries.

88
00:06:58.883 --> 00:07:02.330
The first looks for
expensive beer and its price.

89
00:07:03.370 --> 00:07:06.856
Let's say we consider a beer to be
expensive if it costs more than $15

90
00:07:06.856 --> 00:07:07.569
per bottle.

91
00:07:08.810 --> 00:07:10.200
From the schema,

92
00:07:10.200 --> 00:07:13.830
we know that the price information is
available in the table called Sells.

93
00:07:14.900 --> 00:07:17.470
So the FROM clause should use Sells.

94
00:07:18.620 --> 00:07:20.490
The WHERE clause is intuitive and

95
00:07:20.490 --> 00:07:24.200
specifies the price of the beer
to be greater than 15.

96
00:07:24.200 --> 00:07:28.873
Now notice that the Sells relation
also has a column called bar.

97
00:07:28.873 --> 00:07:35.170
Now, if two different bars sell
the same beer at the same price,

98
00:07:35.170 --> 00:07:37.570
we'll get both entries in the result.

99
00:07:37.570 --> 00:07:39.620
But that's not what we want.

100
00:07:39.620 --> 00:07:43.850
Now regardless of the multiplicity
of bars that have the same price for

101
00:07:43.850 --> 00:07:46.340
the same beer,
we want the result just once.

102
00:07:48.320 --> 00:07:53.688
So this is achieved through
the SELECT DISTINCT statement,

103
00:07:53.688 --> 00:07:59.384
which ensures that the result
relation will have no duplicate.

104
00:07:59.384 --> 00:08:03.736
The second example shows the case
where more than one condition

105
00:08:03.736 --> 00:08:06.045
must be specified by the result.

106
00:08:06.045 --> 00:08:09.774
In this query,
the business must be in San Diego and

107
00:08:09.774 --> 00:08:13.764
at the same time it must be
a temporary license holder,

108
00:08:13.764 --> 00:08:17.870
which means the license
number should start with 32.

109
00:08:17.870 --> 00:08:23.280
As we see here, these conditions
are put together by the AND operator.

110
00:08:25.230 --> 00:08:29.330
Thus, the query will pick
the third record in the table

111
00:08:29.330 --> 00:08:33.560
because the first record satisfy the first
condition and not the second condition.

112
00:08:34.950 --> 00:08:38.780
In a few slides, we'll come back to
the evaluation of this type of queries

113
00:08:38.780 --> 00:08:40.625
in the context of big data.

114
00:08:40.625 --> 00:08:47.340
Now, remember, one can also place a limit
on the number of results to return.

115
00:08:47.340 --> 00:08:52.163
If our database is large, and
we need only five results, for

116
00:08:52.163 --> 00:08:56.620
example, for a sample to display,
we can say LIMIT 5.

117
00:08:56.620 --> 00:09:03.706
Now, the exact syntax of this LIMIT
clause may vary between DBMS vendors.
WEBVTT

1
00:00:02.450 --> 00:00:06.680
Now if the table of beers was large and
had millions of entries,

2
00:00:07.820 --> 00:00:11.480
the table would possibly need
to be split over many machines.

3
00:00:12.600 --> 00:00:16.720
Another way of saying that is that
the table will be partitioned

4
00:00:16.720 --> 00:00:18.960
across a number of machines.

5
00:00:18.960 --> 00:00:21.210
Since a query simply
performs a selection and

6
00:00:21.210 --> 00:00:24.330
projection here,
it can be evaluated in parallel.

7
00:00:25.860 --> 00:00:28.860
Remember that name is
the primary key of the table.

8
00:00:30.070 --> 00:00:32.790
One standard way of partitioning the data

9
00:00:32.790 --> 00:00:35.260
is called a range partitioning
by the primary key.

10
00:00:37.210 --> 00:00:41.760
This simply means that the rows
of the table are put in groups

11
00:00:41.760 --> 00:00:45.440
depending on the alphabetical
order of the name value.

12
00:00:46.570 --> 00:00:50.950
So beers with names starting with E and
B here are placed in Machine 1.

13
00:00:50.950 --> 00:00:53.910
Those starting with C and
D are in Machine 2.

14
00:00:53.910 --> 00:00:58.337
And if there are too many rows for
entries where the name starts with H,

15
00:00:58.337 --> 00:01:00.902
maybe H is split into Machines 5 and 6.

16
00:01:00.902 --> 00:01:02.260
This is shown in the sketch here.

17
00:01:03.640 --> 00:01:08.620
Next, we will show how queries
are performed over partition tables.

18
00:01:08.620 --> 00:01:13.230
But before we do that, you should know
that all database management companies,

19
00:01:13.230 --> 00:01:18.090
like IBM, Chair Data, Microsoft, and
others, have a solution like this for

20
00:01:18.090 --> 00:01:21.670
large volumes of data,
where data partitioning is used.

21
00:01:21.670 --> 00:01:26.030
Newer systems, like Spark and SQL,
are naturally distributed, and

22
00:01:26.030 --> 00:01:27.419
therefore, offer data partitioning.

23
00:01:30.470 --> 00:01:33.620
So, we show the same partition
tables as we saw before.

24
00:01:33.620 --> 00:01:35.180
Now we'll ask two queries.

25
00:01:36.550 --> 00:01:41.340
The first query asks for
all tuples as records from the beers table

26
00:01:42.500 --> 00:01:45.290
where the name of the beer starts with Am.

27
00:01:46.970 --> 00:01:49.400
And the second query is
exactly what we asked before.

28
00:01:52.544 --> 00:01:55.560
The first query in
the SQL looks like this.

29
00:01:56.730 --> 00:02:01.434
We said SELECT* FROM Beers to mean
all attributes from table beers.

30
00:02:01.434 --> 00:02:05.790
The WHERE clause shows the syntax for
a partial match query.

31
00:02:06.940 --> 00:02:10.850
In this query,
there are two new syntax elements.

32
00:02:10.850 --> 00:02:12.630
The first is a predicate called like.

33
00:02:14.480 --> 00:02:18.080
When we use like,
we're telling the query engine

34
00:02:18.080 --> 00:02:21.870
that we only have partial information
about the string we want to match.

35
00:02:22.870 --> 00:02:25.870
This partly specified string
is called a string pattern.

36
00:02:27.290 --> 00:02:30.320
That means, there is this part
of the string we know and

37
00:02:30.320 --> 00:02:31.460
a part that we do not know.

38
00:02:33.050 --> 00:02:39.300
In this case, we know that our design
string starts with Am, so we'd write Am,

39
00:02:39.300 --> 00:02:44.250
and then we put % to refer to the part
of the string that we do not know.

40
00:02:44.250 --> 00:02:48.410
Putting them together, we get Am%.

41
00:02:48.410 --> 00:02:52.671
If we wanted to find, say,
Am somewhere in the middle of the string,

42
00:02:52.671 --> 00:02:55.027
we would write the pattern as %Am%.

43
00:02:55.027 --> 00:03:01.680
The second query is not new.

44
00:03:01.680 --> 00:03:03.580
We saw it in the last slide.

45
00:03:03.580 --> 00:03:08.490
However, as we'll see next, evaluating
the second query will be a little more

46
00:03:08.490 --> 00:03:12.930
tricky in a partition database than
that we usually see for big data.

47
00:03:17.390 --> 00:03:20.780
Let's talk about the first query
in this data partition setting.

48
00:03:20.780 --> 00:03:27.160
The question to ask is, do we need to
touch all partitions to answer the query?

49
00:03:27.160 --> 00:03:33.720
Of course not, we know that the name is
a primary key for the table of beers.

50
00:03:33.720 --> 00:03:37.710
We also know that the system did arrange
partitioning on the name attribute.

51
00:03:38.710 --> 00:03:43.260
This means that the evaluation
process should only access Machine 1

52
00:03:43.260 --> 00:03:47.370
because no other machine will have
records for names starting with A.

53
00:03:48.670 --> 00:03:50.600
Now this is exactly what we, as humans,

54
00:03:50.600 --> 00:03:55.780
do when we look up an entry in
a multivolume encyclopedia.

55
00:03:55.780 --> 00:03:59.300
We look for the starting words, then
figure out which specific volume would

56
00:03:59.300 --> 00:04:01.410
have that entry,
then pick up just that volume.

57
00:04:02.830 --> 00:04:06.330
Thus, so long as the system
knows the partitioning strategy,

58
00:04:07.380 --> 00:04:09.310
it can make its job much more efficient.

59
00:04:10.760 --> 00:04:14.920
When a system processes
thousands of queries per second,

60
00:04:14.920 --> 00:04:17.430
this kind of efficiency actually matters.

61
00:04:18.970 --> 00:04:22.785
Now raised partitioning is
only one of many partitioning

62
00:04:22.785 --> 00:04:25.650
schemes used in a database system, okay.

63
00:04:26.790 --> 00:04:29.760
Let's try to answer the second query
in the same partition setting.

64
00:04:30.820 --> 00:04:34.650
Now the query condition is on
the second attribute, manf.

65
00:04:35.690 --> 00:04:37.900
Now in one sense, it's a simpler query.

66
00:04:37.900 --> 00:04:39.392
There is no light pattern here, and

67
00:04:39.392 --> 00:04:42.789
we know exactly the string that we are
looking for, namely the string Heineken.

68
00:04:43.930 --> 00:04:47.310
However, this time,
we really cannot get away

69
00:04:47.310 --> 00:04:51.520
by using the partitioning information
because the partitioning activity is

70
00:04:51.520 --> 00:04:55.130
different from the attribute on which
the query condition is applied.

71
00:04:56.300 --> 00:05:00.023
So this query will need
to go to all partitions.

72
00:05:00.023 --> 00:05:05.200
Technically speaking,
the query needs to be broadcast

73
00:05:05.200 --> 00:05:08.870
from the primary machine to all machines,
as shown here.

74
00:05:11.500 --> 00:05:15.320
Next, this broadcast query
will be independently, and

75
00:05:15.320 --> 00:05:19.470
in parallel,
execute the query on the local machine.

76
00:05:20.840 --> 00:05:24.420
Then, these results need to be
brought back into the primary machine.

77
00:05:25.690 --> 00:05:28.760
And then,
they need to be unioned together.

78
00:05:28.760 --> 00:05:31.880
And only then, the results can be
formed and returned to the client.

79
00:05:33.130 --> 00:05:35.490
Now, this might seem
like a lot of extra work.

80
00:05:36.510 --> 00:05:41.440
However, remember, the shaded part of
the query is executed in parallel,

81
00:05:41.440 --> 00:05:43.830
which is the essence of
dealing with large data.

82
00:05:46.350 --> 00:05:51.770
Now, at this point, you might be thinking,
wait a minute, what if I had 100 machines,

83
00:05:52.940 --> 00:05:55.800
and the desired data
is only in 20 of them?

84
00:05:57.340 --> 00:06:01.870
Should we needlessly go through all 100
machines, find nothing in 80 of them, and

85
00:06:01.870 --> 00:06:04.080
return 0 results from those machines?

86
00:06:04.080 --> 00:06:05.760
Then why do the extra work?

87
00:06:05.760 --> 00:06:06.790
Can it not be avoided?

88
00:06:08.560 --> 00:06:12.280
Well, to do this, it would need
one more piece in the solution,

89
00:06:12.280 --> 00:06:13.700
it's called an index structure.

90
00:06:14.990 --> 00:06:16.290
Very simply,

91
00:06:16.290 --> 00:06:20.930
an index can be thought of as a reverse
table, where given the value in a column,

92
00:06:20.930 --> 00:06:25.220
you would get back the records where the
value appears as shown in the figure here.

93
00:06:26.810 --> 00:06:31.147
Using an index speeds up query
processing significantly.

94
00:06:31.147 --> 00:06:34.940
With indexes, we can solve this
problem in many different ways.

95
00:06:37.110 --> 00:06:41.930
The top table shows the case where
each machine has its own index for

96
00:06:41.930 --> 00:06:43.400
the manf column.

97
00:06:43.400 --> 00:06:47.980
This is called a local index because
the index is in every machine

98
00:06:47.980 --> 00:06:51.050
that holds the data for
that table on that machine.

99
00:06:53.070 --> 00:06:55.670
In this case,
looking up Heineken in the index,

100
00:06:55.670 --> 00:06:57.950
we would know which records
would have the data.

101
00:06:59.500 --> 00:07:04.750
Since the index is local, the main query
will indeed go to all machines, but

102
00:07:04.750 --> 00:07:08.660
the lookup will be really instant, and the
empty results would return very quickly.

103
00:07:11.010 --> 00:07:13.410
In the second case,
we adopted a different solution.

104
00:07:14.610 --> 00:07:20.500
Here, there is an index on the main
machine, all on a separate index server.

105
00:07:20.500 --> 00:07:23.720
Now when we place a data
record in a machine,

106
00:07:23.720 --> 00:07:27.959
this index keeps an account of the machine
that contains the record with that value.

107
00:07:29.240 --> 00:07:30.720
Look at the second table to the right.

108
00:07:32.510 --> 00:07:37.398
Given the value of Heineken,
we know that it is only in three machines,

109
00:07:37.398 --> 00:07:41.960
and therefore,
we can avoid going to the other machines.

110
00:07:41.960 --> 00:07:45.840
Clearly, we can always use
both indexing schemes.

111
00:07:45.840 --> 00:07:49.736
This will use more space,
but queries will be faster.

112
00:07:49.736 --> 00:07:54.717
Now this gives you some of the choices
you may need to make with big data,

113
00:07:54.717 --> 00:07:59.220
whether you use a parallel DBMS or
a distributed data solution.
WEBVTT

1
00:00:01.850 --> 00:00:05.240
Welcome to Big Data Integration and
Processing.

2
00:00:05.240 --> 00:00:08.190
 Welcome to course three of
the big data specialization.

3
00:00:08.190 --> 00:00:09.710
I'm Amarnath Gupta.

4
00:00:09.710 --> 00:00:11.650
 And I'm Ilkay Altintas.

5
00:00:11.650 --> 00:00:14.590
We are really excited to
work with you in this course

6
00:00:14.590 --> 00:00:19.820
to develop your understanding and skills
in big data integration and processing.

7
00:00:19.820 --> 00:00:24.180
By now you might have just
finished our first two courses and

8
00:00:24.180 --> 00:00:27.870
learned the basics of big data
modelling and management.

9
00:00:27.870 --> 00:00:30.480
If you haven't, it's not required.

10
00:00:30.480 --> 00:00:33.560
But for those that less background
in the data modelling and

11
00:00:33.560 --> 00:00:37.680
data management areas you
might find it valuable.

12
00:00:37.680 --> 00:00:42.030
 We understand that you may not have
any background on data management.

13
00:00:42.030 --> 00:00:47.382
We are going to introduce Query languages,
we'll first look at SQL in some detail,

14
00:00:47.382 --> 00:00:51.740
and then move to Query languages for
MongoDB which is a semi structured

15
00:00:51.740 --> 00:00:56.260
data management system, and Aerospike,
which is a key value store.

16
00:00:57.740 --> 00:01:02.200
 We will also introduce concepts
related to processing of big data as

17
00:01:02.200 --> 00:01:03.890
big data pipelines.

18
00:01:03.890 --> 00:01:08.160
We talk about data structures and
transformations related to batch and

19
00:01:08.160 --> 00:01:11.230
stream processing as steps in a pipeline.

20
00:01:11.230 --> 00:01:14.100
Providing us a way to talk
about big data processing

21
00:01:14.100 --> 00:01:17.180
without getting into the details
of the underlying technologies.

22
00:01:18.630 --> 00:01:23.710
Once we have reviewed the concepts and
related systems, we will switch gears

23
00:01:23.710 --> 00:01:28.370
to hands on exercises with Spark,
one of the most popular big data engines.

24
00:01:29.470 --> 00:01:34.060
We will show you examples of patch and
stream processing using Spark.

25
00:01:35.780 --> 00:01:39.410
 As you know for
many data science applications

26
00:01:39.410 --> 00:01:44.140
one has to use many different databases
and analyze the integrated data.

27
00:01:44.140 --> 00:01:50.160
In fact, data integration is one leading
cause leading to the bigness of data.

28
00:01:51.410 --> 00:01:54.780
We'll give you a rapid exposure to
information integration systems

29
00:01:54.780 --> 00:01:59.530
through use cases and point out the big
data aspects one should pay attention to.

30
00:02:00.760 --> 00:02:06.620
 We are also excited to show you
examples of data processing using Splunk.

31
00:02:06.620 --> 00:02:10.510
Our goal here is to provide you
with simple hands on exercises

32
00:02:10.510 --> 00:02:12.550
that require no programming, but

33
00:02:12.550 --> 00:02:18.660
show you how one can use interfaces like
Splunk to manage and process big data.

34
00:02:18.660 --> 00:02:20.820
We wish you a fun time learning and

35
00:02:20.820 --> 00:02:26.080
hope to hear from you in the discussions
forums and learner stories as usual.

36
00:02:26.080 --> 00:02:27.880
 Well, happy learning and think big data
WEBVTT

1
00:00:02.627 --> 00:00:07.359
In this video, we will talk about
the challenges of ingesting and

2
00:00:07.359 --> 00:00:12.178
processing big data and
remind ourselves why need any paradigm and

3
00:00:12.178 --> 00:00:14.690
programming models for big data.

4
00:00:15.900 --> 00:00:20.710
After this video, you will be able to
summarize the requirements of programming

5
00:00:20.710 --> 00:00:23.650
models for big data and
why you should care about them.

6
00:00:24.730 --> 00:00:27.800
You will also be able to explain
how the challenges of big

7
00:00:27.800 --> 00:00:32.750
data related to its variety, volume and
velocity affects its processing.

8
00:00:36.090 --> 00:00:41.880
Before we start,
let's imagine an online gaming newscase,

9
00:00:41.880 --> 00:00:44.870
just like the one we have for
Catch the Pink Flamingo.

10
00:00:47.140 --> 00:00:52.123
You just introduced the game,
and users started signing up.

11
00:00:52.123 --> 00:00:55.101
You start with a traditional
relational database,

12
00:00:55.101 --> 00:00:57.950
keeping track of user sessions and
other events.

13
00:01:00.130 --> 00:01:05.030
Your game server receives
an event notification every time

14
00:01:05.030 --> 00:01:08.710
a user opens his session and
makes a point in the game.

15
00:01:09.990 --> 00:01:13.690
Initially, everything is great,
your game is working and

16
00:01:13.690 --> 00:01:17.230
the database is able to handle the event
streams coming into the server.

17
00:01:18.490 --> 00:01:23.960
However, suddenly your game becomes
highly popular a good problem to have.

18
00:01:25.890 --> 00:01:29.670
The database management system in
your game server won't be able to

19
00:01:29.670 --> 00:01:31.660
handle the load anymore.

20
00:01:31.660 --> 00:01:35.630
You start getting errors that the events
can't be inserted into the database

21
00:01:35.630 --> 00:01:37.240
at the speed they are coming in.

22
00:01:38.650 --> 00:01:45.240
You decide that you will have a buffer or
a queue to process the advancing chunks.

23
00:01:45.240 --> 00:01:50.600
Maybe also at the same time processing
them to be organized in windows of time or

24
00:01:50.600 --> 00:01:51.260
game sessions.

25
00:01:53.390 --> 00:01:58.916
However, in time as the demand goes up,
you will need more processing nodes and

26
00:01:58.916 --> 00:02:02.638
even more database servers
that can handle the load.

27
00:02:02.638 --> 00:02:08.042
This is, a typical scenario that
most web sites face when confronted

28
00:02:08.042 --> 00:02:13.373
with big data issues related to
volume and velocity of information.

29
00:02:13.373 --> 00:02:15.894
As this scenario demonstrates,

30
00:02:15.894 --> 00:02:20.580
solving the problem in one step
might be possible initially.

31
00:02:20.580 --> 00:02:25.382
But the more reactive fixes
the game developers add, the system

32
00:02:25.382 --> 00:02:29.630
becomes less robust and
more complicated to evolve.

33
00:02:31.880 --> 00:02:35.210
While the developers initially
started with an application and

34
00:02:35.210 --> 00:02:36.360
the database to manage.

35
00:02:37.400 --> 00:02:41.614
Now they have to manage a number
of issues related to this

36
00:02:41.614 --> 00:02:46.920
infrastructure management just to
keep up with the load on the system.

37
00:02:46.920 --> 00:02:52.282
Similarly, the database servers
can be effected and corrupted.

38
00:02:52.282 --> 00:02:57.150
The replication and fault tolerance of
them need to be handled separately.

39
00:02:58.240 --> 00:03:01.745
Let's start by going through these issues.

40
00:03:01.745 --> 00:03:05.300
Let's say,
one of the processing nodes went down.

41
00:03:06.470 --> 00:03:11.970
The system needs to manage and
restart the processing and

42
00:03:11.970 --> 00:03:14.920
there will be potentially some
data loss in the meantime.

43
00:03:16.400 --> 00:03:19.560
The system would need to
check every processing node

44
00:03:19.560 --> 00:03:21.020
before it can discard data.

45
00:03:22.040 --> 00:03:28.373
Each note and each database has
to be replicated separately.

46
00:03:28.373 --> 00:03:35.255
Batch computations that need data from
multiple data servers need to access and

47
00:03:35.255 --> 00:03:42.453
maintain use of the data separately which
might end up being quite slow and costly.

48
00:03:42.453 --> 00:03:46.705
Big data processing techniques
we will address in this course,

49
00:03:46.705 --> 00:03:51.430
will help you to reduce the management
of the mentioned complexities,

50
00:03:51.430 --> 00:03:55.226
including failing servers and
breaking compute nodes.

51
00:03:55.226 --> 00:04:00.840
While helping with the scalability of the
management and processing infrastructure.

52
00:04:02.610 --> 00:04:07.170
We will talk about using big data systems
like Spark to achieve data parallel

53
00:04:07.170 --> 00:04:12.130
processing scalability for
data applications on commodity clusters.

54
00:04:13.450 --> 00:04:18.080
We will use to Spark Runtime Libraries and
Programming Models to

55
00:04:18.080 --> 00:04:22.230
demonstrate how big data systems can
be used for application management.

56
00:04:23.630 --> 00:04:28.260
To summarize, what our imaginary game
application needs from big data system.

57
00:04:29.830 --> 00:04:35.021
First of all, there needs to be a way
to use common big data operations

58
00:04:35.021 --> 00:04:39.778
to manage and split large volumes
of events data streaming in.

59
00:04:39.778 --> 00:04:43.740
This means the partitioning and
placement of data in and

60
00:04:43.740 --> 00:04:49.400
out of computer memory along with a model
to synchronize the datasets later on.

61
00:04:50.960 --> 00:04:54.410
The access to data should
be achieved in a fast way.

62
00:04:56.060 --> 00:04:58.710
The game developers need
to be able to deploy

63
00:04:58.710 --> 00:05:03.630
many event processing jobs to
distributed processing nodes at once.

64
00:05:03.630 --> 00:05:07.350
And these are potentially the data
nodes we move the computations to.

65
00:05:08.850 --> 00:05:13.430
It should also enable
reliability of the computing and

66
00:05:13.430 --> 00:05:16.190
enable fault tolerance from failures.

67
00:05:16.190 --> 00:05:19.998
This means enabling
programmable replications and

68
00:05:19.998 --> 00:05:22.685
recovery of event data when needed.

69
00:05:22.685 --> 00:05:24.119
It should be easily

70
00:05:24.119 --> 00:05:29.238
scalable to a distributed set of
nodes where the data gets produced.

71
00:05:29.238 --> 00:05:32.639
It should also enable scaling out.

72
00:05:32.639 --> 00:05:38.963
Scaling out is simply adding new
resources like distributed computers to

73
00:05:38.963 --> 00:05:44.681
process more or faster data at
scale without losing performance.

74
00:05:44.681 --> 00:05:47.360
There are many data
types in an online game.

75
00:05:48.360 --> 00:05:51.440
Although, we talked about
time click events and

76
00:05:51.440 --> 00:05:56.390
scores, it would be easy to imagine
there are graphs of players,

77
00:05:56.390 --> 00:05:59.790
text-based chats, and
images that need to be processed.

78
00:06:01.390 --> 00:06:04.700
Our big data system should
enable processing of such

79
00:06:04.700 --> 00:06:09.500
a mixed variety of data and
potentially optimize handling of

80
00:06:09.500 --> 00:06:12.870
each type separately as well
as together when needed.

81
00:06:15.180 --> 00:06:19.750
In addition, our system should
have been able both streaming and

82
00:06:19.750 --> 00:06:25.140
batch processing, enabling all
the processing to be debuggable and

83
00:06:25.140 --> 00:06:27.950
extensible with minimal effort.

84
00:06:27.950 --> 00:06:32.110
That means being able to handle
operations at small chunks of data

85
00:06:32.110 --> 00:06:36.450
streams with minimal delay,
that is what we call low latency.

86
00:06:37.810 --> 00:06:44.050
While at the same time handle processing
of potentially all available data

87
00:06:44.050 --> 00:06:49.370
in batch form and
all through the same system architecture.

88
00:06:51.160 --> 00:06:56.330
Latency is a word that we use and
hear a lot in big data processing,

89
00:06:57.370 --> 00:07:02.340
here we refer to how fast the data
is being processed, or simply

90
00:07:02.340 --> 00:07:09.400
the difference between production or event
time and processing time of a data entry.

91
00:07:09.400 --> 00:07:13.560
In other words, latency is quantification

92
00:07:13.560 --> 00:07:17.330
of the delay in the processing of
the streaming data in the system.

93
00:07:19.400 --> 00:07:22.700
While some big data
systems are good at it.

94
00:07:22.700 --> 00:07:27.220
Hadoop for instance is not a great choice
for operations that require low latency.

95
00:07:29.540 --> 00:07:32.962
Let's finish by remembering
the real reasons for

96
00:07:32.962 --> 00:07:36.310
all these requirements
of big data processing.

97
00:07:36.310 --> 00:07:41.559
Making a different from processing
in a traditional data architecture.

98
00:07:41.559 --> 00:07:47.148
Big data has varying volume and
velocity requiring the dynamic and

99
00:07:47.148 --> 00:07:50.575
scalable batch and stream processing.

100
00:07:50.575 --> 00:07:55.048
Big data has a variety requiring
management of data in many

101
00:07:55.048 --> 00:07:59.626
different data systems and
integration of it all at scale.
WEBVTT

1
00:00:01.250 --> 00:00:03.485
Next, we'll describe
aggregation functions.

2
00:00:04.760 --> 00:00:07.190
We have seen the first query before.

3
00:00:07.190 --> 00:00:11.460
Select count(*) simply
translates to a count function.

4
00:00:12.710 --> 00:00:18.351
Now we could also say
db.Drinkers.find.count.

5
00:00:18.351 --> 00:00:20.880
But using count directly
is more straightforward.

6
00:00:23.190 --> 00:00:27.720
Now, let's ask to count the number
of unique addresses for drinkers.

7
00:00:29.160 --> 00:00:32.740
So, we don't care what the address is.

8
00:00:32.740 --> 00:00:34.210
We just care if it exists.

9
00:00:35.660 --> 00:00:41.605
This is accomplished through
the $exists:true expression.

10
00:00:41.605 --> 00:00:45.590
Thus, if an address exists for
a drinker, it will be counted.

11
00:00:47.740 --> 00:00:52.190
Another area where we need to count is
when we have an array valued attribute,

12
00:00:52.190 --> 00:00:53.040
like places.

13
00:00:54.580 --> 00:00:59.200
If we just want the number
of elements in the raw list,

14
00:00:59.200 --> 00:01:05.267
we'll write db.country.findplaces.length
and we'll get six.

15
00:01:05.267 --> 00:01:10.538
However, if we want distinct values, we'll
use distinct instead of find and then

16
00:01:10.538 --> 00:01:15.531
use the length for counting the number
of distinct elements, in this case 4.

17
00:01:17.927 --> 00:01:24.178
Now, MongoDB uses an internal machinery
called the aggregation framework,

18
00:01:24.178 --> 00:01:29.208
which is modeled on the concept
of data processing pipelines.

19
00:01:29.208 --> 00:01:34.840
That means documents enter
a multi-stage pipeline which transforms

20
00:01:34.840 --> 00:01:40.286
the documents at each stage until
it becomes an aggregated result.

21
00:01:40.286 --> 00:01:44.078
Now we have seeing a similar mechanism for
relational data.

22
00:01:44.078 --> 00:01:48.440
The aggregation pipelines starts
by using the aggregate primitive.

23
00:01:49.680 --> 00:01:55.480
The most basic pipeline stages provides
filters that operate like queries and

24
00:01:55.480 --> 00:01:59.130
the document transformations that
modify the form of the output document.

25
00:02:00.310 --> 00:02:06.890
The primary filter operation is $match,
which is followed by a query condition.

26
00:02:06.890 --> 00:02:10.401
In this case, status is A.

27
00:02:10.401 --> 00:02:14.652
And expectedly, the $match operation
produces a smaller number of documents

28
00:02:14.652 --> 00:02:16.500
to be processed at the next stage.

29
00:02:17.960 --> 00:02:22.730
This is usually followed
by the $group operation.

30
00:02:22.730 --> 00:02:26.040
Now this operation needs to know which
attributes should be grouped together.

31
00:02:27.130 --> 00:02:31.258
In the example here cust_id
is the grouping attribute so

32
00:02:31.258 --> 00:02:35.750
it is passed as a parameter
to the $group function.

33
00:02:35.750 --> 00:02:38.143
Now notice the syntax.

34
00:02:38.143 --> 00:02:44.649
_id:$cust_id says that the grouped
data will have an _id attribute,

35
00:02:44.649 --> 00:02:48.573
and its value will be
picked from the cust_id

36
00:02:48.573 --> 00:02:53.128
attribute from the previous
stage of computation.

37
00:02:53.128 --> 00:02:57.466
Thus, the $ before the cust_id
is telling the system that

38
00:02:57.466 --> 00:03:01.990
cust_id is a known variable in
the system and not a constant.

39
00:03:01.990 --> 00:03:04.639
The $group operation also needs a reducer,

40
00:03:04.639 --> 00:03:10.250
which is a function that operates on an
activity to produce an aggregate result.

41
00:03:10.250 --> 00:03:13.590
In this case, the reduce function is sum,

42
00:03:14.980 --> 00:03:17.720
which operates on the amount
attribute from the previous stage.

43
00:03:18.790 --> 00:03:23.560
Like $cust_id, we use $amount to
indicate that it's a variable.

44
00:03:25.070 --> 00:03:27.590
As we saw in the relational case,

45
00:03:27.590 --> 00:03:32.580
data can be partitioned into chunks on
the same machine or on different machines.

46
00:03:32.580 --> 00:03:35.320
These chunks are called chards.

47
00:03:35.320 --> 00:03:42.284
The aggregation pipeline of MongoDB
can operate on a charded collection.

48
00:03:42.284 --> 00:03:47.156
The grouping operation in MongoDB
can accept multiple attributes like

49
00:03:47.156 --> 00:03:48.650
the four shown here.

50
00:03:48.650 --> 00:03:53.915
Also shown is a post grouping directive
to sort on the basis of two attributes.

51
00:03:55.380 --> 00:04:00.150
The first is a computed count
variable in ascending order.

52
00:04:00.150 --> 00:04:03.728
So the one designates the ascending order.

53
00:04:03.728 --> 00:04:05.870
The next sorting attribute is secondary.

54
00:04:05.870 --> 00:04:08.966
That means if two groups have
the same value for count,

55
00:04:08.966 --> 00:04:13.110
then they'll be further sorted
based on the category value.

56
00:04:13.110 --> 00:04:18.860
But this time the order is descending
because of the -1 directive

57
00:04:22.207 --> 00:04:27.280
In course two we have seen Solar,
a text search engine from Apache.

58
00:04:28.410 --> 00:04:32.830
MongoDB has a built in text search engine
which can be invoked through the same

59
00:04:32.830 --> 00:04:34.360
aggregation framework we saw before.

60
00:04:35.360 --> 00:04:40.006
Imagine that MongoDB documents in this
case are really text documents placed in

61
00:04:40.006 --> 00:04:41.730
a collection called articles.

62
00:04:43.270 --> 00:04:48.560
In this case, the $match directive of
the aggregate function must be told

63
00:04:48.560 --> 00:04:51.955
it's going to perform a text
function on the article's corpus.

64
00:04:53.320 --> 00:04:55.884
The actual text function is $search.

65
00:04:55.884 --> 00:05:01.080
We set search terms like "Hillary
Democrat" such that having

66
00:05:01.080 --> 00:05:07.210
either term in a document will
satisfy the search requirement.

67
00:05:07.210 --> 00:05:11.260
As is the case of any text engine,

68
00:05:11.260 --> 00:05:15.410
the results of any search returns
a list of documents, each with a score.

69
00:05:16.600 --> 00:05:21.780
The next task is to tell MongDB to
sort the results based on textScore.

70
00:05:23.050 --> 00:05:24.290
What's the $meta here?

71
00:05:25.420 --> 00:05:29.532
Meta stands for metadata,
that is additional information.

72
00:05:29.532 --> 00:05:33.860
Remember that the aggregation
operations are executed in a pipeline.

73
00:05:35.090 --> 00:05:38.705
Any step in the pipeline can
produce some extra data, or

74
00:05:38.705 --> 00:05:41.780
metadata, for each processed document.

75
00:05:41.780 --> 00:05:45.470
In this example, the metadata
produced by the search function

76
00:05:45.470 --> 00:05:48.670
is a computed attribute called textScore.

77
00:05:48.670 --> 00:05:54.627
So this directive tells the system to pick
up this specific metadata attribute and

78
00:05:54.627 --> 00:05:59.828
use it to populate the score attribute
which would be used for sorting.

79
00:05:59.828 --> 00:06:04.501
Finally, the $project class
does exactly what is expected.

80
00:06:04.501 --> 00:06:09.006
It tells the system to output only
the title of each document and

81
00:06:09.006 --> 00:06:10.284
suppress its id.

82
00:06:14.760 --> 00:06:19.120
The last item in our
discussion of MongoDB is join.

83
00:06:19.120 --> 00:06:22.820
We have seen that join is a vital
operation for data management operations.

84
00:06:25.010 --> 00:06:32.062
Interestingly, MongoDB introduced this
equivalent of join only in version 3.2.

85
00:06:32.062 --> 00:06:36.320
So, the joining in MongoDB also
happens in the aggregation framework.

86
00:06:37.740 --> 00:06:40.420
There are a few ways of
expressing joins in MongoDB.

87
00:06:41.470 --> 00:06:46.980
We show one here that explicitly performs
a join to a function called look up.

88
00:06:48.690 --> 00:06:51.445
We use an example form
the MongoDB documentation.

89
00:06:51.445 --> 00:06:58.380
Now here are two document collections,
order and inventory.

90
00:06:58.380 --> 00:07:03.600
Notice that the item key in
orders has values abc, jkl, etc.

91
00:07:04.830 --> 00:07:10.100
And the sku key in the inventory has
comparable values abc, def, etc.

92
00:07:10.100 --> 00:07:12.270
So these two are joinable by value.

93
00:07:13.550 --> 00:07:19.167
The way to specify this join,
one can use this query.

94
00:07:19.167 --> 00:07:24.124
The db.orders.aggregate
declaration states that orders is

95
00:07:24.124 --> 00:07:27.311
sort of the home, or local collection.

96
00:07:27.311 --> 00:07:32.592
Now in the aggregate, the function
$lookup needs to know what to look up for

97
00:07:32.592 --> 00:07:34.420
each document in orders.

98
00:07:36.050 --> 00:07:41.010
The from attribute specifies the name
of the collection as inventory.

99
00:07:42.590 --> 00:07:44.410
The next two parameters are the local and

100
00:07:44.410 --> 00:07:48.591
foreign matching keys,
which are item and sku, respectively.

101
00:07:49.976 --> 00:07:54.760
The last item, as:,
is a construction part of

102
00:07:54.760 --> 00:07:59.380
the join operation which says how to
structure the match items into the result.

103
00:08:00.780 --> 00:08:03.770
Now before we show you the results,
let's see what should match.

104
00:08:05.740 --> 00:08:10.020
The abc item in order should
match the abc in sku.

105
00:08:11.120 --> 00:08:16.050
Similarly, the jkl item
should match the jkl in sku.

106
00:08:17.260 --> 00:08:19.540
Okay, but there is one more twist.

107
00:08:21.020 --> 00:08:22.920
Here is the actual result.

108
00:08:22.920 --> 00:08:27.090
The first two records show
exactly what we expect.

109
00:08:27.090 --> 00:08:29.760
There is a new field called
inventory-docs in the batching record.

110
00:08:31.640 --> 00:08:34.580
The third record however,
shows something interesting.

111
00:08:36.170 --> 00:08:40.250
Inventory has two records shown here,
what do they match?

112
00:08:41.670 --> 00:08:46.890
Now they match the empty
document in orders because

113
00:08:46.890 --> 00:08:50.700
orders has a document
whose item field is null.

114
00:08:51.980 --> 00:08:57.355
So it matches documents and
inventory where the sku item is also null,

115
00:08:57.355 --> 00:09:02.290
explicitly as in document 5, or
implicitly as in document 6.

116
00:09:04.801 --> 00:09:08.948
This concludes our discussion of
queries in the context of MongoDB.
WEBVTT

1
00:00:00.720 --> 00:00:05.150
In this hands on activity, we'll be
using Pandas to read CSV files and

2
00:00:05.150 --> 00:00:07.300
perform various queries on them.

3
00:00:07.300 --> 00:00:10.070
Pandas is a data analysis library for
Python.

4
00:00:12.140 --> 00:00:14.730
First, we'll create a new
Jupyter Python Notebook.

5
00:00:16.070 --> 00:00:20.060
Next, we will use Pandas to read
a CSV file into a DataFrame.

6
00:00:21.375 --> 00:00:25.490
We'll then view the contents of the
DataFrame and see how to filter rows and

7
00:00:25.490 --> 00:00:26.130
columns of it.

8
00:00:27.600 --> 00:00:31.410
Next, we will perform average and
sum operations on the DataFrame.

9
00:00:31.410 --> 00:00:36.760
And finally, show how to merge two
DataFrames by joining on a single column.

10
00:00:39.160 --> 00:00:39.660
Let's begin.

11
00:00:41.420 --> 00:00:44.370
We'll start by creating
a new iPython notebook.

12
00:00:44.370 --> 00:00:48.570
Clicking on New and
selecting Python 3 under notebooks.

13
00:00:51.900 --> 00:00:56.350
First, we'll import the Pandas
library by writing import pandas.

14
00:00:58.010 --> 00:01:00.710
Remember that in iPython notebooks,
to run a command,

15
00:01:00.710 --> 00:01:02.980
we hold down the shift key and hit Enter.

16
00:01:06.599 --> 00:01:12.395
Next, let's read buyclicks.csv
into a Pandas DataFrame.

17
00:01:12.395 --> 00:01:14.730
We'll put it in a variable
called buyclicksDF.

18
00:01:18.135 --> 00:01:22.751
We'll read it using pandas.read_csv,

19
00:01:22.751 --> 00:01:27.630
and we'll read the buy-clicks.csv file.

20
00:01:30.966 --> 00:01:34.870
We can see the contents of the file by
just running the variable by itself.

21
00:01:39.948 --> 00:01:44.398
And notice that the file has many rows and
then iPython truncates this,

22
00:01:44.398 --> 00:01:45.740
the dot, dot, dot.

23
00:01:49.227 --> 00:01:52.027
We can see only the top five
rows by calling .head(5).

24
00:01:53.417 --> 00:01:58.343
Next, let's look at only two

25
00:01:58.343 --> 00:02:04.500
columns in the buyclicks data.

26
00:02:04.500 --> 00:02:06.310
Let's look at price and user ID.

27
00:02:07.330 --> 00:02:13.680
We can do these by first entering
buyclicks DataFrame in the same text for

28
00:02:13.680 --> 00:02:19.040
specifying only certain columns to
show is open bracket open bracket and

29
00:02:19.040 --> 00:02:20.950
then the name of the columns
you want to view.

30
00:02:20.950 --> 00:02:25.700
So, want your price and
user ID, and again,

31
00:02:25.700 --> 00:02:31.395
we only want to see the first five rows,
so we'll do .head(5).

32
00:02:35.016 --> 00:02:39.350
Now, let's query the buyclicks data for
only the prices less than 3.

33
00:02:39.350 --> 00:02:44.494
First, we'll enter buyclicksDF,
One square bracket,

34
00:02:44.494 --> 00:02:49.007
to filter our particular column,
we enter buyclicksDF and then column name.

35
00:02:50.440 --> 00:02:54.440
Now, we specify the limit of
the query by entering <3.

36
00:03:02.378 --> 00:03:06.080
This shows first five rows where
the price is less than three.

37
00:03:08.409 --> 00:03:12.205
We can also perform aggregate
operations on Panda's DataFrames.

38
00:03:13.650 --> 00:03:21.673
We can sum all the price data by
entering buyclicksDF['price'].sum.

39
00:03:25.760 --> 00:03:29.470
Another aggregate operation we can
perform is looking at the average.

40
00:03:29.470 --> 00:03:31.820
Let's look at the average for price.

41
00:03:31.820 --> 00:03:33.540
The function is called mean.

42
00:03:34.720 --> 00:03:40.782
So, once your buyclicksDF ['price'].mean.

43
00:03:45.341 --> 00:03:48.460
Can also join two DataFrames
on a single column.

44
00:03:49.870 --> 00:03:54.620
First, let's read in another
CSV into a different DataFrame.

45
00:03:54.620 --> 00:03:57.700
We'll read in adclicks.csv.

46
00:03:57.700 --> 00:04:04.723
So we'll says adclicksDF
= pandas.read_csv,

47
00:04:04.723 --> 00:04:10.286
we'll say ('ad-clicks.csv').

48
00:04:16.538 --> 00:04:20.870
To verify that we read this data
successfully, let's look at the contents.

49
00:04:29.077 --> 00:04:32.063
Now, let's combine
the buyclicks DataFrame and

50
00:04:32.063 --> 00:04:34.700
the adclicks data frame
on the user ID call.

51
00:04:36.690 --> 00:04:39.480
We'll put the result in the new
DataFrame called mergeDF.

52
00:04:39.480 --> 00:04:46.400
So we'll say mergeDF = adclicksDF.merge

53
00:04:50.465 --> 00:04:54.820
Then, we need to say which
DataFrame we're merging with,

54
00:04:54.820 --> 00:04:59.590
buyclicksDF, and
the column that we're merging on.

55
00:04:59.590 --> 00:05:02.080
So we'll say on='userid'.

56
00:05:08.461 --> 00:05:11.681
Finally, we can look at the contents
with this merged DataFrame.
WEBVTT

1
00:00:01.510 --> 00:00:05.380
So now from MongoDB we will go to
Aerospike which is a key value store.

2
00:00:07.030 --> 00:00:09.120
Key value stores typically offer an API.

3
00:00:10.480 --> 00:00:14.630
That is the way to access data using a
programming language like Python or Java.

4
00:00:15.850 --> 00:00:19.620
We will take a very brief look
at Aerospike, which offers both

5
00:00:19.620 --> 00:00:23.550
a programmatic access and
a limited amount of query access to data.

6
00:00:25.550 --> 00:00:27.910
The data model of Aerospike
is illustrated here.

7
00:00:29.310 --> 00:00:34.960
Data are organized in lean spaces which
can be in memory or on flash disks.

8
00:00:36.410 --> 00:00:38.840
Name spaces are top level data containers.

9
00:00:39.980 --> 00:00:45.080
The way you collect data in name spaces
relates to how data is stored and managed.

10
00:00:46.270 --> 00:00:50.395
So name space contains records,
indexes, and policies.

11
00:00:50.395 --> 00:00:55.130
Now policies dictate name space
behavior like how data is stored,

12
00:00:55.130 --> 00:00:57.790
whether it's on RAM or disk, or

13
00:00:57.790 --> 00:01:02.150
how how many replicas exist for
a record, and when records expire.

14
00:01:04.840 --> 00:01:08.606
A name space can contain sets,
you can think of them as tables.

15
00:01:08.606 --> 00:01:11.470
So here there are two sets,
people and places,

16
00:01:11.470 --> 00:01:15.140
and a set of records
which are not in any set.

17
00:01:16.750 --> 00:01:20.620
Within a record,
data is stored in one or many bins.

18
00:01:21.980 --> 00:01:25.490
Bins consist of a name and a value.

19
00:01:27.820 --> 00:01:31.970
The example written here is in Java, and

20
00:01:31.970 --> 00:01:34.230
you don't have to know Java to
follow the main point here.

21
00:01:36.068 --> 00:01:40.470
Here we are creating indexes on key
value data that's handled by Aerospike.

22
00:01:41.780 --> 00:01:43.500
This data set comes from Twitter.

23
00:01:44.990 --> 00:01:50.310
Each field of a tweet is extracted and
put into Aerospike as a key value pair.

24
00:01:50.310 --> 00:01:55.420
So we declare the namespace to be

25
00:01:55.420 --> 00:01:59.880
example, and the record set to be tweet.

26
00:02:01.510 --> 00:02:07.460
The name of the index to be TestIndex,
and the name of the bin as user name.

27
00:02:09.490 --> 00:02:14.330
Since this index is stored
on disk an SQL-like command,

28
00:02:14.330 --> 00:02:18.638
like SHOW INDEX, shows the content
of the index as you can see here.

29
00:02:18.638 --> 00:02:23.220
This routine shows

30
00:02:23.220 --> 00:02:27.419
how data can be inserted into
Aerospike programmatically.

31
00:02:27.419 --> 00:02:31.500
Again, the goal is to point out a few
salient aspects of data insertion

32
00:02:31.500 --> 00:02:33.090
regardless of the syntax of the language.

33
00:02:33.090 --> 00:02:38.120
Now since this is a key value store,
one first needs to define the key.

34
00:02:40.220 --> 00:02:45.414
This line here says that the key in
the namespace call example, and set call

35
00:02:45.414 --> 00:02:50.699
tweet, is the value of the function getId,
which returns the ID of a tweet.

36
00:02:53.288 --> 00:02:57.028
When the data is populated,
we are essentially creating bins.

37
00:02:57.028 --> 00:03:01.500
Here, user name is the attribute and

38
00:03:01.500 --> 00:03:04.290
the screen name obtained
from the tweet is the value.

39
00:03:05.510 --> 00:03:09.440
The actual insertion
happens here in the client.

40
00:03:11.200 --> 00:03:15.574
The client.put statement,
where we need to mention the key and

41
00:03:15.574 --> 00:03:18.510
the bins we have just created.

42
00:03:18.510 --> 00:03:21.260
Now why are we inserting
two bins at a time?

43
00:03:21.260 --> 00:03:23.140
Two bins with two ids and user name?

44
00:03:24.150 --> 00:03:27.180
This is an idiosyncrasy
of the Aerospike client.

45
00:03:29.810 --> 00:03:34.190
After data is inserted one can
create other data using AQL

46
00:03:34.190 --> 00:03:35.450
which is very much like SQL.

47
00:03:36.870 --> 00:03:41.920
This screenshot shows a part of
the output of a simple select star query.

48
00:03:44.680 --> 00:03:49.100
Now in your hands-on exercise, you'll be
able to play with the Aerospike data.

49
00:03:50.520 --> 00:03:56.740
This is just a screenshot showing
the basic query syntax of AQL,

50
00:03:56.740 --> 00:04:00.110
that is Aerospike Query Language,
and a few examples.

51
00:04:01.370 --> 00:04:04.960
The last two lines show a couple of
interesting features of the language.

52
00:04:06.090 --> 00:04:12.200
The operation between 0 and 99,
is a nicer way of stating a range query,

53
00:04:12.200 --> 00:04:14.850
which gives a lower and
upper limits on a variable.

54
00:04:16.030 --> 00:04:20.550
The last line shows the operation cost,

55
00:04:20.550 --> 00:04:24.670
which transforms one type
of data to another type.

56
00:04:24.670 --> 00:04:29.047
Here it transforms coordinates,
that is latitude and longitude data,

57
00:04:29.047 --> 00:04:33.784
to a JSON format called GeoJSON which is
designed to represent geographic data

58
00:04:33.784 --> 00:04:35.092
in a JSON structure.

59
00:04:38.762 --> 00:04:42.421
We will finish our coverage of queries
with a quick reference to an advanced

60
00:04:42.421 --> 00:04:44.870
topic, which is beyond
the scope of this course.

61
00:04:46.068 --> 00:04:50.780
Now you have seen in prior courses that
streaming data is complex to process

62
00:04:50.780 --> 00:04:53.310
because a stream is infinite in nature.

63
00:04:54.480 --> 00:04:57.640
Now does this have any impact on
query languages and evaluation?

64
00:04:58.790 --> 00:05:01.840
The answer is that it absolutely does.

65
00:05:01.840 --> 00:05:03.590
We'll mention one such impact here.

66
00:05:05.150 --> 00:05:08.700
This shows a pictorial
depiction of streaming data.

67
00:05:09.890 --> 00:05:14.170
The data is segmented into five pieces,
shown in the white boxes in the upper row.

68
00:05:15.620 --> 00:05:20.280
These can be gathered, for example every
few seconds, or every 200 data objects.

69
00:05:21.980 --> 00:05:24.970
The query defines a window to select

70
00:05:24.970 --> 00:05:27.770
key of these data objects
as a unit of processing.

71
00:05:27.770 --> 00:05:28.860
Here case three.

72
00:05:30.000 --> 00:05:34.120
So three units of data
are picked up in a window unit.

73
00:05:34.120 --> 00:05:37.020
To get the next window it
is moved by two units,

74
00:05:38.030 --> 00:05:40.280
this is called a slide of the window.

75
00:05:41.970 --> 00:05:46.030
Since the window size is three and
the slide is two,

76
00:05:46.030 --> 00:05:50.130
one unit of information overlaps
between the two consecutive windows.

77
00:05:51.450 --> 00:05:56.056
The lower line in this diagram shows
the initialized item, let's ignore it,

78
00:05:56.056 --> 00:06:03.980
followed by two window sets of
data records for processing.

79
00:06:05.090 --> 00:06:08.670
Thus the query language therefore,
will have to specify a query

80
00:06:08.670 --> 00:06:14.650
that's an SQL query, over a window,
which is also specified in the query.

81
00:06:14.650 --> 00:06:18.980
Now in a traffic data stream example,
the SQL statement might look like this.

82
00:06:20.330 --> 00:06:23.490
Where the window size is 30 second, and

83
00:06:23.490 --> 00:06:26.520
the slide is the same size as window
giving output every 30 seconds.

84
00:06:27.760 --> 00:06:32.650
So streaming data results in changes
in both the query language and

85
00:06:32.650 --> 00:06:34.560
the way queries are processed.
WEBVTT

1
00:00:00.008 --> 00:00:04.900
In this hands-on activity,
we will be querying documents in MongoDB.

2
00:00:06.020 --> 00:00:10.480
First, we will start a MongoDB server and
then run the MongoDB Shell.

3
00:00:10.480 --> 00:00:15.730
We will see how to show the databases and
collections in the MongoDB Server.

4
00:00:15.730 --> 00:00:19.520
We will look at an example
document in the database and

5
00:00:19.520 --> 00:00:21.529
see how to find distinct values for
a field.

6
00:00:23.360 --> 00:00:26.540
Next, we will search for
specific field values and

7
00:00:26.540 --> 00:00:28.620
see how to filter fields
returned in a query.

8
00:00:30.530 --> 00:00:34.070
Finally, we will search using
regular expressions and operators.

9
00:00:36.903 --> 00:00:41.361
Let's begin,
first we'll start the MongoDB server.

10
00:00:41.361 --> 00:00:46.830
Open the terminal window by clicking on
the terminal icon, the top of the toolbar.

11
00:00:46.830 --> 00:00:53.680
We'll cd into
Downloads/big-data-3-mongodb.

12
00:00:53.680 --> 00:00:59.495
We'll start the MongoDB server by running

13
00:00:59.495 --> 00:01:05.675
./mongodb/bin/mongod --dbpath db.

14
00:01:05.675 --> 00:01:08.930
The arguments dbpath db

15
00:01:08.930 --> 00:01:13.330
specified that the database on
MongoDB is in the directory named db.

16
00:01:15.260 --> 00:01:16.780
Run this to start the server.

17
00:01:20.530 --> 00:01:24.413
Next, we'll start the MongoDB Shell so
that we can perform queries.

18
00:01:24.413 --> 00:01:29.416
We'll open another terminal window.

19
00:01:29.416 --> 00:01:36.417
Again, cd into
Downloads/big-data-3-mongodb.

20
00:01:36.417 --> 00:01:41.955
We'll start the shell by
running ./mongodb/bin/mongo.

21
00:01:49.734 --> 00:01:55.237
We can see what databases are available in
the MongoDB server by running the command,

22
00:01:55.237 --> 00:01:55.933
show dbs.

23
00:01:59.521 --> 00:02:02.680
We've created the sample database
with JSON data from Twitter.

24
00:02:04.400 --> 00:02:08.400
We can use the use command
to change to this database.

25
00:02:08.400 --> 00:02:10.245
We'll run use sample.

26
00:02:13.078 --> 00:02:16.828
We can see the collections in this
database by running, show collections.

27
00:02:19.858 --> 00:02:22.656
There's only one collection called users.

28
00:02:22.656 --> 00:02:26.987
So, all of the queries will be
using the users collection.

29
00:02:26.987 --> 00:02:31.945
Let's count how many documents there
are in the users collection by writing

30
00:02:31.945 --> 00:02:33.349
db.users.count.

31
00:02:38.225 --> 00:02:43.241
We can look at one of these documents
by writing db.users.findOne.

32
00:02:51.078 --> 00:02:55.443
This document contains
Jason from a Twitter tweet.

33
00:02:55.443 --> 00:02:58.740
You can see the field names
here with their values.

34
00:02:58.740 --> 00:03:02.380
There are also nested fields
under the user field.

35
00:03:02.380 --> 00:03:04.410
And each of these fields also has values.

36
00:03:05.730 --> 00:03:08.870
You can find the distinct values for

37
00:03:08.870 --> 00:03:12.320
particular field,
by using the distinct command.

38
00:03:12.320 --> 00:03:15.058
Let's find the distinct values for
the username field.

39
00:03:15.058 --> 00:03:21.234
We'll write
db.users.distinct("user_name").

40
00:03:30.324 --> 00:03:35.193
Next, let's find all the documents in
this collection where the field username

41
00:03:35.193 --> 00:03:36.990
matches the specific value.

42
00:03:38.050 --> 00:03:41.360
The value we'll search for
is ActionSportsJax.

43
00:03:42.730 --> 00:03:46.851
We'll run the command db.users.find.

44
00:03:46.851 --> 00:03:49.980
And the field name is username, and

45
00:03:49.980 --> 00:03:54.688
the value we're searching for
is ActionSportsJax.

46
00:04:04.405 --> 00:04:07.050
Results of this query is
compressed all in one line.

47
00:04:08.650 --> 00:04:11.401
If we append the .pretty
to the previous query.

48
00:04:11.401 --> 00:04:13.874
We can see the formatted output.

49
00:04:13.874 --> 00:04:16.776
So we'll run the same
command with append .pretty.

50
00:04:23.289 --> 00:04:25.980
We can filter the fields
returned from the queries.

51
00:04:27.560 --> 00:04:31.709
Let's perform the same query again but
only show the tweet_id field.

52
00:04:33.200 --> 00:04:36.090
We can do this by adding a second
argument to the find command.

53
00:04:37.440 --> 00:04:42.648
So we'll run the same command again,

54
00:04:42.648 --> 00:04:49.827
but add a second argument,
saying tweet_ID: 1.

55
00:04:49.827 --> 00:04:55.748
The underscore ID field is a primary
key used in all documents in MongoDB.

56
00:04:55.748 --> 00:04:58.280
You can turn this off by adding
another field to our filter.

57
00:05:00.130 --> 00:05:03.287
We'll run the same command again,
but turn of _ID.

58
00:05:11.820 --> 00:05:16.550
Next, we use the regular expression search
to find strings containing the document.

59
00:05:17.680 --> 00:05:21.640
For example, if we want to find all
the tweets containing the text FIFA,

60
00:05:21.640 --> 00:05:25.171
we can run db.users.find tweet_text FIFA.

61
00:05:32.170 --> 00:05:35.850
There are no results in this query
because this query is searching for

62
00:05:35.850 --> 00:05:37.580
tweet text equals FIFA.

63
00:05:39.030 --> 00:05:43.590
That is the entire contents and
the value of tweet_text must be FIFA.

64
00:05:44.720 --> 00:05:46.110
Instead, if we want to look for

65
00:05:46.110 --> 00:05:51.230
FIFA anywhere in the tweet_text,
we can do a regular expression search.

66
00:05:51.230 --> 00:05:55.750
To do this,
replace the double quotes with slashes.

67
00:05:55.750 --> 00:05:57.850
So we'll run the same command again.

68
00:05:57.850 --> 00:06:03.573
But replacing double quotes with slashes.

69
00:06:03.573 --> 00:06:07.423
We can count how many documents are
returned by this query by running the same

70
00:06:07.423 --> 00:06:09.540
command again, but appending .count.

71
00:06:13.325 --> 00:06:17.948
We can also search for documents in
MongoDB where the field values are greater

72
00:06:17.948 --> 00:06:20.020
than or less than a certain value.

73
00:06:21.170 --> 00:06:26.162
For example, lets find tweet
mention count greater than six.

74
00:06:26.162 --> 00:06:32.977
We'll run db.users.find, and
the field name is tweet_mentioned_count.

75
00:06:35.670 --> 00:06:41.265
And we want to look for the value
where this field is greater than six.

76
00:06:41.265 --> 00:06:47.940
So I'll enter a { $gt : 6 }.
WEBVTT

1
00:00:01.500 --> 00:00:06.870
In a prior course we looked at JSON as
an example of semi-structured data,

2
00:00:06.870 --> 00:00:10.020
and we demonstrated that JSON
data can be thought of as a tree.

3
00:00:11.200 --> 00:00:14.180
In this course,
we'll focus on querying JSON data.

4
00:00:15.330 --> 00:00:20.340
Before we start, lets review
the details of the JSON structure, and

5
00:00:20.340 --> 00:00:24.180
get an initial sense of how
to query this form of data.

6
00:00:25.720 --> 00:00:28.590
Let's consider a simple
JSON collection and

7
00:00:28.590 --> 00:00:32.330
look at the structures,
substructures actually, it's composed of.

8
00:00:34.160 --> 00:00:37.990
The atomic element in the structure
is a key value pair, for example,

9
00:00:37.990 --> 00:00:43.520
name is the key and sue is the value,
in this case, an atomic string value.

10
00:00:45.560 --> 00:00:47.178
To query a key value pair,

11
00:00:47.178 --> 00:00:52.123
we should be able perform one basic
operation given the key, return the value.

12
00:00:54.140 --> 00:01:00.830
Now, the value can also be an array,
an array is a list.

13
00:01:02.060 --> 00:01:06.940
So the query operations on it can either
be on its position in the list or

14
00:01:06.940 --> 00:01:08.370
on the value.

15
00:01:08.370 --> 00:01:12.980
Thus, we should be able to ask for the
second element of the array called badges.

16
00:01:12.980 --> 00:01:17.180
Or we should be able to seek objects
of which the key called badges

17
00:01:17.180 --> 00:01:18.270
has a value of blue.

18
00:01:20.130 --> 00:01:25.360
Notice here that the document
collection here is itself an array,

19
00:01:25.360 --> 00:01:29.130
within square brackets and
it's just two elements in it.

20
00:01:29.130 --> 00:01:33.664
The top level array does not have a key,
by default it's called db.

21
00:01:36.820 --> 00:01:41.135
These key value peers
are structured as tuples,

22
00:01:41.135 --> 00:01:44.607
often with a name In the snippet shown,

23
00:01:44.607 --> 00:01:48.644
favorites has a tuple
of two key value pairs.

24
00:01:48.644 --> 00:01:53.530
Now, tuples can be thought of as
relational records, as the operations

25
00:01:53.530 --> 00:01:57.800
would include, projection of an attribute,
and selection over a set of tables.

26
00:02:00.330 --> 00:02:03.060
On the other hand,
the area called 'points',

27
00:02:03.060 --> 00:02:07.180
has two tuples, these two tuples named.

28
00:02:07.180 --> 00:02:10.880
As you will see, we'll address
these tuples by their positions.

29
00:02:12.570 --> 00:02:16.000
Finally, this one has nesting,

30
00:02:16.000 --> 00:02:20.110
that means a mini structure can be
embedded within another structure.

31
00:02:21.250 --> 00:02:24.220
So we need operations
that will let us navigate

32
00:02:24.220 --> 00:02:27.350
from one structure to any of
it's embedded structures.

33
00:02:30.030 --> 00:02:35.180
Now just like a basic SQL query states,
which parts of which records from one or

34
00:02:35.180 --> 00:02:39.840
more people should be reported,
a MongoDB query states

35
00:02:39.840 --> 00:02:43.700
which parts of which documents from
a document collection should be returned.

36
00:02:45.010 --> 00:02:51.088
The primary query is expressed as a find
function, which contains two arguments and

37
00:02:51.088 --> 00:02:56.494
an optional qualifier, there are four
things to notice in this function.

38
00:02:58.142 --> 00:03:00.773
The first is the term collection,

39
00:03:00.773 --> 00:03:05.402
this tells the system which
document collection to use, and

40
00:03:05.402 --> 00:03:11.410
therefore is roughly similar to the From
clause when restricted to one table.

41
00:03:11.410 --> 00:03:18.629
So if the name of the collection is beers,
the first part would say db.beers.find.

42
00:03:18.629 --> 00:03:23.098
The second item is a query filter
which lists all conditions that

43
00:03:23.098 --> 00:03:28.159
the retrieved documents should satisfy,
so it's like a Where clause.

44
00:03:30.170 --> 00:03:34.370
Now, if we want to return everything,
then this filter is left blank.

45
00:03:35.550 --> 00:03:39.719
Otherwise, we'll fill it in a couple
of ways shown in the next few slides.

46
00:03:41.330 --> 00:03:45.652
The third term is a projection class
which is essentially a list of variables

47
00:03:45.652 --> 00:03:47.550
that we want to see in the output.

48
00:03:49.520 --> 00:03:53.950
The fourth and last item sits
after the find function ends and

49
00:03:53.950 --> 00:03:57.370
is separated by a dot,
it's called a cursor modifier.

50
00:03:58.370 --> 00:04:02.760
The word cursor relates back
to SQL where cursor is defined

51
00:04:02.760 --> 00:04:06.120
as a block of results that is
returned to the user in one chunk.

52
00:04:07.410 --> 00:04:11.357
This becomes important when the set of
results is too large to be returned all

53
00:04:11.357 --> 00:04:14.256
together, and
the user may need to specify how much, or

54
00:04:14.256 --> 00:04:16.614
what portion of results
they actually want.

55
00:04:18.710 --> 00:04:21.776
So, we'll start out with a few queries,

56
00:04:21.776 --> 00:04:26.156
where we show how the same query
can be expressed in SQL, and

57
00:04:26.156 --> 00:04:31.220
in MongoDB The first query
wants everything from Beers.

58
00:04:31.220 --> 00:04:34.740
The SQL query is structured
on the table Beers, and

59
00:04:34.740 --> 00:04:37.660
the SELECT * asks to return all rows.

60
00:04:38.740 --> 00:04:43.230
In MongoDB the same query
is more succincted,

61
00:04:43.230 --> 00:04:45.830
since the name of the collection
is already specified in

62
00:04:45.830 --> 00:04:49.650
calling the find function,
the body of the find function is empty.

63
00:04:50.650 --> 00:04:55.020
That means there are no query conditions
and no projection clauses in it.

64
00:04:55.020 --> 00:04:59.550
The second query

65
00:04:59.550 --> 00:05:04.160
needs to return the variables beer and
price for all records.

66
00:05:05.810 --> 00:05:12.120
So the find function here needs an empty
query condition denoted by the open and

67
00:05:12.120 --> 00:05:17.860
closed brace symbols, but the projection
clauses are specifically identified.

68
00:05:17.860 --> 00:05:21.760
There is a 1 if an attribute is output and
a 0 if it is not.

69
00:05:21.760 --> 00:05:26.500
As a shortcut,
only variables with 1 are required.

70
00:05:28.000 --> 00:05:29.204
Okay, so when do you use 0?

71
00:05:30.360 --> 00:05:31.870
A common situation is the following.

72
00:05:33.160 --> 00:05:37.548
Every MongoDB document has
an identifier named _id.

73
00:05:40.139 --> 00:05:44.410
By default every query will
return the id of the document.

74
00:05:45.700 --> 00:05:49.150
If you don't want it to return
this designated attribute,

75
00:05:49.150 --> 00:05:55.970
you should explicitly say, _id:0.

76
00:05:55.970 --> 00:05:58.630
Next we will add query conditions.

77
00:05:58.630 --> 00:06:02.640
That is the equivalent of
the Where clause in SQL.

78
00:06:02.640 --> 00:06:07.490
Our query number three has the query
condition where name is equal to a value.

79
00:06:08.600 --> 00:06:15.020
In MongoDB, that equal to translate
to a variable colon value form.

80
00:06:16.070 --> 00:06:21.135
Notice the symbol used for
a string is quotes.

81
00:06:22.987 --> 00:06:25.720
Query four is more interesting for
two reasons.

82
00:06:26.770 --> 00:06:30.980
First, we see a way in which
the distinct operation is specified.

83
00:06:32.580 --> 00:06:38.670
Notice here that the primary query
function is not find any more but

84
00:06:38.670 --> 00:06:40.310
a new function called distinct.

85
00:06:41.790 --> 00:06:45.990
As we'll see later again in our slides,

86
00:06:45.990 --> 00:06:49.660
MongoDB uses a few special query
functions for some of the operations.

87
00:06:51.520 --> 00:06:55.620
So, you need to know, which function
should be used in what context,

88
00:06:55.620 --> 00:06:57.270
when you write MogoDB queries.

89
00:06:58.640 --> 00:07:03.191
Secondly, in this query,
we have a non-equality condition,

90
00:07:03.191 --> 00:07:06.153
namely, the price is greater than 15.

91
00:07:08.370 --> 00:07:12.418
This example shows MongoDB style
of using operators in a query.

92
00:07:12.418 --> 00:07:17.090
It's always variable:

93
00:07:17.090 --> 00:07:21.470
followed by MongoDB's name for the
operator, and then the comparison value.

94
00:07:22.580 --> 00:07:24.959
So where would you find
MongoDB's operators?

95
00:07:26.620 --> 00:07:28.890
Here are some of the operators
supported in MongoDB.

96
00:07:30.300 --> 00:07:33.860
These operators and others are listed
in the URL shown at the bottom.

97
00:07:35.210 --> 00:07:37.430
The operators shown here are color coded.

98
00:07:38.470 --> 00:07:41.866
The top blue set
are the comparison operators.

99
00:07:41.866 --> 00:07:47.760
We see the $gt, greater than,
operation that we used in the last slide.

100
00:07:50.100 --> 00:07:54.780
The green colored operations are array
operations which we'll see shortly.

101
00:07:54.780 --> 00:07:58.370
And the yellow operators at the bottom
are logical operations that

102
00:07:58.370 --> 00:08:02.070
combine two conditions in different ways
like the AND operation we saw in SQL.

103
00:08:02.070 --> 00:08:07.860
Now, the last operator $nor,
is interesting,

104
00:08:07.860 --> 00:08:13.110
because it is used to specify queries
when neither of two conditions must hold.

105
00:08:13.110 --> 00:08:16.960
For example, find all beer
whose name is neither bad nor

106
00:08:16.960 --> 00:08:20.080
is the price less than $6 per bottle.

107
00:08:20.080 --> 00:08:23.021
Now I would strongly encourage you
to play with these operators in

108
00:08:23.021 --> 00:08:24.098
your hands on session.

109
00:08:27.220 --> 00:08:29.980
Now I'm sure you remember
the like query in SQL.

110
00:08:31.420 --> 00:08:37.000
MongoDB uses regular expressions
to specify partial string matches.

111
00:08:37.000 --> 00:08:39.120
Now some of you may not know
what a regular expression is.

112
00:08:39.120 --> 00:08:41.660
Let's first use some examples.

113
00:08:43.680 --> 00:08:48.150
The first example is the same query
we saw before when we're asking for

114
00:08:48.150 --> 00:08:53.514
beer manufacturers,
whose name has a sub string A-M in it,

115
00:08:53.514 --> 00:08:56.620
so A-M can appear
anywhere within the name.

116
00:08:57.630 --> 00:08:58.550
To do this,

117
00:08:58.550 --> 00:09:03.890
the query condition first states that
it is going to use a $regex operation.

118
00:09:04.980 --> 00:09:10.340
And then we have to give
the partial string as /am/.

119
00:09:10.340 --> 00:09:16.250
Then it gives the directive
that this match

120
00:09:16.250 --> 00:09:21.560
should be case insensitive by placing
an i after the partial string.

121
00:09:21.560 --> 00:09:28.070
And if we just wanted to do names we
would stop right after the find function.

122
00:09:28.070 --> 00:09:33.120
But here we also want to do a count,
which is a post operation

123
00:09:33.120 --> 00:09:38.140
after the find, so we use .count
at the end of the find function.

124
00:09:39.860 --> 00:09:46.560
Now, what if we have the same query,
but we want the partial string A-m?

125
00:09:46.560 --> 00:09:49.610
To appear at the beginning of the name and

126
00:09:49.610 --> 00:09:52.070
you'd like the A to really
be a capital letter.

127
00:09:53.460 --> 00:09:57.540
In this case we use
the caret sign to indicate

128
00:09:57.540 --> 00:09:59.950
that the partial string is at
the beginning of the name.

129
00:10:01.090 --> 00:10:05.786
Naturally we also drop the i at the end
because the match is no longer case

130
00:10:05.786 --> 00:10:10.389
insensitive A more complex

131
00:10:10.389 --> 00:10:15.445
partial string pattern will be a case
where our name starts with capital A-m,

132
00:10:15.445 --> 00:10:19.600
then has a number of characters
in the middle and ends with corp.

133
00:10:20.810 --> 00:10:26.140
So for the first part,
the string pattern is ^Am.

134
00:10:26.140 --> 00:10:31.370
For the second part, that is,
any character in the middle, we use dot

135
00:10:31.370 --> 00:10:37.480
to represent any character, and star
to represent zero or more occurrences.

136
00:10:37.480 --> 00:10:40.090
For the third part, we say corp but

137
00:10:40.090 --> 00:10:44.580
put a dollar at the end to say that it
must appear at the end of the string.

138
00:10:45.830 --> 00:10:49.330
The regular expression pattern is
a sub-language, in itself, and

139
00:10:49.330 --> 00:10:52.500
is supported by most
programming languages today.

140
00:10:52.500 --> 00:10:56.530
We will refer you to the following
URL to learn more about it.

141
00:10:58.120 --> 00:11:01.270
Also, note an example that,
instead of saying, find.count,

142
00:11:01.270 --> 00:11:06.200
we can directly use the count function,
natively defined in MongoDB.

143
00:11:08.680 --> 00:11:13.070
One important feature of JSON is that
everything contain arrays, as a type of

144
00:11:13.070 --> 00:11:19.390
collection objects, this enables us
to query arrays in multiple ways.

145
00:11:19.390 --> 00:11:23.460
One of them,
is to consider an array as a list and

146
00:11:23.460 --> 00:11:28.310
perform intersection operations,
the first query shows this.

147
00:11:28.310 --> 00:11:29.840
The data item is shown on the right.

148
00:11:31.190 --> 00:11:35.600
It has the area value attribute
called tags with three entries.

149
00:11:35.600 --> 00:11:40.390
The first query asks if two specific
strings belong to the array.

150
00:11:41.660 --> 00:11:46.510
In other words, it wants to get
the document whose tagged attribute

151
00:11:46.510 --> 00:11:48.480
intersects with the query supplied array.

152
00:11:49.520 --> 00:11:53.640
In this case, there is an intersection and
the document is returned.

153
00:11:55.250 --> 00:11:57.950
In the second case, it is asking for

154
00:11:57.950 --> 00:12:01.570
a documents who's tags
attribute has no intersection.

155
00:12:02.920 --> 00:12:08.880
Now notice the $nin operator so
there is no intersection with this list.

156
00:12:10.730 --> 00:12:14.810
So in this document there exists and
intersection so nothing will be returned.

157
00:12:16.940 --> 00:12:21.560
A different kind of array query uses
the positions of the list elements and

158
00:12:21.560 --> 00:12:23.680
wants to extract a portion of the array.

159
00:12:24.950 --> 00:12:28.000
This is illustrated in
the third query which asks for

160
00:12:28.000 --> 00:12:31.140
the second and third items of the array.

161
00:12:31.140 --> 00:12:35.890
To encode this in MongoDB,
we use the $slice operator

162
00:12:35.890 --> 00:12:41.890
which needs two parameters,
the number of variable limits to skip,

163
00:12:41.890 --> 00:12:44.370
and the number of variable limits
to extract after skipping.

164
00:12:45.420 --> 00:12:50.910
In this case, we need to extract items two
and three, so the skip value is one and

165
00:12:50.910 --> 00:12:55.130
the number of items is two,
thus returning summer and Japanese.

166
00:12:56.320 --> 00:13:00.990
Now, we could get the same result if we
pose the query using the last statement.

167
00:13:02.080 --> 00:13:06.740
In this case, the minus says that
the system should count from the end and

168
00:13:07.800 --> 00:13:10.360
the true says that it should
extract two elements.

169
00:13:11.990 --> 00:13:16.096
Now if we omitted the minus sign,
it will come from the beginning and

170
00:13:16.096 --> 00:13:17.981
fetch the first two elements.

171
00:13:21.300 --> 00:13:27.570
Finally, we can also ask for a document
who's second element in tags is summer.

172
00:13:27.570 --> 00:13:33.529
In this case we use an array index
tags.1 to denote the second element.

173
00:13:37.580 --> 00:13:42.232
Compound statements are queries
with multiple query conditions that

174
00:13:42.232 --> 00:13:45.750
are combined using logical operations.

175
00:13:45.750 --> 00:13:51.167
The query shown here has one condition,
which is the and,are in terms of MongoDB,

176
00:13:51.167 --> 00:13:53.640
the $and of three different clauses.

177
00:13:54.710 --> 00:13:57.310
The last clause is
the most straight forward,

178
00:13:57.310 --> 00:14:00.368
it states that the desired
item should not be Coors.

179
00:14:00.368 --> 00:14:07.630
The first clause is an or, that is,
a $or, between two sub-conditions,

180
00:14:07.630 --> 00:14:13.622
A the prices either 3.99, or B it is 4.99.

181
00:14:13.622 --> 00:14:18.154
The second clause is also an or
of two sub conditions,

182
00:14:18.154 --> 00:14:23.101
A the rating is good, and
B the quantity is less than 20.

183
00:14:24.474 --> 00:14:27.019
This query shows that the $and and

184
00:14:27.019 --> 00:14:32.170
the $or operators need a list
that is an array of arguments.

185
00:14:32.170 --> 00:14:34.343
To draw a quick comparison,

186
00:14:34.343 --> 00:14:38.609
here's the example of the same
query imposed with SQL.

187
00:14:43.250 --> 00:14:47.612
Now, an important feature of
semi-structured data is that

188
00:14:47.612 --> 00:14:49.070
it allows nesting.

189
00:14:50.120 --> 00:14:54.750
We showed three documents here,
where there is an area named points,

190
00:14:54.750 --> 00:14:58.620
which in turn has two tuples with
the elements points and bonus.

191
00:14:58.620 --> 00:15:04.060
Let's assume that these three
documents are part of a collection, so

192
00:15:04.060 --> 00:15:07.630
they form three items in
an area called users.

193
00:15:08.800 --> 00:15:13.060
Our goal is to show how we can
write queries to extract data

194
00:15:13.060 --> 00:15:15.350
from these documents with nesting.

195
00:15:16.620 --> 00:15:19.300
The first query wants
to find documents for

196
00:15:19.300 --> 00:15:23.860
which, the value of points should
be less than or equal to 80.

197
00:15:23.860 --> 00:15:26.052
Now, which ones?

198
00:15:26.052 --> 00:15:29.120
Now, points.0,

199
00:15:29.120 --> 00:15:34.016
refers to the first tuple
under the outer points, and

200
00:15:34.016 --> 00:15:39.800
points.0.points, refers to
the first element of that tuple.

201
00:15:39.800 --> 00:15:42.980
Clearly, only the second
documents satisfies this query.

202
00:15:45.280 --> 00:15:48.150
Now, what happens if we have
the same query but we drop the zero?

203
00:15:49.410 --> 00:15:54.260
Now, we are looking for points.points
without specifying the array index.

204
00:15:54.260 --> 00:15:56.830
This means that the points element in

205
00:15:56.830 --> 00:16:00.050
any of the tuples should
have a value of 80 or less.

206
00:16:01.170 --> 00:16:05.403
So now, the first and the second
document will satisfy the query.

207
00:16:07.900 --> 00:16:11.340
We can put multiple conditions
as seen in the third query.

208
00:16:12.490 --> 00:16:16.740
It looks for a document where the points
element of a tuple, should be utmost 81,

209
00:16:16.740 --> 00:16:23.440
and the bonus should be exactly 20, and
clearly the second document qualifies.

210
00:16:23.440 --> 00:16:25.500
But does the third document qualify?

211
00:16:26.930 --> 00:16:32.640
In this case, the first tuple
satisfies points greater than 81 and

212
00:16:32.640 --> 00:16:34.870
the second tuple satisfies
bonus equal to 20.

213
00:16:34.870 --> 00:16:41.190
The answer is no, because the comma
is treated as an implicit and

214
00:16:41.190 --> 00:16:45.280
condition within the same double,
as shown in the yellow braces.

215
00:16:47.320 --> 00:16:50.770
Now remember that we said in course
two that all semi-structured

216
00:16:50.770 --> 00:16:52.070
data can be viewed as a tree.

217
00:16:53.490 --> 00:16:57.730
Now, what if I pick a node of a tree and
ask for all the descendents of that node?

218
00:16:58.880 --> 00:17:02.660
That would require the system
to recursively get chideNodes,

219
00:17:02.660 --> 00:17:04.510
over increasing depth from the given node.

220
00:17:05.820 --> 00:17:11.141
Unfortunately, at this time,
MongoDB does not support recursive search.
WEBVTT

1
00:00:02.463 --> 00:00:07.381
A different kind of scaling problem arises
when we try to answer queries over a large

2
00:00:07.381 --> 00:00:12.158
number of data sources, but before we do
that let's see how a query is answered in

3
00:00:12.158 --> 00:00:14.570
the virtual data integration setting.

4
00:00:15.990 --> 00:00:18.819
We are going to use a toy
scenario in a medical setting

5
00:00:19.890 --> 00:00:22.070
simple as we have four data sources.

6
00:00:22.070 --> 00:00:24.320
Each with one table for
the sake of simplicity.

7
00:00:25.570 --> 00:00:30.760
Notice that two sources, S1 and
S3, have the same schema.

8
00:00:30.760 --> 00:00:34.380
Now this is entirely possible because
sources may be independent of each other.

9
00:00:35.750 --> 00:00:39.310
Further, there is no guarantee that
they would have the same exact content.

10
00:00:40.390 --> 00:00:44.400
Maybe these two sources represent
clinics at different locations.

11
00:00:46.260 --> 00:00:48.860
So next, we look at the target schema.

12
00:00:50.020 --> 00:00:54.630
For simplicity, let's consider that
it's not an algorithmically creative

13
00:00:54.630 --> 00:00:59.090
probabilistic mediator schema but just a
manually designed schema with five tables.

14
00:01:00.670 --> 00:01:05.620
But while we assume that
the target schema is fixed

15
00:01:05.620 --> 00:01:08.710
we want the possibility that
we can add more sources.

16
00:01:08.710 --> 00:01:11.280
That means more clinics
as the system grows.

17
00:01:13.970 --> 00:01:16.210
Now I'm beginning to
add the schema mapping.

18
00:01:17.370 --> 00:01:21.430
Now there are several techniques
of specifying schema mappings.

19
00:01:21.430 --> 00:01:24.090
One of them is called Local-as-View.

20
00:01:24.090 --> 00:01:29.830
This means we write the relations in each
source as a view over the target schema.

21
00:01:31.590 --> 00:01:35.398
But this way of writing the query,
as we can see here, may seem odd to you.

22
00:01:35.398 --> 00:01:39.280
It's called syntax, but
you don't need to know it.

23
00:01:39.280 --> 00:01:45.300
Just as an example, the first few things
the treats relation in S1 maps to,

24
00:01:45.300 --> 00:01:47.220
so you see that arrow, that means maps to.

25
00:01:48.790 --> 00:01:53.030
So it maps to the query select doctor,

26
00:01:53.030 --> 00:01:58.890
chronic disease from treats patient,
has chronic disease.

27
00:01:58.890 --> 00:02:04.550
Where treatspatient.patient Is equal
to has chronic disease dot patient.

28
00:02:07.050 --> 00:02:08.520
We see the query here in the yellow box.

29
00:02:09.760 --> 00:02:14.200
The only thing we should notice here
is that the select class on the query

30
00:02:14.200 --> 00:02:15.880
has two attributes doctor and

31
00:02:15.880 --> 00:02:21.040
chronic disease which are exactly the same
attributes of the treats relation in S1.

32
00:02:23.360 --> 00:02:25.820
Now let's ask a query that
gives the target schema.

33
00:02:26.820 --> 00:02:30.620
Which doctors are responsible for
discharging patients?

34
00:02:30.620 --> 00:02:32.777
Which translates to
the SQL query shown here.

35
00:02:34.491 --> 00:02:39.344
Now, the problem is how to translate
this query to a query that can

36
00:02:39.344 --> 00:02:41.120
be sent to the sources.

37
00:02:42.160 --> 00:02:47.985
Now ideally this should be simplest query
with no extra operations as shown here.

38
00:02:47.985 --> 00:02:55.020
S3 treats means treats
relation in sources 3.

39
00:02:55.020 --> 00:02:57.500
Now you can see the ideal answer.

40
00:02:58.600 --> 00:03:03.200
To find such an optimal query
reformulation, it turns out that this

41
00:03:03.200 --> 00:03:08.550
process is very complex and becomes
worse as a number of sources increases.

42
00:03:10.090 --> 00:03:12.840
Thus query reformulation

43
00:03:12.840 --> 00:03:17.200
becomes a significant scalability problem
in a big data integration scenario.

44
00:03:21.000 --> 00:03:22.640
Let's look at the second use case

45
00:03:23.860 --> 00:03:27.070
public health is a significant
component of our healthcare system.

46
00:03:28.150 --> 00:03:33.630
Public health systems monitor detect and
take action when epidemics strike.

47
00:03:33.630 --> 00:03:38.500
Not so long ago we have witnessed public
health concerns due to Anthrax virus,

48
00:03:38.500 --> 00:03:40.240
the swine flu and the bird flu.

49
00:03:41.510 --> 00:03:46.180
But these epidemics caused a group
called WADDS to develop a system for

50
00:03:46.180 --> 00:03:47.070
disease surveillance.

51
00:03:48.230 --> 00:03:52.670
This system would connect all local
hospitals in the Washington, DC area, and

52
00:03:52.670 --> 00:03:55.290
is designed to exchange
disease information.

53
00:03:55.290 --> 00:04:00.030
For example, if a hospital lab has
identified a new strain of a virus,

54
00:04:00.030 --> 00:04:02.630
other hospitals and the Centers for
Disease Control, CDC,

55
00:04:02.630 --> 00:04:05.260
in the network,
should be able to know about it.

56
00:04:07.300 --> 00:04:12.020
It should be clear that this needs a data
integration solution where the data

57
00:04:12.020 --> 00:04:17.490
sources would be the labs, the data would
be the lab tests medical records and

58
00:04:17.490 --> 00:04:21.170
even genetic profiles of the virus and
the subjects who might be infected.

59
00:04:22.180 --> 00:04:26.630
The table here shows the different
components with this architecture.

60
00:04:26.630 --> 00:04:29.590
We will just digest the necessary
parts for our requirement.

61
00:04:30.620 --> 00:04:35.488
Just know that RIM which stands for
Reference Information Model is global

62
00:04:35.488 --> 00:04:40.370
schema that this industry has developed
and expects to use as a standard.

63
00:04:43.820 --> 00:04:47.330
Why we want to exchange and combine new
information from different hospitals?

64
00:04:48.430 --> 00:04:50.990
Every hospital is independent and

65
00:04:50.990 --> 00:04:54.190
can implement their own information
system any way they see fit.

66
00:04:55.450 --> 00:04:59.080
Therefore even when there
are standards like HL-7 that

67
00:04:59.080 --> 00:05:03.230
specify what kind of data a held
cache system should have an exchange.

68
00:05:03.230 --> 00:05:08.520
There are considerable variations in
the implementation of the standard itself.

69
00:05:08.520 --> 00:05:12.660
For example the two wide boxes show
a difference in representation

70
00:05:12.660 --> 00:05:17.620
of the same kind of data, this should
remind you of the data variety problem.

71
00:05:18.800 --> 00:05:23.810
Let's say, we have a patient
with ID 19590520 whose lab

72
00:05:25.280 --> 00:05:29.020
reports containing her plasma protein
measurements are required for

73
00:05:29.020 --> 00:05:30.440
analyzing her health condition.

74
00:05:31.590 --> 00:05:35.550
The problem is that the patient
went to three different clinics and

75
00:05:35.550 --> 00:05:38.990
four different labs which all
implement the standards differently.

76
00:05:40.290 --> 00:05:41.540
On top of it?

77
00:05:41.540 --> 00:05:45.420
Each clinic uses its own
electronic medical record system

78
00:05:45.420 --> 00:05:47.230
which we have a very large amount of data.

79
00:05:48.510 --> 00:05:53.300
So the data integration system's job is to
transform the data from the source schema

80
00:05:53.300 --> 00:05:58.150
to the schema of the receiving
system in this case the rim system.

81
00:05:59.410 --> 00:06:02.730
This is sometimes called
the data exchange problem.

82
00:06:05.380 --> 00:06:09.140
Informally a data exchange
problem can be defined like this.

83
00:06:10.780 --> 00:06:14.300
Suppose we have a given database
whose relations are known.

84
00:06:15.750 --> 00:06:18.920
Let us say we also know
the target database's schema and

85
00:06:18.920 --> 00:06:20.700
the constraints the schema will satisfy.

86
00:06:22.330 --> 00:06:26.780
Further we know the desired schema
mappings between the source and

87
00:06:26.780 --> 00:06:28.300
this target schema.

88
00:06:28.300 --> 00:06:34.040
What we do not know is how to populate
the tuples in the target database.

89
00:06:34.040 --> 00:06:38.493
From the tuples in the socialization in
such a way that both schema mappings and

90
00:06:38.493 --> 00:06:41.540
target constraints
are simultaneously satisfied.

91
00:06:44.729 --> 00:06:48.854
In many domains like healthcare,
a significant amount of effort has been

92
00:06:48.854 --> 00:06:52.260
spend by the industry in
standardizing schemas and values.

93
00:06:53.440 --> 00:06:57.320
For example LOINC is a standard for
medical lab observations.

94
00:06:58.720 --> 00:07:01.320
Here item like systolic blood pressure or

95
00:07:01.320 --> 00:07:05.580
gene mutation are encoded in this
specific way as given by this standard.

96
00:07:06.630 --> 00:07:11.060
So, if we want to write that
the systolic/diastolic pressure of

97
00:07:11.060 --> 00:07:17.190
a individual is 132 by 90, we'll not write
out the string systolic blood pressure,

98
00:07:17.190 --> 00:07:19.920
but use the code for it.

99
00:07:19.920 --> 00:07:24.070
The ability to use standard code
is not unique to healthcare data.

100
00:07:24.070 --> 00:07:26.919
The 50 states of the US all
have two letter abbreviations.

101
00:07:28.210 --> 00:07:34.210
Generalizing therefore, whenever we have
data such as the domain is finite and have

102
00:07:34.210 --> 00:07:39.910
a standard set of code available, we give
a new opportunity of handling big deal.

103
00:07:41.610 --> 00:07:45.260
Mainly, reducing the data
size through compression.

104
00:07:47.110 --> 00:07:49.700
The compression refers to a way of

105
00:07:49.700 --> 00:07:52.920
creating an encoded
representation of data.

106
00:07:52.920 --> 00:07:56.980
So that this encoder form is smaller
than the original representation.

107
00:07:58.970 --> 00:08:02.420
A common encoding method is
called dictionary encoding.

108
00:08:02.420 --> 00:08:05.980
Consider a database with 10 million
record of patient visits a lab.

109
00:08:07.250 --> 00:08:11.140
Each record indicates a test and
its results.

110
00:08:11.140 --> 00:08:14.140
Now we show it this way like
in a columnar structure

111
00:08:15.220 --> 00:08:20.040
to make the point that the data is kept
in a column stored relational database

112
00:08:20.040 --> 00:08:22.430
rather than a row store
relational database.

113
00:08:23.650 --> 00:08:26.380
Now consider the column for test code.

114
00:08:26.380 --> 00:08:29.470
Where the type of test is codified
according to the standard.

115
00:08:31.470 --> 00:08:35.340
We replace a string representation
of the standard by a number.

116
00:08:37.750 --> 00:08:40.540
The mapping between
the original test code and

117
00:08:40.540 --> 00:08:43.870
the encoded number are also
stored separately.

118
00:08:43.870 --> 00:08:45.810
Now suppose there
are a total of 500 tests.

119
00:08:47.240 --> 00:08:52.780
So this separate table called
the dictionary here has 500 rows,

120
00:08:52.780 --> 00:08:57.500
which is clearly much smaller
than ten million right?

121
00:08:57.500 --> 00:09:02.300
Now 500 distinct values can be
represented by encoding them in 9 bits,

122
00:09:02.300 --> 00:09:04.114
because 2 to the power of 9 is 512.

123
00:09:06.060 --> 00:09:09.750
Other encoding techniques would be applied
to attributes like date and patient ID.

124
00:09:10.920 --> 00:09:15.890
That's full large data we cannot reduce
the number of total actual rules.

125
00:09:15.890 --> 00:09:18.090
So we have to store all ten million rules.

126
00:09:18.090 --> 00:09:23.893
But we can reduce the amount of space
required by storing data in a column

127
00:09:23.893 --> 00:09:28.925
oriented data store and
by using compression, indeed modern

128
00:09:28.925 --> 00:09:35.621
systems use credit processing algorithms
to operate directly on compress data.

129
00:09:39.025 --> 00:09:42.636
Data compression is an important
technology for big data.

130
00:09:45.555 --> 00:09:50.560
And just like is a set of qualified
terms for lab tests, clinical data

131
00:09:50.560 --> 00:09:55.500
also uses SNOMED which stands for
systematized nomenclature of medicine.

132
00:09:56.770 --> 00:09:59.780
SNOMED is a little more
than just a vocabulary.

133
00:10:00.900 --> 00:10:02.599
It does have a vocabulary of course.

134
00:10:03.620 --> 00:10:08.656
The vocabulary is the collection
of medical terms in human and

135
00:10:08.656 --> 00:10:14.790
medicine to provide codes, terms, synonyms
and definitions that cover anatomy,

136
00:10:14.790 --> 00:10:18.980
diseases, findings, procedures,
micro organisms, substances etcetera.

137
00:10:19.990 --> 00:10:21.590
But it also has relationships.

138
00:10:23.060 --> 00:10:27.277
As you can see,
a renal cyst is related to kidney because

139
00:10:27.277 --> 00:10:30.615
kidney's the finding site of a renal cyst.

140
00:10:32.890 --> 00:10:37.080
If we query against an ontology,
it would look like a graph grid.

141
00:10:38.150 --> 00:10:38.920
In this box,

142
00:10:38.920 --> 00:10:43.120
we are asking to find all patient
findings with a benign tumor morphology.

143
00:10:44.350 --> 00:10:49.100
In terms of querying, we are looking for
edges of a graph where one noticed

144
00:10:49.100 --> 00:10:54.470
the concept that we need to find which is
connected to a node called benign neoplasm

145
00:10:54.470 --> 00:10:59.050
that is benign tumor through an edge
called associated morphology

146
00:11:00.070 --> 00:11:04.700
that applying this query against
the data here produces all benign tumors

147
00:11:04.700 --> 00:11:08.970
of specific organs as you can
see by the orange rounded group.

148
00:11:11.170 --> 00:11:13.410
But now that we have these terms,

149
00:11:13.410 --> 00:11:17.760
we can use these terms to search
outpatient records with these terms would

150
00:11:17.760 --> 00:11:22.910
have been used so
what's the essence of this used case.

151
00:11:24.030 --> 00:11:29.170
This is can shows that an in division
system in a public health domain and

152
00:11:29.170 --> 00:11:32.320
in many other domains must
be able to handle variety.

153
00:11:33.910 --> 00:11:40.520
In this case there's a Global Schema
called RiM shown here all queries and

154
00:11:40.520 --> 00:11:45.000
analyses performed by a data analysts
should be against this global schema.

155
00:11:46.230 --> 00:11:50.870
However, the actual data which is
generated by different medical facilities

156
00:11:50.870 --> 00:11:55.970
would need to be transformed
into data in this schema.

157
00:11:55.970 --> 00:12:00.220
This would require not only
format conversions, but

158
00:12:00.220 --> 00:12:05.630
it would need to respect all constraints
imposed by the source and by the target.

159
00:12:05.630 --> 00:12:10.520
For example, a source may not distinguish
between an emergency surgical procedure

160
00:12:10.520 --> 00:12:12.430
and a regular surgical procedure.

161
00:12:12.430 --> 00:12:16.429
But the target may want to
put them in different tables.

162
00:12:18.850 --> 00:12:20.850
We also saw that
the integration system for

163
00:12:20.850 --> 00:12:24.920
this used case would need to use
quantified data but this gives

164
00:12:24.920 --> 00:12:28.970
us the opportunity to use data compression
to gauge story and query efficiency.

165
00:12:30.120 --> 00:12:35.352
In terms of variety, we saw how
relational data like patient records XML

166
00:12:35.352 --> 00:12:40.603
data like HL7 events, and
graph data like ontologies, are co-used.

167
00:12:42.866 --> 00:12:47.407
To support this, the integration
system must be able to do both model

168
00:12:47.407 --> 00:12:50.400
transformation and query transformation.

169
00:12:51.560 --> 00:12:55.260
Query transformation is the process of
taking a query on the target schema.

170
00:12:56.490 --> 00:13:00.540
Converting it to a query
against a different data model.

171
00:13:00.540 --> 00:13:06.100
For example, part of an SQL query against
the RIM may need to go to snowmad and

172
00:13:06.100 --> 00:13:09.240
hence when you to be converted to
a graph query in the snowmad system.

173
00:13:10.590 --> 00:13:13.933
Model transformation is
a process of taking data

174
00:13:13.933 --> 00:13:17.274
represented in one model
in one source system and

175
00:13:17.274 --> 00:13:22.263
converting it to an equivalent data
in another model the target system.
WEBVTT

1
00:00:00.820 --> 00:00:02.600
Hello, my name is Victor Lou.

2
00:00:02.600 --> 00:00:07.090
I am the Solutions Engineer of
Global Philanthropy of Daameer.org.

3
00:00:07.090 --> 00:00:08.440
Helping researchers, academia,

4
00:00:08.440 --> 00:00:11.750
and non-profits find the insights
that matter using data.

5
00:00:11.750 --> 00:00:15.280
Today I am here to give you
a production environment,

6
00:00:15.280 --> 00:00:17.410
personalized music recommendation project.

7
00:00:17.410 --> 00:00:21.150
That was done for PonoMusic and
so what is Pono Music?

8
00:00:23.120 --> 00:00:26.355
Pono Music is revolutionizing music
listening by bringing back native,

9
00:00:26.355 --> 00:00:28.290
high-resolution audio.

10
00:00:28.290 --> 00:00:31.550
It's the way that artists intended and
recorded their music.

11
00:00:31.550 --> 00:00:36.710
Neil Young, the iconic artist and
musician is also the founder and CEO.

12
00:00:36.710 --> 00:00:40.020
Pono Music is the only complete
high-resolution music ecosystem.

13
00:00:40.020 --> 00:00:43.950
That includes the Pono player, the Pono
Music store, the Pono Music community.

14
00:00:45.200 --> 00:00:48.030
They have been super
generous in allowing us to

15
00:00:48.030 --> 00:00:51.020
take a look behind the scenes as how
we built out the recommendations.

16
00:00:53.630 --> 00:00:56.540
So, David Meer supports their mission
by providing a music recommendation

17
00:00:56.540 --> 00:01:00.900
engine that is both scalable and
flexible as Pono grows their user base.

18
00:01:00.900 --> 00:01:05.220
Let's begin by visiting Pono
music's website as you can see,

19
00:01:05.220 --> 00:01:07.740
they have a few shelves that focus on

20
00:01:07.740 --> 00:01:11.320
various recommendations that are not
tailored to each individual user.

21
00:01:11.320 --> 00:01:14.670
If you login,
then an additional shelf would appear

22
00:01:14.670 --> 00:01:17.690
that would deliver the recommendations
that are created in data gear.

23
00:01:17.690 --> 00:01:19.029
So how does DataMirror accomplish this.

24
00:01:19.029 --> 00:01:24.279
In a nutshell, DataMirror is a end to end,
antalis platform which contains ingestion,

25
00:01:24.279 --> 00:01:29.543
preparation analysis, visualization and
operationalization all the same platform.

26
00:01:29.543 --> 00:01:32.300
And having all the capabilities
in one platform is superior to

27
00:01:32.300 --> 00:01:34.240
traditional technology stack.

28
00:01:34.240 --> 00:01:39.300
Integration between disparate technologies
brings a lot of unnecessary challenges.

29
00:01:39.300 --> 00:01:41.880
We take advantage of open
source big data technologies.

30
00:01:41.880 --> 00:01:46.261
Specifically, we run natively and
Hadoop and Spark for our back end.

31
00:01:46.261 --> 00:01:49.600
And we leverage D3JS on the front end for
visualizations.

32
00:01:49.600 --> 00:01:51.821
Our Excel-like interface makes it easy for

33
00:01:51.821 --> 00:01:55.960
folks who don't know how to code to
also get in with the data modeling.

34
00:01:55.960 --> 00:01:57.720
It is very much self-service.

35
00:01:57.720 --> 00:02:00.001
In the case of Pono Music's
data mirror deployment,

36
00:02:00.001 --> 00:02:02.790
we have deployed a Hadoop
on Amazon on web services.

37
00:02:02.790 --> 00:02:04.410
But we can also be deployed off premise if

38
00:02:04.410 --> 00:02:06.290
needed it doesn't have
to be in the clouds.

39
00:02:06.290 --> 00:02:09.820
In fact, we could work with many many
different distributions of Hadoop.

40
00:02:09.820 --> 00:02:12.320
We can access dean rear through
any modern web browser.

41
00:02:12.320 --> 00:02:16.300
As you can see here we're using Google
Chrome but it works on any browser, right?

42
00:02:16.300 --> 00:02:18.460
So lets go ahead and take a look at
the a; let's log into data grip,

43
00:02:18.460 --> 00:02:21.640
so we're going to go ahead and
log in here..

44
00:02:23.390 --> 00:02:28.250
So what you're seeing here is the browser
tab and we can see various artifacts

45
00:02:28.250 --> 00:02:32.364
which include connections such
as connections to the pull nose,

46
00:02:32.364 --> 00:02:36.434
salesforce.com instance or
a connection to AWS S3 instance.

47
00:02:36.434 --> 00:02:38.226
Because DataMirror comes with so

48
00:02:38.226 --> 00:02:42.708
many prebuilt connectors which include
Connections to salesforce.com and S3.

49
00:02:42.708 --> 00:02:46.034
We can easily grant access to
those systems in order to pull or

50
00:02:46.034 --> 00:02:48.550
push data in and out of the dev.

51
00:02:48.550 --> 00:02:51.760
For example, let's take a quick look at
the salesforce.com configuration here.

52
00:02:51.760 --> 00:02:59.290
As you can see, we've already configured
salesforce as the connector type.

53
00:02:59.290 --> 00:03:00.530
When we hit next here,

54
00:03:00.530 --> 00:03:04.370
we'll we that you can authorize
a data manager to retrieve data.

55
00:03:04.370 --> 00:03:06.380
We're not going to do it because
this is a production environment, so

56
00:03:06.380 --> 00:03:09.300
I don't want to disrupt the pools.

57
00:03:09.300 --> 00:03:13.210
But if we were to click it, it'll give
us a salesfloor.com blogging stream.

58
00:03:13.210 --> 00:03:17.830
And so as soon as we login, the Datameer
would have access via the O-Off token.

59
00:03:17.830 --> 00:03:20.130
And so this is valid for a period of time.

60
00:03:20.130 --> 00:03:20.650
I'm going to go ahead and

61
00:03:20.650 --> 00:03:23.910
hit the cancel button as this
is a production environment.

62
00:03:23.910 --> 00:03:28.290
And so now we're ready to look
at an import job artifact.

63
00:03:28.290 --> 00:03:31.888
So this is the sales force import job and

64
00:03:31.888 --> 00:03:36.152
as you can see we go in here and
we configure it.

65
00:03:36.152 --> 00:03:41.830
We can see that it's connected to
the Pono Music SMDC connection.

66
00:03:44.753 --> 00:03:49.269
I'm going to go ahead and
hit next here.with salesforce.com,

67
00:03:49.269 --> 00:03:52.079
we just have to simply define the SOQL,

68
00:03:52.079 --> 00:03:56.526
which is basically a querying
language that's based on SQL.

69
00:03:56.526 --> 00:03:59.860
To locate data contained within
the salesforce.com objects.

70
00:03:59.860 --> 00:04:04.997
You can see the select statement here,
you can also see the familiar

71
00:04:04.997 --> 00:04:10.059
from statement, and you can also
see the where statement as well.

72
00:04:10.059 --> 00:04:12.459
So in order to protect
the privacy of the users,

73
00:04:12.459 --> 00:04:16.830
Datameer has the capability to obfuscate
sort of columns that are sensitive.

74
00:04:16.830 --> 00:04:19.770
And so in other words, we can scramble
sensitive fields such as email or

75
00:04:19.770 --> 00:04:21.370
physical addresses.

76
00:04:21.370 --> 00:04:25.228
And at this point in time, I'm going to
go ahead and cancel out, and I'm going to

77
00:04:25.228 --> 00:04:28.866
show the next part of this demo in
an obfuscated workbook environment.

78
00:04:28.866 --> 00:04:33.813
The artifacts contained in the obfuscated
folders are duplicates of the import job

79
00:04:33.813 --> 00:04:38.130
of the workbook from their production
counterparts as you can see here.

80
00:04:40.360 --> 00:04:44.510
So as you can see in the import
job settings we have configured

81
00:04:44.510 --> 00:04:51.320
the same connections here but
for the obfuscated columns here.

82
00:04:51.320 --> 00:04:54.970
Now we're obfuscating the owner
email as well as the street address.

83
00:04:54.970 --> 00:04:55.690
Now we hit Next here.

84
00:04:59.259 --> 00:05:03.140
And so you can see, various fields are
being pulled here from Salesforce, right.

85
00:05:03.140 --> 00:05:05.130
And in particular,
I want to highlight the.

86
00:05:06.150 --> 00:05:09.780
The owner email, that field, so as you
can see these are the obfuscated fields.

87
00:05:09.780 --> 00:05:13.420
Similarly, the actual street address,
that's also masked as well.

88
00:05:13.420 --> 00:05:17.375
I'm going to go ahead and hit Cancel.

89
00:05:17.375 --> 00:05:20.780
I will go back to, rather we're going to
go check out the workbook environment,

90
00:05:20.780 --> 00:05:23.300
which is where we do the preparation and
analysis.

91
00:05:28.465 --> 00:05:31.070
To the obfuscated workbook environment.

92
00:05:31.070 --> 00:05:34.260
As you can see we're starting
with a raw source of data here.

93
00:05:34.260 --> 00:05:38.930
This was the data that was
contained in the data sheet.

94
00:05:38.930 --> 00:05:43.500
Take note that there's a variety of icons
that indicate different types of sheets.

95
00:05:43.500 --> 00:05:45.610
So you could see here this
is the raw data type.

96
00:05:45.610 --> 00:05:48.360
This is a regular worksheet.

97
00:05:48.360 --> 00:05:51.400
You have a union sheet and
you have a joint sheet and

98
00:05:51.400 --> 00:05:54.440
we'll go into a little bit
more detail as we get to them.

99
00:05:54.440 --> 00:05:57.700
So, as you can see,
the interface is very much like Excel.

100
00:05:57.700 --> 00:06:02.830
Looking at the second sheet pairs, and
it's using a function called group by.

101
00:06:02.830 --> 00:06:07.380
So the group by function is a type of
aggregation function in Datameer speak.

102
00:06:07.380 --> 00:06:10.640
As you can see, you can build
the functions by pointing and clicking.

103
00:06:10.640 --> 00:06:14.040
But you can also enter the formula
to create nested functions or

104
00:06:14.040 --> 00:06:19.310
Apply arithmetic
operations just as a cell.

105
00:06:19.310 --> 00:06:21.770
We take all the unique combinations
of albums purchased for

106
00:06:21.770 --> 00:06:24.730
each of the unique email
using a group pair function.

107
00:06:25.890 --> 00:06:28.710
Then we pull out each of the elements
of the pairings using a list element

108
00:06:28.710 --> 00:06:30.310
function, so let's see here.

109
00:06:31.360 --> 00:06:34.230
If you were to build a function
from scratch, you can see this.

110
00:06:34.230 --> 00:06:39.950
So then you create a group by function,
and then direct it at a column.

111
00:06:39.950 --> 00:06:41.120
You basically pass it to argument.

112
00:06:43.890 --> 00:06:47.856
To find out the occurrence
of each pairing of albums,

113
00:06:47.856 --> 00:06:52.188
we're going to go ahead and
look at this LR sheet.

114
00:06:52.188 --> 00:06:55.477
And so these are, we're using group
by functions as well here, so

115
00:06:55.477 --> 00:06:58.372
we're grouping by the first item one,
and then item two.

116
00:06:58.372 --> 00:07:00.088
And then, we're doing a group count,

117
00:07:00.088 --> 00:07:03.430
which is counting the frequency
of occurrences of those cells.

118
00:07:03.430 --> 00:07:06.350
Similarly, we're going to
do a similar thing but

119
00:07:06.350 --> 00:07:11.096
with the item two first now, and then
item one, and then doing a group count.

120
00:07:11.096 --> 00:07:13.780
And the reason is,
we don't care about the order.

121
00:07:13.780 --> 00:07:17.810
And so, we repeat this on the next sheet
reversing the order of the albums to make

122
00:07:17.810 --> 00:07:21.460
sure that we capture all the combinations
since the order doesn't matter to us.

123
00:07:24.260 --> 00:07:28.887
And then we use our unit sheet function
to append the second frequency

124
00:07:28.887 --> 00:07:34.140
counting sheet to the first frequency
counting sheet, as you can see here.

125
00:07:34.140 --> 00:07:36.962
It's drag and drop, of course, and
so we've connected them together.

126
00:07:36.962 --> 00:07:39.248
You can do multi-sheets but
in this case we only have two, right.

127
00:07:39.248 --> 00:07:42.260
So it's, okay.

128
00:07:42.260 --> 00:07:47.770
We ignore the co-occurrent set sheet as it
is currently not used in the final output.

129
00:07:47.770 --> 00:07:51.010
We were kind of experimenting
with other recommendation models.

130
00:07:51.010 --> 00:07:52.660
We're rebuilding those workbooks.

131
00:07:52.660 --> 00:07:56.780
So one of the really cool features for
we can execute the workbook, but

132
00:07:56.780 --> 00:07:59.162
deselect the storage of
intermediary sheets.

133
00:07:59.162 --> 00:08:02.958
This means that we only need
relevant functions are executed

134
00:08:02.958 --> 00:08:07.740
at a to the data lineage of columns that
are contained in the selector sheet.

135
00:08:07.740 --> 00:08:12.620
Moving on, we have to create
a separate sheet from the raw data,

136
00:08:12.620 --> 00:08:16.770
that counts the number of times an album
occurs across all of the purchases.

137
00:08:16.770 --> 00:08:20.780
So you can see here, which students will
buy an album and then we do a count.

138
00:08:24.890 --> 00:08:29.768
We bring the code occurrences and
the frequency together using the join

139
00:08:29.768 --> 00:08:33.599
feature as you can see,
it is a drag and drop interface.

140
00:08:33.599 --> 00:08:36.601
You got the obfuscated workbook
each of the worksheets here and

141
00:08:36.601 --> 00:08:39.100
within each worksheets,
you got various columns.

142
00:08:39.100 --> 00:08:45.484
You can bring those, you can drag and
drop them into here to do the joins.

143
00:08:45.484 --> 00:08:50.730
You can do different types of joins,
you got the inner, outer, etc.

144
00:08:50.730 --> 00:08:53.206
You can also do multi column joins.

145
00:08:53.206 --> 00:08:54.491
You can do multi sheet joins.

146
00:08:54.491 --> 00:08:59.168
You can also select which columns
you want to keep during the join.

147
00:08:59.168 --> 00:09:02.705
Go ahead and cancel out here.

148
00:09:02.705 --> 00:09:06.580
Datameer does not allow us to add
additional functions to a joint sheet so

149
00:09:06.580 --> 00:09:09.269
we duplicate it by right clicking and
then hitting the duplicate button.

150
00:09:11.820 --> 00:09:14.780
And then we create the next sheet,
which contains a link to columns, so

151
00:09:14.780 --> 00:09:17.570
it will be linked to the rating sheet.

152
00:09:17.570 --> 00:09:22.480
This is, see the link columns back
to the joint sheets and then.

153
00:09:23.760 --> 00:09:28.880
We simply add a calculation
where it's the number of times

154
00:09:28.880 --> 00:09:32.730
the co-occurrence occurs divided by
the number of times that the first album

155
00:09:32.730 --> 00:09:35.950
in the column appears
throughout the data set.

156
00:09:35.950 --> 00:09:40.284
The idea is to give a high recommendation
for albums that appear frequently

157
00:09:40.284 --> 00:09:44.503
together While simultaneously
penalizing albums that are too common.

158
00:09:44.503 --> 00:09:48.513
And so at this point,
I would like to write a few more joins

159
00:09:48.513 --> 00:09:51.773
that bring together the album ID,
the email,

160
00:09:51.773 --> 00:09:56.980
recommended album ID based on the album
ID and email, so take a look here.

161
00:09:59.310 --> 00:10:03.472
So, as you can see here and then,

162
00:10:03.472 --> 00:10:08.460
before we finish,
we also do an anti joint by performing

163
00:10:08.460 --> 00:10:14.170
the outer left joint with
the almighty in the email.

164
00:10:14.170 --> 00:10:18.830
And then filtering, so this is,
by the way, this is a double column joint.

165
00:10:20.090 --> 00:10:24.060
And then we do a filter, so
then we filter out all the, for

166
00:10:24.060 --> 00:10:26.840
the obfuscated owner
fields that are empty.

167
00:10:26.840 --> 00:10:30.192
We apply a filter to it.

168
00:10:30.192 --> 00:10:35.996
So finally, we use a few group I
functions to do some deep duplication,

169
00:10:35.996 --> 00:10:38.915
so we use group by functions again.

170
00:10:38.915 --> 00:10:42.637
And then, so we make sure that the album
functions are unique for each user and

171
00:10:42.637 --> 00:10:44.250
then we use a group max function.

172
00:10:45.780 --> 00:10:49.292
In order to preserve the highest
rating for each of the unique

173
00:10:50.950 --> 00:10:55.310
recommendations the final desired
output is on the recommenders sheet.

174
00:10:55.310 --> 00:11:01.710
It leverages the group top end function,
so you can see here group top numbers.

175
00:11:01.710 --> 00:11:05.349
So we only want to look at the to
20 recommendations for each user.

176
00:11:05.349 --> 00:11:09.759
And in addition,
we also create a email row number field,

177
00:11:09.759 --> 00:11:13.720
which is a key that needed for
salesforce.com, and

178
00:11:13.720 --> 00:11:18.142
it is generated by a simple
concatenation function here.

179
00:11:18.142 --> 00:11:22.642
Something that note that [SOUND] is
big on data governance, data lineage.

180
00:11:22.642 --> 00:11:26.632
As you can see, we can keep track of
how all the raw data flows through

181
00:11:26.632 --> 00:11:29.650
from the raw data set all
the way to the end product.

182
00:11:31.950 --> 00:11:36.651
So each workbook functions can be
also exported in JSON format to keep

183
00:11:36.651 --> 00:11:38.605
track of revision history.

184
00:11:38.605 --> 00:11:41.470
We're not ready to operationalize
things and so how do we do it?

185
00:11:41.470 --> 00:11:45.700
Get our workbook configurations.

186
00:11:45.700 --> 00:11:46.860
Go ahead and exit out of the workbook.

187
00:11:49.420 --> 00:11:50.710
Let's go back to
the production environment.

188
00:11:56.340 --> 00:11:57.128
As you can see,

189
00:11:57.128 --> 00:12:01.670
the workbook recalculation retriggers
when the import job is completed.

190
00:12:01.670 --> 00:12:03.820
We also only retain the latest results, so

191
00:12:03.820 --> 00:12:09.440
we select purge historical results,
as you can see here.

192
00:12:09.440 --> 00:12:14.970
As I mentioned earlier, a data mirror
makes it easy to operationalize a workbook

193
00:12:14.970 --> 00:12:17.630
because we only select
the final output sheets.

194
00:12:17.630 --> 00:12:21.000
And, so you see everything's unchecked
except for the recommended sheet.

195
00:12:22.730 --> 00:12:26.420
And Datameer basically will automatically
determine what is a critical path

196
00:12:26.420 --> 00:12:28.990
of the intermediary sheets to
produce the desired output sheet.

197
00:12:30.430 --> 00:12:33.031
Once the recommendation
workbook is calculated,

198
00:12:33.031 --> 00:12:35.029
then this triggers the export job run.

199
00:12:35.029 --> 00:12:38.032
I'm going to go ahead and
move to the export job artifact.

200
00:12:38.032 --> 00:12:40.194
Exit out of this.

201
00:12:40.194 --> 00:12:42.361
Go ahead and save, or not save.

202
00:12:42.361 --> 00:12:47.223
We'll see the export job,
let's go ahead and configure that.

203
00:12:52.087 --> 00:12:55.127
Hit Next, Share.

204
00:12:55.127 --> 00:13:00.240
CSV outputs, so.

205
00:13:02.817 --> 00:13:04.557
This triggers the export job to run,

206
00:13:04.557 --> 00:13:08.600
which is how we basically push
the results of the recommendations to S3.

207
00:13:08.600 --> 00:13:14.310
We elected to use S3 because we
are already hosting at services and

208
00:13:14.310 --> 00:13:16.630
S3 is an affordable storage solution.

209
00:13:16.630 --> 00:13:19.695
We cannot push directly to
salesforce.com at the present because

210
00:13:19.695 --> 00:13:22.920
It's a only connection.

211
00:13:22.920 --> 00:13:26.726
Therefore, we need to push
data to the storage area,

212
00:13:26.726 --> 00:13:30.123
which can be scheduled for using Apex.

213
00:13:30.123 --> 00:13:33.796
A general challenge for
is the role limitation on each pool.

214
00:13:33.796 --> 00:13:38.386
Fortunately, DataMirror comes
with the option to push

215
00:13:38.386 --> 00:13:41.873
files that are broken
into one megabyte chunks.

216
00:13:41.873 --> 00:13:46.796
So as you can see here in
advanced settings 1 megabyte, and

217
00:13:46.796 --> 00:13:51.140
then we set a type of
consecutive numbering scheme.

218
00:13:51.140 --> 00:13:53.852
And so it's dynamic naming conventions
using time stamps and numbering for

219
00:13:53.852 --> 00:13:55.530
each of the chunks.

220
00:13:55.530 --> 00:13:58.830
So that sums up the current
co-curds/d deployment.

221
00:13:58.830 --> 00:14:03.018
As you've seen, we have easily
integrated data from Salesforce.com,

222
00:14:03.018 --> 00:14:07.272
created a recommendation model, and
set up a push to S3 automatically for

223
00:14:07.272 --> 00:14:09.856
easy consumption back to Salesforce.com.

224
00:14:09.856 --> 00:14:13.469
Finally, we operationalized this by
triggering each of the steps sequentially

225
00:14:13.469 --> 00:14:16.297
all in sitting on top of
the powerful Hadoop platform.

226
00:14:16.297 --> 00:14:19.070
What's next?

227
00:14:19.070 --> 00:14:23.590
Well, Pono Music is working on
implementing Google Analytics tracking for

228
00:14:23.590 --> 00:14:26.450
each user at the Apple pages.

229
00:14:26.450 --> 00:14:29.450
So let's go back to
the Apple Music Store here.

230
00:14:29.450 --> 00:14:32.620
Just for an example, let's take a look
at the Train Led Zeppelin Two Album.

231
00:14:33.670 --> 00:14:36.650
As you can see all of those album
IDs that we were looking at earlier,

232
00:14:36.650 --> 00:14:38.390
these are actually embedded in the URL.

233
00:14:40.300 --> 00:14:44.517
This means that in the near future, we
can actually adapt recommendations based

234
00:14:44.517 --> 00:14:49.053
on the buying behavior to recommendations
based on both buying and browser behavior.

235
00:14:49.053 --> 00:14:53.313
We can look at recommendations based on
genre, perhaps we can try to look at most

236
00:14:53.313 --> 00:14:57.526
recent purchases or browsing behavior in
the last three to six to nine months.

237
00:14:57.526 --> 00:15:01.541
Or we could use album metadata such as
album release date to give additional

238
00:15:01.541 --> 00:15:03.270
recommendations.

239
00:15:03.270 --> 00:15:07.698
Also, it is quite simple to just
duplicate the recommendations workbook in

240
00:15:07.698 --> 00:15:10.409
[INAUDIBLE] to try any
number of these options.

241
00:15:10.409 --> 00:15:13.172
And so,
that means we can do a lot of AB testing,

242
00:15:13.172 --> 00:15:18.600
as we track how users react to each of
the modified recommendation algorithms.

243
00:15:18.600 --> 00:15:21.170
The possibilities are literally endless.

244
00:15:21.170 --> 00:15:24.051
So anyhow, thanks again for
checking out how Data Mirror

245
00:15:24.051 --> 00:15:27.305
builds a simple code current
recommendation engine for music.

246
00:15:27.305 --> 00:15:29.636
Please feel free to direction questions or

247
00:15:29.636 --> 00:15:32.492
comments to me at
victor.liu@datamirror.com.

248
00:15:32.492 --> 00:15:34.220
Thanks again.
WEBVTT

1
00:00:01.380 --> 00:00:09.185
Amarnath just finished overviewing
querying and integrational big data.

2
00:00:09.185 --> 00:00:13.160
Although, fundamental understanding
of these concepts are important.

3
00:00:14.300 --> 00:00:17.520
Some of these tasks can be accomplished

4
00:00:17.520 --> 00:00:21.650
using graphical user interface
based tools or software products.

5
00:00:22.800 --> 00:00:27.870
In this short module,
we introduce you to two of those tools.

6
00:00:27.870 --> 00:00:29.810
Splunk and Datameer.

7
00:00:31.860 --> 00:00:37.720
We have selected a few videos from
the site of our sponsors Splunk,

8
00:00:37.720 --> 00:00:41.900
to give you an overview of using such
tools for different applications.

9
00:00:42.930 --> 00:00:45.940
You will also have a short
hands-on activity on Splunk.

10
00:00:47.810 --> 00:00:52.210
In addition,
we provide you a comprehensive video on

11
00:00:52.210 --> 00:00:56.110
using Datamere in
the digital music industry.

12
00:00:56.110 --> 00:01:01.150
Keep in mind, Splunk and
Datamere are just two of the many

13
00:01:01.150 --> 00:01:06.613
tools in this category, but
they represent a huge industry.
WEBVTT

1
00:00:15.850 --> 00:00:20.205
Open XC is an open source hardware-software platform.

2
00:00:20.205 --> 00:00:25.230
We wanted to see what correlations we could actually create within this model dashboards.

3
00:00:25.230 --> 00:00:30.915
It plugs into your car, and it gives you all the data that you can possibly want.

4
00:00:30.915 --> 00:00:33.510
What we're interested in is looking at how you

5
00:00:33.510 --> 00:00:36.070
can make the car much more modular and customizable,

6
00:00:36.070 --> 00:00:38.365
getting a lot of this data and afford cars

7
00:00:38.365 --> 00:00:40.750
out and letting developers do whatever they want with it.

8
00:00:40.750 --> 00:00:44.170
You could kind of like update technology as technology progresses.

9
00:00:44.170 --> 00:00:45.930
So what we're really trying to do is like catch up with

10
00:00:45.930 --> 00:00:50.060
the consumer electronics design cycle not the automotive design cycle types.

11
00:00:50.060 --> 00:00:52.265
The clock hasn't started yet.

12
00:00:52.265 --> 00:00:56.120
One of the tests that we did was a gas car versus an electric car.

13
00:00:56.120 --> 00:00:58.230
So we're trying to see which is faster,

14
00:00:58.230 --> 00:00:59.965
which is expecting the most energy,

15
00:00:59.965 --> 00:01:02.070
which is the most cost efficient.

16
00:01:02.070 --> 00:01:05.240
And we wanted to

17
00:01:05.240 --> 00:01:09.025
measure doesn't electric car driver drive different than a gas car driver.

18
00:01:09.025 --> 00:01:11.345
So actually by using the Open XC data,

19
00:01:11.345 --> 00:01:13.120
and by splunking that creating links in

20
00:01:13.120 --> 00:01:17.190
dashboards will give it to match ups and correlations.

21
00:01:23.110 --> 00:01:29.965
It all comes down to how much are they mashing the gas.

22
00:01:29.965 --> 00:01:35.410
We've been internally calling that dashboard the "Lead Foot Dashboard".

23
00:01:45.700 --> 00:01:53.060
Who's the most efficient driver?

24
00:01:53.060 --> 00:01:56.740
There's one of our drivers actually going low and

25
00:01:56.740 --> 00:02:00.085
slow and actually using into the brakes not really gnashing on the gas,

26
00:02:00.085 --> 00:02:02.635
and driving more like a normal person would.

27
00:02:02.635 --> 00:02:07.265
When you drive an electric car, it's a lot quicker off the line than a gas powered car.

28
00:02:07.265 --> 00:02:10.215
So, we find people kind of smashing

29
00:02:10.215 --> 00:02:13.975
on the gas pedal but sort of easing up on the electric pedal.

30
00:02:13.975 --> 00:02:15.595
So I've got some data for you here.

31
00:02:15.595 --> 00:02:17.950
We see. What's this I'm doing?

32
00:02:17.950 --> 00:02:19.995
It says you're doing.

33
00:02:19.995 --> 00:02:20.645
There we go.

34
00:02:20.645 --> 00:02:24.570
Battery car is high. There you go.

35
00:02:24.570 --> 00:02:27.230
Hurry up.

36
00:02:27.230 --> 00:02:29.735
There we go. We don't even need because Open XC, We just cut it.

37
00:02:29.735 --> 00:02:31.670
These are basically all of

38
00:02:31.670 --> 00:02:35.160
the sensor readings that are being broadcast from that Firefly box.

39
00:02:35.160 --> 00:02:38.455
It's plugged into the diagnostics port of the car.

40
00:02:38.455 --> 00:02:41.350
It's about how much power can we kind of give

41
00:02:41.350 --> 00:02:45.190
away in order to incentivize a community of makers.

42
00:02:45.190 --> 00:02:47.645
We want to be able to actually harness that and create

43
00:02:47.645 --> 00:02:50.605
an environment where they can much more quickly,

44
00:02:50.605 --> 00:02:51.990
experiment with our ideas,

45
00:02:51.990 --> 00:02:54.725
enrich the platform in general,

46
00:02:54.725 --> 00:02:59.130
make our vehicles in that sense more valuable.
WEBVTT

1
00:00:02.180 --> 00:00:05.380
In this hands on activity,
we will be performing queries in Splunk.

2
00:00:06.620 --> 00:00:09.980
First, we will open a browser and
login to Splunk.

3
00:00:09.980 --> 00:00:13.590
Next, we will import a CSV file and
search its contents.

4
00:00:13.590 --> 00:00:18.030
We will see how to filter fields for
specific values and

5
00:00:18.030 --> 00:00:20.532
also perform statistical
calculations on the data.

6
00:00:20.532 --> 00:00:23.678
Let's begin.

7
00:00:23.678 --> 00:00:28.236
First, open a web browser and
navigate to the Splunk web page.

8
00:00:28.236 --> 00:00:33.103
We'll enter localhost:8000.

9
00:00:36.793 --> 00:00:41.870
Next we'll log in to Splunk using
admin and the default password.

10
00:00:51.160 --> 00:00:55.231
Next, we'll import a CSV file into Splunk.

11
00:00:55.231 --> 00:00:56.952
We'll click on Add Data.

12
00:01:00.159 --> 00:01:05.704
Upload We'll click on Select File.

13
00:01:08.366 --> 00:01:11.838
And we'll choose the census.csv
file that we downloaded.

14
00:01:16.364 --> 00:01:17.583
Click Next.

15
00:01:22.148 --> 00:01:25.010
On the left,
it should say source type csv.

16
00:01:25.010 --> 00:01:30.066
If it does not, click on the button and

17
00:01:30.066 --> 00:01:35.128
go down to Structured and select csv.

18
00:01:35.128 --> 00:01:37.290
In this table,
we see a preview of the data.

19
00:01:39.050 --> 00:01:42.100
You should see the column names
of the CSV file at the top.

20
00:01:44.260 --> 00:01:45.360
Click on Next.

21
00:01:48.372 --> 00:01:49.888
Review.

22
00:01:51.026 --> 00:01:52.227
And Submit.

23
00:01:54.602 --> 00:01:58.797
Now that the file has been imported
successfully, click on Start Searching.

24
00:02:04.511 --> 00:02:09.539
In the search box,
it fills in the default query,

25
00:02:09.539 --> 00:02:16.260
source="cencus.csv" the host name and
sourcetype="csv"

26
00:02:18.161 --> 00:02:22.067
We could change these fields
to search other files or

27
00:02:22.067 --> 00:02:25.894
other data types if we
imported those into Splunk.

28
00:02:25.894 --> 00:02:31.135
Now let's search census.csv for
particular values in the fields.

29
00:02:31.135 --> 00:02:34.478
Let's search for all the data
where the state is California.

30
00:02:37.251 --> 00:02:40.908
We'll enter STNAME="California".

31
00:02:46.621 --> 00:02:50.521
You'll see the results down here.

32
00:02:50.521 --> 00:02:53.604
You could search for
other states by using or.

33
00:02:53.604 --> 00:02:57.581
For example, we can add OR
STNAME="Alaska".

34
00:02:57.581 --> 00:03:01.840
This will search for
state names equal to California or Alaska.

35
00:03:05.450 --> 00:03:07.270
We can add conditions
to our query as well.

36
00:03:07.270 --> 00:03:14.930
Let's search for state equals California
whose population was over one million.

37
00:03:14.930 --> 00:03:21.467
We'll do this by saying
STNAME = "California"

38
00:03:21.467 --> 00:03:26.925
CENSUS2010POP > 1000000.

39
00:03:32.477 --> 00:03:35.900
Can limit the results to one or
more columns.

40
00:03:35.900 --> 00:03:42.446
You do this by adding pipe table
CTYNAME to the end of our query.

41
00:03:47.573 --> 00:03:52.336
In Spunk queries, the pipe command is used
to send the outputs from the first part of

42
00:03:52.336 --> 00:03:53.778
the query into the next.

43
00:03:56.139 --> 00:03:58.441
You can also show more than
one column from the output.

44
00:03:58.441 --> 00:04:02.982
If we add a comma CENSUS2010POP
to the end of this,

45
00:04:02.982 --> 00:04:07.145
we'll see both the city name and
the population.

46
00:04:07.145 --> 00:04:10.944
You can also see a visualization of this
data by clicking on the Visualization tab.

47
00:04:13.883 --> 00:04:18.344
At the bottom, on the x-axis,
we see the county names, and

48
00:04:18.344 --> 00:04:21.420
the y values are the population numbers.

49
00:04:24.300 --> 00:04:27.994
Now let's perform some
statistics on this data.

50
00:04:27.994 --> 00:04:31.630
We'll begin by counting the number of
records where the state is California.

51
00:04:33.570 --> 00:04:39.951
You can do this by saying
STNAME="California" pipe stats count.

52
00:04:44.282 --> 00:04:49.468
Now switch back to the statistics tab,
see the result here

53
00:04:52.360 --> 00:04:55.775
Now let's see the total population for
California.

54
00:04:55.775 --> 00:05:01.241
You can replace count
with sum(CENUS2010POP)

55
00:05:05.290 --> 00:05:07.923
You can also calculate
the average population.

56
00:05:07.923 --> 00:05:09.781
We'll replace sum with mean.
WEBVTT

1
00:00:01.970 --> 00:00:06.280
Hello, My name is Mitch Fleichmann,
senior instructor here at Splunk.

2
00:00:06.280 --> 00:00:08.670
Today we are going to
install Splunk on Linux.

3
00:00:10.230 --> 00:00:13.760
I'm using one of
the Splunk Education Linux servers.

4
00:00:13.760 --> 00:00:14.564
Let's examine the environment.

5
00:00:17.146 --> 00:00:21.040
First of all,
notice I'm logged in as the user splunker.

6
00:00:21.040 --> 00:00:24.600
As a best practice,
do not install Splunk as the root user.

7
00:00:27.730 --> 00:00:33.498
I'm currently in the /opt directory,
where I've already downloaded

8
00:00:33.498 --> 00:00:38.426
the tarball from splunk.com/download for
this platform.

9
00:00:40.954 --> 00:00:45.755
As a final check, let's check the system
to make sure we have the correct operating

10
00:00:45.755 --> 00:00:46.932
system and kernel.

11
00:00:49.680 --> 00:00:56.072
This is indeed a Linux machine, let's
get confirmation that it is also 64 bit.

12
00:01:02.580 --> 00:01:04.090
So we are good to go.

13
00:01:04.090 --> 00:01:07.519
The next step is to unzip and
untar the installer and

14
00:01:07.519 --> 00:01:10.729
that we can do with the gunzip and
tar commands.

15
00:01:18.205 --> 00:01:23.650
When everything untucks, you'll notice
a new sub directory created named splunk.

16
00:01:24.960 --> 00:01:28.670
And we can navigate to the splunk/bin
directory to start up Splunk.

17
00:01:31.450 --> 00:01:36.000
Couple of ways to start
Splunk with no switches.

18
00:01:37.330 --> 00:01:41.890
On the first startup, you'll be prompted
to read and agree to the software license.

19
00:01:43.610 --> 00:01:47.141
Or as a shortcut, you can start
up Splunk and accept the license.

20
00:01:55.707 --> 00:02:00.315
And notice a couple of port numbers
being grabbed, port 8000 for

21
00:02:00.315 --> 00:02:03.902
splunk web and port 8089 for
splunk management.

22
00:02:08.003 --> 00:02:09.528
The splunk daemon has started.

23
00:02:12.675 --> 00:02:14.603
Splunk is generating its own keys.

24
00:02:17.707 --> 00:02:21.520
And we can see the splunk web interface,
the URL.

25
00:02:26.050 --> 00:02:26.950
As a best practice,

26
00:02:26.950 --> 00:02:31.050
you may also want to consider automating
splunk to start when the machine boots.

27
00:02:32.060 --> 00:02:37.635
That you can do as the root user.

28
00:02:37.635 --> 00:02:41.841
By issuing the splunk enable command,

29
00:02:41.841 --> 00:02:46.686
enable boot-start as the -user splunker,

30
00:02:46.686 --> 00:02:51.793
the same user that we
just consult splunk with.

31
00:03:00.498 --> 00:03:03.960
So lets log in to splunk web and
see how the system looks.

32
00:03:03.960 --> 00:03:05.410
And for that we'll go back to the browser.

33
00:03:07.980 --> 00:03:09.230
Go to the appropriate URL.

34
00:03:13.340 --> 00:03:15.360
And we can see upon first login,

35
00:03:15.360 --> 00:03:20.730
you are prompted to login with
the credentials admin, password changeme.

36
00:03:28.180 --> 00:03:29.780
Since this is the first login,

37
00:03:29.780 --> 00:03:33.170
you also coach to change your
password to something more secure.

38
00:03:33.170 --> 00:03:36.218
And it's highly recommended to
follow this best practice as well.

39
00:03:46.435 --> 00:03:48.027
And also on first connection,

40
00:03:48.027 --> 00:03:51.715
you'll also see a splash screen
showing you what's new in version 6.

41
00:03:53.160 --> 00:03:54.950
In this case, Powerful Analytics.

42
00:03:56.190 --> 00:03:58.790
Some changes to the UI to
make it more intuitive.

43
00:04:01.400 --> 00:04:04.919
Simplified component management for
cluster management,

44
00:04:04.919 --> 00:04:06.647
folder management and so on.

45
00:04:08.737 --> 00:04:10.651
Also a richer developer experience.

46
00:04:14.147 --> 00:04:17.560
Close down this window and
you can explore the navigation options.

47
00:04:18.940 --> 00:04:20.410
And notice in the left side,

48
00:04:20.410 --> 00:04:23.470
you see a panel showing you
the apps to navigate to and manage.

49
00:04:24.960 --> 00:04:29.060
And on the right side, you see some
panel showing you data in the system and

50
00:04:29.060 --> 00:04:30.380
various links for help.

51
00:04:32.730 --> 00:04:34.936
Let's go to the search and
reporting app by clicking here.

52
00:04:39.527 --> 00:04:43.879
And you can see the search far up top,

53
00:04:43.879 --> 00:04:49.788
some tips on how to search and
then data to search.

54
00:04:49.788 --> 00:04:53.720
Since this is a fresh install,
there is no data to search, so

55
00:04:53.720 --> 00:04:57.049
the next step is to index data and
begin searching.

56
00:04:57.049 --> 00:04:57.890
Good luck.
WEBVTT

1
00:00:00.025 --> 00:00:04.660
[SOUND] Hello, this is Chris Busheers,

2
00:00:04.660 --> 00:00:08.881
part of the Splunk education team.

3
00:00:08.881 --> 00:00:13.530
In this video, I'll show you how
install Splunk onto a Window's server.

4
00:00:13.530 --> 00:00:16.869
First we need to get the software
from the splunk.com download page.

5
00:00:18.010 --> 00:00:22.710
We will need to select that
the platform is 32 or 64-bit.

6
00:00:22.710 --> 00:00:25.605
If you aren't sure if your system is 32 or

7
00:00:25.605 --> 00:00:28.200
64-bit, you can check
your system properties.

8
00:00:29.320 --> 00:00:34.750
As you can see, this server is 64-bit,
so we can install that version.

9
00:00:34.750 --> 00:00:39.720
If we saw a system type of 32-bit,
we would download the 32-bit version.

10
00:00:40.720 --> 00:00:43.020
We run the installer by
double clicking on it.

11
00:00:44.220 --> 00:00:48.920
There is button to view the license
agreement, and a check box to accept it.

12
00:00:50.310 --> 00:00:53.800
At this point we can either install
Splunk with the defaults, or

13
00:00:53.800 --> 00:00:55.160
customize our installation.

14
00:00:56.290 --> 00:01:00.379
Let's click on the customize options
to see what settings can be selected.

15
00:01:01.660 --> 00:01:05.860
The first option is to change
the installation location of Splunk.

16
00:01:05.860 --> 00:01:09.940
We are fine with this location,
so we click Next.

17
00:01:09.940 --> 00:01:15.100
Now we must choose what account type
to install Splunk as, Local System or

18
00:01:15.100 --> 00:01:15.830
Domain Account.

19
00:01:17.070 --> 00:01:21.420
A Local System account will allow
Splunk to access all data on, or

20
00:01:21.420 --> 00:01:23.550
forwarded, to this machine.

21
00:01:23.550 --> 00:01:26.140
A Domain Account will allow
you to collect logs and

22
00:01:26.140 --> 00:01:31.070
metrics from remote machines as
well as local and forwarded data.

23
00:01:31.070 --> 00:01:35.010
You are required to provide a Domain
Account with the proper domain rights

24
00:01:35.010 --> 00:01:36.730
to use this type.

25
00:01:36.730 --> 00:01:40.530
Local System works well for
us, so we click Next.

26
00:01:40.530 --> 00:01:45.012
We can select to have a shortcut to Splunk
added, and click Install to continue.

27
00:01:45.012 --> 00:01:48.319
[MUSIC]

28
00:01:48.319 --> 00:01:52.750
Once installed, we can select to have
Splunk launch, and click Finish.

29
00:01:52.750 --> 00:01:55.720
This Splunk web interface
opens in our default browser.

30
00:01:56.750 --> 00:02:01.130
We enter the default user name of admin,
and a password of change made.

31
00:02:01.130 --> 00:02:04.400
A dialog box appears asking
us to change our password.

32
00:02:04.400 --> 00:02:07.310
It is always best practice to do this.

33
00:02:07.310 --> 00:02:10.820
Once logged in, we are taken to
the Splunk launcher homepage.

34
00:02:10.820 --> 00:02:13.780
And that's all it takes to get
Splunk installed on Windows.

35
00:02:13.780 --> 00:02:15.895
Now dig in, and start exploring.

36
00:02:15.895 --> 00:02:21.000
[SOUND]
WEBVTT

1
00:00:03.530 --> 00:00:04.180
Our third and

2
00:00:04.180 --> 00:00:09.080
final case is applicable to most companies
that create customer-focused products.

3
00:00:11.030 --> 00:00:15.270
They want to understand how their
customers are responding to the products,

4
00:00:15.270 --> 00:00:17.800
how the product marketing
efforts are performing,

5
00:00:17.800 --> 00:00:21.470
what kind of problems customers
are encountering, and what new features or

6
00:00:21.470 --> 00:00:24.330
feature improvements the customers
are seeking, and so forth.

7
00:00:25.380 --> 00:00:28.010
But how does the company
get this information?

8
00:00:28.010 --> 00:00:31.060
What kind of data sources
would carry this information?

9
00:00:31.060 --> 00:00:33.710
The figure show some of these sources.

10
00:00:33.710 --> 00:00:40.210
They are in focused user surveys,
emails sent by the customers, in blogs and

11
00:00:40.210 --> 00:00:46.350
product review forums, specialized
groups on social media and user forums.

12
00:00:46.350 --> 00:00:52.990
In short, they are on the Internet or
in material received through the Internet.

13
00:00:52.990 --> 00:00:54.250
Now, how many sources are there?

14
00:00:55.830 --> 00:00:57.200
Two.

15
00:00:57.200 --> 00:00:58.940
The number would vary.

16
00:00:58.940 --> 00:01:01.060
A new sites, a new postings, and

17
00:01:01.060 --> 00:01:02.950
new discussion threads
would come up all the time.

18
00:01:04.000 --> 00:01:07.870
In all of these,
the goal is to identify information that

19
00:01:07.870 --> 00:01:12.130
truly relates to the companies product,
its features and its utility.

20
00:01:14.470 --> 00:01:17.530
To cast this as a type
of big data problem,

21
00:01:17.530 --> 00:01:20.871
we look at a task that computer
scientists called Data Fusion.

22
00:01:22.690 --> 00:01:27.840
Consider a set of data sources, S,
as we mentioned on the last slide and

23
00:01:27.840 --> 00:01:29.320
a set of data items, D.

24
00:01:30.630 --> 00:01:35.420
A data item represents a particular
aspect of a real world entity

25
00:01:35.420 --> 00:01:37.290
which in our case is
a product of the company.

26
00:01:39.180 --> 00:01:44.860
For each data item, a source can, but
not necessarily will, provide a value.

27
00:01:44.860 --> 00:01:46.020
For example,

28
00:01:46.020 --> 00:01:51.450
the usability of an ergonomically
split keyboard can have a value good.

29
00:01:52.590 --> 00:01:57.500
The value can be atomic,
like good, or a set, or a list or

30
00:01:57.500 --> 00:01:58.910
sometimes embedded in the string.

31
00:02:00.370 --> 00:02:04.270
For example, the cursor sometimes
freezes when using the touchpad,

32
00:02:05.410 --> 00:02:09.300
is a string which has
a value about the touchpad.

33
00:02:11.400 --> 00:02:16.130
The goal of Data Fusion is to find
the values of Data Items from a source.

34
00:02:18.060 --> 00:02:23.160
In many cases, the system would find
a unique true value of an item.

35
00:02:23.160 --> 00:02:27.580
For example, the launch data of a product
in Europe should be the same true value

36
00:02:27.580 --> 00:02:29.530
regardless of the data
source one looks at.

37
00:02:30.700 --> 00:02:34.520
In other cases, we could find
a value distribution of an item.

38
00:02:34.520 --> 00:02:37.960
For example, the usability of our
keyboard may have a value distribution.

39
00:02:39.070 --> 00:02:43.960
That's with Data Fusion, we should be
able to collect the values of real world

40
00:02:43.960 --> 00:02:46.790
items from a subset of data sources.

41
00:02:46.790 --> 00:02:49.850
It is a subset because
not all Data Sources

42
00:02:49.850 --> 00:02:51.940
will have relevant information
about the Data Item.

43
00:02:53.440 --> 00:02:56.212
There are some other versions
of what a Data Fusion is but for

44
00:02:56.212 --> 00:02:58.992
our purposes we'll stick with
this general description.

45
00:03:01.190 --> 00:03:05.856
Now one obvious problem with the Internet
is that there are too many data

46
00:03:05.856 --> 00:03:09.530
sources at any time,
these lead to many difficulties.

47
00:03:10.910 --> 00:03:14.570
First, it is to be understood
that with too many data sources

48
00:03:14.570 --> 00:03:17.310
there will be many values for
the same item.

49
00:03:18.530 --> 00:03:21.190
Often these will differ and
sometimes they will conflict.

50
00:03:22.580 --> 00:03:25.740
A standard technique in this case
is to use a voting mechanism.

51
00:03:27.200 --> 00:03:31.820
However, even a voting
mechanism can be complex

52
00:03:31.820 --> 00:03:33.350
due to problems with the data source.

53
00:03:35.040 --> 00:03:38.800
One of the problems is to estimate
the trustworthiness of the source.

54
00:03:40.130 --> 00:03:42.080
For each data source,

55
00:03:42.080 --> 00:03:48.260
we need to evaluate whether it's reporting
some basic or known facts correctly.

56
00:03:48.260 --> 00:03:51.510
If a source mentions details
about a rainbow colored iPhone,

57
00:03:51.510 --> 00:03:55.010
which does not exist,
it's trustworthiness reduces

58
00:03:55.010 --> 00:03:57.960
because of the falsity of
the provided value of this data item.

59
00:03:59.270 --> 00:04:03.140
Accordingly, a higher vote count can be
assigned to a more trustworthy source.

60
00:04:04.590 --> 00:04:07.640
And then, this can be used in voting.

61
00:04:09.560 --> 00:04:11.260
The second aspect is Copy Detection.

62
00:04:12.700 --> 00:04:16.670
Detecting weather once was has copied
information from another can be very

63
00:04:16.670 --> 00:04:19.490
important for
detail fusion task in customer analytics.

64
00:04:20.660 --> 00:04:22.680
If a source has copied information,

65
00:04:23.750 --> 00:04:28.900
it's such that discounted vote count
can be assigned to a copy value and

66
00:04:28.900 --> 00:04:34.190
voting that means the copy in
source will have less weight.

67
00:04:35.210 --> 00:04:40.160
Now this is especially relevant when we
compute value distributions, because if we

68
00:04:40.160 --> 00:04:45.680
treat copies as genuine information, we
will statistically bias the distribution.

69
00:04:45.680 --> 00:04:50.279
Now here is active research on how to
detect copies, how to determine bias and

70
00:04:50.279 --> 00:04:54.685
then arrive at a statistically sound
estimation of value distribution.

71
00:04:54.685 --> 00:04:59.628
But to our knowledge, these methods are
yet to be applied to existing software for

72
00:04:59.628 --> 00:05:01.070
big data integration.

73
00:05:04.473 --> 00:05:06.240
It should be very clear by now but

74
00:05:06.240 --> 00:05:10.240
there are two kinds of big data
situations when it comes to information.

75
00:05:11.400 --> 00:05:16.180
The first two uses cases that we
saw requires an integration system

76
00:05:16.180 --> 00:05:20.380
to consider all sources because
the application demand so.

77
00:05:21.620 --> 00:05:27.660
In contrast, problems where data comes
from too many redundant, potentially

78
00:05:27.660 --> 00:05:32.380
unreliable sources like the Internet, the
best results can be obtained if we have

79
00:05:32.380 --> 00:05:36.490
a way of evaluating the worthiness of
sources before information integration.

80
00:05:37.660 --> 00:05:40.790
But this problem is
called Source Selection.

81
00:05:40.790 --> 00:05:44.570
The picture on the right shows the result
of a cost benefit analysis for

82
00:05:44.570 --> 00:05:46.080
data fusion.

83
00:05:46.080 --> 00:05:49.239
The x-axis indicates the number
of sources used, and

84
00:05:49.239 --> 00:05:53.400
the y-axis measures the proportion
of true results that were returned.

85
00:05:55.090 --> 00:05:59.480
We can clearly see that the plot peaks
around six-to-eight sources, and

86
00:05:59.480 --> 00:06:01.990
that the efficiency falls
as more sources are added.

87
00:06:03.790 --> 00:06:08.930
In a cost benefit analysis,
the cost must include both the human and

88
00:06:08.930 --> 00:06:10.580
the computational costs,

89
00:06:10.580 --> 00:06:14.270
while the benefit is a function of
the accuracy of the fusion result.

90
00:06:14.270 --> 00:06:19.210
The technique for
solving this problem comes from economics.

91
00:06:20.520 --> 00:06:23.870
Assuming that cost and
benefits are measure in the same unit, for

92
00:06:23.870 --> 00:06:24.660
example, dollars.

93
00:06:25.780 --> 00:06:28.580
They proposed to continue
selecting sources

94
00:06:28.580 --> 00:06:32.740
until the marginal benefit is
less than the marginal cost.

95
00:06:34.150 --> 00:06:38.210
Now recent techniques were performing
this computation at quite scalable.

96
00:06:38.210 --> 00:06:41.910
In one setting,
selecting the most beneficial sources

97
00:06:41.910 --> 00:06:45.520
from a total of one million
sources took less than one hour.

98
00:06:47.900 --> 00:06:51.980
This completes our coverage of
the big data integration problems.
WEBVTT

1
00:00:00.830 --> 00:00:02.180
Hello and welcome.

2
00:00:02.180 --> 00:00:07.180
My name is Rene and I'll be sharing with
you, how to create a report using Pivot.

3
00:00:07.180 --> 00:00:11.920
To access the Pivot interface,
click Pivot on the navigation menu.

4
00:00:13.630 --> 00:00:17.370
The first step is to select
a prebuilt data model.

5
00:00:17.370 --> 00:00:21.260
Now the data model allows you
to create compelling reports and

6
00:00:21.260 --> 00:00:26.090
dashboards without having to know
how to write complex search queries.

7
00:00:26.090 --> 00:00:31.000
As a side note data models are typically
created by a knowledge manager that

8
00:00:31.000 --> 00:00:34.200
understands their
organization's index data.

9
00:00:34.200 --> 00:00:38.760
Grasps the search language and
are familiar with lookups,

10
00:00:38.760 --> 00:00:42.640
transactions, field extractions and
calculated fields.

11
00:00:43.750 --> 00:00:48.260
For our example, we're going to
use Buttercup Games Online Sales.

12
00:00:50.200 --> 00:00:54.960
Now, this data model includes
the online sales activities.

13
00:00:54.960 --> 00:00:58.100
It's made up of nine objects.

14
00:00:58.100 --> 00:01:03.660
Each object represents a specific set
of events in a hierarchical structure.

15
00:01:05.290 --> 00:01:12.390
The http request would include the largest
number of events in this data model.

16
00:01:12.390 --> 00:01:15.540
The next object, successful request,

17
00:01:15.540 --> 00:01:20.950
would be a subset of the http
request events and so on.

18
00:01:20.950 --> 00:01:25.710
Let's say that we want to create a report
for purchases over the last seven days.

19
00:01:25.710 --> 00:01:28.560
We would select the most
appropriate object.

20
00:01:28.560 --> 00:01:32.200
In this case we're going to
select successful purchase.

21
00:01:33.910 --> 00:01:37.940
After you have selected the object
you will notice that a new

22
00:01:37.940 --> 00:01:39.460
Pivot view will display.

23
00:01:40.720 --> 00:01:45.950
If you notice, the count of
successful purchase is the total

24
00:01:45.950 --> 00:01:50.580
number of events for this specific object.

25
00:01:50.580 --> 00:01:52.780
Now, it is based off of all time.

26
00:01:54.470 --> 00:01:59.840
Before you begin, you should have an idea
of what type of report you want to create.

27
00:01:59.840 --> 00:02:02.720
So let's say that we
want to create a bar chart.

28
00:02:02.720 --> 00:02:08.120
We're going to select that
from the left navigation pane.

29
00:02:08.120 --> 00:02:12.375
So the third icon will allow me
to click and select Bar Chart.

30
00:02:14.278 --> 00:02:18.130
So at this point we are ready
to start building our report.

31
00:02:19.560 --> 00:02:21.310
The first selection is Time Range.

32
00:02:21.310 --> 00:02:24.480
Let's set that to the last seven days.

33
00:02:26.030 --> 00:02:30.780
The next one is Filter, now we don't need
to define Filter, because we've already

34
00:02:30.780 --> 00:02:36.210
selected an object of successful purchase
and that's what we're reporting against.

35
00:02:37.720 --> 00:02:42.570
Now, for my x axis,
I'm going to select product name.

36
00:02:43.620 --> 00:02:46.040
You'll notice that I can set the label.

37
00:02:46.040 --> 00:02:48.170
I can set the sort order.

38
00:02:48.170 --> 00:02:51.091
I can even set how many
maximum bars that I want.

39
00:02:52.728 --> 00:02:56.988
The next thing to define is the y axis.

40
00:02:56.988 --> 00:03:01.970
At this time it's set to
count of successful purchase.

41
00:03:01.970 --> 00:03:05.325
But I want the total sales by product.

42
00:03:05.325 --> 00:03:07.785
So I'm going to select price and

43
00:03:07.785 --> 00:03:12.655
then you'll notice that my
value is already set to sum.

44
00:03:12.655 --> 00:03:14.475
So I'm going to keep it as sum.

45
00:03:16.340 --> 00:03:18.030
My report is starting to look really good.

46
00:03:18.030 --> 00:03:25.370
But after reviewing it I might decide that
I want to see values segmented by host.

47
00:03:25.370 --> 00:03:30.670
So let's do that,
in order to set the segmentation

48
00:03:30.670 --> 00:03:38.290
of a specific product name I could go and
set the color to the field that I want.

49
00:03:38.290 --> 00:03:39.824
So I'm going to select Host.

50
00:03:42.936 --> 00:03:49.490
Now, you'll see that my total
sales are segmented by host.

51
00:03:50.966 --> 00:03:52.566
So we really like this report.

52
00:03:52.566 --> 00:03:54.190
Let's go ahead and save it.

53
00:03:55.240 --> 00:03:57.860
So we're going to click on Save As.

54
00:03:59.050 --> 00:04:01.560
Select report and give it a title.

55
00:04:01.560 --> 00:04:06.810
Let's call it Product Sales by Host.

56
00:04:06.810 --> 00:04:13.684
We're going to click Save and
then we're going to view our report.

57
00:04:16.563 --> 00:04:19.880
It was that easy to create
a report through Pivot.

58
00:04:21.850 --> 00:04:26.030
Now, let's just say that we want to go and
work with another Pivot.

59
00:04:26.030 --> 00:04:28.810
This time we're going to just go and

60
00:04:28.810 --> 00:04:32.070
create a statistic table
to view some of that data.

61
00:04:32.070 --> 00:04:34.917
Let's go ahead and click Pivot again.

62
00:04:36.700 --> 00:04:39.090
We'll select the same data model.

63
00:04:39.090 --> 00:04:43.637
Buttercup games online sales and
let's keep to the object that

64
00:04:43.637 --> 00:04:47.855
we're familiar with right now,
successful purchases.

65
00:04:54.902 --> 00:04:59.920
As I mentioned, I want to just
simply create a statistic table.

66
00:05:00.990 --> 00:05:03.400
I want to show you how
we'd go about doing that.

67
00:05:04.620 --> 00:05:11.700
So the first step is to think about what
are some of the row data that you want?

68
00:05:11.700 --> 00:05:18.790
Let's just assume that we want to view
the product names within their categories.

69
00:05:18.790 --> 00:05:24.500
So, let's go ahead, under Split Rows,
we're going to select category first.

70
00:05:25.530 --> 00:05:28.060
Notice that I can set a label if I want.

71
00:05:28.060 --> 00:05:32.500
Let's go ahead and
set it capitalized category.

72
00:05:32.500 --> 00:05:33.897
I'm going to add that to my table.

73
00:05:33.897 --> 00:05:38.403
You'll see that it
becomes the first column.

74
00:05:38.403 --> 00:05:42.240
Now I want to add another column.

75
00:05:42.240 --> 00:05:46.535
Let's go ahead and
click the plus right next to categories.

76
00:05:46.535 --> 00:05:49.995
Again we're still working
with the split rows.

77
00:05:51.065 --> 00:05:56.375
I'm going to select product name, again if
you want to set a label you can do that.

78
00:05:56.375 --> 00:05:58.955
Now this is the label of our column.

79
00:06:00.175 --> 00:06:00.675
Add to table.

80
00:06:04.367 --> 00:06:09.282
And I want you to notice that
now we are viewing each product

81
00:06:09.282 --> 00:06:11.400
within their category.

82
00:06:12.580 --> 00:06:16.130
Now at this point if you look at
the statistic that is being defined,

83
00:06:16.130 --> 00:06:19.850
it's the count of successful purchases.

84
00:06:19.850 --> 00:06:22.570
That is defined as column values.

85
00:06:23.940 --> 00:06:28.960
Now if I want to add to this,
I can append and have another column.

86
00:06:28.960 --> 00:06:34.050
Let's go ahead and click the plus next
to the Count of successful purchase.

87
00:06:35.480 --> 00:06:37.300
This time I want to add the price.

88
00:06:38.520 --> 00:06:41.216
And you'll notice that I have
different values available.

89
00:06:41.216 --> 00:06:44.662
I could do a Sum, a Count,
an Average, Max, Min.

90
00:06:44.662 --> 00:06:51.274
Let's keep it as a Sum and
let's add a label of Total Sales.

91
00:06:51.274 --> 00:06:53.795
And let's Add To Table.

92
00:06:55.955 --> 00:06:59.675
So this is just to show you
how quick it is to go and

93
00:06:59.675 --> 00:07:03.955
build a table with the statistics
that you would need.

94
00:07:03.955 --> 00:07:07.977
Now at this point, if ever you decide,
I only want to view the data for

95
00:07:07.977 --> 00:07:09.260
the last seven days.

96
00:07:09.260 --> 00:07:13.590
Just note that you could go
back up to your filter and

97
00:07:13.590 --> 00:07:16.630
your first filter is
based off of all time.

98
00:07:16.630 --> 00:07:20.870
So we could set that to
the last seven days.

99
00:07:20.870 --> 00:07:23.610
So of course that the data
in our table will change.

100
00:07:24.750 --> 00:07:29.030
Now, the last thing I want to show you
here is if ever you did want to see

101
00:07:30.120 --> 00:07:37.075
the count and the total sales per host,
as we've defined in our previous example.

102
00:07:37.075 --> 00:07:41.615
That is where you could go
to the split columns and

103
00:07:41.615 --> 00:07:44.325
define how you wish to
split that information.

104
00:07:44.325 --> 00:07:47.825
So let's go ahead and select host again.

105
00:07:47.825 --> 00:07:50.731
And I'm just going to
click on add to table.

106
00:07:55.074 --> 00:08:02.820
Now I could see that the information
has been split by host.

107
00:08:02.820 --> 00:08:08.460
I have www1 count of successful
purchases as well as total sales.

108
00:08:08.460 --> 00:08:15.810
And then I have the split of www2 I have
the count and the total sales and etc.

109
00:08:15.810 --> 00:08:20.950
So this concludes our short
video hope you enjoy it.

110
00:08:20.950 --> 00:08:21.450
Thank you.
WEBVTT

1
00:00:01.860 --> 00:00:03.350
Welcome.

2
00:00:03.350 --> 00:00:08.570
In this short module we'll talk about
information integration which refers

3
00:00:08.570 --> 00:00:13.470
to the problem of using many different
information sources to accomplish a task.

4
00:00:14.780 --> 00:00:19.939
In this module, we'll look at the problems
and solutions through a few use cases.

5
00:00:22.330 --> 00:00:27.830
So after this video, you'll be able to
explain the data integration problem,

6
00:00:29.250 --> 00:00:34.930
define integrated views and schema
mapping, describe the impact of increasing

7
00:00:34.930 --> 00:00:40.306
the number of data sources, appreciate
the need to use data compression,

8
00:00:40.306 --> 00:00:46.910
And describe record linking,
data exchange, and data fusion tasks.

9
00:00:50.008 --> 00:00:54.230
Our first use case starts with
an example given at an IBM website for

10
00:00:54.230 --> 00:00:55.760
their information integration products.

11
00:00:57.150 --> 00:01:00.580
It represents a very common
scenario in today's business world.

12
00:01:02.100 --> 00:01:06.040
Due to the changing market dynamics,
companies

13
00:01:06.040 --> 00:01:09.620
are always selling off a part of their
company or acquiring another company.

14
00:01:11.330 --> 00:01:15.950
As these mergers and acquisitions happen,
databases which were developed and

15
00:01:15.950 --> 00:01:19.900
stored separately in different companies
would now need to be brought together.

16
00:01:21.680 --> 00:01:23.640
Now take a minute to read this case.

17
00:01:28.134 --> 00:01:32.644
This is the case of an expanding financial
services group that's growing its customer

18
00:01:32.644 --> 00:01:34.260
base in different countries.

19
00:01:36.420 --> 00:01:41.980
And all they want is a single view
of their entire customer base.

20
00:01:41.980 --> 00:01:46.590
In other words, it does not matter
which previous company originally had

21
00:01:46.590 --> 00:01:51.920
the customers, Suncorp-Metway
want to consolidate all customer

22
00:01:51.920 --> 00:01:58.130
information as if they were
in one single database.

23
00:01:58.130 --> 00:02:02.223
And in reality, of course, they may
not want to buy a huge machine and

24
00:02:02.223 --> 00:02:05.340
migrate every subsidiary
company's data into it.

25
00:02:06.680 --> 00:02:12.338
What they're looking to create is possibly
a software solution which would make all

26
00:02:12.338 --> 00:02:18.710
customer-related data to appear as though
they were together as a single database.

27
00:02:18.710 --> 00:02:22.290
This software solution is called
an information integration system.

28
00:02:23.590 --> 00:02:27.920
This will help them ensure that they have
a uniform set of marketing campaigns for

29
00:02:27.920 --> 00:02:29.040
all their customers.

30
00:02:31.780 --> 00:02:36.893
Let's try to see, hypothetically, of
course, what might be involved in creating

31
00:02:36.893 --> 00:02:41.511
this combined data and what kind of use
the integrated data might result in.

32
00:02:41.511 --> 00:02:44.330
So we first create
a hypothetical scenario.

33
00:02:45.552 --> 00:02:48.920
Although Suncorp have a large
number of data sources,

34
00:02:48.920 --> 00:02:51.290
we will take a much simpler situation and

35
00:02:51.290 --> 00:02:55.610
have only two data sources from two
different financial service companies.

36
00:02:57.010 --> 00:02:58.190
The first data source,

37
00:02:58.190 --> 00:03:03.090
which is an insurance company that manages
it's data with a relation of DBMS,

38
00:03:03.090 --> 00:03:08.660
this database has nine tables where the
primary object of information is a policy.

39
00:03:10.040 --> 00:03:13.090
The company offers many
different types of policies

40
00:03:13.090 --> 00:03:16.380
sold to individual people by their agents.

41
00:03:16.380 --> 00:03:20.740
Now as it's true for all insurance
companies, policyholders pay their monthly

42
00:03:20.740 --> 00:03:26.130
dues, and sometimes people make claims
against their insurance policies.

43
00:03:26.130 --> 00:03:30.080
When they do, the details of the claims
are maintained in the database.

44
00:03:31.170 --> 00:03:35.220
These claims can belong to different
categories, and when the claims have

45
00:03:35.220 --> 00:03:39.780
paid to the claimants, the transaction
is recorded in the transactions table.

46
00:03:41.350 --> 00:03:43.640
As we have done several times now,

47
00:03:43.640 --> 00:03:46.930
the primary keys of the table
are the underlined attributes here.

48
00:03:48.900 --> 00:03:51.000
The second company in
our example is a bank,

49
00:03:52.060 --> 00:03:53.850
which also uses a relational database.

50
00:03:55.240 --> 00:03:59.500
In this bank, both individuals and
businesses called corporations here,

51
00:03:59.500 --> 00:04:00.140
can have accounts.

52
00:04:02.900 --> 00:04:05.050
Now accounts can be of different types.

53
00:04:05.050 --> 00:04:07.900
For example, a money market account
is different from a savings account.

54
00:04:09.330 --> 00:04:13.910
A bank also maintains its transactions
in a table, which can be really large.

55
00:04:15.620 --> 00:04:20.050
But the dispute in a bank record
case happens when the bank is

56
00:04:20.050 --> 00:04:24.690
charged a customer, or the customer has
declined responsibility of the charge.

57
00:04:24.690 --> 00:04:29.030
This can happen, for example, if a
customer's Internet account was hacked or

58
00:04:29.030 --> 00:04:30.260
a debit card got stolen.

59
00:04:31.700 --> 00:04:33.800
The bank keeps a record
of these anomalies and

60
00:04:33.800 --> 00:04:38.260
fraudulent events in a disputes table,
all right.

61
00:04:38.260 --> 00:04:42.730
Let's see what happens after the data
from these two subsidiary companies

62
00:04:42.730 --> 00:04:43.640
are integrated.

63
00:04:46.410 --> 00:04:50.970
After the merger the company wants to
do a promotional goodwill activity.

64
00:04:51.970 --> 00:04:54.490
They would like to offer
a small discount to their

65
00:04:54.490 --> 00:04:59.130
insurance policyholders if they're also
customers of the newly acquired bank.

66
00:05:00.350 --> 00:05:02.650
How do you identify these customers?

67
00:05:02.650 --> 00:05:03.960
Let's see.

68
00:05:03.960 --> 00:05:08.882
In other words, we need to use the table
shown on the left to create the table

69
00:05:08.882 --> 00:05:12.103
shown on the right called
discount candidates.

70
00:05:12.103 --> 00:05:16.547
One is to create a yellow tables from
the insurance company database, and

71
00:05:16.547 --> 00:05:21.351
the blue table from the bank database,
and then join them to construct the table

72
00:05:21.351 --> 00:05:25.960
with a common customer ID, and both
the policyKey and bank account number.

73
00:05:27.290 --> 00:05:31.600
Now, this relation, which is derived that
is computed by querying two different

74
00:05:31.600 --> 00:05:36.260
data sources and combining their results,
is called an integrated view.

75
00:05:37.330 --> 00:05:42.400
It is integrated because the data is
retrieved from different data sources,

76
00:05:43.610 --> 00:05:47.640
and it's called a view because
in database terminology

77
00:05:47.640 --> 00:05:50.680
it is a relation computed
from other relations.

78
00:05:53.498 --> 00:05:56.780
To populate the integrated
view discount candidates,

79
00:05:56.780 --> 00:05:59.470
we need to go through a step
called schema mapping.

80
00:06:00.580 --> 00:06:04.550
The term mapping means to
establish correspondence between

81
00:06:04.550 --> 00:06:08.850
the attributes of the view, which is
also called a target relation, and

82
00:06:08.850 --> 00:06:10.120
that of the source relations.

83
00:06:11.200 --> 00:06:16.270
For example, we can map the full address
from individuals to the address attribute

84
00:06:16.270 --> 00:06:21.380
in discountCandidates, but
this would only be true for

85
00:06:21.380 --> 00:06:25.300
customers whose names and
addresses match in the two databases.

86
00:06:27.040 --> 00:06:32.120
As you can see, policyholders uses
the full name of a customer, whereas

87
00:06:32.120 --> 00:06:36.050
individuals has it broken down into first
name, middle initial, and last name.

88
00:06:37.520 --> 00:06:42.760
On the other hand, full address is
a single field in individuals, but

89
00:06:42.760 --> 00:06:46.397
represented in full attributes
in the policyholders relation.

90
00:06:48.090 --> 00:06:51.650
The mappings of account number and
policyKey are more straightforward.

91
00:06:52.680 --> 00:06:55.752
Well, what about customer ID which doesn't
correspond to anything in the four

92
00:06:55.752 --> 00:06:57.300
input relations?

93
00:06:57.300 --> 00:06:58.959
We'll come back to this later on.

94
00:07:01.019 --> 00:07:04.470
Okay, now we'll define
an integrated relation.

95
00:07:04.470 --> 00:07:06.020
How do we query?

96
00:07:06.020 --> 00:07:07.090
For example,

97
00:07:07.090 --> 00:07:11.170
how do you find the bank account number
of a person whose policyKey is known?

98
00:07:12.530 --> 00:07:15.020
You might think, what's the problem here?

99
00:07:15.020 --> 00:07:16.590
We have a table.

100
00:07:16.590 --> 00:07:22.020
Just say select account number from
discount candidates where policyKey

101
00:07:22.020 --> 00:07:26.680
is equal to 4-937528734, and we're done.

102
00:07:28.730 --> 00:07:35.310
Well, yes, you can write this query,
but how the query be evaluated?

103
00:07:35.310 --> 00:07:37.820
That depends on what's called the query

104
00:07:37.820 --> 00:07:40.610
architecture of the data
integration system.

105
00:07:40.610 --> 00:07:43.130
The figure on the left shows
the elements of this architecture.

106
00:07:44.290 --> 00:07:47.694
We'll discover it in more detail,
but on this slide,

107
00:07:47.694 --> 00:07:50.810
we'll just describe
the three axes of this cube.

108
00:07:51.830 --> 00:07:57.090
The vertical z axis specifies
whether we have one data source or

109
00:07:57.090 --> 00:07:58.320
multiple data sources.

110
00:07:59.320 --> 00:08:02.390
Our interest is in the case where
there are multiple data sources.

111
00:08:04.218 --> 00:08:10.110
The x axis asks whether the integrated
data is actually stored physically

112
00:08:10.110 --> 00:08:16.280
in some place or whether it is computed
on the fly, each time a query is asked.

113
00:08:16.280 --> 00:08:22.120
If it is all precomputed and stored,
we say that the data is materialized.

114
00:08:22.120 --> 00:08:24.940
And if it is computed on the fly,
we say it's virtual.

115
00:08:26.480 --> 00:08:30.174
The y axis asks whether
there is a single schema or

116
00:08:30.174 --> 00:08:34.319
global schema defined all
over the data integrated for

117
00:08:34.319 --> 00:08:39.364
an application or whether the data
stay in different computers and

118
00:08:39.364 --> 00:08:43.531
it is accessed in a peer-to-peer
manner at runtime.

119
00:08:43.531 --> 00:08:48.267
Thus, the seemingly simple select
project query will be evaluated

120
00:08:48.267 --> 00:08:53.870
depending on which part of the cube
our architecture implements.

121
00:08:53.870 --> 00:08:56.880
But for now,
let's return to our example use case.

122
00:08:59.900 --> 00:09:03.920
An obvious goal of an information
integration system

123
00:09:03.920 --> 00:09:05.260
is to be complete and accurate.

124
00:09:06.770 --> 00:09:10.790
Complete means no eligible record
from the source should be absent in

125
00:09:10.790 --> 00:09:11.670
the target relation.

126
00:09:12.970 --> 00:09:18.668
Accurate means all the entries in
the integrated relation should be correct.

127
00:09:18.668 --> 00:09:23.148
Now we said on the previous
slide that a matching

128
00:09:23.148 --> 00:09:27.740
customer is a person who
was in both databases and

129
00:09:27.740 --> 00:09:32.590
has the same name and
address in the two databases.

130
00:09:32.590 --> 00:09:34.310
Now let's look at some example records.

131
00:09:35.390 --> 00:09:39.110
Specifically, consider the records
marked by the three arrows.

132
00:09:40.510 --> 00:09:45.320
The two bank accounts and
the policy record do not match for name or

133
00:09:45.320 --> 00:09:47.050
for address.

134
00:09:47.050 --> 00:09:49.360
So our previous method would discard them.

135
00:09:50.510 --> 00:09:52.030
But look at the records closely.

136
00:09:53.380 --> 00:09:56.070
Do you think they might all
belong to the same customer?

137
00:09:57.560 --> 00:09:59.740
Maybe this lady has a maiden name and

138
00:09:59.740 --> 00:10:03.150
a married name, and
has moved from one address to another.

139
00:10:04.290 --> 00:10:07.289
Maybe she changed her Social Security
number somewhere along the way.

140
00:10:08.820 --> 00:10:11.640
So this is called
a record linkage problem.

141
00:10:12.730 --> 00:10:17.510
That means we would like to ensure that
the set of data records that belong to

142
00:10:17.510 --> 00:10:22.040
a single entity are recognized,
perhaps by clustering

143
00:10:22.040 --> 00:10:26.790
the values of different attributes or
by using a set of matching rules so

144
00:10:26.790 --> 00:10:31.130
that we know how to deal with it
during the integration process.

145
00:10:31.130 --> 00:10:33.600
For example, we need to determine

146
00:10:33.600 --> 00:10:36.250
which of the addresses should be
used in the integrated relation.

147
00:10:37.570 --> 00:10:38.650
Which of the two bank accounts?

148
00:10:40.070 --> 00:10:44.800
If the answer is both accounts 102 and
103, we will need to change

149
00:10:44.800 --> 00:10:50.380
the schema of the target's relation
to a list instead of an atomic number

150
00:10:50.380 --> 00:10:53.290
to avoid creating multiple tuples for
the same entity.

151
00:10:55.990 --> 00:11:00.380
As we saw, the schema of adding
process is a task of figuring out

152
00:11:00.380 --> 00:11:04.850
how elements of the schema from two
sources would relate to each other and

153
00:11:04.850 --> 00:11:07.940
determining how they would
map the target schema.

154
00:11:08.980 --> 00:11:13.180
You also saw that this is not really
a simple process, that we are trying to

155
00:11:13.180 --> 00:11:18.600
produce one integrated relation using
a couple of relations from each source.

156
00:11:20.130 --> 00:11:25.250
In a Big Data situation,
there are dozens of data sources, or

157
00:11:25.250 --> 00:11:31.880
more because the company's growing and
each source may have a few hundred tables.

158
00:11:31.880 --> 00:11:35.370
So it becomes very hard to actually solve

159
00:11:35.370 --> 00:11:40.419
this correspondence-making problem
completely and accurately just because

160
00:11:40.419 --> 00:11:44.920
the number of combinations one has to
go through is really, really high.

161
00:11:47.120 --> 00:11:52.250
One practical way to tackle this problem
is not to do a full-scale detail

162
00:11:52.250 --> 00:11:57.147
integration in the beginning but
adopt what's called a pay-as-you-go model.

163
00:11:57.147 --> 00:12:00.189
The pay-as-you-go data
management principle is simple.

164
00:12:01.380 --> 00:12:06.214
The system should provide some basic
integration services at the outset and

165
00:12:06.214 --> 00:12:11.123
then evolve the schema mappings between
the different sources on an as needed

166
00:12:11.123 --> 00:12:12.660
basis.

167
00:12:12.660 --> 00:12:17.190
So given a query, the system should
generate a best effort or approximate

168
00:12:17.190 --> 00:12:21.310
answers from the data sources for
a perfect schema mappings do not exist.

169
00:12:22.310 --> 00:12:26.015
When it discovers a large number
of sophisticated queries or

170
00:12:26.015 --> 00:12:30.375
data mining tasks over certain sources,
it will guide the users to make

171
00:12:30.375 --> 00:12:34.387
additional efforts to integrate
these sources more precisely.

172
00:12:36.401 --> 00:12:40.270
Okay, so how does the first
approximate schema mapping performed?

173
00:12:41.660 --> 00:12:46.920
One approach to do this is called
Probabilistic Schema Mapping.

174
00:12:46.920 --> 00:12:48.920
We'll describe it in more detail next.

175
00:12:51.120 --> 00:12:52.450
In the previous step,

176
00:12:52.450 --> 00:12:58.050
we just decided to create the disk count
candidates rrelation in an ad hoc way.

177
00:12:58.050 --> 00:12:59.990
And in a Big Data situation,

178
00:12:59.990 --> 00:13:03.920
we need to carefully determine
what the integrated schema,

179
00:13:03.920 --> 00:13:08.000
also called mediated schemas, should be,
and we should evaluate them properly.

180
00:13:09.640 --> 00:13:15.550
Since our toy company is trying
to create a single customer view,

181
00:13:15.550 --> 00:13:19.660
it's natural to create an integrated
table called customers.

182
00:13:19.660 --> 00:13:21.690
But how can we design this table?

183
00:13:21.690 --> 00:13:22.550
Here are some options.

184
00:13:23.820 --> 00:13:29.487
We can create the customer table to
include individuals and corporations and

185
00:13:29.487 --> 00:13:34.384
then use a flag called customer
type to distinguish between them.

186
00:13:34.384 --> 00:13:39.051
Now in the mediated schema then,
the individuals first name,

187
00:13:39.051 --> 00:13:43.191
middle initial, last name,
policyholder's name and

188
00:13:43.191 --> 00:13:48.140
corporation's name would all
map to Customer_Name similarly.

189
00:13:49.390 --> 00:13:53.978
The individual's full address,
the corporation's registered address,

190
00:13:53.978 --> 00:13:57.440
and the policyholder's address,
plus city, plus state,

191
00:13:57.440 --> 00:14:01.530
plus zip would all map
to customer address.

192
00:14:01.530 --> 00:14:06.010
Now we can enumerate all such choices of
which attributes to group together and

193
00:14:06.010 --> 00:14:09.230
map for each single attribute
in the target schema.

194
00:14:10.620 --> 00:14:15.630
But no matter how you'll do it,
it will never be a perfect fit because

195
00:14:15.630 --> 00:14:18.590
not all these combinations
would go well together.

196
00:14:18.590 --> 00:14:22.160
For example, should that date of
birth be included in this table?

197
00:14:23.550 --> 00:14:24.870
Would it make sense for corporations?

198
00:14:26.810 --> 00:14:32.310
In Probabilistic Mediated Schema Design,
we answer this question by

199
00:14:32.310 --> 00:14:35.410
associating probability values
with each of these options.

200
00:14:38.040 --> 00:14:42.410
To compute these values, we need to
quantify the relationships between

201
00:14:42.410 --> 00:14:46.202
attributes by figuring out which
attributes should be grouped or

202
00:14:46.202 --> 00:14:48.710
clustered together?

203
00:14:48.710 --> 00:14:52.140
Now two pieces of information
available in the source schemas

204
00:14:52.140 --> 00:14:55.497
can serve as evidence for
attribute clustering.

205
00:14:55.497 --> 00:14:59.740
One, the parallel similarity
of source attributes, and two,

206
00:14:59.740 --> 00:15:04.640
statistical properties
of service attributes.

207
00:15:05.800 --> 00:15:10.260
The first piece of information indicates
when two attributes are likely to be

208
00:15:10.260 --> 00:15:14.459
similar and is used for
creating multiple mediated schemas.

209
00:15:15.680 --> 00:15:19.050
One can apply a collection of
attribute matching modules

210
00:15:19.050 --> 00:15:21.650
to compute pairwise similarity.

211
00:15:21.650 --> 00:15:23.755
For example, individual names and

212
00:15:23.755 --> 00:15:26.700
policyholder names
are possibly quite similar.

213
00:15:27.930 --> 00:15:32.878
Are individual names versus
corporation names similar?

214
00:15:32.878 --> 00:15:37.283
Now, the similarity between two
source attributes, EIN and EZ,

215
00:15:37.283 --> 00:15:42.560
measure how closely the two attributes
represent the same real world concept.

216
00:15:44.520 --> 00:15:47.230
The second piece of information indicates

217
00:15:47.230 --> 00:15:51.650
when two attributes are likely
to be different, and is used for

218
00:15:51.650 --> 00:15:54.599
assigning probabilities to
each of the mediated schemas.

219
00:15:56.370 --> 00:15:59.770
For example, date of birth and

220
00:15:59.770 --> 00:16:02.410
corporation name possibly
will never co-occur together.

221
00:16:04.050 --> 00:16:07.250
But for
large schemas with large data volumes,

222
00:16:07.250 --> 00:16:11.980
one can estimate these measures by
taking samples from the actual database

223
00:16:11.980 --> 00:16:16.312
to come up with reasonable
similarity co-occurrence scores.

224
00:16:16.312 --> 00:16:22.658
To illustrate attribute regrouping, we
take a significant re-simplified example.

225
00:16:22.658 --> 00:16:27.543
Here we want to create customer
transactions as a mediated relation

226
00:16:27.543 --> 00:16:32.180
based upon the bank transactions and
insurance transactions.

227
00:16:33.530 --> 00:16:36.159
Each attribute is given
an abbreviation for simplicity.

228
00:16:38.030 --> 00:16:41.575
Below, you can see three
possible mediated schemas.

229
00:16:42.770 --> 00:16:47.140
In the first one,
the transaction begin and end times

230
00:16:47.140 --> 00:16:52.270
from bank transactions are grouped into
the same cluster as transaction date time

231
00:16:52.270 --> 00:16:56.040
from the insurance transactions,
because all of them are the same type.

232
00:16:57.280 --> 00:17:02.000
Similarly, transaction party, that
means who is giving or receiving money,

233
00:17:03.010 --> 00:17:06.610
and transaction description are grouped
together with transaction details.

234
00:17:08.490 --> 00:17:11.130
The second schema keeps
all of them separate.

235
00:17:12.320 --> 00:17:16.145
And the third candidate schema
groups some of them and not others.

236
00:17:19.998 --> 00:17:23.860
Now that we have multiple mediated
schemas, which one should we choose?

237
00:17:25.030 --> 00:17:27.990
Now I'm presenting here
a qualitative account of the method.

238
00:17:29.280 --> 00:17:32.730
The primary goal is to look for
what we can call consistency.

239
00:17:34.430 --> 00:17:38.260
A source schema is consistent
with a mediated schema if

240
00:17:38.260 --> 00:17:43.070
two different attributes of the source
schema do not occur in one cluster.

241
00:17:44.190 --> 00:17:48.990
And in the example, Med3,
that means related schema three,

242
00:17:48.990 --> 00:17:54.560
is more consistent with bank
transactions because, unlike Med1,

243
00:17:54.560 --> 00:17:58.700
it keeps TBT,
TET in two different clusters.

244
00:17:59.800 --> 00:18:06.090
Once this is done, we can count the number
of consistent sources for each candidate

245
00:18:06.090 --> 00:18:10.650
mediated schema and then use this count
to come up with a probability estimate.

246
00:18:11.890 --> 00:18:16.100
This estimate can then be used
to choose the k best schemas.

247
00:18:17.590 --> 00:18:22.030
Should one ever choose more
than one just best schema?

248
00:18:22.030 --> 00:18:23.860
Well, that's a hard question
to answer in general.

249
00:18:25.280 --> 00:18:29.932
It is done when the top capability
estimates are very close to each other.
WEBVTT

1
00:00:00.025 --> 00:00:04.576
[SOUND] The Splunk platform for
operational intelligence is

2
00:00:04.576 --> 00:00:10.493
a revolutionary suite of products that
is unlocking unprecedented value for

3
00:00:10.493 --> 00:00:13.779
thousands of customers around the world.

4
00:00:13.779 --> 00:00:15.970
Why Splunk?

5
00:00:15.970 --> 00:00:18.420
It all starts with machine data.

6
00:00:18.420 --> 00:00:20.970
Machine data is the big data generated by

7
00:00:20.970 --> 00:00:23.710
all the technologies that
power our businesses.

8
00:00:23.710 --> 00:00:28.320
From the applications, servers, websites,
and network devices in the data center and

9
00:00:28.320 --> 00:00:31.300
the cloud, to the mobile device
in the palm of your hand.

10
00:00:32.340 --> 00:00:37.460
Thermostats, train sensors, electric cars,
and the Internet of things.

11
00:00:37.460 --> 00:00:39.412
Machine data is everywhere.

12
00:00:39.412 --> 00:00:41.710
It's fast-growing and complex.

13
00:00:41.710 --> 00:00:44.550
It's also incredibly valuable, why?

14
00:00:44.550 --> 00:00:48.630
Because it contains a definitive
record of all activity and behavior.

15
00:00:49.970 --> 00:00:54.380
Splunk software collects and
indexes this data at massive scale,

16
00:00:54.380 --> 00:00:58.340
from wherever it's generated,
regardless of format or source.

17
00:00:58.340 --> 00:01:02.290
Users can quickly and
easily monitor, search, analyze, and

18
00:01:02.290 --> 00:01:05.680
report on their data, all in real time.

19
00:01:05.680 --> 00:01:07.640
Machine data is different.

20
00:01:07.640 --> 00:01:11.450
It can't be processed and
analyzed using traditional methods.

21
00:01:11.450 --> 00:01:16.540
Splunk software does not rely on brittle
schemas and inflexible databases.

22
00:01:16.540 --> 00:01:19.660
Splunk is easy to deploy, easy to use, and

23
00:01:19.660 --> 00:01:25.290
easy to scale, whether on premises or
in a public, private, or hybrid cloud.

24
00:01:25.290 --> 00:01:27.960
Splunk is also available
as a cloud service.

25
00:01:27.960 --> 00:01:31.942
And for big data environments that
used Hadoop for cheap app storage,

26
00:01:31.942 --> 00:01:34.522
we have Hunk, Splunk analytics for Hadoop.

27
00:01:34.522 --> 00:01:36.950
[MUSIC]

28
00:01:36.950 --> 00:01:39.843
Our customers from around
the world illustrate why so

29
00:01:39.843 --> 00:01:42.070
many organizations use Splunk.

30
00:01:42.070 --> 00:01:46.710
Intuit has standardized on Splunk,
delivering operational visibility for

31
00:01:46.710 --> 00:01:52.060
their leading online products, including
QuickBooks, Quicken, and TurboTax.

32
00:01:52.060 --> 00:01:55.770
Intuit considers Splunk one of
their cornerstone technologies

33
00:01:55.770 --> 00:01:59.950
that is helping them innovate and
deliver better service to their customers.

34
00:01:59.950 --> 00:02:03.210
Cisco, one of the world's
largest technology providers,

35
00:02:03.210 --> 00:02:06.940
empowers their global security
team with Splunk enterprise

36
00:02:06.940 --> 00:02:11.220
to gain a centralized view into
end user and system activities.

37
00:02:11.220 --> 00:02:15.000
Splunk has dramatically helped
improve their incident detection and

38
00:02:15.000 --> 00:02:15.690
response rate.

39
00:02:17.000 --> 00:02:20.650
Splunk was key to Domino's Pizza's
success during the Super Bowl,

40
00:02:20.650 --> 00:02:24.020
by monitoring database uptime and
order response.

41
00:02:24.020 --> 00:02:26.710
Armed with new levels of
customer understanding,

42
00:02:26.710 --> 00:02:30.490
Domino's was able to strengthen their
online business, in addition to saving

43
00:02:30.490 --> 00:02:34.960
hundreds of thousands of dollars replacing
legacy technologies with Splunk.

44
00:02:34.960 --> 00:02:39.350
We are thrilled to hear Domino's
call Splunk their secret sauce.

45
00:02:39.350 --> 00:02:43.200
Another great Splunk story includes
cars and the Internet of things.

46
00:02:44.810 --> 00:02:48.813
Working together with the Ford Motor
Company and Ford's OpenXC platform,

47
00:02:48.813 --> 00:02:52.816
Splunk delivered connected car dashboards
to examine driving behavior and

48
00:02:52.816 --> 00:02:54.079
vehicle performance.

49
00:02:54.079 --> 00:02:58.529
UK-based Tesco is one of
the world's largest retailers,

50
00:02:58.529 --> 00:03:04.650
with nearly 1 million customers and
a half billion online orders per week.

51
00:03:04.650 --> 00:03:07.480
Customer satisfaction
is a critical metric.

52
00:03:07.480 --> 00:03:12.580
Tesco deployed Splunk to gain a unified
view across their websites, transactions,

53
00:03:12.580 --> 00:03:17.410
and business, gaining valuable digital
intelligence about their customers.

54
00:03:17.410 --> 00:03:22.820
Splunk was founded to pursue a disruptive
vision to make machine data accessible,

55
00:03:22.820 --> 00:03:25.490
usable, and valuable to everyone.

56
00:03:25.490 --> 00:03:27.280
Find out more about Splunk and

57
00:03:27.280 --> 00:03:30.820
operational intelligence by
downloading the executive summary.

58
00:03:30.820 --> 00:03:31.660
Or better yet,

59
00:03:31.660 --> 00:03:36.230
experience the value firsthand by
downloading Splunk software for free.

60
00:03:36.230 --> 00:03:39.565
Chances are someone in your
organization already has.

61
00:03:39.565 --> 00:03:45.649
[SOUND]
WEBVTT

1
00:00:02.630 --> 00:00:04.630
Aggregations in Big Data Pipelines.

2
00:00:07.220 --> 00:00:10.680
After this video you will
be able to compare and

3
00:00:10.680 --> 00:00:15.740
select the aggregation operation that
you require to solve your problem.

4
00:00:15.740 --> 00:00:21.650
Explain how you can use aggregations to
compact your dataset and reduce volume.

5
00:00:21.650 --> 00:00:22.890
That is in many cases.

6
00:00:24.260 --> 00:00:29.586
And design complex operations in your
pipeline, using a series of aggregations.

7
00:00:32.313 --> 00:00:39.189
Aggregation is any operation on a data set
that performs a specific transformation,

8
00:00:39.189 --> 00:00:43.979
taking all the related data
elements into consideration.

9
00:00:45.060 --> 00:00:49.550
Let's say we have a bunch of stars
which are of different colors.

10
00:00:50.570 --> 00:00:54.550
Different colors denote diversity or
variety in the data.

11
00:00:56.260 --> 00:01:02.610
To keep things simple, we will use
letter 'f' to denote a transformation.

12
00:01:03.700 --> 00:01:04.720
In the following slides,

13
00:01:04.720 --> 00:01:10.360
we will see examples of how 'f' can take
the shape of different transformations.

14
00:01:13.550 --> 00:01:18.390
If we apply a transformation that does
something using the information of

15
00:01:18.390 --> 00:01:22.430
all the stars here,
we are performing an aggregation.

16
00:01:24.070 --> 00:01:29.850
Loosely speaking, we can say
that applying a transformation 'f'

17
00:01:29.850 --> 00:01:34.830
that takes all the elements of data
as input is called 'aggregation'.

18
00:01:38.490 --> 00:01:44.400
One of the simplest aggregations is
summation over all the data elements.

19
00:01:44.400 --> 00:01:48.750
In this case,
let's say every star counted as 1.

20
00:01:48.750 --> 00:01:52.586
Summing over all the stars gives 14,

21
00:01:52.586 --> 00:01:57.119
which is the summation of 3 stars for
yellow,

22
00:01:57.119 --> 00:02:01.897
5 stars for green, and
6 stars for color pink.

23
00:02:04.779 --> 00:02:10.513
Another aggregation that you could perform
is summation of individual star colors,

24
00:02:10.513 --> 00:02:13.070
that is, grouping the sums by color.

25
00:02:14.200 --> 00:02:20.360
So, If each star is a 1,
adding each group will result in 3 for

26
00:02:20.360 --> 00:02:25.530
yellow stars, 5 for green stars,
and 6 for pink stars.

27
00:02:26.860 --> 00:02:27.952
In this case,

28
00:02:27.952 --> 00:02:33.611
the aggregation function 'f' will output
3 tuples of star colors and counts.

29
00:02:36.065 --> 00:02:41.190
In a sales scenario, each color could
denote a different product type.

30
00:02:42.470 --> 00:02:47.320
And the number 1 could be replaced
by revenue generated by a product

31
00:02:47.320 --> 00:02:49.230
in each city the product is sold.

32
00:02:50.600 --> 00:02:54.250
In fact,
we will keep coming back to this analogy.

33
00:02:56.420 --> 00:03:01.188
You can also perform average
over items of similar kind,

34
00:03:01.188 --> 00:03:06.150
such as sums grouped by color.

35
00:03:08.060 --> 00:03:10.630
Continuing the example earlier,

36
00:03:10.630 --> 00:03:15.060
you can calculate average revenue per
product type using this aggregation.

37
00:03:17.530 --> 00:03:22.270
Other simple yet useful aggregational
operations to help you extract meaning

38
00:03:22.270 --> 00:03:29.010
from large data sets are maximum,
minimum, and standard deviation.

39
00:03:29.010 --> 00:03:34.090
Remember, you can always perform
aggregation as a series of operations,

40
00:03:34.090 --> 00:03:38.140
such as maximum of the sums per product.

41
00:03:38.140 --> 00:03:41.510
That is, summation followed by maximum.

42
00:03:42.725 --> 00:03:46.920
If you first sum sales for
each city, that is for

43
00:03:46.920 --> 00:03:51.640
each product,
then you can take the maximum of it

44
00:03:51.640 --> 00:03:57.050
by applying maximum function to
the result of the summation function.

45
00:03:57.050 --> 00:04:01.870
In this case, you get the product,
which has maximum sales in the country.

46
00:04:04.040 --> 00:04:08.084
Aggregation over Boolean data
sets that can have true-false or

47
00:04:08.084 --> 00:04:14.430
one-zero values could be a complex mixture
of AND, OR, and NOT logical operations.

48
00:04:16.030 --> 00:04:20.110
A lot of problems become easy
to manipulate using sets.

49
00:04:20.110 --> 00:04:24.008
Because sets don't allow duplicate values.

50
00:04:24.008 --> 00:04:27.710
Depending on your application,
this could be very useful.

51
00:04:28.800 --> 00:04:33.730
For example, to count the number
of products from a sales table,

52
00:04:33.730 --> 00:04:38.920
you can simply take all
the sales tables and create sets

53
00:04:38.920 --> 00:04:44.130
of these products in those tables,
and take a union of these sets.

54
00:04:46.090 --> 00:04:52.130
To summarize, by choosing the right
aggregation, you can generate compact and

55
00:04:52.130 --> 00:04:58.720
meaningful insights that enable faster and
effective decision making in business.

56
00:04:58.720 --> 00:05:00.960
You will find that in most cases,

57
00:05:00.960 --> 00:05:04.270
aggregation results in
smaller output data sets.

58
00:05:05.450 --> 00:05:10.140
Hence, aggregation is an important tool
set to keep in pocket when dealing with

59
00:05:10.140 --> 00:05:12.669
large data sets, and big data pipelines.
WEBVTT

1
00:00:02.723 --> 00:00:05.960
Big Data Processing Pipelines:
A Dataflow Approach.

2
00:00:07.160 --> 00:00:11.420
Most big data applications
are composed of a set of operations

3
00:00:11.420 --> 00:00:13.980
executed one after another as a pipeline.

4
00:00:14.980 --> 00:00:17.520
Data flows through these operations,

5
00:00:17.520 --> 00:00:20.177
going through various
transformations along the way.

6
00:00:20.177 --> 00:00:24.080
We also call this dataflow graphs.

7
00:00:24.080 --> 00:00:27.625
So to understand big data
processing we should start by

8
00:00:27.625 --> 00:00:30.100
understanding what dataflow means.

9
00:00:31.820 --> 00:00:36.781
After this video you will be able to
summarize what dataflow means and

10
00:00:36.781 --> 00:00:38.844
it's role in data science.

11
00:00:38.844 --> 00:00:44.096
Explain split->do->merge as a big
data pipeline with examples,

12
00:00:44.096 --> 00:00:46.770
and define the term data parallel.

13
00:00:48.910 --> 00:00:53.484
Let's consider the hello
world MapReduce example for

14
00:00:53.484 --> 00:00:57.461
WordCount which reads one or
more text files and

15
00:00:57.461 --> 00:01:03.043
counts the number of occurrences
of each word in these text files.

16
00:01:03.043 --> 00:01:07.483
You are by now very familiar with
this example, but as a reminder,

17
00:01:07.483 --> 00:01:10.891
the output will be a text
file with a list of words and

18
00:01:10.891 --> 00:01:14.250
their occurrence frequencies
in the input data.

19
00:01:16.451 --> 00:01:20.985
In this application,
the files were first split into HDFS

20
00:01:20.985 --> 00:01:25.990
cluster nodes as partitions of
the same file or multiple files.

21
00:01:27.560 --> 00:01:30.680
Then a map operation, in this case,

22
00:01:30.680 --> 00:01:36.340
a user defined function to count words
was executed on each of these nodes.

23
00:01:36.340 --> 00:01:42.460
And all the key values that were output
from map were sorted based on the key.

24
00:01:42.460 --> 00:01:47.380
And the key values with the same word
were moved or shuffled to the same node.

25
00:01:48.920 --> 00:01:54.130
Finally, the reduce operation
was executed on these nodes

26
00:01:54.130 --> 00:01:57.900
to add the values for
key-value pairs with the same keys.

27
00:01:59.920 --> 00:02:05.880
If you look back at this example, we see
that there were four distinct steps,

28
00:02:05.880 --> 00:02:11.070
namely the data split step,
the map step, the shuffle and

29
00:02:11.070 --> 00:02:13.030
sort step, and the reduce step.

30
00:02:14.310 --> 00:02:18.417
Although, the word count
example is pretty simple it

31
00:02:18.417 --> 00:02:22.980
represents a large number of
applications that these three

32
00:02:22.980 --> 00:02:27.652
steps can be applied to achieve
data parallel scalability.

33
00:02:27.652 --> 00:02:33.290
We refer in general to this
pattern as "split-do-merge".

34
00:02:35.040 --> 00:02:41.490
In these applications, data flows
through a number of steps, going through

35
00:02:41.490 --> 00:02:46.990
transformations with various scalability
needs, leading to a final product.

36
00:02:48.180 --> 00:02:50.310
The data first gets partitioned.

37
00:02:51.430 --> 00:02:56.450
The split data goes through a set of
user-defined functions to do something,

38
00:02:57.840 --> 00:03:02.420
ranging from statistical operations to
data joins to machine learning functions.

39
00:03:03.870 --> 00:03:08.510
Depending on the application's
data processing needs,

40
00:03:08.510 --> 00:03:13.350
these "do something" operations can
differ and can be chained together.

41
00:03:14.620 --> 00:03:20.130
In the end results can be combined
using a merging algorithm or

42
00:03:20.130 --> 00:03:22.300
a higher-order function like reduce.

43
00:03:23.580 --> 00:03:27.520
We call the stitched-together
version of these sets of steps for

44
00:03:27.520 --> 00:03:30.970
big data processing "big data pipelines".

45
00:03:33.700 --> 00:03:38.180
The term pipe comes from
a UNIX separation that

46
00:03:38.180 --> 00:03:43.490
the output of one running program gets
piped into the next program as an input.

47
00:03:43.490 --> 00:03:47.870
As you might imagine,
one can string multiple programs together

48
00:03:47.870 --> 00:03:52.560
to make longer pipelines with various
scalability needs at each step.

49
00:03:53.740 --> 00:03:56.750
However, for big data processing,

50
00:03:56.750 --> 00:04:02.110
the parallelism of each step in
the pipeline is mainly data parallelism.

51
00:04:02.110 --> 00:04:07.284
We can simply define data parallelism
as running the same functions

52
00:04:07.284 --> 00:04:13.380
simultaneously for the elements or
partitions of a dataset on multiple cores.

53
00:04:13.380 --> 00:04:17.447
For example,
in our word count example, data

54
00:04:17.447 --> 00:04:21.720
parallelism occurs in every
step of the pipeline.

55
00:04:22.770 --> 00:04:26.530
There's definitely parallelization
during map over the input

56
00:04:26.530 --> 00:04:30.460
as each partition gets
processed as a line at a time.

57
00:04:30.460 --> 00:04:32.920
To achieve this type of data parallelism,

58
00:04:32.920 --> 00:04:37.560
we must decide on the data granularity
of each parallel computation.

59
00:04:37.560 --> 00:04:38.930
In this case, it is a line.

60
00:04:41.022 --> 00:04:46.560
We also see a parallel grouping of
data in the shuffle and sort phase.

61
00:04:46.560 --> 00:04:50.630
This time, the parallelization is
over the intermediate products,

62
00:04:50.630 --> 00:04:53.050
that is, the individual key-value pairs.

63
00:04:55.140 --> 00:04:58.810
And after the grouping of
the intermediate products

64
00:04:58.810 --> 00:05:03.270
the reduce step gets parallelized
to construct one output file.

65
00:05:04.360 --> 00:05:08.470
You have probably noticed that
the data gets reduced to a smaller set

66
00:05:08.470 --> 00:05:09.060
at each step.

67
00:05:11.370 --> 00:05:13.890
Although, the example we have given is for

68
00:05:13.890 --> 00:05:18.020
batch processing, similar techniques
apply to stream processing.

69
00:05:19.060 --> 00:05:20.740
Let's discuss this for

70
00:05:20.740 --> 00:05:24.690
our simplified advanced stream
data from an online game example.

71
00:05:25.910 --> 00:05:30.070
In this case, your event gets ingested

72
00:05:30.070 --> 00:05:34.960
through a real time big data ingestion
engine, like Kafka or Flume.

73
00:05:36.380 --> 00:05:40.400
Then they get passed into
a Streaming Data Platform for

74
00:05:40.400 --> 00:05:44.670
processing like Samza,
Storm or Spark streaming.

75
00:05:45.750 --> 00:05:51.910
This is a valid choice for processing
data one event at a time or chunking

76
00:05:51.910 --> 00:05:57.390
the data into Windows or Microbatches
of time or other features.

77
00:05:58.940 --> 00:06:04.470
Any pipeline processing of data can
be applied to the streaming data here

78
00:06:04.470 --> 00:06:07.410
as we wrote in a batch-
processing Big Data engine.

79
00:06:09.350 --> 00:06:15.034
The process stream data can then be
served through a real-time view or

80
00:06:15.034 --> 00:06:17.279
a batch-processing view.

81
00:06:17.279 --> 00:06:22.380
Real-time view is often subject to change
as potentially delayed new data comes in.

82
00:06:23.590 --> 00:06:28.490
The storage of the data can be
accomplished using H-Base, Cassandra,

83
00:06:28.490 --> 00:06:32.660
HDFS, or
many other persistent storage systems.

84
00:06:34.120 --> 00:06:39.110
To summarize, big data pipelines
get created to process data

85
00:06:39.110 --> 00:06:43.990
through an aggregated set of steps that
can be represented with the split-

86
00:06:43.990 --> 00:06:48.370
do-merge pattern with
data parallel scalability.

87
00:06:48.370 --> 00:06:51.040
This pattern can be
applied to many batch and

88
00:06:51.040 --> 00:06:54.100
streaming data processing applications.

89
00:06:54.100 --> 00:06:58.958
Next we will go through some processing
steps in a big data pipeline in

90
00:06:58.958 --> 00:07:03.589
more detail, first conceptually,
then practically in Spark.
WEBVTT

1
00:00:01.140 --> 00:00:05.070
Now that we went through an overview
of the Spark ecosystem and

2
00:00:05.070 --> 00:00:09.640
the components of the Spark stack, it is
time for us to start learning more about

3
00:00:09.640 --> 00:00:13.960
its architecture and run our first
Spark program in the cloud VM.

4
00:00:15.580 --> 00:00:19.710
After this video you will be
able to describe how Spark

5
00:00:19.710 --> 00:00:24.690
does in-memory processing using the
Resilient Distributed Dataset abstraction,

6
00:00:25.890 --> 00:00:29.740
explain the inner workings of
the Spark architecture, and

7
00:00:29.740 --> 00:00:34.090
summarize how Spark manages and
executes code on Clusters.

8
00:00:35.510 --> 00:00:39.531
I mentioned a few times that
Spark is efficient because it

9
00:00:39.531 --> 00:00:43.739
uses an abstraction called RDDs for
in memory processing.

10
00:00:45.704 --> 00:00:48.970
What this means might not be
clear to some of you yet.

11
00:00:50.170 --> 00:00:52.660
Let's remember the alternative.

12
00:00:52.660 --> 00:00:57.153
In Hadoop MapReduce,
each step, also pipeline,

13
00:00:57.153 --> 00:01:02.272
reads from disk to memory,
performs the computations and

14
00:01:02.272 --> 00:01:06.770
writes back its output from
the memory to the disk.

15
00:01:08.290 --> 00:01:13.520
However, writing data to disk
is a costly operation and

16
00:01:13.520 --> 00:01:17.220
this cost becomes even more
with large volumes of data.

17
00:01:18.650 --> 00:01:20.010
Here is an interesting fact.

18
00:01:21.320 --> 00:01:25.664
Memory operations can
be up to 100,000 times

19
00:01:25.664 --> 00:01:28.250
faster than disk operations in some cases.

20
00:01:29.510 --> 00:01:33.430
Spark instead takes advantage of this and
allows for

21
00:01:33.430 --> 00:01:37.810
immediate results of transformations
in different stages of the pipeline and

22
00:01:37.810 --> 00:01:40.650
memory, like MAP and REDUCE here.

23
00:01:41.980 --> 00:01:45.540
Here, we see that the outputs
of MAP operations

24
00:01:45.540 --> 00:01:49.890
are shared with reduce operations
without being written to the disk.

25
00:01:50.980 --> 00:01:55.900
The containers where the data
gets stored in memory

26
00:01:55.900 --> 00:02:00.370
are called resilient distributed
datasets or RDDs for short.

27
00:02:02.210 --> 00:02:06.730
RDDs are how Spark distributes data and
computations across

28
00:02:06.730 --> 00:02:12.180
the nodes of a commodity cluster,
preferably with large memory.

29
00:02:12.180 --> 00:02:14.380
Thanks to this abstraction,

30
00:02:14.380 --> 00:02:18.790
Spark has proven to be 100 times
faster for some applications.

31
00:02:20.360 --> 00:02:23.960
Let's define all the words
in this interesting name

32
00:02:23.960 --> 00:02:26.510
beginning with the last one, datasets.

33
00:02:28.196 --> 00:02:36.790
Datasets that RDD distributes comes
from a batch data storage like HDFS,

34
00:02:36.790 --> 00:02:42.120
no SQL databases, text files or streaming
data ingestion systems like Cafco.

35
00:02:43.330 --> 00:02:47.760
It can even conveniently read and
distribute the data from your local disc

36
00:02:47.760 --> 00:02:52.600
like text files into Spark, or
even a hierarchy of folders.

37
00:02:54.750 --> 00:03:01.200
When Spark reads data from these sources,
it generates RDDs for them.

38
00:03:01.200 --> 00:03:07.930
The Spark operations can transform RDDs
into other RDDs like any other data.

39
00:03:07.930 --> 00:03:13.650
Here it's important to mention
that RDDs are immutable.

40
00:03:13.650 --> 00:03:17.610
This means that you cannot
change them partially.

41
00:03:17.610 --> 00:03:22.970
However, you can create new RDDs by
a series of one or many transformations.

42
00:03:24.570 --> 00:03:28.200
Next, let's look at what distributed
means in the RDD context.

43
00:03:29.480 --> 00:03:34.680
As I mentioned before, RDDs distribute
partitioned data collections and

44
00:03:34.680 --> 00:03:40.740
computations on clusters even
across a number of machines.

45
00:03:41.770 --> 00:03:44.860
For example, running on the Amazon Cloud.

46
00:03:46.160 --> 00:03:50.880
The complexity of this operation is
hidden by this very simple interface.

47
00:03:52.140 --> 00:03:57.230
Computations are a diverse set of
transformations of RDDs like map,

48
00:03:57.230 --> 00:03:59.230
filter and join.

49
00:03:59.230 --> 00:04:05.940
And also actions on the RDDs like counting
and saving them persistently on disk.

50
00:04:05.940 --> 00:04:10.560
The partitioning of data can be changed
dynamically to optimize Spark's

51
00:04:10.560 --> 00:04:11.170
performance.

52
00:04:12.620 --> 00:04:17.420
The last element is resilient, and
it's very important because in a large

53
00:04:17.420 --> 00:04:21.640
scale computing environment it is
pretty common to have node failures.

54
00:04:22.850 --> 00:04:25.710
It's very important to be
able to recover from these

55
00:04:25.710 --> 00:04:28.480
situations without losing
any work already done.

56
00:04:29.570 --> 00:04:34.360
For full tolerance in such situations,
Spark tracks the history of each

57
00:04:34.360 --> 00:04:39.220
partition, keeping a lineage
over RDDs over time,

58
00:04:39.220 --> 00:04:44.170
so every point in your calculations,
Spark knows which are the partitions

59
00:04:44.170 --> 00:04:47.560
needed to recreate the partition
in case it gets lost.

60
00:04:48.710 --> 00:04:50.480
And if that happens,

61
00:04:50.480 --> 00:04:55.730
then Spark automatically figures out
where it can start the recompute from and

62
00:04:55.730 --> 00:04:59.890
optimizes the amount of processing
needed to recover from the failure.

63
00:05:01.410 --> 00:05:06.087
Before we create our first Spark program
using RDDs in Pi Spark in the cloud

64
00:05:06.087 --> 00:05:09.343
layer VM,
let's review Spark's architecture.

65
00:05:12.239 --> 00:05:16.772
From a bird's eye view,
Spark has two main components,

66
00:05:16.772 --> 00:05:19.610
a driver program and worker nodes.

67
00:05:21.900 --> 00:05:25.300
The driver program is where
your application starts.

68
00:05:26.900 --> 00:05:30.760
It distributes RDDs on your
computational cluster and

69
00:05:30.760 --> 00:05:35.435
makes sure the transformations and
actions on these RDDs are performed.

70
00:05:37.530 --> 00:05:42.250
Driver programs create
a connection to a Spark cluster or

71
00:05:42.250 --> 00:05:45.950
your local Spark through
a Spark context object.

72
00:05:47.010 --> 00:05:50.770
The default Spark context
in the Spark shell is

73
00:05:50.770 --> 00:05:54.680
an object called SC for Spark context.

74
00:05:55.980 --> 00:06:01.260
For example, in the upcoming reading for
creating word counts in Spark,

75
00:06:01.260 --> 00:06:05.252
we will use SC as the context to

76
00:06:05.252 --> 00:06:10.210
generate RDDs for a text file
using the line of code shown here.

77
00:06:11.650 --> 00:06:17.340
The driver program manages a potentially
large number of nodes called worker nodes.

78
00:06:18.780 --> 00:06:23.740
On a local computer, we can assume
that there's only one worker node and

79
00:06:23.740 --> 00:06:25.590
it is where the Spark operations execute.

80
00:06:27.100 --> 00:06:32.230
A worker node in Spark keeps
a running Java virtual machine,

81
00:06:32.230 --> 00:06:36.000
called JVM commonly, called the executor.

82
00:06:38.080 --> 00:06:40.913
Depending on the illustration,

83
00:06:40.913 --> 00:06:45.566
executor can execute task
related to mapping stages or

84
00:06:45.566 --> 00:06:50.130
reducing stages or
other Spark specific pipelines.

85
00:06:50.130 --> 00:06:56.642
This Java virtual machine is the core
that all the computation is executed,

86
00:06:56.642 --> 00:07:03.780
and this is the interface also to the rest
of the Big Data storage systems and tools.

87
00:07:05.320 --> 00:07:10.560
For example, if we ever had the Hadoop
file system, HTFS, as the storage system,

88
00:07:10.560 --> 00:07:14.650
then on each worker node,
some of the data will be stored locally.

89
00:07:14.650 --> 00:07:19.445
As you know, the most important point
of this computing framework is to bring

90
00:07:19.445 --> 00:07:21.084
the computation to data.

91
00:07:21.084 --> 00:07:24.770
So Spark will send some
computational jobs to be executed

92
00:07:24.770 --> 00:07:30.190
on the data that are already available
on the machine thanks to HDFS.

93
00:07:30.190 --> 00:07:32.930
Data will be read from HDFS and

94
00:07:32.930 --> 00:07:38.580
get processed in memory,
the results will be stored as one RDD.

95
00:07:38.580 --> 00:07:45.012
The actual computation is running
straight in the executor,

96
00:07:45.012 --> 00:07:50.214
that is the JVM that runs your Scala or
Java codes.

97
00:07:50.214 --> 00:07:54.970
Instead, if you are using PySpark
then there will be several Python

98
00:07:54.970 --> 00:07:57.723
processes generally, one for task but

99
00:07:57.723 --> 00:08:01.490
you can configure it depending
on your application.

100
00:08:03.999 --> 00:08:09.770
In a real Big Data scenario, we have many
worker nodes running tasks internally.

101
00:08:10.990 --> 00:08:15.220
It is important to have a system that can
automatically manage provisioning and

102
00:08:15.220 --> 00:08:16.420
restarting of these nodes.

103
00:08:17.540 --> 00:08:20.550
The cluster manager in
Spark has this capability.

104
00:08:21.970 --> 00:08:27.660
Spark currently supports mainly three
interfaces for cluster management,

105
00:08:27.660 --> 00:08:34.620
namely Spark's standalone cluster manager,
the Apache Mesos, and Hadoop YARN.

106
00:08:36.190 --> 00:08:40.441
Standalone means that there's a special
Spark process that takes care of

107
00:08:40.441 --> 00:08:42.567
restarting nodes that are failing or

108
00:08:42.567 --> 00:08:45.668
starting nodes at the beginning
of the computation.

109
00:08:45.668 --> 00:08:50.276
YARN and Mesos are two external research
measures that can be used also for

110
00:08:50.276 --> 00:08:51.400
these purposes.

111
00:08:54.080 --> 00:08:57.400
Choosing a cluster manager
to fit your application and

112
00:08:57.400 --> 00:09:00.280
infrastructure can be quite confusing.

113
00:09:00.280 --> 00:09:04.020
Here, we give you a good
article as a starting point on

114
00:09:04.020 --> 00:09:07.030
how to pick the right cluster manager for
your organization.

115
00:09:08.470 --> 00:09:13.130
To summarize, the Spark architecture
includes a driver program.

116
00:09:14.270 --> 00:09:18.918
The driver program communicates
with the cluster manager for

117
00:09:18.918 --> 00:09:22.404
monitoring and
provisioning of resources and

118
00:09:22.404 --> 00:09:27.783
communicates directly with worker
nodes to submit and execute tasks.

119
00:09:27.783 --> 00:09:33.410
RDDs get created and passed within
transformations running in the executable.

120
00:09:34.900 --> 00:09:39.691
Finally, let's see how this
setup works on the Cloudera VM.

121
00:09:39.691 --> 00:09:43.797
In the Cloudera VM,
we are using Spark in standalone mode and

122
00:09:43.797 --> 00:09:46.820
everything is running locally.

123
00:09:46.820 --> 00:09:53.615
So it's a single machine and on the same
machine we have our driver program,

124
00:09:53.615 --> 00:09:58.089
the executor JVM and
our single PySpark process.

125
00:09:58.089 --> 00:10:02.771
With that, we are ready to start with
our first reading to install Spark and

126
00:10:02.771 --> 00:10:06.946
then running our word count program
using the Spark environment.
WEBVTT

1
00:00:01.430 --> 00:00:05.170
After a brief overview of some
of the processing systems

2
00:00:05.170 --> 00:00:09.480
in the Big Data Landscape, it is time for
us to dive deeper into Spark.

3
00:00:10.820 --> 00:00:15.170
Spark was initiated at
UC Berkeley in 2009 and

4
00:00:15.170 --> 00:00:19.840
was transferred to
Apache Software Foundation in 2013.

5
00:00:19.840 --> 00:00:24.240
Since then, Spark has become a top
level project with many users and

6
00:00:24.240 --> 00:00:25.700
contributors worldwide.

7
00:00:27.550 --> 00:00:32.530
After this video, you will be able
to list the main motivations for

8
00:00:32.530 --> 00:00:36.980
the development of Spark,
draw the Spark stack as a layer diagram,

9
00:00:38.320 --> 00:00:42.130
And explain the functionality of
the components in the Spark stack.

10
00:00:44.520 --> 00:00:49.810
As we have discussed in our earlier
discussions, while Hadoop is great for

11
00:00:49.810 --> 00:00:53.460
batch processing using
the MapReduce programming module,

12
00:00:53.460 --> 00:00:55.710
it has shortcomings in a number of ways.

13
00:00:57.110 --> 00:01:02.000
First of all, since it is limited to
Map and Reduce based transformations,

14
00:01:02.000 --> 00:01:07.020
one has to restrict their big data
pipeline to map and reduce steps.

15
00:01:08.520 --> 00:01:11.680
But the number of applications
can be implemented using Map and

16
00:01:11.680 --> 00:01:14.970
Reduce, it's not always possible and

17
00:01:14.970 --> 00:01:18.810
it is often not the most efficient
way to express a big data pipeline.

18
00:01:20.460 --> 00:01:25.920
For example, you might want to do a join
operation between different data sets or

19
00:01:25.920 --> 00:01:28.430
you might want to filter or
sample your data.

20
00:01:29.440 --> 00:01:32.460
Or you might have a more
complicated data pipeline with

21
00:01:32.460 --> 00:01:36.680
several steps including joins and
group byes.

22
00:01:36.680 --> 00:01:41.280
It might have a Map and Reduce face,
but maybe another map face after that.

23
00:01:42.660 --> 00:01:48.140
These types of operations are hard or
impossible to express using MapReduce and

24
00:01:48.140 --> 00:01:51.760
cannot be accommodated by
the MapReduce framework in Hadoop.

25
00:01:53.080 --> 00:01:56.830
Another important bottleneck in
Hadoop MapReduce that is critical for

26
00:01:56.830 --> 00:02:01.880
performance, is that MapReduce relies
heavily on reading data from disc.

27
00:02:03.680 --> 00:02:08.470
This is especially a problem for iterative
algorithms that require taking several

28
00:02:08.470 --> 00:02:12.800
passes through the data using
a number of transformations.

29
00:02:12.800 --> 00:02:16.940
Since each transformation will need
to read its inputs from the disk,

30
00:02:16.940 --> 00:02:20.670
this will end up in a performance
bottleneck due to IO.

31
00:02:22.730 --> 00:02:25.610
Most machine learning pipelines
are in this category,

32
00:02:25.610 --> 00:02:28.850
making Hadoop MapReduce not ideal for
machine learning.

33
00:02:29.900 --> 00:02:34.070
And as I mentioned in the system overview,
the only programming language

34
00:02:34.070 --> 00:02:38.190
that MapReduce provides
a native interface for is Java.

35
00:02:38.190 --> 00:02:43.500
Although, it's possible to run Python code
to implementation for it is more complex

36
00:02:43.500 --> 00:02:48.020
and not very efficient especially when
you are running not with text data, but

37
00:02:48.020 --> 00:02:49.320
with floating point numbers.

38
00:02:50.650 --> 00:02:54.417
The programming language issue
also affects how interactive

39
00:02:54.417 --> 00:02:55.830
the environment is.

40
00:02:55.830 --> 00:02:58.490
Most data scientist
prefer to use scripting

41
00:02:58.490 --> 00:03:02.120
languages due to their
interactive shell capabilities.

42
00:03:02.120 --> 00:03:06.670
Not having such an interface in Hadoop
really makes it difficult to use and

43
00:03:06.670 --> 00:03:07.940
adapt my many in the field.

44
00:03:09.530 --> 00:03:13.410
In addition in the big data
era having support for

45
00:03:13.410 --> 00:03:16.280
streaming data processing is a key for

46
00:03:16.280 --> 00:03:20.920
being able to run similar analysis on
both real time and historical data.

47
00:03:22.380 --> 00:03:26.160
Spark came out of the need to
extend the MapReduce framework

48
00:03:26.160 --> 00:03:28.690
to overcome this shortcomings and

49
00:03:28.690 --> 00:03:33.080
provide an expressive cluster computing
environment that can provide interactive

50
00:03:33.080 --> 00:03:37.820
querying, efficient iterative analytics
and streaming data processing.

51
00:03:39.140 --> 00:03:42.970
So, how does Apache Spark provide
solutions for these problems?

52
00:03:44.810 --> 00:03:48.820
Spark provides a very rich and
expressive programming module that

53
00:03:48.820 --> 00:03:53.690
gives you more than 20 highly efficient
distributed operations or transformations.

54
00:03:54.695 --> 00:03:58.810
Pipe-lining any of these steps in Spark
simply takes a few lines of code.

55
00:04:00.470 --> 00:04:05.040
Another important feature of Spark is
the ability to run these computations

56
00:04:05.040 --> 00:04:05.550
in memory.

57
00:04:06.610 --> 00:04:10.350
It's ability to cache and
process data in memory,

58
00:04:10.350 --> 00:04:14.325
makes it significantly faster for
iterative applications.

59
00:04:14.325 --> 00:04:19.665
This is proven to provide a factor
of ten or even 100 speed-up

60
00:04:19.665 --> 00:04:23.805
in the performance of some algorithms,
especially using large data sets.

61
00:04:25.635 --> 00:04:30.625
Additionally, Spark provides support for
batch and streaming workloads at once.

62
00:04:31.790 --> 00:04:36.675
Last but not least,
Spark provides simple APIs for Python,

63
00:04:36.675 --> 00:04:41.570
Scala, Java and SQL programming
through an interactive shell to

64
00:04:41.570 --> 00:04:46.590
accomplish analytical tasks through both
external and its built-in libraries.

65
00:04:48.750 --> 00:04:52.500
The Spark layer diagram,
also called Stack,

66
00:04:52.500 --> 00:04:57.250
consists of components that build on
top of the Spark computational engine.

67
00:04:58.510 --> 00:05:04.860
This engine distributes and monitors tasks
across the nodes of a commodity cluster.

68
00:05:05.960 --> 00:05:10.400
The components built on top of this
engine are designed to interact and

69
00:05:10.400 --> 00:05:12.320
communicate through this common engine.

70
00:05:13.430 --> 00:05:17.520
Any improvements to
the underlying engine becomes

71
00:05:17.520 --> 00:05:21.980
an improvement in the other components,
thanks to such close interaction.

72
00:05:23.390 --> 00:05:28.790
This also enables building applications
that's span across these different

73
00:05:28.790 --> 00:05:33.950
components like querying data using
Spark SQL and applying machine learning

74
00:05:33.950 --> 00:05:39.230
algorithms, the query results using Sparks
machine learning library and MLlib.

75
00:05:41.120 --> 00:05:45.240
The Spark Core is where
the core capability is

76
00:05:45.240 --> 00:05:48.520
of the Spark Framework are implemented.

77
00:05:48.520 --> 00:05:50.010
This includes support for

78
00:05:50.010 --> 00:05:54.460
distributed scheduling,
memory management and full tolerance.

79
00:05:55.570 --> 00:05:59.460
Interaction with different schedulers,
like YARN and Mesos and

80
00:05:59.460 --> 00:06:04.030
various NoSQL storage systems like
HBase also happen through Spark Core.

81
00:06:06.020 --> 00:06:10.068
A very important part of
Spark Core is the APIs for

82
00:06:10.068 --> 00:06:15.422
defining resilient distributed data sets,
or RDDs for short.

83
00:06:15.422 --> 00:06:18.858
RDDs are the main programming
abstraction in Spark,

84
00:06:18.858 --> 00:06:23.910
which carry data across many computing
nodes in parallel, and transform it.

85
00:06:23.910 --> 00:06:29.614
Spark SQL is the component of Spark
that provides querying structured and

86
00:06:29.614 --> 00:06:33.763
unstructured data through
a common query language.

87
00:06:33.763 --> 00:06:38.830
It can connect to many data sources and
provide APIs to convert

88
00:06:38.830 --> 00:06:43.714
query results to RDDs in Python,
Scala and Java programs.

89
00:06:43.714 --> 00:06:49.780
Spark Streaming is where data
manipulations take place in Spark.

90
00:06:51.060 --> 00:06:57.120
Although, not a native real-time interface
to datastreams, Spark streaming enables

91
00:06:57.120 --> 00:07:01.140
creating small aggregates of data coming
from streaming data ingestion systems.

92
00:07:03.490 --> 00:07:08.000
These aggregate datasets
are called micro-batches and

93
00:07:08.000 --> 00:07:11.680
they can be converted into RDBs in
Spark Streaming for processing.

94
00:07:13.740 --> 00:07:16.740
MLlib is Sparks native library for

95
00:07:16.740 --> 00:07:21.240
machine learning algorithms
as well as model evaluation.

96
00:07:21.240 --> 00:07:26.050
All of the functionality is potentially
ported to any programming language Sparks

97
00:07:26.050 --> 00:07:30.030
supports and
is designed to scale out using Spark.

98
00:07:31.190 --> 00:07:35.840
GraphX is the graph analytics
library of Spark and

99
00:07:35.840 --> 00:07:41.270
enables the Vertex edge data model of
graphs to be converted into RDDs as

100
00:07:41.270 --> 00:07:45.328
well as providing scalable implementations
of graph processing algorithms.

101
00:07:47.390 --> 00:07:52.655
To summarize, through these
layers Spark provides diverse,

102
00:07:52.655 --> 00:07:57.731
scalable interactive management and
analyses of big data.

103
00:07:57.731 --> 00:08:03.166
The interactive shell enables data
scientists to conduct exploratory

104
00:08:03.166 --> 00:08:08.416
analysis and create big data pipelines,
while also enabling the big

105
00:08:08.416 --> 00:08:13.941
data system integration engineers
to scale these analytical pipelines

106
00:08:13.941 --> 00:08:18.850
across commodity computing clusters and
cloud environments.
WEBVTT

1
00:00:01.670 --> 00:00:05.470
There are many big data
processing systems, but

2
00:00:05.470 --> 00:00:09.260
how do we make sense of them in order to
take full advantage of these systems?

3
00:00:10.320 --> 00:00:13.820
In this video,
we will review some of them, and

4
00:00:13.820 --> 00:00:17.600
a way to categorize big data processing
systems as we go through our review.

5
00:00:19.290 --> 00:00:24.330
After this video, you will be able
to recall the Hadoop ecosystem,

6
00:00:25.490 --> 00:00:30.050
draw a layer diagram with three layers for
data storage, data processing and

7
00:00:30.050 --> 00:00:31.930
workflow management.

8
00:00:31.930 --> 00:00:36.800
Summarize an evaluation criteria for
big data processing systems and

9
00:00:36.800 --> 00:00:41.363
explain the properties of Hadoop,
Spark, Flink,

10
00:00:41.363 --> 00:00:45.720
Beam, and Storm as major big
data processing systems.

11
00:00:46.750 --> 00:00:49.900
In our introduction to big data course,

12
00:00:49.900 --> 00:00:53.500
we talked about a version of
the layer diagram for the tools

13
00:00:53.500 --> 00:00:59.030
in the Hadoop ecosystem organized
vertically based on the interface.

14
00:00:59.030 --> 00:01:03.225
Lower level interface is the storage and
scheduling on the bottom and

15
00:01:03.225 --> 00:01:06.560
higher level languages and
interactivity at the top.

16
00:01:07.830 --> 00:01:12.040
Most of the tools in the Hadoop ecosystem
are initially built to complement

17
00:01:12.040 --> 00:01:17.410
the capabilities of Hadoop for distributed
filesystem management using HDFS.

18
00:01:18.440 --> 00:01:23.240
Data processing using the MapReduce
engine, and resource scheduling and

19
00:01:23.240 --> 00:01:25.210
negotiation using the YARN engine.

20
00:01:26.310 --> 00:01:30.170
Over time,
a number of new projects were built

21
00:01:30.170 --> 00:01:35.150
either to add to these
complimentary tools or to handle

22
00:01:35.150 --> 00:01:39.750
additional types of big data management
and processing not available in Hadoop.

23
00:01:41.520 --> 00:01:46.250
Arguably, the most important
change to Hadoop over time

24
00:01:46.250 --> 00:01:51.210
was the separation of YARN from
the MapReduce programming model

25
00:01:51.210 --> 00:01:54.510
to solely handle resource
management concerns.

26
00:01:55.780 --> 00:02:00.660
This allowed for Hadoop to be extensible
to different programming models.

27
00:02:00.660 --> 00:02:04.950
And enabled the development of
a number of processing engines for

28
00:02:04.950 --> 00:02:06.920
batch and stream processing.

29
00:02:08.250 --> 00:02:12.580
Another way to look at the vast number of
tools that have been added to the Hadoop

30
00:02:12.580 --> 00:02:15.170
ecosystem, is from the point of view of

31
00:02:15.170 --> 00:02:17.820
their functionality in the big
data processing pipeline.

32
00:02:18.870 --> 00:02:24.500
Simply put, these associate to three
distinct layers for data management and

33
00:02:24.500 --> 00:02:31.380
storage, data processing, and resource
coordination and workflow management.

34
00:02:32.550 --> 00:02:37.760
In our second course,
we talked about the bottom layer

35
00:02:37.760 --> 00:02:42.770
in this diagram in detail,
namely the data management and storage.

36
00:02:44.600 --> 00:02:50.440
While this layer includs Hadoop's HDFS
there are a number of other systems that

37
00:02:50.440 --> 00:02:56.250
rely on HDFS as a file system or implement
their own no SQL storage options.

38
00:02:58.290 --> 00:03:02.940
As big data can have a variety of
structure, semi structured and

39
00:03:02.940 --> 00:03:07.858
unstructured formats, and
gets analyzed through a variety of tools,

40
00:03:07.858 --> 00:03:12.890
many tools were introduced to
fit this variety of needs.

41
00:03:12.890 --> 00:03:15.180
We call these big data management systems.

42
00:03:18.040 --> 00:03:22.630
We reviewed redis and
Aerospike as key value stores,

43
00:03:22.630 --> 00:03:25.670
where each data item is
identified with a unique key.

44
00:03:27.150 --> 00:03:30.827
We got some practical
experience with Lucene and

45
00:03:30.827 --> 00:03:35.048
Gephi as vector and
graph data stores, respectively.

46
00:03:36.800 --> 00:03:40.505
We also talked about Vertica
as a column store database,

47
00:03:40.505 --> 00:03:44.300
where information is stored
in columns rather than rows.

48
00:03:45.880 --> 00:03:49.070
Cassandra and
Hbase are also in this category.

49
00:03:50.270 --> 00:03:56.402
Finally, we introduce Solr and
Asterisk DB for managing unstructured and

50
00:03:56.402 --> 00:04:02.260
semi-structured text, and
mongoDB as a document store.

51
00:04:02.260 --> 00:04:07.547
The processing layer is where these
different varieties of data gets

52
00:04:07.547 --> 00:04:14.030
retrieved, integrated, and analyzed,
which is the primary focus of this class.

53
00:04:16.232 --> 00:04:19.061
In the integration and processing layer

54
00:04:19.061 --> 00:04:23.966
we roughly refer to the tools that
are built on top of the HDFS and YARN,

55
00:04:23.966 --> 00:04:28.480
although some of them work with
other storage and file systems.

56
00:04:30.180 --> 00:04:37.140
YARN is a significant enabler of many of
these tools making a number of batch and

57
00:04:37.140 --> 00:04:42.111
stream processing engines like Storm,
Spark, Flink and being possible.

58
00:04:43.700 --> 00:04:47.340
We will revisit these processing
engines and explain why we have so

59
00:04:47.340 --> 00:04:48.890
many later in this lecture.

60
00:04:49.970 --> 00:04:54.940
This layer also includes tools like Hive,
or Spark SQL, for

61
00:04:54.940 --> 00:04:59.120
bringing a query interface
on top of the storage layer.

62
00:04:59.120 --> 00:05:04.280
Pig, for scripting simple big data
pipelines using the MapReduce framework.

63
00:05:04.280 --> 00:05:09.009
And a number of specialized analytical
libraries for machine learning and

64
00:05:09.009 --> 00:05:14.262
graph analytics, like Giraph as GraphX of
Spark are examples of such libraries for

65
00:05:14.262 --> 00:05:15.551
graph processing.

66
00:05:15.551 --> 00:05:20.187
And Mahout on top of the Hadoop stack and
MLlib of Spark are two options for

67
00:05:20.187 --> 00:05:21.480
machine learning.

68
00:05:22.640 --> 00:05:26.760
Although we have a basic overview of
graph processing and machine learning for

69
00:05:26.760 --> 00:05:31.700
big data analytics later in this course,
we won't go into the details here.

70
00:05:31.700 --> 00:05:36.530
Instead, we will have a dedicated
course on each of them

71
00:05:36.530 --> 00:05:38.670
later in this specialization.

72
00:05:38.670 --> 00:05:46.720
The third and top layer in our diagram is
the coordination and management layer.

73
00:05:46.720 --> 00:05:50.890
This is where integration,
scheduling, coordination, and

74
00:05:50.890 --> 00:05:56.410
monitoring of applications across many
tools in the bottom two layers take place.

75
00:05:57.440 --> 00:06:01.870
This layer is also where the results of
the big data analysis gets communicated to

76
00:06:01.870 --> 00:06:07.630
other programs, websites, visualization
tools, and business intelligence tools.

77
00:06:07.630 --> 00:06:12.830
Workflow management systems help to
develop automated solutions that can

78
00:06:12.830 --> 00:06:17.960
manage and coordinate the process of
combining data management and analytical

79
00:06:17.960 --> 00:06:23.264
tests in a big data pipeline, as
a configurable, structured set of steps.

80
00:06:25.020 --> 00:06:29.544
The workflow driven thinking also
matches this basic process of data

81
00:06:29.544 --> 00:06:31.970
science that we overviewed before.

82
00:06:33.220 --> 00:06:37.420
Oozie is an example of a workflow
scheduler that can interact with

83
00:06:37.420 --> 00:06:40.295
many of the tools in the integration and
processing layer.

84
00:06:41.690 --> 00:06:46.390
Zookeeper is the resource coordination and
monitoring tool and

85
00:06:46.390 --> 00:06:50.990
manages and coordinates all these tools
and middleware named after animals.

86
00:06:52.460 --> 00:06:56.429
Although virtual management is
my personal research area and

87
00:06:56.429 --> 00:06:58.793
I talk more about it in other venues,

88
00:06:58.793 --> 00:07:03.999
in this specialization we focus mainly
on big data integration and processing.

89
00:07:03.999 --> 00:07:09.450
And we will not have a specific
lecture on this layer in this course.

90
00:07:09.450 --> 00:07:13.430
We give you a reading on big data
workflows after this video as

91
00:07:13.430 --> 00:07:16.750
further information and
a starting point for the subject.
WEBVTT

1
00:00:02.380 --> 00:00:06.830
In data integration and
processing pipelines,

2
00:00:06.830 --> 00:00:11.910
data goes through a number of operations,
which can apply

3
00:00:11.910 --> 00:00:17.960
a specific function to it, can work
the data from one format to another,

4
00:00:17.960 --> 00:00:24.097
join data with other data sets, or
filter some values out of a data set.

5
00:00:25.240 --> 00:00:31.350
We generally refer to these as
transformations, some of which can also

6
00:00:31.350 --> 00:00:36.950
be specially named aggregations as you
have seen in Amarnath's earlier lectures.

7
00:00:36.950 --> 00:00:43.010
In this video we will reveal some common
transformation operations that we see

8
00:00:43.010 --> 00:00:48.490
in these pipelines, some of which,
we refer to as data parallel patterns.

9
00:00:50.575 --> 00:00:55.265
After this video you will
be able to list common data

10
00:00:55.265 --> 00:01:00.355
transformations within big data pipelines,
and design

11
00:01:00.355 --> 00:01:05.235
a conceptual data processing pipeline
using the basic data transformations.

12
00:01:07.551 --> 00:01:12.770
Simply speaking, transformations
are higher order functions or

13
00:01:12.770 --> 00:01:17.960
tools to convert your data from
one form to another, just like

14
00:01:17.960 --> 00:01:23.530
we would use tools at the wood shop
to transform logs into furniture.

15
00:01:24.900 --> 00:01:27.630
When we look at big data
pipelines used today,

16
00:01:28.650 --> 00:01:32.840
map is probably the most
common transformation we find.

17
00:01:34.340 --> 00:01:39.230
The map operation is one of the basic
building blocks of the big data pipeline.

18
00:01:40.760 --> 00:01:46.200
When you want to apply a process
to each member of a collection,

19
00:01:46.200 --> 00:01:49.370
such as adding 10% bonus to each

20
00:01:49.370 --> 00:01:53.770
person's salary on a given month a map
operation comes in very handy.

21
00:01:55.520 --> 00:02:00.490
It takes your process and
understand that it is required to perform

22
00:02:00.490 --> 00:02:04.370
the same operation or
process to each member of the set.

23
00:02:06.610 --> 00:02:10.970
The figure on the left
here shows the application

24
00:02:10.970 --> 00:02:15.396
of a map function to data
depicted in grey color.

25
00:02:15.396 --> 00:02:22.800
Here colors red, blue, and
yellow are keys to identify each data set.

26
00:02:24.480 --> 00:02:31.130
As you see, each data set is executed
separately even for the same colored key.

27
00:02:35.010 --> 00:02:39.944
The reduce operation helps you then
to collectively apply the same

28
00:02:39.944 --> 00:02:42.860
process to objects of similar nature.

29
00:02:44.680 --> 00:02:50.030
For example, when you want to add your
monthly spending in different categories,

30
00:02:50.030 --> 00:02:56.350
like grocery, fuel, and dining out,
the reduce operation is very useful.

31
00:02:58.500 --> 00:03:01.900
In our figure here on the top left,

32
00:03:01.900 --> 00:03:06.472
we see that data sets in
grey with the same color

33
00:03:06.472 --> 00:03:11.760
are keys grouped together
using a reduced function.

34
00:03:12.940 --> 00:03:16.320
Reds together, blues together,
and yellows together.

35
00:03:18.780 --> 00:03:23.579
It would be a good idea to check out
the Spark word count hands-on to see

36
00:03:23.579 --> 00:03:28.400
how map and reduce can be used
effectively for getting things done.

37
00:03:29.560 --> 00:03:34.550
Map and reduce are types of
transformations that work on a single

38
00:03:34.550 --> 00:03:39.460
list of key and data pairings just
like we see on the left of our figure.

39
00:03:42.460 --> 00:03:47.520
Now let's consider a scenario where
we have two data sets identified

40
00:03:47.520 --> 00:03:53.090
by the same keys just like the two
sets and colors in our diagram.

41
00:03:55.140 --> 00:04:00.120
Many operations have such needs where
we have to look at all the pairings of

42
00:04:00.120 --> 00:04:04.740
all key value pairs,
just like crossing two matrices.

43
00:04:06.900 --> 00:04:11.800
For a practical example
Imagine you have two teams,

44
00:04:11.800 --> 00:04:16.870
a sales team with two people, and
an operations team with four people.

45
00:04:18.050 --> 00:04:22.940
In an event you would want each
person to meet every other person.

46
00:04:24.030 --> 00:04:27.210
In this case, a cross product, or

47
00:04:27.210 --> 00:04:32.200
a cartesian product, becomes a good
choice for organizing the event and

48
00:04:32.200 --> 00:04:36.000
sharing each pairs' meeting location and
travel time to them.

49
00:04:37.770 --> 00:04:43.470
In a cross or cartesian product operation,
each data partition gets

50
00:04:43.470 --> 00:04:49.450
paired with all other data partitions,
regardless of its key.

51
00:04:49.450 --> 00:04:52.370
This sometimes gets
referred to as all pairs.

52
00:04:54.990 --> 00:05:00.160
Now add to the cross product by
just grouping together the data

53
00:05:00.160 --> 00:05:05.490
partitions with the same key,
just like the red data.

54
00:05:07.090 --> 00:05:09.128
And the yellow data partitions here.

55
00:05:12.005 --> 00:05:16.930
This is a typical match or join operation.

56
00:05:16.930 --> 00:05:22.190
As we see in the figure here, match
is very similar to the cross product,

57
00:05:22.190 --> 00:05:25.760
except that it is more
selective in forming pairs.

58
00:05:26.810 --> 00:05:29.420
Every pair must have something in common.

59
00:05:30.510 --> 00:05:34.920
This something in common is
usually referred to as a key.

60
00:05:36.490 --> 00:05:40.330
For example,
each person in your operations team and

61
00:05:40.330 --> 00:05:43.510
sales team is assigned
to a different product.

62
00:05:43.510 --> 00:05:48.500
You only want those people to meet
who are working on the same product.

63
00:05:48.500 --> 00:05:52.490
In this case your key is product.

64
00:05:52.490 --> 00:05:55.290
And you can perform and
match operation and

65
00:05:55.290 --> 00:05:59.670
send e-mails to those people
who share a common product.

66
00:06:01.530 --> 00:06:06.756
The number of e-mails is likely to be less
than when you performed a cartesian or

67
00:06:06.756 --> 00:06:10.980
a cross product, therefore reducing
the cost of the operation.

68
00:06:12.270 --> 00:06:19.170
In a match operation, only the keys
with data in both sets get joined,

69
00:06:20.240 --> 00:06:23.920
and become a part of the final
output of the transformation.

70
00:06:26.600 --> 00:06:31.600
Now let's consider listing
the data sets with all the keys,

71
00:06:31.600 --> 00:06:33.790
even if they don't exist in both sets.

72
00:06:35.850 --> 00:06:40.550
Consider a scenario where you
want to do brainstorming sessions

73
00:06:40.550 --> 00:06:43.690
of people from operations and sales, and

74
00:06:43.690 --> 00:06:47.560
get people who work on the same
products in the same rooms.

75
00:06:50.390 --> 00:06:52.750
A co-group operation will do this for you.

76
00:06:54.290 --> 00:06:58.160
You give it a product name
as they key to work with and

77
00:06:58.160 --> 00:07:01.530
the two tables, the sales team and
operations team.

78
00:07:02.800 --> 00:07:07.740
The co-group will create groups
which contain team members

79
00:07:07.740 --> 00:07:13.690
working on common products even if a
product doesn't exist in one of the sets.

80
00:07:16.520 --> 00:07:20.520
The last operation we will
see is the filter operation.

81
00:07:20.520 --> 00:07:23.760
Filter works much like a test

82
00:07:23.760 --> 00:07:28.340
where only elements that pass
a test are shown in the output.

83
00:07:30.040 --> 00:07:34.770
Consider as a set that contains teams and
a number of members in their teams.

84
00:07:35.890 --> 00:07:38.930
If your game requires people to pair up,

85
00:07:38.930 --> 00:07:42.570
you may want to select teams which
have an even number of members.

86
00:07:43.570 --> 00:07:49.130
In this case, you can create a test
that only passes the teams which have

87
00:07:49.130 --> 00:07:56.690
an even number of team members shown as
divided by 2 with 0 in the remainder.

88
00:07:59.608 --> 00:08:04.147
The real effectiveness of the basic
transformation we saw here

89
00:08:04.147 --> 00:08:09.025
is in pipelining them in a way that
helps you to solve your specific

90
00:08:09.025 --> 00:08:13.819
problem just as you would perform
a series of tasks on a real block

91
00:08:13.819 --> 00:08:18.610
of wood to make a fine piece of
woodwork that you can use to steer your

92
00:08:18.610 --> 00:08:22.495
ship, which in this case is
your business or research.
WEBVTT

1
00:00:00.550 --> 00:00:03.512
Now that we revealed all three layers,

2
00:00:03.512 --> 00:00:08.556
we are ready to come back to
the Integration and Processing layer.

3
00:00:08.556 --> 00:00:12.652
Just a simple Google search for
Big Data Processing Pipelines

4
00:00:12.652 --> 00:00:17.378
will bring a vast number of pipelines
with large number of technologies

5
00:00:17.378 --> 00:00:22.041
that support scalable data cleaning,
preparation, and analysis.

6
00:00:24.091 --> 00:00:28.286
How do we make sense of it all to
make sure we use the right tools for

7
00:00:28.286 --> 00:00:29.540
our application?

8
00:00:30.660 --> 00:00:35.490
We will continue our lecture to review
a set of evaluation criteria for

9
00:00:35.490 --> 00:00:40.750
these systems and some of the big data
processing systems based on this criteria.

10
00:00:42.250 --> 00:00:47.850
Depending on the resources we have access
to and characteristics of our application,

11
00:00:47.850 --> 00:00:51.130
we apply several
considerations to evaluate and

12
00:00:51.130 --> 00:00:52.920
pick a software stack for big data.

13
00:00:54.010 --> 00:00:59.410
Of these, first one we consider
is the Execution Model,

14
00:00:59.410 --> 00:01:04.450
and the expressivity of it to support for
various transformations of batch or

15
00:01:04.450 --> 00:01:07.430
streaming data, or
sometimes interactive computing.

16
00:01:08.920 --> 00:01:12.550
Semantics of streaming,
including exactly once or

17
00:01:12.550 --> 00:01:17.620
at least one processing for each event, or
being able to keep the state of the data,

18
00:01:17.620 --> 00:01:21.680
is an important concern for
this execution model.

19
00:01:21.680 --> 00:01:26.110
Latency is another important criteria,
depending on the application.

20
00:01:27.210 --> 00:01:30.760
Having a low latency system
is very important for

21
00:01:30.760 --> 00:01:34.345
applications like online gaming and
hazards management.

22
00:01:34.345 --> 00:01:38.340
Whereas most applications
are less time critical,

23
00:01:38.340 --> 00:01:43.800
like search engine indexing, and would
be fine with a batch processing ability.

24
00:01:44.820 --> 00:01:49.760
Scalability for both small and
large datasets and different

25
00:01:49.760 --> 00:01:54.070
analytical methods and algorithms,
is also an important evaluation criteria.

26
00:01:55.290 --> 00:01:59.430
As well as support for
different programming language

27
00:01:59.430 --> 00:02:03.470
of the libraries used by the analytical
tools that we have access to.

28
00:02:04.700 --> 00:02:09.030
Finally, while all big data
tools provide fault tolerance,

29
00:02:09.030 --> 00:02:14.390
the mechanics of how the fault tolerance is
handled is an important issue to consider.

30
00:02:15.790 --> 00:02:18.900
Let's review five of
the big data processing

31
00:02:18.900 --> 00:02:23.390
engines supported by the Apache Foundation
using this evaluation criteria.

32
00:02:25.300 --> 00:02:30.230
The MapReduce implementation of
Hadoop provides a batch execution

33
00:02:30.230 --> 00:02:35.230
model where the data from HDFS gets
loaded into mappers before processing.

34
00:02:36.440 --> 00:02:39.380
There is no in-memory processing support,

35
00:02:39.380 --> 00:02:44.490
meaning the mappers write the data on
files before the reducers can read it,

36
00:02:44.490 --> 00:02:48.250
resulting in a high-latency and
less scalable execution.

37
00:02:49.960 --> 00:02:53.690
This also hinders
the performance of iterative and

38
00:02:53.690 --> 00:02:58.827
interactive applications that require many
steps of transformations using MapReduce.

39
00:03:01.150 --> 00:03:04.835
Although the only native
programming interface for

40
00:03:04.835 --> 00:03:09.870
MapReduce is in Java, other programming
languages like Python provide modules or

41
00:03:09.870 --> 00:03:14.370
libraries for Hadoop MapReduce
programming, however with less efficiency.

42
00:03:16.180 --> 00:03:20.730
Data replication is the primary
method of fault tolerance,

43
00:03:20.730 --> 00:03:26.270
which in turn affects the scalability and
execution speed further.

44
00:03:26.270 --> 00:03:29.560
Spark was built to support iterative and

45
00:03:29.560 --> 00:03:34.730
interactive big data processing
pipelines efficiently using an in-memory

46
00:03:34.730 --> 00:03:39.800
structure called Resilient Distributed
Datasets, or shortly, RDDs.

47
00:03:41.040 --> 00:03:45.860
In addition to map and reduce operations,
it provides support for

48
00:03:45.860 --> 00:03:49.890
a range of transformation
operations like join and filter.

49
00:03:49.890 --> 00:03:55.711
Any pipeline of transformations can
be applied to these RDD's in-memory,

50
00:03:55.711 --> 00:04:00.829
making Spark's performance very high for
iterative processing.

51
00:04:02.538 --> 00:04:07.480
The RDD extraction is also designed
to handle fault tolerance with

52
00:04:07.480 --> 00:04:09.230
less impact on performance.

53
00:04:10.886 --> 00:04:13.170
In addition to HDFS,

54
00:04:13.170 --> 00:04:18.200
Spark can read data from many storage
platforms and it provides support for

55
00:04:18.200 --> 00:04:24.155
streaming data applications using
a technique called micro-batching.

56
00:04:24.155 --> 00:04:30.250
Its latency can be on the order of
seconds depending on the batch size,

57
00:04:30.250 --> 00:04:34.030
which is relatively slower compared
to native streaming platforms.

58
00:04:35.600 --> 00:04:40.320
Spark has support for a number of
programming languages, including Scala and

59
00:04:40.320 --> 00:04:44.260
Python as the most popular ones,
as well as built-in libraries for

60
00:04:44.260 --> 00:04:46.500
graph processing and machine learning.

61
00:04:47.530 --> 00:04:51.415
Although Flink has very
similar transformations and

62
00:04:51.415 --> 00:04:55.890
in-memory data extractions with Spark,
it provides direct support for

63
00:04:55.890 --> 00:04:59.517
streaming data,
making it a lower-latency framework.

64
00:05:00.620 --> 00:05:04.720
It provides connection interfaces to
streaming data ingestion engines like

65
00:05:04.720 --> 00:05:06.020
Kafka and Flume.

66
00:05:07.460 --> 00:05:11.340
Flink supports application
programming interfaces in Java and

67
00:05:11.340 --> 00:05:12.966
Scala just like Spark.

68
00:05:12.966 --> 00:05:17.760
Starting with it's original
version called Stratosphere,

69
00:05:17.760 --> 00:05:23.010
Flink had it's own execution engine
called Nephele, and had an ability

70
00:05:23.010 --> 00:05:27.670
to run both on Hadoop and also separately
in its own execution environment.

71
00:05:29.060 --> 00:05:33.110
In addition to map and reduce,
Flink provides abstractions for

72
00:05:33.110 --> 00:05:37.055
other data parallel database
patterns like join and group by.

73
00:05:39.210 --> 00:05:44.010
One of the biggest advantage of using
Flink comes from it's optimizer

74
00:05:44.010 --> 00:05:47.460
to pick and apply the best pattern and
execution strategy.

75
00:05:48.490 --> 00:05:52.080
There has been experiments comparing
fault tolerance features of Flink

76
00:05:52.080 --> 00:05:56.720
to those of Sparks, which conclude
that Sparks slightly better for Spark.

77
00:05:57.840 --> 00:06:03.080
The Beam system, from Google,
is a relatively new system for

78
00:06:03.080 --> 00:06:06.890
batch and stream processing with
a data flow programming model.

79
00:06:08.080 --> 00:06:13.090
It initially used Google's own Cloud data
flow as an execution environment, but

80
00:06:13.090 --> 00:06:17.228
Spark and Flink backends for
it have been implemented recently.

81
00:06:17.228 --> 00:06:22.370
It's a low-latency environment with
high reviews on fault tolerance.

82
00:06:23.460 --> 00:06:28.456
It currently provides application
programming interfaces in Java and

83
00:06:28.456 --> 00:06:31.295
Scala, and a Python SDK is in the works.

84
00:06:31.295 --> 00:06:34.532
SDK means software development kit.

85
00:06:34.532 --> 00:06:40.690
Beam provides a very strong streaming and
windowing framework for streaming data.

86
00:06:40.690 --> 00:06:45.417
And it is highly scalable and reliable,
allowing it to make trade-off

87
00:06:45.417 --> 00:06:49.690
decisions between accuracy,
speed, and cost of processing.

88
00:06:51.710 --> 00:06:53.730
Storm has been designed for

89
00:06:53.730 --> 00:06:58.960
stream processing in real
time with very low-latency.

90
00:06:58.960 --> 00:07:03.930
It defined input stream interface
abstractions called spouts, and

91
00:07:03.930 --> 00:07:06.080
computation abstractions called bolts.

92
00:07:07.420 --> 00:07:11.700
Spouts and bolts can be pipelined
together using a data flow approach.

93
00:07:11.700 --> 00:07:15.430
That data gets queued until
the computation acknowledges

94
00:07:15.430 --> 00:07:16.190
the receipt of it.

95
00:07:18.310 --> 00:07:21.080
A master node tracks running jobs and

96
00:07:21.080 --> 00:07:24.860
ensures all data is processed
by the computations on workers.

97
00:07:26.430 --> 00:07:31.952
Nathan Mars, the lead developer for
Storm, built the Lambda Architecture

98
00:07:31.952 --> 00:07:37.744
using Storm for stream processing and
Hadoop MapReduce for batch processing.

99
00:07:40.660 --> 00:07:45.740
The Lambda Architecture
originally used Storm for

100
00:07:45.740 --> 00:07:50.229
speed layer and Hadoop and
HBase for batch and

101
00:07:50.229 --> 00:07:54.490
serving layers, as seen in this diagram.

102
00:07:55.840 --> 00:08:00.760
However, it was later used as a more
general framework that can combine

103
00:08:00.760 --> 00:08:06.990
the results of stream and batch processing
executed in multiple big data systems.

104
00:08:06.990 --> 00:08:12.700
This diagram shows a generalized Lambda
Architecture containing some of the tools

105
00:08:12.700 --> 00:08:19.930
we discussed earlier, including using
Spark for both batch and speed layers.

106
00:08:21.540 --> 00:08:25.530
In this course, we picked Spark
as a big data integration and

107
00:08:25.530 --> 00:08:30.600
processing environment since it supports
most of our evaluation criteria.

108
00:08:30.600 --> 00:08:35.862
And this hybrid data processing
architecture using built-in data querying,

109
00:08:35.862 --> 00:08:38.619
streaming, and analytical libraries.

110
00:08:38.619 --> 00:08:43.586
We will continue our discussion with
Spark and hands-on exercises in Spark.
WEBVTT

1
00:00:01.740 --> 00:00:04.460
Analytical operations
in big data pipelines.

2
00:00:05.940 --> 00:00:07.730
After this video,

3
00:00:07.730 --> 00:00:13.080
you will be able to list common analytical
operations within big data pipelines and

4
00:00:13.080 --> 00:00:16.650
describe sample applications for
these analytical operations.

5
00:00:18.550 --> 00:00:24.200
In this lesson, we will be
looking at analytical operations.

6
00:00:24.200 --> 00:00:28.810
These are operations used in analytics,
which is the process of

7
00:00:28.810 --> 00:00:33.370
transforming data into insights for
making more informed decisions.

8
00:00:34.690 --> 00:00:39.720
The purpose of analytical operations is to
analyze the data to discover meaningful

9
00:00:39.720 --> 00:00:44.089
trends and patterns, in order to gain
insights into the problem being studied.

10
00:00:45.138 --> 00:00:47.810
The knowledge gained from these insights

11
00:00:47.810 --> 00:00:51.890
ultimately lead to more informed
decisions driven by data.

12
00:00:54.150 --> 00:00:58.326
Here are some common analytical operations
that we will discuss in this lecture.

13
00:00:58.326 --> 00:01:02.704
Classification, clustering,

14
00:01:02.704 --> 00:01:06.380
path analysis and connectivity analysis.

15
00:01:07.630 --> 00:01:09.490
Let's start with classification.

16
00:01:10.730 --> 00:01:18.130
In classification, the goal is to predict
a categorical target from the input data.

17
00:01:18.130 --> 00:01:21.290
A categorical target is one
with discreet values or

18
00:01:21.290 --> 00:01:23.820
categories, instead of continuous values.

19
00:01:25.470 --> 00:01:28.460
For example, this diagram shows

20
00:01:28.460 --> 00:01:34.260
a classification task to determine the
risk associated with a loan application.

21
00:01:34.260 --> 00:01:37.801
The input consists of the loan amount,

22
00:01:37.801 --> 00:01:44.571
applicant information such as income,
age, debts, and a down payment.

23
00:01:46.202 --> 00:01:50.903
From this input data,
the task is to determine whether

24
00:01:50.903 --> 00:01:54.898
the loan application is low risk or
high risk.

25
00:01:57.216 --> 00:02:00.852
There are many classification techniques
or algorithms that can be used for

26
00:02:00.852 --> 00:02:02.480
this problem.

27
00:02:02.480 --> 00:02:07.160
We will discuss a specific one, namely,
decision tree in the next slide.

28
00:02:09.330 --> 00:02:13.590
The decision tree algorithm is
one technique for classification.

29
00:02:13.590 --> 00:02:15.000
With this technique,

30
00:02:15.000 --> 00:02:20.040
decisions to perform the classification
task are modeled as a tree structure.

31
00:02:22.070 --> 00:02:26.770
For the loan risk assessment problem,
a simple decision tree is shown here,

32
00:02:27.960 --> 00:02:32.470
where the loan application is
classified as being either low risk, or

33
00:02:32.470 --> 00:02:34.930
high risk, based on the loan amount.

34
00:02:34.930 --> 00:02:37.750
The applicant's income,
and the applicant's age.

35
00:02:40.090 --> 00:02:44.110
The decision tree algorithm is implemented
in many machine learning tools.

36
00:02:45.310 --> 00:02:51.718
This diagram shows how to specify
decision tree from input data, KNIME.

37
00:02:51.718 --> 00:02:55.650
A graphical user-interface-based
machine learning platform.

38
00:02:57.370 --> 00:03:02.400
Some examples of classification
are the prediction of whether cells from

39
00:03:02.400 --> 00:03:07.540
a tumor are benign or
malignant, categorization of

40
00:03:07.540 --> 00:03:12.630
handwritten digits as being zero,
one, two, etc, up to nine.

41
00:03:14.180 --> 00:03:18.760
And determining whether a credit card
transaction is legitimate or fraudulent,

42
00:03:20.940 --> 00:03:25.255
and classification of a loan application
as being low-risk, medium-risk or

43
00:03:25.255 --> 00:03:26.540
high-risk, as you've seen.

44
00:03:28.370 --> 00:03:31.970
Another common analytical
operation is cluster analysis.

45
00:03:33.180 --> 00:03:35.880
In cluster analysis, or clustering,

46
00:03:35.880 --> 00:03:40.320
the goal is to organize similar
items in to groups of association.

47
00:03:41.880 --> 00:03:47.045
This diagram shows an example of cluster
analysis in which customers are clustered

48
00:03:47.045 --> 00:03:51.510
into groups according to their
preferences of movie genre.

49
00:03:53.080 --> 00:03:56.450
So, customers who like Sci-Fi
movies are grouped together.

50
00:03:58.010 --> 00:04:01.814
Those who like drama movies
are grouped together,

51
00:04:01.814 --> 00:04:06.337
and customers who like horror
movies are grouped together.

52
00:04:06.337 --> 00:04:10.482
With this grouping, new movies,
as well as other products,

53
00:04:10.482 --> 00:04:15.184
such as books, can be offered to
the right type of costumers in order to

54
00:04:15.184 --> 00:04:18.073
generate interest and increase revenue.

55
00:04:20.775 --> 00:04:25.516
A simple and commonly used algorithm for
cluster analysis is k-means.

56
00:04:27.163 --> 00:04:31.670
With k-means,
samples are divided into k clusters.

57
00:04:31.670 --> 00:04:35.100
This clustering is done in order
to minimize the variance or

58
00:04:35.100 --> 00:04:38.620
similarity between samples
within the same cluster

59
00:04:38.620 --> 00:04:41.590
using some similarity
measures such as distance.

60
00:04:42.690 --> 00:04:47.210
In this example, k is equal to three, and

61
00:04:47.210 --> 00:04:53.170
k-means divides the original data shown
on the left into three clusters,

62
00:04:53.170 --> 00:04:57.620
shown as blue, green, and
red on the chart on the right.

63
00:04:59.700 --> 00:05:03.530
The k-means clustering algorithm is
implemented on many machine-learning

64
00:05:03.530 --> 00:05:04.180
platforms.

65
00:05:05.360 --> 00:05:08.550
The code here shows how to read in and

66
00:05:08.550 --> 00:05:13.190
parse input data, and
perform k-means clustering on the data.

67
00:05:13.190 --> 00:05:18.210
Other examples of cluster analysis
are grouping a company’s customer base

68
00:05:18.210 --> 00:05:24.560
into distinct segments for more effective
targeted marketing, finding articles or

69
00:05:24.560 --> 00:05:28.240
webpages with similar topics for
retrieving relevant information.

70
00:05:30.060 --> 00:05:35.844
Identification of areas in the city with
rates of particular types of crimes for

71
00:05:35.844 --> 00:05:40.077
effective management of law
enforcement resources, and

72
00:05:40.077 --> 00:05:46.055
determining different groups of weather
patterns such as rainy, cold or snowy.

73
00:05:48.190 --> 00:05:51.632
Classification and cluster analysis
are considered machine learning and

74
00:05:51.632 --> 00:05:53.620
analytical operations.

75
00:05:53.620 --> 00:05:56.790
There are also analytical
operations from graph analytics,

76
00:05:56.790 --> 00:06:01.790
which is the field of analytics where
the underlying data is structured as, or

77
00:06:01.790 --> 00:06:03.670
can be modeled as the set of graphs.

78
00:06:05.020 --> 00:06:09.190
One analytical operation using
graphs as path analysis,

79
00:06:09.190 --> 00:06:13.000
which analyzes sequences of nodes and
edges in a graph.

80
00:06:14.390 --> 00:06:17.500
A common application of path analysis

81
00:06:17.500 --> 00:06:21.980
is to find routes from one
location to another location.

82
00:06:21.980 --> 00:06:27.440
For example, you might want to find the
shortest path from your home to your work.

83
00:06:27.440 --> 00:06:32.100
This path may be different depending on
conditions such as the day of the week,

84
00:06:32.100 --> 00:06:36.450
time of day, traffic congestion,
weather and etc.

85
00:06:38.390 --> 00:06:43.530
This code shows some operations for
path analysis on neo4j,

86
00:06:43.530 --> 00:06:48.033
which is a graph database system
using a query language called Cypher.

87
00:06:49.210 --> 00:06:54.440
The first operation finds the shortest
path between specific nodes in a graph.

88
00:06:55.610 --> 00:06:59.460
The second operation finds all
the shortest paths in a graph.

89
00:07:00.890 --> 00:07:04.770
Connectivity analysis of graphs
has to do with finding and

90
00:07:04.770 --> 00:07:07.970
tracking groups to determine
interactions between entities.

91
00:07:09.110 --> 00:07:13.330
Entities in highly interacting
groups are more connected

92
00:07:13.330 --> 00:07:17.680
to each other than to entities
of other groups in a graph.

93
00:07:18.820 --> 00:07:21.860
These groups are called communities, and

94
00:07:21.860 --> 00:07:26.280
are interesting to analyze as they
give insights into the degree and

95
00:07:26.280 --> 00:07:31.410
patterns of the interaction between
entities, and also between communities.

96
00:07:32.880 --> 00:07:39.570
Some applications of connectivity analysis
are to extract conversation threads.

97
00:07:39.570 --> 00:07:42.030
For example,
by looking at tweets and retweets.

98
00:07:43.250 --> 00:07:47.720
To find interacting groups, for example,
to determine which users are interacting

99
00:07:47.720 --> 00:07:53.900
with each other users, to find
influencers, for example, to understand

100
00:07:53.900 --> 00:07:58.720
who are the main users leading to
the conversation about a particular topic.

101
00:07:58.720 --> 00:08:01.550
Or, who do people pay attention to?

102
00:08:01.550 --> 00:08:05.020
This information can be used to
identify the fewest number of

103
00:08:05.020 --> 00:08:06.990
people with the greatest influence.

104
00:08:06.990 --> 00:08:11.830
For example, for political campaigns,
or marketing on social media.

105
00:08:13.360 --> 00:08:15.680
This code shows some operations for

106
00:08:15.680 --> 00:08:20.210
connectivity analysis on neo4j using
the query language, Cypher, again.

107
00:08:21.980 --> 00:08:25.660
The first operation finds the degree
of all the nodes in a graph,

108
00:08:25.660 --> 00:08:30.450
and the second creates a histogram
of degrees for all nodes in a graph

109
00:08:30.450 --> 00:08:35.990
to determine how connected a node in
a graph is, we need to look at its degree.

110
00:08:35.990 --> 00:08:39.860
The degree of a node is the number
of edges connected to the node.

111
00:08:41.220 --> 00:08:45.250
A degree histogram shows the distribution
of node degrees in the graph and

112
00:08:45.250 --> 00:08:50.330
is useful in comparing graphs and
identifying types of users, for

113
00:08:50.330 --> 00:08:55.410
example, those who follow, versus those
who are followed in social networks.

114
00:08:56.940 --> 00:09:01.970
To summarize and add to these techniques,
the decision tree algorithm for

115
00:09:01.970 --> 00:09:06.470
classification and k-means algorithm for
cluster analysis that we covered in this

116
00:09:06.470 --> 00:09:09.040
lecture are techniques
from machine learning.

117
00:09:10.210 --> 00:09:15.650
Machine learning is a field of analytics
focused on the study and construction of

118
00:09:15.650 --> 00:09:20.819
computer systems that can learn from data
without being explicitly programmed.

119
00:09:22.140 --> 00:09:25.540
Our course on machine learning in
this specialization will cover these

120
00:09:25.540 --> 00:09:29.230
algorithms in more detail,
along with other algorithms used for

121
00:09:29.230 --> 00:09:31.660
classification and cluster analysis.

122
00:09:31.660 --> 00:09:34.690
As well as algorithms for
other machine learning tasks,

123
00:09:34.690 --> 00:09:39.360
such as regression,
association analysis, and tools for

124
00:09:39.360 --> 00:09:42.480
implementing and
executing machine learning algorithms.

125
00:09:44.380 --> 00:09:49.130
As a summary of the Graph Analytics,
the Path Analytics technique for finding

126
00:09:49.130 --> 00:09:53.390
the shortest path and the connectivity
analysis technique for analyzing communities

127
00:09:53.390 --> 00:09:58.260
that we discussed earlier,
are techniques used in graph analytics.

128
00:09:58.260 --> 00:10:02.970
As explained earlier,
graph analytics is the field of analytics,

129
00:10:02.970 --> 00:10:07.520
where the underlying data is structured or
can be modeled as a set of graphs.

130
00:10:08.760 --> 00:10:13.200
Our graph analytics course in the
specialization will cover these and other

131
00:10:13.200 --> 00:10:18.174
graph techniques, and we'll also cover
tools and platforms for graph analytics.

132
00:10:18.174 --> 00:10:25.610
In summary, analytic operations
are used to discover meaningful

133
00:10:25.610 --> 00:10:31.115
patterns in the data in order to provide
insights into the problem being studied.

134
00:10:31.115 --> 00:10:36.635
We looked at some of the examples of
analytical operations for classification,

135
00:10:36.635 --> 00:10:41.355
cluster analysis, path analysis and
connectivity analysis in this lecture.
WEBVTT

1
00:00:01.226 --> 00:00:02.488
In this hands-on activity,

2
00:00:02.488 --> 00:00:05.520
we'll be performing word count on
the complete works of Shakespeare.

3
00:00:07.110 --> 00:00:11.350
First, we will copy the Shakespeare
text into the Hadoop file system.

4
00:00:11.350 --> 00:00:14.847
Next, we will create a new
Jupyter Notebook, and

5
00:00:14.847 --> 00:00:17.937
read the Shakespeare
text into a Spark RDD.

6
00:00:17.937 --> 00:00:21.177
We will then perform WordCount
using map and reduce,

7
00:00:21.177 --> 00:00:24.352
and write the results to HDFS and
view the contents.

8
00:00:27.553 --> 00:00:29.301
Let's begin.

9
00:00:29.301 --> 00:00:33.300
In the intro to big data course,
we copy the Shakespeare text into HDFS.

10
00:00:33.300 --> 00:00:36.630
Let's see if it's still there.

11
00:00:36.630 --> 00:00:38.150
If not, we can copy it now.

12
00:00:38.150 --> 00:00:42.180
Click on the terminal icon
at the top of the toolbar.

13
00:00:43.950 --> 00:00:49.830
Now we can run hadoop fs- ls to see what's
in our hadoop filesystem directory.

14
00:00:51.900 --> 00:00:54.380
There are no files in HTFS,
so let's copy it.

15
00:00:54.380 --> 00:01:01.070
If you already have words.txt in your HTFS
directory, you can skip this next step.

16
00:01:02.450 --> 00:01:06.837
Cd into downloads,

17
00:01:06.837 --> 00:01:15.359
big-data-3/spark-wordcount.

18
00:01:15.359 --> 00:01:17.450
We can do ls to see the file.

19
00:01:18.960 --> 00:01:21.390
Let's copy this file to HTFS.

20
00:01:21.390 --> 00:01:28.221
We run Hadoop,
fs copy from local, words.txt.

21
00:01:33.314 --> 00:01:37.800
We can write Hadoop fs -ls again
to verify that the file is there.

22
00:01:41.290 --> 00:01:42.600
Now let's do work count in spark.

23
00:01:44.220 --> 00:01:47.380
We will do this in an iPython
notebook using Jupyter server.

24
00:01:49.070 --> 00:01:51.587
Look on the web browser icon,
the top of the toolbar.

25
00:01:55.112 --> 00:01:59.706
And go to the Jupyter server URL,
which is local host port 8889.

26
00:02:02.360 --> 00:02:04.789
Next, let's create a new iPython notebook

27
00:02:10.685 --> 00:02:16.855
The first step is to read the words.txt
files in HTFS into a spark RDD.

28
00:02:16.855 --> 00:02:18.572
We'll call the RDD, lines.

29
00:02:22.140 --> 00:02:27.128
We can read it using the spark context SC,
in calling the text file method.

30
00:02:32.609 --> 00:02:37.041
The argument is the URL of
the word set TXT file and HDFS.

31
00:02:47.270 --> 00:02:51.998
Let's run this We can

32
00:02:51.998 --> 00:02:56.920
view the contents of this RDD
by calling lines.take(5).

33
00:03:01.553 --> 00:03:05.010
The argument 5 says how many
lines to show of the RDD.

34
00:03:06.930 --> 00:03:11.112
Next, we'll transform this RDD
of lines into an RDD of words.

35
00:03:11.112 --> 00:03:15.755
We'll say, words = lines.flatmap,

36
00:03:18.940 --> 00:03:20.852
lambda line:

37
00:03:24.123 --> 00:03:28.568
line.split Double quote
space double quote.

38
00:03:31.374 --> 00:03:38.852
This creates a new RDD called, words,
by running flatMap over the line RDD.

39
00:03:38.852 --> 00:03:41.092
The argument is this lambda expression.

40
00:03:43.729 --> 00:03:47.310
A lambda in Python is a simple way
to declare a one line expression.

41
00:03:48.860 --> 00:03:51.890
In this case,
there's one argument called line and

42
00:03:51.890 --> 00:03:55.160
we called it split method on this line and
we split on spaces.

43
00:03:57.080 --> 00:03:59.720
We can run this and
look at the contents of words.

44
00:04:09.720 --> 00:04:12.500
We can see that each element
now is an individual word.

45
00:04:14.769 --> 00:04:17.136
Next, we'll create tuples of these words.

46
00:04:18.907 --> 00:04:23.138
We'll put them in a new RDD called tuples.

47
00:04:23.138 --> 00:04:26.405
Enter, tuples

48
00:04:26.405 --> 00:04:31.433
= words.map:lambda

49
00:04:31.433 --> 00:04:36.465
word; (word, 1).

50
00:04:41.144 --> 00:04:45.080
This creates the tuples
by transforming words.

51
00:04:45.080 --> 00:04:47.370
This uses map and another lambda function.

52
00:04:47.370 --> 00:04:53.400
In this case, the lambda takes
one argument and returns a tuple.

53
00:04:53.400 --> 00:04:55.350
Where the first value of
the tuple is the word.

54
00:04:56.500 --> 00:04:58.320
The second value, is the number 1.

55
00:04:58.320 --> 00:05:04.460
Not that in this case, we use map,
whereas before, we used flat map.

56
00:05:06.490 --> 00:05:09.930
In this case, we want a tuple for
every word in the words.

57
00:05:09.930 --> 00:05:13.690
So we have a one to one mapping
between inputs and outputs.

58
00:05:13.690 --> 00:05:19.064
Previously, while we were splitting lines
into word, each line had multiple words.

59
00:05:21.677 --> 00:05:22.348
In general,

60
00:05:22.348 --> 00:05:26.950
you want to use map when you have a one to
one mapping between inputs and outputs.

61
00:05:26.950 --> 00:05:31.804
In flatMap you have a one to many or
none mapping between inputs and

62
00:05:31.804 --> 00:05:37.607
outputs Let's run this and look at tuples.

63
00:05:45.043 --> 00:05:50.538
We can see that each word now has
a tuple initialized with the count of 1.

64
00:05:50.538 --> 00:05:55.080
We can now count all the words by
combining or reducing these tuples.

65
00:05:55.080 --> 00:05:57.190
We'll put this in a new RDD called counts.

66
00:05:58.660 --> 00:06:03.039
So we'll say counts equals
tuples.reduce by key.

67
00:06:07.602 --> 00:06:11.955
Lambda a,b:.

68
00:06:11.955 --> 00:06:13.125
a + b.

69
00:06:16.313 --> 00:06:20.038
In this case, the lambda function
takes two arguments, a and be, and

70
00:06:20.038 --> 00:06:22.199
will return the result of adding a and b.

71
00:06:24.876 --> 00:06:26.071
To view the result,

72
00:06:33.657 --> 00:06:37.815
You can see now that the counts for
each words have been created.

73
00:06:37.815 --> 00:06:40.839
We can write this result back to HDFS.

74
00:06:40.839 --> 00:06:50.239
Let's say
counts.coalesce(1).saveAsTextFile,

75
00:06:50.239 --> 00:06:53.120
and then the URL.

76
00:07:04.966 --> 00:07:09.675
The coalesce means we only
want a single output file.

77
00:07:09.675 --> 00:07:12.172
Let's go back to our shell and
view the results.

78
00:07:15.582 --> 00:07:18.930
We'll run hadoop fs -ls
to see the directory.

79
00:07:21.809 --> 00:07:24.553
And run it again to look inside
the wordcount directory.

80
00:07:29.294 --> 00:07:32.602
And once more,
to look inside wordcount/outputDir.

81
00:07:38.808 --> 00:07:45.420
As you recall,
the output from hadoop jobs is part-0000.

82
00:07:45.420 --> 00:07:48.837
This is also true for spark jobs.

83
00:07:48.837 --> 00:07:51.484
Let's copy this file to
the local file system.

84
00:07:51.484 --> 00:07:55.899
We'll run hadoop fs CopyToLocal

85
00:07:55.899 --> 00:08:02.212
wordcount/outputDir/part-00000.

86
00:08:09.530 --> 00:08:11.066
You can view the results with more.
WEBVTT

1
00:00:01.550 --> 00:00:02.530
In this hands on activity,

2
00:00:02.530 --> 00:00:05.190
we will be using Spark Streaming
to read weather data.

3
00:00:06.900 --> 00:00:09.470
First, we open
the Spark Streaming Jupyter Notebook.

4
00:00:10.610 --> 00:00:13.680
Next, we will look at sensor format and
measurement types.

5
00:00:14.695 --> 00:00:19.110
We'll then create a Spark DStream of
weather data, read the measurements, and

6
00:00:19.110 --> 00:00:20.630
create a sliding window of the data.

7
00:00:20.630 --> 00:00:26.180
We will define a function to display the
maximum and minimum values in the window.

8
00:00:26.180 --> 00:00:28.770
We start to stream processing
to give their results.

9
00:00:31.350 --> 00:00:35.610
Before we begin this activity, we need
to change the virtual box settings for

10
00:00:35.610 --> 00:00:37.100
our carder virtual machine.

11
00:00:39.110 --> 00:00:42.330
Start streaming needs more
than one thread of execution.

12
00:00:42.330 --> 00:00:45.620
So we need to change the settings to
add more than one virtual processor.

13
00:00:47.180 --> 00:00:52.930
First, shut down your cloudera virtual
machine and go to the virtual box manager.

14
00:00:54.960 --> 00:00:58.300
Select the cloudera virtual box and
click on settings.

15
00:01:00.720 --> 00:01:05.040
Next, click on system, click on Processor.

16
00:01:07.520 --> 00:01:14.444
And change the number of
CPU's to be two or more.

17
00:01:14.444 --> 00:01:19.910
When you're done, click okay,
and start the machine as usual.

18
00:01:23.740 --> 00:01:24.760
Let's begin.

19
00:01:24.760 --> 00:01:27.280
First, click on the browser icon
at the top of the tool bar.

20
00:01:29.370 --> 00:01:35.560
Navigate to the Jupyter Notebook server,
monitoring local host calling 8889.

21
00:01:35.560 --> 00:01:39.778
We'll then go in to downloads.

22
00:01:39.778 --> 00:01:43.337
Big data 3.

23
00:01:43.337 --> 00:01:46.186
Spark-streaming.

24
00:01:46.186 --> 00:01:48.616
Let's then open Spark-Streaming notebook.

25
00:01:51.346 --> 00:01:55.070
This first line, shows the example
data we get from the weather station.

26
00:01:56.680 --> 00:01:59.680
Each line has a time stamp and
a set of measurements.

27
00:02:01.860 --> 00:02:05.200
Each of these abbreviations is
a particular type of measurement,

28
00:02:05.200 --> 00:02:06.680
followed by the actual value.

29
00:02:09.140 --> 00:02:11.328
The next cell shows the key for
these measurements.

30
00:02:11.328 --> 00:02:16.190
For this hands-on, we are interested
in the average wind direction.

31
00:02:16.190 --> 00:02:18.059
Which is abbreviated as DM.

32
00:02:20.470 --> 00:02:24.178
This next cell, defines a function
that parses each line of text and

33
00:02:24.178 --> 00:02:26.080
pulls out the average wind speed.

34
00:02:27.610 --> 00:02:29.860
We define it here, so
we don't have to type it in later.

35
00:02:31.360 --> 00:02:32.627
Let's run this cell.

36
00:02:35.308 --> 00:02:38.790
Next, let's create
a streaming spark context.

37
00:02:38.790 --> 00:02:40.870
First, we'll need to import the module.

38
00:02:40.870 --> 00:02:46.590
We'll enter from pyspark.streaming
import StreamingContext.

39
00:02:46.590 --> 00:02:51.130
We can create a new streaming context.

40
00:02:51.130 --> 00:02:52.920
We'll put in in a variable called ssc.

41
00:02:54.400 --> 00:02:59.218
We'll enter ssc = StreamingContext(sc,1).

42
00:02:59.218 --> 00:03:02.710
The SC is a StreamingContext.

43
00:03:02.710 --> 00:03:07.070
The 1 specifies the batch interval,
1 second in this case.

44
00:03:07.070 --> 00:03:07.760
Let's run this.

45
00:03:10.040 --> 00:03:11.580
Next, we'll create a dstream.

46
00:03:13.100 --> 00:03:17.100
We'll import the streaming weather data,
over a TCP connection.

47
00:03:17.100 --> 00:03:19.099
We'll put this in a dstream called, Lines.

48
00:03:21.940 --> 00:03:26.249
Let's say lines = ssc.socketTextStream,

49
00:03:26.249 --> 00:03:31.661
we'll enter the host name in
port of the weather station,

50
00:03:31.661 --> 00:03:35.757
rtd.hpwren.ucsd.edu for 12028.

51
00:03:35.757 --> 00:03:36.784
Let's run this.

52
00:03:40.924 --> 00:03:46.260
Next, we'll create a new d-stream called
vals that would hold the measurements.

53
00:03:46.260 --> 00:03:52.390
We'll say vals = lines.flatMap parse.

54
00:03:52.390 --> 00:03:55.020
This calls the parse function,
we defined above for

55
00:03:55.020 --> 00:03:57.228
each of the lines coming
from the weather station.

56
00:03:57.228 --> 00:04:01.620
The resulting D-Stream will have just
the average wind direction values.

57
00:04:02.850 --> 00:04:03.570
We'll run this.

58
00:04:07.150 --> 00:04:11.340
Next, we'll create a window that
will aggregate the D-Stream values.

59
00:04:13.250 --> 00:04:17.470
We'll say, window = vals.window(10,5).

60
00:04:17.470 --> 00:04:22.850
The first argument specifies that the
length of the window should be 10 seconds.

61
00:04:22.850 --> 00:04:27.515
The second argument specifies that
the window should move every 5 seconds.

62
00:04:27.515 --> 00:04:29.900
Let's run this.

63
00:04:31.740 --> 00:04:34.480
Next, we'll define a function
that prints the minimum and

64
00:04:34.480 --> 00:04:36.240
maximum values that we see.

65
00:04:36.240 --> 00:04:37.710
We'll start by entering the definition.

66
00:04:39.440 --> 00:04:44.230
Def stats,
this will take an rdd as an argument.

67
00:04:47.330 --> 00:04:50.478
Next, let's print the entire
contents of the rdd.

68
00:04:50.478 --> 00:04:54.736
Print, parenthesis rdd.collect,

69
00:04:54.736 --> 00:04:59.932
this'll print the entire
content of the rdd.

70
00:04:59.932 --> 00:05:01.484
In a real big data application,

71
00:05:01.484 --> 00:05:04.020
this will be impractical due
to the size of the data.

72
00:05:05.020 --> 00:05:07.940
However, for this hands on,
the rdd is small, and so

73
00:05:07.940 --> 00:05:10.330
we can use this to see
the contents of the rdd.

74
00:05:12.720 --> 00:05:14.670
Next, we'll print the min and max.

75
00:05:15.940 --> 00:05:17.279
Before we do that however,

76
00:05:17.279 --> 00:05:21.303
we should check to make sure that
the size of the rdd is greater than zero.

77
00:05:21.303 --> 00:05:24.767
We'll check that rdd.count
is greater than 0.

78
00:05:29.285 --> 00:05:34.647
Finally, we'll print the MinID, MAX.

79
00:05:34.647 --> 00:05:39.826
We'll enter print (“max = {} min =

80
00:05:39.826 --> 00:05:44.853
{}”) Outside of the quote we'll do

81
00:05:44.853 --> 00:05:50.653
.format(rdd.max,rdd.min())).

82
00:05:50.653 --> 00:05:54.286
Let's run this, next,

83
00:05:54.286 --> 00:05:59.860
let's call this function stats.

84
00:05:59.860 --> 00:06:01.944
So all the rdds in our sliding window.

85
00:06:01.944 --> 00:06:08.366
I'll enter window.foreachRDD(stats).

86
00:06:08.366 --> 00:06:13.539
Run this.

87
00:06:13.539 --> 00:06:16.110
We're now ready to start
our streaming processing.

88
00:06:16.110 --> 00:06:19.880
We can do this by entering ssc.start.

89
00:06:19.880 --> 00:06:21.567
We'll run this to start the streaming.

90
00:06:29.214 --> 00:06:32.975
When we want to stop this streaming,
we'll run ssc.stop

91
00:06:38.028 --> 00:06:40.200
Please scroll up and
look at the beginning of the output.

92
00:06:43.130 --> 00:06:47.770
We'll see that it's printing the full
window and the min and max values.

93
00:06:50.010 --> 00:06:52.610
Notice that in the beginning,
the window is not yet filled.

94
00:06:52.610 --> 00:06:54.610
In this case, there's only three entries.

95
00:06:55.760 --> 00:06:59.029
We count to see that the window
is moving by five measurements.

96
00:07:00.220 --> 00:07:03.510
For example, the last five
measurements in the second window,

97
00:07:04.540 --> 00:07:06.870
are the first five measurements
in the third window.
WEBVTT

1
00:00:00.770 --> 00:00:04.034
In this hands on activity we
will be using SparkSQL to

2
00:00:04.034 --> 00:00:06.119
query data from an SQL database.

3
00:00:07.604 --> 00:00:11.825
First we will open
the SparkSQL Jupyter Notebook.

4
00:00:11.825 --> 00:00:14.080
We will connect Spark to a Postgres table.

5
00:00:15.740 --> 00:00:20.214
And then view the Spark DataFrame
schema and count the rows.

6
00:00:20.214 --> 00:00:22.040
We will view the contents
of the data frame.

7
00:00:23.220 --> 00:00:25.110
See how to filter rows and columns.

8
00:00:26.130 --> 00:00:29.174
And finally perform aggregate
operation on a column.

9
00:00:33.636 --> 00:00:34.714
Let's begin.

10
00:00:34.714 --> 00:00:38.247
First, click on the browser icon,
the top of the toolbar.

11
00:00:38.247 --> 00:00:41.062
>> [SOUND]
>> Next,

12
00:00:41.062 --> 00:00:45.288
navigate to the Jupyter Notebook server.

13
00:00:45.288 --> 00:00:48.275
It's localhost:8889.

14
00:00:51.117 --> 00:00:54.358
Go to Downloads,

15
00:00:54.358 --> 00:00:58.900
Big Data 3, Spark SQL.

16
00:01:00.460 --> 00:01:02.040
To open the SparkSQL Notebook.

17
00:01:02.040 --> 00:01:07.710
The first three cells have
already been entered for you.

18
00:01:09.760 --> 00:01:16.915
First, we import the SQLContext, run this.

19
00:01:16.915 --> 00:01:24.878
Next, we create an SQLContext
from the SparkContext run this.

20
00:01:24.878 --> 00:01:29.676
And next, we'll create a Spark DataFrame
from a Postgres table.

21
00:01:32.980 --> 00:01:34.950
We used the read attribute format.

22
00:01:36.820 --> 00:01:40.020
The jdbc argument means that we're
using a Java database connection.

23
00:01:42.180 --> 00:01:44.000
The next line sets the URL option.

24
00:01:44.000 --> 00:01:49.081
It says we're using Postgres
database running on the local host.

25
00:01:49.081 --> 00:01:54.287
The database name is Cloudera and
the username is Cloudera.

26
00:01:54.287 --> 00:01:56.048
The second option, DB table,

27
00:01:56.048 --> 00:02:00.000
says we want our data frame
to be the game clicks table.

28
00:02:00.000 --> 00:02:01.130
And finally we call load.

29
00:02:03.290 --> 00:02:04.400
Let's execute this.

30
00:02:06.370 --> 00:02:11.153
You can see the schema of the data
frame by calling df.printschema.

31
00:02:14.602 --> 00:02:17.593
This shows the name of each
column along with the data type.

32
00:02:20.499 --> 00:02:24.253
We can count the rows in this
df frame by calling df.count.

33
00:02:31.512 --> 00:02:35.786
We can look at the first five
rows by calling df.show(5)

34
00:02:39.493 --> 00:02:41.362
This shows all the columns
in the data frame.

35
00:02:43.240 --> 00:02:46.260
We can select specific columns
by using the select method.

36
00:02:48.080 --> 00:02:52.275
Let's select just the User ID and
Team Level columns.

37
00:02:52.275 --> 00:02:57.860
I'll enter df.select("userid",teamlevel.

38
00:02:57.860 --> 00:02:58.641
Parenthesis.

39
00:03:01.524 --> 00:03:04.610
And finally we only want to
see the top five rows.

40
00:03:04.610 --> 00:03:06.312
So we'll do .show(5).

41
00:03:10.870 --> 00:03:15.368
We can also select rows
that have a specific value.

42
00:03:15.368 --> 00:03:19.067
Let's look for the rows where
the team level is greater than one.

43
00:03:19.067 --> 00:03:23.092
We'll enter df.filter.

44
00:03:23.092 --> 00:03:27.393
We'll specify that we want team level
greater than one by entering df,

45
00:03:27.393 --> 00:03:30.412
square bracket, team level,
greater than one.

46
00:03:34.336 --> 00:03:38.183
And again, we only want the user ID and
team level columns.

47
00:03:43.078 --> 00:03:45.368
And finally only the first five rows.

48
00:03:50.624 --> 00:03:55.366
We can use the group by method to
aggregate a particular column.

49
00:03:55.366 --> 00:03:59.542
For example, the ishit column
has a value of zero or one.

50
00:03:59.542 --> 00:04:04.611
And we can use group by to count how
many times each of these values occurs.

51
00:04:04.611 --> 00:04:12.068
Or a df.groupby ishit, and
we'll call count to count the values

52
00:04:21.520 --> 00:04:25.552
We can also perform aggregate statistical
operations on the data in a data frame.

53
00:04:27.000 --> 00:04:31.140
Let's compute the mean and
sum values for ishit.

54
00:04:31.140 --> 00:04:36.756
First we need to import
the statistical functions we'll

55
00:04:36.756 --> 00:04:41.910
run from.pyspark.sql.functions
import star.

56
00:04:41.910 --> 00:04:47.891
Next we'll run df.select
(mean) ishit,sum ishit

57
00:04:55.375 --> 00:04:58.580
We can also join two data
frames on a particular column.

58
00:04:59.980 --> 00:05:04.740
Let's join the existing data frame of the
game clicks table with the adclicks table.

59
00:05:05.850 --> 00:05:08.410
First, we need to create data frame for
the adclicks table.

60
00:05:09.810 --> 00:05:10.796
Let's go back up.

61
00:05:15.286 --> 00:05:16.991
Copy the content of this cell,

62
00:05:23.747 --> 00:05:26.795
Paste it.

63
00:05:26.795 --> 00:05:31.356
We put the adclicks table
the data frame called df2.

64
00:05:31.356 --> 00:05:34.462
And we'll change the db table
option to the adclicks.

65
00:05:38.376 --> 00:05:39.388
Run it.

66
00:05:41.687 --> 00:05:44.117
Let's print the schema of df2.

67
00:05:52.283 --> 00:05:56.064
You can see that it also has
a column called user id.

68
00:05:56.064 --> 00:06:00.024
So let's join the game clicks
data frame with the add

69
00:06:00.024 --> 00:06:02.643
clicks data frame on this column.

70
00:06:02.643 --> 00:06:06.229
We put the result in a new
data frame called merged.

71
00:06:06.229 --> 00:06:15.490
We'll say merge = df.join.df2 "userid".

72
00:06:15.490 --> 00:06:15.990
We'll run it.

73
00:06:17.820 --> 00:06:19.163
Let's look at the schema.

74
00:06:19.163 --> 00:06:21.193
We'll call merge.printschema.

75
00:06:25.763 --> 00:06:29.499
We can see that this merged data frame
has the column for both game clicks and

76
00:06:29.499 --> 00:06:30.105
adclicks.

77
00:06:31.780 --> 00:06:35.537
Finally we'll look at the top five
rows in this merged data frame.

78
00:06:35.537 --> 00:06:38.208
We'll run merge.show.
WEBVTT

1
00:00:02.020 --> 00:00:05.610
We have now seen some
simple transformations and

2
00:00:05.610 --> 00:00:10.900
how Spark can create RDDs from
each other using transformations.

3
00:00:10.900 --> 00:00:15.390
We learned that transformations are
evaluated after an action is performed.

4
00:00:16.810 --> 00:00:21.800
So we can simply define actions as RDD
operations that trigger the evaluation of

5
00:00:21.800 --> 00:00:27.530
the transformation pipeline and return
the final result to the driver program or

6
00:00:27.530 --> 00:00:29.810
save the results to a persistent storage.

7
00:00:31.660 --> 00:00:36.580
We can also call them the last
step in a Spark pipeline.

8
00:00:36.580 --> 00:00:38.700
Let's now look at a few action operations.

9
00:00:40.870 --> 00:00:46.150
After this video, you will be able to
explain the steps of a Spark pipeline

10
00:00:46.150 --> 00:00:52.010
ending with a collect action and list
four common action operations in Spark.

11
00:00:54.750 --> 00:00:57.740
A very common action in Spark is collect.

12
00:00:59.090 --> 00:01:03.875
In this example, we can imagine that
initially we are reading from HDFS.

13
00:01:05.390 --> 00:01:10.020
The RDD partitions that go through
the transformation steps in our big data

14
00:01:10.020 --> 00:01:14.970
pipeline are defined as flatMap and
groupbyKey.

15
00:01:16.290 --> 00:01:21.625
When the final step is done,
the collect action is called and

16
00:01:21.625 --> 00:01:25.840
Spark sends all the tasks for
execution to the worker notes.

17
00:01:28.372 --> 00:01:32.940
Collect will send all the resulting
RDDs from the workers and

18
00:01:32.940 --> 00:01:37.140
copy them to the Java virtual
machine on the driver program.

19
00:01:37.140 --> 00:01:41.600
And then, this will be piped
also to our Python shell.

20
00:01:43.150 --> 00:01:48.076
While collect copies all the data,
another action, take,

21
00:01:48.076 --> 00:01:51.720
copies the first n results of the driver.

22
00:01:53.690 --> 00:01:57.440
If the results are too large
to fit in the driver memory,

23
00:01:57.440 --> 00:02:01.290
then there's an opportunity to write
them directly to HDFS instead.

24
00:02:03.230 --> 00:02:07.850
Among many other actions,
reduce is probably the most famous one.

25
00:02:08.980 --> 00:02:13.300
Reduce takes two elements and
returns a result, like sum.

26
00:02:13.300 --> 00:02:19.680
But in this case, we don't have a key,
we just have a large area of some values.

27
00:02:19.680 --> 00:02:22.150
And we are running this function over and

28
00:02:22.150 --> 00:02:26.580
over again to reduce everything
to one single value.

29
00:02:26.580 --> 00:02:29.210
For example,
to the global sum of everything.

30
00:02:30.905 --> 00:02:34.945
Another very useful action Is saveAsText,

31
00:02:34.945 --> 00:02:38.585
to save the results to local disk or
HDFS, and

32
00:02:38.585 --> 00:02:44.485
this is very useful if the output of
the power computation is pretty large.
WEBVTT

1
00:00:02.200 --> 00:00:06.550
Hello, I hope you enjoyed your first
programming experience with Spark.

2
00:00:07.580 --> 00:00:10.890
Although the words count
example is simple,

3
00:00:10.890 --> 00:00:14.420
it is useful in starting to
understand how to work with RDDs.

4
00:00:16.040 --> 00:00:22.820
After this video, you'll be able to use
two methods to create RDDs in Spark,

5
00:00:22.820 --> 00:00:28.500
explain what immutable means,
interpret a Spark program as a pipeline

6
00:00:28.500 --> 00:00:33.860
of transformations and actions, and
list the steps to create a Spark program.

7
00:00:36.390 --> 00:00:37.970
So let's remember where we are.

8
00:00:39.540 --> 00:00:43.620
We have a Driver Program that
defines the Spark context.

9
00:00:44.700 --> 00:00:48.419
This is the entry point
to your application.

10
00:00:48.419 --> 00:00:53.980
The driver converts all the data to RDDs,
and

11
00:00:53.980 --> 00:00:59.720
everything from this point on
gets managed using the RDDs.

12
00:00:59.720 --> 00:01:03.320
RDDs can be constructed from files or
any other storage.

13
00:01:04.500 --> 00:01:08.650
They can also be constructed from
data structures for collections and

14
00:01:08.650 --> 00:01:10.150
programs, like lists.

15
00:01:11.940 --> 00:01:18.306
All the transformations and actions on
these RDDs take place either locally,

16
00:01:18.306 --> 00:01:22.560
or on the Worker Nodes
managed by a Cluster Manager.

17
00:01:25.190 --> 00:01:29.560
Each transformation results in
a new updated version of the RDD.

18
00:01:29.560 --> 00:01:33.480
The RDDs at the end get converted and

19
00:01:33.480 --> 00:01:37.319
saved in a persistent storage like HDFS or
your local drive.

20
00:01:40.000 --> 00:01:46.480
As we mentioned before,
RDDs get created in the Driver Program.

21
00:01:46.480 --> 00:01:48.680
The developer of the Driver Program,

22
00:01:48.680 --> 00:01:52.820
who in this case is you,
is responsible for creating them.

23
00:01:55.030 --> 00:01:58.650
You can just read in a file
through your Spark Context, or

24
00:01:58.650 --> 00:02:04.300
as we have in this example,
you can provide an existing collection,

25
00:02:04.300 --> 00:02:07.820
like a list to be turned into
a distributed collection.

26
00:02:10.400 --> 00:02:15.130
You can also create an integer
RDD using parallelize,

27
00:02:16.340 --> 00:02:18.690
and provide a number of partitions for

28
00:02:18.690 --> 00:02:23.000
distribution as we do create
the numbers RDD in this line.

29
00:02:25.850 --> 00:02:29.440
Here, the range function in Python

30
00:02:30.460 --> 00:02:33.738
will give us a list of
numbers starting from 0 to 9.

31
00:02:33.738 --> 00:02:40.340
The parallelize function
will create three partitions

32
00:02:40.340 --> 00:02:45.160
of the RDD to be distributed, based on
the parameter that was provided to it.

33
00:02:46.440 --> 00:02:51.040
Spark will decide how to assign partitions
to our executors and worker nodes.

34
00:02:53.110 --> 00:02:58.037
The distributed RDDs can in the end be
gathered into a single partition on

35
00:02:58.037 --> 00:03:01.286
the driver using
the collect transformation.

36
00:03:07.478 --> 00:03:12.225
Now let's think of a scenario were we
start processing the created RDDs.

37
00:03:14.060 --> 00:03:19.252
There are two types of operations
that help with processing in Spark,

38
00:03:19.252 --> 00:03:22.158
namely Transformations and Actions.

39
00:03:24.829 --> 00:03:29.734
All partitions written in RDD,
go through the same transformation in

40
00:03:29.734 --> 00:03:34.820
the worker node, executors when
a transformation is applied to an RDD.

41
00:03:36.420 --> 00:03:40.110
Spark uses lazy evaluation for
transformations.

42
00:03:41.470 --> 00:03:45.310
That means they will not be
immediately executed, but

43
00:03:45.310 --> 00:03:47.610
instead wait for
an action to be performed.

44
00:03:49.440 --> 00:03:53.890
The transformations get computed
when an action is executed.

45
00:03:53.890 --> 00:03:56.810
For this reason,
a lot of times you will see run

46
00:03:56.810 --> 00:04:01.300
time errors showing up at the action stage
and not at the transformation stages.

47
00:04:02.340 --> 00:04:04.840
It is very similar to Haskell or Erlang,

48
00:04:04.840 --> 00:04:07.120
if any of you are familiar
with these languages.

49
00:04:09.730 --> 00:04:12.870
Let's put some names on
these transformations.

50
00:04:12.870 --> 00:04:18.530
We can have a pipeline by converting a
text file into an RDD with two partitions.

51
00:04:19.840 --> 00:04:25.440
Filter some values out of it, and
maybe apply a map function to it.

52
00:04:25.440 --> 00:04:30.350
In the end, the run,
the collect action on the mapped RDDs

53
00:04:30.350 --> 00:04:34.620
to evaluate the results of the pipeline
and convert the outputs into results.

54
00:04:35.680 --> 00:04:41.440
Here, filter and map are transformations,
and collect is the action.

55
00:04:43.340 --> 00:04:47.990
Although the RDDs are in memory,
and they are not persistent,

56
00:04:47.990 --> 00:04:51.240
we can use the cash function
to make them persistent cash.

57
00:04:53.090 --> 00:04:58.684
For example, in order to reuse the RDD
created from a database query that could

58
00:04:58.684 --> 00:05:03.792
otherwise be costly to re-execute,
we can instead cache these RDDs.

59
00:05:06.461 --> 00:05:09.883
We need to use caution when
using the cache option,

60
00:05:09.883 --> 00:05:14.432
as it can consume too much memory and
generate a bottleneck itself.

61
00:05:17.713 --> 00:05:25.070
As a part of the Word Count example, we
mapped the words RDD to generate tuples.

62
00:05:25.070 --> 00:05:29.350
We then applied reduceByKey
to tuples to generate counts.

63
00:05:30.470 --> 00:05:34.465
In the end, we convert the number
of partitions to one so

64
00:05:34.465 --> 00:05:38.120
that output is one file
when written to this later.

65
00:05:38.120 --> 00:05:41.980
Otherwise, output will be spread
over multiple files on disk.

66
00:05:43.572 --> 00:05:49.410
Finally, saveAsTextFile is an action
that kickstarts the computation and

67
00:05:49.410 --> 00:05:50.120
writes to disk.

68
00:05:52.040 --> 00:05:55.700
To summarize, in a typical Spark program

69
00:05:55.700 --> 00:06:00.400
we create RDDs from external storage or
local collections like lists.

70
00:06:01.610 --> 00:06:05.130
Then we apply transformations
to these RDDs,

71
00:06:05.130 --> 00:06:09.820
like filter, map, and reduceByKey.

72
00:06:09.820 --> 00:06:14.800
These transformations get lazily
evaluated until an action is performed.

73
00:06:15.970 --> 00:06:22.330
Actions are performed both for local and
parallel computation to generate results.

74
00:06:22.330 --> 00:06:26.780
Next, we will talk more about
transformation and actions in Spark.
WEBVTT

1
00:00:01.210 --> 00:00:05.330
In the last video,
we talked about the programming model for

2
00:00:05.330 --> 00:00:10.930
Spark where RDD's get generated from
external datasets and gets partitioned.

3
00:00:12.450 --> 00:00:18.710
We said, RDDs are immutable meaning they
can't be changed in place even partially.

4
00:00:19.740 --> 00:00:23.690
They need a transformation
operation applied to them and

5
00:00:23.690 --> 00:00:25.650
get converted into a new RDD.

6
00:00:27.650 --> 00:00:32.652
This is essential for keeping track
of all the processing that has been

7
00:00:32.652 --> 00:00:38.094
applied to our dataset providing the
ability to keep a linear chain of RDDs.

8
00:00:38.094 --> 00:00:44.360
In addition, as a part of a big data
pipeline, we start with an RDD.

9
00:00:44.360 --> 00:00:48.842
And through several transformation steps,
many other RDDs as

10
00:00:48.842 --> 00:00:53.840
intermediate products get executed
until we get to our final result.

11
00:00:55.070 --> 00:00:58.910
We also mention that
an important feature of Spark

12
00:00:58.910 --> 00:01:02.060
is that all these transformation are lazy.

13
00:01:03.820 --> 00:01:07.820
This means they don't execute
immediately when applied to an RDD.

14
00:01:09.040 --> 00:01:13.670
So when we apply a transformation,
nothing happens right away.

15
00:01:13.670 --> 00:01:17.850
We are basically preparing our big
data pipeline to be executed later.

16
00:01:18.990 --> 00:01:23.936
When we are done defining all
the transformations and perform an action,

17
00:01:23.936 --> 00:01:28.961
Spark will take care of finding the best
way to execute this computation and

18
00:01:28.961 --> 00:01:32.729
then start all the necessary
tasks in our worker nodes.

19
00:01:32.729 --> 00:01:37.108
In this video, we will explain some
common transformation in Spark.

20
00:01:38.985 --> 00:01:43.537
After this video, you will be able
to explain the difference between

21
00:01:43.537 --> 00:01:47.001
a narrow transformation and
wide transformation.

22
00:01:47.001 --> 00:01:51.748
Describe map, flatmap,
filter and coalesce as narrow

23
00:01:51.748 --> 00:01:56.404
transformations and
list two wide transformations.

24
00:01:59.240 --> 00:02:04.330
Let's take at look at, probably the
simplest transformation, which is a map.

25
00:02:06.280 --> 00:02:10.160
By now,
you're well versed in home networks.

26
00:02:10.160 --> 00:02:16.240
It applies the function to each
partition or element of an RDD.

27
00:02:16.240 --> 00:02:19.690
This is a one to one transformation.

28
00:02:19.690 --> 00:02:24.599
It is also in the category of element-wise
transformations since it transforms

29
00:02:24.599 --> 00:02:26.847
every element of an RDD separately.

30
00:02:30.805 --> 00:02:34.945
The code example in the blue
box here applies a function

31
00:02:34.945 --> 00:02:38.645
called lower to all
the elements in a text_RDD.

32
00:02:39.900 --> 00:02:40.960
The lower function

33
00:02:42.180 --> 00:02:45.810
turns all the characters in
a line to lower case letters.

34
00:02:46.880 --> 00:02:51.650
So the input is one line
of text with any kind of

35
00:02:51.650 --> 00:02:56.360
capitalization and the outfit is going
to be the same line, all lower case.

36
00:02:58.130 --> 00:03:03.160
In this example, we have two worker
nodes drawn as orange boxes.

37
00:03:04.710 --> 00:03:08.328
The black boxes are partitions
of our dataset.

38
00:03:08.328 --> 00:03:11.842
We work by partition and not by element.

39
00:03:11.842 --> 00:03:17.036
As you would remember this, it is the
difference between Spark and MapReduce.

40
00:03:19.488 --> 00:03:24.418
The partition is just a chunk of our data
with some number of elements in it and

41
00:03:24.418 --> 00:03:28.964
the map function gets applied to all
elements in that partition in each

42
00:03:28.964 --> 00:03:30.440
worker node locally.

43
00:03:31.850 --> 00:03:35.748
Each node applies the map
function to the data or

44
00:03:35.748 --> 00:03:39.555
RDD partition they received independently.

45
00:03:39.555 --> 00:03:43.560
Let's look at a few more in
element-wise transformation category.

46
00:03:46.251 --> 00:03:49.190
FlatMap is very similar to map.

47
00:03:50.290 --> 00:03:55.573
However, instead of returning
an individual element for each map,

48
00:03:55.573 --> 00:04:01.323
it returns an RDD with an aggregate of
all the results for all the elements.

49
00:04:01.323 --> 00:04:03.888
In the example in the blue box,

50
00:04:03.888 --> 00:04:08.132
the split_words fuction
takes a line as an input,

51
00:04:08.132 --> 00:04:13.970
which is one element and it's output
is each word as a single element.

52
00:04:13.970 --> 00:04:18.120
So, it splits a line to words.

53
00:04:19.760 --> 00:04:21.570
The same thing gets done for each line.

54
00:04:23.180 --> 00:04:26.130
When the output for
all the lines is flattened,

55
00:04:26.130 --> 00:04:29.520
we get a simple
one-dimensional list of words.

56
00:04:31.220 --> 00:04:36.480
So, we'll get all the words in
all the lines in just one list.

57
00:04:38.070 --> 00:04:43.703
Depending on the line length the output
partitions might be of different sizes.

58
00:04:43.703 --> 00:04:47.463
Detected here by the height of
each black box being different.

59
00:04:49.977 --> 00:04:54.750
In Spark terms, map and
flatMap are narrow transformations.

60
00:04:56.040 --> 00:05:01.390
Narrow transformation refers to
the processing where the processing

61
00:05:01.390 --> 00:05:06.827
logic depends only on data that is
already residing in the partition and

62
00:05:06.827 --> 00:05:09.466
data shuffling is not necessary.

63
00:05:12.080 --> 00:05:16.030
Another very important
transformation is filter.

64
00:05:16.030 --> 00:05:21.957
Often, we're interested just
in a subset of our data or

65
00:05:21.957 --> 00:05:25.118
we want to get rid of bad data.

66
00:05:25.118 --> 00:05:30.105
Filter transformation takes the function
take executes on each element

67
00:05:30.105 --> 00:05:31.575
of a RDD partition and

68
00:05:31.575 --> 00:05:36.740
returns only the elements that
the transformation element returns true.

69
00:05:39.219 --> 00:05:44.281
The example code in the blue box here,
applies a filter

70
00:05:44.281 --> 00:05:50.014
function that filters out words
that start with the letter a.

71
00:05:50.014 --> 00:05:54.130
The function starts with a,
takes the input word,

72
00:05:54.130 --> 00:06:00.080
then transforms it to lowercase and
then checks if the word starts with a.

73
00:06:02.920 --> 00:06:08.640
So, the output of this operation will be
a list with only words that start with a.

74
00:06:10.870 --> 00:06:12.670
This is another narrow transformation.

75
00:06:14.270 --> 00:06:17.930
So, it only gets executed locally

76
00:06:17.930 --> 00:06:22.070
without the need to shuffle any RDD
partitions across the word kernels.

77
00:06:24.320 --> 00:06:30.486
The output of filter depends on
the input and the filter functions.

78
00:06:30.486 --> 00:06:31.663
In some cases,

79
00:06:31.663 --> 00:06:37.003
even if you started with even RDD
partitions within the worker nodes,

80
00:06:37.003 --> 00:06:43.247
the RDD size can significantly vary across
the workers after a filter operation,

81
00:06:43.247 --> 00:06:48.588
then this happens is a pretty good
idea to join some of those partitions

82
00:06:48.588 --> 00:06:53.772
to increase performance and
even out processing across clusters.

83
00:06:53.772 --> 00:07:00.399
This transformation is called coalesce.

84
00:07:00.399 --> 00:07:06.060
Coalesce simply helps with balancing
the data partition numbers and sizes.

85
00:07:07.220 --> 00:07:12.148
When you have significantly reduced
your initial data after some filters and

86
00:07:12.148 --> 00:07:13.817
other transformations,

87
00:07:13.817 --> 00:07:18.311
having a large number of partitions
might not be very useful anymore.

88
00:07:18.311 --> 00:07:19.063
In this case,

89
00:07:19.063 --> 00:07:23.521
you can use coalesce to reduce the number
of partitions to a more manageable number.

90
00:07:26.421 --> 00:07:31.941
Until now, we talked about narrow
transformations that happen in a worker

91
00:07:31.941 --> 00:07:36.947
node locally without having to
transfer data through the network.

92
00:07:36.947 --> 00:07:40.572
Now, let's start talking
about wide transformations.

93
00:07:43.048 --> 00:07:45.693
Let's remember our Word Count example.

94
00:07:45.693 --> 00:07:52.420
As a part of the Word Count example, we
map the words RDD could generate tuples.

95
00:07:52.420 --> 00:07:58.110
The output of map is a key value pair
list where the key is the word and

96
00:07:58.110 --> 00:07:59.540
the value is always one.

97
00:08:01.420 --> 00:08:07.390
We then apply it reduceByKey
to tuples to generate counts,

98
00:08:07.390 --> 00:08:10.360
which simply sums the values for
each key or word.

99
00:08:11.670 --> 00:08:18.555
Let's imagine for a second that we use
groupByKey instead of reduceByKey.

100
00:08:18.555 --> 00:08:23.453
We will come back to reduceByKey
in just a little bit.

101
00:08:23.453 --> 00:08:26.774
Remember, mapped outputs tuples,

102
00:08:26.774 --> 00:08:31.920
which is a list of key value
pairs in the forms of word one.

103
00:08:33.670 --> 00:08:39.146
At each worker node, we will have
tuples that have the same word as key.

104
00:08:39.146 --> 00:08:45.213
In this example, apple as the key and
1 as the count and 2 worker nodes.

105
00:08:47.720 --> 00:08:52.367
Trying to group together, all the counts
of a word across worker nodes

106
00:08:52.367 --> 00:08:55.690
requires shuffling of
data between these nodes.

107
00:08:56.800 --> 00:09:00.466
Just like we do for the word apple here.

108
00:09:00.466 --> 00:09:05.616
GroupByKey is the transformation
that helps us combine values with

109
00:09:05.616 --> 00:09:11.596
the same key into a list without applying
a special user define function to it.

110
00:09:11.596 --> 00:09:17.435
As you see on the right, the result
of a groupByKey transformation on all

111
00:09:17.435 --> 00:09:23.292
the map outputs by the word apple is
the key ends up in a list with all ones.

112
00:09:26.741 --> 00:09:32.109
If you instead apply the function to
list like summing up all the values,

113
00:09:32.109 --> 00:09:35.542
then we could have had
the word count results.

114
00:09:35.542 --> 00:09:36.715
In this case, 2.

115
00:09:36.715 --> 00:09:43.346
If we instead applied a function to
the list like summing up all the values,

116
00:09:43.346 --> 00:09:47.468
then we would have had
the word count results.

117
00:09:47.468 --> 00:09:51.655
If we need to apply such functions to
a group of values related to a key like

118
00:09:51.655 --> 00:09:54.140
this, we use the reduceByKey operation.

119
00:09:56.220 --> 00:10:01.670
ReduceByKey helps us to combine
the value using a reduce function,

120
00:10:01.670 --> 00:10:04.580
which in the word count
case is a simple summation.

121
00:10:06.420 --> 00:10:10.420
In groupByKey and
reduceByKey transformations,

122
00:10:11.650 --> 00:10:15.860
we observe the behavior that require
shuffling of the data across work nodes,

123
00:10:17.350 --> 00:10:20.830
we call such transformations
wide transformations.

124
00:10:22.340 --> 00:10:27.440
In wide transformation operations,
processing depends on data

125
00:10:27.440 --> 00:10:32.940
residing in multiple partitions
distributed across worker nodes and this

126
00:10:32.940 --> 00:10:37.430
requires data shuffling over the network
to bring related datasets together.

127
00:10:39.220 --> 00:10:44.630
As a summary, we have listed a small
number of transformations in Spark with

128
00:10:44.630 --> 00:10:49.240
some examples and distinguished between
them as narrow and wide transformations.

129
00:10:50.520 --> 00:10:55.743
Although this is a good start,
I advise you to go through the list

130
00:10:55.743 --> 00:11:01.760
provided at the link shown here after
you complete this beginner course.

131
00:11:01.760 --> 00:11:06.662
Read about the rest of the transformations
in Spark before you start programming in

132
00:11:06.662 --> 00:11:09.192
Spark and have fun with transformations.
WEBVTT

1
00:00:02.170 --> 00:00:05.580
Lastly, we will introduce
you to Spark GraphX.

2
00:00:07.970 --> 00:00:13.021
After this video,
you will be able to describe GraphX is,

3
00:00:13.021 --> 00:00:17.970
explain how vertices and
edges are stored in GraphX, and

4
00:00:17.970 --> 00:00:21.702
describe how Pregel works at a high level.

5
00:00:24.295 --> 00:00:29.875
GraphX is Apache Spark's Application
Programming Interface for

6
00:00:29.875 --> 00:00:33.570
graphs and graph-parallel computation.

7
00:00:33.570 --> 00:00:37.001
GraphX uses a property graph model.

8
00:00:37.001 --> 00:00:40.420
This means, both nodes.

9
00:00:40.420 --> 00:00:44.500
And edges in a graph can
have attributes and values.

10
00:00:46.860 --> 00:00:52.955
In GraphX, the node properties
are stored in a vertex table and

11
00:00:52.955 --> 00:00:57.330
edge properties are stored
in an edge table.

12
00:00:58.930 --> 00:01:04.429
The connectivity information, that is,
which edge connects which nodes,

13
00:01:04.429 --> 00:01:08.501
is stored separately from the node and
edge properties.

14
00:01:11.696 --> 00:01:16.400
GraphX is built on special RDDs for
vertices and edges.

15
00:01:17.890 --> 00:01:23.432
VertexRDD represents a set of vertices,

16
00:01:23.432 --> 00:01:28.823
all of which have an attribute called A.

17
00:01:28.823 --> 00:01:33.865
The EdgeRDD here extends
this basic edge storing by

18
00:01:33.865 --> 00:01:40.326
the edges in columnar format on
each partition for performance.

19
00:01:40.326 --> 00:01:46.740
Note that VertexID are defined
to be unique by design.

20
00:01:47.780 --> 00:01:52.257
The edge class is an object
with a source vertex and

21
00:01:52.257 --> 00:01:56.209
destination vertex and an edge attribute.

22
00:01:58.257 --> 00:02:04.511
In addition to the vortex and
edge views of the property graph,

23
00:02:04.511 --> 00:02:07.584
GraphX also has triplet view.

24
00:02:07.584 --> 00:02:14.350
The triplet view logically joins
vortex and edge properties.

25
00:02:17.003 --> 00:02:21.131
GraphX has an operator that
can execute operations

26
00:02:21.131 --> 00:02:24.880
from the Pregel library for
graph analytics.

27
00:02:26.420 --> 00:02:32.000
This Pregel operator executes
in a series of super steps

28
00:02:32.000 --> 00:02:35.700
which defines a messaging protocol for
vertices.

29
00:02:37.620 --> 00:02:39.824
We will revisit graph analytics and

30
00:02:39.824 --> 00:02:43.946
using GraphX in more detail in
course five of the specialization.

31
00:02:45.934 --> 00:02:51.070
In summary, Spark can be used for
graph parallel computations.

32
00:02:52.300 --> 00:02:58.740
GraphX uses special RDDs for
storing vertex and edge information.

33
00:03:00.170 --> 00:03:04.165
And the pregel operator works
in a series of super steps.
WEBVTT

1
00:00:01.580 --> 00:00:04.435
Now, we will introduce you to Spark MLlib.

2
00:00:06.080 --> 00:00:11.020
After this video, you will be
able to describe what MLlib is,

3
00:00:11.020 --> 00:00:15.865
list the main categories of
techniques available in MLlib,

4
00:00:15.865 --> 00:00:20.530
and explain code segments
containing MLlib algorithms.

5
00:00:20.530 --> 00:00:26.750
MLlib is a scalable machine learning
library that runs on top of Spark Core.

6
00:00:26.750 --> 00:00:31.370
It provides distributed implementations
of commonly used machine learning

7
00:00:31.370 --> 00:00:32.750
algorithms and utilities.

8
00:00:34.070 --> 00:00:40.260
As with Spark Core, MLlib has APIs for
Scala, Java, Python, and R.

9
00:00:42.520 --> 00:00:45.120
MLlib offers many algorithms and

10
00:00:45.120 --> 00:00:48.170
techniques commonly used in
a machine learning process.

11
00:00:49.170 --> 00:00:51.710
The main categories are machine learning,

12
00:00:51.710 --> 00:00:55.860
statistics and some common utility
tools for common techniques.

13
00:00:57.120 --> 00:00:58.970
As the name suggests,

14
00:00:58.970 --> 00:01:02.380
many machine learning algorithms
are available in MLlib.

15
00:01:03.760 --> 00:01:08.440
These are algorithms to build models for
classification, regression, and

16
00:01:08.440 --> 00:01:08.990
clustering.

17
00:01:10.140 --> 00:01:14.110
There are also techniques for
evaluating the resulting models.

18
00:01:14.110 --> 00:01:17.740
For example,
you can compute the values for

19
00:01:17.740 --> 00:01:22.560
a receiver of creating characteristic
that we call an ROC curve.

20
00:01:22.560 --> 00:01:25.350
A common statistical technique for

21
00:01:25.350 --> 00:01:28.550
plotting the performance
of a binary classifier.

22
00:01:30.180 --> 00:01:34.040
Statistical functions
are also provided in MLlib.

23
00:01:34.040 --> 00:01:38.655
Examples are summary statistics,
means, standard deviation, etc.

24
00:01:39.850 --> 00:01:43.440
Correlations and
methods to sample a dataset.

25
00:01:45.420 --> 00:01:50.360
MLlib also has techniques commonly
used in the machine learning process,

26
00:01:50.360 --> 00:01:53.040
such as dimensionality reduction and

27
00:01:53.040 --> 00:01:56.630
feature transformation methods for
preprocessing the data.

28
00:01:57.690 --> 00:02:01.550
In short, Spark MLlib offers

29
00:02:01.550 --> 00:02:05.220
many techniques often used in
a machine learning pipeline.

30
00:02:07.280 --> 00:02:13.410
Let's take a look at an example to
compute summary statistics using MLlib.

31
00:02:13.410 --> 00:02:17.920
Note that we will use the spark pipe
of API similar to the ones used for

32
00:02:17.920 --> 00:02:19.520
our other examples in this course.

33
00:02:20.980 --> 00:02:25.220
Here is the code segment to
compute summary statistics for

34
00:02:25.220 --> 00:02:29.180
a data set consisting
of columns of numbers.

35
00:02:29.180 --> 00:02:33.370
Lines of code are in white, and
the comments are in orange.

36
00:02:33.370 --> 00:02:38.759
The first line imports statistics
functions from the stat module.

37
00:02:38.759 --> 00:02:44.710
The second line creates an RDD
of Vectors with the data.

38
00:02:44.710 --> 00:02:48.500
You can think of each vector
as a column in a data matrix.

39
00:02:49.800 --> 00:02:53.670
The next line denoted with three invokes

40
00:02:53.670 --> 00:02:58.210
the column stats function to compute
summary statistics for each column.

41
00:02:59.240 --> 00:03:04.820
The last three lines show
by four print out the mean,

42
00:03:04.820 --> 00:03:08.480
variance and number of non-zero
entries for each column.

43
00:03:10.070 --> 00:03:14.640
As you can see from this example,
computing the summary statistics for

44
00:03:14.640 --> 00:03:17.700
a data set is very
straightforward using a MLlib.

45
00:03:20.410 --> 00:03:22.490
Here is another example.

46
00:03:22.490 --> 00:03:26.600
Although we will go through the ratio
learning details in our next course,

47
00:03:26.600 --> 00:03:30.260
here we give you a hint of how to
use two ratio learning techniques.

48
00:03:30.260 --> 00:03:32.920
One for Classification,
and one for Clustering.

49
00:03:34.270 --> 00:03:40.130
This code segment, shows the six steps to
build a DecisionTree for classification.

50
00:03:40.130 --> 00:03:43.138
The first line imports
the DecisionTree module,

51
00:03:43.138 --> 00:03:46.510
the second line imports MLUtils module,

52
00:03:48.190 --> 00:03:53.600
the next line fails the DecisionTree
to classify the data for two classes.

53
00:03:53.600 --> 00:03:58.570
Then, the model is printed out and
finally the model is saved in a file.

54
00:04:01.580 --> 00:04:05.490
Here is another MLlib example,
this time for clustering.

55
00:04:05.490 --> 00:04:10.480
This code segment shows the 5-step
code to build a k-means clustering.

56
00:04:12.420 --> 00:04:17.520
The first line imports the k-means module,
the second line

57
00:04:17.520 --> 00:04:22.040
imports an array module from numpy,

58
00:04:22.040 --> 00:04:27.048
the next two lines read in the data and
parses it using space as the limiter,

59
00:04:27.048 --> 00:04:31.100
then the k-means model

60
00:04:31.100 --> 00:04:36.270
is built by dividing the parsedData
into three clusters.

61
00:04:36.270 --> 00:04:39.923
Finally, the cluster centers
are printed out for each.

62
00:04:43.259 --> 00:04:47.790
In summary,
MLlib is Spark's machine learning library.

63
00:04:49.240 --> 00:04:50.710
It provides algorithms and

64
00:04:50.710 --> 00:04:53.880
techniques that are implemented
using distributors processing.

65
00:04:54.970 --> 00:04:57.327
The main categories of algorithms and

66
00:04:57.327 --> 00:05:01.968
techniques available in machine
learning library are machine learning,

67
00:05:01.968 --> 00:05:06.324
statistics and utility functions for
the machine learning process.
WEBVTT

1
00:00:02.072 --> 00:00:04.356
Over the next couple of videos,

2
00:00:04.356 --> 00:00:09.020
we will introduce you to the basic
components of the Spark stack.

3
00:00:10.660 --> 00:00:13.440
In this lecture, we start with Spark SQL.

4
00:00:15.000 --> 00:00:20.610
After this video, you will be able to
process structured data using Spark's SQL

5
00:00:20.610 --> 00:00:25.140
module and explain the numerous
benefits of Spark SQL.

6
00:00:28.240 --> 00:00:34.150
Spark SQL is the component of Spark
that enables querying structured and

7
00:00:34.150 --> 00:00:37.510
unstructured data through
a common query language.

8
00:00:38.630 --> 00:00:43.720
It can connect to many data sources and
provides APIs to convert

9
00:00:43.720 --> 00:00:48.430
the query results to RDDs in Python,
Scala, and Java programs.

10
00:00:50.670 --> 00:00:53.991
Spark SQL gives a mechanism for

11
00:00:53.991 --> 00:00:58.467
SQL users to deploy SQL queries on Spark.

12
00:01:01.047 --> 00:01:05.806
Spark SQL enables business
intelligence tools to connect to

13
00:01:05.806 --> 00:01:10.854
Spark using standard connection
protocols like JDBC and ODBC.

14
00:01:13.379 --> 00:01:18.346
Spark SQL also provides APIs to
convert the query data into DataFrames

15
00:01:18.346 --> 00:01:20.290
to hold distributed data.

16
00:01:21.410 --> 00:01:26.757
DataFrames are organized as named
columns and basically look like tables.

17
00:01:30.248 --> 00:01:36.320
The first step to run any SQL Spark
is to create a SQLContext.

18
00:01:38.580 --> 00:01:43.360
Once you have an SQLContext,
you want to leverage it

19
00:01:43.360 --> 00:01:48.200
to create a DataFrame so you can deploy
complex operations on the data set.

20
00:01:49.200 --> 00:01:52.480
DataFrames can be created
from existing RDDs,

21
00:01:52.480 --> 00:01:55.270
Hive tables or many other data sources.

22
00:01:57.910 --> 00:02:05.020
A file can be read and converted into
a DataFrame using a single command.

23
00:02:06.610 --> 00:02:14.440
The show function here will display
the DataFrame in your Spark show.

24
00:02:14.440 --> 00:02:18.950
RDDs can be converted to DataFrames but
require a little more work.

25
00:02:19.980 --> 00:02:23.130
First you will have to
convert each line into a row.

26
00:02:24.410 --> 00:02:28.640
Once your data is in a DataFrame,
you can perform all sorts of

27
00:02:28.640 --> 00:02:33.640
transformation operations on it
as shown here, including show,

28
00:02:33.640 --> 00:02:37.590
printSchema, select, filter and groupBy.

29
00:02:39.850 --> 00:02:44.910
To summarize, Spark SQL lets you
run relational queries on Spark.

30
00:02:46.500 --> 00:02:51.110
It also lets you connect to
a variety of databases, and

31
00:02:51.110 --> 00:02:54.750
deploy business intelligence
tools over Spark.

32
00:02:54.750 --> 00:02:58.707
We will go through some of this
functionality in one of the readings and

33
00:02:58.707 --> 00:03:00.795
in the upcoming hands-on session.
WEBVTT

1
00:00:01.630 --> 00:00:05.040
Next we will talk about Spark streaming.

2
00:00:06.650 --> 00:00:12.474
After this video you will be able to
summarize how Spark reads streaming data,

3
00:00:12.474 --> 00:00:17.240
list several sources of streaming
data supported by Spark, and

4
00:00:17.240 --> 00:00:20.077
describe Spark's sliding windows.

5
00:00:20.077 --> 00:00:25.778
Spark streaming provides
scalable processing for

6
00:00:25.778 --> 00:00:30.954
real-time data and
runs on top of Spark Core.

7
00:00:32.816 --> 00:00:36.319
Continuous data streams are converted or

8
00:00:36.319 --> 00:00:42.130
grouped into discrete RDDs which
can then be processed in parallel.

9
00:00:43.620 --> 00:00:47.357
Spark Streaming provides APIs for Scala,

10
00:00:47.357 --> 00:00:51.620
Java, and Python,
like other Spark products.

11
00:00:55.370 --> 00:01:00.820
Spark Streaming can read
data from many different

12
00:01:00.820 --> 00:01:06.278
types of resources,
including Kafka and Flume.

13
00:01:06.278 --> 00:01:12.778
Kafka is a high throughput published
subscribed messaging system,

14
00:01:12.778 --> 00:01:17.044
and Flume collects and
aggregates log data.

15
00:01:17.044 --> 00:01:22.125
Spark Streaming can also read from batch

16
00:01:22.125 --> 00:01:26.609
input data sources, such as HDFS,

17
00:01:26.609 --> 00:01:31.720
S3, and many other non SQL databases.

18
00:01:31.720 --> 00:01:39.368
Additionally, Spark Streaming can read
directly from Twitter, raw TCP sockets,

19
00:01:39.368 --> 00:01:45.306
and many other data sources that
are real-time data providers.

20
00:01:48.358 --> 00:01:49.280
So how does it all work?

21
00:01:50.320 --> 00:01:55.910
Here we show you a flow of
transformations and actions

22
00:01:55.910 --> 00:02:01.131
which you will try in the upcoming reading
and hands-on exercises on Spark Streaming.

23
00:02:01.131 --> 00:02:05.102
Spark streaming reads

24
00:02:05.102 --> 00:02:10.760
streaming data and
converts it into micro batches

25
00:02:10.760 --> 00:02:16.350
which we call DStreams which is short for
discretized stream.

26
00:02:19.378 --> 00:02:24.736
In this example a 10 second
stream gets converted

27
00:02:24.736 --> 00:02:29.980
into five RDDs using a batch
length of 2 seconds.

28
00:02:31.800 --> 00:02:36.920
Similar to other RDDs,
transformations such as map, reduce,

29
00:02:36.920 --> 00:02:39.515
and filter can be applied to DStreams.

30
00:02:39.515 --> 00:02:45.720
DStreams can be aggregated

31
00:02:45.720 --> 00:02:52.310
into Windows allowing you to apply
computations on sliding window of data.

32
00:02:53.860 --> 00:02:58.165
In this example the Window
size is 4 seconds and

33
00:02:58.165 --> 00:03:01.510
the sliding interval is 2 seconds.

34
00:03:04.826 --> 00:03:09.594
In summary, Spark Streaming is
Spark's library to work with

35
00:03:09.594 --> 00:03:12.260
streaming data in near real time.

36
00:03:14.375 --> 00:03:19.210
DStreams can be used just
like any other RDD and

37
00:03:19.210 --> 00:03:22.940
can go through the same
transformation as batch datasets.

38
00:03:24.560 --> 00:03:31.907
DStreams can create a sliding window to
perform calculations on a window of time.
