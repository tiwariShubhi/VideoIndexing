1
00:00:00,870 --> 00:00:04,540
In this short video we’ll
talk about how Meltwater

2
00:00:04 --> 00:00:09
helped Danone using sentiment analysis.

3
00:00:09 --> 00:00:12
Meltwater is a company
that helps other companies

4
00:00:12 --> 00:00:17
analyze what people are saying about
them and manage their online reputation.

5
00:00:18 --> 00:00:22
One of the case studies on their
website is about Danone baby nutrition.

6
00:00:24 --> 00:00:29
Meltwater helped Danone to monitor
the opinions through social media for

7
00:00:29 --> 00:00:31
one of their marketing campaigns.

8
00:00:31 --> 00:00:35
They were able to measure what
was impactful and what was not,

9
00:00:35 --> 00:00:36
through such monitoring.

10
00:00:37 --> 00:00:42
Meltwater also helped Danone manage
a potential reputation issue.

11
00:00:42 --> 00:00:45
When a crisis occurred
related to horse DNA

12
00:00:45 --> 00:00:48
being in some meat products across Europe.

13
00:00:48 --> 00:00:52
While Danone was confident that they
didn't have an issue with their products,

14
00:00:52 --> 00:00:57
having the information a couple of
hours before news hit the UK press.

15
00:00:57 --> 00:00:59
Allowed them to check and

16
00:00:59 --> 00:01:04
reassure their customers that their
products were safe to consume.

17
00:01:04 --> 00:01:09
You can imagine millions of mothers
having been reassured and happy for

18
00:01:09 --> 00:01:11
Danone's efforts on this.

19
00:01:11 --> 00:01:16
This is an excellent story about how
big data helped manage public opinion.

20
00:01:16 --> 00:01:21
And I'm sure Meltwater was able to help
them to measure the opinion impact through

21
00:01:21 --> 00:01:22
social media as well.

1
00:01:23 --> 00:01:26
Big data is now being
generated all around us.

2
00:01:26 --> 00:01:27
So what?

3
00:01:27 --> 00:01:29
It's the applications.

4
00:01:29 --> 00:01:34
It is the way in which big data can
serve human needs that makes it valued.

5
00:01:35 --> 00:01:39
Let's look at a few examples of the
applications big data is allowing us to

6
00:01:39 --> 00:01:41
imagine and build.

7
00:01:59 --> 00:02:05
Big data allows us to build better models,
which produce higher precision results.

8
00:02:06 --> 00:02:11
We are witnessing hugely innovative
approaches in how companies

9
00:02:11 --> 00:02:13
market themselves and sell products.

10
00:02:13 --> 00:02:15
How human resources are managed.

11
00:02:15 --> 00:02:18
How disasters are responded to.

12
00:02:18 --> 00:02:22
And many other applications that
evidenced based data is being

13
00:02:22 --> 00:02:24
used to influence decisions.

14
00:02:26 --> 00:02:28
What exactly does that mean?

15
00:02:28 --> 00:02:30
Here is one example.

16
00:02:30 --> 00:02:32
Many of you might have experienced it,
I do.

17
00:02:34 --> 00:02:38
Data, Amazon keeps some
things I've been looking at

18
00:02:38 --> 00:02:41
allows them to personalize
what they show me.

19
00:02:41 --> 00:02:46
Which hopefully helps narrow down
the huge raft of options I might get

20
00:02:46 --> 00:02:49
than just searching on dinner plates.

21
00:02:49 --> 00:02:54
Now, businesses can leverage technology
to make better informed decisions

22
00:02:54 --> 00:02:59
that are actually based on signals
generated by actual consumers, like me.

23
00:03:01 --> 00:03:05
Big data enables you to hear
the voice of each consumer as

24
00:03:05 --> 00:03:08
opposed to consumers at large.

25
00:03:09 --> 00:03:13
Now, many companies,
including Walmart and Target,

26
00:03:13 --> 00:03:19
use this information to personalize their
communications with their costumers, which

27
00:03:19 --> 00:03:23
in turns leads to better met consumer
expectations and happier customers.

28
00:03:25 --> 00:03:31
Which basically is to say, big data
has enabled personalized marketing.

29
00:03:31 --> 00:03:36
Consumers are copiously generating
publicly accessible data through

30
00:03:36 --> 00:03:38
social media sites,
like Twitter or Facebook.

31
00:03:39 --> 00:03:44
Through such data, the companies
are able to see their purchase history,

32
00:03:44 --> 00:03:48
what they searched for, what they watched,
where they have been, and

33
00:03:48 --> 00:03:51
what they're interested in
through their likes and shares.

34
00:03:52 --> 00:03:57
Let's look at some examples of how
companies are putting this information to

35
00:03:57 --> 00:04:01
build better marketing campaigns and
reach the right customers.

36
00:04:04 --> 00:04:08
One area we are all familiar with
are the recommendation engines.

37
00:04:08 --> 00:04:14
These engines leverage user patterns and
product features

38
00:04:14 --> 00:04:20
to predict best match product for
enriching the user experience.

39
00:04:20 --> 00:04:22
If you ever shopped on Amazon,

40
00:04:22 --> 00:04:26
you know you get recommendations
based on your purchase.

41
00:04:26 --> 00:04:29
Similarly, Netflix would
recommend you to watch

42
00:04:29 --> 00:04:32
new shows based on your viewing history.

43
00:04:34 --> 00:04:40
Another technique that companies use is
sentiment analysis, or in simple terms,

44
00:04:40 --> 00:04:44
analysis of the feelings around events and
products.

45
00:04:45 --> 00:04:49
Remember the blue plates I
purchased on Amazon.com?

46
00:04:49 --> 00:04:53
I not only can read the reviews
before purchasing them,

47
00:04:53 --> 00:04:57
I can also write a product
review once I receive my plates.

48
00:04:59 --> 00:05:02
This way, other customers can be informed.

49
00:05:03 --> 00:05:09
But more importantly, Amazon can keep
a watch on the product reviews and

50
00:05:09 --> 00:05:11
trends for a particular product.

51
00:05:11 --> 00:05:13
In this case, blue plates.

52
00:05:13 --> 00:05:20
For example, they can judge if a product
review is positive or negative.

53
00:05:21 --> 00:05:25
In this case,
while the first review is negative,

54
00:05:27 --> 00:05:29
the next two reviews are positive.

55
00:05:31 --> 00:05:35
Since these reviews are written in
English using a technique called natural

56
00:05:35 --> 00:05:38
language processing, and
other analytical methods,

57
00:05:38 --> 00:05:44
Amazon can analyze the general opinion of
a person or public about such a product.

58
00:05:46 --> 00:05:51
This is why sentiment analysis often
gets referred to as opinion mining.

59
00:05:53 --> 00:05:57
News channels are filled with
Twitter feed analysis every time

60
00:05:57 --> 00:06:01
an event of importance occurs,
such as elections.

61
00:06:02 --> 00:06:08
Brands utilize sentiment analysis
to understand how customers

62
00:06:08 --> 00:06:13
relate to their product,
positively, negatively, neutral.

63
00:06:13 --> 00:06:16
This depends heavily on use of
natural language processing.

64
00:06:19 --> 00:06:21
Mobile devices are ubiquitous and

65
00:06:21 --> 00:06:24
people almost always carry
their cellphones with them.

66
00:06:25 --> 00:06:29
Mobile advertising is a huge market for
businesses.

67
00:06:30 --> 00:06:35
Platforms utilize the sensors
in mobile devices,

68
00:06:35 --> 00:06:40
such as GPS, and
provide real time location based ads,

69
00:06:40 --> 00:06:45
offer discounts,
based on this deluge of data.

70
00:06:45 --> 00:06:50
This time, let's imagine that
I bought a new house and

71
00:06:50 --> 00:06:54
I happen to be in a few
miles range of a Home Depot.

72
00:06:54 --> 00:06:57
Sending me mobile coupons about paint,
shelves, and

73
00:06:57 --> 00:07:01
other new home related purchases
would remind me of Home Depot.

74
00:07:02 --> 00:07:05
There's a big chance I
would stop by Home Depot.

75
00:07:05 --> 00:07:06
Bingo!

76
00:07:06 --> 00:07:11
Now I would like to take a moment to
analyze what kinds of big data are needed

77
00:07:11 --> 00:07:12
to make this happen.

78
00:07:13 --> 00:07:17
There's definitely the integration
of my consumer information and

79
00:07:17 --> 00:07:22
the online and offline databases
that include my recent purchases.

80
00:07:22 --> 00:07:27
But more importantly,
the geolocation data that falls under

81
00:07:27 --> 00:07:31
a larger type of big data,
spacial big data.

82
00:07:31 --> 00:07:33
We will talk about spacial
data later in this class.

83
00:07:35 --> 00:07:39
Let's now talk about how the global
consumer behavior can be used for

84
00:07:39 --> 00:07:40
product growth.

85
00:07:42 --> 00:07:45
We are now moving from
personalize marketing

86
00:07:45 --> 00:07:48
to the consumer behavior as a whole.

87
00:07:49 --> 00:07:54
Every business wants to understand
their consumer’s collective

88
00:07:54 --> 00:07:59
behavior in order to capture
the ever-changing landscape.

89
00:07:59 --> 00:08:04
Several big data products enable this
by developing models to capture user

90
00:08:04 --> 00:08:10
behavior and allow businesses to target
the right audience for their product.

91
00:08:12 --> 00:08:15
Or, develop new products for
uncharted territories.

92
00:08:17 --> 00:08:19
Let's look at this example.

93
00:08:19 --> 00:08:22
After an analysis of their sales for
weekdays,

94
00:08:22 --> 00:08:28
an airline company might notice that their
morning flights are always sold out,

95
00:08:28 --> 00:08:31
while their afternoon
flights run below capacity.

96
00:08:31 --> 00:08:37
This company might decide to add more
morning flights based on such analysis.

97
00:08:38 --> 00:08:43
Notice that they are not using
individual consumer choices, but

98
00:08:43 --> 00:08:48
using all the flights purchased without
consideration to who purchased them.

99
00:08:50 --> 00:08:51
They might, however,

100
00:08:51 --> 00:08:56
decide to pay closer attention to
the demographic of these consumers

101
00:08:56 --> 00:09:00
using big data to also add similar
flights in other geographical regions.

102
00:09:02 --> 00:09:07
With rapid advances in genome
sequencing technology,

103
00:09:07 --> 00:09:13
the life sciences industry is experiencing
an enormous draw in biomedical big data.

104
00:09:15 --> 00:09:21
This biomedical data is being used
by many applications in research and

105
00:09:21 --> 00:09:23
personalized medicine.

106
00:09:23 --> 00:09:28
Did you know genomics data is one of
the largest growing big data types?

107
00:09:28 --> 00:09:35
Between 100 million and 2 billion human
genomes could be sequenced by year 2025.

108
00:09:35 --> 00:09:36
Impressive.

109
00:09:38 --> 00:09:41
This [INAUDIBLE] sequence data demands for

110
00:09:41 --> 00:09:46
between 2 exabytes and
40 exabytes in data storage.

111
00:09:46 --> 00:09:52
In comparison, all of YouTube only
requires 1 to 2 exabytes a year.

112
00:09:54 --> 00:09:57
An exabyte is 10 to the power 18 bites.

113
00:09:57 --> 00:10:03
That is, 18 zeros after 40.

114
00:10:03 --> 00:10:10
Of course, analysis of such massive
volumes of sequence data is expensive.

115
00:10:10 --> 00:10:12
It could take up to 10,000
trillion CPU hours.

116
00:10:16 --> 00:10:21
One of the biomedical applications
that this much data is enabling

117
00:10:21 --> 00:10:22
is personalized medicine.

118
00:10:24 --> 00:10:28
Before personalized medicine,
most patients without a specific type and

119
00:10:28 --> 00:10:31
stage of cancer received
the same treatment,

120
00:10:31 --> 00:10:34
which worked better for
some than the others.

121
00:10:36 --> 00:10:42
Research in this area is enabling
development of methods to analyze

122
00:10:42 --> 00:10:47
large scale data to develop solutions
that tailor to each individual,

123
00:10:47 --> 00:10:50
and hence hypothesize
to be more effective.

124
00:10:52 --> 00:10:58
A person with cancer may now still receive
a treatment plan that is standard,

125
00:10:58 --> 00:11:00
such as surgery to remove a tumor.

126
00:11:01 --> 00:11:05
However, the doctor may
also be able to recommend

127
00:11:05 --> 00:11:07
some type of personalized
cancer treatment.

128
00:11:09 --> 00:11:13
A big challenge in biomedical big data
applications, like many other fields,

129
00:11:13 --> 00:11:18
is how we can integrate many types of data
sources to gain further insight problem.

130
00:11:20 --> 00:11:23
In one of our future lectures,
my colleagues here at

131
00:11:23 --> 00:11:28
the Supercomputer Center,
will explain how he and his colleague have

132
00:11:28 --> 00:11:33
used big data from a variety of sources
for personalized patient interventions.

133
00:11:35 --> 00:11:41
Another application of big data comes
from interconnected mesh of large

134
00:11:41 --> 00:11:46
number of sensors implanted
across smart cities.

135
00:11:46 --> 00:11:50
Analysis of data generated
from sensors in real time

136
00:11:50 --> 00:11:55
allows cities to deliver better
service quality to inhabitants.

137
00:11:55 --> 00:12:00
And reduce unwanted affect such
as pollution, traffic congestion,

138
00:12:00 --> 00:12:03
higher than optimal cost on
delivering urban services.

139
00:12:04 --> 00:12:06
Let's take our city, San Diego.

140
00:12:08 --> 00:12:13
San Diego generates a huge volumes
of data from many sources.

141
00:12:13 --> 00:12:19
Traffic sensors, satellites,
camera networks, and more.

142
00:12:19 --> 00:12:21
What if we could integrate and

143
00:12:21 --> 00:12:26
synthesize these data streams to
do even more for our community?

144
00:12:27 --> 00:12:29
Using such big data,

145
00:12:29 --> 00:12:34
we can work toward making San Diego
the prototype digital city.

146
00:12:34 --> 00:12:37
Not only for life-threatening hazards, but

147
00:12:37 --> 00:12:42
making our daily lives better, such as
managing traffic flow more efficiently or

148
00:12:42 --> 00:12:46
maximizing energy savings,
even as we'll see next, wildfires.

149
00:12:48 --> 00:12:53
If you want to read more,
here's a link to the AT Kearney report,

150
00:12:53 --> 00:12:56
where they talk about other
areas using big data.

151
00:12:57 --> 00:13:01
As a summary,
big data has a huge potential

152
00:13:01 --> 00:13:06
to enable models with higher
precision in many application areas.

153
00:13:07 --> 00:13:12
And these highly precise models
are influencing and transforming business.

1
00:14:35 --> 00:14:38
Big Data Generated By People,
how is it being used?

2
00:14:39 --> 00:14:42
We listed a number of challenges for

3
00:14:42 --> 00:14:45
using unstructured data
generated by human activities.

4
00:14:47 --> 00:14:51
Now let's look at some of the emerging
technologies to tackle these challenges.

5
00:14:51 --> 00:14:56
And see some examples that turn
unstructured data into valuable insights.

6
00:15:15 --> 00:15:20
Although unstructured data specially
the kind generated by people has

7
00:15:20 --> 00:15:23
a number of challenges.

8
00:15:23 --> 00:15:27
The good news is that the business
culture of today is shifting

9
00:15:27 --> 00:15:31
to tackle these challenges and
take full advantage of such data.

10
00:15:32 --> 00:15:36
As it is often said,
a challenge is a perfect opportunity.

11
00:15:37 --> 00:15:40
This is certainly the case for
big data and

12
00:15:40 --> 00:15:43
these challenges have created
a tech industry of it's own.

13
00:15:44 --> 00:15:49
This industry is mostly centered or
as we would say, layered or

14
00:15:49 --> 00:15:55
stacked, around a few fundamental
open source big data frameworks.

15
00:15:55 --> 00:15:59
Need big data tools
are designed from scratch

16
00:15:59 --> 00:16:02
to manage unstructured information and
analyze it.

17
00:16:02 --> 00:16:06
A majority of these tools
are based on an open source

18
00:16:06 --> 00:16:08
big data framework called Hadoop.

19
00:16:09 --> 00:16:14
Hadoop is designed to support
the processing of large data sets

20
00:16:14 --> 00:16:16
in a distributed computing environment.

21
00:16:16 --> 00:16:21
This definition would already give you a
hint that it tackles the first challenge.

22
00:16:21 --> 00:16:25
Namely, the volume of
unstructured information.

23
00:16:26 --> 00:16:29
Hadoop can handle big batches
of distributed information but

24
00:16:29 --> 00:16:32
most often there's a need for

25
00:16:32 --> 00:16:37
a real time processing of people generated
data like Twitter or Facebook updates.

26
00:16:39 --> 00:16:43
Financial compliance monitoring is another
area of our central time processing is

27
00:16:43 --> 00:16:47
needed, in particular
to reduce market data.

28
00:16:48 --> 00:16:54
Social media and market data are two
types of what we call high velocity data.

29
00:16:55 --> 00:16:59
Storm and
Spark are two other open source frameworks

30
00:16:59 --> 00:17:03
that handle such real time
data generated at a fast rate.

31
00:17:04 --> 00:17:05
Both Storm and

32
00:17:05 --> 00:17:10
Spark can integrate data with any
database or data storage technology.

33
00:17:11 --> 00:17:15
As we have emphasized
before unstructured data

34
00:17:15 --> 00:17:19
does not have a relational data model so
it doesn't generally

35
00:17:19 --> 00:17:24
fit into the traditional data warehouse
model based on relational databases.

36
00:17:25 --> 00:17:29
Data warehouses are central repositories
of integrated data from one or

37
00:17:29 --> 00:17:30
more sources.

38
00:17:32 --> 00:17:38
The data that gets stored in warehouses,
gets extracted from multiple sources.

39
00:17:39 --> 00:17:43
It gets transformed into
a common structured form and

40
00:17:43 --> 00:17:47
it can slow that into
the central database for

41
00:17:47 --> 00:17:51
use by workers creating analytical
reports throughout an enterprise.

42
00:17:52 --> 00:17:59
This Exact Transform Load
process is commonly called ETL.

43
00:17:59 --> 00:18:03
This approach was fairly standard in
enterprise data systems until recently.

44
00:18:04 --> 00:18:08
As you probably noticed,
it is fairly static and

45
00:18:08 --> 00:18:11
does not fit well with today's
dynamic big data world.

46
00:18:12 --> 00:18:17
So how do today's businesses
get around this problem?

47
00:18:17 --> 00:18:21
Many businesses today are using a hybrid
approach in which their smaller

48
00:18:21 --> 00:18:25
structured data remains in
their relational databases, and

49
00:18:25 --> 00:18:30
large unstructured datasets get stored
in NoSQL databases in the cloud.

50
00:18:32 --> 00:18:37
NoSQL Data technologies are based
on non-relational concepts and

51
00:18:37 --> 00:18:42
provide data storage options
typically on computing clouds

52
00:18:42 --> 00:18:46
beyond the traditional relational
databases centered rate houses.

53
00:18:48 --> 00:18:53
The main advantage of using
NoSQL solutions is their ability

54
00:18:53 --> 00:18:58
to organize the data for
scalable access to fit the problem and

55
00:18:58 --> 00:19:01
objectives pertaining to
how the data will be used.

56
00:19:03 --> 00:19:08
For example, if the data will be used
in an analysis to find connections

57
00:19:08 --> 00:19:14
between data sets, then the best
solution is a graph database.

58
00:19:15 --> 00:19:18
Neo4j is an example of a graph database.

59
00:19:19 --> 00:19:23
Graph networks is a topic that
the graph analytics course

60
00:19:23 --> 00:19:27
later in this specialization,
we'll explain in depth.

61
00:19:27 --> 00:19:33
If the data will be best accessed using
key value pairs like a search engine

62
00:19:33 --> 00:19:39
scenario, the best solution is probably
a dedicated key value paired database.

63
00:19:42 --> 00:19:45
Cassandra is an example
of a key value database.

64
00:19:47 --> 00:19:48
These, and

65
00:19:48 --> 00:19:52
many other types of NoSQL systems will
be explained further in course two.

66
00:19:54 --> 00:19:58
So we are now confident that there
are emerging technologies for

67
00:19:58 --> 00:20:02
individual challenges to manage
people generated unstructured data.

68
00:20:03 --> 00:20:07
But how does one take advantage
of these to generate value?

69
00:20:09 --> 00:20:17
As we saw big data must pass through a
series of steps before it generates value.

70
00:20:17 --> 00:20:21
Namely data access, storage,
cleaning, and analysis.

71
00:20:23 --> 00:20:29
One approach to solve this problem is
to run each stage as a different layer.

72
00:20:30 --> 00:20:34
And use tools available to
fit the problem at hand, and

73
00:20:34 --> 00:20:37
scale analytical solutions to big data.

74
00:20:37 --> 00:20:42
In coming lectures, we will see
important tools that you can use

75
00:20:42 --> 00:20:45
to solve your big data problems in
addition to the ones you have seen today.

76
00:20:47 --> 00:20:51
Now let's take a step back and remind
ourselves what some of the value was.

77
00:20:53 --> 00:20:57
Remember how companies can listen to the
real voice of customers using big data?

78
00:20:59 --> 00:21:03
It is this type of generated
data that enabled it.

79
00:21:04 --> 00:21:09
Sentiment analysis analyzes social
media and other data to find

80
00:21:09 --> 00:21:14
whether people associate positively or
negatively with you business.

81
00:21:14 --> 00:21:19
Organizations are utilizing
processing of personal data to

82
00:21:19 --> 00:21:21
understand the true
preferences of their customers.

83
00:21:23 --> 00:21:28
Now let's take a fun quiz to guess how
much Twitter data companies analyze

84
00:21:28 --> 00:21:31
every day to measure sentiment
around their product.

85
00:21:33 --> 00:21:35
The answer is 12 terabytes a day.

86
00:21:37 --> 00:21:42
For comparison,
you would need to listen continuously for

87
00:21:42 --> 00:21:45
two years to finish listening
to 1 terabyte of music.

88
00:21:47 --> 00:21:49
Another example application area for

89
00:21:49 --> 00:21:52
people generated data is customer
behavior modeling and prediction.

90
00:21:54 --> 00:21:58
Amazon, Netflix and
a lot of other organizations,

91
00:21:58 --> 00:22:02
use analytics to analyze
preferences of their customers.

92
00:22:03 --> 00:22:09
Based on consumer behavior, organizations
suggest better products to customers,

93
00:22:10 --> 00:22:14
and in turn have happier customers and
higher profits.

94
00:22:15 --> 00:22:21
Another application area where the value
comes in the form of societal impact and

95
00:22:21 --> 00:22:24
social welfare, is disaster management.

96
00:22:25 --> 00:22:28
As you have seen in my wildfire example,

97
00:22:28 --> 00:22:32
there are many types of big data that
can help with disaster response.

98
00:22:33 --> 00:22:37
Data in the form of pictures and
tweets, helps facilitate

99
00:22:37 --> 00:22:42
a collective response to disaster
situations, such as evacuations through

100
00:22:42 --> 00:22:46
the safest route based on community
feedback through social media.

101
00:22:47 --> 00:22:50
There are also networks that
turn crowd sourcing and

102
00:22:50 --> 00:22:54
big data analytics into collective
disaster response tools.

103
00:22:55 --> 00:22:58
The International Network
of Crisis Mappers,

104
00:22:58 --> 00:23:03
also called Crisis Mappers Net,
is the largest of such networks and

105
00:23:03 --> 00:23:08
includes an active international
community of volunteers.

106
00:23:08 --> 00:23:14
Crisis Mappers use big data in the form
of aerial and satellite imagery,

107
00:23:14 --> 00:23:19
participatory maps and
live Twitter updates to analyze

108
00:23:19 --> 00:23:25
the data using geospatial platforms,
advanced visualization,

109
00:23:25 --> 00:23:30
live simulation and computational and
statistical models.

110
00:23:31 --> 00:23:37
Once analyzed the results get reported to
rapid response and humanitarian agencies

111
00:23:38 --> 00:23:44
in the form of mobile and
web applications.

112
00:23:44 --> 00:23:50
In 2015, right after the Nepal earthquake
Crises Mappers crowd source the analysis

113
00:23:50 --> 00:23:55
of tweets and mainstream media to
rapidly access disaster damage and

114
00:23:55 --> 00:24:01
needs and to identify where
humanitarian help is needed.

115
00:24:01 --> 00:24:06
This example is amazing and shows how
big data can have huge impacts for

116
00:24:06 --> 00:24:09
social welfare in times of need.

117
00:24:09 --> 00:24:12
You can learn more about this
story at the following link.

118
00:24:14 --> 00:24:19
As a summary, although there are
challenges in working with unstructured

119
00:24:19 --> 00:24:24
people generated data at a scale and
speed that applications demand.

120
00:24:24 --> 00:24:28
There are also emerging technologies and
solutions that are being

121
00:24:28 --> 00:24:32
used by many applications to generate
value from the rich source of information.

1
00:39:06 --> 00:39:11
Big Data Generated By People,
The Unstructured Challenge.

2
00:39:27 --> 00:39:32
People are generating massive amounts of
data every day through their activities on

3
00:39:32 --> 00:39:37
various social media networking sites
like Facebook, Twitter, and LinkedIn.

4
00:39:37 --> 00:39:42
Or online photo sharing sites like
Instagram, Flickr, or Picasa.

5
00:39:44 --> 00:39:46
And video sharing websites like YouTube.

6
00:39:48 --> 00:39:53
In addition an enormous amount of
information gets generated via

7
00:39:53 --> 00:39:58
blogging and commenting,
internet searches, more via text messages,

8
00:39:59 --> 00:40:02
email, and through personal documents.

9
00:40:03 --> 00:40:08
Most of this data is text-heavy and
unstructured,

10
00:40:08 --> 00:40:14
that is non-conforming to
a well-defined data model.

11
00:40:14 --> 00:40:18
We can also consider this
data to be content with

12
00:40:18 --> 00:40:21
occasionally some
description attached to it.

13
00:40:21 --> 00:40:26
This much activity Leads
to a huge growth in data.

14
00:40:27 --> 00:40:31
Did you know that in a single day,
Facebook users produce

15
00:40:31 --> 00:40:36
more data than combined US
academic research libraries?

16
00:40:38 --> 00:40:41
Let's look at some similar
daily data volume numbers

17
00:40:42 --> 00:40:44
from some of the biggest online platforms.

18
00:40:45 --> 00:40:49
It is amazing that some of these
numbers are in the petabyte range for

19
00:40:49 --> 00:40:50
daily activity.

20
00:40:51 --> 00:40:54
A petabyte is a thousand terabytes.

21
00:40:56 --> 00:41:01
The sheer size of mostly unstructured
data generated by humans

22
00:41:01 --> 00:41:02
brings a lot of challenges.

23
00:41:04 --> 00:41:11
Unstructured data refers to data that does
not conform to a predefined data model.

24
00:41:13 --> 00:41:16
So no relation model and no SQL.

25
00:41:17 --> 00:41:22
It is mostly anything that we
don't store in a traditional

26
00:41:22 --> 00:41:23
Relational database management system.

27
00:41:25 --> 00:41:28
Consider a sales receipt that
you get from a grocery store.

28
00:41:29 --> 00:41:32
It has a section for a date, a section for

29
00:41:32 --> 00:41:36
store name, and
a section for total amount.

30
00:41:38 --> 00:41:40
This is an example of structure.

31
00:41:41 --> 00:41:46
Humans generate a lot of
unstructured data in form of text.

32
00:41:46 --> 00:41:48
There's no given format to that.

33
00:41:48 --> 00:41:52
Look at all the documents that you
have written with your hand so far.

34
00:41:53 --> 00:41:58
Collectively, it is a bank of unstructured
data you have personally generated.

35
00:41:59 --> 00:42:04
In fact, 80 to 90% of all data in

36
00:42:04 --> 00:42:08
the world is unstructured and
this number is rapidly growing.

37
00:42:10 --> 00:42:15
Examples of unstructured data generated
by people includes texts, images,

38
00:42:15 --> 00:42:22
videos, audio,
internet searches, and emails.

39
00:42:22 --> 00:42:27
In addition to it's rapid growth
major challenges of unstructured data

40
00:42:27 --> 00:42:32
include multiple data formats,
like webpages, images, PDFs,

41
00:42:32 --> 00:42:40
power point, XML, and other formats that
were mainly built for human consumption.

42
00:42:40 --> 00:42:47
Think of it, although I can sort my
email with date, sender and subject.

43
00:42:47 --> 00:42:50
It would be really difficult
to write a program,

44
00:42:50 --> 00:42:55
to categorize all my email messages
based on their content and

45
00:42:55 --> 00:43:01
organize them for me accordingly another
challenge of human generated data

46
00:43:01 --> 00:43:07
is the volume and fast generation of data,
which is what we call velocity.

47
00:43:07 --> 00:43:13
Just take a moment to study this info
graphic, and observe what happens in one

48
00:43:13 --> 00:43:18
minute on the internet, and
consider how much to contribute to it.

49
00:43:21 --> 00:43:28
Moreover, confirmation of unstructured
data is often time consuming and costly.

50
00:43:28 --> 00:43:34
The costs and
time of the process of acquiring, storing,

51
00:43:34 --> 00:43:40
cleaning, retrieving, and processing
unstructured data can add up to quite and

52
00:43:40 --> 00:43:43
investment before we can start
reaping value from this process.

53
00:43:45 --> 00:43:47
It can be pretty hard
to find the tools and

54
00:43:47 --> 00:43:51
people to implement such a process and
reap value in the end.

55
00:43:53 --> 00:43:56
As a summary,
although there is an enormous amount of

56
00:43:56 --> 00:44:00
data generated by people,
most of this data is unstructured.

57
00:44:01 --> 00:44:05
The challenges of working with
unstructured data should not

58
00:44:05 --> 00:44:07
be taken lightly.

59
00:44:07 --> 00:44:13
Next, we'll look at how businesses are
tackling these challenges to gain insight.

60
00:44:13 --> 00:44:16
And thus, value out of working
with people generated data.

1
01:23:23 --> 01:23:27
As we have already seen, there are many
different exciting applications

2
01:23:27 --> 01:23:30
that are being enabled
by the Big Data era.

3
01:23:31 --> 01:23:35
As part of my core research here at
the San Diego Supercomputer Center,

4
01:23:35 --> 01:23:37
I work on building methodologies and

5
01:23:37 --> 01:23:42
tools to make Big Data useful to dynamic
data driven scientific applications.

6
01:23:43 --> 01:23:47
My colleagues and I work on many grand
challenge data science applications,

7
01:23:47 --> 01:23:52
in all areas of science and engineering,
including genomics, geoinformatics,

8
01:23:52 --> 01:23:58
metro science, energy management,
biomedicine, and personalized health.

9
01:23:59 --> 01:24:04
What is common to all these applications
is their unique way of bringing

10
01:24:04 --> 01:24:08
together new modes of data and
computing research.

11
01:24:08 --> 01:24:14
Let me tell you the one I'm passionate

12
01:24:14 --> 01:24:19
about, Wildfire Analytics,

13
01:24:19 --> 01:24:25
which breaks up into two components,

14
01:24:25 --> 01:24:29
prediction and response.

15
01:24:40 --> 01:24:43
Why is this so important?

16
01:24:43 --> 01:24:47
On May 2014, in San Diego County where

17
01:24:47 --> 01:24:52
the instructors of this
specialization live and work,

18
01:24:52 --> 01:24:58
there were 14 fires burning,
as many as nine at one time,

19
01:24:58 --> 01:25:04
which burned a total of 26,000 acres,
11,000 hectares,

20
01:25:04 --> 01:25:10
an area just less than the size
of the City of San Francisco.

21
01:25:12 --> 01:25:18
Six people were injured and
one person died.

22
01:25:18 --> 01:25:22
And these wildfires
resulted in a total cost of

23
01:25:22 --> 01:25:26
over $60 million US in damage and
firefighting.

24
01:25:27 --> 01:25:32
These wildfires can become so severe
that we actually call them firestorms.

25
01:25:34 --> 01:25:37
Although we cannot
control such fire storms,

26
01:25:37 --> 01:25:41
something we can do is to get ahead
of them by predicting their behavior.

27
01:25:42 --> 01:25:47
This is why disaster management
of ongoing wildfires relies

28
01:25:47 --> 01:25:51
heavily on understanding their
direction and rate of spread.

29
01:25:52 --> 01:25:55
As these fires are a part of our lives,

30
01:25:55 --> 01:26:00
we wanted to see if we can use Big Data to
monitor, predict and manage a firestorm.

31
01:26:02 --> 01:26:04
Why can Big Data help?

32
01:26:04 --> 01:26:08
As we will see in this video,
indeed, wildfire prevention and

33
01:26:08 --> 01:26:12
response can benefit from many
streams in our data torrent.

34
01:26:12 --> 01:26:18
Some streams are generated by
people through devices they carry.

35
01:26:18 --> 01:26:23
A lot come from sensors and satellites,
things that measure environmental factors.

36
01:26:23 --> 01:26:27
And some come from organizational data,
including area maps,

37
01:26:27 --> 01:26:32
better service updates and field content
databases, which archive how much

38
01:26:32 --> 01:26:37
registers vegetation and other types of
fuel are in the way of a potential fire.

39
01:26:39 --> 01:26:42
What makes this a Big Data problem?

40
01:26:42 --> 01:26:47
Because novel approaches and
responses can be taken if

41
01:26:47 --> 01:26:51
we can integrate this many
diverse data streams.

42
01:26:51 --> 01:26:56
Many such data sources have already
existed for quite some time.

43
01:26:56 --> 01:27:01
But what is lacking in disaster
management today is a dynamic system

44
01:27:01 --> 01:27:06
integration of real time sensor networks,
satellite imagery,

45
01:27:06 --> 01:27:12
near real time data management tools,
wildfire simulation tools,

46
01:27:12 --> 01:27:16
connectivity to emergency command centers,
and

47
01:27:16 --> 01:27:20
all these before, during,
and after a firestorm.

48
01:27:21 --> 01:27:25
As you will see,
the integration of diverse streams and

49
01:27:25 --> 01:27:29
novel ways is really what's driving
our ability to see new things and

50
01:27:29 --> 01:27:33
develop predictive analytics,
which may help improve our world.

51
01:27:34 --> 01:27:36
What are these diverse sources?

52
01:27:37 --> 01:27:42
One of the most important data sources
is sensor data streaming in from weather

53
01:27:42 --> 01:27:48
stations and satellites,
such sensed data include temperature,

54
01:27:48 --> 01:27:49
humidity, air pressure.

55
01:27:51 --> 01:27:55
We can also include image
data streaming from

56
01:27:55 --> 01:28:00
mountaintop cameras and
satellites in this category.

57
01:28:00 --> 01:28:03
Another important data source
comes from institutions

58
01:28:03 --> 01:28:06
such as
the San Diego Supercomputer Center,

59
01:28:06 --> 01:28:10
which generate data related
to wildfire modeling.

60
01:28:10 --> 01:28:15
These include past and current fire
perimeter maps put together by

61
01:28:15 --> 01:28:20
the authorities and fuel maps that
tell us about the vegetation,

62
01:28:20 --> 01:28:24
and other types of fuel in a fire's path.

63
01:28:24 --> 01:28:30
These types of data sources are often
static or updated at a slow rate,

64
01:28:30 --> 01:28:36
but they provide valuable data
that is well-curated and verified.

65
01:28:37 --> 01:28:42
A huge part of data on fires is
actually generated by the public

66
01:28:42 --> 01:28:46
on social media sites such as Twitter,
which support photo sharing resources.

67
01:28:48 --> 01:28:54
These are the hardest data sources to
streamline during an existing fire, but

68
01:28:54 --> 01:28:58
they can be very valuable once
integrated with other data sources.

69
01:29:00 --> 01:29:06
Imagine synthesizing all the pictures
on Twitter about an ongoing fire or

70
01:29:06 --> 01:29:09
checking the public sentiment
around the boundaries of a fire.

71
01:29:11 --> 01:29:15
Once you have access to such
information at your fingertips,

72
01:29:15 --> 01:29:18
there are many things you
can do with such data.

73
01:29:18 --> 01:29:23
You can simply monitor it, or
maybe you can visualize it.

74
01:29:25 --> 01:29:30
But it's not until you bring all these
different types of data sources together

75
01:29:30 --> 01:29:34
and integrate them with real time analysis
and predictive modeling that you can

76
01:29:34 --> 01:29:39
really make contributions in predicting
and responding to wildfire emergencies.

77
01:29:41 --> 01:29:44
So now,
I will like you to take a moment and

78
01:29:44 --> 01:29:49
imagine how Big Data might help
with firefighting in the future.

79
01:29:49 --> 01:29:54
All these streams of data will come
together in 3D displays that can show

80
01:29:54 --> 01:29:59
all the related information along
with weather and fire predictions,

81
01:29:59 --> 01:30:02
just like the way tornadoes
are managed today.

1
02:53:25 --> 02:53:30
Let's look at a 2nd example where big data
can have a big impact on saving lives.

2
02:53:30 --> 02:53:34
I mean,
literally saving lives one life at a time.

3
02:53:34 --> 02:53:38
I collaborated with a number of
world-class researchers in San Diego, and

4
02:53:38 --> 02:53:42
an industrial group who are dedicated to
improving human health through research

5
02:53:42 --> 02:53:44
and practice of precision medicine.

6
02:53:46 --> 02:53:48
What is precision medicine?

7
02:53:48 --> 02:53:52
It is an emerging area of medicine
targeted toward an individual person.

8
02:53:52 --> 02:53:57
Analysing her genetics, her environment,
her daily activities so

9
02:53:57 --> 02:54:01
that one can detect or
predict a health problem early,

10
02:54:01 --> 02:54:06
help prevent disease and
in case of illness provide the right drug

11
02:54:06 --> 02:54:10
at the right dose that is
suitable just for her.

12
02:54:10 --> 02:54:15
Very recently the White House and
the National Institute of Health here

13
02:54:15 --> 02:54:20
in the U.S. have declared it to be
a top priority area for research and

14
02:54:20 --> 02:54:22
development for the next decade.

15
02:54:22 --> 02:54:25
The expected learning
outcome of this video is for

16
02:54:25 --> 02:54:28
you to give example of sensor,
organizational and

17
02:54:28 --> 02:54:31
people-generated data used
in precision medicine.

18
02:54:33 --> 02:54:37
And, explain why the integration of
different kinds of data is critical

19
02:54:37 --> 02:54:38
in advancing healthcare.

20
02:54:40 --> 02:54:44
For any technology to
succeed in real life we need

21
02:54:44 --> 02:54:48
not only a certain level of maturity of
the technology itself, but a number of

22
02:54:48 --> 02:54:54
enabling factors including social
economic environment, market demands,

23
02:54:54 --> 02:54:58
consumer readiness, cost effectiveness,
all of which must work together.

24
02:55:00 --> 02:55:03
Why is big data for
precision medicine important now?

25
02:55:03 --> 02:55:04
Let's see.

26
02:55:05 --> 02:55:08
An important aspect of precision
medicine is to utilize

27
02:55:08 --> 02:55:13
an individual's genetic profile for
his or her own diagnoses and treatment.

28
02:55:14 --> 02:55:16
Analyzing the human genome,

29
02:55:16 --> 02:55:20
which holds the key to human health
is rapidly becoming more affordable.

30
02:55:21 --> 02:55:25
Today's cost to sequence a genome is less

31
02:55:25 --> 02:55:27
than 10% of what it
cost just back in 2008.

32
02:55:27 --> 02:55:33
But, human genomic data is big.

33
02:55:33 --> 02:55:34
How big?

34
02:55:34 --> 02:55:38
In a perfect world, just the three
billion letters of your genome

35
02:55:38 --> 02:55:41
would require about 700
megabytes to store.

36
02:55:42 --> 02:55:47
In the real world, meaning the kind of
data generated from genome sequencing

37
02:55:47 --> 02:55:51
machines, we need 200GB to store a genome.

38
02:55:52 --> 02:55:56
And it takes now about
a day to sequence a genome.

39
02:55:56 --> 02:56:01
We are finally beginning to create more
electronic records that can be stored and

40
02:56:01 --> 02:56:03
manipulated in digital media.

41
02:56:04 --> 02:56:09
Most doctors offices and hospitals now
use electronic health record systems

42
02:56:09 --> 02:56:13
which contain all details of
a patient's visit and lab test.

43
02:56:14 --> 02:56:15
How big is this data?

44
02:56:16 --> 02:56:21
As a quick example The Samaritan Medical
Center Watertown New York at 294 that

45
02:56:21 --> 02:56:27
Community Hospital reported
120 terabytes as of 2013.

46
02:56:27 --> 02:56:31
The data value more than double
in just the last two years.

47
02:56:32 --> 02:56:37
So clearly just in a past two years
dramatic changes have prepared the health

48
02:56:37 --> 02:56:42
care industry to produce and analyze
larger mounts of complex patient data.

49
02:56:44 --> 02:56:49
To summarize what we have seen so far
the key components of these changes are:

50
02:56:49 --> 02:56:55
Reduced cost of data generation and
analysis, increased availability of cheap

51
02:56:55 --> 02:57:01
large data storage, and they increased
digitization of previously paper records.

52
02:57:02 --> 02:57:07
But we need one more capability to advance
toward the promised land of individualized

53
02:57:07 --> 02:57:08
health care practices.

54
02:57:09 --> 02:57:14
We need to combine various types of
data produce by different groups in

55
02:57:14 --> 02:57:15
a meaningful way.

56
02:57:16 --> 02:57:20
Let's look at this issue from
the same point of view as Ilka did.

57
02:57:20 --> 02:57:23
With her discussion of how big data
can help with wildfire analytics.

58
02:57:25 --> 02:57:28
The key is the integration of
multiple types of data sources.

59
02:57:28 --> 02:57:33
Data from sensors,
organizations and people.

60
02:57:33 --> 02:57:36
In the next few slides,
we look at each of these, and

61
02:57:36 --> 02:57:41
then I'll share a story about some of the
new and really exciting ways people data

62
02:57:41 --> 02:57:46
especially has the potential to
change healthcare big data landscape.

63
02:57:48 --> 02:57:49
Let's start with sensor data.

64
02:57:50 --> 02:57:55
Sure, digital hospital equipment
have been producing sensor data for

65
02:57:55 --> 02:57:59
years, but it was unlikely that
the data was ever stored or

66
02:57:59 --> 02:58:03
shared, let alone
analyzed retrospectively.

67
02:58:03 --> 02:58:05
These were intended for

68
02:58:05 --> 02:58:09
real-time use, to inform healthcare
professionals, and then got discarded.

69
02:58:11 --> 02:58:14
Now we have many more sensors and
deployment.

70
02:58:14 --> 02:58:16
And many more places
that are capturing and

71
02:58:16 --> 02:58:19
explicitly gathering information
to be stored and analyzed.

72
02:58:21 --> 02:58:23
Let's just take a new kind of data

73
02:58:23 --> 02:58:26
that's increasingly becoming
common in our daily lives.

74
02:58:28 --> 02:58:30
Fitness devices are everywhere now

75
02:58:31 --> 02:58:35
their sales have skyrocketed
in the last few years.

76
02:58:35 --> 02:58:40
They are in wristbands, watches, shoes and
vests, directly communicating with your

77
02:58:40 --> 02:58:45
personal mobile device, tracking several
activity variables like blood pressure,

78
02:58:45 --> 02:58:50
different types of activities,
blood glucose levels, etc at every moment.

79
02:58:50 --> 02:58:52
Their goal is to improve wellness.

80
02:58:52 --> 02:58:56
By having you monitor
your daily status and

81
02:58:56 --> 02:58:59
hopefully improve your
lifestyle to stay healthy.

82
02:58:59 --> 02:59:04
But the data they generate can be
very useful medical information

83
02:59:04 --> 02:59:08
because this data is about what
happens in your normal life and

84
02:59:08 --> 02:59:09
not just when you go to the doctor.

85
02:59:11 --> 02:59:13
How much data do they generate?

86
02:59:13 --> 02:59:17
The device called FitBit can
produce several gigabytes a day.

87
02:59:17 --> 02:59:22
Could this data be used to save healthcare
costs, effect a healthier lifestyle?

88
02:59:22 --> 02:59:23
That's a question mark.

89
02:59:25 --> 02:59:29
It's safe to guess that this data
alone wouldn't drive the dream of

90
02:59:29 --> 02:59:31
precision medicine.

91
02:59:31 --> 02:59:35
But what if we consider integrating
it with other sources of data

92
02:59:35 --> 02:59:38
like electronic health records or
a genomic profile?

93
02:59:39 --> 02:59:41
This remains an open question.

94
02:59:42 --> 02:59:46
This is an open arena for research that
my colleagues at scripts are doing.

95
02:59:46 --> 02:59:51
It's also a potentially significant area
for product and business development.

96
02:59:51 --> 02:59:54
Let's look at some examples of health
related data being generated by

97
02:59:54 --> 02:59:55
organizations.

98
02:59:56 --> 02:59:59
Many public databases
including those curated and

99
02:59:59 --> 03:00:03
managed by NCBI, the National Center for
Biotechnology Information,

100
03:00:03 --> 03:00:07
had been created to capture the basic
scientific data and knowledge for

101
03:00:07 --> 03:00:12
humans and other model organisms at
the different building blocks of life.

102
03:00:12 --> 03:00:17
These databases carry both experimental
and computed data that are necessary to

103
03:00:17 --> 03:00:20
observations for
unconquered diseases like cancer.

104
03:00:20 --> 03:00:24
In addition,
many have created knoweledge-bases

105
03:00:24 --> 03:00:28
like the Geneontology and
The Unified Medical Language System

106
03:00:28 --> 03:00:32
to assemble human knowledge in
a machine processable form.

107
03:00:32 --> 03:00:35
These are just a few examples of
organizational data sources and

108
03:00:35 --> 03:00:39
governmental data gathered by health
care systems around the world

109
03:00:39 --> 03:00:42
could also be used as a massive
source of information.

110
03:00:43 --> 03:00:46
But really some of
the most interesting and

111
03:00:46 --> 03:00:51
novel opportunities seem likely to come
from the area of people generated data.

112
03:00:52 --> 03:00:56
Mobile healths apps is an area
that is growing significantly.

113
03:00:56 --> 03:00:59
There are apps now to monitor heart rates,
blood pressure, and

114
03:00:59 --> 03:01:01
test oxygen saturation levels.

115
03:01:02 --> 03:01:06
Apps, we might say,
record data from sensors but

116
03:01:06 --> 03:01:09
are also obviously generated from people.

117
03:01:10 --> 03:01:14
But there is more people generated
data that's interesting beyond censure

118
03:01:15 --> 03:01:16
measurements.

119
03:01:16 --> 03:01:19
In 2015 the Webby People's Voice Award

120
03:01:19 --> 03:01:22
went to an app which supports
meditation and mindfulness.

121
03:01:23 --> 03:01:26
Rather than an electronic sensing device,

122
03:01:26 --> 03:01:31
a human would indicate how many
minutes per day they spent meditating.

123
03:01:31 --> 03:01:35
If they interact with the app
which reminds them to be mindful,

124
03:01:35 --> 03:01:39
then we have human generated behavior
that we couldn't get from a sensor.

125
03:01:40 --> 03:01:46
There are well over 100,000 health apps
today in either iTunes or Google Play.

126
03:01:46 --> 03:01:49
And by some estimates
the Mobile Health App market

127
03:01:49 --> 03:01:53
may be worth 27 billion dollars by 2017.

128
03:01:54 --> 03:01:59
So really we are just seen the beginning
of what data might be generated here

129
03:01:59 --> 03:02:05
from what's being called human sensors,
but to really understand where the power

130
03:02:05 --> 03:02:09
of people generated data might take us
in the era of big data for healthcare.

131
03:02:09 --> 03:02:12
Let's imagine how things stand now.

132
03:02:12 --> 03:02:15
In general,
a patient goes to see their doctor and

133
03:02:15 --> 03:02:19
maybe their doctor asks if they have had
any side effects from their medications.

134
03:02:21 --> 03:02:22
The accuracy and

135
03:02:22 --> 03:02:26
hence the quality of data patients provide
in this kind of setting is very low.

136
03:02:27 --> 03:02:29
Not that it's really the patients fault.

137
03:02:30 --> 03:02:33
It might have been days or
weeks ago that they experienced something.

138
03:02:33 --> 03:02:37
They may be unsure whether something
they experienced was actually a reaction

139
03:02:37 --> 03:02:39
to then report it.

140
03:02:39 --> 03:02:42
And there might be details about exactly
when they took a medication that

141
03:02:42 --> 03:02:44
are meaningful, but
they've forgotten it after the fact.

142
03:02:46 --> 03:02:52
Today, people are self reporting reactions
and experiences they are having.

143
03:02:52 --> 03:02:56
We're on Twitter, on blog sites,
online support groups,

144
03:02:56 --> 03:03:01
online data sharing services: these
are sources of data that we've

145
03:03:01 --> 03:03:05
never had before that can be used to
understand in a far more detailed and

146
03:03:05 --> 03:03:10
personal rate the impact of drug
integrations are responses to certain

147
03:03:11 --> 03:03:14
If applications were designed
to integrate doctor and

148
03:03:14 --> 03:03:19
hospital records with information
on when drugs were taken and

149
03:03:19 --> 03:03:24
then to further mine social media or
collect self reports from patients.

150
03:03:24 --> 03:03:27
Who knows what kinds of questions
we will be able to answer?

151
03:03:27 --> 03:03:29
Or new questions we may be able to ask?

1
05:56:54 --> 05:56:56
Where does big data come from?

2
05:56:56 --> 05:57:00
The first thing I would like to
say before talking about big data

3
05:57:00 --> 05:57:02
is that it is not new.

4
05:57:03 --> 05:57:07
Most of the big data sources existed
before, but the scale we use and

5
05:57:07 --> 05:57:10
apply them today has changed.

6
05:57:10 --> 05:57:14
Just look at this image of open
link data on the Internet.

7
05:57:14 --> 05:57:16
I thought this image was so cool.

8
05:57:16 --> 05:57:20
It shows not only there are so
many sources of data, but

9
05:57:20 --> 05:57:21
they're also connected.

10
05:57:22 --> 05:57:25
If you want to check it out yourself,

11
05:57:25 --> 05:57:28
we'll give you the link
at the end of this video.

12
05:57:28 --> 05:57:33
Big data is often boiled down to a few
varieties of data generated by machines,

13
05:57:33 --> 05:57:37
people, and organizations.

14
05:57:37 --> 05:57:42
With machine generated data we refer to
data generated from real time sensors in

15
05:57:42 --> 05:57:47
industrial machinery or vehicles that
logs that track user behavior online,

16
05:57:47 --> 05:57:48
environmental sensors or

17
05:57:48 --> 05:57:52
personal health trackers, and
many other sense data resources.

18
05:57:54 --> 05:57:58
The Large Hadron Collider
generates 40 terabytes of data

19
05:57:58 --> 05:58:01
every second during experiments.

20
05:58:01 --> 05:58:07
But human generated data, we refer to
the vast amount of social media data,

21
05:58:07 --> 05:58:11
status updates, tweets,
photos, and medias.

22
05:58:11 --> 05:58:16
With organizational generated data we
refer to more traditional types of data,

23
05:58:16 --> 05:58:20
including transaction
information in databases and

24
05:58:20 --> 05:58:24
structured data open
stored in data warehouses.

25
05:58:24 --> 05:58:28
Note that big data can be either
structured, semi-structured, or

26
05:58:28 --> 05:58:33
unstructured, which is a topic we will
talk about more later in this course.

27
05:58:34 --> 05:58:40
In most business use cases, any single
source of data on its own is not useful.

28
05:58:41 --> 05:58:46
Real value often comes from combining
these streams of big data sources

29
05:58:46 --> 05:58:51
with each other and
analyzing them to generate new insights,

30
05:58:51 --> 05:58:53
which then goes back into
being big data themselves.

31
05:58:54 --> 05:58:56
Once you have such insights,

32
05:58:56 --> 05:59:01
it then enables what we call data
enabled decisions and actions.

33
05:59:02 --> 05:59:06
Let's now look into these different
types of big data in more detail.

1
11:56:00 --> 11:56:03
Why is Big Data generated
by machines useful?

2
11:56:27 --> 11:56:30
Let's go back for
a second to our first example for

3
11:56:30 --> 11:56:33
machine generated big data planes.

4
11:56:34 --> 11:56:37
What is producing all
that data on the plane?

5
11:56:39 --> 11:56:42
If you look at some of
the sensors that contribute

6
11:56:42 --> 11:56:45
to the half terabyte of
data generated on a plane,

7
11:56:45 --> 11:56:51
we will find that some of it comes from
accelerometers that measure turbulence.

8
11:56:52 --> 11:56:57
There are also sensors built into
the engines for temperature, pressure,

9
11:56:57 --> 11:57:01
many other measurable factors
to detect engine malfunctions.

10
11:57:01 --> 11:57:06
Constant real-time analysis of all
the data collected provides help

11
11:57:06 --> 11:57:10
monitoring and
problem detection at 40,000 feet.

12
11:57:10 --> 11:57:14
That's approximately 12,000
meters above ground.

13
11:57:16 --> 11:57:21
We call this type of
analytical processing in-situ.

14
11:57:21 --> 11:57:26
Previously, in traditional relational
database management systems,

15
11:57:26 --> 11:57:31
data was often moved to
computational space for processing.

16
11:57:31 --> 11:57:36
In Big Data space In-Situ
means bringing the computation

17
11:57:36 --> 11:57:39
to where data is located or,
in this case, generated.

18
11:57:42 --> 11:57:46
A key feature of these types
of real-time notifications is

19
11:57:46 --> 11:57:50
that they enable real-time actions.

20
11:57:50 --> 11:57:54
However, using such a capability
would require you to approach your

21
11:57:54 --> 11:57:57
application and your work differently.

22
11:57:59 --> 11:58:04
If you are using an activity tracker, you
should probably come up with a strategy

23
11:58:04 --> 11:58:08
for how you will incorporate the usage of
these useful gadgets into your lifestyle.

24
11:58:09 --> 11:58:14
Just like that, if you're planning to
incorporate Big Data driven insights into

25
11:58:14 --> 11:58:20
your organization, you need to define
a new strategy, and a new way of working.

26
11:58:22 --> 11:58:28
Most Big Data centric businesses have
updated their culture to be more real-time

27
11:58:28 --> 11:58:33
action oriented, refining real-time
processes to handle anything from customer

28
11:58:33 --> 11:58:37
relations and fraud detection,
to system monitoring and control.

29
11:58:38 --> 11:58:43
In addition, such volumes of real-time
data and analytical operations that need

30
11:58:43 --> 11:58:48
to take place requires an increased
use of scalable computing systems,

31
11:58:48 --> 11:58:52
which need to be a part of the planning
for an organizational Big Data strategy.

32
11:58:54 --> 11:58:58
They see affects of such
changes also in SCADA system.

33
11:58:59 --> 11:59:03
SCADA stands for Supervisory Control and
Data Acquisition.

34
11:59:05 --> 11:59:10
SCADA is a type of industrial control
system for remote monitoring and

35
11:59:10 --> 11:59:16
control of industrial processes
that exists in the physical world,

36
11:59:16 --> 11:59:20
potentially including multiple sites,
many types of sensors.

37
11:59:22 --> 11:59:27
In addition to monitoring and control,
SCADA system can be used to define

38
11:59:27 --> 11:59:32
actions for reduced waste and improved
efficiency in industrial processes,

39
11:59:32 --> 11:59:38
including those of manufacturing and
power generation, public or

40
11:59:38 --> 11:59:42
private infrastructure processes,
including water treatment, oil, and

41
11:59:42 --> 11:59:48
gas pipelines, and
electrical power transmission, and

42
11:59:48 --> 11:59:53
facility processes including buildings,
airports, ships, and space stations.

43
11:59:54 --> 11:59:59
They can even be used in smart
building applications to monitor and

44
11:59:59 --> 12:00:04
control heating, ventilation,
air conditioning systems like HVAC,

45
12:00:04 --> 12:00:07
access, and energy consumption.

46
12:00:08 --> 12:00:13
Again, the management of these processes
once the trends, patterns, and

47
12:00:13 --> 12:00:19
anomalies are identified in real-time
needs to be decided in the Big Data case.

48
12:00:19 --> 12:00:25
As a summary, as the largest and fastest
type of Big Data, machine generated

49
12:00:25 --> 12:00:30
data can uniquely enable real-time
actions in many systems and processes.

50
12:00:31 --> 12:00:39
However, a culture shift is needed for
its computing and real-time action.

1
23:56:38 --> 23:56:40
Big data generated by machines.

2
23:56:41 --> 23:56:43
It's everywhere and there's a lot.

3
23:56:54 --> 23:56:57
Do big planes require big data?

4
23:56:57 --> 23:56:58
Absolutely!

5
23:56:58 --> 23:57:02
Did you know that a Boeing 787 produces

6
23:57:02 --> 23:57:05
half a terabyte of data
every time it flies?

7
23:57:06 --> 23:57:11
Really, almost every part of
the plane updates both the flight and

8
23:57:11 --> 23:57:13
the ground team about
its status constantly.

9
23:57:15 --> 23:57:16
Where's all this data coming from?

10
23:57:17 --> 23:57:21
This is an example of machine-generated
data coming from sensors.

11
23:57:22 --> 23:57:25
If you look at all sources of big data,

12
23:57:25 --> 23:57:29
machine data is the largest
source of big data.

13
23:57:29 --> 23:57:32
Additionally, it is very complex.

14
23:57:32 --> 23:57:38
In general, we call machines that provide
some type of sensing capability smart.

15
23:57:38 --> 23:57:42
Have you ever wondered why you
call your cell phone a smartphone?

16
23:57:43 --> 23:57:46
Because it gives you a way
to track many things,

17
23:57:46 --> 23:57:50
including your geolocation, and
connect you to other things.

18
23:57:51 --> 23:57:54
So what makes a smart device smart, smart?

19
23:57:54 --> 23:58:00
Generally speaking, There are three
main properties of smart devices

20
23:58:00 --> 23:58:05
based on what they do with sensors and
things they encapsulate.

21
23:58:05 --> 23:58:07
They can connect to other devices or

22
23:58:07 --> 23:58:13
networks, they can execute services and
collect data autonomously,

23
23:58:13 --> 23:58:18
that means on their own, they have
some knowledge of the environment.

24
23:58:19 --> 23:58:24
The widespread availability of the smart
devices and their interconnectivity

25
23:58:24 --> 23:58:31
led to a new term being coined,
The Internet of Things.

26
23:58:31 --> 23:58:36
Think of a world of smart devices at home,
in your car, in the office,

27
23:58:36 --> 23:58:42
city, remote rural areas,
the sky, even the ocean,

28
23:58:42 --> 23:58:45
all connected and all generating data.

29
23:58:46 --> 23:58:50
Let's look at an example of a device
that has some of these things in it.

30
23:58:52 --> 23:58:57
An activity tracker is a device or
application for monitoring and

31
23:58:57 --> 23:59:03
tracking fitness-related metrics
such as distance walked or run,

32
23:59:03 --> 23:59:10
calorie consumption, and in some cases,
heartbeat and quality of sleep.

33
23:59:10 --> 23:59:15
What if everyone in New York City
wore an activity tracker?

34
23:59:15 --> 23:59:18
What if everyone wore several?

35
23:59:18 --> 23:59:23
I personally have three activity
trackers that I use on a daily basis.

36
23:59:23 --> 23:59:28
One to track my sleep, another one
to track my physical activity, and

37
23:59:28 --> 23:59:32
a third, my smartphone,
that goes everywhere with me.

38
23:59:32 --> 23:59:38
So it is not that unusual to imagine that
this will be the case for many people.

39
23:59:38 --> 23:59:43
As you have already heard from in
a previous lecture on personalized data,

40
23:59:43 --> 23:59:46
such activity trackers
have enabled a new way

41
23:59:46 --> 23:59:50
of doing patient intervention
via personalized medicine.

42
23:59:51 --> 23:59:56
Similarly, the sensors in planes have
generated a new way of looking at

43
23:59:56 --> 23:59:59
fleet management and flight safety.

44
23:59:59 --> 00:00:04
As a summary,
machines collect data 24/7 via

45
00:00:04 --> 00:00:09
their built-in sensors,
both at personal and industrial scales.

46
00:00:09 --> 00:00:13
And thus, they are the largest
of all the big data sources.

1
23:56:52 --> 23:56:53
Organization-Generated Data.

2
23:56:53 --> 23:57:00
Benefits come from combining
with other data types.

3
23:57:00 --> 23:57:03
How are some organizations
benefiting from big data?

4
23:57:20 --> 23:57:22
Let's look at real world examples to
see the advantages these organizations

5
23:57:22 --> 23:57:23
are getting out of big data.

6
23:57:24 --> 23:57:27
One of these companies is UPS.

7
23:57:28 --> 23:57:32
UPS delivers 16 million shipments per day.

8
23:57:32 --> 23:57:37
They get around 40 million
tracking requests.

9
23:57:37 --> 23:57:37
That's huge.

10
23:57:39 --> 23:57:45
An estimate of how much data UPS has
on its operations is 16 petabytes.

11
23:57:46 --> 23:57:51
Can you guess how much money UPS can
save by reducing each driver's route

12
23:57:51 --> 23:57:52
by just one mile?

13
23:57:54 --> 23:58:00
If they can reduce distance traveled
by each truck by even one mile,

14
23:58:00 --> 23:58:04
UPS can save a whopping $50 million U.S.
per year.

15
23:58:06 --> 23:58:10
This is where big data steps in.

16
23:58:10 --> 23:58:13
Utilizing complex optimization
over large datasets

17
23:58:13 --> 23:58:18
can lead to route optimizations that were
previously not visible to the company.

18
23:58:20 --> 23:58:23
Big data, together with smart processing,

19
23:58:23 --> 23:58:27
enables UPS to manage thousands
of route optimizations.

20
23:58:28 --> 23:58:34
Let's travel from package
delivery to the retail domain.

21
23:58:34 --> 23:58:37
An organization from
the retail shopping domain

22
23:58:37 --> 23:58:40
that heavily utilizes big data is Walmart.

23
23:58:42 --> 23:58:49
Walmart is a big organization that gets
250 million customers in 10,000 stores.

24
23:58:51 --> 23:58:58
Did you know they collect 2.5
petabytes of data per hour?

25
23:59:00 --> 23:59:05
They collect data on Twitter tweets,
local events, local weather,

26
23:59:05 --> 23:59:10
in-store purchases, online clicks and

27
23:59:10 --> 23:59:14
many other sales, customer and
product related data.

28
23:59:16 --> 23:59:22
They use this data to find patterns
such as which products are frequently

29
23:59:22 --> 23:59:27
purchased together, and what is the best
new product to introduce in their stores,

30
23:59:27 --> 23:59:32
to predict demand at
the particular location,

31
23:59:34 --> 23:59:38
and to customize customer recommendations.

32
23:59:38 --> 23:59:41
Overall, by leveraging big data and

33
23:59:41 --> 23:59:46
analytics, Walmart has maintained
its position as a top retailer.

34
23:59:47 --> 23:59:53
UPS and Walmart examples were just two out
of a number of companies using big data.

35
23:59:54 --> 23:59:58
Big data is producing results for
companies in all sectors.

36
00:00:00 --> 00:00:05
Studies forecast spending on
big data technologies to go up

37
00:00:05 --> 00:00:07
drastically in the next five years.

38
00:00:09 --> 00:00:16
A study by Bane and Company
suggests that early adopters of big

39
00:00:16 --> 00:00:21
data analytics have gained a significant
lead over the rest of the corporate world.

40
00:00:23 --> 00:00:25
For graphics referenced here,

41
00:00:26 --> 00:00:31
we see that companies that use
analytics are twice as likely

42
00:00:31 --> 00:00:36
to be in the top quartile of financial
performance within their industries.

43
00:00:37 --> 00:00:42
Five times as likely to make decisions
much faster than market peers.

44
00:00:43 --> 00:00:47
Three times as likely to
execute decisions as intended.

45
00:00:47 --> 00:00:53
And twice as likely to use data very
frequently when making decisions.

46
00:00:55 --> 00:00:57
This points to the growth and
demand of people and

47
00:00:57 --> 00:01:03
technology centered around or
specializing in big data applications.

48
00:01:03 --> 00:01:08
As a summary, organizations are gaining
significant benefit from integrating

49
00:01:08 --> 00:01:13
big data practices into their culture and
breaking their silos.

50
00:01:13 --> 00:01:19
Some major benefits to organizations are
operational efficiency, improved marketing

51
00:01:19 --> 00:01:23
outcomes, higher profits, and
improved customer satisfaction.

1
23:58:14 --> 23:58:20
Big data, generated by organizations,
structured but often siloed.

2
23:58:20 --> 23:58:25
The last type of big data we will discuss
is big data generated by organizations.

3
23:58:27 --> 23:58:32
This type of data is the closest to
what most businesses currently have.

4
23:58:32 --> 23:58:35
But it's considered a bit out of fashion,
or

5
23:58:35 --> 23:58:39
traditional, compared to
other types of big data.

6
23:58:39 --> 23:58:43
However, it is at least as important
as other types of big data.

7
23:59:01 --> 23:59:05
So how do organizations produce data?

8
23:59:05 --> 23:59:08
The answer to how
an organization generates data

9
23:59:08 --> 23:59:11
is very unique to the organization and
context.

10
23:59:12 --> 23:59:16
Each organization has distinct
operation practices and

11
23:59:16 --> 23:59:21
business models, which result in
a variety of data generation platforms.

12
23:59:21 --> 23:59:26
For example, the type and source of data
that a bank gets is very different from

13
23:59:26 --> 23:59:29
what a hardware equipment
manufacturer gets.

14
23:59:31 --> 23:59:37
Some common types of organizational big
data come from commercial transactions,

15
23:59:37 --> 23:59:43
credit cards, government institutions,
e-commerce, banking or

16
23:59:43 --> 23:59:49
stock records, medical records, sensors,
transactions, clicks and so on.

17
23:59:50 --> 23:59:53
Almost every event can
be potentially stored.

18
23:59:55 --> 23:59:59
Organizations store this data for
current and

19
23:59:59 --> 00:00:03
future use, as well as for
analysis of the past.

20
00:00:04 --> 00:00:08
Let's say you're an organization
that collects sales transactions.

21
00:00:08 --> 00:00:13
You can use this data for pattern
recognition to detect correlated products,

22
00:00:14 --> 00:00:19
to estimate demand for
products likely to go up in sales, and

23
00:00:19 --> 00:00:21
capture fraudulent activity.

24
00:00:22 --> 00:00:28
Moreover, when you know your
sales record and can correlate it

25
00:00:28 --> 00:00:34
with your marketing records, you can find
which campaigns really made an impact.

26
00:00:34 --> 00:00:38
You are already becoming
a data savvy organization.

27
00:00:38 --> 00:00:43
Now think of bringing your
sales data together with other

28
00:00:43 --> 00:00:48
external open public data,
such as major world events in the news.

29
00:00:48 --> 00:00:52
You can ask, was it savvy marketing or

30
00:00:52 --> 00:00:57
a consequence of external events
that triggered your sales?

31
00:00:57 --> 00:00:59
Using proper analytics,

32
00:00:59 --> 00:01:04
you can now build inventories to match
your predicted growth and demand.

33
00:01:04 --> 00:01:11
In addition, organizations build and
apply processes to record and

34
00:01:11 --> 00:01:16
monitor business events of interest,
such as registering a customer,

35
00:01:16 --> 00:01:19
manufacturing a product or
taking an order.

36
00:01:19 --> 00:01:24
These processes collect
highly structured data

37
00:01:24 --> 00:01:28
that include transactions,
reference tables, and

38
00:01:28 --> 00:01:33
relationships, as well as
the metadata that sets its context.

39
00:01:35 --> 00:01:40
Usually, structured data is stored in
relational database management systems.

40
00:01:42 --> 00:01:45
However, we call any data
that is in the form of

41
00:01:45 --> 00:01:50
a record located in a fixed field or
file structured data.

42
00:01:50 --> 00:01:54
This definition also
includes spreadsheets.

43
00:01:55 --> 00:02:00
As I have mentioned before,
traditionally this type of

44
00:02:00 --> 00:02:05
highly structured data is the vast
majority of what IT managed and

45
00:02:05 --> 00:02:09
processed in both operational and
business intelligence systems.

46
00:02:10 --> 00:02:14
Let's look at the sales transaction
data in our previous example.

47
00:02:15 --> 00:02:19
If you look at the data in
the relational table on the right.

48
00:02:19 --> 00:02:23
As the name structured suggests,

49
00:02:23 --> 00:02:28
the table is organized to store data using

50
00:02:28 --> 00:02:32
a structure defined by a model.

51
00:02:32 --> 00:02:38
Each column is tagged to tell us what
data that column intends to store.

52
00:02:38 --> 00:02:40
This is what we call a data model.

53
00:02:42 --> 00:02:45
A data model defines
each of these columns and

54
00:02:45 --> 00:02:51
fields in the table, and
defines relationships between them.

55
00:02:51 --> 00:02:57
If you look at the product ID column,
you will see that it includes only

56
00:02:57 --> 00:03:02
identifiers that can potentially be linked
to another table defining these products.

57
00:03:04 --> 00:03:09
The ability to define such relationships,
easily make structured data,

58
00:03:09 --> 00:03:15
or in this case relational databases,
highly adopted by many organizations.

59
00:03:16 --> 00:03:20
There are commonly used
languages like SQL,

60
00:03:20 --> 00:03:25
the Structured Query Language, to extract
data of interest from such tables.

61
00:03:26 --> 00:03:29
This is referred to as querying the data.

62
00:03:31 --> 00:03:37
However, it could even be a challenge
to integrate such structured data.

63
00:03:37 --> 00:03:42
This image shows us a continuum
of technologies to model,

64
00:03:42 --> 00:03:46
collect and query unstructured
data coming from software and

65
00:03:46 --> 00:03:49
hardware components
within an organization.

66
00:03:50 --> 00:03:55
In the past, such challenges
led to information being stored

67
00:03:55 --> 00:03:59
in what we call silos,
even within an organization.

68
00:04:00 --> 00:04:05
Many organizations have traditionally
captured data at the department level,

69
00:04:05 --> 00:04:10
without proper infrastructure and
policy to share and integrate this data.

70
00:04:10 --> 00:04:15
This has hindered the growth of
scalable pattern recognition

71
00:04:15 --> 00:04:18
to the benefits of
the entire organization.

72
00:04:18 --> 00:04:22
Because no one system has access to
all data that the organization owns.

73
00:04:24 --> 00:04:28
Each data set is compartmentalized.

74
00:04:28 --> 00:04:34
If such silos are left untouched,
organizations risk having outdated,

75
00:04:34 --> 00:04:38
unsynchronized, and
even invisible data sets.

76
00:04:39 --> 00:04:43
Organizations are realizing
the detrimental outcomes of this rigid

77
00:04:43 --> 00:04:44
structure.

78
00:04:44 --> 00:04:50
And changing policies and infrastructure
to enable integrated processing of

79
00:04:50 --> 00:04:53
all data to the entire
organization's benefit.

80
00:04:53 --> 00:04:58
Cloud-based solutions
are seen as agile and

81
00:04:58 --> 00:05:01
low capital intensive
solutions in this area.

82
00:05:01 --> 00:05:07
As a summary, while highly structured
organizational data is very useful and

83
00:05:07 --> 00:05:11
trustworthy, and
thus a valuable source of information,

84
00:05:11 --> 00:05:15
organizations must pay special
attention to breaking up

85
00:05:15 --> 00:05:20
the silos of information to
make full use of its potential.

1
00:03:34 --> 00:03:37
The key, integrating diverse data.

2
00:03:47 --> 00:03:50
Whatever your big data application is, and

3
00:03:50 --> 00:03:55
the types of big data you are using
the real value will come from integrating

4
00:03:55 --> 00:04:00
different types of data sources,
and analyzing them at scale.

5
00:04:00 --> 00:04:02
So how do we start getting this value?

6
00:04:02 --> 00:04:04
Sometimes, all it takes,

7
00:04:04 --> 00:04:09
is looking at the data you already
collect in a different way.

8
00:04:09 --> 00:04:13
And it can mean a big difference
in your return on investment.

9
00:04:14 --> 00:04:19
This new story from June 2015
mentions that Carnival Cruises

10
00:04:19 --> 00:04:24
is using structured and unstructured
data from a variety of sources.

11
00:04:26 --> 00:04:29
Carnival turns it into profit

12
00:04:29 --> 00:04:32
using price optimization
techniques on the integrated data.

13
00:04:33 --> 00:04:36
For you to achieve such a success story,

14
00:04:36 --> 00:04:41
you will need to include data
integration into your big data practice.

15
00:04:41 --> 00:04:45
However, there are some unique challenges
when attempting to integrate these

16
00:04:45 --> 00:04:50
diverse data sources and
scaling the solutions.

17
00:04:50 --> 00:04:54
Course Two on on the specialization,
we'll teach you more about these issues.

18
00:04:55 --> 00:05:00
But let's take a moment to define why
effect of data integration is useful?

19
00:05:01 --> 00:05:07
Data integration means bringing
together data from diverse sources and

20
00:05:07 --> 00:05:11
turning them into coherent and
more useful information.

21
00:05:12 --> 00:05:14
We also call this knowledge.

22
00:05:15 --> 00:05:21
The main objective here is taming or
more technically managing data and

23
00:05:21 --> 00:05:26
turning it into something you can
make use of programmatically.

24
00:05:26 --> 00:05:31
A data integration process
involves many parts.

25
00:05:31 --> 00:05:37
It starts with discovering,
accessing, and monitoring data and

26
00:05:37 --> 00:05:43
continues with modeling and transforming
data from a variety of sources.

27
00:05:43 --> 00:05:47
But why do we need data
integration in the first place?

28
00:05:47 --> 00:05:50
Let's start by focusing on
differences between big data sets

29
00:05:50 --> 00:05:52
coming from different sources.

30
00:05:53 --> 00:05:58
You might have flat file formatted data,
relational database data,

31
00:05:58 --> 00:06:05
data encoded in XML or JSON,
both common for internet generated data.

32
00:06:05 --> 00:06:09
These different formats and
models are useful

33
00:06:09 --> 00:06:12
because they are designed to express
different data in unique ways.

34
00:06:13 --> 00:06:16
In a way, different data formats and

35
00:06:16 --> 00:06:23
models make big data more useful and
more challenging all at the same time.

36
00:06:23 --> 00:06:27
When you integrate data in different
formats, you make the final

37
00:06:27 --> 00:06:32
product richer in the number of
features you describe the data with.

38
00:06:32 --> 00:06:37
For example,
by integrating environmental sensor and

39
00:06:37 --> 00:06:42
camera data with geographical
information system data, such as in

40
00:06:42 --> 00:06:48
my wildfire prediction application,
I can use the spacial data capabilities

41
00:06:48 --> 00:06:53
with non-spacial data to more
accurately run fire simulations.

42
00:06:55 --> 00:06:59
In the past, although we were able
to see the images of the fire

43
00:07:01 --> 00:07:05
from mountain top cameras,
just like this image, we were still

44
00:07:05 --> 00:07:09
not able to tell what the exact
location of the fire is automatically.

45
00:07:11 --> 00:07:15
Now, when a fire is detected
from a mountain top camera,

46
00:07:16 --> 00:07:21
viewsheds are used to estimate
the location of the fire.

47
00:07:23 --> 00:07:28
This location information can
be fed into the fire simulator

48
00:07:28 --> 00:07:32
as early as it's detected
to predict the size and

49
00:07:32 --> 00:07:36
location of the fire in the next
hour more accurately and faster.

50
00:07:37 --> 00:07:42
Similarly, I can use real time
data with eye curve data sets, and

51
00:07:42 --> 00:07:43
use them all together.

52
00:07:44 --> 00:07:49
Additionally, by bringing
the data together, and

53
00:07:49 --> 00:07:56
providing programmable access to it, I'm
now making each data set more accessible.

54
00:07:57 --> 00:08:02
Moreover, integration of
diverse datasets significantly

55
00:08:02 --> 00:08:06
reduces the overall data complexity
in my data-driven product.

56
00:08:07 --> 00:08:13
The data becomes more available for
use and unified as a system of its own.

57
00:08:14 --> 00:08:19
One advantage of such an integration
is not often mentioned.

58
00:08:20 --> 00:08:23
Such a streamlined and
integrated data system

59
00:08:23 --> 00:08:27
can increase the collaboration between
different parts of your data systems.

60
00:08:28 --> 00:08:32
Each part can now clearly see

61
00:08:32 --> 00:08:36
how their data is integrated
into the overall system.

62
00:08:36 --> 00:08:41
Including the user scenarios and the
security and privacy processes around it.

63
00:08:43 --> 00:08:50
Overall by integrating diverse data
streams you add value to your big data and

64
00:08:50 --> 00:08:54
improve your business even
before you start analyzing it.

65
00:08:55 --> 00:08:59
Next, we will focus on the dimensions
of the scalability and

66
00:08:59 --> 00:09:03
discuss how we can start tackling
some of these challenges.

1
00:12:38 --> 00:12:40
What launched the big data era?

2
00:12:41 --> 00:12:46
In this video, you will learn about
two new opportunities that have

3
00:12:46 --> 00:12:49
contributed to the launch
of the big data era.

4
00:13:04 --> 00:13:09
Opportunities are often
a signal of changing times.

5
00:13:11 --> 00:13:17
In 2013, an influential report by
a company called McKinsey claimed that

6
00:13:17 --> 00:13:22
the area of data science will be the
number one catalyst for economic growth.

7
00:13:24 --> 00:13:27
McKinsey identified one of our new

8
00:13:27 --> 00:13:31
opportunities that contributed to
the launch of the big data era.

9
00:13:31 --> 00:13:36
A growing torrent of data.

10
00:13:36 --> 00:13:42
This refers to the idea that data seems to
be coming continuously and at a fast rate.

11
00:13:43 --> 00:13:48
Think about this,
today you can buy a hard drive to store

12
00:13:48 --> 00:13:52
all the music in the world for only $600.

13
00:13:53 --> 00:14:01
That's an amazing storage capability over
any previous forms of music storage.

14
00:14:01 --> 00:14:06
In 2010 there were 5 billion
mobile phones in use.

15
00:14:06 --> 00:14:10
You can be sure that
there are more today and

16
00:14:10 --> 00:14:14
as I'm sure you will understand,
these phones and

17
00:14:14 --> 00:14:19
the apps we install on them
are a big source of big data,

18
00:14:19 --> 00:14:24
which all the time, every day,
contributes to our core.

19
00:14:24 --> 00:14:27
And Facebook, which recently just set

20
00:14:27 --> 00:14:32
a record of having one billion
people login in a single day,

21
00:14:32 --> 00:14:37
has more that 30 billion pieces
of content shared every month.

22
00:14:38 --> 00:14:40
Well, that number's from 2013.

23
00:14:40 --> 00:14:44
So I'm sure that it's much
higher than that now.

24
00:14:45 --> 00:14:48
Does it make you think how many
Facebook share's you made last month?

25
00:14:50 --> 00:14:54
All this leads to projections
of serious growth.

26
00:14:54 --> 00:15:01
40% in global data per year,
and 5% in global IT spending.

27
00:15:01 --> 00:15:06
This much data has sure
pushed the data science field

28
00:15:06 --> 00:15:09
to start remaining itself and
the business world of today.

29
00:15:10 --> 00:15:15
But, there's something else contributing
to the catalyzing power of data science.

30
00:15:15 --> 00:15:18
It is called cloud computing.

31
00:15:18 --> 00:15:21
We call this on demand computing.

32
00:15:21 --> 00:15:25
Cloud computing is one of
the ways in which computing

33
00:15:25 --> 00:15:30
has now become something that
we ca do anytime, and anywhere.

34
00:15:30 --> 00:15:33
You may be surprised to know
that some of your favorite

35
00:15:33 --> 00:15:36
apps are from businesses
being run from coffee shops.

36
00:15:36 --> 00:15:41
This new ability,
combined with our torrent of data,

37
00:15:41 --> 00:15:47
gives us the opportunity to perform novel,
dynamic and scalable data analysis,

38
00:15:47 --> 00:15:50
to tell us new things about our world and
ourself.

39
00:15:50 --> 00:15:57
To summarize, a new torrent of big data
combined with computing capability

40
00:15:57 --> 00:16:02
anytime, anywhere has been at the core
of the launch of the big data era.

1
00:28:40 --> 00:28:41
Asking the Right Questions.

2
00:28:56 --> 00:29:02
The first step in any process is to define
what it is you are trying to tackle.

3
00:29:04 --> 00:29:06
What is the problem that
needs to be addressed, or

4
00:29:06 --> 00:29:09
the opportunity that
needs to be ascertained.

5
00:29:10 --> 00:29:14
Without this,
you won't have a clear goal in mind, or

6
00:29:14 --> 00:29:17
know when you've solved your problem.

7
00:29:17 --> 00:29:21
An example question is,
how can sales figures and

8
00:29:21 --> 00:29:27
call center logs be combined
to evaluate a new product,

9
00:29:27 --> 00:29:32
or in a manufacturing process,
how can data from multiple

10
00:29:32 --> 00:29:38
sensors in an instrument be used
to detect instrument failure?

11
00:29:38 --> 00:29:41
How can we understand our customers and

12
00:29:41 --> 00:29:46
market better to achieve
effective target marketing?

13
00:29:46 --> 00:29:50
Next you need to assess the situation
with respect to the problem or

14
00:29:50 --> 00:29:53
the opportunity you have defined.

15
00:29:53 --> 00:29:59
This is a step where you need to
exercise caution analyzing risks,

16
00:29:59 --> 00:30:02
costs, benefits, contingencies,

17
00:30:02 --> 00:30:07
regulations, resources and
requirements of the situation.

18
00:30:07 --> 00:30:09
What are the requirements of the problem?

19
00:30:09 --> 00:30:12
What are the assumptions and constraints?

20
00:30:12 --> 00:30:14
What resources are available?

21
00:30:14 --> 00:30:18
This is in terms of both personnel and
capital,

22
00:30:18 --> 00:30:22
such as computer systems, instruments etc.

23
00:30:22 --> 00:30:26
What are the main costs
associated with this project?

24
00:30:26 --> 00:30:28
What are the potential benefits?

25
00:30:28 --> 00:30:31
What risks are there in
pursuing the project?

26
00:30:31 --> 00:30:36
What are the contingencies to
potential risks, and so on?

27
00:30:36 --> 00:30:41
Answers to these questions will help you
get a better overview of the situation.

28
00:30:41 --> 00:30:44
And better understanding of
what the project involves.

29
00:30:44 --> 00:30:49
Then you need to define your goals and
objectives,

30
00:30:49 --> 00:30:52
based on the answers to these questions.

31
00:30:52 --> 00:30:56
Defining success criteria
is also very important.

32
00:30:56 --> 00:30:59
What do you hope to achieve
by the end of this project?

33
00:30:59 --> 00:31:01
Having clear goals and and

34
00:31:01 --> 00:31:07
success criteria will help you to assess
the project throughout its life cycle.

35
00:31:07 --> 00:31:11
Once you know the problem you want to
address and understand the constraints and

36
00:31:11 --> 00:31:16
goals, then you can formulate
the plan to come up with the answer,

37
00:31:16 --> 00:31:20
that is the solution to
your business problem.

38
00:31:20 --> 00:31:25
As a summary, defining the questions
you're looking to find answers for

39
00:31:25 --> 00:31:30
is a huge factor contributing to
the success of a data science project.

40
00:31:31 --> 00:31:35
By following the explained set of steps,
you can formulate better questions

41
00:31:35 --> 00:31:40
to solve using analytical skills and
link them to business value.

1
01:00:22 --> 01:00:23
Building a Big Data Strategy.

2
01:00:42 --> 01:00:48
Before we focus on big data strategy,
let's look at what strategy means.

3
01:00:49 --> 01:00:52
Although it is associated
with a military term,

4
01:00:53 --> 01:00:58
a dictionary search on strategy shows
the meaning as a plan of action or

5
01:00:58 --> 01:01:02
policy designed to achieve a major or
overall aim.

6
01:01:05 --> 01:01:10
This definition calls out the four major
parts that need to be in any strategy.

7
01:01:10 --> 01:01:17
Namely, aim, policy, plan, and action.

8
01:01:18 --> 01:01:21
Now, we are talking about
a big data strategy.

9
01:01:21 --> 01:01:24
So what do these four terms mean for us?

10
01:01:24 --> 01:01:29
When building our big data strategy,
we look at what we have,

11
01:01:29 --> 01:01:34
what high level goals we want to achieve,
what we need to do to get there,

12
01:01:34 --> 01:01:39
and what are the policies around
data from the beginning to the end.

13
01:01:43 --> 01:01:48
A big data strategy starts
with big objectives.

14
01:01:48 --> 01:01:52
Notice that I didn't say it
starts with collecting data

15
01:01:52 --> 01:01:58
because in this activity we
are really trying to identify what

16
01:01:58 --> 01:02:03
data is useful and
why by focusing on what data to collect.

17
01:02:03 --> 01:02:07
Every organization or team is unique.

18
01:02:07 --> 01:02:10
Different projects have
different objectives.

19
01:02:10 --> 01:02:14
Hence, it's important to first
define what your team's goals are.

20
01:02:14 --> 01:02:19
Have you ever had the scenario where you
see the temperature on the weather report

21
01:02:19 --> 01:02:22
and someone else highlights
the humidity instead?

22
01:02:23 --> 01:02:26
To find problems relevant to solve and

23
01:02:26 --> 01:02:32
data related to it, it might be
useful to start with your objectives.

24
01:02:32 --> 01:02:34
Once you define these objectives, or

25
01:02:34 --> 01:02:40
more generally speaking, questions to turn
big data into advantage for your business,

26
01:02:40 --> 01:02:44
you can look at what you have and
analyze the gaps and actions to get there.

27
01:02:45 --> 01:02:49
It is important to focus
on both short term and

28
01:02:49 --> 01:02:52
long term objectives in this activity.

29
01:02:52 --> 01:02:56
These objectives should also be linked
to big data analytics with business

30
01:02:56 --> 01:02:57
objectives.

31
01:02:57 --> 01:03:02
To make the best use of big data,
each company needs to evaluate how

32
01:03:02 --> 01:03:07
data science or big data analytics would
add value to their business objectives.

33
01:03:09 --> 01:03:12
Once you have established that
analytics can help your business,

34
01:03:12 --> 01:03:16
you need to create
a culture to embrace it.

35
01:03:16 --> 01:03:18
The first and foremost ingredient for

36
01:03:18 --> 01:03:23
a successful data science program
is organizational buy-in.

37
01:03:23 --> 01:03:26
A big data strategy must
have commitment and

38
01:03:26 --> 01:03:30
sponsorship from the company's leadership.

39
01:03:30 --> 01:03:35
Goals for using big data analytics should
be developed with all stakeholders and

40
01:03:35 --> 01:03:39
clearly communicated to
everyone in the organization.

41
01:03:39 --> 01:03:43
So that its value is understood and
appreciated by all.

42
01:03:46 --> 01:03:49
The next step is to build
your data science team.

43
01:03:51 --> 01:03:55
A diverse team with data scientists,
information technologists,

44
01:03:55 --> 01:04:01
application developers, and business
owners is necessary to be effective.

45
01:04:01 --> 01:04:05
As well as the mentality that everyone
works together as partners with

46
01:04:05 --> 01:04:06
common goals.

47
01:04:06 --> 01:04:07
Remember, one for all.

48
01:04:09 --> 01:04:13
No one is a customer or
service provider of another.

49
01:04:13 --> 01:04:18
Rather, everyone works together and
delivers as a team.

50
01:04:21 --> 01:04:24
Since big data is a team game,
and multi-disciplinary,

51
01:04:24 --> 01:04:29
a big part of a big data
strategy is constant training

52
01:04:29 --> 01:04:32
of team members on new big data tools and
analytics.

53
01:04:32 --> 01:04:36
As well as business practices and
objectives.

54
01:04:36 --> 01:04:41
This becomes even more critical if your
business depends on deep expertise

55
01:04:41 --> 01:04:46
on one or more subject areas with subject
matter experts working on problems,

56
01:04:46 --> 01:04:47
utilizing big data.

57
01:04:50 --> 01:04:56
Such businesses might have subject matter
experts who can be trained to add big

58
01:04:56 --> 01:05:01
data skills, and provide more value added
support than a newcomer would have.

59
01:05:01 --> 01:05:06
Similarly, any project member would
be trained to understand what

60
01:05:06 --> 01:05:09
the business objectives and
products are, and how he or

61
01:05:09 --> 01:05:14
she can utilize big data to improve those
objectives using his or her skills.

62
01:05:17 --> 01:05:23
Many organizations might benefit by having
a small data science team whose main job

63
01:05:23 --> 01:05:29
is do data experiments and test new ideas
before they get deployed at full scale.

64
01:05:31 --> 01:05:34
They might come up with
a new idea themselves

65
01:05:34 --> 01:05:35
based on the analysis they perform.

66
01:05:37 --> 01:05:39
They take more research level role.

67
01:05:40 --> 01:05:45
However, their findings can drastically
shape your business strategy

68
01:05:45 --> 01:05:46
almost on a daily basis.

69
01:05:48 --> 01:05:51
The impact of such teams
becomes evident over time

70
01:05:51 --> 01:05:55
as other parts of your organization starts
to see the results of their finding and

71
01:05:55 --> 01:05:57
analysis affecting their strategies.

72
01:05:59 --> 01:06:04
They become strategic partners of
all verticals in your business.

73
01:06:04 --> 01:06:06
Once you see that something works,

74
01:06:06 --> 01:06:11
you can start collecting more data to see
similar results at organizational scale.

75
01:06:14 --> 01:06:19
Since data is key to any big
data initiative, it is essential

76
01:06:19 --> 01:06:23
that data across the organization
is easily accessed and integrated.

77
01:06:25 --> 01:06:29
Data silos as you know, are like
a death knell on effective analytics.

78
01:06:31 --> 01:06:35
So barriers to data
access must be removed.

79
01:06:35 --> 01:06:40
Opening up the silos must be encouraged
and supported from the organization's

80
01:06:40 --> 01:06:45
leaders in order to promote a data
sharing mindset for the company.

81
01:06:47 --> 01:06:50
Another aspect of defining
your big data strategy

82
01:06:50 --> 01:06:53
is defining the policies around big data.

83
01:06:54 --> 01:06:59
Although it has an amazing amount
of potential for your business,

84
01:06:59 --> 01:07:03
using big data should also raise some
concerns in long term planning for data.

85
01:07:04 --> 01:07:08
Although this is a very complex issue,

86
01:07:08 --> 01:07:12
here are some questions you should
think of addressing around policy.

87
01:07:12 --> 01:07:14
What are the privacy concerns?

88
01:07:14 --> 01:07:17
Who should have access to,
or control data?

89
01:07:17 --> 01:07:23
What is the lifetime of data,
which is sometimes defined as volatility,

90
01:07:23 --> 01:07:24
anatomy of big data?

91
01:07:25 --> 01:07:28
How does data get curated and cleaned up?

92
01:07:30 --> 01:07:33
What ensures data quality
in the long term?

93
01:07:34 --> 01:07:38
How do different parts of your
organization communicate or

94
01:07:38 --> 01:07:40
interoperate using this data?

95
01:07:41 --> 01:07:45
Are there any legal and
regulatory standards in place?

96
01:07:47 --> 01:07:51
Cultivating an analytics
driven culture is crucial

97
01:07:51 --> 01:07:53
to the success of a big data strategy.

98
01:07:55 --> 01:07:58
The mindset that you want to
establish is that analytics

99
01:07:58 --> 01:08:03
is an integral part of doing business,
not a separate afterthought.

100
01:08:04 --> 01:08:09
Analytics activities must be tied
to your business objectives, and

101
01:08:09 --> 01:08:12
you must be willing to use analytics
in driving business decisions.

102
01:08:13 --> 01:08:18
Analytics and business together bring
about exciting opportunities and

103
01:08:18 --> 01:08:20
growth to your big data strategy.

104
01:08:23 --> 01:08:27
Finally, one size does not fit all.

105
01:08:27 --> 01:08:29
Hence, big data technologies and

106
01:08:29 --> 01:08:34
analytics is growing rapidly as your
business is an evolving entity.

107
01:08:35 --> 01:08:40
You have to iterate your strategy to
take advantage of new advances and

108
01:08:40 --> 01:08:43
also make your business more
dynamic in the face of change.

109
01:08:46 --> 01:08:50
As a summary,
when building a big data strategy,

110
01:08:50 --> 01:08:54
it is important to integrate big data
analytics with business objectives.

111
01:08:55 --> 01:08:59
Communicate goals and
provide organizational buy-in for

112
01:08:59 --> 01:09:01
analytics projects.

113
01:09:01 --> 01:09:05
Build teams with diverse talents,
and establish a teamwork mindset.

114
01:09:07 --> 01:09:11
Remove barriers to data access and
integration.

115
01:09:11 --> 01:09:16
Finally, these activities need to be
iterated to respond to new business

116
01:09:16 --> 01:09:18
goals and technological advances.

1
02:09:38 --> 02:09:42
In this video, we'll talk about a new
that is usually not covered much.

2
02:09:43 --> 02:09:44
It's called valence.

3
02:09:52 --> 02:09:56
Simply put Valence
refers to Connectedness.

4
02:09:57 --> 02:10:00
The more connected data is,
the higher it's valences.

5
02:10:01 --> 02:10:04
The term valence comes from chemistry.

6
02:10:05 --> 02:10:09
In chemistry, we talk about core electrons
and valence electrons of an atom.

7
02:10:10 --> 02:10:15
Valence electrons are in the outer most
shell, have the highest energy level and

8
02:10:15 --> 02:10:18
are responsible for
bonding with other atoms.

9
02:10:18 --> 02:10:23
That higher valence results in greater
boding, that is greater connectedness.

10
02:10:24 --> 02:10:28
This idea is carried over into our
definition of the term valence

11
02:10:28 --> 02:10:30
in the context of big data.

12
02:10:32 --> 02:10:35
Data items are often directly
connected to one another.

13
02:10:35 --> 02:10:38
A city is connected to
the country it belongs to.

14
02:10:39 --> 02:10:43
Two Facebook users are connected
because they are friends.

15
02:10:43 --> 02:10:45
An employee is connected
to his work place.

16
02:10:46 --> 02:10:48
Data could also be indirectly connected.

17
02:10:49 --> 02:10:53
Two scientists are connected,
because they are both physicists.

18
02:10:54 --> 02:11:00
For a data collection valence measures
the ratio of actually connected data items

19
02:11:00 --> 02:11:04
to the possible number of connections
that could occur within the collection.

20
02:11:05 --> 02:11:08
The most important aspect of valence

21
02:11:08 --> 02:11:10
is that the data connectivity
increases over time.

22
02:11:11 --> 02:11:16
The series of network graphs comes from
a social experiment where scientists

23
02:11:16 --> 02:11:21
attending a conference were asked to meet
other scientists they did not know before.

24
02:11:21 --> 02:11:22
After several rounds of meetings,

25
02:11:22 --> 02:11:26
they found new connections
shown by their red edges.

26
02:11:26 --> 02:11:32
Increase in valence can lead to emergent
group behavior in people networks,

27
02:11:32 --> 02:11:37
like creation of new groups and coalitions
that have shared values and goals.

28
02:11:39 --> 02:11:41
A high valence data set is denser.

29
02:11:43 --> 02:11:46
This makes many regular,
analytic critiques very inefficient.

30
02:11:47 --> 02:11:51
More complex analytical methods
must be adopted to account for

31
02:11:51 --> 02:11:52
the increasing density.

32
02:11:53 --> 02:11:57
More interesting challenges arise due
to the dynamic behavior of the data.

33
02:11:59 --> 02:12:01
Now there is a need to model and

34
02:12:01 --> 02:12:05
predict how valence of a connected data
set may change with time and volume.

35
02:12:06 --> 02:12:11
The dynamic behavior also leads to
the problem of event detection,

36
02:12:11 --> 02:12:15
such as bursts in the local
cohesion in parts of the data.

37
02:12:15 --> 02:12:17
And emergent behavior
in the whole data set,

38
02:12:17 --> 02:12:20
such as increased
polarization in a community.

1
04:21:58 --> 04:22:01
Now we'll talk about a form of
scalability called variety.

2
04:22:03 --> 04:22:07
In this case, scale does not
refer to the largeness of data.

3
04:22:07 --> 04:22:09
It refers to increased diversity.

4
04:22:18 --> 04:22:21
Here is an important mantra
you need to think about.

5
04:22:22 --> 04:22:27
When we, as data scientists, think of
data variety, we think of the additional

6
04:22:27 --> 04:22:32
complexity that results from more kinds
of data that we need to store, process,

7
04:22:32 --> 04:22:34
and combine.

8
04:22:34 --> 04:22:36
Now, many years ago when I
started studying data management,

9
04:22:37 --> 04:22:40
we always thought of data as tables.

10
04:22:41 --> 04:22:47
These tables could be in spreadsheets or
databases or just files, but somehow

11
04:22:47 --> 04:22:50
they will be modeled and manipulated
as rows and columns of of tables.

12
04:22:51 --> 04:22:57
Now, tables are still really important and
dominant, however today a much wider

13
04:22:57 --> 04:23:01
variety of data are collected, stored, and
analyzed to solve real world problems.

14
04:23:02 --> 04:23:07
Image data, text data, network data,
geographic maps, computer

15
04:23:07 --> 04:23:12
generated simulations are only a few of
the types of data we encounter everyday.

16
04:23:13 --> 04:23:18
The heterogeneity of data can be
characterized along several dimensions.

17
04:23:18 --> 04:23:19
We mentioned four such axes here.

18
04:23:21 --> 04:23:24
Structural variety refers
to the difference in

19
04:23:24 --> 04:23:26
the representation of the data.

20
04:23:26 --> 04:23:30
For example, an EKG signal is very
different from a newspaper article.

21
04:23:31 --> 04:23:36
A satellite image of wildfires from
NASA is very different from tweets

22
04:23:36 --> 04:23:39
sent out by people who
are seeing the fire spread.

23
04:23:40 --> 04:23:45
Media variety refers to the medium
in which the data gets delivered.

24
04:23:45 --> 04:23:49
The audio of a speech versus
the transcript of the speech

25
04:23:49 --> 04:23:52
may represent the same information
in two different media.

26
04:23:53 --> 04:23:56
Data objects like news video
may have multiple media.

27
04:23:56 --> 04:24:00
An image sequence, an audio,
and closed captioned text,

28
04:24:00 --> 04:24:02
all time synchronized to each other.

29
04:24:03 --> 04:24:08
Semantic variety is best
described two examples.

30
04:24:08 --> 04:24:12
We often use different units for
quantities we measure.

31
04:24:12 --> 04:24:16
Sometimes we also use qualitative
versus quantitative measures.

32
04:24:16 --> 04:24:20
For example, age can be a number or

33
04:24:20 --> 04:24:23
we represent it by terms like infant,
juvenile, or adult.

34
04:24:25 --> 04:24:28
Another kind of semantic
variety comes from different

35
04:24:28 --> 04:24:30
assumptions of conditions on the data.

36
04:24:30 --> 04:24:36
For example, if we conduct two income
surveys on two different groups of people,

37
04:24:36 --> 04:24:37
we may not be able to compare or

38
04:24:37 --> 04:24:41
combine them without knowing more
about the populations themselves.

39
04:24:42 --> 04:24:44
The variation and
availability takes many forms.

40
04:24:45 --> 04:24:49
For one, data can be available real time,

41
04:24:49 --> 04:24:53
like sensor data, or it can be stored,
like patient records.

42
04:24:54 --> 04:24:57
Similarly data can be
accessible continuously, for

43
04:24:57 --> 04:24:58
example from a traffic cam.

44
04:24:59 --> 04:25:01
Versus intermittently, for

45
04:25:01 --> 04:25:04
example, only when the satellite
is over the region of interest.

46
04:25:05 --> 04:25:09
This makes a difference between what
operations one can do with data,

47
04:25:09 --> 04:25:12
especially if the volume
of the data is large.

48
04:25:13 --> 04:25:16
We'll cover this in more
detail in course two

49
04:25:16 --> 04:25:21
when we explore the different genres
of data and how we model them.

50
04:25:22 --> 04:25:25
We should not think that
a single data object, or

51
04:25:25 --> 04:25:29
a collection of similar data objects,
will be all uniform in themselves.

52
04:25:29 --> 04:25:32
Emails, for example, is a hybrid entity.

53
04:25:33 --> 04:25:37
Some of this information can be a table,
like shown here.

54
04:25:38 --> 04:25:40
Now, the body of the email
usually has text in it.

55
04:25:42 --> 04:25:45
However, some of the text may
have ornaments around them.

56
04:25:45 --> 04:25:46
For example,

57
04:25:46 --> 04:25:51
the part highlighted in yellow represents
something called a markup on text.

58
04:25:52 --> 04:25:55
We'll get to markups later in the course.

59
04:25:55 --> 04:25:57
Emails contain attachments.

60
04:25:57 --> 04:25:59
These are files, or embedded images,

61
04:25:59 --> 04:26:03
or other multimedia objects
that the mailer allows.

62
04:26:03 --> 04:26:07
This screenshot from my Outlook
shows the image of a scanned image

63
04:26:07 --> 04:26:08
of a handwritten note.

64
04:26:09 --> 04:26:13
When you take a collection of
all emails from your mailbox, or

65
04:26:13 --> 04:26:17
that from an organization,
you will see that senders and

66
04:26:17 --> 04:26:19
receivers form a communication network.

67
04:26:21 --> 04:26:25
In 2001, there was a famous scandal around
a company called Enron that engaged in

68
04:26:25 --> 04:26:29
fraudulent financial reporting practices.

69
04:26:29 --> 04:26:34
Their email network, partly shown here,
has been studied by data scientist to find

70
04:26:34 --> 04:26:39
usual and unusual patterns of connections
among the people in the organization.

71
04:26:40 --> 04:26:44
An email collection can also
have it's own semantics.

72
04:26:44 --> 04:26:48
For example, an email cannot refer to,
that means cannot copy or

73
04:26:48 --> 04:26:49
forward, a previous email.

74
04:26:50 --> 04:26:55
Finally, an email server
is a real-time data source.

75
04:26:55 --> 04:26:57
But an email repository is not.

76
04:26:58 --> 04:27:02
Does email, and email collections,
demonstrate significant

77
04:27:02 --> 04:27:07
internal variation in structure,
media, semantics, and availability?

1
08:49:04 --> 08:49:07
Characteristics of Big Data- Velocity.

2
08:49:23 --> 08:49:27
Velocity refers to the increasing
speed at which big data is created and

3
08:49:27 --> 08:49:32
the increasing speed at which the data
needs to be stored and analyzed.

4
08:49:33 --> 08:49:38
Processing of data in real-time to match
its production rate as it gets generated

5
08:49:38 --> 08:49:41
is a particular goal
of big data analytics.

6
08:49:41 --> 08:49:45
For example,
this type of capability allows for

7
08:49:45 --> 08:49:49
personalization of advertisement
on the web pages you visit

8
08:49:49 --> 08:49:53
based on your recent search,
viewing, and purchase history.

9
08:49:53 --> 08:49:59
If a business cannot take advantage
of the data as it gets generated, or

10
08:49:59 --> 08:50:03
at the speed analysis of it is needed,
they often miss opportunities.

11
08:50:04 --> 08:50:09
In order to build a case for the
importance of this dimension of big data,

12
08:50:09 --> 08:50:11
let's imagine we are taking a road trip.

13
08:50:12 --> 08:50:16
You're looking for
some better information to start packing.

14
08:50:16 --> 08:50:18
In this case, the newer the information,

15
08:50:18 --> 08:50:23
the higher its relevance
in deciding what to pack.

16
08:50:23 --> 08:50:25
Would you use last month's
weather information or

17
08:50:25 --> 08:50:28
data from last year at this time?

18
08:50:28 --> 08:50:33
Or, would you use the weather
information from this week, yesterday or

19
08:50:33 --> 08:50:34
better, today?

20
08:50:35 --> 08:50:39
It makes sense to obtain the latest
information about weather and

21
08:50:39 --> 08:50:42
process it in a way that
makes your decisions easier.

22
08:50:42 --> 08:50:48
If the information is old,
it doesn't matter how accurate it is.

23
08:50:49 --> 08:50:53
Being able to catch up with
the velocity of big data and

24
08:50:53 --> 08:50:58
analyzing it as it gets generated can
even impact the quality of human life.

25
08:50:58 --> 08:51:04
Sensors and smart devices monitoring
the human body can detect abnormalities

26
08:51:04 --> 08:51:10
in real time and trigger immediate action,
potentially saving lives.

27
08:51:10 --> 08:51:14
This type of processing is what
we call real time processing.

28
08:51:14 --> 08:51:19
Real-time processing is quite
different from its remote relative,

29
08:51:19 --> 08:51:19
batch processing.

30
08:51:22 --> 08:51:26
Batch processing was the norm
until a couple of years ago.

31
08:51:26 --> 08:51:30
Large amounts of data would be
fed into large machines and

32
08:51:30 --> 08:51:31
processed for days at a time.

33
08:51:33 --> 08:51:37
While this type of processing is still
very common today, decisions based on

34
08:51:37 --> 08:51:43
information that is even few days old
can be catastrophic to some businesses.

35
08:51:45 --> 08:51:49
Organizations which make
decisions on latest data

36
08:51:49 --> 08:51:50
are more likely to hit the target.

37
08:51:52 --> 08:51:56
For this reason it's important
to match the speed of processing

38
08:51:56 --> 08:52:01
with the speed of information generation,
and get real time decision making power.

39
08:52:01 --> 08:52:05
In addition, today's sensor-powered

40
08:52:05 --> 08:52:10
socioeconomic climate
requires faster decisions.

41
08:52:10 --> 08:52:15
Hence, we can not wait for
all the data to be first produced,

42
08:52:15 --> 08:52:17
then fed into a machine.

43
08:52:18 --> 08:52:22
There are many applications where
new information is streaming and

44
08:52:22 --> 08:52:26
needs to be integrated
with existing data to

45
08:52:26 --> 08:52:30
produce decisions such as emergency
response planning in a tornado, or

46
08:52:30 --> 08:52:36
deciding trading strategies in real time,
or getting estimates in advertising.

47
08:52:38 --> 08:52:44
We have to digest chunks of data as they
are produced and give meaningful results.

48
08:52:46 --> 08:52:48
As more data comes in,

49
08:52:48 --> 08:52:52
your results will need to adapt to
reflect this change in the input.

50
08:52:54 --> 08:53:00
Decisions based on processing of already
acquired data such as batch processing,

51
08:53:00 --> 08:53:02
may give an incomplete picture.

52
08:53:02 --> 08:53:08
And hence, the applications need real
time status of the context at hand.

53
08:53:08 --> 08:53:09
That is, streaming analysis.

54
08:53:11 --> 08:53:16
Fortunately, with the event
of cheap sensors technology,

55
08:53:16 --> 08:53:21
mobile phones, and social media,
we can obtain the latest information

56
08:53:21 --> 08:53:25
at a much rapid rate and
in real time in comparison with the past.

57
08:53:26 --> 08:53:30
So how do you make sure we match
the velocity of the expectations

58
08:53:30 --> 08:53:32
to gain insights from big data?

59
08:53:32 --> 08:53:35
With the velocity of the big data.

60
08:53:35 --> 08:53:38
Rate of generation, retrieval,

61
08:53:38 --> 08:53:41
or processing of data is
application specific.

62
08:53:42 --> 08:53:47
The need for real time data-driven
actions within a business case is

63
08:53:47 --> 08:53:52
what in the end dictates the velocity
of analytics over big data.

64
08:53:53 --> 08:53:57
Sometimes precision of a minute is needed.

65
08:53:57 --> 08:53:59
Sometimes half a day.

66
08:53:59 --> 08:54:04
Let's look at these four paths and
discuss when to pick the right one for

67
08:54:04 --> 08:54:06
your analysis.

68
08:54:06 --> 08:54:10
The dollar signs next to the numbers
in this example indicate how costly

69
08:54:10 --> 08:54:13
the operation is.

70
08:54:13 --> 08:54:15
The more dollars, the higher the cost.

71
08:54:16 --> 08:54:20
When the timeliness of processed
information plays no role in decision

72
08:54:20 --> 08:54:25
making, the speed at which data
is generated becomes irrelevant.

73
08:54:25 --> 08:54:31
In other words, you can wait for
as long as it takes to process data.

74
08:54:31 --> 08:54:33
Days, months, weeks.

75
08:54:33 --> 08:54:37
And once processing is over,
you will look at the results and

76
08:54:37 --> 08:54:38
probably share them with someone.

77
08:54:39 --> 08:54:44
When timeliness is not an issue,
you can choose any of the four paths.

78
08:54:45 --> 08:54:48
You will likely pick the cheapest one.

79
08:54:49 --> 08:54:52
When timeliness of end result is an issue

80
08:54:52 --> 08:54:56
deciding which of the four paths
to choose is not so simple.

81
08:54:56 --> 08:55:00
You will have to make a decision
based on cost of hardware,

82
08:55:00 --> 08:55:05
time sensitivity of information,
future scenarios.

83
08:55:05 --> 08:55:10
In other words,
this becomes a business driven question.

84
08:55:10 --> 08:55:16
For example, if speed is really important
at all costs, you will pick path four.

85
08:55:16 --> 08:55:23
As a summary, we need to pay attention
to the velocity of big data.

86
08:55:23 --> 08:55:27
Streaming data gives information
on what's going on right now.

87
08:55:27 --> 08:55:32
Streaming data has velocity, meaning
it gets generated at various rates.

88
08:55:32 --> 08:55:37
And analysis of such data in
real time gives agility and

89
08:55:37 --> 08:55:41
adaptability to maximize
benefits you want to extract.

1
17:44:46 --> 17:44:49
Characteristics of Big Data, Veracity.

2
17:45:05 --> 17:45:08
Veracity of Big Data refers
to the quality of the data.

3
17:45:10 --> 17:45:14
It sometimes gets referred
to as validity or

4
17:45:14 --> 17:45:18
volatility referring to
the lifetime of the data.

5
17:45:19 --> 17:45:23
Veracity is very important for
making big data operational.

6
17:45:24 --> 17:45:28
Because big data can be noisy and
uncertain.

7
17:45:28 --> 17:45:35
It can be full of biases,
abnormalities and it can be imprecise.

8
17:45:35 --> 17:45:39
Data is of no value if it's not accurate,

9
17:45:39 --> 17:45:44
the results of big data analysis are only
as good as the data being analyzed.

10
17:45:46 --> 17:45:51
This is often described in analytics
as junk in equals junk out.

11
17:45:53 --> 17:45:57
So we can say although big data
provides many opportunities to make

12
17:45:57 --> 17:46:03
data enabled decisions,
the evidence provided by data

13
17:46:03 --> 17:46:07
is only valuable if the data
is of a satisfactory quality.

14
17:46:08 --> 17:46:11
There are many different
ways to define data quality.

15
17:46:12 --> 17:46:15
In the context of big data,

16
17:46:15 --> 17:46:19
quality can be defined as a function
of a couple of different variables.

17
17:46:20 --> 17:46:26
Accuracy of the data, the trustworthiness
or reliability of the data source.

18
17:46:26 --> 17:46:29
And how the data was generated
are all important factors

19
17:46:29 --> 17:46:31
that affect the quality of data.

20
17:46:33 --> 17:46:38
Additionally how meaningful the data
is with respect to the program that

21
17:46:38 --> 17:46:45
analyzes it, is an important factor, and
makes context a part of the quality.

22
17:46:47 --> 17:46:52
In this chart from 2015,
we see the volumes of data increasing,

23
17:46:52 --> 17:46:57
starting with small amounts
of enterprise data to larger,

24
17:46:57 --> 17:47:02
people generated voice over IP and
social media data and

25
17:47:02 --> 17:47:06
even larger machine generated sensor data.

26
17:47:06 --> 17:47:11
We also see that the uncertainty of
the data increases as we go from

27
17:47:11 --> 17:47:13
enterprise data to sensor data.

28
17:47:13 --> 17:47:16
This is as we would expect it to be.

29
17:47:16 --> 17:47:20
Traditional enterprise
data in warehouses have

30
17:47:20 --> 17:47:26
standardized quality solutions
like master processes for extract,

31
17:47:26 --> 17:47:31
transform and load of the data which
we referred to as before as ETL.

32
17:47:31 --> 17:47:36
As enterprises started incorporating less
structured and unstructured people and

33
17:47:36 --> 17:47:39
machine data into their
big data solutions,

34
17:47:39 --> 17:47:43
the data become messier and
more uncertain.

35
17:47:43 --> 17:47:45
There are many reasons for this.

36
17:47:46 --> 17:47:52
First, unstructured data on the internet
is imprecise and uncertain.

37
17:47:52 --> 17:47:58
In addition, high velocity big data
leaves very little or no time for

38
17:47:58 --> 17:48:04
ETL, and in turn hindering the quality
assurance processes of the data.

39
17:48:04 --> 17:48:09
Let's look at these product reviews for
a banana slicer on amazon.com.

40
17:48:09 --> 17:48:13
One of the five star reviews say that

41
17:48:13 --> 17:48:19
it saved her marriage and compared it
to the greatest inventions in history.

42
17:48:19 --> 17:48:23
Another five star reviewer said
that his parole officer recommended

43
17:48:23 --> 17:48:27
the slicer as he is not
allowed to be around knives.

44
17:48:27 --> 17:48:29
These are obviously fake reviewers.

45
17:48:30 --> 17:48:34
Now think of an automated product
assessment going through such

46
17:48:34 --> 17:48:38
splendid reviews and estimating lots
of sales for the banana slicer and

47
17:48:38 --> 17:48:43
in turn suggesting stocking more
of the slicer in the inventory.

48
17:48:43 --> 17:48:44
Amazon will have problems.

49
17:48:45 --> 17:48:51
For a more serious case let's look at
the Google flu trends case from 2013.

50
17:48:51 --> 17:48:56
For January 2013,
the Google Friends actually

51
17:48:56 --> 17:49:02
estimated almost twice as many
flu cases as was reported by CDC,

52
17:49:02 --> 17:49:05
the Centers for
Disease Control and Prevention.

53
17:49:07 --> 17:49:11
The primary reason behind this was that
Google Flu Trends used a big data on

54
17:49:11 --> 17:49:17
the internet and did not account properly
for uncertainties about the data.

55
17:49:18 --> 17:49:22
Maybe the news and social media
attention paid to the particularly

56
17:49:22 --> 17:49:27
serious level of flu that
year effected the estimate.

57
17:49:27 --> 17:49:31
And resulted in what we
call an over estimation.

58
17:49:31 --> 17:49:34
This is a perfect example for

59
17:49:34 --> 17:49:40
how inaccurate the results can be if
only big data is used in the analysis.

60
17:49:40 --> 17:49:44
Imagine the economical impact of
making health care preparations for

61
17:49:44 --> 17:49:46
twice the amount of flu cases.

62
17:49:46 --> 17:49:48
That would be huge.

63
17:49:48 --> 17:49:52
The Google flu trends example
also brings up the need for

64
17:49:52 --> 17:49:58
being able to identify where exactly
the big data they used comes from.

65
17:49:58 --> 17:50:01
What transformation did
big data go through up

66
17:50:01 --> 17:50:04
until the moment it was used for
a estimate?

67
17:50:04 --> 17:50:08
This is what we refer
to as data providence.

68
17:50:08 --> 17:50:12
Just like we refer to
an artifacts provenance.

69
17:50:12 --> 17:50:18
As a summary, the growing
torrents of big data pushes for

70
17:50:18 --> 17:50:21
fast solutions to utilize
it in analytical solutions.

71
17:50:22 --> 17:50:27
This creates challenges on
keeping track of data quality.

72
17:50:27 --> 17:50:31
What has been collected,
where it came from, and

73
17:50:31 --> 17:50:33
how it was analyzed prior to its use.

74
17:50:35 --> 17:50:37
This is akin to an art artifact

75
17:50:37 --> 17:50:40
having providence of everything
it has gone through.

76
17:50:41 --> 17:50:46
But even more complicated to achieve
with large volumes of data coming

77
17:50:46 --> 17:50:49
in varieties and velocities.

1
11:35:34 --> 11:35:37
Characteristics of Big Data- Volume.

2
11:35:47 --> 11:35:53
Volume is the big data dimension that
relates to the sheer size of big data.

3
11:35:54 --> 11:35:58
This volume can come from
large datasets being shared or

4
11:35:58 --> 11:36:04
many small data pieces and
events being collected over time.

5
11:36:05 --> 11:36:09
Every minute 204 million emails are sent,

6
11:36:09 --> 11:36:15
200,000 photos are uploaded, and 1.8
million likes are generated on Facebook.

7
11:36:15 --> 11:36:22
On YouTube, 1.3 million videos are viewed
and 72 hours of video are uploaded.

8
11:36:25 --> 11:36:28
But how much data are we talking about?

9
11:36:28 --> 11:36:33
The size and the scale of storage for
big data can be massive.

10
11:36:33 --> 11:36:37
You heard me say words that
start with peta, exa and

11
11:36:37 --> 11:36:43
yotta, to define size, but
what does all that really mean?

12
11:36:43 --> 11:36:49
For comparison, 100 megabytes will
hold a couple of encyclopedias.

13
11:36:50 --> 11:36:54
A DVD is around 5 GBs, and

14
11:36:54 --> 11:36:59
1 TB would hold around 300
hours of good quality video.

15
11:36:59 --> 11:37:06
A data-oriented business currently
collects data in the order of terabytes,

16
11:37:06 --> 11:37:09
but petabytes are becoming more
common to our daily lives.

17
11:37:10 --> 11:37:16
CERN's large hadron collider
generates 15 petabytes a year.

18
11:37:16 --> 11:37:21
According to predictions by an IDC report
sponsored by a big data company called

19
11:37:21 --> 11:37:28
EMC, digital data, will grow by
a factor of 44 until the year 2020.

20
11:37:28 --> 11:37:32
This is a growth from 0.8 zetabytes,

21
11:37:33 --> 11:37:38
In 2009 to 35.2 zettabytes in 2020.

22
11:37:38 --> 11:37:45
A zettabyte is 1 trillion gigabytes,
that's 10 to the power of 21.

23
11:37:45 --> 11:37:48
The effects of it will be huge!

24
11:37:49 --> 11:37:54
Think of all the time, cost,
energy that will be used to store and

25
11:37:54 --> 11:37:57
make sense of such an amount of data.

26
11:37:57 --> 11:38:00
The next era will be yottabytes.

27
11:38:00 --> 11:38:05
Ten to the power of 24 and
brontobytes, ten to the power of 27.

28
11:38:05 --> 11:38:10
Which is really hard to imagine for
most of us at this time.

29
11:38:10 --> 11:38:15
This is also what we call data
at an astronomical scale.

30
11:38:15 --> 11:38:18
The choice of putting the Milky Way Galaxy

31
11:38:19 --> 11:38:23
in the middle of the circle
is not just for aesthetics.

32
11:38:24 --> 11:38:28
This is what we would see if
we were to scale up 10 to

33
11:38:28 --> 11:38:30
the 21 times into the universe.

34
11:38:30 --> 11:38:31
Cool, isn't it?

35
11:38:32 --> 11:38:35
Please refer to the reading
in this module called,

36
11:38:35 --> 11:38:41
what does astronomical scale mean,
for a nice video on the powers of ten.

37
11:38:41 --> 11:38:48
All of these point to an exponential
growth in data volume and storage.

38
11:38:48 --> 11:38:51
What is the relevance of
this much data in our world?

39
11:38:52 --> 11:38:54
Remember the planes collecting big data?

40
11:38:55 --> 11:38:59
Our hope, as passengers,
is data means better flight safety.

41
11:39:00 --> 11:39:06
The idea is to understand that businesses
and organizations are collecting and

42
11:39:06 --> 11:39:10
leveraging large volumes of data
to improve their end products,

43
11:39:10 --> 11:39:14
whether it is safety, reliability,
healthcare, or governance.

44
11:39:14 --> 11:39:19
In general, in business the goal

45
11:39:19 --> 11:39:24
is to turn this much data into
some form of business advantage.

46
11:39:24 --> 11:39:29
The question is how do we
utilize larger volumes of data

47
11:39:29 --> 11:39:31
to improve our end product's quality?

48
11:39:31 --> 11:39:34
Despite a number of
challenges related to it.

49
11:39:35 --> 11:39:40
There are a number of challenges related
to the massive volumes of big data.

50
11:39:42 --> 11:39:45
The most obvious one is of course storage.

51
11:39:45 --> 11:39:48
As the size of the data increases so

52
11:39:48 --> 11:39:52
does the amount of storage space
required to store that data efficiently.

53
11:39:53 --> 11:39:58
However, we also need to be able
to retrieve that large amount of

54
11:39:58 --> 11:40:00
data fast enough, and

55
11:40:00 --> 11:40:06
move it to processing units in a timely
fashion to get results when we need them.

56
11:40:06 --> 11:40:09
This brings additional challenges
such as networking, bandwidth,

57
11:40:09 --> 11:40:11
cost of storing data.

58
11:40:11 --> 11:40:15
In-house versus cloud storage and
things like that.

59
11:40:15 --> 11:40:20
Additional challenges arise during
processing of such large data.

60
11:40:20 --> 11:40:24
Most existing analytical methods
won't scale to such sums of

61
11:40:24 --> 11:40:27
data in terms of memory,
processing, or IO needs.

62
11:40:29 --> 11:40:30
This means their performance will drop.

63
11:40:32 --> 11:40:36
You might be able to get good performance
for data from hundreds of customers.

64
11:40:36 --> 11:40:43
But how about scaling your solution
to 1,000 or 10,000 customers?

65
11:40:43 --> 11:40:49
As the volume increases performance and
cost start becoming a challenge.

66
11:40:50 --> 11:40:55
Businesses need a holistic
strategy to handle processing of

67
11:40:55 --> 11:41:00
large scale data to their benefit
in the most cost effective manner.

68
11:41:00 --> 11:41:03
Evaluating the options across
the dimensions mentioned here,

69
11:41:03 --> 11:41:07
is the first step when it comes to
continuously increasing data size.

70
11:41:07 --> 11:41:12
We will revisit this topic
later on in this course.

71
11:41:12 --> 11:41:18
As a summary volume is the dimension
of big data related to its size and

72
11:41:18 --> 11:41:19
its exponential growth.

73
11:41:20 --> 11:41:26
The challenges with working with volumes
of big data include cost, scalability,

74
11:41:26 --> 11:41:30
and performance related to their storage,
access, and processing.

1
23:17:06 --> 23:17:08
Data Science.

2
23:17:08 --> 23:17:10
Getting value out of big data.

3
23:17:30 --> 23:17:35
We have all heard data science turned
data into insights or even actions.

4
23:17:35 --> 23:17:38
But what does that really mean?

5
23:17:38 --> 23:17:40
Data science can be
thought of as a basis for

6
23:17:40 --> 23:17:45
empirical research where data is used
to induce information for observations.

7
23:17:46 --> 23:17:50
These observations are mainly data,
in our case,

8
23:17:50 --> 23:17:53
big data, related to a business or
scientific case.

9
23:17:53 --> 23:17:58
Insight is a term

10
23:17:58 --> 23:18:03
we use to refer to the data
products of data science.

11
23:18:03 --> 23:18:07
It is extracted from a diverse amount
of data through a combination of

12
23:18:07 --> 23:18:11
exploratory data analysis and modeling.

13
23:18:11 --> 23:18:17
The questions are sometimes more specific,
and sometimes it requires

14
23:18:17 --> 23:18:22
looking at the data and patterns in it
to come up with the specific question.

15
23:18:24 --> 23:18:29
Another important point to recognize
is that data science is not static.

16
23:18:29 --> 23:18:32
It is not one time analysis.

17
23:18:32 --> 23:18:36
It involves a process where models
generated to lead to insights

18
23:18:36 --> 23:18:41
are constantly improved through further
empirical evidence, or simply, data.

19
23:18:43 --> 23:18:49
For example, a book retailer like
Amazon.com can constantly improve

20
23:18:49 --> 23:18:53
the model of a customer's book preferences
using the customer demographic,

21
23:18:54 --> 23:18:59
his or her previous purchases and
the book reviews of the customer.

22
23:19:01 --> 23:19:06
The book retailer can also
uses information to predict

23
23:19:06 --> 23:19:11
which customers are likely
to like any book,

24
23:19:11 --> 23:19:15
and take action to market
the book to those customers.

25
23:19:17 --> 23:19:20
This is where we see insights
being turned into action.

26
23:19:23 --> 23:19:28
As we have seen in the book marketing
example, using data science and analysis

27
23:19:28 --> 23:19:33
of the past and current information,
data science generates actions.

28
23:19:34 --> 23:19:37
This is not just an analysis of the past,
but

29
23:19:37 --> 23:19:40
rather generation of actionable
information for the future.

30
23:19:42 --> 23:19:46
This is what we can call a prediction,
like the weather forecast.

31
23:19:47 --> 23:19:53
When you decide what to wear for
the day based on the forecast of the day,

32
23:19:53 --> 23:19:57
you are taking action based
on insight delivered to you.

33
23:19:57 --> 23:20:02
Just like this, business leaders and
decision makers

34
23:20:02 --> 23:20:06
take action based on the evidence
provided by their data science teams.

35
23:20:08 --> 23:20:09
We set data science teams.

36
23:20:10 --> 23:20:15
This comes from the breadth of information
and skill that it takes to make it happen.

37
23:20:16 --> 23:20:21
You have probably seen diagrams like
this one that describe data science.

38
23:20:22 --> 23:20:26
Data science happens at
the intersection of computer science,

39
23:20:26 --> 23:20:28
mathematics and business expertise.

40
23:20:31 --> 23:20:33
If we zoom deeper into this diagram and

41
23:20:33 --> 23:20:39
open the sets of expertise we will
see a variation of this figure.

42
23:20:40 --> 23:20:46
Even at this level, all of these circles
require deeper knowledge and skills

43
23:20:46 --> 23:20:52
in areas like domain expertise, data
engineering, statistics and computing.

44
23:20:55 --> 23:20:59
An even deeper analysis of these
skills will lead you to skills like

45
23:20:59 --> 23:21:04
machine learning, statistical modeling,
relational algebra,

46
23:21:04 --> 23:21:08
business passion, problem solving and
data visualization.

47
23:21:08 --> 23:21:11
That's a lot of skills to have for
a single person.

48
23:21:16 --> 23:21:21
These wide range of skills and
definitions of data scientists having them

49
23:21:21 --> 23:21:25
all led to discussions like
are data scientists unicorns?

50
23:21:26 --> 23:21:28
Meaning they don't exist.

51
23:21:29 --> 23:21:33
There are data science experts who have
expertise in more than one of these

52
23:21:33 --> 23:21:35
skills, for sure.

53
23:21:35 --> 23:21:37
But they're relatively rare, and

54
23:21:37 --> 23:21:41
still would probably need help from
an expert on some of these areas.

55
23:21:42 --> 23:21:48
So, in reality, data scientists
are teams of people who act like one.

56
23:21:49 --> 23:21:52
They are passionate about the story and
the meaning behind data.

57
23:21:54 --> 23:21:58
They understand they problem
they are trying to solve, and

58
23:21:58 --> 23:22:01
aim to find the right analytical
methods to solve this problem.

59
23:22:01 --> 23:22:08
And they all have an interest in
engineering solutions to solve problems.

60
23:22:10 --> 23:22:15
They also have curiosity about each
others work, and have communication

61
23:22:15 --> 23:22:19
skills to interact with the team and
present their ideas and results to others.

62
23:22:21 --> 23:22:27
As a summary, a data science team often
comes together to analyze situations,

63
23:22:27 --> 23:22:33
business or scientific cases, which none
of the individuals can solve on their own.

64
23:22:33 --> 23:22:36
There are lots of moving
parts to the solution.

65
23:22:36 --> 23:22:40
But in the end, all these parts
should come together to provide

66
23:22:40 --> 23:22:43
actionable insight based on big data.

67
23:22:45 --> 23:22:50
Being able to use evidence-based
insight in business decisions is more

68
23:22:50 --> 23:22:52
important now than ever.

69
23:22:52 --> 23:22:56
Data scientists have a combination
of technical, business and

70
23:22:56 --> 23:22:58
soft skills to make this happen.

1
22:40:02 --> 22:40:05
Getting started,
characteristics of big data.

2
22:40:23 --> 22:40:27
By now you have seen that big data is a
blanket term that is used to refer to any

3
22:40:27 --> 22:40:32
collection of data so large and
complex that it exceeds the processing

4
22:40:32 --> 22:40:37
capability of conventional data
management systems and techniques.

5
22:40:37 --> 22:40:41
The applications of big data are endless.

6
22:40:41 --> 22:40:46
Every part of business and society
are changing in front our eyes due to that

7
22:40:46 --> 22:40:50
fact that we now have so much more
data and the ability for analyzing.

8
22:40:52 --> 22:40:54
But how can we characterize big data?

9
22:40:55 --> 22:40:58
You can say, I know it when i see it.

10
22:40:58 --> 22:41:01
But there are easier ways to do it.

11
22:41:01 --> 22:41:05
Big data is commonly characterized
using a number of V's.

12
22:41:06 --> 22:41:11
The first three are volume,
velocity, and variety.

13
22:41:11 --> 22:41:17
Volume refers to the vast amounts of
data that is generated every second,

14
22:41:17 --> 22:41:21
mInutes, hour, and
day in our digitized world.

15
22:41:23 --> 22:41:29
Variety refers to the ever increasing
different forms that data can come in

16
22:41:29 --> 22:41:34
such as text, images,
voice, and geospatial data.

17
22:41:36 --> 22:41:41
Velocity refers to the speed at
which data is being generated and

18
22:41:41 --> 22:41:44
the pace at which data moves
from one point to the next.

19
22:41:46 --> 22:41:48
Volume, variety, and

20
22:41:48 --> 22:41:54
velocity are the three main dimensions
that characterize big data.

21
22:41:54 --> 22:41:55
And describe its challenges.

22
22:41:57 --> 22:42:02
We have huge amounts of data
in different formats, and

23
22:42:02 --> 22:42:06
varying quality which must
be processed quickly.

24
22:42:07 --> 22:42:10
More Vs have been introduced
to the big data community

25
22:42:10 --> 22:42:14
as we discover new challenges and
ways to define big data.

26
22:42:15 --> 22:42:19
Veracity and
valence are two of these additional V's we

27
22:42:19 --> 22:42:23
will pay special attention to as
a part of this specialization.

28
22:42:24 --> 22:42:31
Veracity refers to the biases,
noise, and abnormality in data.

29
22:42:31 --> 22:42:36
Or, better yet, It refers to the often
unmeasurable uncertainties and

30
22:42:36 --> 22:42:40
truthfulness and trustworthiness of data.

31
22:42:40 --> 22:42:46
Valence refers to the connectedness
of big data in the form of graphs,

32
22:42:46 --> 22:42:47
just like atoms.

33
22:42:48 --> 22:42:54
Moreover, we must be sure to
never forget our sixth V, value.

34
22:42:55 --> 22:42:57
How do big data benefit you and
your organization?

35
22:42:59 --> 22:43:01
Without a clear strategy and

36
22:43:01 --> 22:43:05
an objective with the value
they are getting from big data.

37
22:43:05 --> 22:43:09
It is easy to imagine that organizations
will be sidetracked by all these

38
22:43:09 --> 22:43:14
challenges of big data, and not be
able to turn them into opportunities.

39
22:43:15 --> 22:43:19
Now let's start looking into the first
five of these V's in detail.

1
21:23:23 --> 21:23:25
How does data science happen?

2
21:23:25 --> 21:23:26
Five P's of data science.

3
21:23:28 --> 21:23:34
Now that we identified what data science
is and how companies can strategize around

4
21:23:34 --> 21:23:39
big data to start building a purpose,
let's come back to using data science

5
21:23:39 --> 21:23:44
to get value out of big data around
the purpose or questions they defined.

6
21:24:02 --> 21:24:04
Our experience with building and

7
21:24:04 --> 21:24:10
observing successful data science projects
led to a method around the craft with

8
21:24:10 --> 21:24:16
five distinct components that can be
defined as components of data science.

9
21:24:16 --> 21:24:20
Here we define data science
as a multi-disciplinary

10
21:24:20 --> 21:24:24
craft that combines
people teaming up around

11
21:24:24 --> 21:24:29
application-specific purpose that
can be achieved through a process,

12
21:24:31 --> 21:24:35
big data computing platforms,
and programmability.

13
21:24:38 --> 21:24:43
All of these should lead to
products where the focus really

14
21:24:43 --> 21:24:47
is on the questions or purpose that are
defined by your big data strategy ideas.

15
21:24:49 --> 21:24:54
There are many technology,
data and analytical research, and

16
21:24:54 --> 21:24:58
development related activities
around the questions.

17
21:24:58 --> 21:25:02
But in the end, everything we do
in this phase is to reach to that

18
21:25:02 --> 21:25:04
final product based on our purposes.

19
21:25:05 --> 21:25:08
So, it makes sense to start with it and

20
21:25:08 --> 21:25:11
build a process around how
we make this product happen.

21
21:25:13 --> 21:25:16
Remember the wild fire
prediction project I described?

22
21:25:17 --> 21:25:22
One of the products we described
there was the rate of spread and

23
21:25:22 --> 21:25:23
direction of an ongoing fire.

24
21:25:25 --> 21:25:27
We have identified questions and

25
21:25:27 --> 21:25:32
the process that led us to
the product in the end to solve it.

26
21:25:33 --> 21:25:38
We brought together experts around
the table for fire modeling,

27
21:25:38 --> 21:25:42
data management, time series analysis,
scalable computing,

28
21:25:42 --> 21:25:46
Geographical Information Systems,
and emergency response.

29
21:25:49 --> 21:25:52
I asked them,
let's not dive into the techniques yet.

30
21:25:52 --> 21:25:54
What is the problem at large?

31
21:25:55 --> 21:26:01
How do we see ourselves solving it?

32
21:26:01 --> 21:26:05
A typical conversation around
the process starts with this question.

33
21:26:07 --> 21:26:12
Then from then on,
drilling down to many areas of expertise,

34
21:26:12 --> 21:26:14
often we blur lines between the steps.

35
21:26:16 --> 21:26:21
My wildfire team would start listing
things like, we don't have an integrated

36
21:26:21 --> 21:26:26
system or we don't have real-time
access to data programmatically,

37
21:26:26 --> 21:26:29
so we can't analyze fires on the fly.

38
21:26:29 --> 21:26:33
Or they can say, I can't integrate
sensor data with satellite data.

39
21:26:35 --> 21:26:41
All of this leads me to challenges
I can then use to define problems.

40
21:26:42 --> 21:26:46
There are many dimensions of data science
to think about within this discussion.

41
21:26:47 --> 21:26:52
Let's start with the obvious ones,
people and purpose.

42
21:26:55 --> 21:27:00
People refers to a data science team or
the projects stakeholders.

43
21:27:00 --> 21:27:03
As you know by now,
they're expert in data and

44
21:27:03 --> 21:27:08
analytics, business, computing,
science, or big data management,

45
21:27:08 --> 21:27:14
like all the set of experts I
listed in my wildfire scenario.

46
21:27:14 --> 21:27:20
The purpose refers to the challenge or
set of challenges defined by your

47
21:27:20 --> 21:27:25
big data strategy, like solving the
question related to the rate of spread and

48
21:27:25 --> 21:27:29
direction of the fire perimeter
in the wildfire case.

49
21:27:33 --> 21:27:38
Since there's a predefined team
with a purpose, a great place for

50
21:27:38 --> 21:27:43
this team to start with is
a process they could iterate on.

51
21:27:43 --> 21:27:48
We can simply say, people with purpose
will define a process to collaborate and

52
21:27:48 --> 21:27:49
communicate around.

53
21:27:51 --> 21:27:54
The process is conceptual
in the beginning and

54
21:27:54 --> 21:27:58
defines the set of steps an how
everyone can contribute to it.

55
21:28:02 --> 21:28:04
There are many ways to
look at the process.

56
21:28:05 --> 21:28:11
One way of looking at it is
as two distinct activities,

57
21:28:11 --> 21:28:14
mainly big data engineering and

58
21:28:14 --> 21:28:18
big data analytics, or
computational big data science,

59
21:28:18 --> 21:28:22
as I like to call it, as more than simple
analytics is being performed here.

60
21:28:24 --> 21:28:31
A more detailed way of looking at
the process reveals five distinct steps or

61
21:28:31 --> 21:28:36
activities of this data science process,

62
21:28:36 --> 21:28:42
namely acquire, prepare,

63
21:28:42 --> 21:28:47
analyze, report, and act.

64
21:28:47 --> 21:28:50
We can simply say that
data science happens

65
21:28:50 --> 21:28:52
at the boundary of all these steps.

66
21:28:52 --> 21:28:57
Ideally, this process should
support experimental work and

67
21:28:57 --> 21:29:01
dynamic scalability on the big data and
computing platforms.

68
21:29:05 --> 21:29:10
This five step process can be used in
alternative ways in real life big data

69
21:29:10 --> 21:29:14
applications, if we add the dependencies
of different tools to each other.

70
21:29:15 --> 21:29:19
The influence of big data pushes for

71
21:29:19 --> 21:29:23
alternative scalability approaches
at each step of the process.

72
21:29:25 --> 21:29:28
Just like you would scale
each step on its own,

73
21:29:28 --> 21:29:31
you can scale the whole
process as a whole in the end.

74
21:29:35 --> 21:29:41
One can simply say, all of these steps
have reporting needs in different forms.

75
21:29:44 --> 21:29:51
Or there is a need to draw all these
activities as an iterating process,

76
21:29:51 --> 21:29:56
including build, explore, and
scale for big data as steps.

77
21:29:59 --> 21:30:03
Big data analysis needs alternative
data management techniques and

78
21:30:03 --> 21:30:08
systems, as well as analytical tools and
methods.

79
21:30:10 --> 21:30:15
Multiple modes of scalability is needed
based on dynamic data and computing loads.

80
21:30:16 --> 21:30:20
In addition,
change in physical infrastructure,

81
21:30:20 --> 21:30:25
streaming data specific urgencies
arising from special events

82
21:30:25 --> 21:30:28
can also require multiple
modes of scalability.

83
21:30:30 --> 21:30:32
In this intro course, for simplicity,

84
21:30:32 --> 21:30:38
we will refer to the process as a set of
five sequential activities that iterate.

85
21:30:39 --> 21:30:44
However, we'll touch on scalability as
needed in our example applications.

86
21:30:47 --> 21:30:51
As a part of building
your big data process,

87
21:30:51 --> 21:30:55
it's important to simply
mention two other P's.

88
21:30:55 --> 21:31:00
The first one is big data platforms,
like the ones in the Hadoop framework,

89
21:31:00 --> 21:31:04
or other computing platforms
to scale different steps.

90
21:31:04 --> 21:31:08
The scalability should be in
the mind of all team members and

91
21:31:08 --> 21:31:10
get communicated as an expectation.

92
21:31:12 --> 21:31:16
In addition, the scalable process
should be programmable through

93
21:31:16 --> 21:31:22
utilization of reusable and
reproducible programming interfaces

94
21:31:22 --> 21:31:27
to libraries, like systems middleware,
analytical tools,

95
21:31:27 --> 21:31:31
visualization environments, and
end user reporting environments.

96
21:31:35 --> 21:31:37
Thinking of big data
applications as a process,

97
21:31:37 --> 21:31:43
including a set of activities that
the team members can collaborate over,

98
21:31:43 --> 21:31:47
also helps to build metrics for
accountability to be built into it.

99
21:31:47 --> 21:31:52
This way, expectations on cost, time,

100
21:31:52 --> 21:31:55
optimization of deliverables,
and time lines can be discussed

101
21:31:55 --> 21:32:00
between the the members starting with
the beginning of the data science process.

102
21:32:03 --> 21:32:06
Sometimes we may not be able
to do this in one step.

103
21:32:08 --> 21:32:13
And joint explorations like statistical
evaluations of intermediate results or

104
21:32:13 --> 21:32:16
accuracy of sample data
sets become important.

105
21:32:18 --> 21:32:23
As a summary, data science can be
defined as a craft of using the five P's

106
21:32:23 --> 21:32:28
identified in this lecture,
leading to a sixth P, the data product.

107
21:32:29 --> 21:32:33
Having a process within the more
business-driven Ps, like people and

108
21:32:33 --> 21:32:38
purpose, and the more technically
driven P's, like platforms and

109
21:32:38 --> 21:32:43
programmability, leads to
a streamlined approach that starts and

110
21:32:43 --> 21:32:47
ends with the product, team
accountability, and collaboration in mind.

111
21:32:48 --> 21:32:52
Data science process
provides guidelines for

112
21:32:52 --> 21:32:57
implementing big data solution,
as it helps to organize efforts and

113
21:32:57 --> 21:33:04
ensures all critical steps taken conforms
to pre-define and agreed upon metrics.

1
18:56:28 --> 18:56:30
Step one, acquiring data.

2
18:56:44 --> 18:56:49
The first step in the data science
process is to acquire the data.

3
18:56:50 --> 18:56:54
You need to obtain the source material
before analyzing or acting on it.

4
18:56:56 --> 18:57:02
The first step in acquiring data is
to determine what data is available.

5
18:57:02 --> 18:57:06
Leave no stone unturned when it comes
to finding the right data sources.

6
18:57:07 --> 18:57:11
You want to identify suitable
data related to your problem and

7
18:57:12 --> 18:57:16
make use of all data that is relevant
to your problem for analysis.

8
18:57:17 --> 18:57:21
Leaving out even a small
amount of important data

9
18:57:21 --> 18:57:23
can lead to incorrect conclusions.

10
18:57:26 --> 18:57:29
Data, comes from, many places, local and

11
18:57:29 --> 18:57:34
remote, in many varieties,
structured and un-structured.

12
18:57:34 --> 18:57:37
And, with different velocities.

13
18:57:37 --> 18:57:42
There are many techniques and technologies
to access these different types of data.

14
18:57:42 --> 18:57:44
Let's discuss a few examples.

15
18:57:46 --> 18:57:51
A lot of data exists in
conventional relational databases,

16
18:57:51 --> 18:57:53
like structure big data
from organizations.

17
18:57:54 --> 18:58:00
The tool of choice to access data from
databases is structured query language or

18
18:58:00 --> 18:58:05
SQL, which is supported by all
relational databases management systems.

19
18:58:06 --> 18:58:12
Additionally, most data base systems
come with a graphical application

20
18:58:12 --> 18:58:17
environment that allows you to query and
explore the data sets in the database.

21
18:58:19 --> 18:58:26
Data can also exist in files such as
text files and Excel spreadsheets.

22
18:58:26 --> 18:58:30
Scripting languages are generally
used to get data from files.

23
18:58:30 --> 18:58:36
A scripting language is a high
level programming language

24
18:58:36 --> 18:58:41
that can be either general purpose or
specialized for specific functions.

25
18:58:42 --> 18:58:49
Common scripting languages with support
for processing files are Java Script,

26
18:58:49 --> 18:58:54
Python, PHP, Perl, R, and
MATLAB, and are many others.

27
18:58:56 --> 18:59:01
An increasingly popular way
to get data is from websites.

28
18:59:01 --> 18:59:05
Web pages are written using
a set of standards approved by

29
18:59:05 --> 18:59:09
a world wide web consortium or
shortly, W3C.

30
18:59:09 --> 18:59:13
This includes a variety of formats and
services.

31
18:59:14 --> 18:59:20
One common format is
the Extensible Markup Language, or XML,

32
18:59:20 --> 18:59:25
which uses markup symbols or tabs to
describe the contents on a webpage.

33
18:59:27 --> 18:59:32
Many websites also host web services which
produce program access to their data.

34
18:59:35 --> 18:59:38
There are several types of web services.

35
18:59:38 --> 18:59:41
The most popular is REST because it's so
easy to use.

36
18:59:43 --> 18:59:47
REST stand for
Representational State Transfer.

37
18:59:47 --> 18:59:51
And it is an approach to implementing
web services with performance,

38
18:59:51 --> 18:59:54
scalability and maintainability in mind.

39
18:59:55 --> 18:59:59
Web socket services are also
becoming more popular

40
18:59:59 --> 19:00:02
since they allow real time
modifications from web sites.

41
19:00:05 --> 19:00:09
NoSQL storage systems are increasingly
used to manage a variety of data

42
19:00:09 --> 19:00:10
types in big data.

43
19:00:11 --> 19:00:15
These data stores are databases
that do not represent data

44
19:00:15 --> 19:00:20
in a table format with columns and rows as
with conventional relational databases.

45
19:00:21 --> 19:00:27
Examples of these data stores include
Cassandra, MongoDB and HBASE.

46
19:00:28 --> 19:00:33
NoSQL data stores provide APIs
to allow users to access data.

47
19:00:34 --> 19:00:39
These APIs can be used directly or in an
application that needs to access the data.

48
19:00:41 --> 19:00:46
Additionally, most NoSQL
systems provide data access

49
19:00:46 --> 19:00:48
via a web service interface, such a REST.

50
19:00:51 --> 19:00:54
Now, let's discuss our wildfire case study

51
19:00:54 --> 19:00:59
as a real project that acquires data
using several different mechanisms.

52
19:01:00 --> 19:01:07
The WIFIRE project stores sensor data from
weather stations in a relational database.

53
19:01:08 --> 19:01:13
We use SQL to retrieve this data
from the database to create

54
19:01:13 --> 19:01:17
models to identify weather patterns
associated with Santa Anna conditions.

55
19:01:19 --> 19:01:23
To determine whether a particular weather
station is currently experiencing

56
19:01:23 --> 19:01:30
Santa Anna conditions, we access real
time data using a web socket service.

57
19:01:31 --> 19:01:35
Once we start listening to this service,

58
19:01:35 --> 19:01:37
we receive weather station
measurements as they occur.

59
19:01:39 --> 19:01:44
This data is then processed and
compared to patterns found by our models

60
19:01:44 --> 19:01:48
to determine if a weather station is
experiencing Santa Ana conditions.

61
19:01:50 --> 19:01:53
At the same time Tweets are retrieved

62
19:01:53 --> 19:01:57
using hashtags related to any fire
that is occurring in the region.

63
19:01:58 --> 19:02:03
The Tweet messages are retrieves
using the Twitter REST service.

64
19:02:03 --> 19:02:08
The idea is to determine the sentiment
of these tweets to see if people

65
19:02:08 --> 19:02:15
are expressing fear, anger or are simply
nonchalant about the nearby fire.

66
19:02:15 --> 19:02:20
The combination of sensor data and
tweet sentiments helps

67
19:02:20 --> 19:02:25
to give us a sense of the urgency
of the fire situation.

68
19:02:25 --> 19:02:29
As a summary,
big data comes from many places.

69
19:02:30 --> 19:02:33
Finding and evaluating data

70
19:02:33 --> 19:02:37
useful to your big data analytics is
important before you start acquiring data.

71
19:02:38 --> 19:02:40
Depending on the source and

72
19:02:40 --> 19:02:44
structure of data,
there are alternative ways to access it.

1
13:59:11 --> 13:59:14
Step 2-A: Exploring Data

2
13:59:27 --> 13:59:32
After you've put together the data
that you need for your application,

3
13:59:32 --> 13:59:36
you might be tempted to immediately
build models to analyze the data.

4
13:59:37 --> 13:59:38
Resist this temptation.

5
13:59:39 --> 13:59:44
The first step after getting
your data is to explore it.

6
13:59:44 --> 13:59:48
Exploring data is a part of
the two-step data preparation process.

7
13:59:50 --> 13:59:53
You want to do some
preliminary investigation

8
13:59:53 --> 13:59:59
in order to gain a better understanding of
the specific characteristics of your data.

9
13:59:59 --> 14:00:01
In this step, you'll be looking for

10
14:00:01 --> 14:00:05
things like correlations,
general trends, and outliers.

11
14:00:06 --> 14:00:10
Without this step, you will not be
able to use the data effectively.

12
14:00:13 --> 14:00:16
Correlation graphs can be used
to explore the dependencies

13
14:00:16 --> 14:00:18
between different variables in the data.

14
14:00:20 --> 14:00:24
Graphing the general trends of
variables will show you if there is

15
14:00:24 --> 14:00:30
a consistent direction in which the values
of these variables are moving towards,

16
14:00:31 --> 14:00:33
like sales prices going up or down.

17
14:00:35 --> 14:00:43
In statistics, an outlier is a data point
that's distant from other data points.

18
14:00:43 --> 14:00:46
Plotting outliers will
help you double check for

19
14:00:46 --> 14:00:48
errors in the data due to measurements.

20
14:00:48 --> 14:00:54
In some cases, outliers that are not
errors might make you find a rare event.

21
14:00:56 --> 14:01:02
Additionally, summary statistics provide
numerical values to describe your data.

22
14:01:04 --> 14:01:09
Summary statistics are quantities
that capture various characteristics

23
14:01:09 --> 14:01:13
of a set of values with a single number or
a small set of numbers.

24
14:01:15 --> 14:01:19
Some basic summary statistics
that you should compute for

25
14:01:19 --> 14:01:24
your data set are mean, median,
range, and standard deviation.

26
14:01:26 --> 14:01:31
Mean and median are measures of
the location of a set of values.

27
14:01:31 --> 14:01:35
Mode is the value that occurs
most frequently in your data set.

28
14:01:36 --> 14:01:41
And range and standard deviation
are measures of spread in your data.

29
14:01:42 --> 14:01:47
Looking at these measures will give you
an idea of the nature of your data.

30
14:01:49 --> 14:01:52
They can tell you if there's
something wrong with your data.

31
14:01:53 --> 14:01:57
For example, if the range of the values
for age in your data includes

32
14:01:57 --> 14:02:01
negative numbers, or
a number much greater than 100,

33
14:02:01 --> 14:02:06
there's something suspicious in
the data that needs to be examined.

34
14:02:09 --> 14:02:14
Visualization techniques also
provide a quick and effective, and

35
14:02:14 --> 14:02:20
overall a very useful way to look at
data in this preliminary analysis step.

36
14:02:20 --> 14:02:24
A heat map, such as the one shown here,

37
14:02:24 --> 14:02:28
can quickly give you the idea
of where the hotspots are.

38
14:02:28 --> 14:02:31
Many other different types
of graphs can be used.

39
14:02:32 --> 14:02:36
Histograms show that
the distribution of the data and

40
14:02:36 --> 14:02:39
can show skewness or unusual dispersion.

41
14:02:40 --> 14:02:45
Boxplots are another type of plot for
showing data distribution.

42
14:02:47 --> 14:02:53
Line graphs are useful for seeing how
values in your data change over time.

43
14:02:53 --> 14:02:56
Spikes in the data are also easy to spot.

44
14:02:58 --> 14:03:03
Scatter plots can show you
correlation between two variables.

45
14:03:03 --> 14:03:06
Overall, there are many types
of graph to visualize data.

46
14:03:07 --> 14:03:11
They are very useful in helping
you understand the data you have.

47
14:03:13 --> 14:03:17
In summary, what you get by
exploring your data is a better

48
14:03:17 --> 14:03:21
understanding of the complexity of
the data you have to work with.

49
14:03:22 --> 14:03:25
This, in turn,
will guide the rest of your process.

1
04:02:34 --> 04:02:40
Step 2-B: Pre-processing Data

2
04:02:54 --> 04:02:58
The raw data that you get directly from
your sources are never in the format that

3
04:02:58 --> 04:03:00
you need to perform analysis on.

4
04:03:01 --> 04:03:05
There are two main goals in
the data pre-processing step.

5
04:03:05 --> 04:03:10
The first is to clean the data to
address data quality issues, and

6
04:03:10 --> 04:03:15
the second is to transform the raw
data to make it suitable for analysis.

7
04:03:17 --> 04:03:21
A very important part of data preparation

8
04:03:21 --> 04:03:23
is to address quality
of issues in your data.

9
04:03:23 --> 04:03:28
Real-world data is messy.

10
04:03:28 --> 04:03:34
There are many examples of quality issues
with data from real applications including

11
04:03:34 --> 04:03:39
inconsistent data like a customer with two
different addresses, duplicate customer

12
04:03:39 --> 04:03:45
records, for example, customers address
recorded at two different sales locations.

13
04:03:46 --> 04:03:48
And the two recordings don't agree.

14
04:03:50 --> 04:03:53
Missing customer agent demographics or
studies.

15
04:03:55 --> 04:03:59
Missing values like missing a customer
age in the demographic studies.

16
04:04:01 --> 04:04:06
invalid data like an invalid zip code for
example, a six digit code.

17
04:04:07 --> 04:04:13
And outliers like a sense of failure
causing values to be much higher or

18
04:04:13 --> 04:04:15
lower than expected for a period of time.

19
04:04:17 --> 04:04:19
Since we get the data downstream

20
04:04:19 --> 04:04:22
we usually have little control
over how the data is collected.

21
04:04:24 --> 04:04:29
Preventing data quality problems as
the data is being collected is not

22
04:04:29 --> 04:04:30
often an option.

23
04:04:31 --> 04:04:33
So we have the data that we get and

24
04:04:33 --> 04:04:37
we have to address quality issues
by detecting and correcting them.

25
04:04:39 --> 04:04:43
Here are some approaches we can take
to address this quality issues.

26
04:04:45 --> 04:04:48
We can remove data records
with missing values.

27
04:04:50 --> 04:04:53
We can merge duplicate records.

28
04:04:53 --> 04:04:57
This will require a way to determine
how to resolve conflicting values.

29
04:04:58 --> 04:05:03
Perhaps it makes sense to retain the newer
value whenever there's a conflict.

30
04:05:05 --> 04:05:08
For invalid values, the best estimate for

31
04:05:08 --> 04:05:12
a reasonable value can be
used as a replacement.

32
04:05:12 --> 04:05:16
For example, for
a missing age value for an employee,

33
04:05:16 --> 04:05:21
a reasonable value can be estimated based
on the employee's length of employment.

34
04:05:23 --> 04:05:28
Outliers can also be removed if
they are not important to the task.

35
04:05:30 --> 04:05:34
In order to address data
quality issues effectively,

36
04:05:34 --> 04:05:38
knowledge about the application,
such as how the data was collected,

37
04:05:38 --> 04:05:43
the user population, and the intended
uses of the application is important.

38
04:05:45 --> 04:05:49
This domain knowledge is
essential to making informed

39
04:05:49 --> 04:05:53
decisions on how to handle incomplete or
incorrect data.

40
04:05:56 --> 04:05:59
The second part of preparing data

41
04:05:59 --> 04:06:03
is to manipulate the clean data into
the format needed for analysis.

42
04:06:04 --> 04:06:06
The step is known by many names.

43
04:06:08 --> 04:06:13
Data manipulation,
data preprocessing, data wrangling,

44
04:06:13 --> 04:06:17
and even data munging, some operations for

45
04:06:17 --> 04:06:23
this type of operation I mean data
munging, wrangling, preprocessing,

46
04:06:23 --> 04:06:28
include, scaling, transformation,
feature selection,

47
04:06:28 --> 04:06:32
dimensionality reduction,
and data manipulation.

48
04:06:35 --> 04:06:42
Scaling involves changing the range of
values to be between a specified range.

49
04:06:42 --> 04:06:44
Such as from zero to one.

50
04:06:46 --> 04:06:50
This is done to avoid having certain
features that large values from

51
04:06:50 --> 04:06:52
dominating the results.

52
04:06:52 --> 04:06:56
For example,
in analyzing data with height and weight.

53
04:06:56 --> 04:07:01
To magnitude of weight values is much
greater than of the height values.

54
04:07:03 --> 04:07:06
So scaling all values
to be between zero and

55
04:07:06 --> 04:07:11
one will equalize contributions from
both height and weight features.

56
04:07:14 --> 04:07:20
Various transformations can be performed
on the data to reduce noise and

57
04:07:20 --> 04:07:20
variability.

58
04:07:22 --> 04:07:24
One such transformation is aggregation.

59
04:07:26 --> 04:07:31
Aggregate data generally results
in data with less variability,

60
04:07:31 --> 04:07:32
which may help with your analysis.

61
04:07:34 --> 04:07:39
For example, daily sales figures
may have many serious changes.

62
04:07:40 --> 04:07:45
Aggregating values to weekly or monthly
sales figures will result in similar data.

63
04:07:48 --> 04:07:53
Other filtering techniques can also be
used to remove variability in the data.

64
04:07:53 --> 04:07:57
Of course, this comes at
the cost of less detailed data.

65
04:07:57 --> 04:08:01
So these factors must be weighed for
the specific application.

66
04:08:04 --> 04:08:10
Future selection can involve removing
redundant or irrelevant features,

67
04:08:10 --> 04:08:14
combining features, and
creating new features.

68
04:08:15 --> 04:08:18
During the exploring data step,

69
04:08:18 --> 04:08:21
you might have discovered that
two features are correlated.

70
04:08:23 --> 04:08:26
In that case one of these
features can be removed

71
04:08:26 --> 04:08:29
without negatively affecting
the analysis results.

72
04:08:30 --> 04:08:33
For example,
the purchase price of a product and

73
04:08:33 --> 04:08:37
the amount of sales tax paid,
are likely to be correlated.

74
04:08:38 --> 04:08:42
Eliminating the sales tax amount,
then will be beneficial.

75
04:08:44 --> 04:08:46
Removing redundant or

76
04:08:46 --> 04:08:51
irrelevant features will make
the subsequent analysis much simpler.

77
04:08:53 --> 04:08:59
In other cases, you may want to combine
features or create new ones.

78
04:08:59 --> 04:09:03
For example,
adding the applicant's education level

79
04:09:03 --> 04:09:06
as a feature to a loan approval
application would make sense.

80
04:09:08 --> 04:09:13
There are also algorithms to automatically
determine the most relevant features,

81
04:09:13 --> 04:09:16
based on various mathematical properties.

82
04:09:19 --> 04:09:24
Dimensionality reduction is useful when
the data set has a large number of

83
04:09:24 --> 04:09:24
dimensions.

84
04:09:26 --> 04:09:29
It involves finding a smaller
subset of dimensions that

85
04:09:29 --> 04:09:32
captures most of
the variation in the data.

86
04:09:34 --> 04:09:37
This reduces the dimensions of the data

87
04:09:37 --> 04:09:42
while eliminating irrelevant features and
makes analysis simpler.

88
04:09:44 --> 04:09:46
A technique commonly used for

89
04:09:46 --> 04:09:50
dimensional reduction is called
principle component analysis or PCA.

90
04:09:54 --> 04:10:00
Raw data often has to be manipulated to
be in the correct format for analysis.

91
04:10:01 --> 04:10:06
For example, from samples recording
daily changes in stock prices,

92
04:10:06 --> 04:10:09
we may want the capture price changes for

93
04:10:09 --> 04:10:12
a particular market segments
like real estate or health care.

94
04:10:14 --> 04:10:19
This would require determining which
stocks belong to which market segment.

95
04:10:19 --> 04:10:23
Grouping them together, and
perhaps computing the mean, range,

96
04:10:23 --> 04:10:25
standard deviation for each group.

97
04:10:28 --> 04:10:28
In summary,

98
04:10:29 --> 04:10:34
data preparation is a very important
part of the data science process.

99
04:10:34 --> 04:10:39
In fact, this is where you will spend most
of your time on any data science effort.

100
04:10:40 --> 04:10:45
It can be a tedious process,
but it is a crucial step.

101
04:10:45 --> 04:10:49
Always remember, garbage in, garbage out.

102
04:10:49 --> 04:10:50
If you don't spend the time and

103
04:10:50 --> 04:10:55
effort to create good data for the
analysis, you will not get good results

104
04:10:55 --> 04:10:59
no matter how sophisticated
the analysis technique you're using is.

1
08:13:33 --> 08:13:36
Step 3: Analyzing Data.

2
08:13:45 --> 08:13:48
Now that you have your
data nicely prepared,

3
08:13:48 --> 08:13:51
the next step is to analyze the data.

4
08:13:52 --> 08:13:56
Data analysis involves building
a model from your data,

5
08:13:56 --> 08:13:58
which is called input data.

6
08:13:59 --> 08:14:05
The input data is used by the analysis
technique to build a model.

7
08:14:07 --> 08:14:11
What your model generates
is the output data.

8
08:14:11 --> 08:14:14
There are different types of problems, and

9
08:14:14 --> 08:14:16
so there are different types
of analysis techniques.

10
08:14:18 --> 08:14:23
The main categories of analysis techniques
are classification, regression,

11
08:14:23 --> 08:14:29
clustering, association analysis,
and graph analysis.

12
08:14:29 --> 08:14:31
We will describe each one.

13
08:14:31 --> 08:14:36
In classification, the goal is to
predict the category of the input data.

14
08:14:37 --> 08:14:43
An example of this is predicting
the weather as being sunny,

15
08:14:43 --> 08:14:46
rainy, windy, or cloudy in this case.

16
08:14:46 --> 08:14:52
Another example is to classify
a tumor as either benign or malignant.

17
08:14:54 --> 08:15:00
In this case, the classification is
referred to as binary classification,

18
08:15:00 --> 08:15:02
since there are only two categories.

19
08:15:02 --> 08:15:05
But you can have many categories as well,

20
08:15:05 --> 08:15:10
as the weather prediction problem
shown here having four categories.

21
08:15:10 --> 08:15:14
Another example is to identify
handwritten digits as

22
08:15:14 --> 08:15:18
being in one of the ten
categories from zero to nine.

23
08:15:21 --> 08:15:26
When your model has to predict
a numeric value instead of a category,

24
08:15:26 --> 08:15:29
then the task becomes
a regression problem.

25
08:15:30 --> 08:15:34
An example of regression is to
predict the price of a stock.

26
08:15:35 --> 08:15:39
The stock price is a numeric value,
not a category.

27
08:15:39 --> 08:15:42
So this is a regression task
instead of a classification task.

28
08:15:44 --> 08:15:48
Other examples of regression
are estimating the weekly sales of a new

29
08:15:48 --> 08:15:52
product and
predicting the score on a test.

30
08:15:52 --> 08:15:58
In clustering, the goal is to
organize similar items into groups.

31
08:15:58 --> 08:16:04
An example is grouping a company's
customer base into distinct segments for

32
08:16:04 --> 08:16:09
more effective targeted marketing
like seniors, adults and

33
08:16:09 --> 08:16:11
teenagers, as we see here.

34
08:16:11 --> 08:16:16
Another such example is identifying
areas of similar topography,

35
08:16:16 --> 08:16:20
like mountains, deserts,
plains for land use application.

36
08:16:20 --> 08:16:25
Yet another example is determining
different groups of weather patterns,

37
08:16:25 --> 08:16:28
like rainy, cold or snowy.

38
08:16:28 --> 08:16:31
The goal in association analysis
is to come up with a set

39
08:16:31 --> 08:16:36
of rules to capture associations
within items or events.

40
08:16:36 --> 08:16:40
The rules are used to determine when
items or events occur together.

41
08:16:40 --> 08:16:45
A common application of association
analysis is known as market

42
08:16:45 --> 08:16:50
basket analysis, which is used to
understand customer purchasing behavior.

43
08:16:50 --> 08:16:55
For example, association analysis
can reveal that banking customers

44
08:16:55 --> 08:17:00
who have certificate of deposit accounts,
surety CDs, also

45
08:17:00 --> 08:17:04
tend to be interested in other investment
vehicles, such as money market accounts.

46
08:17:04 --> 08:17:08
This information can be used for
cross-selling.

47
08:17:08 --> 08:17:12
If you advertise money market
accounts to your customers with CDs,

48
08:17:12 --> 08:17:15
they're likely to open such an account.

49
08:17:15 --> 08:17:20
According to data mining folklore,
a supermarket chain used association

50
08:17:20 --> 08:17:24
analysis to discover a connection between
two seemingly unrelated products.

51
08:17:24 --> 08:17:29
They discovered that many customers who go
to the supermarket late on Sunday night

52
08:17:29 --> 08:17:35
to buy diapers also tend to buy beer,
who are likely to be fathers.

53
08:17:35 --> 08:17:38
This information was then
used to place beer and

54
08:17:38 --> 08:17:43
diapers close together and
they saw a jump in sales of both items.

55
08:17:43 --> 08:17:46
This is the famous diaper beer connection.

56
08:17:46 --> 08:17:51
When your data can be transformed into
a graph representation with nodes and

57
08:17:51 --> 08:17:55
links, then you want to use graph
analytics to analyze your data.

58
08:17:55 --> 08:17:59
This kind of data comes about when
you have a lot of entities and

59
08:17:59 --> 08:18:03
connections between those entities,
like social networks.

60
08:18:03 --> 08:18:08
Some examples where graph analytics can
be useful are exploring the spread of

61
08:18:08 --> 08:18:12
a disease or epidemic by analyzing
hospitals' and doctors' records.

62
08:18:12 --> 08:18:17
Identification of security threats
by monitoring social media,

63
08:18:17 --> 08:18:19
email and text data.

64
08:18:19 --> 08:18:24
And optimization of mobile
communications network traffic.

65
08:18:24 --> 08:18:29
And optimization of mobile
telecommunications network traffic,

66
08:18:29 --> 08:18:32
to ensure call quality and
reduce dropped calls.

67
08:18:32 --> 08:18:36
Modeling starts with selecting,
one of the techniques we listed

68
08:18:36 --> 08:18:41
as the appropriate analysis technique,
depending on the type of problem you have.

69
08:18:41 --> 08:18:45
Then you construct the model
using the data you've prepared.

70
08:18:45 --> 08:18:50
To validate the model,
you apply it to new data samples.

71
08:18:50 --> 08:18:52
This is to evaluate how well the model

72
08:18:52 --> 08:18:55
does on data that was
used to construct it.

73
08:18:55 --> 08:18:59
The common practice is to divide
the prepared data into a set of data for

74
08:18:59 --> 08:19:03
constructing the model and
reserving some of the data for

75
08:19:03 --> 08:19:06
evaluating the model after
it has been constructed.

76
08:19:06 --> 08:19:10
You can also use new data prepared the
same way as with the data that was used to

77
08:19:10 --> 08:19:12
construct model.

78
08:19:12 --> 08:19:16
Evaluating the model depends on the type
of analysis techniques you used.

79
08:19:16 --> 08:19:20
Let's briefly look at how
to evaluate each technique.

80
08:19:20 --> 08:19:25
For classification and regression,
you will have the correct output for

81
08:19:25 --> 08:19:27
each sample in your input data.

82
08:19:27 --> 08:19:29
Comparing the correct output and

83
08:19:29 --> 08:19:34
the output predicted by the model,
provides a way to evaluate the model.

84
08:19:34 --> 08:19:38
For clustering,
the groups resulting from clustering

85
08:19:38 --> 08:19:41
should be examined to see if they
make sense for your application.

86
08:19:41 --> 08:19:46
For example, do the customer
segments reflect your customer base?

87
08:19:46 --> 08:19:50
Are they helpful for
use in your targeted marketing campaigns?

88
08:19:50 --> 08:19:52
For association analysis and

89
08:19:52 --> 08:19:57
graph analysis, some investigation will be
needed to see if the results are correct.

90
08:19:57 --> 08:20:00
For example, network traffic delays

91
08:20:00 --> 08:20:06
need to be investigated to see what your
model predicts is actually happening.

92
08:20:06 --> 08:20:09
And whether the sources of the delays
are where they are predicted

93
08:20:09 --> 08:20:11
to be in the real system.

94
08:20:12 --> 08:20:17
After you have evaluated your model to get
a sense of its performance on your data,

95
08:20:17 --> 08:20:20
you will be able to
determine the next steps.

96
08:20:20 --> 08:20:24
Some questions to consider are,
should the analysis be

97
08:20:24 --> 08:20:28
performed with more data in order
to get a better model performance?

98
08:20:28 --> 08:20:30
Would using different data types help?

99
08:20:30 --> 08:20:32
For example, in your clustering results,

100
08:20:32 --> 08:20:37
is it difficult to distinguish
customers from distinct regions?

101
08:20:37 --> 08:20:40
Would adding zip code
to your input data help

102
08:20:40 --> 08:20:43
to generate finer grained
customer segments?

103
08:20:43 --> 08:20:47
Do the analysis results suggest
a more detailed look at

104
08:20:47 --> 08:20:48
some aspect of the problem?

105
08:20:48 --> 08:20:52
For example, predicting sunny
weather gives very good results,

106
08:20:52 --> 08:20:55
but rainy weather
predictions are just so-so.

107
08:20:55 --> 08:21:00
This means that you should take a closer
look at your examples for rainy weather.

108
08:21:00 --> 08:21:04
Perhaps you just need more
samples of rainy weather, or

109
08:21:04 --> 08:21:07
perhaps there are some
anomalies in those samples.

110
08:21:07 --> 08:21:13
Or maybe there are some missing data
that needs to be included in order

111
08:21:13 --> 08:21:15
to completely capture rainy weather.

112
08:21:15 --> 08:21:20
The ideal situation would be that your
model platforms very well with respect to

113
08:21:20 --> 08:21:22
the success criteria that were determined

114
08:21:22 --> 08:21:26
when you defined the problem at
the beginning of the project.

115
08:21:26 --> 08:21:29
In that case, you're ready to
move on to communicating and

116
08:21:29 --> 08:21:33
acting on the results that you
obtained from your analysis.

117
08:21:33 --> 08:21:38
As a summary, data analysis involves
selecting the appropriate technique for

118
08:21:38 --> 08:21:42
your problem, building the model,
then evaluating the results.

119
08:21:44 --> 08:21:46
As there are different types of problems,

120
08:21:46 --> 08:21:49
there are also different
types of analysis techniques.

1
16:35:24 --> 16:35:27
Step four, reporting insights.

2
16:35:37 --> 16:35:42
The fourth step in our data science
process is reporting the insights gained

3
16:35:42 --> 16:35:43
from our analysis.

4
16:35:45 --> 16:35:49
This is a very important step
to communicate your insights and

5
16:35:49 --> 16:35:52
make a case for
what actions should follow.

6
16:35:54 --> 16:35:59
It can change shape based on your
audience and should not be taken lightly.

7
16:36:00 --> 16:36:01
So how do you get started?

8
16:36:05 --> 16:36:10
The first thing to do is to look
at your analysis results and

9
16:36:10 --> 16:36:15
decide what to present or report as the
biggest value or biggest set of values.

10
16:36:16 --> 16:36:21
In deciding what to present you
should ask yourself these questions.

11
16:36:23 --> 16:36:24
What is the punchline?

12
16:36:24 --> 16:36:27
In other words, what are the main results?

13
16:36:29 --> 16:36:34
What added value do
these results provide or

14
16:36:34 --> 16:36:36
how can the model add to the application?

15
16:36:39 --> 16:36:43
How do the results compare to
the success criteria determined at

16
16:36:43 --> 16:36:45
the beginning of the project?

17
16:36:48 --> 16:36:52
Answers to these questions are the items
you need to include in your report or

18
16:36:52 --> 16:36:52
presentation.

19
16:36:54 --> 16:36:58
So make them the main topics and
gather facts to back them up.

20
16:37:01 --> 16:37:06
Keep in mind that not all of
your results may be rosy.

21
16:37:06 --> 16:37:11
Your analysis may show results that are
counter to what you were hoping to find,

22
16:37:11 --> 16:37:15
or results that are inconclusive or
puzzling.

23
16:37:16 --> 16:37:18
You need to show these results as well.

24
16:37:20 --> 16:37:24
Domain experts may find some of
these results to be puzzling, and

25
16:37:24 --> 16:37:28
inconclusive findings may
lead to additional analysis.

26
16:37:30 --> 16:37:33
Remember the point of
reporting your findings

27
16:37:33 --> 16:37:36
is to determine what
the next step should be.

28
16:37:39 --> 16:37:43
All findings must be presented so
that informed decisions can be made.

29
16:37:46 --> 16:37:51
Visualization is an important
tool in presenting your results.

30
16:37:52 --> 16:37:57
The techniques that we discuss and
explore in data can be used here as well.

31
16:37:57 --> 16:37:58
What were they?

32
16:37:58 --> 16:38:03
Scatter plots, line graphs,
heat maps, and other types of graphs

33
16:38:05 --> 16:38:08
are effective ways to present
your results visually.

34
16:38:10 --> 16:38:14
This time you're not
plotting the input data, but

35
16:38:14 --> 16:38:17
you're plotting the output
data with similar tools.

36
16:38:19 --> 16:38:23
You should also have tables with
details from your analysis as backups,

37
16:38:23 --> 16:38:26
if someone wants to take
a deeper dive into the results.

38
16:38:29 --> 16:38:32
There are many visualization
tools that are available.

39
16:38:33 --> 16:38:36
Some of the most popular open
source ones are listed here.

40
16:38:38 --> 16:38:42
R is a software package for
general data analysis.

41
16:38:43 --> 16:38:46
It has powerful visualization
capabilities as well.

42
16:38:48 --> 16:38:52
Python is a general purpose
programming language

43
16:38:52 --> 16:38:57
that also has a number of packages to
support data analysis and graphics.

44
16:38:58 --> 16:39:01
D3 is a JavaScript library for

45
16:39:01 --> 16:39:06
producing interactive web based
visualizations and data driven documents.

46
16:39:08 --> 16:39:13
Leaflet is a lightweight mobile
friendly JavaScript library

47
16:39:13 --> 16:39:15
to create interactive maps.

48
16:39:17 --> 16:39:22
Tableau Public Allows you
to create visualizations,

49
16:39:22 --> 16:39:29
in your public profile, and share them,
or put them, on a site, or blog.

50
16:39:29 --> 16:39:34
Google Charts provides
cross-browser compatibility,

51
16:39:34 --> 16:39:39
and closed platform portability
to iPhones and Android.

52
16:39:41 --> 16:39:49
Timeline is a JavaScript library
that allows you to create timelines.

53
16:39:49 --> 16:39:55
In summary, you want to report your
findings by presenting your results and

54
16:39:55 --> 16:39:59
value add with graphs
using visualization tools.

1
09:15:23 --> 09:15:26
Step 5, Act, turning insights into action.

2
09:15:36 --> 09:15:41
Now that you have evaluated
the results from your analysis and

3
09:15:41 --> 09:15:46
generated reports on the potential value
of the results, the next step is to

4
09:15:46 --> 09:15:52
determine what action or actions should
be taken, based on the insights gained?

5
09:15:52 --> 09:15:55
Remember why we started
bringing together the data and

6
09:15:55 --> 09:15:57
analyzing it in the first place?

7
09:15:57 --> 09:16:02
To find actionable insights
within all these data sets,

8
09:16:02 --> 09:16:05
to answer questions, or for
improving business processes.

9
09:16:07 --> 09:16:08
For example,

10
09:16:08 --> 09:16:12
is there something in your process that
should change to remove bottle necks?

11
09:16:13 --> 09:16:17
Is there data that should be added to your
application to make it more accurate?

12
09:16:18 --> 09:16:23
Should you segment your population
into more well defined groups for

13
09:16:23 --> 09:16:25
more effective targeted marketing?

14
09:16:26 --> 09:16:30
This is the first step in
turning insights into action.

15
09:16:30 --> 09:16:33
Now that you've determined
what action to take,

16
09:16:33 --> 09:16:38
the next step is figuring out
how to implement the action.

17
09:16:38 --> 09:16:42
What is necessary to add this action
into your process or application?

18
09:16:43 --> 09:16:45
How should it be automated?

19
09:16:46 --> 09:16:52
The stakeholders need to be identified and
become involved in this change.

20
09:16:52 --> 09:16:57
Just as with any process improvement
changes, we need to monitor and

21
09:16:57 --> 09:17:01
measure the impact of the action
on the process or application.

22
09:17:02 --> 09:17:06
Assessing the impact
leads to an evaluation.

23
09:17:06 --> 09:17:10
Evaluating results from the implemented
action will determine your next steps.

24
09:17:11 --> 09:17:16
Is there additional analysis that need
to be performed in order to yield

25
09:17:16 --> 09:17:17
even better results?

26
09:17:19 --> 09:17:20
What data should be revisited?

27
09:17:21 --> 09:17:25
Are there additional opportunities
that should be explored?

28
09:17:25 --> 09:17:30
For example, let's not forget
what big data enables us to do.

29
09:17:30 --> 09:17:37
Real-time actions based on high
velocity streaming information.

30
09:17:37 --> 09:17:42
We need to define what part of our
business needs real-time action to be

31
09:17:42 --> 09:17:46
able to influence the operations or
the interaction with the customer.

32
09:17:48 --> 09:17:53
Once we define these real time actions,
we need to make sure that

33
09:17:53 --> 09:17:58
there are automated systems, or
processes to perform such actions, and

34
09:17:58 --> 09:18:02
provide failure recovery
in case of problems.

35
09:18:02 --> 09:18:04
As a summary, big data and

36
09:18:04 --> 09:18:10
data science are only useful if
the insights can be turned into action,

37
09:18:10 --> 09:18:15
and if the actions are carefully
defined and evaluated.

1
18:33:37 --> 18:33:39
Steps in the Data Science Process.

2
18:33:47 --> 18:33:52
We have already seen a simple linear
form of data science process,

3
18:33:52 --> 18:33:57
including five distinct activities
that depend on each other.

4
18:33:59 --> 18:34:04
Let's summarize each activity further
before we go into the details of each.

5
18:34:04 --> 18:34:10
Acquire includes anything that makes
us retrieve data including; finding,

6
18:34:10 --> 18:34:15
accessing, acquiring, and moving data.

7
18:34:15 --> 18:34:21
It includes identification of and
authenticated access to all related data.

8
18:34:21 --> 18:34:26
And transportation of data from
sources to distributed files systems.

9
18:34:27 --> 18:34:33
It includes way to subset and match
the data to regions or times of interest.

10
18:34:33 --> 18:34:36
As we sometimes refer to
it as geo-spacial query.

11
18:34:38 --> 18:34:44
The next activity is prepare data,
we divide the pre-data activity.

12
18:34:44 --> 18:34:47
Into two steps based on
the nature of the activity.

13
18:34:47 --> 18:34:52
Namely, explore data and pre-process data.

14
18:34:52 --> 18:34:57
The first step in data preparation
involves literally looking

15
18:34:57 --> 18:35:02
at the data to understand its nature,
what it means, its quality and format.

16
18:35:04 --> 18:35:08
It often takes a preliminary
analysis of data, or

17
18:35:08 --> 18:35:10
samples of data, to understand it.

18
18:35:11 --> 18:35:14
This is why this step is called explore.

19
18:35:15 --> 18:35:19
Once we know more about the data
through exploratory analysis,

20
18:35:19 --> 18:35:22
the next step is pre-processing
of data for analysis.

21
18:35:22 --> 18:35:28
Pre-processing includes cleaning data,
sub-setting or

22
18:35:28 --> 18:35:33
filtering data, creating data,
which programs can read and

23
18:35:33 --> 18:35:39
understand, such as modeling raw
data into a more defined data model,

24
18:35:39 --> 18:35:43
or packaging it using
a specific data format.

25
18:35:44 --> 18:35:47
If there are multiple data sets involved,

26
18:35:47 --> 18:35:53
this step also includes integration
of multiple data sources, or streams.

27
18:35:53 --> 18:35:58
The prepared data then would be
passed onto the analysis step,

28
18:35:58 --> 18:36:00
which involves selection of
analytical techniques to use,

29
18:36:01 --> 18:36:05
building a model of the data,
and analyzing results.

30
18:36:06 --> 18:36:10
This step can take a couple
of iterations on its own or

31
18:36:10 --> 18:36:13
might require data scientists
to go back to steps one and

32
18:36:13 --> 18:36:18
two to get more data or
package data in a different way.

33
18:36:18 --> 18:36:24
Step four for communicating results
includes evaluation of analytical results.

34
18:36:24 --> 18:36:28
Presenting them in a visual way,
creating reports that include

35
18:36:28 --> 18:36:32
an assessment of results with
respect to success criteria.

36
18:36:33 --> 18:36:37
Activities in this step can often be
referred to with terms like interpret,

37
18:36:37 --> 18:36:40
summarize, visualize, or post process.

38
18:36:41 --> 18:36:46
The last step brings us back to the very
first reason we do data science,

39
18:36:47 --> 18:36:48
the purpose.

40
18:36:49 --> 18:36:54
Reporting insights from analysis and
determining actions from insights based

41
18:36:54 --> 18:36:59
on the purpose you initially defined
is what we refer to as the act step.

42
18:37:00 --> 18:37:05
We have now seen all the steps in
a typical data science process.

43
18:37:06 --> 18:37:11
Please note that this is an iterative
process and findings from one step

44
18:37:11 --> 18:37:14
may require the previous step to
be repeated with new information.

1
13:10:52 --> 13:10:56
The Sixth V, Value.

2
13:10:56 --> 13:10:57
In this module,

3
13:10:57 --> 13:11:03
we described the five ways which are
considered to be dimensions of big data.

4
13:11:04 --> 13:11:09
Each way presented a challenging
dimension of big data namely,

5
13:11:09 --> 13:11:15
size, complexity, speed,
quality, and connectedness.

6
13:11:17 --> 13:11:22
Although we can list some other rays base
on the context, we prefer to list these

7
13:11:22 --> 13:11:28
five s fundamental dimensions that this
big data specialization helps you work on.

8
13:11:29 --> 13:11:33
However, at the heart of
the big data challenge

9
13:11:33 --> 13:11:38
is turning all of the other dimensions
into truly useful business value.

10
13:11:40 --> 13:11:44
The idea behind processing all
this big data in the first place

11
13:11:44 --> 13:11:46
is to bring value to the problem at hand.

12
13:11:47 --> 13:11:52
In week two we will explore how
to take the first steps into

13
13:11:52 --> 13:11:55
starting to generate
value out of big data.

14
13:11:56 --> 13:12:02
Now that we saw all the ways, let's focus
on an example of a big data challenge.

15
13:12:02 --> 13:12:06
Let's imagine now that you're part
of a company called Eglence Inc.

16
13:12:08 --> 13:12:12
One of the products of Eglence Inc
is a highly popular mobile game

17
13:12:12 --> 13:12:15
called Catch the Pink Flamingo.

18
13:12:15 --> 13:12:20
It's a multi-user game where
the users have to catch special types

19
13:12:20 --> 13:12:24
of pink flamingos that randomly
pop up on the world map on their

20
13:12:24 --> 13:12:28
screens based on the mission
that gets updated randomly.

21
13:12:28 --> 13:12:33
The game is played by millions of
people online throughout the world.

22
13:12:33 --> 13:12:37
One of the goals of the game is to form
a network of players to collectively

23
13:12:37 --> 13:12:42
cover the world map with pink flamingo
sightings and compete other groups.

24
13:12:42 --> 13:12:45
Users can pick their groups
based on player stats.

25
13:12:46 --> 13:12:51
The game's website sends free
cool stuff to registered users.

26
13:12:51 --> 13:12:57
Registration requires users to enter
demographic information such gender,

27
13:12:57 --> 13:13:02
year of birth, city, highest education,
and things like that.

28
13:13:02 --> 13:13:07
However, most of the users enter
inaccurate information about themselves,

29
13:13:07 --> 13:13:10
just like most of us do.

30
13:13:10 --> 13:13:14
To help improve the game,
the game collects realtime usage activity

31
13:13:14 --> 13:13:19
data from each player and
feeds them to it's data servers.

32
13:13:19 --> 13:13:25
The players of this game are
enthusiastically active on social media,

33
13:13:25 --> 13:13:28
and have strong
associations with the game.

34
13:13:28 --> 13:13:33
A popular Twitter hashtag for
this game is, CatchThePinkFlamingo,

35
13:13:33 --> 13:13:38
which gets more than 200,000
mentions worldwide per day.

36
13:13:38 --> 13:13:43
There are strong communities of
users who meet via social media and

37
13:13:43 --> 13:13:45
get together to play the game.

38
13:13:45 --> 13:13:52
Now, imagine yourself as the big data
solutions architect for Fun Games Inc.

39
13:13:52 --> 13:13:57
There are definitely examples of all three
types of data sources in this example.

40
13:13:58 --> 13:14:03
The mobile app generates data for
the analysis of user activity.

41
13:14:03 --> 13:14:08
Twitter conversations of players form
a rich source of unstructured data from

42
13:14:08 --> 13:14:09
people.

43
13:14:09 --> 13:14:10
And the customer and

44
13:14:10 --> 13:14:14
game records are examples of data
that this organization collects.

45
13:14:16 --> 13:14:19
This is a challenging big data example

46
13:14:19 --> 13:14:22
where all characteristics of
big data are represented.

47
13:14:22 --> 13:14:25
There are high volumes of player, game and

48
13:14:25 --> 13:14:29
Twitter data,
which also speaks to the variety of data.

49
13:14:29 --> 13:14:33
The data streams from the mobile app,
website, and

50
13:14:33 --> 13:14:38
social media in real-time, which can
be defined as high velocity data.

51
13:14:38 --> 13:14:43
The quality of demographic data
users enter is not clear, and

52
13:14:43 --> 13:14:48
there are networks of players which
are related to the balance of big data.

1
02:25:40 --> 02:25:43
Cloud Computing: An Important
Big Data Enabler.

2
02:25:58 --> 02:26:03
In our first lecture in this course,
we mentioned the cloud as one of

3
02:26:03 --> 02:26:07
the two influences of
the launch of the big data era.

4
02:26:08 --> 02:26:12
We called it on-demand computing, and

5
02:26:12 --> 02:26:16
we said that it enables us to
compute any time any anywhere.

6
02:26:16 --> 02:26:18
Simply, whenever we demand it.

7
02:26:19 --> 02:26:20
In this video,

8
02:26:20 --> 02:26:25
we will see how we deploy the cloud to
our benefit in our big data applications.

9
02:26:26 --> 02:26:28
The main idea behind cloud computing

10
02:26:28 --> 02:26:33
is to transform computing
infrastructure into a commodity.

11
02:26:33 --> 02:26:38
So application developers can focus on
solving application-specific challenges

12
02:26:38 --> 02:26:41
instead of trying to build
infrastructure to run on.

13
02:26:41 --> 02:26:43
So how does this happen?

14
02:26:43 --> 02:26:46
We can simply define
a cloud computing service,

15
02:26:46 --> 02:26:49
as a rental service for computing.

16
02:26:49 --> 02:26:52
You rent what you want,
and return upon usage.

17
02:26:53 --> 02:26:56
Think about this, you wouldn't buy, or

18
02:26:56 --> 02:27:01
even build, a truck every time you
have to move a piece of furniture.

19
02:27:01 --> 02:27:02
You would simply rent.

20
02:27:04 --> 02:27:07
Why build a computing
cluster when you can rent?

21
02:27:07 --> 02:27:10
Especially if you are not
using it all the time.

22
02:27:13 --> 02:27:17
Similarly, you can rent a car or
a bike when you are on vacation.

23
02:27:17 --> 02:27:21
So you can bike anytime, anywhere.

24
02:27:22 --> 02:27:24
Let's dig into this question.

25
02:27:24 --> 02:27:29
What factors do you consider when you're
developing a solution for your yourself or

26
02:27:29 --> 02:27:29
your client?

27
02:27:30 --> 02:27:33
Should you build a hardware and
software resources yourself?

28
02:27:34 --> 02:27:39
Or should you rent these
resources from the cloud?

29
02:27:40 --> 02:27:44
Let's look at in-house hardware and
software resource building first.

30
02:27:46 --> 02:27:50
If you choose to develop in-house
capabilities, you have to hire people and

31
02:27:50 --> 02:27:53
buy hardware that suits your requirements.

32
02:27:54 --> 02:28:00
These includes, but not limited to,
buying networking hardware,

33
02:28:00 --> 02:28:07
storage disks, upgrading hardware
when it becomes obsolete, and so on.

34
02:28:07 --> 02:28:10
Not to forget, the real estate
cost of keeping the hardware.

35
02:28:11 --> 02:28:14
How do you estimate the size
of your hardware needs?

36
02:28:14 --> 02:28:17
Do you make a five year estimate,
or ten year?

37
02:28:19 --> 02:28:22
In today's fast changing world,

38
02:28:22 --> 02:28:24
it is becoming harder to
estimate future demands.

39
02:28:26 --> 02:28:30
Getting the software that fits
your needs is equally challenging.

40
02:28:30 --> 02:28:33
Most software installations
require a lot of tweaking and

41
02:28:33 --> 02:28:36
manual intervention that
require a lot of skills.

42
02:28:37 --> 02:28:40
You will need your engineers to do this.

43
02:28:40 --> 02:28:43
Compatibility issues bring
problems that are hard to foresee.

44
02:28:44 --> 02:28:47
Most software is updated on a daily basis.

45
02:28:47 --> 02:28:50
You must ensure you're updated.

46
02:28:50 --> 02:28:54
This insures you avoid security risks and
get the best.

47
02:28:56 --> 02:29:03
Over all, building your own data center or
computing power house can be expensive.

48
02:29:03 --> 02:29:10
And it can be time consuming,
maintaining it is a task by itself.

49
02:29:10 --> 02:29:13
This requires high initial
capital investments and

50
02:29:13 --> 02:29:17
efficient operation of several
departments in your business,

51
02:29:17 --> 02:29:20
which you might not have if
you are a startup company.

52
02:29:20 --> 02:29:24
Most people forget to include
the cost of disposing old hardware.

53
02:29:24 --> 02:29:26
Now lets see what the cloud can do for us.

54
02:29:28 --> 02:29:33
Cloud's benefits are similar to what you
would get from a rental car company.

55
02:29:35 --> 02:29:39
You pay for what you use,
which means a low capital investment.

56
02:29:41 --> 02:29:45
You don't need to go to the dealership,
do a negotiation, get a bank loan,

57
02:29:45 --> 02:29:46
get insurance.

58
02:29:47 --> 02:29:50
That means quick implementation
of your projects.

59
02:29:51 --> 02:29:55
Just like you don't need to buy
a car if you only need a car for

60
02:29:55 --> 02:30:00
a limited use, deploying your application
on a server that is geographically

61
02:30:00 --> 02:30:05
closer to your client can give you
fast service and happy customers.

62
02:30:07 --> 02:30:12
For startup and small business,
it can be challenging to do so.

63
02:30:12 --> 02:30:14
Cloud lets you do this with a click.

64
02:30:16 --> 02:30:20
You can be sitting at a coffeeshop or your
home and starting your Internet business,

65
02:30:20 --> 02:30:24
without a huge capital investment,
thanks to the cloud.

66
02:30:24 --> 02:30:29
And you don't need to have a five or
ten year resource estimation plan.

67
02:30:29 --> 02:30:33
Adapt to your requirements faster if
your business is growing faster than you

68
02:30:33 --> 02:30:34
thought.

69
02:30:35 --> 02:30:39
Cloud lets you forget about
the resource management problems and

70
02:30:39 --> 02:30:44
lets you focus on your business's products
or domain expertise with minimal cost.

71
02:30:45 --> 02:30:50
Just as you can rent a truck or
a convertible at a rental car company,

72
02:30:50 --> 02:30:53
you can build your own
custom machine on cloud.

73
02:30:53 --> 02:30:57
With a custom machine,
we mean a commodity cluster

74
02:30:57 --> 02:31:00
made out of the right type of
computing nodes for your application.

75
02:31:02 --> 02:31:05
You pick not only a CPU or a GPU, but

76
02:31:05 --> 02:31:10
pick from a whole menu of compute,
memory and storage choices.

77
02:31:10 --> 02:31:12
It's a buffet on the cloud.

78
02:31:12 --> 02:31:17
Design machines to suit your application
requirements, data size and analytics.

79
02:31:19 --> 02:31:21
Get what you want, and
pay for what you use.

80
02:31:23 --> 02:31:25
Compare this with buying and

81
02:31:25 --> 02:31:29
maintaining all combinations of
hardware that you possibly would use.

82
02:31:29 --> 02:31:33
That is so costly and
not possible at all times.

83
02:31:35 --> 02:31:40
Thanks to all these advantages, there
are many cloud server providers today.

84
02:31:40 --> 02:31:41
And the numbers are growing.

85
02:31:43 --> 02:31:46
Here we list some of the players
in the cloud computing market.

86
02:31:48 --> 02:31:50
Take a moment to look at them.

87
02:31:50 --> 02:31:53
You will probably recognize
some big names and

88
02:31:53 --> 02:31:56
some others you have not
even heard of before.

89
02:31:58 --> 02:32:03
As a summary, cloud does the heavy
lifting, so your team can extract

90
02:32:03 --> 02:32:08
value from data with getting bogged
down in the infrastructure details.

91
02:32:09 --> 02:32:11
Cloud provides convenient and

92
02:32:11 --> 02:32:16
viable solutions for scaling your
prototype to a full fledged application.

93
02:32:16 --> 02:32:21
You can leverage the experts
to handle security,

94
02:32:21 --> 02:32:24
robustness, and
let them handle the technical issues.

95
02:32:25 --> 02:32:29
Your team can work on
utilizing your strengths

96
02:32:29 --> 02:32:32
to solve your domain specific problem.

1
04:58:12 --> 04:58:15
Cloud Service Models: Exploration
of Choices.

2
04:58:30 --> 04:58:34
There are many levels of services that
you can get from cloud providers.

3
04:58:35 --> 04:58:39
Any cloud computing discussion will
involve terms like application

4
04:58:39 --> 04:58:44
as a service, platform as a service,
and infrastructure as a service.

5
04:58:46 --> 04:58:50
All of these refer to business
models around using the cloud

6
04:58:50 --> 04:58:55
with different levels of engagement and
servicing similar to rental agreements.

7
04:58:57 --> 04:59:05
IaaS, infrastructure as a service, can be
defined as a bare minimum rental service.

8
04:59:07 --> 04:59:12
This is like renting a truck from
a company that you can assume has hardware

9
04:59:12 --> 04:59:17
and you do the packing of your furniture,
and drive to your new house.

10
04:59:21 --> 04:59:25
You as the user of the service install and
maintain an operating system,

11
04:59:25 --> 04:59:29
and other applications in
the infrastructure as a service model.

12
04:59:30 --> 04:59:34
The Amazon EC2 cloud is a good example for
this model.

13
04:59:36 --> 04:59:39
PaaS, platform as a service,

14
04:59:39 --> 04:59:45
is the model where a user is provided
with an entire computing platform.

15
04:59:45 --> 04:59:50
This could include the operating system
and programming languages that you need.

16
04:59:52 --> 04:59:57
It could extend to include the database
of your choice, or even a web server.

17
04:59:57 --> 04:59:58
You can develop, and

18
04:59:58 --> 05:00:02
run your own application software,
on top of these layers.

19
05:00:04 --> 05:00:09
The Google App engine and Microsoft Azure
are two examples of this model.

20
05:00:11 --> 05:00:17
SaaS, the software as a service model,
is the model

21
05:00:17 --> 05:00:23
in which the cloud service provider takes
the responsibilities for the hardware and

22
05:00:23 --> 05:00:28
software environment such as the operating
system and the application software.

23
05:00:29 --> 05:00:35
This means you can work on using
the application to solve your problem.

24
05:00:38 --> 05:00:41
Dropbox is a very popular
software as a service platform.

25
05:00:44 --> 05:00:49
Ultimately, the decision of which service
you want to explore is a function of

26
05:00:49 --> 05:00:50
several variables.

27
05:00:51 --> 05:00:55
It depends on the skill level of your
team to handle computing environment,

28
05:00:55 --> 05:00:58
development and maintenance.

29
05:00:58 --> 05:01:01
It also depends on how you
might need to use the service.

30
05:01:03 --> 05:01:05
You need to pick the right
service model that

31
05:01:05 --> 05:01:08
best fits you in terms of long term goals.

32
05:01:09 --> 05:01:14
Finally, when you're deploying a cloud
service you also have to understand

33
05:01:14 --> 05:01:19
all the security risks, since your
data resides on third party service.

34
05:01:21 --> 05:01:26
Security is a very important
aspect in today's world of growing

35
05:01:26 --> 05:01:28
digitization of information.

36
05:01:28 --> 05:01:31
You must make your client's
data safety a top priority,

37
05:01:31 --> 05:01:35
and hence this should be an important
criteria in your decision.

38
05:01:37 --> 05:01:41
All the security risks
must be understood and

39
05:01:41 --> 05:01:45
evaluated as your data resides
on third party servers.

40
05:01:47 --> 05:01:50
We are seeing other forms
of services being added

41
05:01:50 --> 05:01:52
to the family of cloud services.

42
05:01:53 --> 05:01:55
The logic of infrastructure, platform, and

43
05:01:55 --> 05:01:58
Software as a Service is
getting extended further.

44
05:02:00 --> 05:02:03
XaaS is an umbrella term that signifies

45
05:02:03 --> 05:02:07
even finer-grain control over computing
resources that you want to rent.

46
05:02:07 --> 05:02:13
For example, storage as a service,
communication as a service,

47
05:02:13 --> 05:02:15
marketing as a service, and so on.

48
05:02:16 --> 05:02:24
As a summary, infrastructure as a service,
platform as a service, and application

49
05:02:24 --> 05:02:28
as a service are three main class service
models that are being used with success.

50
05:02:29 --> 05:02:35
Picking one will depend on the number
of variables which are company's goals.

51
05:02:35 --> 05:02:38
These three models have
inspired many similar models

52
05:02:38 --> 05:02:40
to emerge around cloud computing.

1
10:00:50 --> 10:00:54
In this lecture we'll be learning about
the basic file manipulation commands for

2
10:00:54 --> 10:00:55
the Hadoop File System or HDFS.

3
10:00:55 --> 10:00:59
We'll first start by downloading
a text file of words.

4
10:00:59 --> 10:01:05
We'll use this text file to copy to and
from the local file system in HDFS and

5
10:01:05 --> 10:01:07
later on we'll use it
to run word count on.

6
10:01:08 --> 10:01:11
After we download the text file,
we'll open a terminal shell and

7
10:01:11 --> 10:01:15
copy the text file from
the local file system to HDFS.

8
10:01:16 --> 10:01:19
Next, we'll copy the file within HDFS and

9
10:01:19 --> 10:01:24
also see how to copy file from
HDFS to the local file system.

10
10:01:24 --> 10:01:28
Finally, we'll see how to
delete a file in HDFS.

11
10:01:28 --> 10:01:29
Let's start.

12
10:01:29 --> 10:01:32
We're going to download text
file to copy into HDFS.

13
10:01:32 --> 10:01:36
It doesn't matter what the contents of
the text file is, so we'll download

14
10:01:36 --> 10:01:40
the complete works of Shakespeare
since it contains interesting text.

15
10:01:43 --> 10:01:46
First, click on the icon here
to launch a web browser.

16
10:01:50 --> 10:01:53
Now we'll search Google for
the complete works of Shakespeare.

17
10:02:07 --> 10:02:09
I'm going to be using
this first link here.

18
10:02:09 --> 10:02:13
And we'll provide this link
in the reading section.

19
10:02:16 --> 10:02:20
So this is the complete
works of Shakespeare and

20
10:02:20 --> 10:02:25
we'll save it to a text file in the local
file system by clicking on the icon here,

21
10:02:25 --> 10:02:30
the Open Menu and selecting save page.

22
10:02:33 --> 10:02:37
So we'll call it words.txt.

23
10:02:37 --> 10:02:42
And the Save in folder, it's going to
save it into the Downloads directory.

24
10:02:47 --> 10:02:52
Once that completes, we'll open a terminal
window by clicking on the icon here.

25
10:02:56 --> 10:03:03
So if we go into the downloads
Ddrectory by typing a cd Downloads and

26
10:03:03 --> 10:03:11
running ls, we can see that words.txt
was successfully downloaded.

27
10:03:14 --> 10:03:21
Moving on, let's copy words.txt from the
local file system to the HDFS file system.

28
10:03:21 --> 10:03:28
The command to do this is hadoop
fs- copyFromLocal words.txt.

29
10:03:30 --> 10:03:33
When I run this,
it'll copy it from the local directory and

30
10:03:33 --> 10:03:35
local file system to HDFS.

31
10:03:37 --> 10:03:42
We can see that the file was
copied by running hadoop fs -ls.

32
10:03:42 --> 10:03:47
You can see that the file
was successfully copied.

33
10:03:50 --> 10:03:55
Next, we can copy this file
to another file within HDFS.

34
10:03:55 --> 10:04:04
We can do this by running hadoop
fs -cp words.txt words2.txt.

35
10:04:04 --> 10:04:10
The first words.txt is the file
that already exists in HDFS.

36
10:04:12 --> 10:04:14
The second words 2.txt

37
10:04:14 --> 10:04:17
is the new file that we're going to
create when we run this command.

38
10:04:18 --> 10:04:23
Let's run it, and again we're can run

39
10:04:23 --> 10:04:28
hadoopfs-ls to see the files in HDFS.

40
10:04:28 --> 10:04:34
We can see the original file words.txt,
and the copy that was made, words2.txt.

41
10:04:36 --> 10:04:42
Let's copy words2.txt from
HDFS to the local filesystem.

42
10:04:42 --> 10:04:50
We can do this by running hadoop
fs -copyToLocal words2.txt.

43
10:04:53 --> 10:04:59
After I run this command, I can call ls to
see the contents of the local file system.

44
10:05:01 --> 10:05:06
So now we have the new file words2.txt
which we've just copied from HDFS.

45
10:05:09 --> 10:05:12
The last step in this lecture
is to delete a file in HDFS.

46
10:05:12 --> 10:05:18
We can delete words2.txt
by running hadoop fs,

47
10:05:18 --> 10:05:22
but is rn words2.txt.

48
10:05:25 --> 10:05:27
As you can see,
it printed that it deleted the file.

49
10:05:27 --> 10:05:31
We can also run hadoop fs- ls,
to verify that the file is deleted.

50
10:05:35 --> 10:05:40
You can see that there's only the original
words.txt, and words2.txt was deleted.

1
20:06:30 --> 20:06:32
Getting started.

2
20:06:32 --> 20:06:34
Why do we worry about foundations?

3
20:06:44 --> 20:06:49
Starting next week you'll start
diving into the details of the Hadoop

4
20:06:49 --> 20:06:51
framework for big data.

5
20:06:51 --> 20:06:56
Before you start, taking a little
bit of time to understand some core

6
20:06:56 --> 20:07:01
concepts will help you to digest the
information on Hadoop better and faster.

7
20:07:02 --> 20:07:06
Imagine yourself attending
a chemistry lab.

8
20:07:06 --> 20:07:10
Before you start hearing about
the tubes and mixtures, you really need

9
20:07:10 --> 20:07:15
to understand the chemistry or theory
of the practical concepts in the lab.

10
20:07:15 --> 20:07:19
Similarly, learning these
concepts now will help

11
20:07:19 --> 20:07:23
you with your understanding of the
practical concepts in the Hadoop lectures.

12
20:07:25 --> 20:07:30
In addition, we want to prepare you to
understand the tools beyond Hadoop.

13
20:07:30 --> 20:07:36
Any big data system that you find
will be built on these core concepts.

14
20:07:36 --> 20:07:39
So these foundations will
help you beyond this course.

15
20:07:40 --> 20:07:41
Now, let's get started.

1
16:14:13 --> 16:14:14
Getting started.

2
16:14:14 --> 16:14:16
Why Hadoop?

3
16:14:16 --> 16:14:18
We have all heard that Hadoop and

4
16:14:18 --> 16:14:23
related projects in this
ecosystem are great for big data.

5
16:14:23 --> 16:14:29
This module will answer the four Ws and
an H about why this statement is true.

6
16:14:41 --> 16:14:44
Before we dive further into
the details of Hadoop,

7
16:14:44 --> 16:14:49
let's take a moment to analyze the
characteristics of the Hadoop ecosystem.

8
16:14:51 --> 16:14:53
What's in the ecosystem?

9
16:14:53 --> 16:14:54
Why is it beneficial?

10
16:14:56 --> 16:14:57
Where is it used?

11
16:14:58 --> 16:14:59
Who uses it?

12
16:15:00 --> 16:15:02
And how do these tools work?

13
16:15:05 --> 16:15:10
The Hadoop ecosystem frameworks and
applications that we will describe in this

14
16:15:10 --> 16:15:14
module have several overarching themes and
goals.

15
16:15:14 --> 16:15:19
First, they provide scalability
to store large volumes of data

16
16:15:19 --> 16:15:21
on commodity hardware.

17
16:15:23 --> 16:15:27
As the number of systems increases,
so does the chance for crashes and

18
16:15:27 --> 16:15:29
hardware failures.

19
16:15:29 --> 16:15:34
A second goal, supported by most
frameworks in the Hadoop ecosystem,

20
16:15:34 --> 16:15:37
is the ability to gracefully
recover from these problems.

21
16:15:39 --> 16:15:45
In addition, as we have mentioned before,
big data comes in

22
16:15:45 --> 16:15:51
a variety of flavors, such as text files,
graph of social networks,

23
16:15:51 --> 16:15:57
streaming sensor data and raster images.

24
16:15:57 --> 16:16:01
A third goal for
the Hadoop ecosystem then,

25
16:16:01 --> 16:16:06
is the ability to handle these different
data types for any given type of data.

26
16:16:08 --> 16:16:11
You can find several projects in
the ecosystem that support it.

27
16:16:13 --> 16:16:16
A fourth goal of the Hadoop ecosystem

28
16:16:16 --> 16:16:19
is the ability to facilitate
a shared environment.

29
16:16:19 --> 16:16:26
Since even modest-sized
clusters can have many cores,

30
16:16:26 --> 16:16:29
it is important to allow multiple
jobs to execute simultaneously.

31
16:16:31 --> 16:16:34
Why buy servers only to let them sit idle?

32
16:16:36 --> 16:16:41
Another goal of the Hadoop ecosystem
is providing value for your enterprise.

33
16:16:43 --> 16:16:48
The ecosystem includes a wide
range of open source projects

34
16:16:48 --> 16:16:51
backed by a large active community.

35
16:16:52 --> 16:16:57
These projects are free to use and
easy to find support for.

36
16:16:59 --> 16:17:01
In the following lectures in this module,

37
16:17:01 --> 16:17:06
we will take a more detailed
look at the Hadoop ecosystem.

38
16:17:06 --> 16:17:10
First, we will explore the kinds
of projects available and

39
16:17:10 --> 16:17:12
the types of capabilities they provide.

40
16:17:14 --> 16:17:20
Next we will take a deeper look at
the three main parts of Hadoop.

41
16:17:20 --> 16:17:23
The Hadoop distributed file system,
or HDFS.

42
16:17:24 --> 16:17:26
YARN, the scheduler and resource manager.

43
16:17:28 --> 16:17:32
And MapReduce, a programming model for
processing big data.

44
16:17:34 --> 16:17:39
We will then discuss cloud computing, and
the types of service models it provides.

45
16:17:40 --> 16:17:45
We will also describe situations in
which Hadoop is not the best solution.

46
16:17:47 --> 16:17:51
This module then concludes with
two readings involving hands-on

47
16:17:51 --> 16:17:55
experience with HDFS and MapReduce.

48
16:17:55 --> 16:17:56
So let's get started.

1
08:32:08 --> 08:32:12
MapReduce, Simple Programming for
Big Results.

2
08:32:28 --> 08:32:33
MapReduce is a programming model for
the Hadoop ecosystem.

3
08:32:33 --> 08:32:36
It relies on YARN to schedule and

4
08:32:36 --> 08:32:40
execute parallel processing over
the distributed file blocks in HDFS.

5
08:32:41 --> 08:32:46
There are several tools that use the
MapReduce model to provide a higher level

6
08:32:46 --> 08:32:48
interface to other programming models.

7
08:32:48 --> 08:32:53
Hive has a SQL-like interface that
adds capabilities that help with

8
08:32:53 --> 08:32:56
relational data modeling.

9
08:32:56 --> 08:32:59
And Pig is a high level data flow language

10
08:32:59 --> 08:33:02
that adds capabilities that
help with process map modeling.

11
08:33:04 --> 08:33:09
Traditional parallel programming requires
expertise on a number of computing and

12
08:33:09 --> 08:33:11
systems concepts.

13
08:33:11 --> 08:33:16
For example, synchronization
mechanisms like locks, semaphores,

14
08:33:16 --> 08:33:18
and monitors are essential.

15
08:33:18 --> 08:33:22
And incorrectly using them can
either crash your program, or

16
08:33:22 --> 08:33:24
severely impact performance.

17
08:33:26 --> 08:33:28
This high learning curve
makes it difficult.

18
08:33:29 --> 08:33:34
It is also error prone,
since your code can run on hundreds, or

19
08:33:34 --> 08:33:37
thousands of nodes,
each having many cores.

20
08:33:37 --> 08:33:40
And any problem related to
these parallel processes,

21
08:33:40 --> 08:33:43
needs to be handled by
your parallel program.

22
08:33:44 --> 08:33:50
The MapReduce programming model greatly
simplifies running code in parallel

23
08:33:50 --> 08:33:53
since you don't have to deal
with any of these issues.

24
08:33:53 --> 08:33:59
Instead, you only need to create and
map and reduce tasks, and you don't

25
08:33:59 --> 08:34:03
have to worry about multiple threads,
synchronization, or concurrency issues.

26
08:34:05 --> 08:34:08
So, what is a map and reduce?

27
08:34:09 --> 08:34:14
Map and reduce are two concepts
based on functional programming

28
08:34:14 --> 08:34:19
where the output the function
is based solely on the input.

29
08:34:21 --> 08:34:27
Just like in a mathematical function,
f (x) = y, y depends on x.

30
08:34:28 --> 08:34:32
You provide a function, or
operation for a map, and reduce.

31
08:34:33 --> 08:34:37
And the runtime executes it over the data.

32
08:34:37 --> 08:34:42
For map, the operation is
applied on each data element.

33
08:34:42 --> 08:34:47
And in reduce, the operation
summarizes elements in some manner.

34
08:34:48 --> 08:34:54
An example, using map and
reduce will make this concepts more clear.

35
08:34:56 --> 08:34:59
Hello word is a traditional
first program you code

36
08:34:59 --> 08:35:01
when you start to learning
programming languages.

37
08:35:02 --> 08:35:08
The first program to learn, or hello
word of map reduce, is often WordCount.

38
08:35:10 --> 08:35:15
WordCount reads one or
more text files, and

39
08:35:15 --> 08:35:21
counts the number of occurrences
of each word in these files.

40
08:35:21 --> 08:35:24
The output will be a text
file with a list of words and

41
08:35:24 --> 08:35:27
their occurrence frequencies
in the input data.

42
08:35:29 --> 08:35:32
Let's examine each step of WordCount.

43
08:35:33 --> 08:35:38
For simplification we are assuming
we have one big file as an input.

44
08:35:40 --> 08:35:46
Before WordCount runs,
the input file is stored in HDFS.

45
08:35:46 --> 08:35:47
As you know now,

46
08:35:47 --> 08:35:52
HDFS partitions the blocks across
multiple nodes in the cluster.

47
08:35:53 --> 08:36:00
In this case, four partitions labeled,
A, B, C, and D.

48
08:36:00 --> 08:36:05
The first step in MapReduce is to
run a map operation on each node.

49
08:36:07 --> 08:36:12
As the input partitions are read
from HTFS, map is called for

50
08:36:12 --> 08:36:13
each line in the input.

51
08:36:15 --> 08:36:19
Let's look at the first lines
of the input partitions, A and

52
08:36:19 --> 08:36:21
B, and start counting the words.

53
08:36:22 --> 08:36:28
The first line,
in the partition on node A,

54
08:36:28 --> 08:36:33
says, My apple is red and my rose is blue.

55
08:36:33 --> 08:36:39
Similarly, the first line, on partition B,
says, You are the apple of my eye.

56
08:36:41 --> 08:36:45
Let's now see what happens in
the first map node for partition A.

57
08:36:47 --> 08:36:49
Map creates a key value for

58
08:36:49 --> 08:36:56
each word on the line containing
the word as the key, and 1 as the value.

59
08:36:56 --> 08:37:02
In this example, the word apple is
read from the line in partition A.

60
08:37:03 --> 08:37:08
Map produces a key value of (apple, 1).

61
08:37:08 --> 08:37:15
Similarly, the word my is seen
on the first line of A twice.

62
08:37:15 --> 08:37:21
So, the key values of (my,
1), are created.

63
08:37:21 --> 08:37:27
Note that map goes to each node
containing a data block for

64
08:37:27 --> 08:37:32
the file,
instead of the data moving to map.

65
08:37:32 --> 08:37:34
This is moving computation to data.

66
08:37:35 --> 08:37:40
Let's now see what the same map
operation generates for partition B.

67
08:37:40 --> 08:37:44
Since each word only
happens to occur once,

68
08:37:44 --> 08:37:49
a list of all the words with one
key-value pairing each gets generated.

69
08:37:50 --> 08:37:55
Please take a moment to
observe the outputs of map and

70
08:37:55 --> 08:37:59
each key-value pair associated to a word.

71
08:38:09 --> 08:38:16
Next, all the key-values that were output
from map are sorted based on their key.

72
08:38:16 --> 08:38:23
And the key values, with the same word,
are moved, or shuffled, to the same node.

73
08:38:24 --> 08:38:30
To simplify this figure, each node only
has a single word, in orange boxes.

74
08:38:31 --> 08:38:36
But in general,
a node will have many different words.

75
08:38:36 --> 08:38:39
Just like our example from the two
lines in A and B partitions.

76
08:38:40 --> 08:38:46
Here we see that, you and apple,
are assigned to the first node.

77
08:38:46 --> 08:38:50
The word is, to the second node.

78
08:38:50 --> 08:38:54
And the words, rose and red, to the third.

79
08:38:55 --> 08:39:01
Although, for simplicity, we drew four
map nodes and three shuffle nodes.

80
08:39:01 --> 08:39:05
The number of nodes can be extended
as much as the application demands.

81
08:39:08 --> 08:39:14
Next, the reduce operation
executes on these nodes to

82
08:39:14 --> 08:39:19
add values for
key-value pairs with the same keys.

83
08:39:19 --> 08:39:23
For example, (apple, 1), and

84
08:39:23 --> 08:39:30
another (apple, 1), becomes (apple, 2).

85
08:39:30 --> 08:39:35
The result of reduce is
a single key pair for

86
08:39:35 --> 08:39:39
each word that was read in the input file.

87
08:39:39 --> 08:39:43
The key is the word, and
the value is the number of occurrences.

88
08:39:45 --> 08:39:49
If we look back at our WordCount example,

89
08:39:49 --> 08:39:52
we see that there were
three distinct steps.

90
08:39:52 --> 08:39:59
Namely, the map step, the shuffle and
sort step, and the reduce step.

91
08:39:59 --> 08:40:05
Although, the WordCount
example is pretty simple,

92
08:40:05 --> 08:40:10
it represents a large number of
applications to which these three steps

93
08:40:10 --> 08:40:14
can be applied in order to achieve
data parallel scalability.

94
08:40:15 --> 08:40:21
For example, now that you have
seen the WordCount application,

95
08:40:21 --> 08:40:26
consider changing the WordCount
algorithm to index all

96
08:40:26 --> 08:40:29
the URLs by words after a web crawl.

97
08:40:30 --> 08:40:36
This means, instead of pointing to
a number, the keys would refer to URLs.

98
08:40:38 --> 08:40:43
After the map, with this new function,
which by the way is called a user

99
08:40:43 --> 08:40:48
defined function, the output of
shuffle and sort would look like this.

100
08:40:53 --> 08:41:00
Now, when we reduce the URLs, all the URLs
that mention Apple would look like this.

101
08:41:02 --> 08:41:08
This is, in fact, one of the ways
a search engine like Google works.

102
08:41:09 --> 08:41:15
So now, if somebody came to the interface
built for this application,

103
08:41:15 --> 08:41:19
to search for the word apple,
and entered apple,

104
08:41:19 --> 08:41:24
it would be easy to get all
the URLs as the word itself.

105
08:41:25 --> 08:41:29
No wonder the first MapReduce
paper was produced by Google.

106
08:41:29 --> 08:41:34
We will give you a link to the original
Google paper on MapReduce from

107
08:41:34 --> 08:41:37
2004 at the end of this lecture.

108
08:41:39 --> 08:41:41
It is pretty technical, but

109
08:41:41 --> 08:41:45
it gives you a simple overview without
the current system implementations.

110
08:41:46 --> 08:41:50
We just saw how MapReduce can
be used in search engines

111
08:41:50 --> 08:41:52
in addition to counting the words and
documents.

112
08:41:54 --> 08:41:58
Although it's possible to add many
more applications, let's stop here for

113
08:41:58 --> 08:42:02
a general discussion on how the points of

114
08:42:02 --> 08:42:06
data parallelism can be used in
search in this three step pattern.

115
08:42:08 --> 08:42:12
There is definitely parallelization
during the map step.

116
08:42:13 --> 08:42:16
This parallelization is over the input,

117
08:42:16 --> 08:42:20
as each partition gets
processed one line at a time.

118
08:42:20 --> 08:42:25
To achieve this type of data
parallelism we must decide on

119
08:42:25 --> 08:42:30
the data granularity of
each parallel competition.

120
08:42:30 --> 08:42:32
In this case, it will be a line.

121
08:42:33 --> 08:42:40
We also see parallel grouping of
data in the shuffle and sort phase.

122
08:42:40 --> 08:42:44
This time, the parallelization is
over the intermediate products.

123
08:42:45 --> 08:42:48
That is, the individual key-value pairs.

124
08:42:49 --> 08:42:53
And after the grouping of
the intermediate products,

125
08:42:53 --> 08:42:58
the reduce step gets parallelized
to construct one output file.

126
08:43:00 --> 08:43:03
You have probably noticed
that the data gets reduced

127
08:43:03 --> 08:43:05
to a smaller set at each step.

128
08:43:06 --> 08:43:13
This overview gave us an idea of what
kinds of tasks that MapReduce is good for.

129
08:43:13 --> 08:43:18
While MapReduce excels at independent
batch tasks similar to our applications,

130
08:43:18 --> 08:43:23
there are certain kinds of tasks that
you would not want to use MapReduce for.

131
08:43:24 --> 08:43:29
For example,
if your data is frequently changing,

132
08:43:29 --> 08:43:34
MapReduce is slow since it reads
the entire input data set each time.

133
08:43:35 --> 08:43:38
The MapReduce model requires that maps and

134
08:43:38 --> 08:43:42
reduces execute
independently of each other.

135
08:43:42 --> 08:43:44
This greatly simplifies
your job as a designer,

136
08:43:44 --> 08:43:48
since you do not have to deal
with synchronization issues.

137
08:43:48 --> 08:43:53
However, it means that computations
that do have dependencies,

138
08:43:53 --> 08:43:54
cannot be expressed with MapReduce.

139
08:43:56 --> 08:44:00
Finally, MapReduce does
not return any results

140
08:44:00 --> 08:44:03
until the entire process is finished.

141
08:44:03 --> 08:44:06
It must read the entire input data set.

142
08:44:07 --> 08:44:11
This makes it unsuitable for interactive
applications where the results must be

143
08:44:11 --> 08:44:16
presented to the user very quickly,
expecting a return from the user.

144
08:44:17 --> 08:44:22
As a summary, MapReduce hides
complexities of parallel programming and

145
08:44:22 --> 08:44:25
greatly simplifies building
parallel applications.

146
08:44:26 --> 08:44:28
Many types of tasks suitable for

147
08:44:28 --> 08:44:34
MapReduce include search engine
page ranking and topic mapping.

148
08:44:34 --> 08:44:37
Please see the reading after
this lecture on making Pasta Sauce

149
08:44:39 --> 08:44:45
with MapReduce for another fun application
using the MapReduce programming model.

1
17:16:53 --> 17:16:55
Programming Models for Big Data.

2
17:17:14 --> 17:17:18
We have seen that scalable computing
over the internet to achieve

3
17:17:18 --> 17:17:24
data-parallel scalability for big data
applications is now a possibility.

4
17:17:24 --> 17:17:26
Thanks to commodity clusters.

5
17:17:26 --> 17:17:30
Cost-effective commodity clusters
together with advances in distributed

6
17:17:30 --> 17:17:33
file systems to move computation to data,

7
17:17:33 --> 17:17:38
provide a potential to conduct
scalable big data analytics.

8
17:17:38 --> 17:17:41
The next thing we will
talk about is how to take

9
17:17:41 --> 17:17:43
advantage of these
infrastructure advances.

10
17:17:44 --> 17:17:45
What are the right programming models?

11
17:17:46 --> 17:17:52
A programming model is an abstraction or
existing machinery or infrastructure.

12
17:17:52 --> 17:17:55
It is a set of abstract
runtime libraries and

13
17:17:55 --> 17:18:00
programming languages that
form a model of computation.

14
17:18:00 --> 17:18:06
This abstraction level can be low-level
as in machine language in computers.

15
17:18:06 --> 17:18:12
Or very high as in high-level programming
languages, for example, Java.

16
17:18:12 --> 17:18:15
So we can say,
if the enabling infrastructure for

17
17:18:15 --> 17:18:20
big data analysis is distributed
file systems as we mentioned,

18
17:18:20 --> 17:18:25
then the programming model for
big data should enable the programmability

19
17:18:25 --> 17:18:29
of the operations within
distributed file systems.

20
17:18:29 --> 17:18:33
What we mean by this being able to
write computer programs that work

21
17:18:33 --> 17:18:38
efficiently on top of distributed
file systems using big data and

22
17:18:38 --> 17:18:42
making it easy to cope with
all the potential issues.

23
17:18:42 --> 17:18:44
Based on everything we discussed so

24
17:18:44 --> 17:18:49
far, let's describe the requirements for
big data programming models.

25
17:18:49 --> 17:18:54
First of all, such a programming model for
big data should support

26
17:18:54 --> 17:18:59
common big data operations like
splitting large volumes of data.

27
17:19:00 --> 17:19:04
This means for partitioning and
placement of data in and

28
17:19:04 --> 17:19:10
out of computer memory along with a model
to synchronize the datasets later on.

29
17:19:10 --> 17:19:13
The access to data should
be achieved in a fast way.

30
17:19:13 --> 17:19:18
It should allow fast distribution to nodes
within a rack and these are potentially,

31
17:19:18 --> 17:19:20
the data nodes we moved
the computation to.

32
17:19:20 --> 17:19:26
This means scheduling of
many parallel tasks at once.

33
17:19:26 --> 17:19:29
It should also enable
reliability of the computing and

34
17:19:29 --> 17:19:32
full tolerance from failures.

35
17:19:32 --> 17:19:35
This means it should enable
programmable replications and

36
17:19:35 --> 17:19:37
recovery of files when needed.

37
17:19:37 --> 17:19:42
It should be easily scalable to
the distributed notes where the data gets

38
17:19:42 --> 17:19:43
produced.

39
17:19:43 --> 17:19:48
It should also enable adding new resources
to take advantage of distributive

40
17:19:48 --> 17:19:53
computers and scale to more or
faster data without losing performance.

41
17:19:53 --> 17:19:57
This is called scaling out if needed.

42
17:19:57 --> 17:20:00
Since there are a variety
of different types of data,

43
17:20:00 --> 17:20:05
such as documents, graphs,
tables, key values, etc.

44
17:20:05 --> 17:20:09
A programming model should enable
operations over a particular set

45
17:20:09 --> 17:20:10
of these types.

46
17:20:10 --> 17:20:15
Not every type of data may be
supported by a particular model, but

47
17:20:15 --> 17:20:19
the models should be optimized for
at least one type.

48
17:20:19 --> 17:20:21
Is it getting a little complicated?

49
17:20:21 --> 17:20:23
It doesn't have to have to be.

50
17:20:23 --> 17:20:28
In fact, we apply similar models in
our daily lives for everyday tasks.

51
17:20:28 --> 17:20:33
Let's look at the scenario where you
might unknowingly apply this model.

52
17:20:33 --> 17:20:37
Imagine a peaceful Saturday afternoon.

53
17:20:37 --> 17:20:39
You receive a phone call from a friend and
she says,

54
17:20:39 --> 17:20:42
she they will be at your
house in an hour for dinner.

55
17:20:43 --> 17:20:48
It seems like you completely forgot that
you had invited your friends for dinner.

56
17:20:48 --> 17:20:51
So you say, you are looking forward
to it and head to the kitchen.

57
17:20:51 --> 17:20:57
As a quick solution, you decide to
cook pasta with some tomato sauce.

58
17:20:57 --> 17:20:59
You need to take advantage
of parallelization, so

59
17:20:59 --> 17:21:03
that the dinner is ready by the time your
guest arrive, that's within an hour.

60
17:21:03 --> 17:21:07
You call your spouse and your teenage
kids to action in the kitchen.

61
17:21:07 --> 17:21:12
Now, you need to give them directions to
start dicing the ingredients for you.

62
17:21:12 --> 17:21:19
But in the heat of the moment, you end up
mixing the onions, tomatoes and peppers.

63
17:21:19 --> 17:21:20
Instead of sorting them first,

64
17:21:20 --> 17:21:25
you give everyone a randomly mixed
batch of different types of vegetables.

65
17:21:25 --> 17:21:30
They are required to use their computer
powers to chop the vegetables.

66
17:21:30 --> 17:21:34
They need to ensure not mix
different types of veggies.

67
17:21:34 --> 17:21:39
When everyone is done chopping, you want
to group the veggies by their types.

68
17:21:39 --> 17:21:44
You ask each helper to collect items
of the same type, put them in a large

69
17:21:44 --> 17:21:50
bowl and label this large bowl with
the sum of individual bowl weights

70
17:21:50 --> 17:21:55
like tomatoes in one bowl, peppers in
another and the onions in the third bowl.

71
17:21:55 --> 17:21:56
In the end,

72
17:21:56 --> 17:22:02
you have nice large bowls with the total
weight of each vegetable labeled on it.

73
17:22:02 --> 17:22:07
Your helpers are soon done with their work
while you're focused on coordinating their

74
17:22:07 --> 17:22:12
actions and other dinner tasks in the
kitchen, you can start cooking your pasta.

75
17:22:12 --> 17:22:17
What you have just seen is an excellent
example of big data modeling in action.

76
17:22:17 --> 17:22:22
Only it is really the data
processed by human processors.

77
17:22:22 --> 17:22:26
This scenario can be modeled by a common
programming model for big data.

78
17:22:26 --> 17:22:28
Namely MapReduce.

79
17:22:28 --> 17:22:32
MapReduce is a big data programming
model that supports all

80
17:22:32 --> 17:22:36
the requirements of big
data modeling we mentioned.

81
17:22:36 --> 17:22:39
It can model processing large data,

82
17:22:39 --> 17:22:43
split complications into
different parallel tasks and

83
17:22:43 --> 17:22:49
make efficient use of large commodity
clusters and distributed file systems.

84
17:22:49 --> 17:22:53
In addition, it abstracts out
the details of parallelzation,

85
17:22:53 --> 17:22:58
full tolerance, data distribution,
monitoring and load balancing.

86
17:22:59 --> 17:23:00
As a programming model,

87
17:23:00 --> 17:23:04
it has been implemented in a few
different big data frameworks.

88
17:23:04 --> 17:23:08
Next week,
we will see more details on MapReduce and

89
17:23:08 --> 17:23:10
how its Hadoop implementation works.

90
17:23:10 --> 17:23:13
To summarize, programming models for

91
17:23:13 --> 17:23:17
big data are abstractions over
distributed file systems.

92
17:23:17 --> 17:23:22
The desired programming models for
big data should handle large volumes and

93
17:23:22 --> 17:23:23
varieties of data.

94
17:23:23 --> 17:23:27
Support full tolerance and
provide scale out functionality.

95
17:23:27 --> 17:23:29
MapReduce is one of these models,

96
17:23:29 --> 17:23:33
implemented in a variety of
frameworks including Hadoop.

97
17:23:33 --> 17:23:37
We will summarize the inner workings
of the Hadoop implementation next week.

1
10:40:29 --> 10:40:32
In this lecture we will use
Hadoop to run WordCount.

2
10:40:32 --> 10:40:34
First we will open a terminal shell and

3
10:40:34 --> 10:40:37
explore the Hadoop-provided MapReduce
programs.

4
10:40:37 --> 10:40:41
Next we will verify the input
file exists in HDFS.

5
10:40:41 --> 10:40:46
We will then run WordCount and
explore the WordCount output directory.

6
10:40:46 --> 10:40:48
After that we will copy
the WordCount results

7
10:40:48 --> 10:40:51
from HDFS to the local file system and
view them.

8
10:40:51 --> 10:40:53
Let's begin.

9
10:40:53 --> 10:40:56
First we'll open a terminal shell by

10
10:40:56 --> 10:40:59
clicking on the icon at
top of the window here.

11
10:41:02 --> 10:41:05
Next, we'll look at the map produced
programs that come with Hadoop.

12
10:41:05 --> 10:41:11
We can do this by running Hadoop,
jars user jars, Hadoop examples .jar.

13
10:41:15 --> 10:41:20
This command says we're going to use
the jar command to run a program

14
10:41:20 --> 10:41:23
in Hadoop from a jar file.

15
10:41:23 --> 10:41:28
And the jar file that we're running from
is in /usr/jars/hadoop-examples.jar.

16
10:41:28 --> 10:41:32
Many programs written in Java
are distributed via jar files.

17
10:41:32 --> 10:41:39
If we run this command We'll see a list of
different programs that come with Hadoop.

18
10:41:39 --> 10:41:42
So for example, wordcount.

19
10:41:42 --> 10:41:43
Count the words in a text file.

20
10:41:44 --> 10:41:48
Wordmean, count the average
length of words.

21
10:41:48 --> 10:41:54
And other programs, such as sorting and
calculating the length of pi.

22
10:41:56 --> 10:42:00
In the previous lecture we downloaded
the Works of Shakespeare and

23
10:42:00 --> 10:42:01
saved it into HDFS.

24
10:42:01 --> 10:42:06
Let's make sure that file is still
there by running hadoop fs -ls.

25
10:42:09 --> 10:42:13
We can see that the file is still there,
and it's called words.txt.

26
10:42:15 --> 10:42:20
We can run wordcount by running hadoop jar

27
10:42:20 --> 10:42:26
/usr/jars/hadoop-examples.jar wordcount.

28
10:42:26 --> 10:42:28
This command says that
we're going to run a jar,

29
10:42:28 --> 10:42:31
and this is the name of the jar
containing the program.

30
10:42:31 --> 10:42:34
And the program we're
going to run is wordcount.

31
10:42:34 --> 10:42:37
When we run it, we see that it
prints the command line usage for

32
10:42:37 --> 10:42:39
how to run wordcount.

33
10:42:39 --> 10:42:46
This says that wordcount takes one or
more input files and an output name.

34
10:42:46 --> 10:42:50
Now, both the input and
the output are located in HDFS.

35
10:42:50 --> 10:42:56
So we have the input file that we
just listed, words.txt, in HDFS.

36
10:42:56 --> 10:42:56
We can run wordcount.

37
10:42:58 --> 10:43:05
So we'll run hadoop
jar/usr/jars/hadoop-examples.jar

38
10:43:05 --> 10:43:08
wordcount words.txt out.

39
10:43:09 --> 10:43:14
This is saying we're going to run
the WordCount program using words.txt

40
10:43:14 --> 10:43:18
as the input and
put the output in a directory called out.

41
10:43:19 --> 10:43:20
So we'll run it.

42
10:43:27 --> 10:43:30
As wordcount is running,
your prints progress to the screen.

43
10:43:32 --> 10:43:35
It'll print the percentage of map and
reduce completed.

44
10:43:35 --> 10:43:39
And when both of these reach 100%,
then the job is done.

45
10:43:43 --> 10:43:46
Now that the job is complete,
let's look at the results.

46
10:43:48 --> 10:43:52
We can run Hadoop fs
-ls to see the output.

47
10:43:55 --> 10:43:59
This shows that out was created and
this is where our results are stored.

48
10:43:59 --> 10:44:03
Notice that it's
a directory with a d here.

49
10:44:03 --> 10:44:08
So hadoop word count created
the directory to contain the output.

50
10:44:08 --> 10:44:13
Let's look inside that directory
by running Hadoop fs- ls out.

51
10:44:13 --> 10:44:18
[BLANK AUDIO] We can see that there
are two files in this directory.

52
10:44:18 --> 10:44:24
The first is _SUCCESS, this means that
the WordCount job completed successfully.

53
10:44:24 --> 10:44:28
The other file part-r-00000 is a text file

54
10:44:28 --> 10:44:32
containing the output from
the WordCount command

55
10:44:34 --> 10:44:40
Now let's copy this text file to the local
file system from HDFS and then view it.

56
10:44:40 --> 10:44:47
We could copy it by running
hadoop fs -copytolocal

57
10:44:47 --> 10:44:52
out/part-r-00000 local.

58
10:44:54 --> 10:44:57
And we'll say local.txt is the name.

59
10:44:57 --> 10:45:02
You can view the results of this.

60
10:45:02 --> 10:45:04
We're running more local.txt.

61
10:45:07 --> 10:45:08
This will view the contents of the file.

62
10:45:11 --> 10:45:13
We can hit spacebar to scroll down.

63
10:45:16 --> 10:45:19
We see the results of
WordCount in this file.

64
10:45:19 --> 10:45:24
Each line is a particular word and
the second column is the count

65
10:45:24 --> 10:45:29
of how many words of this particular
word was found in the input file.

66
10:45:31 --> 10:45:33
You can hit q to quit

1
21:26:03 --> 21:26:05
Scalable computing over the internet.

2
21:26:21 --> 21:26:25
Most computing is done on
a single compute node.

3
21:26:26 --> 21:26:31
If the computation needs more than
a node or parallel processing,

4
21:26:31 --> 21:26:36
like many scientific computing problems,
we use parallel computers.

5
21:26:37 --> 21:26:42
Simply put, a parallel computer
is a very large number of

6
21:26:42 --> 21:26:48
single computing nodes with specialized
capabilities connected to other network.

7
21:26:49 --> 21:26:56
For example, the Gordon Supercomputer here
at The San Diego Supercomputer Center,

8
21:26:56 --> 21:27:05
has 1,024 compute nodes with 16 cores each
equalling 16,384 compute cores in total.

9
21:27:06 --> 21:27:09
This type of specialized
computer is pretty costly

10
21:27:09 --> 21:27:15
compared to its most recent cousin,
the commodity cluster.

11
21:27:15 --> 21:27:19
The term, commodity cluster,
is often heard in big data conversations.

12
21:27:21 --> 21:27:23
Have you ever wondered
what it exactly means?

13
21:27:24 --> 21:27:29
Commodity clusters are affordable
parallel computers

14
21:27:29 --> 21:27:31
with an average number of computing nodes.

15
21:27:33 --> 21:27:37
They are not as powerful as
traditional parallel computers and

16
21:27:37 --> 21:27:40
are often built out of
less specialized nodes.

17
21:27:41 --> 21:27:44
In fact,
the nodes in the commodity cluster

18
21:27:44 --> 21:27:47
are more generic in their
computing capabilities.

19
21:27:48 --> 21:27:53
The service-oriented computing community
over the internet have pushed for

20
21:27:53 --> 21:27:58
computing to be done on commodity
clusters as distributed computations.

21
21:27:58 --> 21:28:04
And in turn, reducing the cost
of computing over the Internet.

22
21:28:05 --> 21:28:11
In commodity clusters,
the computing nodes are clustered in racks

23
21:28:13 --> 21:28:17
connected to each other
via a fast network.

24
21:28:18 --> 21:28:22
There might be many of such
racks in extensible amounts.

25
21:28:23 --> 21:28:28
Computing in one or
more of these clusters across

26
21:28:28 --> 21:28:33
a local area network or the internet
is called distributed computing.

27
21:28:33 --> 21:28:38
Such architectures enable what
we call data-parallelism.

28
21:28:38 --> 21:28:43
In data-parallelism many jobs
that share nothing can work on

29
21:28:43 --> 21:28:46
different data sets or
parts of a data set.

30
21:28:47 --> 21:28:53
This type of parallelism sometimes
gets called as job level parallelism.

31
21:28:53 --> 21:28:57
But in this specialization,
we will refer to it as data-parallelism

32
21:28:57 --> 21:29:02
in the context of Big-data computing.

33
21:29:02 --> 21:29:07
Large volumes and varieties of big
data can be analyzed using this mode

34
21:29:07 --> 21:29:13
of parallelism, achieving scalability,
performance and cost reduction.

35
21:29:13 --> 21:29:18
As you can imagine, there are many
points of failure inside systems.

36
21:29:19 --> 21:29:25
A node, or
an entire rack can fail at any given time.

37
21:29:25 --> 21:29:29
The connectivity of a rack
to the network can stop or

38
21:29:29 --> 21:29:33
the connections between
individual nodes can break.

39
21:29:34 --> 21:29:39
It is not practical to restart everything
every time, if failure happens.

40
21:29:40 --> 21:29:46
The ability to recover from such
failures is called Fault-tolerance.

41
21:29:46 --> 21:29:51
For Fault-tolerance of such systems,
two neat solutions emerged.

42
21:29:51 --> 21:29:56
Namely, Redundant data storage and

43
21:29:56 --> 21:29:59
restart of failed
individual parallel jobs.

44
21:30:00 --> 21:30:02
We will explain these two solutions next.

45
21:30:04 --> 21:30:08
As a summary the commodity
clusters are a cost effective way

46
21:30:08 --> 21:30:13
of achieving data parallel scalability for
big data applications.

47
21:30:13 --> 21:30:18
These type of systems have a higher
potential for partial failures.

48
21:30:18 --> 21:30:21
It is this type of distributed
computing that pushed for

49
21:30:21 --> 21:30:25
a change towards cost
effective reliable and

50
21:30:25 --> 21:30:29
Fault-tolerant systems for
management and analysis of big data.

1
18:56:33 --> 18:56:38
The Hadoop Distributed File System,
a storage system for big data.

2
18:56:52 --> 18:56:59
As a storage layer, the Hadoop distributed
file system, or the way we call it HDFS.

3
18:57:01 --> 18:57:04
Serves as the foundation for
most tools in the Hadoop ecosystem.

4
18:57:06 --> 18:57:10
It provides two capabilities that
are essential for managing big data.

5
18:57:11 --> 18:57:14
Scalability to large data sets.

6
18:57:14 --> 18:57:17
And reliability to cope
with hardware failures.

7
18:57:19 --> 18:57:24
HDFS allows you to store and
access large datasets.

8
18:57:24 --> 18:57:28
According to Hortonworks,
a leading vendor of Hadoop services,

9
18:57:28 --> 18:57:34
HDFS has shown production
scalability up to 200 petabytes and

10
18:57:34 --> 18:57:37
a single cluster of 4,500 servers.

11
18:57:37 --> 18:57:40
With close to a billion files and blocks.

12
18:57:42 --> 18:57:47
If you run out of space, you can simply
add more nodes to increase the space.

13
18:57:50 --> 18:57:53
HDFS achieves scalability
by partitioning or

14
18:57:53 --> 18:57:57
splitting large files
across multiple computers.

15
18:57:57 --> 18:58:02
This allows parallel access to very large
files since the computations run in

16
18:58:02 --> 18:58:05
parallel on each node
where the data is stored.

17
18:58:06 --> 18:58:09
Typical file size is
gigabytes to terabytes.

18
18:58:11 --> 18:58:18
The default chunk size, the size of
each piece of a file is 64 megabytes.

19
18:58:18 --> 18:58:21
But you can configure this to any size.

20
18:58:22 --> 18:58:26
By spreading the file across many nodes,

21
18:58:26 --> 18:58:30
the chances are increased that a node
storing one of the blocks will fail.

22
18:58:32 --> 18:58:34
What happens next?

23
18:58:34 --> 18:58:36
Do we lose the information
stored in block C?

24
18:58:40 --> 18:58:43
HDFS is designed for
full tolerance in such case.

25
18:58:44 --> 18:58:47
HDFS replicates, or

26
18:58:47 --> 18:58:52
makes a copy of, file blocks on
different nodes to prevent data loss.

27
18:58:54 --> 18:58:59
In this example,
the node that crashed stored block C.

28
18:58:59 --> 18:59:08
But block C was replicated on
two other nodes in the cluster.

29
18:59:10 --> 18:59:15
By default, HDFS maintains
three copies of every block.

30
18:59:16 --> 18:59:18
This is the default replication factor.

31
18:59:19 --> 18:59:23
But you can change it globally for
every file, or

32
18:59:23 --> 18:59:28
on a per file basis.

33
18:59:28 --> 18:59:32
HDFS is also designed to handle
a variety of data types aligned with

34
18:59:32 --> 18:59:33
big data variety.

35
18:59:35 --> 18:59:41
To read a file in HDFS you must
specify the input file format.

36
18:59:41 --> 18:59:45
Similarly to write the file you must
provide the output file format.

37
18:59:47 --> 18:59:52
HDFS provides a set of formats for
common data types.

38
18:59:52 --> 18:59:58
But this is extensible and you can provide
custom formats for your data types.

39
18:59:58 --> 19:00:01
For example text files can be read.

40
19:00:03 --> 19:00:05
Line by line or a word at a time.

41
19:00:07 --> 19:00:13
Geospatial data can be read as vectors or
rasters.

42
19:00:14 --> 19:00:20
Data formats specific to geospatial data,
or

43
19:00:20 --> 19:00:23
other domain specific data formats.

44
19:00:23 --> 19:00:28
Like FASTA, or FASTQ formats for
sequence data genomics.

45
19:00:30 --> 19:00:33
HDFS is comprised of two components.

46
19:00:33 --> 19:00:37
NameNode, and DataNode.

47
19:00:37 --> 19:00:42
These operate using a master
slave relationship.

48
19:00:43 --> 19:00:47
Where the NameNode issues comments
to DataNodes across the cluster.

49
19:00:49 --> 19:00:53
The NameNode is responsible for metadata.

50
19:00:53 --> 19:00:56
And DataNodes provide block storage.

51
19:00:58 --> 19:01:02
There is usually one NameNode per cluster,

52
19:01:02 --> 19:01:06
a DataNode however,
runs on each node in the cluster.

53
19:01:08 --> 19:01:13
In some sense the NameNode
is the administrator or

54
19:01:13 --> 19:01:15
the coordinator of the HDFS cluster.

55
19:01:17 --> 19:01:21
When the file is created,
the NameNode records the name,

56
19:01:21 --> 19:01:25
location in the directory hierarchy and
other metadata.

57
19:01:26 --> 19:01:32
The NameNode also decides which data nodes
to store the contents of the file and

58
19:01:32 --> 19:01:34
remembers this mapping.

59
19:01:36 --> 19:01:39
The DataNode runs on
each node in the cluster.

60
19:01:40 --> 19:01:43
And is responsible for
storing the file blocks.

61
19:01:45 --> 19:01:50
The data node listens to commands from
the name node for block creation,

62
19:01:50 --> 19:01:56
deletion, and replication.

63
19:01:56 --> 19:02:00
Replication provides two key capabilities.

64
19:02:00 --> 19:02:03
Fault tolerance and data locality.

65
19:02:04 --> 19:02:09
As discussed earlier, when a machine in
the cluster has a hardware failure there

66
19:02:09 --> 19:02:14
are two other copies of each block
that are stored on that node.

67
19:02:14 --> 19:02:15
So no data is lost.

68
19:02:16 --> 19:02:20
Replication also means that the same block
will be stored on different nodes on

69
19:02:20 --> 19:02:25
the system which are in different
geographical locations.

70
19:02:27 --> 19:02:32
A location may mean a specific rack or
a data center in a different town.

71
19:02:34 --> 19:02:38
The location is important since we
want to move computation to data and

72
19:02:38 --> 19:02:39
not the other way around.

73
19:02:42 --> 19:02:46
We'll talk about what moving computation
to data means later in this module.

74
19:02:47 --> 19:02:52
As I mentioned earlier, the default
replication factor is three, but

75
19:02:52 --> 19:02:54
you can change this.

76
19:02:54 --> 19:02:59
A high replication factor means more
protection against hardware failures,

77
19:02:59 --> 19:03:02
and better chances for data locality.

78
19:03:02 --> 19:03:05
But it also means increased
storage space is used.

79
19:03:08 --> 19:03:11
As a summary HDFS provides

80
19:03:11 --> 19:03:15
scalable big data storage by
partitioning files over multiple nodes.

81
19:03:16 --> 19:03:21
This helps to scale big data
analytics to large data volumes.

82
19:03:22 --> 19:03:25
The application protects
against hardware failures and

83
19:03:25 --> 19:03:30
provides data locality when we move
analytical complications to data.

1
14:00:03 --> 14:00:06
The Hadoop Ecosystem: So Much free Stuff!

2
14:00:21 --> 14:00:25
How did the big data
open-source movement begin?

3
14:00:25 --> 14:00:29
In 2004 Google published
a paper about their

4
14:00:29 --> 14:00:34
in-house processing framework
they called MapReduce.

5
14:00:34 --> 14:00:38
The next year,
Yahoo released an open-source

6
14:00:38 --> 14:00:43
implementation based on this
framework called Hadoop.

7
14:00:43 --> 14:00:46
In the following years,
other frameworks and

8
14:00:46 --> 14:00:50
tools were released to the community
as open-source projects.

9
14:00:50 --> 14:00:54
These frameworks provided new
capabilities missing in Hadoop,

10
14:00:54 --> 14:00:57
such as SQL like querying or
high level scripting.

11
14:01:00 --> 14:01:04
Today, there are over 100
open-source projects for

12
14:01:04 --> 14:01:07
big data and
this number continues to grow.

13
14:01:09 --> 14:01:12
Many rely on Hadoop, but
some are independent.

14
14:01:14 --> 14:01:19
With so many frameworks and tools
available, how do we learn what they do?

15
14:01:20 --> 14:01:27
We can organize them with a layer diagram
to understand their capabilities.

16
14:01:27 --> 14:01:31
Sometimes we also used the term
stack instead of a layer diagram.

17
14:01:33 --> 14:01:38
In a layer diagram,
a component uses the functionality or

18
14:01:38 --> 14:01:42
capabilities of the components
in the layer below it.

19
14:01:42 --> 14:01:48
Usually components at the same
layer do not communicate.

20
14:01:48 --> 14:01:55
And a component never assumes a specific
tool or component is above it.

21
14:01:55 --> 14:02:01
In this example,
component A is in the bottom layer,

22
14:02:01 --> 14:02:05
which components B and C use.

23
14:02:07 --> 14:02:10
Component D uses B, but not C.

24
14:02:12 --> 14:02:14
And D does not directly use A.

25
14:02:18 --> 14:02:22
Let's look at one set of tools in
the Hadoop ecosystem as a layer diagram.

26
14:02:24 --> 14:02:31
This layer diagram is organized
vertically based on the interface.

27
14:02:31 --> 14:02:36
Low level interfaces, so storage and
scheduling, on the bottom.

28
14:02:36 --> 14:02:40
And high level languages and
interactivity at the top.

29
14:02:42 --> 14:02:48
The Hadoop distributed file system,
or HDFS, is the foundation for

30
14:02:48 --> 14:02:54
many big data frameworks, since it
provides scaleable and reliable storage.

31
14:02:55 --> 14:03:01
As the size of your data increases,
you can add commodity hardware

32
14:03:01 --> 14:03:06
to HDFS to increase storage capacity so

33
14:03:06 --> 14:03:10
it enables scaling out of your resources.

34
14:03:12 --> 14:03:16
Hadoop YARN provides
flexible scheduling and

35
14:03:16 --> 14:03:20
resource management over the HDFS storage.

36
14:03:21 --> 14:03:26
YARN is used at Yahoo to schedule
jobs across 40,000 servers.

37
14:03:29 --> 14:03:34
MapReduce is a programming model
that simplifies parallel computing.

38
14:03:35 --> 14:03:40
Instead of dealing with the complexities
of synchronization and scheduling, you

39
14:03:40 --> 14:03:46
only need to give MapReduce two functions,
map and reduce, as you heard before.

40
14:03:49 --> 14:03:51
This programming model is so

41
14:03:51 --> 14:03:56
powerful that Google previously
used it for indexing websites.

42
14:04:00 --> 14:04:06
MapReduce only assume a limited
model to express data.

43
14:04:06 --> 14:04:11
Hive and Pig are two additional
programming models on

44
14:04:11 --> 14:04:16
top of MapReduce to augment
data modeling of MapReduce

45
14:04:16 --> 14:04:22
with relational algebra and
data flow modeling respectively.

46
14:04:22 --> 14:04:26
Hive was created at
Facebook to issue SQL-like

47
14:04:26 --> 14:04:31
queries using MapReduce
on their data in HDFS.

48
14:04:31 --> 14:04:38
Pig was created at Yahoo to model data
flow based programs using MapReduce.

49
14:04:38 --> 14:04:42
Thanks to YARN stability
to manage resources,

50
14:04:42 --> 14:04:46
not just for MapReduce but
other programming models as well.

51
14:04:46 --> 14:04:51
Giraph was built for
processing large-scale graphs efficiently.

52
14:04:51 --> 14:04:58
For example, Facebook uses Giraph to
analyze the social graphs of its users.

53
14:04:58 --> 14:05:04
Similarly, Storm, Spark,
and Flink were built for

54
14:05:04 --> 14:05:08
real time and
in memory processing of big data on

55
14:05:08 --> 14:05:13
top of the YARN resource scheduler and
HDFS.

56
14:05:13 --> 14:05:18
In-memory processing is a powerful way of
running big data applications even faster,

57
14:05:18 --> 14:05:22
achieving 100x's better performance for
some tasks.

58
14:05:25 --> 14:05:29
Sometimes, your data or
processing tasks are not easily or

59
14:05:29 --> 14:05:34
efficiently represented using the file and
directory model of storage.

60
14:05:35 --> 14:05:40
Examples of this include collections
of key-values or large sparse tables.

61
14:05:42 --> 14:05:50
NoSQL projects such as Cassandra,
MongoDB, and HBase handle these cases.

62
14:05:50 --> 14:05:55
Cassandra was created at Facebook,
but Facebook also used HBase for

63
14:05:55 --> 14:05:57
its messaging platform.

64
14:05:59 --> 14:06:04
Finally, running all of these tools
requires a centralized management system

65
14:06:04 --> 14:06:09
for synchronization, configuration and
to ensure high availability.

66
14:06:10 --> 14:06:14
Zookeeper performs these duties.

67
14:06:14 --> 14:06:18
It was created by Yahoo to wrangle
services named after animals.

68
14:06:21 --> 14:06:23
A major benefit of the Hadoop ecosystem

69
14:06:23 --> 14:06:26
is that all these tools
are open-source projects.

70
14:06:27 --> 14:06:29
You can download and use them for free.

71
14:06:31 --> 14:06:35
Each project has a community of users and
developers that

72
14:06:35 --> 14:06:40
answer questions, fix bugs and
implement new features.

73
14:06:40 --> 14:06:45
You can mix and match to get only
the tools you need to achieve your goals.

74
14:06:47 --> 14:06:51
Alternatively, there are several
pre-built stacks of these tools

75
14:06:51 --> 14:06:55
offered by companies such as Cloudera,
MAPR and Hortonworks.

76
14:06:57 --> 14:07:01
These companies provide the core
software stacks for free and

77
14:07:01 --> 14:07:04
offer commercial support for
production environments.

78
14:07:06 --> 14:07:10
As a summary, the Hadoop ecosystem

79
14:07:10 --> 14:07:14
consists of a growing number
of open-source tools.

80
14:07:14 --> 14:07:18
Providing opportunities to
pick the right tool for

81
14:07:18 --> 14:07:22
the right tasks for
better performance and lower costs.

82
14:07:22 --> 14:07:26
We will reveal some of these
tools in further detail and

83
14:07:26 --> 14:07:30
provide an analysis of when to use
which in the next set of lectures.

1
04:07:31 --> 04:07:34
Generating value from Hadoop and

2
04:07:34 --> 04:07:38
Pre-Built Hadoop Images that
come as off the shelf products.

3
04:07:49 --> 04:07:54
Assembling your own software stack
from scratch can be messy and

4
04:07:54 --> 04:07:57
a lot of work for beginners.

5
04:07:57 --> 04:08:02
The task of setting up the whole stack
could consume a lot of project time and

6
04:08:02 --> 04:08:06
man power, reducing time to deployment.

7
04:08:06 --> 04:08:12
Getting pre-built images is similar
to buying pre-assembled furniture.

8
04:08:12 --> 04:08:17
You can obtain a ready to go software
stack which contains a pre-installed

9
04:08:17 --> 04:08:22
operating system, required libraries and
application software.

10
04:08:23 --> 04:08:26
It saves you from the trouble
of putting the different

11
04:08:26 --> 04:08:29
parts together in the right orientation.

12
04:08:29 --> 04:08:31
You can start using
the furniture right away.

13
04:08:32 --> 04:08:36
Packaging of these pre-built
software images is enabled

14
04:08:36 --> 04:08:39
by virtual machines using
virtualization software.

15
04:08:40 --> 04:08:45
Without going into too much detail,
one of the benefits of virtualization

16
04:08:45 --> 04:08:49
software is that it lets you run a ready
made software stack within minutes.

17
04:08:50 --> 04:08:54
Your software stack comes as a large file.

18
04:08:54 --> 04:08:57
Virtualization software provides
a platform where your stack can run.

19
04:08:59 --> 04:09:04
Many companies provide images for
their version of the Hadoop platform,

20
04:09:04 --> 04:09:07
including a number of
tools of their choice.

21
04:09:08 --> 04:09:12
Hortonworks is one of the companies that
provides a pre-built software stack for

22
04:09:12 --> 04:09:14
both Mac and Windows platforms.

23
04:09:15 --> 04:09:19
Cloudera is another company
that provides pre-installed and

24
04:09:19 --> 04:09:22
assembled software stack images.

25
04:09:22 --> 04:09:25
Cloudera image is what we will
be working with in this course.

26
04:09:26 --> 04:09:29
Many other companies
provide similar images.

27
04:09:30 --> 04:09:35
Additionally, lots of online tutorials for
beginners are on vendors websites for

28
04:09:35 --> 04:09:39
self-training of users
working with these images and

29
04:09:39 --> 04:09:41
the open source tools they include.

30
04:09:43 --> 04:09:47
Once you choose the vendor,
you can check out their website for

31
04:09:47 --> 04:09:49
tutorials on how to get started quickly.

32
04:09:50 --> 04:09:52
There are plenty of resources online for
that.

33
04:09:54 --> 04:09:57
You can deploy pre-built
images over the Cloud.

34
04:09:58 --> 04:10:01
This would further accelerate your
application deployment process.

35
04:10:02 --> 04:10:07
It is always best to evaluate which
approach is most cost effective for

36
04:10:07 --> 04:10:09
your business model and organization.

37
04:10:10 --> 04:10:14
Companies such as Cloudera,
Hortonworks and others,

38
04:10:14 --> 04:10:20
provide step-by-step guides on how to
set up pre-built images on the Cloud.

39
04:10:21 --> 04:10:26
As a summary, using pre-built software
packages have a number of benefits and

40
04:10:26 --> 04:10:29
can significantly accelerate
your big data projects.

41
04:10:30 --> 04:10:37
Even small teams can quickly prototype,
deploy and validate their project ideas.

42
04:10:37 --> 04:10:42
The developed analytical solutions
can be scaled to larger volumes and

43
04:10:42 --> 04:10:46
increase velocities of
data in a matter of hours.

44
04:10:46 --> 04:10:51
These companies also provide
Enterprise level solutions for large,

45
04:10:51 --> 04:10:52
full-fledged applications.

46
04:10:53 --> 04:10:58
An added benefit is that there
are plenty of companies which provide

47
04:10:58 --> 04:11:00
ready-made solutions.

48
04:11:00 --> 04:11:06
That means lots of choices for you to
pick the one most suited to your project.

1
08:18:37 --> 08:18:39
What is a Distributed File System?

2
08:18:53 --> 08:18:56
Most of us have file
cabinets in our offices or

3
08:18:56 --> 08:18:59
homes that help us store
our printed documents.

4
08:19:01 --> 08:19:04
Everyone has their own
method of organizing files,

5
08:19:04 --> 08:19:08
including the way we bin similar
documents into one file, or

6
08:19:08 --> 08:19:12
the way we sort them in alphabetical or
date order.

7
08:19:12 --> 08:19:15
When computers first came out,
the information and

8
08:19:15 --> 08:19:17
programs were stored in punch cards.

9
08:19:19 --> 08:19:22
These punch cards were
stored in file cabinets,

10
08:19:22 --> 08:19:25
just like the physical
file cabinets today.

11
08:19:25 --> 08:19:28
This is where the name,
file system, comes from.

12
08:19:28 --> 08:19:32
The need to store information in
files comes from a larger need

13
08:19:32 --> 08:19:35
to store information in the long-term.

14
08:19:35 --> 08:19:40
This way the information lives
after the computer program, or

15
08:19:40 --> 08:19:44
what we call process,
that produced it terminates.

16
08:19:44 --> 08:19:49
If we don't have files, our access to
such information would not be possible

17
08:19:49 --> 08:19:52
once a program using or producing it.

18
08:19:52 --> 08:19:57
Even during the process, we might need
to store large amounts of information

19
08:19:57 --> 08:20:02
that we cannot store within the program
components or computer memory.

20
08:20:02 --> 08:20:05
In addition, once the data is in a file,

21
08:20:05 --> 08:20:10
multiple processes can access
the same information if needed.

22
08:20:10 --> 08:20:15
For all these reasons, we store
information in files on a hard disk.

23
08:20:15 --> 08:20:18
There are many of these files, and

24
08:20:18 --> 08:20:22
they get managed by your operating system,
like Windows or Linux.

25
08:20:22 --> 08:20:27
How the operating system manages
files is called a file system.

26
08:20:27 --> 08:20:32
How this information is stored
on disk drives has high impact

27
08:20:32 --> 08:20:39
on the efficiency and speed of access to
data, especially in the big data case.

28
08:20:39 --> 08:20:44
While the files have exact addresses for
their locations in the drive, referring

29
08:20:44 --> 08:20:50
to the data units of sequence of these
blocks, that's called the flat structure,

30
08:20:50 --> 08:20:55
or hierarchy construction of index
records, that's called the database.

31
08:20:55 --> 08:21:01
They also have human readable symbolic
names, generally followed by an extension.

32
08:21:02 --> 08:21:06
Extensions tell what kind of file it is,
in general.

33
08:21:06 --> 08:21:10
Programs and
users can access files with their names.

34
08:21:10 --> 08:21:16
The contents of a file can be numeric,
alphabetic, alphanumeric,

35
08:21:16 --> 08:21:17
or binary executables.

36
08:21:18 --> 08:21:22
Most computer users work
on personal laptops or

37
08:21:22 --> 08:21:25
desktop computers with
a single hard drive.

38
08:21:26 --> 08:21:30
In this model, the user is limited
to the capacity of their hard drive.

39
08:21:30 --> 08:21:34
The capacity of different devices vary.

40
08:21:34 --> 08:21:36
For example, while your phone or

41
08:21:36 --> 08:21:41
tablet might have a storage capacity
in the order of gigabytes, your

42
08:21:41 --> 08:21:47
laptop computer might have a terabyte of
storage, but what if you have more data?

43
08:21:47 --> 08:21:50
Some of you probably had
issues in the past with

44
08:21:50 --> 08:21:52
running out of space on your hard drive.

45
08:21:52 --> 08:21:56
A way to solve this is to have
an external hard drive and

46
08:21:56 --> 08:22:00
store your files there or,
you can buy a bigger disk.

47
08:22:00 --> 08:22:05
Both options are a bit of a hassle, to
copy the data to a new disk, aren't they?

48
08:22:05 --> 08:22:07
They might not even be
an option sometimes.

49
08:22:08 --> 08:22:12
Now imagine, you having two computers and

50
08:22:12 --> 08:22:18
storing some of your data in one and
the rest of your data in another.

51
08:22:18 --> 08:22:22
How you organize and partition your data
between these computers is up to you.

52
08:22:23 --> 08:22:27
You might want to store your
work data in one computer and

53
08:22:27 --> 08:22:30
your personal data in another.

54
08:22:30 --> 08:22:34
Distributing data on multiple
computers might be an option, but

55
08:22:34 --> 08:22:36
it raises new issues.

56
08:22:36 --> 08:22:41
In this situation, you need to know
where to find the files you need,

57
08:22:41 --> 08:22:43
depending on what you’re doing.

58
08:22:43 --> 08:22:47
You can find it manageable,
if it’s just your data.

59
08:22:47 --> 08:22:51
But now imagine having
thousands of computers

60
08:22:51 --> 08:22:55
to store your data with big volumes and
variety.

61
08:22:55 --> 08:23:00
Wouldn't it be good to have a system
that can handle the data access and

62
08:23:00 --> 08:23:01
do this for you?

63
08:23:01 --> 08:23:06
This is a case that can be handled
by a distributive file system.

64
08:23:06 --> 08:23:10
Now, let's assume that there
are racks of these computers,

65
08:23:10 --> 08:23:14
often even distributed across the local or

66
08:23:14 --> 08:23:20
wide area network, because such file
systems, distributed file systems.

67
08:23:21 --> 08:23:25
Data sets, or parts of a data set,

68
08:23:25 --> 08:23:29
can be replicated across the nodes
of a distributed file system.

69
08:23:30 --> 08:23:35
Since data is already on these nodes,
then analysis of parts of the data

70
08:23:35 --> 08:23:42
is needed in a data parallel fashion,
computation can be moved to these nodes.

71
08:23:43 --> 08:23:48
Additionally, distributed file
systems replicate the data

72
08:23:48 --> 08:23:53
between the racks, and also computers
distributed across geographical regions.

73
08:23:55 --> 08:23:59
Data replication makes
the system more fault tolerant.

74
08:24:00 --> 08:24:05
That means, if some nodes or
a rack goes down,

75
08:24:05 --> 08:24:10
there are other parts of the system,
the same data can be found and analyzed.

76
08:24:10 --> 08:24:17
Data replication also helps with scaling
the access to this data by many users.

77
08:24:18 --> 08:24:24
Often, if the data is popular, many
reader processes will want access to it.

78
08:24:25 --> 08:24:28
In a highly parallelized replication,

79
08:24:28 --> 08:24:32
each reader can get their own node
to access to and analyze data.

80
08:24:33 --> 08:24:36
This increases overall system performance.

81
08:24:37 --> 08:24:42
Note that a problem with having
such a distributive replication is,

82
08:24:42 --> 08:24:46
that it is hard to make
changes to data over time.

83
08:24:46 --> 08:24:52
However, in most big data systems,
the data is written once and

84
08:24:52 --> 08:24:57
the updates to data is maintained
as additional data sets over time.

85
08:24:57 --> 08:25:02
As a summary, a file system is
responsible from the organization of

86
08:25:02 --> 08:25:05
the long term information
storage in a computer.

87
08:25:05 --> 08:25:10
When many storage computers
are connected through the network,

88
08:25:10 --> 08:25:13
we call it a distributed file system.

89
08:25:13 --> 08:25:19
Distributed file systems provide data
scalability, fault tolerance, and

90
08:25:19 --> 08:25:24
high concurrency through partitioning and
replication of data on many nodes.

1
16:44:04 --> 16:44:06
When to reconsider Hadoop?

2
16:44:20 --> 16:44:23
The Hadoop ecosystem is
growing at a fast pace.

3
16:44:24 --> 16:44:27
This means a lot of stuff
that was difficult, or

4
16:44:27 --> 16:44:31
not supportive, is becoming possible.

5
16:44:33 --> 16:44:34
In this lecture,

6
16:44:34 --> 16:44:38
we will look at some aspects that
clearly make a good match for Hadoop.

7
16:44:40 --> 16:44:44
We will also look at several
aspects that might motivate you

8
16:44:44 --> 16:44:46
to evaluate Hadoop at a deeper level.

9
16:44:48 --> 16:44:52
And does Hadoop really make sense for
your specific problem?

10
16:44:55 --> 16:44:59
First let's look at the key features
that make a problem Hadoop friendly.

11
16:45:00 --> 16:45:05
If you see a large scale growth in
amount of data you will tackle,

12
16:45:05 --> 16:45:07
probably it makes sense to use Hadoop.

13
16:45:08 --> 16:45:13
When you want quick access to your old
data which would otherwise go on tape

14
16:45:13 --> 16:45:18
drives for archival storage,
Hadoop might provide a good alternative.

15
16:45:21 --> 16:45:23
Other Hadoop friendly features include

16
16:45:23 --> 16:45:29
scenarios when you want to use multiple
applications over the same data store.

17
16:45:29 --> 16:45:32
High volume or
high variety are also great indicators for

18
16:45:32 --> 16:45:34
Hadoop as a platform choice.

19
16:45:38 --> 16:45:41
Small data set processing
should raise your eyebrows.

20
16:45:41 --> 16:45:43
Do you really need Hadoop for that?

21
16:45:44 --> 16:45:50
Dig deeper, and find out exactly why you
want to use Hadoop before going ahead.

22
16:45:54 --> 16:45:57
Hadoop is good for data parallelism.

23
16:45:57 --> 16:46:02
As you know, data parallelism is
the simultaneous execution of the same

24
16:46:02 --> 16:46:06
function on multiple nodes across
the elements of a dataset.

25
16:46:07 --> 16:46:12
On the other hand, task parallelism,
as you see in this graph,

26
16:46:13 --> 16:46:18
is the simultaneous execution
of many different functions

27
16:46:18 --> 16:46:22
on multiple nodes across the same or
different data sets.

28
16:46:23 --> 16:46:27
If your problem has task-level
parallelism, you must do further

29
16:46:27 --> 16:46:31
analysis as to which tools you plan
to deploy from the Hadoop ecosystem.

30
16:46:33 --> 16:46:37
What are the precise benefits
that these tools provide?

31
16:46:38 --> 16:46:39
Proceed with caution.

32
16:46:42 --> 16:46:45
Not all algorithms are scalable in Hadoop,
or

33
16:46:45 --> 16:46:49
reducible to one of the programming
models supported by YARN.

34
16:46:51 --> 16:46:56
Hence, if you are looking to deploy
highly coupled data processing algorithms

35
16:46:56 --> 16:46:57
proceed with caution.

36
16:46:59 --> 16:47:02
Do a thorough analysis
before using Hadoop.

37
16:47:02 --> 16:47:06
Are you thinking of

38
16:47:06 --> 16:47:11
throwing away your existing database
solutions and replacing them with Hadoop?

39
16:47:11 --> 16:47:11
Think again.

40
16:47:13 --> 16:47:18
Hadoop may be a good platform where
your diverse data sets can land and

41
16:47:18 --> 16:47:22
get processed into a form
digestible with your database.

42
16:47:23 --> 16:47:28
Hadoop may not be the best data store
solution for your business case.

43
16:47:28 --> 16:47:30
Evaluate, and proceed with caution.

44
16:47:31 --> 16:47:36
HDFS stores data in blocks
of 64 megabytes or larger,

45
16:47:36 --> 16:47:41
so you may have to read an entire
file just to pick one data entry.

46
16:47:43 --> 16:47:46
That makes it a bit harder to
perform random data access.

47
16:47:49 --> 16:47:53
The Hadoop ecosystem is growing
at a faster pace than ever.

48
16:47:54 --> 16:47:58
This slide shows some of the moving
targets in the Hadoop ecosystem and

49
16:47:58 --> 16:48:01
the additional needs which
must be addressed by new tools

50
16:48:01 --> 16:48:03
to the Hadoop ecosystem.

51
16:48:03 --> 16:48:06
Mainly, advanced analytical queries,

52
16:48:07 --> 16:48:11
latency sensitive tasks, and
cyber security of sensitive data.

53
16:48:13 --> 16:48:17
Here, we give pointers to tools you
might want to look into further

54
16:48:17 --> 16:48:20
to understand the challenges
these need tools address.

55
16:48:22 --> 16:48:28
As a summary, although Hadoop is good with
scalability of many algorithms, it is just

56
16:48:28 --> 16:48:33
one model and does not solve all issues
in managing and processing big data.

57
16:48:34 --> 16:48:39
Although it would be possible to find
counterexamples, we can generally say

58
16:48:39 --> 16:48:44
that the Hadoop framework is not the best
for working with small data sets,

59
16:48:44 --> 16:48:48
advanced algorithms that require
a specific hardware type,

60
16:48:48 --> 16:48:53
task level parallelism, infrastructure
replacement, or random data access.

1
09:32:56 --> 09:32:58
YARN, The Resource Manager for Hadoop.

2
09:33:15 --> 09:33:21
YARN is a resource manage layer that
sits just above the storage layer HDFS.

3
09:33:22 --> 09:33:27
YARN interacts with applications and
schedules resources for their use.

4
09:33:28 --> 09:33:33
YARN enables running multiple
applications over HDFC increases

5
09:33:33 --> 09:33:38
resource efficiency and
let's you go beyond the map reduce or

6
09:33:38 --> 09:33:40
even beyond the data
parallel programming model.

7
09:33:42 --> 09:33:46
When Hadoop was first created,
this wasn't the case.

8
09:33:46 --> 09:33:51
In fact, the original Hadoop
stack had no resource manager.

9
09:33:51 --> 09:33:56
These two stacked diagrams, show, some of
it's evolution over the last ten years.

10
09:33:57 --> 09:34:01
One of the biggest limitations
of Hadoop one point zero,

11
09:34:01 --> 09:34:05
was it's inability to support
non-mapreduce applications.

12
09:34:05 --> 09:34:09
It had several terrible
resource utilization.

13
09:34:09 --> 09:34:13
This meant that for advanced
applications such as graph analysis that

14
09:34:13 --> 09:34:16
required different ways of modelling and

15
09:34:16 --> 09:34:20
looking at data, you would need to
move your data to another platform.

16
09:34:20 --> 09:34:23
That's a lot of work if your data is big.

17
09:34:25 --> 09:34:30
Adding YARN in between HDFS and
the applications enabled

18
09:34:30 --> 09:34:35
new systems to be built, focusing on
different types of big data applications

19
09:34:35 --> 09:34:40
such as Giraph for
graph data analysis, Storm for

20
09:34:40 --> 09:34:44
streaming data analysis, and
Spark for in-memory analysis.

21
09:34:45 --> 09:34:49
YARN does so
by providing a standard framework

22
09:34:49 --> 09:34:53
that supports customized application
development in the HADOOP ecosystem.

23
09:34:54 --> 09:34:58
YARN lets you extract maximum
benefits from your data sets

24
09:34:58 --> 09:35:03
by letting you use the tools you
think are best for your big data.

25
09:35:04 --> 09:35:08
Let's take a peek into the architecture
of YARN without getting too technical.

26
09:35:10 --> 09:35:15
In this picture, notice the resource
manager in the center, and

27
09:35:15 --> 09:35:19
the node managers on each of
the three nodes on the right.

28
09:35:21 --> 09:35:26
The resource manager controls all
the resources, and decides who gets what.

29
09:35:28 --> 09:35:33
Node manager operates at machine level and
is in charge of a single machine.

30
09:35:35 --> 09:35:38
Together the resource manager and

31
09:35:38 --> 09:35:41
the node manager form the data
computation framework.

32
09:35:42 --> 09:35:45
Each application gets
an application master.

33
09:35:46 --> 09:35:50
It negotiates resource from
the Resource Manager and

34
09:35:50 --> 09:35:53
it talks to Node Manager
to get its tasks completed.

35
09:35:55 --> 09:36:00
Notice the ovals labeled
Container The container

36
09:36:00 --> 09:36:06
is an abstract Notions that
signifies a resource that is

37
09:36:06 --> 09:36:10
a collection of CPU
memory disk network and

38
09:36:10 --> 09:36:15
other resources within
the compute note to simplify and

39
09:36:15 --> 09:36:20
be less precise you can think
of a container and the Machine.

40
09:36:21 --> 09:36:24
We looked at the essential
gears of the YARN

41
09:36:24 --> 09:36:28
engine to give you an idea of
the key components of YARN.

42
09:36:28 --> 09:36:32
Now when you hear terms like
Resource Manager, Node Manager and

43
09:36:32 --> 09:36:37
Container, you will have an understanding
of what tasks they are responsible for.

44
09:36:40 --> 09:36:46
Here is a real life example to show
the strength Hadoop 2.0 over 1.0.

45
09:36:46 --> 09:36:49
Yahoo was able to run almost

46
09:36:49 --> 09:36:53
twice as many jobs per day with
Yarn than with Hadoop 1.0.

47
09:36:53 --> 09:37:01
They also experienced a substantial
increase in CPU utilization.

48
09:37:01 --> 09:37:02
Yahoo!

49
09:37:02 --> 09:37:05
even claimed that upgrading
to YARN was equal into

50
09:37:05 --> 09:37:09
adding 1000 machines to
their 2500 machine cluster.

51
09:37:09 --> 09:37:10
That's big.

52
09:37:12 --> 09:37:16
YARN success is evident
from an explosive growth of

53
09:37:16 --> 09:37:19
different application that
the Hadoop ecosystem now has.

54
09:37:21 --> 09:37:22
New to yarn?

55
09:37:22 --> 09:37:26
You can use the tool of your choice
over your big data without any hassle.

56
09:37:27 --> 09:37:33
Compare this with Hadoop 1.0 which
was limited to MapReduce alone.

57
09:37:35 --> 09:37:39
Let's review a summary of
the key take-aways about yarn.

58
09:37:39 --> 09:37:44
Yarn gives you many ways for
applications to extract value from data.

59
09:37:45 --> 09:37:50
It lets you run many distributed
applications over the same Hadoop cluster.

60
09:37:51 --> 09:37:56
In addition, YARN reduces
the need to move data around and

61
09:37:56 --> 09:38:01
supports higher resource utilization
resulting in lower costs.

62
09:38:02 --> 09:38:07
It's a scalable platform that has
enabled growth of several applications

63
09:38:07 --> 09:38:11
over the HDFS,
enriching the Hadoop ecosystem.

1
19:11:05 --> 19:11:11
At this point, you hopefully have a good
general idea of what big data means and

2
19:11:11 --> 19:11:12
why big data is important.

3
19:11:13 --> 19:11:16
So now, we need to focus on what to do

4
19:11:16 --> 19:11:18
when we have an application
that uses big data.

5
19:11:19 --> 19:11:23
In this video, we focus on
the problem of managing big data.

6
19:11:24 --> 19:11:25
So at the end of the video,

7
19:11:25 --> 19:11:30
you should be able to describe what data
management means in general, and then

8
19:11:30 --> 19:11:34
specifically recognize the issues that are
involved in the management of big data.

9
19:11:36 --> 19:11:39
First, let's see what data
management means in general.

10
19:11:40 --> 19:11:44
Instead of giving you
definitions of data management,

11
19:11:44 --> 19:11:46
let's think of some questions
that must be asked and

12
19:11:46 --> 19:11:49
answered well if you're to manage
a reasonable amount of data.

13
19:11:51 --> 19:11:54
Now, we can not possibly cover
all questions one should ask for

14
19:11:54 --> 19:11:59
a data-centric application, but here are
some important ones, which range from how

15
19:11:59 --> 19:12:04
we get the data, to how we work with it,
to how we secure it from malicious users.

16
19:12:05 --> 19:12:07
We'll visit these issues one at a time.

1
14:23:13 --> 14:23:15
Ingestion means the process
of getting the data

2
14:23:15 --> 14:23:19
into the data system that
we are building or using.

3
14:23:19 --> 14:23:22
Now you might think,
why is it worth talking about?

4
14:23:23 --> 14:23:26
We'll just read the data from somewhere,
like a file.

5
14:23:26 --> 14:23:29
And then using some command,
place it into the data system.

6
14:23:30 --> 14:23:34
Or we'll have have some
kind of a web form or

7
14:23:34 --> 14:23:37
other visual interface and just fill it
in so that the data goes into the system.

8
14:23:39 --> 14:23:43
Both of these ways of
data ingestion are valid.

9
14:23:43 --> 14:23:44
In fact, they're valid for

10
14:23:44 --> 14:23:47
some big data systems like your
airline reservation system.

11
14:23:48 --> 14:23:51
However when you think
of a large scale system

12
14:23:51 --> 14:23:55
you wold like to have more automation
in the data ingestion processes.

13
14:23:55 --> 14:24:00
And data ingestion then becomes a part of
the big data management infrastructure.

14
14:24:01 --> 14:24:05
So here are some questions you might want
to ask when you automate data ingestion.

15
14:24:07 --> 14:24:08
Now take a minute to read the questions.

16
14:24:11 --> 14:24:15
We'll look at two examples to
explore them in greater detail.

17
14:24:16 --> 14:24:20
The first example is that of a hospital
information system that we discussed in

18
14:24:20 --> 14:24:23
course one in the context
of precision medicine.

19
14:24:23 --> 14:24:27
We said that hospitals collect
terabytes of medical record

20
14:24:27 --> 14:24:30
from different departments and
be considered big data systems.

21
14:24:32 --> 14:24:36
The second example is a cloud based data
store where many people upload their

22
14:24:36 --> 14:24:41
messages, chats, pictures,
videos, music and so fourth.

23
14:24:41 --> 14:24:45
The cloud storage also supports active
communication between the members and

24
14:24:45 --> 14:24:46
store their communication in real time.

25
14:24:48 --> 14:24:53
So let's think of a hypothetical
hospital information information and

26
14:24:53 --> 14:24:55
the answer to depressions
that we are putting there.

27
14:24:55 --> 14:24:57
Now, do not take the numbers
to be very accurate.

28
14:24:57 --> 14:24:58
They are just examples.

29
14:24:59 --> 14:25:02
But it illustrates some important points.

30
14:25:02 --> 14:25:08
One, note that there are two kinds
of likeness associated with data.

31
14:25:08 --> 14:25:12
Some data like medical images
are large data objects by themselves.

32
14:25:14 --> 14:25:18
Secondly, the records
themselves are quite small but

33
14:25:18 --> 14:25:21
the size of the total collection
of records is very high.

34
14:25:23 --> 14:25:28
Two, while there is a lot of patient data,
the number of data sources that is

35
14:25:28 --> 14:25:33
the different departmental systems
contributing to the total information

36
14:25:33 --> 14:25:36
system does not change
very much over time.

37
14:25:36 --> 14:25:41
Three, the rate of data ingestion is
not enormous and is often proportional

38
14:25:41 --> 14:25:46
to the number of patient activities
that takes place at the hospital.

39
14:25:46 --> 14:25:49
Four, the system contains
medical records so

40
14:25:49 --> 14:25:54
data can never be discarded even
when there are errors in the data.

41
14:25:54 --> 14:26:00
The errors in this specific case
are flagged but the data is retained.

42
14:26:01 --> 14:26:05
Now this is the kind of rule
called an error handling policy.

43
14:26:05 --> 14:26:08
Which might be different for
different application problems.

44
14:26:09 --> 14:26:15
An air handling policy is part of
a larger scheme of policies called

45
14:26:15 --> 14:26:16
ingestion policies.

46
14:26:18 --> 14:26:22
Another kind of ingestion policy involves
decisions regarding what the system should

47
14:26:22 --> 14:26:26
do if the data rate suddenly increases or
becomes suspiciously low.

48
14:26:27 --> 14:26:31
In this example we have deliberately
decided not to include it in the design.

49
14:26:32 --> 14:26:36
Now compare the previous case with
the case of the online store of

50
14:26:36 --> 14:26:38
personal information.

51
14:26:38 --> 14:26:40
Again this is just an imaginary example.

52
14:26:40 --> 14:26:43
So don't think of all
the parameters to be exact.

53
14:26:44 --> 14:26:49
Now in this case one, the store will
have a fast growing membership.

54
14:26:50 --> 14:26:54
Each member will use multiple devices
to capture and ingest their data.

55
14:26:56 --> 14:27:02
Two, over all members together, the site
will be updated at a really fast rate,

56
14:27:02 --> 14:27:08
making this a large volume data
store with a fast ingest rate.

57
14:27:08 --> 14:27:13
Three, in this system, our primary
challenge is to keep up with the data

58
14:27:13 --> 14:27:18
rate, and hence, erroneous data will be
discarded after just one edit to reinvest.

59
14:27:20 --> 14:27:25
Four, now there is an actual policy for
handling data overflow,

60
14:27:25 --> 14:27:29
which essentially says,
keep the excess data in a site store.

61
14:27:29 --> 14:27:32
And ingest them when the data
rate becomes slower.

62
14:27:32 --> 14:27:36
But if the site store starts getting full

63
14:27:36 --> 14:27:40
start dropping some incoming data
at a rate of 0.1% at a time.

64
14:27:42 --> 14:27:45
Now we should see why data
ingestion together with it's

65
14:27:45 --> 14:27:48
policies should be an integral
part of a big data system.

66
14:27:48 --> 14:27:51
Especially when it involves
storing fast data.

1
04:51:03 --> 04:51:08
A very significant aspect of data
management is to document, define,

2
04:51:08 --> 04:51:12
implement, and test the set of
operations that are required for

3
04:51:12 --> 04:51:13
a specific application.

4
04:51:14 --> 04:51:19
As we'll see later in the specialization,
some operations are independent of

5
04:51:19 --> 04:51:24
the type of data and some others would
require us to know the nature of the data

6
04:51:24 --> 04:51:27
because the operations make use
of a particular data model.

7
04:51:27 --> 04:51:29
That is the way that it is structured.

8
04:51:30 --> 04:51:34
In general, there are two
broad divisions of operations.

9
04:51:35 --> 04:51:38
Those that work on a singular object and

10
04:51:38 --> 04:51:41
those that work on
collections of data objects.

11
04:51:43 --> 04:51:47
In the first case,
an operation that crops an image,

12
04:51:47 --> 04:51:52
that means extracts a sub
area from an area of pixels,

13
04:51:52 --> 04:51:56
is a single object operation because we
consider the image as a single object.

14
04:51:58 --> 04:52:01
One can think of many subclasses
of the second category

15
04:52:01 --> 04:52:04
where the operations
are on data collections.

16
04:52:04 --> 04:52:09
We briefly referred to three very
common operations that can be done

17
04:52:09 --> 04:52:10
regardless of the nature of the data.

18
04:52:11 --> 04:52:16
The first is to take a collection and
filter out a subset of that collection.

19
04:52:16 --> 04:52:20
The most obvious case is
selecting a subset from a set.

20
04:52:20 --> 04:52:24
In this example, we select circles
whose number is greater than three.

21
04:52:25 --> 04:52:29
A second case is merging two collections
together to form a larger collection.

22
04:52:31 --> 04:52:32
In the example shown,

23
04:52:32 --> 04:52:38
two three structure data items are merged
by fusing the node with a common property.

24
04:52:38 --> 04:52:38
That is two.

25
04:52:40 --> 04:52:44
In the last case,
we compute a function on a collection and

26
04:52:44 --> 04:52:46
return the value of the function.

27
04:52:46 --> 04:52:49
So in this example,
the function is a simple count.

28
04:52:50 --> 04:52:54
In the real world, this kind of aggregate
function can be very complicated.

29
04:52:55 --> 04:53:00
We will come back to this issue when
we talk more about map readings, but

30
04:53:00 --> 04:53:03
in this course, we'll talk about
many different data operations.

31
04:53:04 --> 04:53:07
Every operator must be efficient.

32
04:53:07 --> 04:53:12
That means every operator must
perform its task as fast as possible

33
04:53:12 --> 04:53:16
by taking up as little memory,
or our disk, as possible.

34
04:53:17 --> 04:53:20
Obviously, the time to
perform an operation

35
04:53:20 --> 04:53:23
will depend on the size of the input and
the size of the output.

36
04:53:24 --> 04:53:29
So, if there is an opportunity to use
concurrency where the operator can split

37
04:53:29 --> 04:53:33
its data and have different threads
operate on the pieces at the same time,

38
04:53:33 --> 04:53:35
it should definitely do so.

39
04:53:36 --> 04:53:40
We present a simple example of
an operator we saw on the previous slide.

40
04:53:41 --> 04:53:43
So this operator, called selection,

41
04:53:43 --> 04:53:48
refers to choosing a subset of
a set based on some conditions.

42
04:53:48 --> 04:53:52
Here we are choosing a subset of
circles whose numbers are even.

43
04:53:53 --> 04:53:58
To make it more efficient,
we can take the input data and

44
04:53:58 --> 04:54:02
partition it randomly into two groups.

45
04:54:02 --> 04:54:04
Now, for each group,

46
04:54:04 --> 04:54:08
we can concurrently run the subset
algorithm and get the partial results.

47
04:54:10 --> 04:54:13
For this operation, the partial results
can be directly sent to the output

48
04:54:13 --> 04:54:15
without any additional processing step.

1
09:45:18 --> 09:45:22
Okay we in essence store
the data efficiently.

2
09:45:22 --> 09:45:23
But is it any good?

3
09:45:24 --> 09:45:28
Are there ways of knowing if the data is
potentially error free and useful for

4
09:45:28 --> 09:45:29
the intended purpose?

5
09:45:30 --> 09:45:33
This is the issue of data quality.

6
09:45:33 --> 09:45:36
There are many reasons
why any data application,

7
09:45:36 --> 09:45:40
especially larger applications need
to be mindful of data quality.

8
09:45:41 --> 09:45:44
Here are three reasons, of course
there are more that we do not mention.

9
09:45:45 --> 09:45:50
The first reason emphasizes that
the ultimate use of big data

10
09:45:50 --> 09:45:53
is its ability to give
us actionable insight.

11
09:45:54 --> 09:45:58
Poor quality data leads to poor
analysis and hence to poor decisions.

12
09:45:59 --> 09:46:04
The second related data in regulated
industries in areas like clinical

13
09:46:04 --> 09:46:08
trials for pharmaceutical companies or
financial data like from banks.

14
09:46:09 --> 09:46:14
Errors in data in these industries
can regulate regulations leading to

15
09:46:14 --> 09:46:15
legal complications.

16
09:46:16 --> 09:46:20
The third factor is different
than the first two.

17
09:46:20 --> 09:46:24
It says if your big data should
be used by other people or

18
09:46:24 --> 09:46:27
a third party software
it's very important for

19
09:46:27 --> 09:46:31
the data to give good quality to
gain trust as a leader provider.

20
09:46:32 --> 09:46:36
A class of big data applications
is scientific, where large,

21
09:46:36 --> 09:46:40
integrated collections of data
are created by human experts to

22
09:46:40 --> 09:46:42
understand scientific questions.

23
09:46:42 --> 09:46:46
Ensuring accuracy of data will lead
to correct human engagement and

24
09:46:46 --> 09:46:48
interaction with the data system.

25
09:46:50 --> 09:46:53
Gartner, the well known
technology research and

26
09:46:53 --> 09:46:57
advising company created a 2015
industry report on big data qualities.

27
09:46:59 --> 09:47:02
In this report,
they identify the approaches to meeting

28
09:47:02 --> 09:47:04
the data quality requirements
in the industry.

29
09:47:05 --> 09:47:09
This methods include the adherence
to standards where applicable.

30
09:47:09 --> 09:47:13
It also refers to the need to
create the rules in the data system

31
09:47:13 --> 09:47:17
that can be use to check if the data
passes a set of correct this qualities.

32
09:47:17 --> 09:47:20
Like is even employed above 18.

33
09:47:20 --> 09:47:25
It also includes methods to clean
the data if it's found to have errors or

34
09:47:25 --> 09:47:26
inconsistencies.

35
09:47:27 --> 09:47:32
Further the data quality management should
include a well define work flow on how

36
09:47:32 --> 09:47:37
low quality data could be corrected to
bring it back to a high level of quality.

1
19:32:56 --> 19:32:58
There are many ways of
looking at scalability.

2
19:32:59 --> 19:33:02
And we'll consider them as
we go forward in the course.

3
19:33:03 --> 19:33:07
One way is to consider scaling up and
scaling out.

4
19:33:09 --> 19:33:13
Simply put,
it is a decision between making a machine

5
19:33:13 --> 19:33:18
that makes a server more powerful
versus adding more machines.

6
19:33:18 --> 19:33:22
The first choice will
involve adding more memory,

7
19:33:22 --> 19:33:26
replacing processes with
process of more course and

8
19:33:26 --> 19:33:30
adding more processes within a system with
a very fast internet connection speed.

9
19:33:31 --> 19:33:36
The second choice will involve adding more
machines to a relatively slower network.

10
19:33:37 --> 19:33:39
Now, there are no absolutes here.

11
19:33:40 --> 19:33:44
In many cases, we'll choose the former
to get more performance for

12
19:33:44 --> 19:33:45
large leader systems.

13
19:33:47 --> 19:33:49
The general trend in the big data world,

14
19:33:49 --> 19:33:52
however, is to target
the scale out option.

15
19:33:54 --> 19:33:56
Most big data management systems today

16
19:33:56 --> 19:34:00
are designed to operate over
a cluster up machines and

17
19:34:00 --> 19:34:05
have the ability to adjust as more
machines are added and when machines fail.

18
19:34:06 --> 19:34:11
Cluster management and management
of data operations over a cluster

19
19:34:11 --> 19:34:14
is an important component in today's
big data management systems.

20
19:34:17 --> 19:34:20
Now we'll briefly touch upon
the complex issue of data security.

21
19:34:22 --> 19:34:26
It is obvious that having more sensitive
data implies the need for more security.

22
19:34:27 --> 19:34:30
If the data is within
the walls of an organization,

23
19:34:30 --> 19:34:33
we'll still need a security plan.

24
19:34:33 --> 19:34:36
However, if a big data system
is deployed in the cloud,

25
19:34:36 --> 19:34:40
over multiple machines, security of
data becomes an even bigger challenge.

26
19:34:42 --> 19:34:45
So now we need to ensure the security for
not only the machines,

27
19:34:45 --> 19:34:49
but also the network which will be
heavily used during data transfer

28
19:34:49 --> 19:34:52
across different phases
of data operations.

29
19:34:52 --> 19:34:58
For example, if the data store and
the data analysis are performed over

30
19:34:58 --> 19:35:03
different sets of servers that
every analysis operation gets to

31
19:35:03 --> 19:35:08
an additional overhead of encrypting the
data as the data gets to the network and

32
19:35:08 --> 19:35:11
decrypting when it gets
to the processing server.

33
19:35:12 --> 19:35:14
This effectively increases
operational cost.

34
19:35:16 --> 19:35:21
As of today, while there are many
security products, the methods for

35
19:35:21 --> 19:35:26
ensuring security and achieving data
processing efficiency at the same time

36
19:35:26 --> 19:35:28
remains a research issue
in big data management.

1
15:08:23 --> 15:08:28
Now the goal of a storage infrastructure,
obviously, is to store data.

2
15:08:28 --> 15:08:30
There are two storage related
issues we consider here.

3
15:08:32 --> 15:08:35
The first is the issue of capacity.

4
15:08:35 --> 15:08:38
How much storage should we allocate?

5
15:08:38 --> 15:08:41
That means, what should be the size
of the memory, how large and

6
15:08:41 --> 15:08:44
how many disk units should we have,
and so forth.

7
15:08:45 --> 15:08:49
There is also the issue of scalability.

8
15:08:49 --> 15:08:53
Should the storage devices be
attached directly to the computers

9
15:08:53 --> 15:08:56
to make the direct IO fast but
less scalable?

10
15:08:56 --> 15:08:59
Or should the storage be
attached to the network

11
15:09:00 --> 15:09:02
that connect the computers in the cluster?

12
15:09:02 --> 15:09:05
This will make disk
access a bit slower but

13
15:09:05 --> 15:09:08
allows one to add more
storage to the system easily.

14
15:09:09 --> 15:09:12
Now these questions do
not have a simple answer.

15
15:09:12 --> 15:09:15
If you're interested, you may look up
a website given on your reading list.

16
15:09:16 --> 15:09:21
A different class of questions deals
with the speed of the IU operation.

17
15:09:22 --> 15:09:26
This question is often addressed with
this kind of diagram here called a memory

18
15:09:26 --> 15:09:30
hierarchy, or storage hierarchy, or
sometimes memory storage hierarchy.

19
15:09:32 --> 15:09:36
The top of the pyramid structure shows
a part of memory called cache memory,

20
15:09:37 --> 15:09:40
that lives inside the CPU and
is very fast.

21
15:09:40 --> 15:09:44
There are different levels of cache,
called L1, L2, L3,

22
15:09:44 --> 15:09:50
where L3 is the slowest but
still faster than what we call memory,

23
15:09:50 --> 15:09:53
shown here in orange near the middle.

24
15:09:54 --> 15:09:56
The figure shows their speed
in terms of response times.

25
15:09:58 --> 15:10:01
Notice the memory streamed here
is 65 nanoseconds per access.

26
15:10:02 --> 15:10:03
In contrast,

27
15:10:03 --> 15:10:09
the speed of the traditional hard disk
is of the order of 10 milliseconds.

28
15:10:10 --> 15:10:14
This gap has prompted the design
of many data structures and

29
15:10:14 --> 15:10:19
algorithms.that use a hard disk but
tries to minimize the cost of

30
15:10:19 --> 15:10:24
the IO operations between the fast
memory and the slower disk.

31
15:10:24 --> 15:10:27
But more recently,
a newer kind of storage.

32
15:10:30 --> 15:10:32
Very similar to the flash drives or

33
15:10:32 --> 15:10:36
USBs that we regularly use have made
an entry as a new storage medium.

34
15:10:36 --> 15:10:40
These devices are called SSDs or
Solid State Devices.

35
15:10:40 --> 15:10:42
They are much faster
than spinning hard disks.

36
15:10:43 --> 15:10:47
An even newer addition is
the method called NVMe,

37
15:10:47 --> 15:10:50
NVM stands for non-volatile memory,

38
15:10:50 --> 15:10:55
that makes data transfer between SSDs and
memory much faster.

39
15:10:56 --> 15:11:01
What all this means in a big data system
is that now we have the choice of

40
15:11:01 --> 15:11:06
architecting a storage infrastructure
by choosing how much of each type of

41
15:11:06 --> 15:11:07
storage we need to have.

42
15:11:07 --> 15:11:12
In my own research with large amounts
of data, I have found that using SSDs

43
15:11:12 --> 15:11:18
speed up all look up operations in data by
at least a factor of ten over hard drives.

44
15:11:18 --> 15:11:22
Of course the flip side of
this is the cost factor.

45
15:11:22 --> 15:11:26
The components become increasingly more
expensive as we go from the lower layers

46
15:11:26 --> 15:11:28
of the pyramid to the upper layers.

47
15:11:28 --> 15:11:31
So ultimately, it becomes an issue
of cost-benefit tradeoff.

1
06:19:54 --> 06:19:59
[SOUND] Let us consider
a real life application to

2
06:19:59 --> 06:20:05
demonstrate the utility and
the challenges of big data.

3
06:20:05 --> 06:20:08
Many industries naturally deal
with large amounts of data.

4
06:20:09 --> 06:20:13
For our discussion, we consider
an energy company that provides gas and

5
06:20:13 --> 06:20:16
electricity to its
consumers in an urban area.

6
06:20:18 --> 06:20:22
In this news report, you can see
that Commonwealth Edison, or Con Ed,

7
06:20:22 --> 06:20:24
the gas and electric provider of New York,

8
06:20:24 --> 06:20:29
decided to place smart meters
all through its jurisdictions.

9
06:20:29 --> 06:20:32
That comes to 4.7 million smart meters.

10
06:20:33 --> 06:20:39
Now smart meters are smart because aside
from measuring energy consumption,

11
06:20:39 --> 06:20:42
they have a two way communication
capability between the meter and

12
06:20:42 --> 06:20:45
the central system at the gas and
electric company.

13
06:20:45 --> 06:20:50
In other words, they generate real time
data from the meters to be stored and

14
06:20:50 --> 06:20:52
processed at the central facility.

15
06:20:53 --> 06:20:54
How much data?

16
06:20:55 --> 06:20:57
According to this report,

17
06:20:57 --> 06:21:00
the number of data received at
the center is 1.5 billion per day.

18
06:21:00 --> 06:21:07
So the system will not only consume
this data, but process it and

19
06:21:08 --> 06:21:14
produce output at 15-minute intervals,
and sometimes, 5 minute intervals.

20
06:21:14 --> 06:21:16
Let's do the math.

21
06:21:16 --> 06:21:18
That comes to ingesting and processing

22
06:21:20 --> 06:21:24
about 10.5 million data
points per 15 minutes.

23
06:21:26 --> 06:21:30
So what kind of computation must
take place within these 15 minutes?

24
06:21:30 --> 06:21:35
Well, one obvious computation is billing,
where one needs to compute who, especially

25
06:21:35 --> 06:21:40
in the commercial sector, who actually
owns the meter and should be billed?

26
06:21:41 --> 06:21:43
This requires combining the meter data

27
06:21:43 --> 06:21:47
with the data in the customer
database maintained by the company.

28
06:21:47 --> 06:21:51
But let's just consider
computation related to analytics.

29
06:21:51 --> 06:21:55
We can list at least four
different kinds of computations.

30
06:21:56 --> 06:22:01
The first is computing the consumption
pattern per user, not per meter.

31
06:22:01 --> 06:22:06
Per user, where the output is
a histogram of hourly usage.

32
06:22:06 --> 06:22:10
So the x axis of the histogram
is hourly intervals And

33
06:22:10 --> 06:22:13
the y-axis is a number of units consumed.

34
06:22:14 --> 06:22:19
This leads to the computed both daily and
over larger time periods.

35
06:22:19 --> 06:22:22
To determine the hourly requirements for
this consumer.

36
06:22:24 --> 06:22:27
The second computation relates
to estimating the effects of

37
06:22:27 --> 06:22:31
outdoor temperature on the electricity
consumption of each consumer.

38
06:22:32 --> 06:22:34
For those you who
are statistically inclined,

39
06:22:34 --> 06:22:39
this often involves fitting a piece-wise
linear progression model to the data.

40
06:22:42 --> 06:22:47
The third task is to extract the daily
consumption trends that occur

41
06:22:47 --> 06:22:49
regardless of the outdoor temperature.

42
06:22:49 --> 06:22:53
This is again a statistical computation,
and may require something like

43
06:22:53 --> 06:22:57
a periodic alter regression algorithm,
for time series theta.

44
06:22:57 --> 06:22:59
The algorithm is not that important there.

45
06:23:00 --> 06:23:05
What's more important is the ability to
make make good prediction has a direct

46
06:23:05 --> 06:23:09
economic impact because the company
needs to buy energy from others.

47
06:23:09 --> 06:23:14
For example, an under-prediction
implies they'll end up paying more for

48
06:23:14 --> 06:23:17
buying energy at the last moment to
meet the consumer's requirements.

49
06:23:19 --> 06:23:24
The fourth task is to find groups of
similar consumers based on their usage

50
06:23:24 --> 06:23:28
pattern so that the company can determine
how many distinct groups of customers

51
06:23:28 --> 06:23:33
there are and design targeted energy
saving campaigns for each group.

52
06:23:34 --> 06:23:39
This requires finding similarities
over large number of time series data,

53
06:23:39 --> 06:23:41
which is a complex computation.

54
06:23:42 --> 06:23:46
Regardless of the number and
complexity of computation required,

55
06:23:46 --> 06:23:51
the company's constrained by the fact that
it has only 15 minutes to process the data

56
06:23:51 --> 06:23:56
before the next and
computation has to be performed.

57
06:23:56 --> 06:24:00
That issue is not just
the bigness of the data, but

58
06:24:00 --> 06:24:03
the strip and
strings of the arrival to output time.

59
06:24:05 --> 06:24:12
The analytics has value, only if it can be
completed within the life-cycle deadline.

60
06:24:12 --> 06:24:16
So if we were to design a big data system
for such a company, you would need to

61
06:24:16 --> 06:24:20
understand how much are the computation
can be executed in parallel And

62
06:24:20 --> 06:24:25
how many machines with what kind of
capability are required to handle the data

63
06:24:25 --> 06:24:29
rate and the number and complexity of
the analytical computations needed?

1
12:44:30 --> 12:44:31
Hi, my name is Chad Berkley.

2
12:44:31 --> 12:44:36
I'm the CTO of FlightStats, and I'm here
today to talk to you a little bit about

3
12:44:36 --> 12:44:40
our platform and
how we acquire and process data.

4
12:44:40 --> 12:44:44
But first of all, I'd like to start by
just kind of introducing the company and

5
12:44:44 --> 12:44:49
telling you a little bit
about what we're all about.

6
12:44:49 --> 12:44:53
So FlightStats is a data company and

7
12:44:53 --> 12:44:59
we basically are the leading provider
of global real-time flight status data.

8
12:44:59 --> 12:45:04
We pull in data from over 500 sources and
we aggregate that data back together, and

9
12:45:04 --> 12:45:09
we sell it out to our customers which our
other businesses as well as consumers.

10
12:45:10 --> 12:45:13
So just to give you a little bit
of information about the scope and

11
12:45:13 --> 12:45:15
scale of what we do.

12
12:45:15 --> 12:45:18
Like I said,
we have over 500 sources of data.

13
12:45:18 --> 12:45:23
And on a daily basis,
we process about 15 million flight events,

14
12:45:23 --> 12:45:27
those that includes landings,
arrivals, departures.

15
12:45:28 --> 12:45:33
Any time the status of the flight changes,
we got some sort of message on that.

16
12:45:33 --> 12:45:38
We process about 260 million aircraft
positions per day, so we have an extensive

17
12:45:38 --> 12:45:44
network that monitors graph positions for
realtime flight tracking applications.

18
12:45:44 --> 12:45:49
And we also handle about One million PNRs,
or passenger name records,

19
12:45:49 --> 12:45:54
which are the actual data type
of an itenerary any time you

20
12:45:54 --> 12:45:59
book travel, a PNR is created for
you, for your travel.

21
12:45:59 --> 12:46:04
And it includes all of the segments,
like air travel, ferries,

22
12:46:04 --> 12:46:09
hotels, taxis, Anyhting that
can be scheduled on your trip.

23
12:46:10 --> 12:46:12
And we basically take in all that data and

24
12:46:12 --> 12:46:17
we aggregate it together and
we sell it back out.

25
12:46:17 --> 12:46:20
And how most people kind of know us for
FlightStats.com, that's our

26
12:46:20 --> 12:46:25
consumer site for business where we
handle about 2 million daily requests.

27
12:46:25 --> 12:46:27
And we handle about 1
million mobile app requests.

28
12:46:28 --> 12:46:35
That our b to b side, we cert out a lot
of data by APIs and real time data feeds.

29
12:46:35 --> 12:46:39
People make about 15 million
API requests to us everyday.

30
12:46:39 --> 12:46:43
And we also send out about one and half
million flight and trip notifications.

31
12:46:43 --> 12:46:47
So if you get a push
notification to your phone,

32
12:46:47 --> 12:46:49
telling you that your flight is delayed or
on time.

33
12:46:49 --> 12:46:52
That possibly has come from us.

34
12:46:52 --> 12:46:57
So a little bit about how the data
flows through our company.

35
12:46:57 --> 12:47:00
We bring in all these
different types of data and

36
12:47:00 --> 12:47:04
our sources and it flows through
our data acquisition team.

37
12:47:04 --> 12:47:10
We have a team whose primary purpose
is to pull in all sorts of raw data,

38
12:47:10 --> 12:47:12
a very heterogeneous datasets.

39
12:47:12 --> 12:47:18
And process that into a normalized form.

40
12:47:18 --> 12:47:21
So if you kind of follow the blue arrow
in this diagram you can see that it goes

41
12:47:21 --> 12:47:23
through this raw data
channel through the data hub.

42
12:47:23 --> 12:47:26
Which the data hub is a central
component of our system that I'll talk

43
12:47:26 --> 12:47:29
a little bit more about in a second.

44
12:47:29 --> 12:47:32
So the blue data, the blue line is
raw data coming in from the source

45
12:47:32 --> 12:47:35
It goes through our data
acquisition system,

46
12:47:35 --> 12:47:39
it turns into that purple line
which is a normalized form.

47
12:47:39 --> 12:47:43
It then goes back through our data hub
again and into our processing engine.

48
12:47:43 --> 12:47:47
Our processing engine is really where
most of the business logic happens.

49
12:47:47 --> 12:47:50
The first thing we have to do
is we have to match any piece of

50
12:47:50 --> 12:47:53
flight information against
a flight that we know about and

51
12:47:53 --> 12:47:56
primarily the way we know about
flights is through schedules.

52
12:47:56 --> 12:48:01
So we import schedules on a daily
basis from one of our partner

53
12:48:01 --> 12:48:02
schedule providers.

54
12:48:03 --> 12:48:08
That data,
once it's matched is then processed and

55
12:48:08 --> 12:48:12
the processing basically looks at each
message, and tries to determine if

56
12:48:12 --> 12:48:15
we think that that message needs
to be passed on to consumers.

57
12:48:15 --> 12:48:19
So you know it looks at things like,
have we seen that message before.

58
12:48:19 --> 12:48:21
Or is it a duplicate?

59
12:48:21 --> 12:48:23
Is it from a data source that we trust?

60
12:48:24 --> 12:48:28
Are there other things going on
that we need to know about that

61
12:48:28 --> 12:48:31
may impact whether that message is true or
not?

62
12:48:31 --> 12:48:34
And once we decide that a message
should be passed through,

63
12:48:34 --> 12:48:36
if you follow the green line.

64
12:48:36 --> 12:48:38
It goes into our process
data channel on our hub, and

65
12:48:38 --> 12:48:41
it's then pushed out to
a couple different places.

66
12:48:41 --> 12:48:42
So first of all,

67
12:48:42 --> 12:48:46
it goes into our production database which
is where all of our real time data lives.

68
12:48:46 --> 12:48:50
That database serves data to our websites,

69
12:48:50 --> 12:48:55
to our mobile apps, and
a variety of other places.

70
12:48:55 --> 12:48:57
It also goes into our
data warehouse which,

71
12:48:57 --> 12:49:00
is where our analytics products use it.

72
12:49:00 --> 12:49:02
I'll talk a bit more
about that in a minute.

73
12:49:02 --> 12:49:06
And then, the stream of data actually
goes up to so many of our customers.

74
12:49:06 --> 12:49:08
We don't need a database on our side.

75
12:49:08 --> 12:49:10
They would rather build
a database on their side so

76
12:49:10 --> 12:49:14
we actually just stream all of
the processed data directly to them.

77
12:49:14 --> 12:49:17
They then host it within
their own systems.

78
12:49:20 --> 12:49:27
So a little bit more about the hub, the
hub is central to how we move data around.

79
12:49:27 --> 12:49:30
It's a technology that
we developed in-house.

80
12:49:30 --> 12:49:34
And it's an object storage based,
scalable, highly available,

81
12:49:34 --> 12:49:38
multi-channel data queuing and
eventing system.

82
12:49:38 --> 12:49:44
The object storage part is,
we use Amazon S3 to store this data.

83
12:49:44 --> 12:49:47
So it's an object storage system.

84
12:49:47 --> 12:49:49
It's scalable,
we can scale it horizontally or

85
12:49:49 --> 12:49:55
vertically depending on, but the what
type of data is flowing through it.

86
12:49:55 --> 12:49:56
It's highly available meaning,

87
12:49:56 --> 12:49:59
that we have multiple instances
of it in different data centers.

88
12:49:59 --> 12:50:02
So if one that goes down we can
easily pull another one up or

89
12:50:02 --> 12:50:05
it's we're going to cross
multiple instances.

90
12:50:05 --> 12:50:07
And then it's multi channel.

91
12:50:07 --> 12:50:12
So it's got a rest interface and
any surface can create

92
12:50:12 --> 12:50:16
a new channel within the system and
start posting data to it.

93
12:50:16 --> 12:50:21
That data is then queued based on
the time that it comes in, and

94
12:50:21 --> 12:50:24
other services can be listening for
events on those channels.

95
12:50:24 --> 12:50:28
So, as soon as a new piece of data
comes into one of those channels,

96
12:50:28 --> 12:50:32
any service that's listening on that
channel, gets an event notification.

97
12:50:32 --> 12:50:35
They, that service can then
act upon that piece of data.

98
12:50:35 --> 12:50:39
And do whatever processing
it may need to do.

99
12:50:39 --> 12:50:44
This project is open source and
anybody can download it and use it.

100
12:50:47 --> 12:50:51
So a little bit about some of the data
that we collect and aggregate.

101
12:50:51 --> 12:50:56
And FLIFO is kind of the industry term for
flight information.

102
12:50:56 --> 12:51:02
And primarily we look at kind of
the five different parts of flight.

103
12:51:02 --> 12:51:06
So we pull in information on gate
departure, and then that becomes a runway

104
12:51:06 --> 12:51:10
departure, basically when the wheels
go up, that is a runway departure.

105
12:51:10 --> 12:51:17
We do in-flight positional tracking,
so when your flight is moving along,

106
12:51:17 --> 12:51:21
about once every ten seconds we get
notified of its latitude and longitude.

107
12:51:21 --> 12:51:23
And its heading and its speed and

108
12:51:23 --> 12:51:27
its vertical center descent rate and
several other variables.

109
12:51:28 --> 12:51:30
Then once it lands,

110
12:51:30 --> 12:51:34
as soon as the wheels touch the ground,
we're notified of a runway arrival and

111
12:51:34 --> 12:51:38
when the door is opened at the gate,
we have gate arrival information.

112
12:51:38 --> 12:51:44
All five of these data fields
come in three different forms.

113
12:51:44 --> 12:51:47
So we have a scheduled,
scheduled departure and arrival.

114
12:51:47 --> 12:51:52
We have estimated departure and arrival
which can come from a variety of sources,

115
12:51:52 --> 12:51:56
either airlines, airports,
positional data, et cetera.

116
12:51:56 --> 12:52:01
And then, we have actuals so
if we have an airport or

117
12:52:01 --> 12:52:05
an airline that's sending us data about
exactly when the wheels touch down, or

118
12:52:05 --> 12:52:09
exactly when that door opens on that
aircraft, we push that data as well.

119
12:52:09 --> 12:52:14
We also generate some
data at flight stops.

120
12:52:14 --> 12:52:19
So special incidents, if an aircraft
has an issue It's in the news.

121
12:52:19 --> 12:52:24
We do flag our content with
a message from our support staff.

122
12:52:24 --> 12:52:27
We do some prediction.

123
12:52:27 --> 12:52:31
Right now we're just starting to get into
that market, or we're actually trying to

124
12:52:31 --> 12:52:36
predict 24 hours out whether a flight
will be delayed, disrupted or on time.

125
12:52:37 --> 12:52:42
We do some synthetic positions,
so over oceans, primarily.

126
12:52:42 --> 12:52:47
We don't get tracking data
on aircraft over the oceans.

127
12:52:47 --> 12:52:51
There is currently no satellite-based
tracking system for aircraft.

128
12:52:51 --> 12:52:57
So we basically take the last
known position a heading, a speed.

129
12:52:57 --> 12:53:01
And if we have a flight plan, we'll use
a flight plan to synthesize the positions

130
12:53:01 --> 12:53:05
when we're not getting actual
positions over large bodies of water.

131
12:53:06 --> 12:53:10
We also generate notifications,
so the push alerts,

132
12:53:10 --> 12:53:15
the preflight emails,
delay notifications those types of things.

133
12:53:15 --> 12:53:19
We create those based on what we see
in the data that's coming in to us.

134
12:53:21 --> 12:53:25
We store all of our historical
data in a data warehouse,

135
12:53:25 --> 12:53:28
so right now we have six years
of historical flight data.

136
12:53:28 --> 12:53:31
And that powers our analytics products,

137
12:53:31 --> 12:53:35
so we allow airlines to do competitive
analysis, and route analysis.

138
12:53:36 --> 12:53:38
Routes are very important to airlines.

139
12:53:38 --> 12:53:40
That's how they compete
with each other and

140
12:53:40 --> 12:53:45
that's primarily how they
are judged by the FAA and

141
12:53:45 --> 12:53:50
other governmental organizations
on whether they're on-time or not.

142
12:53:50 --> 12:53:55
We also do airport operations analysis
things like taxi in and taxi out times.

143
12:53:55 --> 12:53:58
Very important for lots of airports,
runway utilization,

144
12:53:58 --> 12:54:03
hourly passenger flows through airports,
that type of information.

145
12:54:03 --> 12:54:05
And we do on-time performance metrics so.

146
12:54:05 --> 12:54:08
Airlines can look at how they're doing.

147
12:54:08 --> 12:54:10
How many flights did they complete?

148
12:54:10 --> 12:54:12
How many flights were on
time within 14 minutes?

149
12:54:14 --> 12:54:17
And they can compare themselves
to their competitors.

150
12:54:19 --> 12:54:24
So we host all of this in
a hybrid cloud architecture.

151
12:54:24 --> 12:54:28
Hybrid cloud basically means that we have
our own private datacenter resources and

152
12:54:28 --> 12:54:33
we also host resources in
the Amazon Web Services cloud.

153
12:54:34 --> 12:54:38
Most of our core data processing and
service layer is in our private data

154
12:54:38 --> 12:54:42
center and we're getting ready to spin
up a second private data center as well.

155
12:54:42 --> 12:54:47
Right now, our main data center is in
Portland, Oregon, and we're going to spin

156
12:54:47 --> 12:54:52
another one up on the east coast of
the United States probably in Q2 or Q3.

157
12:54:54 --> 12:54:58
For our API's we try to keep
those close to our customers so

158
12:54:58 --> 12:55:01
API end points and
web end points live in Amazon.

159
12:55:01 --> 12:55:07
And they are automatically routed to
whichever end point is closest to you,

160
12:55:07 --> 12:55:08
you will automatically be routed to them.

161
12:55:09 --> 12:55:13
All of our private infrastructure
is virtualized with VMWare.

162
12:55:13 --> 12:55:17
We pretty much have a fully
virtualized environment.

163
12:55:20 --> 12:55:27
And we're an Agile shop, so
we have six small, fast teams.

164
12:55:27 --> 12:55:32
Those are product centric teams, we
allow them to be as customer interactive

165
12:55:32 --> 12:55:35
as they need to be, and
we try to make our teams semi autonomous.

166
12:55:35 --> 12:55:39
So, teams get to choose their own tools,
they get to choose their

167
12:55:39 --> 12:55:44
own development methodologies,
they choose a variety of things.

168
12:55:44 --> 12:55:47
And We physically allow them

169
12:55:47 --> 12:55:49
to do what they need to do to get
their job done as quickly as possible.

170
12:55:51 --> 12:55:53
We try to automate everything.

171
12:55:53 --> 12:55:56
You do something once manually and
then the next time you write a script or

172
12:55:56 --> 12:55:57
program to do it.

173
12:55:57 --> 12:55:58
And we also measure everything.

174
12:55:58 --> 12:56:02
Right now, we're taking in about
2.5 billion metrics per month off

175
12:56:02 --> 12:56:03
of our systems.

176
12:56:03 --> 12:56:07
And we use those metrics to monitor
our application performance,

177
12:56:07 --> 12:56:11
to monitor revenue, to monitor pretty
much everything we do in the company.

178
12:56:11 --> 12:56:14
We really try to enable
total system awareness,

179
12:56:14 --> 12:56:19
everything from the hardware layer
up to the website is monitored.

180
12:56:20 --> 12:56:24
And we use industry best practices and
tools and of course, we try to recruit and

181
12:56:24 --> 12:56:28
hire the best talent possible a little
bit about our software stock.

182
12:56:28 --> 12:56:30
We're primarily a Java shop.

183
12:56:31 --> 12:56:34
Our core processing services
are all written in Java.

184
12:56:34 --> 12:56:38
We do use node JS in our
Microservice Edge layer.

185
12:56:38 --> 12:56:43
And node JS is actually starting to move
more down into the processing service

186
12:56:43 --> 12:56:44
layer as well.

187
12:56:44 --> 12:56:46
We use many different types of data bases.

188
12:56:46 --> 12:56:50
Our primary realtime
database is Post Press.

189
12:56:50 --> 12:56:54
And we use Mongo for
the backend of our API services.

190
12:56:55 --> 12:56:59
On the website, we're all HTML5 and
we're moving to React and Redux, and

191
12:56:59 --> 12:57:04
we're making use of Elasticsearch for
quick searching and indexing on our data.

192
12:57:04 --> 12:57:08
And, of course, we have iOS and
Android mobile applications.

193
12:57:08 --> 12:57:14
So you can find out more about
Flightstats on our website.

194
12:57:14 --> 12:57:16
If you need data for your applications,

195
12:57:16 --> 12:57:20
please go to the developer center
at developer.flightstats.com.

196
12:57:20 --> 12:57:23
You can sign up for
a free test account and

197
12:57:23 --> 12:57:26
be able to pull data
directly off of our APIs.

198
12:57:26 --> 12:57:29
If you're interested in the Hub,
like I said that's open source.

199
12:57:29 --> 12:57:33
Please check out the Git Hub page and
if you have any additional questions,

200
12:57:33 --> 12:57:37
feel free to contact
myself John Berkeley and

201
12:57:37 --> 12:57:41
I'd be happy to answer any
of your questions via email.

202
12:57:41 --> 12:57:43
Thanks for listening today and
hope you have a great day.

203
12:57:43 --> 12:57:44
Bye.

1
01:42:17 --> 01:42:21
Different data sources in the game
industry include using your finger.

2
01:42:21 --> 01:42:23
What type of device is coming from?

3
01:42:23 --> 01:42:25
The amount of headsets.

4
01:42:25 --> 01:42:26
It's pretty much infinite,

5
01:42:26 --> 01:42:29
as far as the number of ways we
can bring data in from the game.

6
01:42:29 --> 01:42:34
And it could be joystick or mouse,
keyboards, there's lots of ways

7
01:42:34 --> 01:42:38
as well as what happens inside
the game itself as far cars or

8
01:42:38 --> 01:42:42
the driving, tires,
flying machines, anything.

9
01:42:49 --> 01:42:52
The volume of data,
it really depends on the type of game and

10
01:42:52 --> 01:42:54
how often they want to send the data in.

11
01:42:54 --> 01:42:58
How many types of events they've tagged
and how many users are playing the game.

12
01:42:58 --> 01:43:01
So if you have a user, you've 5 million
users that are playing your game and

13
01:43:01 --> 01:43:04
you're tapping and
you're tracking each tap of the screen,

14
01:43:04 --> 01:43:07
of where they went or each click of
the mouse as they were using it,

15
01:43:07 --> 01:43:09
you're going to get
a lot of volume of data.

16
01:43:09 --> 01:43:14
And so
you need to be prepared to bring in a lot

17
01:43:14 --> 01:43:20
of different data very, very quickly.

18
01:43:20 --> 01:43:23
As far as the variety of data, it depends.

19
01:43:23 --> 01:43:25
You have round pizzas and
you have round tires.

20
01:43:25 --> 01:43:26
I mean, they're completely different.

21
01:43:26 --> 01:43:30
They're both round, but there's different
ways that you're going to want to know how

22
01:43:30 --> 01:43:34
many pepperoni are on one pizza and
how many lug nuts go on a tire.

23
01:43:34 --> 01:43:36
So the variety is really unlimited,
as well.

24
01:43:36 --> 01:43:38
It really just depends on each game.

25
01:43:38 --> 01:43:41
So, you have to be prepared to
bring in all kinds of data.

26
01:43:41 --> 01:43:45
Touch data, wheel data,
track speeds, anything.

27
01:43:45 --> 01:43:50
And if you put taxonomy together that
you can define as an example of verb,

28
01:43:50 --> 01:43:56
object, location, value and any number
of other sources, then you can basically

29
01:43:56 --> 01:44:01
track anything you want as long as they
all fall into the same kind of buckets.

30
01:44:01 --> 01:44:03
And the buckets can be different
sizes based on the type of events.

31
01:44:03 --> 01:44:06
They don't all need to be 4,
some can be 2, some can be 20,

32
01:44:06 --> 01:44:08
it doesn't really matter.

33
01:44:15 --> 01:44:19
The modeling challenge has really come
down to who designs the structure at which

34
01:44:19 --> 01:44:22
you store your data and
how you want to retrieve that data.

35
01:44:22 --> 01:44:26
Those kind of of storage and
retrieval models are very, very important,

36
01:44:26 --> 01:44:29
because what it really
comes down to is speed.

37
01:44:29 --> 01:44:32
You can record a lot of data and
it can take you five years to query it,

38
01:44:32 --> 01:44:33
it doesn't really do you any good.

39
01:44:33 --> 01:44:38
So you need to make sure you plan for
reporting speed, because that's ultimately

40
01:44:38 --> 01:44:42
what within the organisation needs is
the ability to report on it very quickly.

41
01:44:42 --> 01:44:47
The management challenges really come
down to trying to figure out what data to

42
01:44:47 --> 01:44:48
store.

43
01:44:48 --> 01:44:51
A lot of times we go into various
companies and you've got producers sitting

44
01:44:51 --> 01:44:54
across the hall from designers, and
they don't even know each other.

45
01:44:54 --> 01:44:57
They don't realize what they want and
the programmer says,

46
01:44:57 --> 01:45:01
I'm going to put these events in and the
product manager who wants to figure out

47
01:45:01 --> 01:45:05
how many times somebody crashed says,
well, I need these events in.

48
01:45:05 --> 01:45:09
So unless they're communicating, you're
going to get the wrong type of data.

49
01:45:09 --> 01:45:13
So the management challenge is trying
to make sure everybody communicates,

50
01:45:13 --> 01:45:15
they decide on the taxonomy and
the structure and

51
01:45:15 --> 01:45:19
then we can go forward with tagging and
getting in the entire game working.

52
01:45:25 --> 01:45:27
We process, stayed in two main ways.

53
01:45:27 --> 01:45:29
One is streaming data.

54
01:45:29 --> 01:45:31
One is batched or scheduled data.

55
01:45:31 --> 01:45:35
Streaming data has scripts that run
instantly the minute the data arrives.

56
01:45:35 --> 01:45:38
And so as the data come in,
it gets processed and

57
01:45:38 --> 01:45:40
then stored In a reporting format.

58
01:45:40 --> 01:45:46
So they can easily generate reports
up to the second very, very quickly.

59
01:45:46 --> 01:45:49
Batch processing data really depends on
the type of data where it's coming from.

60
01:45:49 --> 01:45:53
Most of the time when we download
data from iTunes or YouTube or

61
01:45:53 --> 01:45:57
something like that, it comes in a CSV or
a very similar format.

62
01:45:57 --> 01:46:00
There's not really a lot of
processing we need to do.

63
01:46:00 --> 01:46:02
It's more of ingesting that data.

64
01:46:02 --> 01:46:06
There are processes we need to run
with some type of batch data, but

65
01:46:06 --> 01:46:10
most of the batch data we
receive comes in as a CSV or

66
01:46:10 --> 01:46:12
other similar already processed formats.

67
01:46:12 --> 01:46:17
So typically, while you can do processing
in both modes most processing typically

68
01:46:17 --> 01:46:21
happens with the streaming real-time data
than it does with offline batch data.

69
01:46:27 --> 01:46:33
We actually didn't use any
technology in the a big data space.

70
01:46:33 --> 01:46:35
We created our own, from scratch.

71
01:46:35 --> 01:46:39
What we did was we decided
what kind of model we wanted.

72
01:46:39 --> 01:46:40
How we were going to store data?

73
01:46:40 --> 01:46:42
How we were going to retrieve data?

74
01:46:42 --> 01:46:44
And ultimately,
how we were going to reduce the data?

75
01:46:44 --> 01:46:47
Because the more and more and more data
you have, the slower it is to actually do

76
01:46:47 --> 01:46:50
a query, because you have to look through
all the different pieces of data.

77
01:46:50 --> 01:46:54
A lot of databases solve this problem for
you, but they were really doing it in more

78
01:46:54 --> 01:46:57
generic way were we needed
something very specific.

79
01:46:57 --> 01:47:02
So we started from scratch building our
own data storage and retrieval, and

80
01:47:02 --> 01:47:04
reporting from the ground up.

81
01:47:04 --> 01:47:08
When it came to scalability,
it was really about designing the parts of

82
01:47:08 --> 01:47:10
the system that could be
independently scaled.

83
01:47:10 --> 01:47:14
So if data's coming in in real-time,
we send that into what we call a gateway.

84
01:47:14 --> 01:47:17
And the gateway could be three gateways,
let's say.

85
01:47:17 --> 01:47:18
But if the data starts
getting over loaded,

86
01:47:18 --> 01:47:20
I just have to add another gateway.

87
01:47:20 --> 01:47:23
And a gateway is just this little
light layer that just receives data,

88
01:47:23 --> 01:47:26
passes it on and goes back to it's job,
doesn't do anything else.

89
01:47:26 --> 01:47:29
So it can receive a lot
of data very quickly, but

90
01:47:29 --> 01:47:31
also I can just add another one.

91
01:47:31 --> 01:47:34
And it just automatically logs in and
adds itself to a list and

92
01:47:34 --> 01:47:37
now the data is being distributed
amongst four gateways instead.

93
01:47:37 --> 01:47:39
Query engine is the same way.

94
01:47:39 --> 01:47:41
When you're doing queries to try
to get data out of the system, so

95
01:47:41 --> 01:47:42
you can build reports.

96
01:47:42 --> 01:47:46
If I need more query engines,
because people are doing more reporting,

97
01:47:46 --> 01:47:47
we can add more query engines.

98
01:47:47 --> 01:47:51
So, the idea behind scalability is
trying to break the services up into

99
01:47:51 --> 01:47:54
the type of services that
they most make sense.

100
01:47:54 --> 01:47:55
So that if you need to,

101
01:47:55 --> 01:47:58
you can add just the service without
rebuilding the entire platform.

102
01:48:05 --> 01:48:08
My advice for people designing systems for
big data is to first,

103
01:48:08 --> 01:48:11
try to understand what
you want to accomplish.

104
01:48:11 --> 01:48:12
What's the goal?

105
01:48:12 --> 01:48:15
I mean, we're going to ingest everything
and we're going to report on everything.

106
01:48:15 --> 01:48:22
It's not really something that you can
achieve without some special thought.

107
01:48:22 --> 01:48:25
If you're going to focus on,
your area's going to be say gardening,

108
01:48:25 --> 01:48:29
then look at what kind of things you're
going to do in the gardening area and

109
01:48:29 --> 01:48:31
try to focus on what that
type of data is going to be.

110
01:48:31 --> 01:48:34
This isn't going to restrict you
to only being a gardener, but

111
01:48:34 --> 01:48:37
it is going to give you focus
on how to design your system, so

112
01:48:37 --> 01:48:39
that they're actually going to work for
you.

113
01:48:39 --> 01:48:43
You're going to continually evolve your
systems, add more things to them and

114
01:48:43 --> 01:48:44
grow them.

115
01:48:44 --> 01:48:47
I wouldn't suggest starting with
an unlimited variety of options and

116
01:48:47 --> 01:48:49
hoping you're going to
solve all the problems.

117
01:48:49 --> 01:48:56
Start with the goal of what your current
solution is and expand from there.

118
01:49:02 --> 01:49:05
Data can be fun, but
it can also be overwhelming.

119
01:49:05 --> 01:49:11
So, try to keep the data in mind
without keeping the world in mind and

120
01:49:11 --> 01:49:13
I think you'll be just fine.

1
03:31:21 --> 03:31:26
In this video we will provide a quick
summary of the main points from our

2
03:31:26 --> 03:31:28
first course on introduction to big data.

3
03:31:30 --> 03:31:32
If you have just completed
our first course and

4
03:31:32 --> 03:31:36
do not need a refresher,
you may now skip to the next lecture.

5
03:31:38 --> 03:31:43
After this video,
you will be able to recall

6
03:31:43 --> 03:31:47
what started the big data era and
the three main big data sources.

7
03:31:49 --> 03:31:52
Summarize the volume,
variety, velocity and

8
03:31:52 --> 03:31:55
veracity issues related to each source.

9
03:31:56 --> 03:32:02
Explain the five step data science
process to gain value from big data.

10
03:32:03 --> 03:32:06
Remember the main elements
of the Hadoop Stack.

11
03:32:09 --> 03:32:14
We began our first course with
an explanation of how a lead torrent of

12
03:32:14 --> 03:32:20
big data combined with cloud computing
capabilities to process data anytime and

13
03:32:20 --> 03:32:25
anywhere has been at the core of
the launch of the Big Data Era.

14
03:32:28 --> 03:32:33
This big torrent of big data is often
boil down to a few varieties of data

15
03:32:33 --> 03:32:38
generated by machines, people and

16
03:32:38 --> 03:32:43
organizations with machine generated data.

17
03:32:43 --> 03:32:48
We refer to the data generated from real
time sensors and industrial machinery or

18
03:32:48 --> 03:32:50
vehicles.

19
03:32:50 --> 03:32:53
Web logs that track user behavior online.

20
03:32:53 --> 03:32:55
environmental sensors,

21
03:32:55 --> 03:32:58
personal health trackers among
many other sense data sources.

22
03:33:00 --> 03:33:06
With human generated data, we really refer
to the vast amount of social media data,

23
03:33:06 --> 03:33:09
status updates, tweets, photos and videos.

24
03:33:11 --> 03:33:16
With organization generated data,
we refer to more traditional types of data

25
03:33:16 --> 03:33:19
including transaction
information data bases and

26
03:33:19 --> 03:33:22
structure data often
stored in data warehouses.

27
03:33:23 --> 03:33:30
Note that big data can be structured,
semi-structured, and unstructured.

28
03:33:30 --> 03:33:35
Which is a topic we will talk about
more and in depth later in this course.

29
03:33:38 --> 03:33:43
Whatever your big data application is and
the types of big data you're using,

30
03:33:43 --> 03:33:49
the real value will come from integrating
different types of data sources and

31
03:33:49 --> 03:33:51
analyzing them at scale.

32
03:33:53 --> 03:33:56
Overall, by modeling, managing and

33
03:33:56 --> 03:34:00
integrating diverse streams
to improve our business and

34
03:34:00 --> 03:34:04
add value to our big data even
before we start analyzing it.

35
03:34:05 --> 03:34:07
As a part of modeling and

36
03:34:07 --> 03:34:12
managing big data is focusing on
the dimensions of scale availability and

37
03:34:12 --> 03:34:18
considering the challenges associated with
this dimensions to pick the right tools.

38
03:34:22 --> 03:34:27
Volume, variety and
velocity are the main dimensions which

39
03:34:27 --> 03:34:32
we characterized big data and
describe its challenges.

40
03:34:33 --> 03:34:38
We have huge amounts of data
in different formats and

41
03:34:38 --> 03:34:41
varying quality which
must be processed quickly

42
03:34:44 --> 03:34:50
veracity refers to the biases,
noise, and abnormality in data,

43
03:34:50 --> 03:34:56
or the unmeasurable certainty is in the
truthfulness and trustworthiness of data,

44
03:34:57 --> 03:35:01
and valence refers to
the connectedness of big data.

45
03:35:01 --> 03:35:03
Such as in the form of graph networks.

46
03:35:06 --> 03:35:12
Each V presents a challenging
dimension of big data mainly of size,

47
03:35:12 --> 03:35:17
complexity, speed, quality,
and consecutiveness.

48
03:35:17 --> 03:35:20
Although we can list some
other v' based on the context.

49
03:35:20 --> 03:35:25
We prefer to list these five as
fundamental dimensions which

50
03:35:25 --> 03:35:28
this big data specialization
helps you work on.

51
03:35:29 --> 03:35:35
Moreover, we must be sure to
never forget the sixth V: Value,

52
03:35:35 --> 03:35:38
at the heart of the big
data challenge is turning

53
03:35:38 --> 03:35:41
all of the other dimensions into
truly useful business value.

54
03:35:42 --> 03:35:46
How will Big Data benefit you and
your organization?

55
03:35:46 --> 03:35:50
The idea behind processing all
this Big Data in the first place

56
03:35:50 --> 03:35:53
is to bring value to the problem at hand.

57
03:35:54 --> 03:35:58
We need to take steps into
Big Data engineering and

58
03:35:58 --> 03:36:02
scalable data science to
generate value out of Big Data.

59
03:36:03 --> 03:36:04
We have all heard it.

60
03:36:04 --> 03:36:10
Data signs turns big data into insides,
or even actions.

61
03:36:11 --> 03:36:12
But what does that really mean?

62
03:36:14 --> 03:36:19
Data signs can be taught of as
the basis for empirical research.

63
03:36:19 --> 03:36:22
Like data is used to induce
information on the observations.

64
03:36:24 --> 03:36:26
These observations are mainly data.

65
03:36:26 --> 03:36:32
In our case, big data related to
a business or scientific use case.

66
03:36:34 --> 03:36:40
Inside is a term we use to refer to
the data products of data science.

67
03:36:40 --> 03:36:43
It is extracted from
a diverse amount of data

68
03:36:43 --> 03:36:47
through a combination of exploratory
data analysis and modeling.

69
03:36:48 --> 03:36:53
The questions are sometimes less
specific and it can require looking

70
03:36:53 --> 03:36:58
carefully at the data for patterns in
it to come up with a specific question.

71
03:37:00 --> 03:37:05
Another important point to recognize
is that data science is not static

72
03:37:05 --> 03:37:07
one time analysis.

73
03:37:07 --> 03:37:12
It involves a process where models
where you generate give us insights

74
03:37:12 --> 03:37:17
are constantly improve to a further and
prequel evidence and iterations.

1
07:08:38 --> 07:08:40
There are many ways to
look at this process.

2
07:08:41 --> 07:08:48
One way of looking at it
as two distinct activities.

3
07:08:48 --> 07:08:54
Mainly, big data engineering and
big data analytics,

4
07:08:54 --> 07:08:58
or computational big data
science as I like to call it,

5
07:08:58 --> 07:09:01
since more than simple
analytics are being performed.

6
07:09:03 --> 07:09:11
A more detailed way of looking at
the process reveals five listing steps or

7
07:09:11 --> 07:09:15
activities of the data science process,

8
07:09:15 --> 07:09:20
namely acquire, prepare,
analyze, report, and act.

9
07:09:20 --> 07:09:25
We can simply say that data science
happens at the boundary of all the steps.

10
07:09:25 --> 07:09:29
Ideally, this process should
support experimental work,

11
07:09:29 --> 07:09:35
which is constantly iterated and
leads to more scientific exploration,

12
07:09:35 --> 07:09:41
as well as producing actionable
results during these explorations

13
07:09:41 --> 07:09:45
using dynamic scalability on big data and
cloud platforms.

14
07:09:48 --> 07:09:52
This five step process can be used in
alternative ways in real life big data

15
07:09:52 --> 07:09:57
applications if we add the dependencies
of different tools to each other.

16
07:09:59 --> 07:10:02
The influence of big data pushes for

17
07:10:02 --> 07:10:07
alternative scalability approaches
at each step of the process.

18
07:10:08 --> 07:10:12
Acquire includes anything
that helps us retrieve data,

19
07:10:12 --> 07:10:16
including finding, accessing,
acquiring, and moving data.

20
07:10:18 --> 07:10:26
It includes identification of and
authenticated access to all related data,

21
07:10:26 --> 07:10:30
as well as transportation of data
from sources to destinations.

22
07:10:31 --> 07:10:37
It includes ways to subset and
match the data to regions or

23
07:10:37 --> 07:10:42
times of interest, which we sometimes
refer to as geospatial querying.

24
07:10:44 --> 07:10:48
We divide the prepare data
step into two sub-steps,

25
07:10:48 --> 07:10:51
based on the nature of the activity.

26
07:10:52 --> 07:10:58
The first step in data preparation
involves exploring the data

27
07:10:58 --> 07:11:02
to understand its nature,
what it means, its quality, and format.

28
07:11:04 --> 07:11:07
It often takes a preliminary
analysis of data, or

29
07:11:07 --> 07:11:09
samples of data, to understand it.

30
07:11:10 --> 07:11:13
This is why this primary
step is called prepare.

31
07:11:16 --> 07:11:19
Once we know more about the data
through exploratory analysis,

32
07:11:19 --> 07:11:23
the next step is pre-processing
of data for analysis.

33
07:11:24 --> 07:11:29
It includes cleaning data,
subsetting, or filtering data, and

34
07:11:29 --> 07:11:35
creating data, which programs can read and
understand by modelling raw data

35
07:11:35 --> 07:11:41
into a more defined data model, or
packaging it using a specific data format.

36
07:11:42 --> 07:11:48
We will learn more about data models and
data formats later in this course.

37
07:11:49 --> 07:11:52
If there are multiple data sets involved,

38
07:11:52 --> 07:11:57
this step also includes integration
of different data sources or

39
07:11:57 --> 07:12:00
streams, which is a topic we will
explore in our course three.

40
07:12:03 --> 07:12:09
The prepared data then would be
passed on to the analysis step,

41
07:12:09 --> 07:12:12
which involves selection of
analytical techniques to use,

42
07:12:12 --> 07:12:15
building a model of the data,
and analyzing results.

43
07:12:16 --> 07:12:20
This step can take a couple
of iterations on its own or

44
07:12:20 --> 07:12:24
might require a data scientist
to go back to steps 1 and

45
07:12:24 --> 07:12:27
2 to get more data or
package data in a different way.

46
07:12:28 --> 07:12:30
So, exploration never ends.

47
07:12:33 --> 07:12:39
Step 4 for communicating results includes
evaluation of analytical results,

48
07:12:39 --> 07:12:44
presenting them in a visual way,
creating reports that include

49
07:12:44 --> 07:12:49
an assessment of results with
respect to success criteria.

50
07:12:49 --> 07:12:56
Activities in this step can often be
referred to with terms like interpret,

51
07:12:56 --> 07:13:00
summarize, visualize, and post-process.

52
07:13:00 --> 07:13:05
The last step brings us back to the very
first reason we do data science,

53
07:13:05 --> 07:13:06
for a purpose.

54
07:13:08 --> 07:13:13
Reporting insights from analysis and
determining actions from insights based

55
07:13:13 --> 07:13:18
on the purpose you initially defined
is what we refer to as the act step.

56
07:13:20 --> 07:13:25
We have now seen all of the steps
in a typical data science process.

57
07:13:25 --> 07:13:29
Please note that this is an iterative
process and findings from

58
07:13:29 --> 07:13:34
one step may require previous steps
to be repeated, but need information,

59
07:13:34 --> 07:13:38
leading for further exploration and
application of these steps.

60
07:13:39 --> 07:13:44
Scalability of this process to big
data analysis requires the use of

61
07:13:44 --> 07:13:47
big data platforms like Hadoop.

1
14:22:24 --> 14:22:28
The Hadoop ecosystem frameworks and
applications

2
14:22:28 --> 14:22:32
provide such functionality through
several overarching themes and goals.

3
14:22:35 --> 14:22:39
First, they provide scalability
to store large volumes of data

4
14:22:39 --> 14:22:41
on commodity hardware.

5
14:22:42 --> 14:22:45
As the number of systems increase, so

6
14:22:45 --> 14:22:49
does the chance for
crashes and hardware failures.

7
14:22:49 --> 14:22:54
They handle fault tolerance to
gracefully recover from these problems.

8
14:22:56 --> 14:23:01
In addition, they are designed to handle
big data capacity and compressing text

9
14:23:01 --> 14:23:07
files, graphs of social networks,
streaming sensor data and raster images.

10
14:23:07 --> 14:23:09
We can add more data
types to this variety.

11
14:23:10 --> 14:23:12
For any given data type,

12
14:23:13 --> 14:23:17
you can find several projects in
the ecosystem that support it.

13
14:23:19 --> 14:23:22
Finally, they facilitate
a shared environment,

14
14:23:22 --> 14:23:26
allow multiple jobs to
execute simultaneously.

15
14:23:28 --> 14:23:32
Additionally, the Hadoop ecosystem
includes a wide range of open source

16
14:23:32 --> 14:23:38
projects backed by a large and
active community.

17
14:23:38 --> 14:23:42
These projects are free to use and
easy to find support for.

18
14:23:44 --> 14:23:49
Today, there are over 100
Big Data open source projects,

19
14:23:49 --> 14:23:55
and this continues to grow, many rely
on Hadoop, but some are independent.

20
14:23:57 --> 14:24:03
Here is, one way of looking at a subset
of tools in the Hadoop Ecosystem.

21
14:24:04 --> 14:24:09
This layer diagram is organized
vertically based on the interface.

22
14:24:10 --> 14:24:16
Lower level interfaces to storage and
scheduling on the bottom and

23
14:24:16 --> 14:24:20
high level languages and
interactivity at the top.

24
14:24:23 --> 14:24:29
The Hadoop distributed file system,
or HDFS, is the foundation for

25
14:24:29 --> 14:24:35
many big data frameworks since it
provides scalable and reliable storage.

26
14:24:35 --> 14:24:40
As the size of your data increases,
you can add commodity

27
14:24:40 --> 14:24:45
hardware to HDFS to
increase storage capacity.

28
14:24:45 --> 14:24:51
So it enables what we call
scaling out of your resources.

29
14:24:53 --> 14:24:56
Hadoop YARN provide
flexible scheduling and

30
14:24:56 --> 14:25:00
resource management over the HTFS storage.

31
14:25:01 --> 14:25:07
Yarn is use at Yahoo to schedule
jobs across 40,000 servers.

32
14:25:09 --> 14:25:14
MapReduce is a programming model
that simplifies parallel computing.

33
14:25:14 --> 14:25:18
Instead of dealing with the complexities
of synchronization and scheduling you only

34
14:25:18 --> 14:25:24
need to give MapReduce two
functions map and reduce.

35
14:25:24 --> 14:25:26
This programming model is so

36
14:25:26 --> 14:25:31
powerful that Google previously
used it for indexing websites.

37
14:25:31 --> 14:25:36
MapReduce, only assumes
a limited model to express data.

38
14:25:36 --> 14:25:40
Hive, and
Pig are two additional programming models,

39
14:25:40 --> 14:25:44
on top of MapReduce,
to augment data modeling of MapReduce,

40
14:25:44 --> 14:25:48
with relational algebra and
data flow modeling, respectively.

41
14:25:50 --> 14:25:55
Hive was created at Facebook to
issue SQL-like queries using

42
14:25:55 --> 14:25:58
MapReduce on their data in HDFS.

43
14:25:59 --> 14:26:04
Pig was created at Yahoo to model
dataflow based programs using MapReduce.

44
14:26:04 --> 14:26:09
Thanks to YARNs ability to
manage resources, not just for

45
14:26:09 --> 14:26:12
MapReduce but other programming models.

46
14:26:12 --> 14:26:16
Giraph was built for
processing large scale graphs efficiently.

47
14:26:18 --> 14:26:24
For example, Facebook uses Giraph to
analyze the social graphs of its users.

48
14:26:24 --> 14:26:29
Similarly, Storm, Spark and
Flink were built for

49
14:26:29 --> 14:26:33
real time and
In-memory processing of big data.

50
14:26:33 --> 14:26:38
On top of the YARN resource scheduler and
HDFS.

51
14:26:38 --> 14:26:44
In-memory processing is a powerful
way of running big data applications,

52
14:26:44 --> 14:26:50
even faster, achieving 100x better
performance for some tasks.

53
14:26:50 --> 14:26:54
Sometimes your data processing or
tasks are not easily or

54
14:26:54 --> 14:27:00
efficiently represented using the file and
directory model of storage,

55
14:27:00 --> 14:27:06
examples of this include collections
of key values or large sparse tables.

56
14:27:07 --> 14:27:12
NoSQL projects such as
Cassandra MongoDB and

57
14:27:12 --> 14:27:16
HBase handle all these cases.

58
14:27:16 --> 14:27:21
Cassandra was created at Facebook and
Facebook also use HBase for

59
14:27:21 --> 14:27:23
its messaging platform.

60
14:27:25 --> 14:27:31
Finally, running all this tools requires
a centralized management system for

61
14:27:31 --> 14:27:36
synchronization, configuration And
to ensure high availability.

62
14:27:37 --> 14:27:42
Zookeeper, created by Yahoo to
wrangle services named after animals,

63
14:27:42 --> 14:27:44
performs these duties.

64
14:27:47 --> 14:27:51
Just looking at the small number
of Hadoop stack components,

65
14:27:51 --> 14:27:55
we can already see that most of them
are dedicated to data modeling.

66
14:27:55 --> 14:28:00
Management, and
efficient processing of the data.

67
14:28:00 --> 14:28:04
In the rest of this course,
we will give you fundamental knowledge and

68
14:28:04 --> 14:28:10
some practical skills on how to start
modeling and managing your data, and

69
14:28:10 --> 14:28:16
picking the right tools for this activity
from a plethora of big data tools.

1
04:50:41 --> 04:50:44
Welcome to course two of
the big data specialization.

2
04:50:44 --> 04:50:46
I'm Amarnath Gupta.

3
04:50:46 --> 04:50:47
>> And I'm Ilkay Altintas.

4
04:50:47 --> 04:50:50
We are really excited to work
with you in this course,

5
04:50:50 --> 04:50:54
to develop your understanding and skills
in big data modeling and management.

6
04:50:56 --> 04:51:00
You might have just finished our first
course, and see the potential and

7
04:51:00 --> 04:51:02
challenges of big data.

8
04:51:02 --> 04:51:05
If you haven't it's not required but for

9
04:51:05 --> 04:51:09
those with less background in
the area you might find it valuable.

10
04:51:10 --> 04:51:14
>> Let's explain what we mean by
big data modeling and management.

11
04:51:15 --> 04:51:19
Suppose you have an application
where the data is big in a sense and

12
04:51:19 --> 04:51:25
it has a large volume, or has high speed,
or comes with a lot of variations.

13
04:51:25 --> 04:51:29
Even before you think of
how to handle the bigness,

14
04:51:29 --> 04:51:31
you need to have a sense of
what the data looks like.

15
04:51:32 --> 04:51:37
The goal of data modeling is to
formally explore the nature of data, so

16
04:51:37 --> 04:51:40
that you can figure out what
kind of storage you need, and

17
04:51:40 --> 04:51:42
what kind of processing you can do on it.

18
04:51:44 --> 04:51:47
The goal of data management
is to figure out

19
04:51:47 --> 04:51:50
what kind of infrastructure support
you would need for the data.

20
04:51:50 --> 04:51:55
For example, does your environment need
to keep multiple replicas of the data?

21
04:51:55 --> 04:51:58
Do you need to do statistical
computation with the data?

22
04:51:58 --> 04:52:03
Once these operational requirements,
you'll

23
04:52:03 --> 04:52:07
be able to choose the right system that
will let you perform these operations.

24
04:52:08 --> 04:52:13
>> We will also introduce
management of big data as it is

25
04:52:13 --> 04:52:18
streaming from data sources and talk
about storage architectures for big data.

26
04:52:18 --> 04:52:22
For example,
how can high velocity data get ingested,

27
04:52:22 --> 04:52:28
managed, stored in order to enable
real time analytical capabilities.

28
04:52:28 --> 04:52:33
Or what is the difference between
data at rest and data in motion?

29
04:52:33 --> 04:52:36
And how can a data system enable both?

30
04:52:37 --> 04:52:40
>> Once you've understood the basic
concepts of data modeling,

31
04:52:40 --> 04:52:44
data management, and streaming data,
we will introduce you

32
04:52:44 --> 04:52:49
to the characteristics of large volume
data and how to think about that.

33
04:52:49 --> 04:52:53
Thus, we will transition from
classical database management systems,

34
04:52:53 --> 04:52:58
that is DBMSs,
to big data management systems, or BDMSs.

35
04:52:58 --> 04:53:03
We'll present brief overviews of
several big data management systems

36
04:53:03 --> 04:53:04
available in the marketplace today.

37
04:53:05 --> 04:53:11
We are so excited to show you examples
of archived and streaming big data sets.

38
04:53:11 --> 04:53:16
Our goal is to provide you with simple
hands on exercises that require

39
04:53:16 --> 04:53:21
no programming, but show you what
big data looks like, and why various

40
04:53:21 --> 04:53:27
big data management systems are suitable
for specific kinds of big data.

41
04:53:27 --> 04:53:30
In the end you will be
able to design a simple

42
04:53:30 --> 04:53:33
big data information system
using this knowledge.

43
04:53:34 --> 04:53:36
We wish you a fun time learning and

44
04:53:36 --> 04:53:40
hope to hear from you in the discussion
forums and learner stories.

45
04:53:40 --> 04:53:42
>> Happy learning and think big data.

1
09:44:23 --> 09:44:28
This is the second course in the 2016
version of our big data specialization.

2
09:44:28 --> 09:44:32
>> After listening to learners like you,
we have changed some of the content and

3
09:44:32 --> 09:44:34
ordering in the specialization.

4
09:44:35 --> 09:44:39
This course takes you on the first
step of any big data project,

5
09:44:39 --> 09:44:41
its modeling and management.

6
09:44:41 --> 09:44:45
>> We hope that this background
on what big data looks like and

7
09:44:45 --> 09:44:49
how it is modeled and managed in
a large scale system will make you feel

8
09:44:49 --> 09:44:54
more prepared to take steps towards
retrieving and processing big data and

9
09:44:54 --> 09:44:59
perform big data analytics which
are topics coming up in the next courses.

1
19:29:24 --> 19:29:27
The third component of
a data model is constraints.

2
19:29:29 --> 19:29:31
A constraint is a logical statement.

3
19:29:31 --> 19:29:36
That means one can compute and test
whether the statement is true or false.

4
19:29:37 --> 19:29:42
Constraints are part of the data model
because they can specify something about

5
19:29:42 --> 19:29:45
the semantics, that is,
the meaning of the data.

6
19:29:45 --> 19:29:48
For example,
the constraint that a week has seven and

7
19:29:48 --> 19:29:53
only seven days is something that a data
system would not know unless this

8
19:29:53 --> 19:29:57
knowledge is passed on to it
in the form of a constraint.

9
19:29:57 --> 19:30:00
Another constraint, shown here,

10
19:30:03 --> 19:30:07
Tells the system that the number of
titles for a movie is restricted to one.

11
19:30:10 --> 19:30:11
Different data models,

12
19:30:11 --> 19:30:15
as we'll see in the next module,
will have different kinds of constraints.

13
19:30:17 --> 19:30:19
There may be many different
kinds of constraints.

14
19:30:21 --> 19:30:25
A value constraint is a logical
statement about data values.

15
19:30:26 --> 19:30:30
On a previous slide we have said,
that the age, that is,

16
19:30:30 --> 19:30:34
the value of data elements representing
the age of an entity can not be negative.

17
19:30:37 --> 19:30:41
We also saw an example of
uniqueness constraint when we said

18
19:30:41 --> 19:30:44
every movie can have only one title.

19
19:30:46 --> 19:30:48
In the words of logic,

20
19:30:48 --> 19:30:53
there should exist no data object that's
a movie and has more than one title.

21
19:30:54 --> 19:30:58
It's easy to see that enforcing
these constraints requires us

22
19:30:58 --> 19:31:02
to count the number of titles and
then verify that it's one.

23
19:31:02 --> 19:31:07
Now, one can generalize this to count
the number of values associated with

24
19:31:07 --> 19:31:12
each object and check whether it lies
between an upper and lower bound.

25
19:31:13 --> 19:31:17
This is often called a cardinality
constraint of data property.

26
19:31:20 --> 19:31:25
In a medical example here,
the constraint has a lower limit of 0 and

27
19:31:25 --> 19:31:26
an upper limit of 3.

28
19:31:31 --> 19:31:36
A different kind of value constraint can
be enforced by restricting the type of

29
19:31:36 --> 19:31:38
the data allowed in a field.

30
19:31:39 --> 19:31:44
If we do not have such a constraint we
can put any type of data in the field.

31
19:31:44 --> 19:31:49
For example, you can have -99 as
the value of the last name of a person,

32
19:31:49 --> 19:31:51
of course that would be wrong.

33
19:31:51 --> 19:31:55
To ensure that this does not happen,
we can enforce the type of the last name

34
19:31:56 --> 19:32:01
to be a non-numeric alphabetic string.

35
19:32:01 --> 19:32:06
This example shows a logical
expression for this constraint.

36
19:32:06 --> 19:32:11
A type constraint is a special
kind of domain constraint.

37
19:32:11 --> 19:32:13
The domain of a data property or

38
19:32:13 --> 19:32:18
attribute is the possible set of values
that are allowed for that attribute.

39
19:32:18 --> 19:32:22
For example, the possible values for

40
19:32:22 --> 19:32:26
the day part of the date field
can be between 1 and 31.

41
19:32:26 --> 19:32:30
While a month may have
the value between 1 and 12.

42
19:32:30 --> 19:32:37
Or, alternately, a value from the set
January, February, ect till December.

43
19:32:38 --> 19:32:42
Now, one can devise a more complex
constraint, where the value of the date

44
19:32:42 --> 19:32:47
for April, June, and September and
November, are restricted between 1 and 30.

45
19:32:47 --> 19:32:51
And if you think about it,
all three constraints that we have

46
19:32:51 --> 19:32:56
described in the last slide
are value constraints.

47
19:32:56 --> 19:33:00
So they only state how to restrict
the values of some data property.

48
19:33:01 --> 19:33:06
In sharp contrast, structural properties
restrict the structure of the data.

49
19:33:06 --> 19:33:09
We'll choose a more complex example for
this.

50
19:33:11 --> 19:33:14
Suppose we are a matrix,
as shown in the example, and

51
19:33:14 --> 19:33:17
we've restricted to be a square matrix.

52
19:33:17 --> 19:33:21
So the number of columns is exactly
equal to the number of rows.

53
19:33:24 --> 19:33:27
We have not put any restriction
on the number of rows or columns.

54
19:33:27 --> 19:33:28
But just that they have to be the same.

55
19:33:30 --> 19:33:32
Now this constrains
the structure of the matrix and

56
19:33:32 --> 19:33:37
implies that the number of entries in
the structure will be a squared number.

57
19:33:38 --> 19:33:43
If we convert this matrix to a three
column table as shown, and impose

58
19:33:43 --> 19:33:49
the same squareness constraint, it will
translate to a more complex condition.

59
19:33:49 --> 19:33:52
That the number of data
rows will be the square of

60
19:33:52 --> 19:33:55
the number of unique values
in column one of the table.

61
19:33:56 --> 19:34:00
We'll encounter some more structural
constraints in the next module.

1
15:03:24 --> 15:03:28
The second component of a data model is
a set of operations that can be performed

2
15:03:28 --> 15:03:30
on the data.

3
15:03:30 --> 15:03:31
And in this module,

4
15:03:31 --> 15:03:36
we'll discuss the operations without
considering the bigness aspect.

5
15:03:36 --> 15:03:38
In course three,

6
15:03:38 --> 15:03:41
we'll come back to the issue of performing
these operations when the data is large.

7
15:03:43 --> 15:03:46
Now,operations specified
the methods to manipulate the data.

8
15:03:47 --> 15:03:52
SInce different data models are typically
associated with different structures,

9
15:03:52 --> 15:03:55
the operations on them will be different.

10
15:03:55 --> 15:04:00
But some types of operations are usually
performed across all data models.

11
15:04:00 --> 15:04:02
We'll describe a few of them here.

12
15:04:04 --> 15:04:11
One common operation extract a part of
a collection based on the condition.

13
15:04:11 --> 15:04:15
In the example here,
we have a set of records and

14
15:04:15 --> 15:04:20
we're looking for a sub set that
satisfies the condition that

15
15:04:20 --> 15:04:24
the fifth field has a value
greater than 100,000.

16
15:04:24 --> 15:04:28
The only one record
satisfies this requirement.

17
15:04:28 --> 15:04:32
Note that we called this operation
subsetting rather loosely.

18
15:04:33 --> 15:04:38
Depending on the context,
it's also called selection or filtering.

19
15:04:41 --> 15:04:47
The next common operation is retrieving
a part of a structure that is specified.

20
15:04:47 --> 15:04:50
In this case,
we specify that we are interested

21
15:04:50 --> 15:04:54
in just the first two fields
of a collection off records.

22
15:04:55 --> 15:05:01
But this produces a new collection of
records which has only these fields.

23
15:05:02 --> 15:05:05
This operation like before has many names.

24
15:05:05 --> 15:05:07
The most dominant name is projection.

25
15:05:09 --> 15:05:13
In the next module, we'll see several
versions of this operation for

26
15:05:13 --> 15:05:14
different data models.

27
15:05:17 --> 15:05:21
The next two operations are about
combining two collections

28
15:05:21 --> 15:05:22
into a larger one.

29
15:05:23 --> 15:05:27
The term combine may be
interpreted in various ways.

30
15:05:27 --> 15:05:29
The most straightforward
of them is said union.

31
15:05:30 --> 15:05:33
The assumption behind the union operation

32
15:05:33 --> 15:05:36
is that the two collections
involved have the same structure.

33
15:05:36 --> 15:05:43
In other words, if one collection has
four fields and another has 14 fields, or

34
15:05:43 --> 15:05:48
if one has four fields on people and
the dates of birth, and the other has four

35
15:05:48 --> 15:05:52
things about countries and their capitols,
they cannot be combined through union.

36
15:05:54 --> 15:05:58
In the example here,
their two collections have three and

37
15:05:58 --> 15:06:03
two records, respectively, with one
record that's common between them.

38
15:06:03 --> 15:06:04
The green one.

39
15:06:06 --> 15:06:09
The result collection has four record,

40
15:06:09 --> 15:06:14
because duplicates are disallowed
because it's a set operation.

41
15:06:14 --> 15:06:19
There is indeed another version of
union where duplicates are allowed and

42
15:06:19 --> 15:06:21
will produce five records instead of four.

43
15:06:24 --> 15:06:29
The second kind of combining,
called a Join, can be done when

44
15:06:29 --> 15:06:33
the two collections have different data
content but have some common elements.

45
15:06:35 --> 15:06:37
In the example shown,

46
15:06:37 --> 15:06:40
the first field is the common element
between the two collections on the left.

47
15:06:42 --> 15:06:45
In this kind of data combination
there are two stages.

48
15:06:45 --> 15:06:51
First, for each data item think
of a record of collection one,

49
15:06:51 --> 15:06:54
one finds a set of matching
data items in collection two.

50
15:06:56 --> 15:06:59
Thus, the first records of
the two collections match

51
15:06:59 --> 15:07:01
based on the first field.

52
15:07:02 --> 15:07:04
In the second phase of the operation,

53
15:07:04 --> 15:07:07
all fields of the matching
record pairs are put together.

54
15:07:08 --> 15:07:12
In the first record of the result
collection shown on the right,

55
15:07:12 --> 15:07:15
one gets the first four fields
on the first collection and

56
15:07:15 --> 15:07:18
the remaining two fields
from the second collection.

57
15:07:19 --> 15:07:25
Now in this one example, we found one pair
of matching records from the collections.

58
15:07:25 --> 15:07:28
In general, one would find more
than one matching record pairs.

59
15:07:30 --> 15:07:33
As you can see,
this operation is more complex and

60
15:07:33 --> 15:07:37
can be very expensive when the size
of the true collections are large.

1
06:10:58 --> 06:11:04
In the big data world, we often hear the
term structured data, that is, data having

2
06:11:04 --> 06:11:10
a structure which is quite different
from the so-called unstructured data.

3
06:11:10 --> 06:11:11
But what is a structure?

4
06:11:12 --> 06:11:14
Let's consider file 1.

5
06:11:15 --> 06:11:19
It's a typical CSV file that has
three lines with different content.

6
06:11:19 --> 06:11:24
But the file content is uniform
in the sense that each line,

7
06:11:24 --> 06:11:28
call it a record,
has exactly three fields,

8
06:11:28 --> 06:11:31
which we sometimes call data properties or
attributes.

9
06:11:32 --> 06:11:39
Further, the first two of these fields
are strings and the third one is a date.

10
06:11:39 --> 06:11:42
We can add more records, that's
lines with the same pattern of data,

11
06:11:42 --> 06:11:44
to the file in the same fashion.

12
06:11:45 --> 06:11:50
The content will grow, but the pattern of
data organization will remain identical.

13
06:11:51 --> 06:11:56
This repeatable pattern of data
organization makes the file structured.

14
06:11:56 --> 06:12:00
Now let's look at file 2,
which is four records of five fields each,

15
06:12:01 --> 06:12:05
except that the third record seems
to be missing the last entry.

16
06:12:06 --> 06:12:07
Is this file structured?

17
06:12:08 --> 06:12:13
We argue that it is, because the missing
value makes the third record incomplete,

18
06:12:13 --> 06:12:18
but it does not break the structure or
the pattern of the data organization.

19
06:12:18 --> 06:12:21
Let's looks at these
two files side by side.

20
06:12:21 --> 06:12:27
Clearly file 2 has more fields, and hence
is sort of wider than the first file.

21
06:12:27 --> 06:12:29
Would you say that they
have the same structure?

22
06:12:29 --> 06:12:31
Well, on the face of it they don't.

23
06:12:31 --> 06:12:33
But if you think more broadly,

24
06:12:33 --> 06:12:37
you would notice that they
are both collection of k fields.

25
06:12:38 --> 06:12:42
The size of the collection,
respectively three and four, differs.

26
06:12:42 --> 06:12:46
And k is 3 in the first case and
5 in the second.

27
06:12:46 --> 06:12:50
But we can think of 3 and 5 as parameters.

28
06:12:50 --> 06:12:55
In that case, we will say that these
files have been generated by a similar

29
06:12:55 --> 06:13:00
organizational structure, and
hence they have the same data model.

30
06:13:01 --> 06:13:04
Now in contrast, consider this file.

31
06:13:04 --> 06:13:08
Just looking at it, it's impossible to
figure out how the data is organized and

32
06:13:08 --> 06:13:10
how to identify subparts of the data.

33
06:13:11 --> 06:13:13
We would call this data unstructured.

34
06:13:14 --> 06:13:19
Often, compressed data like JPEG images,
MP3 audio files,

35
06:13:19 --> 06:13:24
MPEG3 video files, encrypted data,
are usually unstructured.

36
06:13:25 --> 06:13:30
In module two, we'll elaborate on data
models that are not fully structured or

37
06:13:30 --> 06:13:31
are structured differently.

1
12:24:29 --> 12:24:32
This is the first of two
hands-on exercises involving

2
12:24:32 --> 12:24:35
sensor data from a weather station.

3
12:24:35 --> 12:24:38
In this one, we will look at
static data in a text file.

4
12:24:38 --> 12:24:41
The next one we will look at live data
streaming from the weather station

5
12:24:41 --> 12:24:42
in real time.

6
12:24:42 --> 12:24:46
In this exercise, we will begin
by opening a terminal window and

7
12:24:46 --> 12:24:50
changing into the directory
containing the station measurements.

8
12:24:50 --> 12:24:53
We will look at these measurements in
a text file and then look at the key for

9
12:24:53 --> 12:24:56
these measurements so
we can understand what the values mean.

10
12:24:56 --> 12:24:58
Finally, we will plot the measurements.

11
12:24:59 --> 12:24:59
Let's begin.

12
12:25:00 --> 12:25:04
First, we will open a terminal window by
clicking on the Terminal icon on the top

13
12:25:04 --> 12:25:04
of the toolbar.

14
12:25:07 --> 12:25:13
Next, we'll cd into the directory

15
12:25:13 --> 12:25:19
containing the sensor data.

16
12:25:20 --> 12:25:27
We'll run cd
Downloads/big-data-two/sensor.

17
12:25:29 --> 12:25:36
We can write ls to see
the contents of this directory.

18
12:25:41 --> 12:25:47
The data from the weather station is
in a text file called wx-data.txt.

19
12:25:50 --> 12:25:55
We can run more wx-data.txt to
see the contents of this file.

20
12:26:03 --> 12:26:06
Each line of this file is
a separate set of measurements.

21
12:26:06 --> 12:26:11
There are two columns in this file,
the first column is a time stamp and

22
12:26:11 --> 12:26:14
it's separated by
a second column by a tab.

23
12:26:14 --> 12:26:20
The second column itself has separate
columns and these are separated by commas.

24
12:26:20 --> 12:26:25
The time stamp is the number
of seconds since 1970.

25
12:26:25 --> 12:26:28
You'll notice that it increases for
each time stamp.

26
12:26:30 --> 12:26:33
You'll notice that it increases for
each measurement.

27
12:26:33 --> 12:26:35
But sometimes measurements
come in at the same time.

28
12:26:35 --> 12:26:39
For example this one at 006.

29
12:26:39 --> 12:26:44
The measurements,
we see that the prefix is 0R1 for

30
12:26:44 --> 12:26:47
most of them but some have 0R2.

31
12:26:47 --> 12:26:50
If we look at the other measurements,

32
12:26:50 --> 12:26:55
we see that all the 0R1 measurements
start with Dn, Dm, Dx, and so on.

33
12:26:55 --> 12:27:00
Whereas R2 begins with Ta, Ua, and Pa.

34
12:27:03 --> 12:27:06
If we scroll down in the text
file by hitting the space bar,

35
12:27:06 --> 12:27:09
we'll see there are other
measurements besides R1 and R2.

36
12:27:12 --> 12:27:18
For example, there's R5 that has Th,
Vh, Vs and so on.

37
12:27:18 --> 12:27:22
And there's R0 which has
all the measurements.

38
12:27:22 --> 12:27:26
So Dn, Dm, Dx, Ta, Ua, Pa.

39
12:27:29 --> 12:27:30
And the remaining ones.

40
12:27:33 --> 12:27:36
Next we'll open another internal
window and look at the key to this

41
12:27:36 --> 12:27:39
measurements click on the tool
bar to open the terminal window.

42
12:27:39 --> 12:27:44
cd into downloads big data two sensor

43
12:27:52 --> 12:28:00
And the key to these measurements is
in a file called wxt520format.txt.

44
12:28:00 --> 12:28:05
We can run more wxt520format.txt
to see this file.

45
12:28:13 --> 12:28:16
This file says where each
of the prefix is mean, for

46
12:28:16 --> 12:28:18
example Sn is the wind speed minimum.

47
12:28:18 --> 12:28:25
Sm is the wind speed average.

48
12:28:25 --> 12:28:29
And Ta is the air temperature.

49
12:28:29 --> 12:28:33
So if we go back to our sensor file,
we see here Ta equals 13.9c.

50
12:28:33 --> 12:28:39
That means the air temperature at this

51
12:28:39 --> 12:28:44
time was 13.9 degrees celsius.

52
12:28:44 --> 12:28:50
We can also create a plot of
the data in this text file

53
12:28:50 --> 12:28:54
by running plot-data.py wx-data.txt Ta.

54
12:29:01 --> 12:29:06
This says to plot the data in
the wx-data file and the measure that we

55
12:29:06 --> 12:29:11
want to apply is Ta, which according
to our key, is the air temperature.

56
12:29:15 --> 12:29:20
When we run it, it displays a plot of the
air temperature found in the text file.

1
00:53:49 --> 00:53:51
In this hands-on exercise,

2
00:53:51 --> 00:53:54
we will be looking at an image file,
which uses an array data model.

3
00:53:56 --> 00:54:00
First, we will open a terminal window and
display an image file on the screen.

4
00:54:01 --> 00:54:05
Next, we will examine the structure
of the image, and finally,

5
00:54:05 --> 00:54:08
extract pixel values from
various locations in the image.

6
00:54:10 --> 00:54:11
Let's begin.

7
00:54:11 --> 00:54:14
First, we'll open a terminal window by
clicking on the terminal icon at the top

8
00:54:14 --> 00:54:15
of the toolbar.

9
00:54:22 --> 00:54:26
Next, we'll CDN2 the directory
containing the image,

10
00:54:26 --> 00:54:30
cdn2downloads/bigdata2/image.

11
00:54:37 --> 00:54:40
We can run ls to see the image
in different scripts.

12
00:54:43 --> 00:54:47
The file, Australia.jpg,
is an image that we want to view,

13
00:54:47 --> 00:54:49
we can use eog to view it.

14
00:54:49 --> 00:54:57
Run eog Australia.jpg Eog is
a common image viewer on Linux.

15
00:55:00 --> 00:55:04
Australia.jpg is a satellite image
of the Australian continent.

16
00:55:04 --> 00:55:06
Now let's look at the structure
of this image file.

17
00:55:10 --> 00:55:14
If we go back to our terminal window,
we can run the image

18
00:55:14 --> 00:55:19
viewer in the background by
hitting CTRL+Z, and then bg.

19
00:55:19 --> 00:55:24
We can view the dimensions or
structure of the array data model of this

20
00:55:24 --> 00:55:27
image by running
dimensions.py Australia.jpg.

21
00:55:32 --> 00:55:39
This says that the image has
5250 columns and 4320 rows.

22
00:55:39 --> 00:55:42
So it is a two-dimensional image.

23
00:55:42 --> 00:55:44
Additionally, each cell or

24
00:55:44 --> 00:55:50
pixel within this image is
composed of three 8-bit pixels.

25
00:55:50 --> 00:55:55
These pixels are composed of three
elements, red, green, and blue.

26
00:55:55 --> 00:55:58
We can extract or
view the individual pixel elements

27
00:55:58 --> 00:56:02
at a specific location in the image
by using the pixel.py script.

28
00:56:03 --> 00:56:10
We can run pixel.py Australia.jpg 0
0 to see the value at one location.

29
00:56:18 --> 00:56:22
The 0 0 location is
the corner of the image.

30
00:56:22 --> 00:56:28
If we go back to the image, the corners
are all the ocean, so they're dark blue.

31
00:56:28 --> 00:56:32
If we look at the value that was
extracted, we see that blue has a high

32
00:56:32 --> 00:56:37
value of 50, whereas red and
green are low with 11 and 10.

33
00:56:37 --> 00:56:40
If we view it at another corner,
by looking at 5000 0,

34
00:56:40 --> 00:56:42
we'll see the same value.

35
00:56:50 --> 00:56:54
If we go back to the image, the middle
of the image, which is the land,

36
00:56:54 --> 00:56:56
is orange or yellow.

37
00:56:56 --> 00:57:01
It's definitely not blue, so
let's take a look at a pixel value there.

38
00:57:01 --> 00:57:06
Okay, run pixel.py
Australia.jpg 2000 2000.

39
00:57:12 --> 00:57:20
This says that the red has a value of 118,
green is 89, and the blue is 57.

40
00:57:20 --> 00:57:23
So the red and green are higher than blue,
so it's not ocean.

1
01:51:14 --> 01:51:17
This is the second hands on exercises for
CSV data.

2
01:51:17 --> 01:51:22
In the first, we saw how to import
a CSV file into a spreadsheet and

3
01:51:22 --> 01:51:24
make a simple plot.

4
01:51:24 --> 01:51:27
In this one,
we will learn how to filter data and

5
01:51:27 --> 01:51:28
perform some aggregate operations.

6
01:51:30 --> 01:51:33
We will begin by opening a terminal
window and starting a spreadsheet.

7
01:51:34 --> 01:51:38
Next, we will load the CSV
data into the spreadsheet and

8
01:51:38 --> 01:51:41
perform a filter over several columns.

9
01:51:41 --> 01:51:44
Finally, we will calculate an average and
sum from the data.

10
01:51:44 --> 01:51:47
Let's begin.

11
01:51:47 --> 01:51:51
First, we open a terminal window by
clicking on the terminal icon in

12
01:51:51 --> 01:51:52
the top toolbar.

13
01:51:54 --> 01:51:58
We can start the spreadsheet
by writing oocalc.

14
01:52:03 --> 01:52:06
Next, let's load our CSV
data into the spreadsheet.

15
01:52:06 --> 01:52:14
We click on File > Open, CENSUS.CSV.

16
01:52:17 --> 01:52:20
And click OK on this
dialog to load the data.

17
01:52:24 --> 01:52:29
Column F in the spreadsheet is the state,
and

18
01:52:29 --> 01:52:34
column H is the census population for
2010.

19
01:52:37 --> 01:52:40
Let's create a filter that
just shows the data for

20
01:52:40 --> 01:52:44
California, for
counties larger than one million people.

21
01:52:46 --> 01:52:51
We can create this filter by first
selecting both the state name column and

22
01:52:51 --> 01:52:53
the census 2000 population column.

23
01:52:54 --> 01:53:00
Next we go to data,
filter, standard filter.

24
01:53:04 --> 01:53:08
Here we change the field
name to be the state name,

25
01:53:08 --> 01:53:13
the condition we leave at equals and
the value we use California.

26
01:53:18 --> 01:53:23
This filters all the rows,
unless the state is California.

27
01:53:27 --> 01:53:28
We then want to filter for

28
01:53:28 --> 01:53:32
all the counties whose population
is greater than one million people.

29
01:53:32 --> 01:53:36
To do that, in this second line here,
we change the operator to AND.

30
01:53:38 --> 01:53:43
The field name should be
census 2010 population and

31
01:53:43 --> 01:53:47
the condition should be greater than.

32
01:53:47 --> 01:53:48
Then set the value to be one million.

33
01:53:53 --> 01:53:56
And click OK.

34
01:53:56 --> 01:54:01
We can see that all the data from the
spreadsheet has disappeared except where

35
01:54:01 --> 01:54:03
the state name is California and

36
01:54:03 --> 01:54:06
the population is greater
than one million people.

37
01:54:08 --> 01:54:12
We can reset or remove this
filter to see all the data again

38
01:54:12 --> 01:54:16
by going to Data > Filter.

39
01:54:18 --> 01:54:19
Reset filter.

40
01:54:20 --> 01:54:23
You can perform aggregate operations
on the data in a spreadsheet.

41
01:54:25 --> 01:54:28
Next, let's perform some aggregate
operations over the data.

42
01:54:28 --> 01:54:31
We can compute the average and
the sum of the data.

43
01:54:32 --> 01:54:35
To do this, let's run these
calculations in a separate sheet.

44
01:54:37 --> 01:54:40
Create a new sheet by clicking
on the green plus button.

45
01:54:43 --> 01:54:49
To compute the average we select
a cell and enter =Average( and

46
01:54:49 --> 01:54:55
then we select the data that we
want to compute the average from.

47
01:54:56 --> 01:54:58
If we go back to sheet one.

48
01:54:58 --> 01:55:02
We can select some of
the data from the H column.

49
01:55:02 --> 01:55:06
So let's just choose several
counties in Alabama.

50
01:55:06 --> 01:55:10
When we hit Enter,
it takes us back to sheet one and

51
01:55:10 --> 01:55:13
we can see that the average is computing.

52
01:55:15 --> 01:55:24
Similarly, we can compute the sum
by entering =sum, open parentheses.

53
01:55:24 --> 01:55:28
Going back to sheet one and
selecting the columns we want to sum.

54
01:55:28 --> 01:55:33
When we're done hit Enter and
the sum is computed.

1
03:46:47 --> 03:46:50
In this hands on exercise,
we will be looking at JSON data.

2
03:46:52 --> 03:46:54
First, we will open a terminal window and

3
03:46:54 --> 03:46:58
then look at the contents of a JSON
file containing tweets from Twitter.

4
03:46:59 --> 03:47:02
Next, we will examine
the schema of this JSON file.

5
03:47:03 --> 03:47:06
Finally, we will extract different
fields from the JSON data.

6
03:47:09 --> 03:47:10
Let's begin.

7
03:47:10 --> 03:47:12
First, we'll open a terminal window,

8
03:47:12 --> 03:47:14
by clicking on the terminal
icon at the top of the toolbar.

9
03:47:22 --> 03:47:27
Next, we'll see cd into the directory
containing the json data,

10
03:47:27 --> 03:47:30
by running cddownload/big/data/2/json.

11
03:47:37 --> 03:47:39
We can run ls, to see the json file.

12
03:47:43 --> 03:47:49
The json file is called twitter.json.

13
03:47:49 --> 03:47:52
We can run more twitter.json to
view the contents of this file.

14
03:48:01 --> 03:48:05
The json data contains semi-structured
data, which is nested several levels.

15
03:48:05 --> 03:48:10
There are many tweets in this file, and
it's hard to read using the more command.

16
03:48:10 --> 03:48:14
You can use space to scroll down and
when we're done, hit q.

17
03:48:17 --> 03:48:22
We can run the jsonschema.pi command
to view the schema of this data.

18
03:48:22 --> 03:48:27
We run jsonschema.pitwitter.json

19
03:48:34 --> 03:48:37
And we'll add a pipe more at the end.

20
03:48:41 --> 03:48:45
This shows the nested
fields within this data,

21
03:48:45 --> 03:48:51
at the top level there are fields
like contributors text and id and so

22
03:48:51 --> 03:48:56
on, but there are also fields nested
within these top level fields for

23
03:48:56 --> 03:49:02
example entities also contains
called symbols media hashtags and

24
03:49:02 --> 03:49:05
so on If we scroll down by hitting space,

25
03:49:05 --> 03:49:10
we'll see that there's
several levels of nesting.

26
03:49:10 --> 03:49:17
For example, user also has
follow_request_sent, id, and so on.

27
03:49:21 --> 03:49:25
We can run the print json script to view
the contents of a particular tweet and

28
03:49:25 --> 03:49:27
a particular field within that tweet.

29
03:49:27 --> 03:49:32
Let's run print_json.py.

30
03:49:32 --> 03:49:35
It asks for the file name so

31
03:49:35 --> 03:49:40
we'll enter twitter.json.

32
03:49:43 --> 03:49:46
And we'll look at tweet 99.

33
03:49:48 --> 03:49:51
So let's look at the top
level field called text.

34
03:49:55 --> 03:49:59
So here we see the text for
note the 99th tweet in this file.

35
03:49:59 --> 03:50:04
We could also look at a nested field
within the file, by running print_json

36
03:50:04 --> 03:50:09
again The file name is twitter.json.

37
03:50:11 --> 03:50:13
We'll look at tweet 99 again.

38
03:50:17 --> 03:50:20
And we'll look at the field
entities hashtags.

39
03:50:20 --> 03:50:24
The hashtags that are embedded or
nested within the entities field.

1
07:37:11 --> 07:37:17
This is the first of two hands
on exercises involving CSV Data.

2
07:37:17 --> 07:37:21
In this exercise, we will import
a CSV file into a spreadsheet and

3
07:37:21 --> 07:37:22
make a simple plot.

4
07:37:24 --> 07:37:29
We will begin by opening a terminal window
and looking at a CSV file in the terminal.

5
07:37:29 --> 07:37:32
Next, we will start
the spreadsheet application and

6
07:37:32 --> 07:37:34
import the CSV data into the spreadsheet.

7
07:37:34 --> 07:37:39
We can then look at the rows and columns
of the CSV file and make a simple plot.

8
07:37:42 --> 07:37:44
Let's begin, first,
open a terminal shell by

9
07:37:44 --> 07:37:48
clicking on the black terminal
icon at the top of the toolbar.

10
07:37:56 --> 07:38:01
Next, let's cd into the directory
containing the CSV data.

11
07:38:01 --> 07:38:07
We'll run cd space
Download/big-data-2/csv.

12
07:38:07 --> 07:38:13
We can run ls to

13
07:38:13 --> 07:38:20
see the CSV files.

14
07:38:20 --> 07:38:25
The file census.csv contains
census data for the United States.

15
07:38:25 --> 07:38:30
We can run the command more census.csv
to see the contents of this file.

16
07:38:38 --> 07:38:41
The first line of this file is the header
with the columns separated by commas.

17
07:38:41 --> 07:38:46
You can go down in
the file by hitting space.

18
07:38:50 --> 07:38:51
Hit Q to quit more.

19
07:38:54 --> 07:38:57
Next, let's start
a spreadsheet application.

20
07:38:57 --> 07:39:01
We run oocalc to start this.

21
07:39:09 --> 07:39:13
We can import the census
data CSV file into

22
07:39:13 --> 07:39:17
the spreadsheet by going to File > Open.

23
07:39:21 --> 07:39:25
Clicking on downloads.

24
07:39:25 --> 07:39:31
Big Data 2, CSV, census.csv.

25
07:39:36 --> 07:39:38
In this dialog click OK.

26
07:39:53 --> 07:39:56
You can see into this spreadsheet,

27
07:39:56 --> 07:39:59
import of the CSV data to
a bunch of rows and columns.

28
07:40:01 --> 07:40:05
Each column that was separated
by a comma in the CSV file,

29
07:40:05 --> 07:40:07
is a column in the spreadsheet.

30
07:40:08 --> 07:40:11
We can see that our CSV file
was successfully imported into

31
07:40:11 --> 07:40:12
the spreadsheet.

32
07:40:14 --> 07:40:16
If we scroll down to
the bottom of the spreadsheet,

33
07:40:16 --> 07:40:19
we can see how many rows
there were in the CSV file.

34
07:40:28 --> 07:40:34
There are 3194 rows in the CSV file.

35
07:40:34 --> 07:40:38
If this file instead had millions or
10 millions of rows,

36
07:40:38 --> 07:40:42
then we would have to use a big
data system such as Hadoop or HDFS.

37
07:40:45 --> 07:40:47
Let's scroll back to the top.

38
07:40:52 --> 07:40:58
Next, let's make a simple plot of
some of the data in the CSV file.

39
07:40:58 --> 07:41:03
Let's plot the population estimates for
several years for the state of Alabama.

40
07:41:03 --> 07:41:09
The state of Alabama is
given in the second row and

41
07:41:09 --> 07:41:15
the population estimates
are given in these columns.

42
07:41:15 --> 07:41:21
Let's select J through O,
so you get the population

43
07:41:21 --> 07:41:25
estimate for 2010 through 2015.

44
07:41:29 --> 07:41:33
We can create a plot of these values
by clicking on the Chart button.

45
07:41:38 --> 07:41:41
And clicking finish.

46
07:41:41 --> 07:41:45
In the second hands on for CSV data,
we'll perform some filtering and

47
07:41:45 --> 07:41:47
some aggregate operations over the data.

1
15:18:58 --> 15:18:58
Welcome.

2
15:18:59 --> 15:19:02
In this module,
we'll talk about data models.

3
15:19:03 --> 15:19:06
If you completed the introductory
course of this specialization,

4
15:19:06 --> 15:19:08
you might recall our
video on data variety.

5
15:19:10 --> 15:19:12
One way to characterize data variety

6
15:19:12 --> 15:19:17
is to identify the different models of
data that are used in any application.

7
15:19:19 --> 15:19:20
So what is a data model?

8
15:19:20 --> 15:19:23
And why do we care about data
models in the context of big data?

9
15:19:23 --> 15:19:28
In this lesson, we'll introduce you to
three components of a data model and

10
15:19:28 --> 15:19:29
what they tell us about the data.

11
15:19:30 --> 15:19:35
So after this lesson, you'll be able
to distinguish between structured and

12
15:19:35 --> 15:19:37
unstructured data.

13
15:19:37 --> 15:19:42
Describe four basic data operations namely
selection, projection, union, and join.

14
15:19:42 --> 15:19:47
And enumerate different types of data
constraints like type, value and

15
15:19:47 --> 15:19:49
structural constraints.

16
15:19:49 --> 15:19:53
You'll also be able to explain why
constraints are useful to specify

17
15:19:53 --> 15:19:56
the semantics of data.

18
15:19:56 --> 15:20:00
Now, regardless of whether the data is big
or small, one needs to know or determine

19
15:20:00 --> 15:20:06
the characteristics of data before one can
manipulate or analyze them meaningfully.

20
15:20:06 --> 15:20:10
Let's use a simple example,
suppose you have data is

21
15:20:10 --> 15:20:15
a file of records with fields called first
name, last name and date of birth of

22
15:20:15 --> 15:20:20
the employees in the company that this
file consists of records with fields,

23
15:20:20 --> 15:20:25
and not for instance plain text,
gives us more insight

24
15:20:25 --> 15:20:30
into the organization of the data in the
file and hence is part of the data model.

25
15:20:30 --> 15:20:33
This aspect is called Structure.

26
15:20:33 --> 15:20:37
Similarly, the consideration
that we can perform

27
15:20:37 --> 15:20:40
data arithmetic with
the date of birth field, and

28
15:20:40 --> 15:20:46
not with the first name field, is also
part of our understanding of data model.

29
15:20:46 --> 15:20:48
These are called Operations.

30
15:20:48 --> 15:20:53
Finally, we may know that in
this company no one's age

31
15:20:53 --> 15:20:56
that is today's date minus the date
of birth, can't be less than 18.

32
15:20:56 --> 15:21:02
So it gives us a way to detect records
with blatantly erroneous dates of birth.

33
15:21:02 --> 15:21:03
In the following three videos,

34
15:21:03 --> 15:21:07
we'll look at these three aspects
of data models more carefully.

1
06:40:04 --> 06:40:08
We mentioned before that a data model
is characterized by the structure of

2
06:40:08 --> 06:40:12
the data that it admits,
the operations on that structure, and

3
06:40:12 --> 06:40:14
a way to specify constraints.

4
06:40:15 --> 06:40:16
In this lesson,

5
06:40:16 --> 06:40:21
we'll present a more detailed description
of a number of common data models.

6
06:40:21 --> 06:40:23
We'll start with relational data.

7
06:40:25 --> 06:40:29
It is one of the simplest and most
frequently used data models today, and

8
06:40:29 --> 06:40:33
forms the basis of many other
traditional database management systems,

9
06:40:33 --> 06:40:36
like MySQL, Oracle,
Teradata, and so forth.

10
06:40:38 --> 06:40:41
So after this video you'll be able to

11
06:40:41 --> 06:40:44
describe the structural components
of a relational data model.

12
06:40:46 --> 06:40:49
Demonstrate which components
become a data model's schema.

13
06:40:51 --> 06:40:54
Explain the purpose of primary and
foreign keys.

14
06:40:55 --> 06:40:58
And describe Join and other operations.

15
06:41:01 --> 06:41:02
The primary data structure for

16
06:41:02 --> 06:41:06
a relational model is a table, like
the one shown here for a toy application.

17
06:41:08 --> 06:41:12
But we need to be careful about relational
tables, which are also called relations.

18
06:41:14 --> 06:41:18
This table actually
represents a set of tuples.

19
06:41:20 --> 06:41:25
This is a relational tuple,
represented as a row in the table.

20
06:41:26 --> 06:41:29
We were informally calling
this a record before.

21
06:41:30 --> 06:41:36
But a relational tuple implies that unless
otherwise stated, the elements of it

22
06:41:36 --> 06:41:40
like 203 or 204, Mary and
so forth, are atomic.

23
06:41:41 --> 06:41:46
That is,
they represent one unit of information and

24
06:41:46 --> 06:41:47
cannot be decomposed further.

25
06:41:48 --> 06:41:51
We'll return to this issue
in the next few slides.

26
06:41:52 --> 06:41:56
Thus, this is a relation of six tuples.

27
06:41:58 --> 06:42:01
Remember, the definition of sets,

28
06:42:01 --> 06:42:06
it's a collection of distinct
elements of the same type.

29
06:42:07 --> 06:42:13
That means I cannot add
this tuple to the solution.

30
06:42:13 --> 06:42:17
Because if I do,
it will be introducing a duplicate.

31
06:42:18 --> 06:42:24
Now in practice, many systems will allow
duplicate tuples in a relation, but

32
06:42:24 --> 06:42:29
mechanisms are provided to prevent
duplicate entries if the user so chooses.

33
06:42:29 --> 06:42:31
So I cannot add it.

34
06:42:31 --> 06:42:34
Here is another tuple I cannot add.

35
06:42:34 --> 06:42:36
It has all the right
pieces of information, but

36
06:42:36 --> 06:42:38
it's all in the wrong order.

37
06:42:38 --> 06:42:42
So it is a tuple dissimilar with
the other six tuples in the relation.

38
06:42:43 --> 06:42:46
Okay, so how does the system know
that this tuple is different?

39
06:42:48 --> 06:42:52
This brings our attention to the very
first row that is the header of this table

40
06:42:52 --> 06:42:53
painted in black.

41
06:42:55 --> 06:42:58
This row is part of
the scheme of the table.

42
06:42:58 --> 06:42:59
Lets look at it.

43
06:43:00 --> 06:43:04
It tells us the name of the table,
in this case employee.

44
06:43:05 --> 06:43:09
This also tells us the names of the six
columns called attributes of the relation.

45
06:43:10 --> 06:43:12
And for each column,

46
06:43:12 --> 06:43:16
it tells us the allowed data type, that
is the type constraint for each column.

47
06:43:17 --> 06:43:19
Given this schema,

48
06:43:19 --> 06:43:23
it should now be clear why the last
red row does not belong to this table.

49
06:43:24 --> 06:43:29
The schema in a relational table
can also specify constraints,

50
06:43:30 --> 06:43:33
shown in yellow in the third
line of the schema row.

51
06:43:34 --> 06:43:39
It says that the minimum salary of
a person has to be greater than 25k.

52
06:43:41 --> 06:43:47
Further, it states that every employee
must have a first and last name.

53
06:43:47 --> 06:43:49
They cannot be left null,
that means without a value.

54
06:43:51 --> 06:43:54
Why doesn't department or
title column have this constraint?

55
06:43:56 --> 06:44:00
One answer can be that a newly
hired employee may not be assigned

56
06:44:00 --> 06:44:04
a department or a title yet but
can still be an entry in the table.

57
06:44:05 --> 06:44:09
However, the department column
has another constraint.

58
06:44:10 --> 06:44:14
It restricts the possible values
that is the domain of the attribute

59
06:44:14 --> 06:44:17
to only four possibilities.

60
06:44:17 --> 06:44:20
HR, IT, research, and business.

61
06:44:22 --> 06:44:27
Finally, the first says
that ID is a primary key.

62
06:44:29 --> 06:44:32
This means it is unique for each employee.

63
06:44:32 --> 06:44:36
And for
every employee knowing the primary key for

64
06:44:36 --> 06:44:41
the employee will also uniquely know the
other five attributes of that employee.

65
06:44:43 --> 06:44:46
You should now see that
a table with a primary key

66
06:44:46 --> 06:44:51
logically implies that the table cannot
have a duplicate record because if we do,

67
06:44:51 --> 06:44:54
it will violate the uniqueness constraint
associated with the primary key.

68
06:44:56 --> 06:45:00
Let us introduce a new table containing
the salary history of employees.

69
06:45:01 --> 06:45:05
The employees are identified
with the column EmpID, but

70
06:45:05 --> 06:45:07
these are not new values that
this table happens to have.

71
06:45:08 --> 06:45:13
They are the same IDs that are present in
the ID column of the employee's table,

72
06:45:13 --> 06:45:14
presented earlier.

73
06:45:16 --> 06:45:18
This is reflected in
the statement made on the right.

74
06:45:19 --> 06:45:26
The term References means,
the values in this column can exist

75
06:45:26 --> 06:45:31
only if the same values if you are in
employees the table being referenced,

76
06:45:31 --> 06:45:33
also called the parent table.

77
06:45:35 --> 06:45:41
So in the terminology of the relational
model, the EmpID column of EmpSalaries

78
06:45:41 --> 06:45:47
table is called a foreign key that refers
to the primary key of the Employees table.

79
06:45:49 --> 06:45:55
Note that EmpID is not a primary
key in this EmpSalaries table.

80
06:45:55 --> 06:45:59
Because it is multiple to
post with the same EmpID

81
06:45:59 --> 06:46:01
reflecting the salary of
the employee at different times.

82
06:46:03 --> 06:46:07
You will remember join is a common
operation that we discussed before.

83
06:46:08 --> 06:46:12
So here is an example of
a relational join performed

84
06:46:12 --> 06:46:16
on the first three columns of employee and
EmpSalaries table.

85
06:46:16 --> 06:46:23
Where employees.ID, and EmpSalaries.EmpID
columns are matched for equality.

86
06:46:24 --> 06:46:27
The output table shows
all the columns involved.

87
06:46:28 --> 06:46:30
The common column is represented once.

88
06:46:31 --> 06:46:34
This form of join is
called a Natural Join.

89
06:46:35 --> 06:46:40
It is important to understand that
join is one of the most expensive

90
06:46:40 --> 06:46:44
that means time consuming and
space consuming operations.

91
06:46:45 --> 06:46:51
As data becomes larger, and tables contain
hundreds of millions of tuples, the join

92
06:46:51 --> 06:46:55
operation can easily become a bottleneck
in a larger analytic application.

93
06:46:56 --> 06:47:02
So for analytical big data application
that needs joins, it's very important to

94
06:47:02 --> 06:47:07
choose a suitable data management platform
that makes this operation efficient.

95
06:47:07 --> 06:47:10
We will return to this
issue in module four.

96
06:47:11 --> 06:47:13
We end this video on a practical note.

97
06:47:14 --> 06:47:16
In many scientific and

98
06:47:16 --> 06:47:19
business applications,
people start with CSV files,

99
06:47:19 --> 06:47:24
manipulate them with the spreadsheet,
then migrate their relational system only

100
06:47:24 --> 06:47:28
as an afterthought where the data becomes
too large to handle the spreadsheet.

101
06:47:30 --> 06:47:33
While the spreadsheet offers
many useful features.

102
06:47:33 --> 06:47:39
It does not conform and enforce many
principles of relational data models.

103
06:47:40 --> 06:47:45
Consequently, a large amount of time
may be spent in cleaning up and

104
06:47:45 --> 06:47:48
correcting data errors after
the migration actually happens.

105
06:47:49 --> 06:47:54
Let me show a few examples from
a spreadsheet that has 125,000 rows and

106
06:47:54 --> 06:47:55
over 100 columns.

107
06:47:56 --> 06:48:02
The spreadsheet here lists terrorism
attacks gathered from news media.

108
06:48:02 --> 06:48:05
So each row represents one attack.

109
06:48:06 --> 06:48:10
This is a valuable piece of data for
people who study terrorism.

110
06:48:10 --> 06:48:13
But we are going to look at it from
a relational data modelling viewpoint.

111
06:48:15 --> 06:48:18
First, notice the column marked in green.

112
06:48:19 --> 06:48:23
It lists two weapons used in
the attack separated by a semicolon.

113
06:48:25 --> 06:48:27
Why is this really common?

114
06:48:27 --> 06:48:29
It makes this column non-atomic.

115
06:48:29 --> 06:48:34
It means that this column actually
has two different values.

116
06:48:35 --> 06:48:40
In a relational design, this information
will be moved to another table

117
06:48:40 --> 06:48:43
just like the multiple salaries of
employees were placed in a separate table.

118
06:48:45 --> 06:48:48
Next, notice the column outlined in red.

119
06:48:50 --> 06:48:55
It describes the amount of property
damaged by a possible terrorist attack.

120
06:48:56 --> 06:49:02
In this column, the intended
legitimate values are unknown,

121
06:49:02 --> 06:49:04
minor, major and catastrophic.

122
06:49:05 --> 06:49:10
However, the value in the highlighted
part of the spreadsheet is minor and

123
06:49:10 --> 06:49:15
then within bracket,
likely less than $1 million.

124
06:49:15 --> 06:49:19
Which means a query like
find all attacks for

125
06:49:19 --> 06:49:23
which the property damage is equal to
minor cannot be answered directly.

126
06:49:24 --> 06:49:28
Instead, we need to perform
a substring search for

127
06:49:28 --> 06:49:30
minor in the beginning of the description.

128
06:49:30 --> 06:49:33
Which is doable, but
it's a more expensive operation.

129
06:49:34 --> 06:49:38
This shows the columns of the spreadsheet.

130
06:49:38 --> 06:49:40
So this is part of the schema of the data.

131
06:49:42 --> 06:49:45
If you observe carefully you
will see a recurring pattern.

132
06:49:47 --> 06:49:52
The designer of the data table determined
that there can be at most three types of

133
06:49:52 --> 06:49:57
attacks within a single encounter and
represented with three separate columns.

134
06:49:58 --> 06:50:04
Now in proper relational modeling,
one would say that there is a one to many

135
06:50:04 --> 06:50:09
relationship between the attack and
the number of attack types.

136
06:50:11 --> 06:50:13
In such a case, it would be more prudent

137
06:50:13 --> 06:50:18
to place these attack type
columns in a separate table and

138
06:50:18 --> 06:50:22
connect with the parent using a primary
key, foreign key relationship.

139
06:50:23 --> 06:50:27
Here's another block,
with a similar pattern,

140
06:50:28 --> 06:50:32
this time this is about the types and
subtypes of weapons used.

141
06:50:33 --> 06:50:36
Now can you determine how you might
be able to reorganize this block?

142
06:50:38 --> 06:50:39
We'll leave this as an exercise.

1
13:30:45 --> 13:30:47
It can be said without a doubt,
and the Internet and

2
13:30:47 --> 13:30:50
the worldwide web changed
everything in our lives.

3
13:30:52 --> 13:30:57
The worldwide web is indeed the largest
information source there is today.

4
13:30:57 --> 13:30:58
But what's the data model behind the web?

5
13:30:59 --> 13:31:02
We will say that it is
the semi-structure data model.

6
13:31:04 --> 13:31:08
So after going through this video you
will be able to distinguish between

7
13:31:08 --> 13:31:11
the structured data model that we
talked about the last time and

8
13:31:11 --> 13:31:14
semi-structured data model.

9
13:31:14 --> 13:31:18
Further, you will recognize that the most

10
13:31:18 --> 13:31:22
times the semi-structured data
refers to tree structured data.

11
13:31:23 --> 13:31:29
And you can explain why tree
navigation operations are important for

12
13:31:29 --> 13:31:31
formats like XML and JSON.

13
13:31:33 --> 13:31:35
Let's a take a very simple web page.

14
13:31:37 --> 13:31:40
Now this page does not have
a lot of content or stylization.

15
13:31:40 --> 13:31:43
It doesn't even have links to other pages,
but

16
13:31:43 --> 13:31:46
let's look at the corresponding HTML code.

17
13:31:48 --> 13:31:53
This code is used by the browser so
that it can render the HTML, and

18
13:31:53 --> 13:31:55
notice a few things in this data.

19
13:31:55 --> 13:32:01
The entire data comes within the HTML and
slash HTML blocks.

20
13:32:03 --> 13:32:10
And we similarly have a body begin and
end, a header begin and

21
13:32:10 --> 13:32:15
end, a list begin and end and
a paragraph begin and end.

22
13:32:18 --> 13:32:22
Everywhere here a block is
nested within a larger block.

23
13:32:23 --> 13:32:28
The second item to notice is that
unlike a relational structure

24
13:32:28 --> 13:32:31
there are multiple list items and
multiple paragraphs.

25
13:32:31 --> 13:32:35
And any single document would
have a different number of them.

26
13:32:36 --> 13:32:41
This means while the date object has
some structure it is more flexible.

27
13:32:42 --> 13:32:47
So this is the hallmark office
semi structure date model.

28
13:32:47 --> 13:32:49
Now XML, or

29
13:32:49 --> 13:32:54
the extensible markup language, is another
well known standard to represent data.

30
13:32:54 --> 13:32:58
You can think of XML as a generalization
of HTML where the elements, that's

31
13:32:58 --> 13:33:03
the beginning and end markers within
the angular brackets, can be any string.

32
13:33:03 --> 13:33:06
And not like the ones
allowed by standard HTML.

33
13:33:07 --> 13:33:10
Let's see an example
from a biological case.

34
13:33:12 --> 13:33:16
As you can see, there are two
elements called sample attribute.

35
13:33:18 --> 13:33:21
They do structurally
different because they

36
13:33:21 --> 13:33:24
have different numbers of sub
elements called the value.

37
13:33:26 --> 13:33:28
Another interesting issue
about XML data processing

38
13:33:28 --> 13:33:32
is that you can actually credit for
the structure elements.

39
13:33:32 --> 13:33:37
For example, it is perfectly fine to ask,
what is the name of the element

40
13:33:37 --> 13:33:41
which contains a sub-element whose
textual content is cell type?

41
13:33:42 --> 13:33:48
As you can see, you'll get two results,
sample attribute.

42
13:33:48 --> 13:33:53
An experimental factor because sample
attribute has a sub-element called

43
13:33:53 --> 13:33:57
category and experimental factor
has a subelement called link and

44
13:33:57 --> 13:34:00
each of these subelements
have the value celltape.

45
13:34:02 --> 13:34:05
Now we cannot perform an operation
like this in a relational data model.

46
13:34:05 --> 13:34:10
For example, we cannot say which relation
has a column with a value, John.

47
13:34:12 --> 13:34:17
The same idea can also be seen in JSON or
the Java Script Object Notation, which

48
13:34:17 --> 13:34:23
is a very popular format used for many
different data like Twitter and Facebook.

49
13:34:23 --> 13:34:29
Consider the example here,
all of the format looks different.

50
13:34:29 --> 13:34:32
We have a similar nested
structure varies that is lists

51
13:34:32 --> 13:34:37
containing other lists which will contain
topples Which consists of p value ps.

52
13:34:39 --> 13:34:43
So the key value pairs at atomic
property names and their values.

53
13:34:44 --> 13:34:49
But one way to generalize about all these
different forms of semi structured data

54
13:34:49 --> 13:34:52
is to model them as trees.

55
13:34:52 --> 13:34:54
Let's go back to .xml.

56
13:34:54 --> 13:34:57
The left side shows an XML document, and

57
13:34:57 --> 13:34:59
the right side shows
the corresponding tree.

58
13:35:00 --> 13:35:04
Since the top object of
the root element is document,

59
13:35:04 --> 13:35:05
it is also the root of the tree.

60
13:35:06 --> 13:35:10
Now under document we have
a report element with author and

61
13:35:10 --> 13:35:16
date under it, and also a paper element
with title, author, and source under it.

62
13:35:16 --> 13:35:20
The actual values,
like is the textual content of an element.

63
13:35:22 --> 13:35:25
Since a text data item cannot
have any further components,

64
13:35:25 --> 13:35:28
these text values are always
the leaves of the tree.

65
13:35:30 --> 13:35:35
Now, modeling a document as a tree
has significant advantages.

66
13:35:35 --> 13:35:37
A tree is a well-known data structure,

67
13:35:37 --> 13:35:42
that allows what's called
a navigational access to data.

68
13:35:42 --> 13:35:44
Imagine you are standing
on the note paper.

69
13:35:45 --> 13:35:50
Now you can perform a getParent
operation and navigate the document.

70
13:35:50 --> 13:35:56
Or you can perform a getChildren operation
to get to the title, author and source.

71
13:35:56 --> 13:36:00
You can even perform a getSiblings
operation and get to the report.

72
13:36:01 --> 13:36:06
You can also ask a textual query like
which strings have the substring data and

73
13:36:06 --> 13:36:13
seek their root-to-node path to get to
the path from document to the text nodes.

74
13:36:13 --> 13:36:16
You can possibly see how queries
can be evaluated on the tree,

75
13:36:17 --> 13:36:19
now let us take the query.

76
13:36:19 --> 13:36:21
Who is the author of XML query data model.

77
13:36:23 --> 13:36:29
In one evaluation scheme we can navigate
up from the text note to title,

78
13:36:29 --> 13:36:35
to paper, and then navigate down
to author and then to Don Robie.

79
13:36:35 --> 13:36:38
Well how do we know that we have to get up
to paper before reversing the direction?

80
13:36:40 --> 13:36:43
Well, paper is the least,
that's the lowest in the tree,

81
13:36:43 --> 13:36:49
common ancestor of the author note,
and the XM query data model note.

82
13:36:49 --> 13:36:52
We will come back to semi
structure data in a later module.

1
03:07:35 --> 03:07:39
In this hands-on activity,
we will be looking at graph data in Gephi.

2
03:07:39 --> 03:07:44
First, we will import data into Gephi and
then examine the properties of the graph.

3
03:07:45 --> 03:07:49
Next, we will perform some statistical
operations on the graph data,

4
03:07:49 --> 03:07:51
and then run some different
layout algorithms.

5
03:07:54 --> 03:07:55
Let's begin.

6
03:07:55 --> 03:08:00
We're not running Gephi in the Cloudera
virtual machine, on the Coursera website,

7
03:08:00 --> 03:08:04
there will be a reading with instructions
on how to download, install, and

8
03:08:04 --> 03:08:08
run Gephi on your native hardware,
instead of in the virtual machine.

9
03:08:09 --> 03:08:13
Once you have Gephi started,
let's import the data into Gephi.

10
03:08:13 --> 03:08:20
We'll go to File Import spreadsheet,
and in the CSV dialog,

11
03:08:20 --> 03:08:26
we'll click the button with dot, dot, dot.

12
03:08:26 --> 03:08:31
We'll choose diseasegraph.csv and
click open.

13
03:08:35 --> 03:08:40
Make sure that as table says
edges table Click next.

14
03:08:43 --> 03:08:47
And make sure Create
missing nodes is checked.

15
03:08:47 --> 03:08:51
We'll click finish to import
the CSV file as a graph.

16
03:08:51 --> 03:08:53
Gephi now shows the graph
in the center pane.

17
03:08:55 --> 03:08:57
The little black circles
are the nodes of the graph and

18
03:08:57 --> 03:09:00
the lines between them are the edges.

19
03:09:00 --> 03:09:06
In the top right we can see that
there are 777 nodes and 998 edges.

20
03:09:06 --> 03:09:10
Next let's perform some statistical
operations on this graph.

21
03:09:11 --> 03:09:16
In the statistics pane we
can see average degree.

22
03:09:16 --> 03:09:18
Let's compute the average degree
of the graph by clicking on Run.

23
03:09:23 --> 03:09:27
This says that the average
degree is 2.569.

24
03:09:27 --> 03:09:32
Let's close this,
let's compute the connected

25
03:09:32 --> 03:09:36
components, we'll click on Run.

26
03:09:36 --> 03:09:39
We'll leave this as a directed,
since the graph is directed.

27
03:09:41 --> 03:09:43
Click OK.
It says that there

28
03:09:43 --> 03:09:48
are 5 weakly connected components,
and 761 strongly connected components.

29
03:09:50 --> 03:09:51
Let's close this.

30
03:09:52 --> 03:09:55
Next, let's run some different
layout algorithms over the graph.

31
03:09:57 --> 03:09:59
The bottom left,
we'll go to choose layout.

32
03:10:01 --> 03:10:06
We'll choose Force Atlas and click run.

33
03:10:24 --> 03:10:26
Click stop to stop the layout.

34
03:10:27 --> 03:10:32
We can see that Gephi has grouped
strongly connected components together

35
03:10:32 --> 03:10:33
in different clusters.

36
03:10:33 --> 03:10:35
We can also see that they're parts
of the graph that are not connected.

37
03:10:37 --> 03:10:38
Let's run a different layout algorithm.

38
03:10:40 --> 03:10:45
The combo box,
choose Fruchterman Reingold, click run.

39
03:10:52 --> 03:10:57
After it runs for a few seconds, click
stop, then click the magnifying glass,

40
03:10:57 --> 03:10:59
center on graph, to see the whole graph.

41
03:11:02 --> 03:11:05
In this layout,
all the nodes appear to be equally spaced.

42
03:11:05 --> 03:11:08
But we can also see
the nodes with many edges.

1
06:18:43 --> 06:18:46
In this hands on activity,
we will be working with Lucene,

2
06:18:46 --> 06:18:49
a search engine that uses a vector
space model to index data.

3
06:18:50 --> 06:18:52
First, we will open a terminal window and

4
06:18:52 --> 06:18:56
change into the directory
containing the data and scripts.

5
06:18:56 --> 06:19:00
Next, we will index some text
documents and query terms in Lucene.

6
06:19:01 --> 06:19:04
After that we'll query
using weighted terms or

7
06:19:04 --> 06:19:07
boosting to see how this
changes the rankings.

8
06:19:07 --> 06:19:12
Finally, we will show the term
frequency-inverse document frequency or

9
06:19:12 --> 06:19:14
TF-IDF in terms.

10
06:19:14 --> 06:19:17
Let's begin.

11
06:19:17 --> 06:19:21
First, let's open a terminal window by
clicking on the terminal icon at the top

12
06:19:21 --> 06:19:21
of the tool bar.

13
06:19:25 --> 06:19:29
Next, let's cd into the directory
containing the scripts and data.

14
06:19:29 --> 06:19:30
We'll run cd

15
06:19:30 --> 06:19:40
Downloads/big-data-2/vector/.

16
06:19:40 --> 06:19:44
We'll run ls to see the scripts.

17
06:19:48 --> 06:19:51
The data directory
contains three text files.

18
06:19:51 --> 06:19:55
Each of these text files contains
news data about elections.

19
06:19:57 --> 06:20:03
Let's index these files by
running runLuceneQuery.sh data.

20
06:20:10 --> 06:20:17
Next, let's query Lucene for some terms in
these text documents and see the rankings.

21
06:20:17 --> 06:20:19
Let's query for the term voters.

22
06:20:25 --> 06:20:29
You can see the rankings and
scores that news1.csv ranked first,

23
06:20:29 --> 06:20:34
news2.csv was the second ranking,
and the third was news3.csv.

24
06:20:34 --> 06:20:38
Let's query for delegates.

25
06:20:38 --> 06:20:43
For this term, we see that news2.csv

26
06:20:43 --> 06:20:48
was first, and news1 was second, and

27
06:20:48 --> 06:20:53
news3 did not contain the term at all.

28
06:20:55 --> 06:20:59
Now, let's query for both terms,
voters and delegates.

29
06:20:59 --> 06:21:06
In this result we see that
news2 was ranked first,

30
06:21:06 --> 06:21:11
news1 was ranked second, and

31
06:21:11 --> 06:21:15
news3 was ranked third.

32
06:21:17 --> 06:21:19
Now lets use query term waiting or

33
06:21:19 --> 06:21:22
boosting to increase
the relevance of voters.

34
06:21:22 --> 06:21:27
I can do this by ensuring
voters carat 5 delegates.

35
06:21:27 --> 06:21:35
The carat 5 notation is a syntax for

36
06:21:35 --> 06:21:40
Lucene for boosting.

37
06:21:41 --> 06:21:47
When we run this, we see that now,
news1 is ranked first,

38
06:21:47 --> 06:21:52
news2 is ranked second,
and news3 is ranked third.

39
06:21:52 --> 06:21:56
Notice this is different from the original
query with voters and delegates,

40
06:21:56 --> 06:22:00
where news2 is ranked first and
news1 was ranked second.

41
06:22:02 --> 06:22:05
Now let's look at the term frequency,
inverse document frequency or TF-IDF.

42
06:22:05 --> 06:22:10
We'll enter q to quit this and

43
06:22:10 --> 06:22:15
we'll run Lucene TF-IDF SH data.

44
06:22:27 --> 06:22:33
Let's look at the TF-IDF for voters.

45
06:22:33 --> 06:22:35
You can see that it ranked number 1 for
news1.

46
06:22:36 --> 06:22:40
Second news 2 and news 3 is last.

47
06:22:40 --> 06:22:41
Lets try delegates.

48
06:22:47 --> 06:22:51
Here we see that news 2 had
a higher score than news 1,

49
06:22:51 --> 06:22:54
and news 3 is not listed because
news 3 does not contain this term.

50
06:22:55 --> 06:22:56
Hit q to quit.

1
12:41:38 --> 12:41:43
So the next category of data we discuss
has the form of graphs or networks,

2
12:41:43 --> 12:41:46
the most obvious example
being social networks.

3
12:41:46 --> 12:41:48
Now speaking of social networks,

4
12:41:48 --> 12:41:53
Tim Libzek created a social network
from the Lord of the Rings Trilogy.

5
12:41:53 --> 12:41:56
This graph represents
the characters' allegiances,

6
12:41:56 --> 12:41:58
that is who is faithful
to whom in the books.

7
12:41:58 --> 12:42:02
So the nodes are characters and
other entities, like cities, and

8
12:42:02 --> 12:42:07
the edges connecting pairs of
nodes represent allegiances.

9
12:42:07 --> 12:42:13
So after this video, you'll be able to
identify graph data in practical problems

10
12:42:13 --> 12:42:19
and describe path, neighborhood, and
connectivity operations in graphs.

11
12:42:19 --> 12:42:22
But this specialization includes
a separate course in graph analytics

12
12:42:22 --> 12:42:26
that provides a much more detailed
treatment on the subject.

13
12:42:26 --> 12:42:29
Now what distinguishes a graph
from other data models

14
12:42:29 --> 12:42:33
is that it bears two kinds of information.

15
12:42:33 --> 12:42:38
One, properties and attributes of
entities and relationships, and

16
12:42:38 --> 12:42:42
two, the connectivity structure that
constitutes the network itself.

17
12:42:43 --> 12:42:46
One way to look at this data
is shown in the figure,

18
12:42:46 --> 12:42:48
borrowed from the Apache Spark system.

19
12:42:49 --> 12:42:50
In this representation,

20
12:42:50 --> 12:42:54
the graph on the left is represented
by two tables on the right.

21
12:42:54 --> 12:43:00
The vertex, or node table, gives IDs
to nodes and lists their properties.

22
12:43:01 --> 12:43:03
The edge table has two parts.

23
12:43:03 --> 12:43:07
The colored part represents
the properties of the edge,

24
12:43:07 --> 12:43:11
whereas the white part contains just the
direction of the arrows in the network.

25
12:43:11 --> 12:43:17
Thus, since there is a directed
edge going from node 3 to node 7,

26
12:43:17 --> 12:43:21
there is a tupple 3,
7 in that part of the edge table.

27
12:43:21 --> 12:43:26
Now this form of the graph model is
called the property graph model,

28
12:43:26 --> 12:43:30
which we'll see many times in this
course and in the specialization.

29
12:43:30 --> 12:43:35
Now representing connectivity information
gives graph data a new kind of

30
12:43:35 --> 12:43:39
computing ability that's different from
other data models we have seen so far.

31
12:43:41 --> 12:43:45
Even without looking at the properties
of the nodes and edges, one can get very

32
12:43:45 --> 12:43:50
interesting information just by analyzing
or querying this connectivity structure.

33
12:43:51 --> 12:43:56
Consider a social network with three
types of nodes, user, city, and

34
12:43:56 --> 12:44:02
restaurant, and three types of edges,
friend, likes, and lives in.

35
12:44:03 --> 12:44:06
The leftmost node, AG, represents me.

36
12:44:06 --> 12:44:09
And I'm interested in finding a good
Italian restaurant in New York

37
12:44:09 --> 12:44:14
that my friends, or their friends,
who also live in New York, like.

38
12:44:14 --> 12:44:19
I shall possibly choose IT3 because
it has the highest number of

39
12:44:19 --> 12:44:25
like edges coming into it from people
who have a lives in edge to New York.

40
12:44:25 --> 12:44:27
And at the same time,

41
12:44:27 --> 12:44:30
can be reached by following
the friend edges going out from me.

42
12:44:30 --> 12:44:35
Now this shows a very important class
of operations and ground data, namely

43
12:44:35 --> 12:44:39
traversal, that involves edge following
based on some sort of conditions.

44
12:44:41 --> 12:44:45
A number of path operations
required some sort of optimization.

45
12:44:45 --> 12:44:50
The simplest among these is the well known
shortest path query, which is applied to

46
12:44:50 --> 12:44:55
node networks to find the best route from
a source location to a target location.

47
12:44:55 --> 12:44:59
The second class of optimization
operations is required to find

48
12:44:59 --> 12:45:04
an optimal path that must include
some user specified nodes, for

49
12:45:04 --> 12:45:08
the operation has to determine the order
in which the nodes once we visited.

50
12:45:08 --> 12:45:10
The classical application
is a trip planner,

51
12:45:10 --> 12:45:14
where the user specifies the cities
she wishes to visit, and

52
12:45:14 --> 12:45:18
the operation will optimize the criterion,
like the total distance covered.

53
12:45:18 --> 12:45:23
The third category is a case where
the system must find the best possible

54
12:45:23 --> 12:45:25
path in the network, given two or

55
12:45:25 --> 12:45:30
more optimization criteria,
which cannot be satisfied simultaneously.

56
12:45:30 --> 12:45:34
For example, if I want to travel
from my house to the airport

57
12:45:34 --> 12:45:39
using the shortest distance, but also
minimizing the amount of highway travel,

58
12:45:39 --> 12:45:41
the algorithm must find a best compromise.

59
12:45:41 --> 12:45:45
This is called a pareto-optimality
problem on graphs.

60
12:45:45 --> 12:45:51
The neighborhood of a node N in a graph is
a set of edges directly connected to it.

61
12:45:51 --> 12:45:56
A K neighborhood of N is a collection
of edges between nodes that are,

62
12:45:56 --> 12:45:59
at most, K steps away from N.

63
12:45:59 --> 12:46:02
So going back to our mini social
network graph, Bob, Jill,

64
12:46:02 --> 12:46:07
and Sarah are the first neighbors of AG,
while Max, Tim and

65
12:46:07 --> 12:46:12
Pam belong to the second neighborhood and
not the first neighborhood of AG.

66
12:46:12 --> 12:46:15
Finally, Jen is a third level neighbor.

67
12:46:16 --> 12:46:21
An important class of analysis to perform
with neighborhoods is community finding.

68
12:46:21 --> 12:46:25
A community and a social network can
be a very close group of friends.

69
12:46:25 --> 12:46:29
So the graph shown in this
figure has four communities.

70
12:46:29 --> 12:46:34
One can see in the figure that each
community has a higher density of edges

71
12:46:34 --> 12:46:40
within the community and a lower density
across two different communities.

72
12:46:40 --> 12:46:43
Finding densely connected parts of a graph

73
12:46:43 --> 12:46:47
helps identify neighborhoods that
can be recognized as communities.

74
12:46:47 --> 12:46:53
A more complex class of operations include
finding the best possible clusters,

75
12:46:53 --> 12:46:55
which is another name for
communities in a graph, so

76
12:46:55 --> 12:47:00
that any other grouping of nodes into
communities will be less effective.

77
12:47:00 --> 12:47:05
Now, as graphs become bigger and denser,
these methods become harder to compute.

78
12:47:05 --> 12:47:10
Thus, neighborhood-based
optimization operation present

79
12:47:10 --> 12:47:12
significant scalability challenges.

80
12:47:12 --> 12:47:16
If we inspect the neighborhood of
every node in a graph, sometimes,

81
12:47:16 --> 12:47:20
we'll find neighborhoods that
are different from all others.

82
12:47:20 --> 12:47:23
These neighborhoods are called anomalous.

83
12:47:23 --> 12:47:27
Consider the following four graphs and
on the central red node.

84
12:47:27 --> 12:47:32
The first graph is odd because
it's almost perfectly star shaped.

85
12:47:32 --> 12:47:36
That is, the nodes that the red node
is connected to are almost unconnected

86
12:47:36 --> 12:47:38
amongst themselves.

87
12:47:38 --> 12:47:42
That's really odd because it
doesn't happen in reality much.

88
12:47:42 --> 12:47:43
So it's an anomalous node.

89
12:47:43 --> 12:47:47
The second figure shows
a neighborhood to which

90
12:47:47 --> 12:47:51
a significantly large number of neighbors
has connected amongst themselves.

91
12:47:51 --> 12:47:56
This makes the graph very cliquish,
where a clique refers to a neighborhood

92
12:47:56 --> 12:48:01
where each node is connected to all other
neighborhood nodes in the neighborhood.

93
12:48:01 --> 12:48:03
The third figure shows a neighborhood,

94
12:48:03 --> 12:48:09
where some edges have an unusually
heavy weight compared to the others.

95
12:48:09 --> 12:48:12
The fourth figure shows
a special case of the third,

96
12:48:12 --> 12:48:16
where one edge is predominantly high
rate compared to all the other edges.

97
12:48:18 --> 12:48:20
Connectedness is a fundamental
property of a graph.

98
12:48:21 --> 12:48:23
In a connected graph,

99
12:48:23 --> 12:48:27
each node is reachable from every
other node through some path.

100
12:48:27 --> 12:48:32
If a graph is not connected, but there
are subgraphs of it, which are connected,

101
12:48:32 --> 12:48:36
then these subgraphs are called connected
components of the original graph.

102
12:48:36 --> 12:48:39
In the figure on the right,
there are four connected components.

103
12:48:39 --> 12:48:43
A search gradient like
finding optimal paths

104
12:48:43 --> 12:48:47
should be performed only within
each component and not across them.

105
12:48:48 --> 12:48:52
For large graphs, there are several
new parallelized techniques for

106
12:48:52 --> 12:48:53
the detection of connected components.

107
12:48:55 --> 12:48:57
We will discuss a map
reduce based technique for

108
12:48:57 --> 12:48:59
connected components in a later course.

1
01:30:40 --> 01:30:42
We have discussed quite a few data models,
but

2
01:30:42 --> 01:30:45
there are many other data models
that have been developed for

3
01:30:45 --> 01:30:50
various purposes, and we really cannot
cover all of them in a single course.

4
01:30:50 --> 01:30:54
We'll end these lectures on data models
with an example that may give you

5
01:30:54 --> 01:30:58
an insight into a class of objects that
define in many different applications.

6
01:30:59 --> 01:31:04
So after this video you'll be able
to describe how arrays can serve

7
01:31:04 --> 01:31:10
as a data model, explain why images
can be modeled as vector arrays,

8
01:31:10 --> 01:31:14
specify a set of operations on scalar and
vector arrays.

9
01:31:16 --> 01:31:19
Now, we have all seen the arrays.

10
01:31:19 --> 01:31:23
In the simplest case,
an array is a matrix like this.

11
01:31:23 --> 01:31:25
Let's call this array A.

12
01:31:27 --> 01:31:32
The top row in yellow,
gives the column numbers and

13
01:31:32 --> 01:31:34
the left column, also in yellow,
gives the row numbers.

14
01:31:35 --> 01:31:40
When we need to refer to a value
of the array as A(3, 2),

15
01:31:40 --> 01:31:44
we mean the value of the cell in row 3 and
column 2.

16
01:31:44 --> 01:31:49
This is called indexed structure,
where 3 and

17
01:31:49 --> 01:31:55
2 are the row and column indices that are
necessary to get the value of a data item.

18
01:31:56 --> 01:31:58
The area has two dimensions.

19
01:31:58 --> 01:32:01
So hence there are two indexes.

20
01:32:01 --> 01:32:04
If these were a three dimensional array,
we would have three indexes.

21
01:32:05 --> 01:32:06
Now earlier,

22
01:32:06 --> 01:32:11
we have seen that we can represent the two
dimensional array as a three column table.

23
01:32:11 --> 01:32:14
One column for the row index,
one column for the column index, and

24
01:32:14 --> 01:32:15
the last column for the value.

25
01:32:17 --> 01:32:22
Thus a k dimensional array can be
represented as a relation with k

26
01:32:22 --> 01:32:23
plus one columns.

27
01:32:24 --> 01:32:28
The number of tuples in this
representation will be the product of

28
01:32:28 --> 01:32:32
the size of the first dimension times the
size of the second dimension and so forth.

29
01:32:33 --> 01:32:38
Then in this case,
the size is five in each dimension.

30
01:32:38 --> 01:32:45
So there are 25 C column tuples in
a relation representing the array.

31
01:32:45 --> 01:32:48
A more useful situation occurs
when the cells of an array have

32
01:32:48 --> 01:32:50
a vectors as values.

33
01:32:51 --> 01:32:56
As you can see in the 2D vector array
here, each cell has a three vector.

34
01:32:56 --> 01:32:58
That is a vector with three elements.

35
01:32:59 --> 01:33:02
Therefore, if we want to
receive a cell value and

36
01:33:02 --> 01:33:04
treat it like before,
we'll get back the whole vector.

37
01:33:06 --> 01:33:08
Now, this type of data
should look familiar to you,

38
01:33:08 --> 01:33:12
because images often have a red,
green and blue channels per pixel.

39
01:33:14 --> 01:33:15
In other words,

40
01:33:15 --> 01:33:20
images of vector valued arrays where each
array cell has a three color vector.

41
01:33:22 --> 01:33:26
We can also think of the array model
in the context of satellite images.

42
01:33:26 --> 01:33:29
Where there are many more channels
depending on the range of wavelengths

43
01:33:29 --> 01:33:31
each channel catches.

44
01:33:32 --> 01:33:35
Let us consider the operations
on arrays of vectors.

45
01:33:35 --> 01:33:40
Because it is a combination of two models,
one can create different combinations of

46
01:33:40 --> 01:33:45
array operations, vector operations and
composite operations.

47
01:33:45 --> 01:33:45
Here are some.

48
01:33:46 --> 01:33:49
The dimension of the array here,
the first operation, is two.

49
01:33:50 --> 01:33:53
If we pick up any dimension, say one.

50
01:33:53 --> 01:33:57
The size of it is also two
because they're two elements,

51
01:33:57 --> 01:34:00
zero and one in each dimension.

52
01:34:00 --> 01:34:08
As we saw before, the value of the cell
(1,1) is a vector 16, 301, 74.

53
01:34:08 --> 01:34:14
While the value of A11 component 2 is 74.

54
01:34:14 --> 01:34:19
The length of the vector is a square root
of the sum of the elements of the vector.

55
01:34:19 --> 01:34:25
So length of A11 would come to 310.375.

56
01:34:25 --> 01:34:27
The distance function can be so

57
01:34:27 --> 01:34:31
simple like the Euclidean distance
function between two vectors or

58
01:34:31 --> 01:34:34
the cosine of an angle between them
as we saw in the previous lecture.

59
01:34:36 --> 01:34:39
But it can also be something more complex,
based on the needs of the application.

60
01:34:41 --> 01:34:45
Obviously one can also perform operations
like selection over indices so

61
01:34:45 --> 01:34:50
we can ask which cells had
the zero value greater than 25.

62
01:34:50 --> 01:34:53
Giving as the result zero one and
one zero.

63
01:34:54 --> 01:34:57
You will experience some of these
operations in your hands on session.

1
03:05:36 --> 03:05:40
Next, we'll look at a data model
that has been successfully used to

2
03:05:40 --> 03:05:44
retrieve data from large
collections of text and images.

3
03:05:45 --> 03:05:46
Let's stay with text for now.

4
03:05:47 --> 03:05:51
Text is often thought of
as unstructured data.

5
03:05:51 --> 03:05:54
Primarily because it doesn't really
have attributes and relationships.

6
03:05:55 --> 03:05:59
Instead, it is a sequence of strings
punctuated by line and parent of breaks.

7
03:06:01 --> 03:06:08
So one is to think of a different
way to find and analyze text data.

8
03:06:08 --> 03:06:13
In this video, we'll describe that
finding text from a huge collection of

9
03:06:13 --> 03:06:16
text data is a little different from
the data modules we have seen so far.

10
03:06:18 --> 03:06:22
To find text,
we not only need the text data itself, but

11
03:06:22 --> 03:06:27
we need a different structure that
is computed from the text data.

12
03:06:27 --> 03:06:29
To create the structure,

13
03:06:29 --> 03:06:35
we'll introduce the notion of the document
vector model which we call a vector model.

14
03:06:36 --> 03:06:39
Further you will see
that finding a document

15
03:06:39 --> 03:06:42
is not really an exact search problem.

16
03:06:43 --> 03:06:47
Here, we'll give a query document and

17
03:06:47 --> 03:06:51
ask the system to find all
documents that are similar to it.

18
03:06:52 --> 03:06:54
After this video,
you'll be able to describe

19
03:06:54 --> 03:06:59
how the similarity is computed and
how it is used to search documents.

20
03:07:00 --> 03:07:05
Finally, you will see that search engines
use some form of vector models and

21
03:07:05 --> 03:07:07
similarity search to locate text data.

22
03:07:09 --> 03:07:13
And you will see that the same principle
can be use for finding similar images.

23
03:07:16 --> 03:07:19
Let us describe the concept
of a document to an example.

24
03:07:20 --> 03:07:23
So lets consider three types
of document shown here.

25
03:07:25 --> 03:07:26
Now we'll create a matrix.

26
03:07:30 --> 03:07:31
The rows of the matrix stand for

27
03:07:31 --> 03:07:36
the documents and columns represent
the words in the documents.

28
03:07:36 --> 03:07:39
We put the number of occurrences
of returning the document in

29
03:07:39 --> 03:07:41
the appropriate cell of the matrix.

30
03:07:42 --> 03:07:48
In this case, the count of each term
in each document happens to be one.

31
03:07:48 --> 03:07:50
This is called the term frequency matrix.

32
03:07:53 --> 03:07:56
So now that we have created
the term frequency matrix,

33
03:07:56 --> 03:07:58
which we call tf for short.

34
03:07:59 --> 03:08:04
We'll create a new vector called the
inverse document frequency for each term.

35
03:08:05 --> 03:08:08
We'll explain why we need this
vector on the next slide.

36
03:08:08 --> 03:08:11
First, let's see how it's computed.

37
03:08:13 --> 03:08:17
The number of documents n here is 3.

38
03:08:17 --> 03:08:20
The term new occurs
twice in the collection.

39
03:08:21 --> 03:08:26
So the inverse document frequency or
IDF of the term new

40
03:08:26 --> 03:08:31
is log to the base 2,
n divided by term count.

41
03:08:31 --> 03:08:38
That is, log to the base 2,
3 divided by 2, which is 0.584.

42
03:08:39 --> 03:08:43
We'll show the ideal score for
all six terms here.

43
03:08:44 --> 03:08:49
Now some of you may wonder why we use
log to the base 2 instead of let's

44
03:08:49 --> 03:08:51
say log to the base 10.

45
03:08:51 --> 03:08:54
There is no deep scientific reason for it.

46
03:08:54 --> 03:08:57
It's more of a convention in
many areas of Computer Science

47
03:08:57 --> 03:09:00
when many important
numbers are powers of two.

48
03:09:01 --> 03:09:06
In reality,
log to the base two of x is the same

49
03:09:06 --> 03:09:13
number as log to the base ten of x
times log to the base two of ten.

50
03:09:13 --> 03:09:18
The second number, that is log to
the base two of ten is a constant.

51
03:09:18 --> 03:09:24
So the relative score of IDF does not
change regardless of the base we use.

52
03:09:24 --> 03:09:26
Now let's understand this
number one more time.

53
03:09:27 --> 03:09:31
The document frequency of a term is
the count of that term in the whole

54
03:09:31 --> 03:09:35
collection divided by
the number of documents.

55
03:09:36 --> 03:09:41
Here, we take the inverse of
the document frequency, so

56
03:09:41 --> 03:09:45
that n, the number of documents,
is in the numerator.

57
03:09:47 --> 03:09:52
Now before we continue, let's understand
the intuition behind the IDF vector.

58
03:09:53 --> 03:09:59
Now suppose you have 100
random newspaper articles and

59
03:09:59 --> 03:10:02
let's say 10 of them cover elections.

60
03:10:03 --> 03:10:07
Which means all the others
cover all other subjects.

61
03:10:07 --> 03:10:13
Now in this article, let's say we'll find
the term election 50 times in total.

62
03:10:15 --> 03:10:20
How often do you think you'll find
the term is as in the verb is?

63
03:10:21 --> 03:10:24
You can imagine that it will
occur in all hundred of them and

64
03:10:24 --> 03:10:26
that too, multiple times.

65
03:10:27 --> 03:10:31
We can safely assume that
the number of occurrences of is

66
03:10:31 --> 03:10:33
will be 300 at the very least.

67
03:10:35 --> 03:10:40
Thus the document frequency of is is six
times the document frequency of election.

68
03:10:41 --> 03:10:44
But that doesn't sound right, does it?

69
03:10:44 --> 03:10:49
Is is such a common word that it's
prevalence has a negative impact

70
03:10:49 --> 03:10:51
on its informativeness.

71
03:10:51 --> 03:10:55
So, now if you want to
compute the IDF of is and

72
03:10:55 --> 03:10:58
election, the IDF of is will be far lower.

73
03:11:00 --> 03:11:03
So, IDF acts like a penalty factor for

74
03:11:03 --> 03:11:07
terms which are too widely used
to be considered informative.

75
03:11:10 --> 03:11:13
Now that we have understood
that IDF is a penalty factor,

76
03:11:13 --> 03:11:19
we will multiply the tf numbers through
the IDF numbers giving us this.

77
03:11:22 --> 03:11:27
This is a column-wise multiplication
of the tf numbers with the IDF

78
03:11:27 --> 03:11:31
numbers giving us what we
call the tf-idf matrix.

79
03:11:33 --> 03:11:40
Therefore for each document, we have
a vector represented here as a row.

80
03:11:40 --> 03:11:44
So that row represents the relative
importance of each term in the vocabulary.

81
03:11:44 --> 03:11:48
Vocabulary means the collection of all
words that appear in this collection.

82
03:11:50 --> 03:11:54
If the vocabulary has 3 million entries,
then this vector can get quite long.

83
03:11:56 --> 03:12:00
Also, if the number of document
grows let's just say to 1 billion,

84
03:12:00 --> 03:12:02
then it becomes a big data problem.

85
03:12:04 --> 03:12:07
Now the last column after
each document of vector here

86
03:12:07 --> 03:12:09
is the length of the document vector.

87
03:12:10 --> 03:12:15
Which is really the square root of the sum
of squares of the individual term scores

88
03:12:15 --> 03:12:16
as shown in the formula.

89
03:12:19 --> 03:12:22
To perform a search in the vector space,

90
03:12:22 --> 03:12:26
we write a query just like
we type terms in Google.

91
03:12:26 --> 03:12:29
Here, the number of terms is three.

92
03:12:29 --> 03:12:31
Out of which the term
new appears two times.

93
03:12:33 --> 03:12:37
In fact, this is the maximum frequency
out of all terms in the query.

94
03:12:38 --> 03:12:42
So we take the document vector of
the query and multiply each term

95
03:12:42 --> 03:12:48
by the number of occurrences divided by
two which is the maximum term frequency.

96
03:12:48 --> 03:12:52
Now in this case,
it gives us two non-zero terms.

97
03:12:52 --> 03:12:54
0.584 and

98
03:12:54 --> 03:13:00
0.292 for new and york.

99
03:13:00 --> 03:13:04
Then we compute the length of
the query vector just like we did for

100
03:13:04 --> 03:13:07
the document vectors
on the previous slide.

101
03:13:07 --> 03:13:13
Next, we will compute the similarity
between the query vector and each document

102
03:13:13 --> 03:13:19
with the idea that we'll measure how far
the query vector is from each document.

103
03:13:21 --> 03:13:27
Now there are many similar functions
defined and used for different things.

104
03:13:27 --> 03:13:30
A popular similarity measure
is the cosine function,

105
03:13:30 --> 03:13:37
which measures the cosine function of
the angle between these two vectors.

106
03:13:37 --> 03:13:41
The mathematical formula for
computing the function is given here.

107
03:13:41 --> 03:13:45
The intuition is that if
the vectors are identical,

108
03:13:45 --> 03:13:48
then the angle between them is zero.

109
03:13:48 --> 03:13:50
And therefore,
the cosine function evaluates to one.

110
03:13:52 --> 03:13:54
As the angle increases,

111
03:13:54 --> 03:13:59
the value of the cosine function
decreases to make them more dissimilar.

112
03:14:00 --> 03:14:04
The way to compute the function is to
multiply the corresponding elements of

113
03:14:04 --> 03:14:06
the two vectors.

114
03:14:06 --> 03:14:08
That is the first element
of one with the first

115
03:14:08 --> 03:14:11
element of the second one and so forth.

116
03:14:11 --> 03:14:12
And then sum of these products.

117
03:14:13 --> 03:14:19
Here, the only contributing
terms are from new and

118
03:14:19 --> 03:14:23
york because, these are the only two
non-zero terms in the query vector.

119
03:14:24 --> 03:14:29
This sum is then divided by
the product of the document length and

120
03:14:29 --> 03:14:32
the query length that we
have computed earlier.

121
03:14:33 --> 03:14:36
Look at the result of the distance
function and you will notice that

122
03:14:36 --> 03:14:41
the document 1 is much more similar
to the query than the other two.

123
03:14:42 --> 03:14:47
So while similarity scoring and document
ranking process working effectively,

124
03:14:47 --> 03:14:50
the method is a little cotton dry.

125
03:14:50 --> 03:14:53
More often than not users would
like a little more control

126
03:14:53 --> 03:14:54
over the ranking of terms.

127
03:14:56 --> 03:15:01
One way of accomplishing this is to put
different weights on each query term.

128
03:15:01 --> 03:15:07
And in this example, the query term
york has a default weight of one,

129
03:15:07 --> 03:15:10
times has a weight of two.

130
03:15:10 --> 03:15:14
And post has a weight of five
as specified by the user.

131
03:15:15 --> 03:15:17
So relatively speaking,

132
03:15:17 --> 03:15:23
york has a weight of 1 divided
by 1 + 5 + 2 is equal to 0.125.

133
03:15:23 --> 03:15:29
Times has a weight of 0.25 and
post has a weight of 0.625.

134
03:15:29 --> 03:15:33
Now the scoring method we showed
before will change a bit.

135
03:15:33 --> 03:15:38
The query of vector and its length
were exactly as computed before.

136
03:15:38 --> 03:15:42
However, now each term in the query
vector is further multiplied by these

137
03:15:42 --> 03:15:44
relative weights.

138
03:15:44 --> 03:15:48
In our case,
the term york now has a much higher rate.

139
03:15:49 --> 03:15:55
So as expected, this will change
the ranking of the documents and

140
03:15:55 --> 03:15:58
new york post will have the highest rank.

141
03:15:58 --> 03:16:04
Now similarity search is often used for
images using a vector space model.

142
03:16:04 --> 03:16:07
One can compute futures from images.

143
03:16:07 --> 03:16:10
And one common feature
is a scatter histogram.

144
03:16:10 --> 03:16:12
Consider the image here.

145
03:16:12 --> 03:16:14
One can create the histogram of the red,
green and

146
03:16:14 --> 03:16:20
blue channels where histogram is the count
of pixels having a certain density value.

147
03:16:21 --> 03:16:26
This picture is mostly bright, so the
count of dark pixels is relatively small.

148
03:16:27 --> 03:16:30
Now one can think of
histograms like a vector.

149
03:16:30 --> 03:16:35
Very often the pixel values will
be bend before creating a vector.

150
03:16:35 --> 03:16:40
The table shown is a feature vector
where the numbers for each row have been

151
03:16:40 --> 03:16:44
normalized with the size of the image
to make the row sum equal to one.

152
03:16:44 --> 03:16:49
Similar vectors can be computed of
the image texture, shapes of objects and

153
03:16:49 --> 03:16:50
any other properties.

154
03:16:50 --> 03:16:54
Thus making a vector space model
significant for unstructured data.

1
06:22:29 --> 06:22:34
In our experience as educators
we have observed that

2
06:22:34 --> 06:22:38
learners often make the assumption
that the format of the data

3
06:22:38 --> 06:22:43
is the same as the logical model of the
data in the way that you operate on it.

4
06:22:43 --> 06:22:46
The goal of this very
short lecture is to ensure

5
06:22:46 --> 06:22:49
that we can clearly
distinguish between the two.

6
06:22:49 --> 06:22:54
So, after watching this video you will be
able to explain the difference between

7
06:22:54 --> 06:22:59
format, which is a serialized
representation of the data, as opposed to

8
06:22:59 --> 06:23:03
data model which we have discussed
at length in the previous months.

9
06:23:04 --> 06:23:09
Perhaps the simplest example of a data
format is a csv file, so here is

10
06:23:09 --> 06:23:13
a snippet of a csv file from the global
terrorism database we discussed earlier.

11
06:23:14 --> 06:23:18
We know that CSV or
common separative values means

12
06:23:18 --> 06:23:21
that the term between two commas
is the value of an attribute.

13
06:23:21 --> 06:23:22
But what is this value?

14
06:23:24 --> 06:23:28
The common notion is that it's
a content of a single relation

15
06:23:29 --> 06:23:32
where each line is a record
that's a tuple and

16
06:23:32 --> 06:23:37
the iod value in the CSV corresponds
to the iod attribute as shown here.

17
06:23:38 --> 06:23:42
Now that might very well be true but
let's look at a different example.

18
06:23:42 --> 06:23:45
Let's say this snippet
here is my CSV file.

19
06:23:46 --> 06:23:49
There is no difference between
the previous file and this one.

20
06:23:49 --> 06:23:52
However, here is how I
like to see the data.

21
06:23:54 --> 06:23:56
As you can see this is a graph,

22
06:23:56 --> 06:24:01
and the data model is the same
although the format is still CSV.

1
12:46:29 --> 12:46:32
This is the second hands on exercise for
sensor data.

2
12:46:32 --> 12:46:35
In the first we looked at
static data in a text file.

3
12:46:35 --> 12:46:39
In this one we'll be looking at
real-time streaming measurements.

4
12:46:39 --> 12:46:41
First, we will open a terminal window, and

5
12:46:41 --> 12:46:44
cd into the directory containing
the data and the scripts.

6
12:46:44 --> 12:46:47
Next we'll connect to
the weather station and

7
12:46:47 --> 12:46:49
look at the real-time
data as it streams in.

8
12:46:49 --> 12:46:54
After that, we will look at the key to
remind ourselves what the fields mean.

9
12:46:54 --> 12:46:57
And finally, we will plot the data
streaming from the weather station.

10
12:46:58 --> 12:46:59
Let's begin.

11
12:46:59 --> 12:47:02
First, open a terminal window,
by clicking on the terminal icon.

12
12:47:02 --> 12:47:03
Top of the toolbar.

13
12:47:05 --> 12:47:11
[NOISE] Let's run cd
Downloads/big-data-2/sensor.

14
12:47:20 --> 12:47:23
You can run ls to see
the name of the scripts.

15
12:47:26 --> 12:47:31
Let's run stream-data.py
to see the real-time data.

16
12:47:39 --> 12:47:44
This shows us the real-time measurements
coming from the weather station.

17
12:47:44 --> 12:47:46
By looking at the time stamps,

18
12:47:46 --> 12:47:51
we can see that each measurement arrives
about one second after the previous one.

19
12:47:54 --> 12:47:57
Additionally, we can see
that R1 comes fairly often,

20
12:47:57 --> 12:48:01
whereas other measurements,
such as R2, are not as often.

21
12:48:06 --> 12:48:07
We can open another terminal and

22
12:48:07 --> 12:48:11
look at the key to remind ourselves
what these measurements mean.

23
12:48:22 --> 12:48:27
The key is in wxt-520-format.txt.

24
12:48:27 --> 12:48:32
We could run more
wxt-520-format.txt to view it.

25
12:48:43 --> 12:48:48
If we go back to our live data,
we can see that the 19th

26
12:48:48 --> 12:48:53
measurement here says Ta
was 22.5 degrees Celsius.

27
12:48:54 --> 12:48:59
And look up here,
see that Ta is the air temperature.

28
12:48:59 --> 12:49:03
The next measure we can see
that Dn was equal to 255D.

29
12:49:03 --> 12:49:08
According to our key,
Dn is the wind direction minimum, and

30
12:49:08 --> 12:49:10
the units are degrees.

31
12:49:15 --> 12:49:20
We can also plot specific measurements
streaming live from the weather station.

32
12:49:23 --> 12:49:24
Let's plot the wind speed average.

33
12:49:26 --> 12:49:31
If we look at our key,
we see that the wind speed average is Sm.

34
12:49:31 --> 12:49:37
So we can plot this by running
stream-plot-data.py sm.

35
12:49:49 --> 12:49:52
This plots the data as the weather
station sends it to us.

36
12:49:56 --> 12:49:57
If we look at the x-axis,

37
12:49:57 --> 12:50:00
we can see that one measurement
comes in about every second.

38
12:50:12 --> 12:50:15
We can plot other measurements by
choosing different fields from the key.

39
12:50:16 --> 12:50:25
For example, we can plot the air pressure
by running stream-plot-data.py Pa.

40
12:50:25 --> 12:50:27
Since Pa is the air pressure.

41
12:50:43 --> 12:50:47
First thing we notice is that there's
only one measurement so far in the graph.

42
12:50:49 --> 12:50:52
This means that the air pressure
measurements are not coming as

43
12:50:52 --> 12:50:54
fast as the wind measurements.

44
12:50:54 --> 12:50:56
In fact, we only got one.

1
01:37:25 --> 01:37:26
In this hands-on activity,

2
01:37:26 --> 01:37:30
we'll be looking at real time
data streaming from Twitter.

3
01:37:31 --> 01:37:33
First, we'll open a terminal window and

4
01:37:33 --> 01:37:37
cd into the directory containing
Python scripts to access this data.

5
01:37:38 --> 01:37:42
Next, we'll look at the contents
of tweets streaming from Twitter

6
01:37:42 --> 01:37:44
containing specific words.

7
01:37:45 --> 01:37:48
Finally, we will plot the frequency
of these streaming tweets.

8
01:37:49 --> 01:37:50
Let's begin.

9
01:37:50 --> 01:37:54
First, click on the terminal
icon at the top of the toolbar.

10
01:37:58 --> 01:38:03
Let cd into the directory containing
the python scripts to access the real time

11
01:38:03 --> 01:38:05
data from Twitter.

12
01:38:05 --> 01:38:10
We'll run CD downloads big-data-2 json.

13
01:38:17 --> 01:38:20
We can run ls to see
the files in this directory.

14
01:38:26 --> 01:38:32
The file auth should be created containing
your Twitter authentication information.

15
01:38:33 --> 01:38:36
You could see the separate reading for
how to setup the Twitter app and

16
01:38:36 --> 01:38:37
how to create this file.

17
01:38:41 --> 01:38:45
Next, let's run the script live tweets
to view tweets in real time containing

18
01:38:45 --> 01:38:47
a specific word.

19
01:38:47 --> 01:38:52
Let's run LiveTweets.py president.

20
01:38:58 --> 01:39:01
This will show the real time tweets
containing the word president.

21
01:39:07 --> 01:39:10
[INAUDIBLE] Runs, in the first column,
you can see the time stamp of the tweet.

22
01:39:10 --> 01:39:13
In the second column,
you can see the text.

23
01:39:19 --> 01:39:21
When you're done, hit Ctrl + C.

24
01:39:26 --> 01:39:29
Let's run LiveTweet again using
a different keyword that appear more

25
01:39:29 --> 01:39:31
frequently.

26
01:39:31 --> 01:39:35
Let's use the word time,
we'll run LiveTweet time

27
01:39:49 --> 01:39:55
When we're done, hit Ctrl + C.

28
01:39:55 --> 01:39:59
We can plot the frequency of these Tweets
by running the script plot Tweets.

29
01:40:02 --> 01:40:06
Let's run plot, tweet, president to see
the frequency for the word president.

30
01:40:22 --> 01:40:26
As this runs, we could see
the frequency changes over time.

31
01:40:44 --> 01:40:46
I can see that the maximum was one.

32
01:40:48 --> 01:40:50
And a few times,
there were just one tweet in that second.

33
01:40:54 --> 01:40:56
When you're done looking at the graph,

34
01:40:56 --> 01:40:57
click in the terminal window and
hit Enter.

35
01:41:00 --> 01:41:03
Now, let's plot the frequency for
the word time.

36
01:41:04 --> 01:41:07
Run PlotTweets.py time.

37
01:41:23 --> 01:41:27
This plot shows that the word time appears
a lot more frequently than president.

38
01:41:28 --> 01:41:34
We can see spikes in the frequency
of 40 and a maximum of around 65.

39
01:41:34 --> 01:41:37
When you're done looking at the graph,

40
01:41:37 --> 01:41:41
click on the terminal window and
press enter to quit.

1
03:19:07 --> 03:19:11
With big data streaming from different
sources in varying formats, models, and

2
03:19:11 --> 03:19:16
speeds it is no surprise that we
need to be able to ingest this data

3
03:19:16 --> 03:19:19
into a fast and scalable storage system

4
03:19:19 --> 03:19:24
that is flexible enough to serve many
current and future analytical processes.

5
03:19:26 --> 03:19:31
This is when traditional data warehouses
with strict data models and data

6
03:19:31 --> 03:19:37
formats don't fit the big data challenges
for streaming and batch applications.

7
03:19:38 --> 03:19:44
The concept of a data lake was created in
response of these data big storage and

8
03:19:44 --> 03:19:45
processing challenges.

9
03:19:47 --> 03:19:51
After this video you
will be able to describe

10
03:19:51 --> 03:19:54
how data lakes enable batch
processing of streaming data.

11
03:19:56 --> 03:20:01
Explain the difference between
schema on write and schema on read.

12
03:20:02 --> 03:20:06
Organize data streams and data lakes and

13
03:20:06 --> 03:20:10
data warehouses on a spectrum of
big data management and storage.

14
03:20:13 --> 03:20:14
What is a data lake?

15
03:20:15 --> 03:20:21
Simply speaking, a data lake is
a part of a big data infrastructure

16
03:20:21 --> 03:20:26
that many streams can flow into and

17
03:20:26 --> 03:20:29
get stored for
processing in their original form.

18
03:20:29 --> 03:20:34
We can think of it as a massive
storage depository with huge

19
03:20:34 --> 03:20:39
processing power and ability to handle
a very large number of concurrence,

20
03:20:39 --> 03:20:41
data management and analytical tasks.

21
03:20:43 --> 03:20:48
In 2010,
the Pentaho Corporation's CTO James Dixon

22
03:20:48 --> 03:20:50
defined a data link as follows.

23
03:20:51 --> 03:20:56
If you think of a datamart as a store
of bottled water, cleansed and

24
03:20:56 --> 03:21:00
packaged and structured for
easy consumption,

25
03:21:00 --> 03:21:05
the data lake is a large body of
water in a more natural state.

26
03:21:05 --> 03:21:10
The contents of the data lake stream
in from a source to fill the lake,

27
03:21:10 --> 03:21:17
and various users of the lake can come
to examine it, dive in, or take samples.

28
03:21:17 --> 03:21:20
A data lake works as follows.

29
03:21:21 --> 03:21:23
The data gets loaded from its source,

30
03:21:25 --> 03:21:28
stored in its native
format until it is needed

31
03:21:29 --> 03:21:34
at which time the applications can freely
read the data and add structure to it.

32
03:21:35 --> 03:21:39
This is what we call schema on read.

33
03:21:39 --> 03:21:44
In a traditional data warehouse,
the data is loaded into the warehouse

34
03:21:44 --> 03:21:48
after transforming it into a well
defined and structured format.

35
03:21:48 --> 03:21:51
This is what we call schema on write.

36
03:21:51 --> 03:21:57
Any application using the data needs to
know this format in order to retrieve and

37
03:21:57 --> 03:21:58
use the data.

38
03:21:59 --> 03:22:00
In this approach,

39
03:22:00 --> 03:22:04
data is not loaded into the warehouse
unless there is a use for it.

40
03:22:04 --> 03:22:10
However, schema on read
approach of data lakes ensures

41
03:22:10 --> 03:22:15
all data is stored for
a potentially unknown use at a later time.

42
03:22:15 --> 03:22:18
So how is a data lake
from a data warehouse?

43
03:22:19 --> 03:22:24
A traditional data warehouse
stores data in a hierarchical file

44
03:22:25 --> 03:22:28
system with a well-defined structure.

45
03:22:28 --> 03:22:35
However, a data lake stores data as
flat files with a unique identifier.

46
03:22:35 --> 03:22:39
This often gets referred to as
object storage in big data systems.

47
03:22:42 --> 03:22:48
In data lakes each data is stored
as a binary large object or

48
03:22:48 --> 03:22:51
BLOB and is assigned a unique identifier.

49
03:22:52 --> 03:22:59
In addition, each data object is
tagged with a number of metadata tags.

50
03:23:00 --> 03:23:04
The data can be searched
using these metadata tags

51
03:23:04 --> 03:23:06
to retrieve it when there
is a need to access it.

52
03:23:07 --> 03:23:09
From a users perspective,

53
03:23:09 --> 03:23:14
metadata is stored is not a problem as
long as it is accessible when needed.

54
03:23:14 --> 03:23:21
In Hadoop data architectures,
data is loaded into HDFS and processed

55
03:23:21 --> 03:23:26
using the appropriate data management and
analytical systems on commodity clusters.

56
03:23:27 --> 03:23:33
The selection of the tools is based on the
nature of the problem being solved, and

57
03:23:33 --> 03:23:34
the data format being accessed.

58
03:23:36 --> 03:23:38
We will talk more about
the processing of data streams and

59
03:23:38 --> 03:23:41
data lakes in the next course
in this specialization.

60
03:23:43 --> 03:23:49
To summarize a data lake is
a storage architecture for

61
03:23:49 --> 03:23:51
big data collection and processing.

62
03:23:52 --> 03:23:56
It enables collection of
all data suitable for

63
03:23:56 --> 03:23:59
analysis today and
potentially in the future.

64
03:24:00 --> 03:24:03
Regardless of the data source,
structure, and

65
03:24:03 --> 03:24:09
format it supports storage of data and
transforms it only when it is needed.

66
03:24:10 --> 03:24:15
A data lake ideally supports all parts
of the user base to benefit from

67
03:24:15 --> 03:24:21
this architecture, including business,
storage, analytics and computing experts.

68
03:24:23 --> 03:24:27
Finally, And perhaps most importantly,

69
03:24:27 --> 03:24:31
data lakes are infrastructure components
within a big data architecture

70
03:24:31 --> 03:24:35
that can evolve over time based
on application-specific needs.

1
06:43:42 --> 06:43:45
What is a data stream?

2
06:43:45 --> 06:43:46
After this video,

3
06:43:46 --> 06:43:52
you will be able to summarize the key
characteristics of a data stream.

4
06:43:52 --> 06:43:57
Identify the requirements of
streaming data systems, and

5
06:43:57 --> 06:44:01
recognize the data streams
you use in your life.

6
06:44:01 --> 06:44:04
When we talked about how
big data is generated and

7
06:44:04 --> 06:44:07
the characteristics of the big
data using sound waves.

8
06:44:07 --> 06:44:13
One of the challenges we mentioned was the
velocity of data coming in varying rates.

9
06:44:15 --> 06:44:20
For some applications this
presents the need to process data

10
06:44:20 --> 06:44:24
as it is generated, or
in other words, as it streams.

11
06:44:25 --> 06:44:29
We call these types of applications
Streaming Data Processing Applications.

12
06:44:30 --> 06:44:36
This terminology refers to a constant
stream of data flowing from a source,

13
06:44:36 --> 06:44:41
for example data from a sensory machine or
data from social media.

14
06:44:43 --> 06:44:48
An example application would be making
data-driven marketing decisions in

15
06:44:48 --> 06:44:49
real time.

16
06:44:49 --> 06:44:53
Through the use of data from
real-time sales trends,

17
06:44:53 --> 06:44:56
social media analysis,
and sales distributions.

18
06:44:58 --> 06:45:04
Another example for streaming data
processing is monitoring of industrial or

19
06:45:04 --> 06:45:06
farming machinery in real time.

20
06:45:06 --> 06:45:11
For monitoring and
detection of potential system failures.

21
06:45:11 --> 06:45:13
In fact, any sensor network or

22
06:45:13 --> 06:45:19
internet of things environment
controlled by another entity,

23
06:45:19 --> 06:45:23
or set of entities falls
under this category.

24
06:45:23 --> 06:45:27
For example,
as you have seen in an earlier video,

25
06:45:27 --> 06:45:29
FlightStats is an application.

26
06:45:29 --> 06:45:33
That processes about 60
million weekly flight events

27
06:45:33 --> 06:45:37
that come into their
data acquisition system.

28
06:45:37 --> 06:45:41
And turns it into real-time
intelligence for airlines and

29
06:45:41 --> 06:45:45
millions of travelers
around the world daily.

30
06:45:48 --> 06:45:51
So, how then do we define a data stream?

31
06:45:53 --> 06:45:58
A stream is defined as
a possibly unbounded sequence of

32
06:45:58 --> 06:46:00
data items or records.

33
06:46:00 --> 06:46:06
That may or may not be related to,
or correlated with each other.

34
06:46:06 --> 06:46:11
Each data is generally timestamped and

35
06:46:11 --> 06:46:15
in some cases geo-tagged.

36
06:46:15 --> 06:46:20
As you have seen in our examples,
the data can stream from many sources.

37
06:46:20 --> 06:46:25
Including instruments, and
many internet of things application areas,

38
06:46:25 --> 06:46:29
computer programs, websites,
or social media posts.

39
06:46:30 --> 06:46:35
Streaming data sometimes get
referred to as event data as

40
06:46:35 --> 06:46:39
each data item is treated as an individual
event in a synchronized sequence.

41
06:46:42 --> 06:46:45
Streams pose very difficult challenges for

42
06:46:45 --> 06:46:47
conventional data
management architectures.

43
06:46:47 --> 06:46:54
Which are built primarily on the concept
of persistence, static data collections.

44
06:46:54 --> 06:46:59
Due to the fact that most often we
have only one chance to look at and

45
06:46:59 --> 06:47:02
process streaming data
before more gets piled on.

46
06:47:02 --> 06:47:07
Streaming data management systems cannot
be separated from real-time processing

47
06:47:07 --> 06:47:08
of data.

48
06:47:10 --> 06:47:11
Managing and

49
06:47:11 --> 06:47:16
processing data in motion is a typical
capability of streaming data systems.

50
06:47:17 --> 06:47:19
However, the sheer size, variety and

51
06:47:19 --> 06:47:24
velocity of big data adds further
challenges to these systems.

52
06:47:24 --> 06:47:30
Such systems are designed to manage
relatively simple computations.

53
06:47:30 --> 06:47:33
Such as one record at a time or

54
06:47:33 --> 06:47:38
a set of objects in a short time
window of the most recent data.

55
06:47:39 --> 06:47:43
The computations are done
in near-real-time,

56
06:47:43 --> 06:47:47
sometimes in memory, and
as independent computations.

57
06:47:49 --> 06:47:55
The processing components
often subscribe to a system,

58
06:47:55 --> 06:47:59
or a stream source, non-interactively.

59
06:48:00 --> 06:48:05
This means they sent nothing
back to the source, nor

60
06:48:05 --> 06:48:07
did they establish
interaction with the source.

61
06:48:10 --> 06:48:16
The concept of dynamic steering involves
dynamically changing the next steps or

62
06:48:16 --> 06:48:19
direction of an application
through a continuous

63
06:48:19 --> 06:48:22
computational process using streaming.

64
06:48:23 --> 06:48:29
Dynamic steering is often a part of
streaming data management and processing.

65
06:48:29 --> 06:48:36
A self-driving car is a perfect example
of a dynamic steering application.

66
06:48:37 --> 06:48:41
But all streaming data applications
fall into this category.

67
06:48:41 --> 06:48:45
Such as the online gaming example we
discussed earlier in this course.

68
06:48:45 --> 06:48:49
Amazon Kinesis an other open-source Apache

69
06:48:49 --> 06:48:54
projects like Storm, Flink,
Spark Streaming, and

70
06:48:54 --> 06:48:59
Samza are examples of big
data streaming systems.

71
06:49:01 --> 06:49:04
Many other companies also
provide streaming systems for

72
06:49:04 --> 06:49:07
big data that are frequently
updated in response

73
06:49:07 --> 06:49:11
to the rapidly changing
nature of these technologies.

74
06:49:13 --> 06:49:19
As a summary, dynamic near-real-time
streaming data management,

75
06:49:19 --> 06:49:24
processing, and steering is an important
part of today's big data applications.

76
06:49:25 --> 06:49:28
Next, we will look at some
of the challenges for

77
06:49:28 --> 06:49:30
streaming data management and processing.

1
13:33:12 --> 13:33:15
Now that we have seen what
streaming data means,

2
13:33:15 --> 13:33:19
let’s look at what makes streaming data
different and what some management and

3
13:33:19 --> 13:33:22
processing challenges for
streaming data are.

4
13:33:23 --> 13:33:27
After this video you will
be able to compare and

5
13:33:27 --> 13:33:29
contrast data in motion and data at rest.

6
13:33:31 --> 13:33:34
Differentiate between streaming and
batch data processing.

7
13:33:36 --> 13:33:42
And list management and
processing challenges for streaming data.

8
13:33:42 --> 13:33:44
We often hear the terms data addressed and

9
13:33:44 --> 13:33:47
data in motion,
when talking about big data management.

10
13:33:47 --> 13:33:54
Data-at-rest refers to mostly
static data collected from one or

11
13:33:54 --> 13:34:00
more data sources, and the analysis
happens after the data is collected.

12
13:34:00 --> 13:34:05
The term data-in-motion refers to a mode

13
13:34:05 --> 13:34:09
although similar data
collection methods apply,

14
13:34:09 --> 13:34:13
the data gets analyzed at the same
time it is being generated.

15
13:34:14 --> 13:34:20
Just like the sensor data processing
in a plane or a self-driving car.

16
13:34:20 --> 13:34:25
Analysis of data addressed is called
batch or static processing and

17
13:34:25 --> 13:34:29
the analysis of streaming data
is called stream processing.

18
13:34:30 --> 13:34:35
The run time and memory usage of most
algorithms that process static data,

19
13:34:35 --> 13:34:39
is usually dependent on the data size, and

20
13:34:39 --> 13:34:44
this size can easily be calculated
from files or databases.

21
13:34:45 --> 13:34:52
A key property, of streaming data
processing is the size of the data

22
13:34:52 --> 13:34:57
is unbounded and this changes the types
of algorithms that can be used.

23
13:34:59 --> 13:35:03
Algorithms that require iterating or
looping over the whole data set are not

24
13:35:03 --> 13:35:08
possible since with stream data,
you never get to the end.

25
13:35:09 --> 13:35:15
The modeling and management of streaming
data should enable computations on

26
13:35:15 --> 13:35:20
one data element or a small window
of group of recent data elements.

27
13:35:21 --> 13:35:25
These computations can update metrics,
monitor and

28
13:35:25 --> 13:35:29
plot statistics on the streaming data.

29
13:35:29 --> 13:35:33
Or apply analysis techniques
to the streaming data

30
13:35:33 --> 13:35:37
to learn about the dynamics of
the system as a time series.

31
13:35:38 --> 13:35:41
Since computations need to
be completed in real time,

32
13:35:41 --> 13:35:46
the analysis tasks processing
streaming data should be quicker or

33
13:35:46 --> 13:35:50
not much longer than
the streaming rate of the data.

34
13:35:50 --> 13:35:52
Which we define by it's velocity.

35
13:35:53 --> 13:35:56
In most streaming systems,
the management, and

36
13:35:56 --> 13:36:02
processing system subscribe to
the data source, but doesn't

37
13:36:02 --> 13:36:06
send anything back to the stream source
in terms of feedback or interactions.

38
13:36:08 --> 13:36:12
These requirements for streaming data
processing are quite different than batch

39
13:36:12 --> 13:36:17
processing where the analytical steps
have access to often, all data and

40
13:36:17 --> 13:36:22
can take more time to complete a complex
analytical task with less pressure

41
13:36:22 --> 13:36:27
on the completion time of individual
data management and processing tasks.

42
13:36:29 --> 13:36:32
Most organizations today
use a hybrid architecture.

43
13:36:32 --> 13:36:37
Sometimes get referred to as
the lambda architecture for

44
13:36:37 --> 13:36:42
processing streaming and
back jobs at the same time.

45
13:36:42 --> 13:36:49
In these systems, streaming wheel over
the real-time data is managed and

46
13:36:49 --> 13:36:54
kept until those data elements are pushed

47
13:36:54 --> 13:36:59
to a batch system and become available
to access and process as batch data.

48
13:37:00 --> 13:37:05
In such systems, a stream storage
layer is used to enable fast

49
13:37:05 --> 13:37:10
trees of streams and
ensure data ordering and consistency.

50
13:37:10 --> 13:37:12
And a processing layer for

51
13:37:12 --> 13:37:17
data is used to retrieve data from
the storage layer to analyze it and

52
13:37:17 --> 13:37:22
most probably little bit to a batch
data stream and notify the streaming

53
13:37:22 --> 13:37:27
storage that the data set does no
longer need to be in streaming storage.

54
13:37:27 --> 13:37:33
The big data challenges we discussed
were scalability, data replication,

55
13:37:33 --> 13:37:38
and durability, and fault tolerance arise
in this type of data very significantly.

56
13:37:39 --> 13:37:45
Among many there are two main
challenges that needs to be overcome

57
13:37:45 --> 13:37:50
to avoid data loss, and
enable real time analytical tasks.

58
13:37:50 --> 13:37:55
One challenge in streaming data
process is that the size and

59
13:37:55 --> 13:38:00
frequency of the mean data can
significantly change over time.

60
13:38:01 --> 13:38:06
These changes can be unpredictable and
may be driven by human behavior.

61
13:38:08 --> 13:38:13
For example, streaming data found on
social networks such as Facebook and

62
13:38:13 --> 13:38:16
Twitter can increase in
volume during holidays,

63
13:38:16 --> 13:38:19
sports matches, or major news events.

64
13:38:20 --> 13:38:26
These changes can be periodic and occur,
for example, in the evenings or weekends.

65
13:38:28 --> 13:38:33
For example, people may post messages
on Facebook more in the evening

66
13:38:33 --> 13:38:36
instead of during the day working hours.

67
13:38:36 --> 13:38:41
Streaming data changes may also
be unpredictable and sporadic.

68
13:38:41 --> 13:38:44
There can be an increase in data size and

69
13:38:44 --> 13:38:49
frequency during during major events,
sporting matches and things like that.

70
13:38:49 --> 13:38:55
Other changes include dropping or
missing data or even no data

71
13:38:55 --> 13:39:01
when there are network problems or device
generating the data has hardware problems.

72
13:39:01 --> 13:39:03
As an example of streaming
data fluctuation,

73
13:39:03 --> 13:39:05
consider the number of Tweets per second.

74
13:39:05 --> 13:39:12
On average,
there are 6,000 tweets sent every second.

75
13:39:12 --> 13:39:16
However, in August 2013,
the world record was

76
13:39:16 --> 13:39:21
set when over 144,000 tweets
were sent in a second.

77
13:39:21 --> 13:39:24
That's a factor of 24 increase.

78
13:39:26 --> 13:39:31
At the end of this lesson we will ask
you to focus on Twitter streams for

79
13:39:31 --> 13:39:34
trending topics and any other topic.

80
13:39:35 --> 13:39:38
You will notice how the rates
of Tweets streaming

81
13:39:38 --> 13:39:42
changes between different times and
different topics.

82
13:39:42 --> 13:39:49
To summarize, streaming data must be
handled differently than static data.

83
13:39:49 --> 13:39:54
Unlike static data, where you can
determine the size, streaming data is

84
13:39:54 --> 13:39:59
continually generated, and
you can not process it all at once.

85
13:40:01 --> 13:40:07
Streaming data can unpredictably
change in both size and frequency.

86
13:40:07 --> 13:40:09
This can be due to human behavior.

87
13:40:10 --> 13:40:12
Finally, algorithms for

88
13:40:12 --> 13:40:17
processing streaming data must
be relatively fast and simple.

89
13:40:17 --> 13:40:20
Since you don't know when
the next data arrives.

1
03:13:33 --> 03:13:34
In a previous lecture,

2
03:13:34 --> 03:13:39
we said that new interesting solutions are
emerging in the big data product space.

3
03:13:40 --> 03:13:44
While these solutions do not have
the full fledged power of a DBMS,

4
03:13:44 --> 03:13:49
they offer novel feature combinations that
suit some application space just right.

5
03:13:50 --> 03:13:54
One of these products is Aerospike,

6
03:13:54 --> 03:13:59
which calls itself a distributed
NoSQL database and

7
03:13:59 --> 03:14:03
key value store, and
goes on to say that it is architected for

8
03:14:03 --> 03:14:07
the performance needs of
today's web scale applications.

9
03:14:07 --> 03:14:11
The diagram here is from
an Aerospike whitepaper.

10
03:14:12 --> 03:14:16
It shows how Aerospike relates to
the ecosystem for which it is designed.

11
03:14:18 --> 03:14:23
The top layer shows several applications
for real time consumer facing systems,

12
03:14:23 --> 03:14:28
such as travel recommendation systems,
pricing engines used for

13
03:14:28 --> 03:14:31
stock market applications,
real time decision systems that

14
03:14:31 --> 03:14:35
analyze data to figure out whether
an investment should be done and so forth.

15
03:14:36 --> 03:14:39
Now, all of these systems
have the common need

16
03:14:39 --> 03:14:43
that large amounts of data should be
accessible to them at any point of time.

17
03:14:45 --> 03:14:49
The Aerospike system can interoperate
with Hadoop-based systems, so

18
03:14:49 --> 03:14:54
Spark, or a Legacy database, or
even a real time data source.

19
03:14:54 --> 03:14:58
It can exchange large volumes
of data with any such source and

20
03:14:58 --> 03:15:03
serve fast lookups and
queries to the applications above.

21
03:15:03 --> 03:15:07
Now that translates to a very
high availability robust and

22
03:15:07 --> 03:15:09
strong consistency needs.

23
03:15:11 --> 03:15:17
The figure here presents a high level
architecture diagram of Aerospike.

24
03:15:17 --> 03:15:22
The first item to notice here
is what they call a fast bat,

25
03:15:23 --> 03:15:26
which essentially refers to
the left side of the architecture.

26
03:15:28 --> 03:15:31
The client system processes transactions.

27
03:15:31 --> 03:15:36
That is data that are primarily managed in
a primary index that is a key value store.

28
03:15:38 --> 03:15:42
This index stays in memory for
operational purposes.

29
03:15:42 --> 03:15:47
However, the server also interacts with
the storage layer for persistence.

30
03:15:48 --> 03:15:52
The storage layer uses three
kinds of storage systems,

31
03:15:53 --> 03:15:58
in memory with a dynamic RAM or
DRAM, a regular spinning disk,

32
03:16:00 --> 03:16:05
and flash/SSD, which is solid state device
for fast loading of data when needed.

33
03:16:06 --> 03:16:09
In fact the Aerospike system

34
03:16:09 --> 03:16:13
has optimized its performance with
characteristics of an SSD in mind.

35
03:16:15 --> 03:16:19
For those of who are not sure
what an SSD is, you can think of

36
03:16:19 --> 03:16:24
an SSD as a kind of storage device
whose random read performance is much

37
03:16:24 --> 03:16:28
faster than speeding hard disk and the
write performance is just a little slower.

38
03:16:30 --> 03:16:35
One vendor recently advertised its SSD
has sequence share read speeds of up to

39
03:16:35 --> 03:16:40
2,500 MBPS and the sequential write
speeds as fast as 1,500 MBPS.

40
03:16:43 --> 03:16:45
Now why is this important
in a big data discussion?

41
03:16:47 --> 03:16:52
When we speak of scalability, grade
efficiency, fast transactions, and so

42
03:16:52 --> 03:16:56
forth, we often do not mention
that part of the performance

43
03:16:56 --> 03:17:00
guarantee is governed by the combination
of hardware and software.

44
03:17:02 --> 03:17:07
So the ability to offer more efficient
persistent storage with fast IO

45
03:17:07 --> 03:17:12
implies that while a significant amount
of information can be stored on disk,

46
03:17:12 --> 03:17:16
it can be done without compromising
the overall system performance for

47
03:17:16 --> 03:17:19
an environment that
needs fast data loading.

48
03:17:22 --> 03:17:26
The second point of uniqueness
is a secondary index there.

49
03:17:27 --> 03:17:32
Aerospike built secondary index
fields that are non-primary keys.

50
03:17:32 --> 03:17:36
A non-primary key is a key attribute
that makes a tuple unique,

51
03:17:36 --> 03:17:38
but it has not been
chosen as a primary key.

52
03:17:39 --> 03:17:43
In Aerospike, secondary indices
are stored in main memory.

53
03:17:43 --> 03:17:48
They are built on every node in a cluster
and co-located with the primary index.

54
03:17:49 --> 03:17:54
Each secondary index entry
contains references to records,

55
03:17:54 --> 03:17:56
which are local to the node.

56
03:17:58 --> 03:18:03
As a key value store,
Aerospike uses standard database like

57
03:18:03 --> 03:18:10
scalar types like integer, string, and
so forth, as well as lists like Reddis.

58
03:18:10 --> 03:18:17
The map type is similar to the hashtag of
Reddis and contains attribute value pairs.

59
03:18:17 --> 03:18:23
Since it is focused on real time web
application, it supports geospatial data,

60
03:18:23 --> 03:18:27
like places with latitude and
longitude values or regency polygons.

61
03:18:30 --> 03:18:35
This allows them to perform KV store
operations, for example, like is

62
03:18:35 --> 03:18:40
the location of this in La Jolla,
which is a point-in-polygon query.

63
03:18:40 --> 03:18:45
Or a distance query, like find hotels
within three miles of my location.

64
03:18:46 --> 03:18:49
KV queries are constructed
programmatically.

65
03:18:50 --> 03:18:57
Now interestingly, Aerospike also provides
a more declarative language called AQL.

66
03:18:57 --> 03:19:02
AQL looks very similar to SQL,

67
03:19:02 --> 03:19:05
the Standard Query Language for
relational databases.

68
03:19:06 --> 03:19:08
A query like select name and

69
03:19:08 --> 03:19:14
age from user star profiles projects
out the name and age values

70
03:19:14 --> 03:19:19
from the profile's record set that
belongs to the ding space called users.

71
03:19:20 --> 03:19:24
The language also allows
advocate functions, like sum and

72
03:19:24 --> 03:19:27
average, and other user defined functions,

73
03:19:27 --> 03:19:31
which the system may evaluate through
a map produced time operation.

74
03:19:34 --> 03:19:37
We mentioned earlier that while most
medium assists today offer base

75
03:19:37 --> 03:19:42
guarantees, Aerospike, despite being
a distributor information system,

76
03:19:42 --> 03:19:44
actually offers ACID guarantees.

77
03:19:45 --> 03:19:48
This is accomplished using
a number of techniques.

78
03:19:50 --> 03:19:54
We'll consider a few of them to give you
a flavor of the mechanisms that current

79
03:19:54 --> 03:19:59
systems use to balance between
large scale data management and

80
03:19:59 --> 03:20:03
transaction management in a cluster
where nodes can join or leave.

81
03:20:04 --> 03:20:08
You may recall that consistency
means two different things,

82
03:20:09 --> 03:20:13
one is to ensure that all constraints,
like domain constraints, are satisfied.

83
03:20:14 --> 03:20:17
The second meaning is applied
to distributor systems and

84
03:20:17 --> 03:20:22
ensures all copies of a data
item in a cluster are in sync.

85
03:20:23 --> 03:20:27
For operations and
single keys with replication and

86
03:20:27 --> 03:20:32
secondary indices,
Aerospike provides immediate consistency

87
03:20:32 --> 03:20:35
using synchronous writes to
replicas within the cluster.

88
03:20:36 --> 03:20:41
Synchronous write means the write
process will be considered

89
03:20:41 --> 03:20:44
successful only if
the replica is all subdated.

90
03:20:45 --> 03:20:49
No other write is allowed on the record
while the object of replica is pending.

91
03:20:51 --> 03:20:55
So what happens if there is an increase
in the number of write operations due to

92
03:20:55 --> 03:20:57
an increase in ingestion rate?

93
03:20:58 --> 03:21:04
In Aerospike, it is possible to relax
this immediate consistency condition

94
03:21:04 --> 03:21:08
by bypassing some of
the consistency checks.

95
03:21:08 --> 03:21:13
But if this is done, eventual
consistency will still be enforced.

96
03:21:16 --> 03:21:21
Durability is achieved by storing data
in the flash SSD on every node and

97
03:21:21 --> 03:21:23
performing direct reads from the flash.

98
03:21:25 --> 03:21:28
Now durability is also maintained through
the process of replication because we have

99
03:21:28 --> 03:21:30
multiple copies of data.

100
03:21:30 --> 03:21:36
So even if one node fails, the latest copy
of the last data is available from one or

101
03:21:36 --> 03:21:39
more replica nodes in the same cluster,

102
03:21:39 --> 03:21:42
as well as in nodes residing
in remote clusters.

103
03:21:44 --> 03:21:47
But does that just
contradict the CAP theorem?

104
03:21:48 --> 03:21:53
The CAP theorem holds when
the network is partitioned.

105
03:21:55 --> 03:21:58
That means when nodes in
different parts of the network

106
03:21:58 --> 03:22:00
have different data content.

107
03:22:01 --> 03:22:06
Aerospike reduces and tries to completely
eliminate the situation by making

108
03:22:06 --> 03:22:12
sure that the master knows exactly
where all the other nodes are.

109
03:22:12 --> 03:22:15
And the replication is
happening properly even when

110
03:22:15 --> 03:22:17
the new nodes are joining the network.

1
06:35:50 --> 06:35:55
In the previous modules, we talked
about data variety and streaming data.

2
06:35:56 --> 06:36:01
In this module, we'll focus on a central
issue in large scale data processing and

3
06:36:01 --> 06:36:07
management and that is when should
we use Hadoop or Yarn style system?

4
06:36:07 --> 06:36:12
And when should we use a database system
that can perform parallel operations?

5
06:36:12 --> 06:36:16
And then we'll explore how the state
of the art big data management systems

6
06:36:16 --> 06:36:19
address these issues of volume and
variety.

7
06:36:20 --> 06:36:24
We start with the problem
of high volume data and

8
06:36:24 --> 06:36:26
two contrasting approaches for
handling them.

9
06:36:28 --> 06:36:30
So after this video, you'll be able to

10
06:36:31 --> 06:36:35
explain the various advantages of
using a DBMS over a file system.

11
06:36:36 --> 06:36:40
Specify the differences between
parallel and distributed DBMS.

12
06:36:40 --> 06:36:45
And briefly describe
a MapReduce-style DBMS and

13
06:36:45 --> 06:36:49
its relationship with the current DBMSs.

14
06:36:49 --> 06:36:54
In the early days,
when database systems weren't around or

15
06:36:54 --> 06:36:59
just came in, databases were designed
as a set of application programs.

16
06:37:00 --> 06:37:05
They were written to handle data that
resided in files in a file system.

17
06:37:06 --> 06:37:09
However, soon this
approach led to problems.

18
06:37:10 --> 06:37:14
First, there are multiple file formats.

19
06:37:14 --> 06:37:20
And often, there was a duplication
of information in different files.

20
06:37:20 --> 06:37:24
Or the files simply had inconsistent
information that was very hard to

21
06:37:24 --> 06:37:29
determine, especially when the data was
large and the file content was complex.

22
06:37:31 --> 06:37:35
Secondly, there wasn't
a uniform way to access data.

23
06:37:36 --> 06:37:40
Each data access task, like finding
employees in a department sorted by their

24
06:37:40 --> 06:37:43
salary versus finding
employees in all departments

25
06:37:43 --> 06:37:48
sorted by their start date needed to
be written as a separate program.

26
06:37:49 --> 06:37:52
So people ended up writing
different programs for

27
06:37:52 --> 06:37:54
data access, as well as data update.

28
06:37:56 --> 06:37:59
A third problem was rooted to
the enforcement of constraints,

29
06:37:59 --> 06:38:02
often called integrity constraints.

30
06:38:02 --> 06:38:08
For example, to say something like every
employee has exactly one job title.

31
06:38:08 --> 06:38:11
One had arrived that condition,
as part of an application program called.

32
06:38:12 --> 06:38:15
So if you want to change the constraint,
you need to look for

33
06:38:15 --> 06:38:17
the programs where such
a rule is hard coded.

34
06:38:19 --> 06:38:23
The fourth problem has to
do with system failures.

35
06:38:23 --> 06:38:27
Supposed Joe, an employee becomes
the leader of a group and moves in to

36
06:38:27 --> 06:38:32
the office of the old leader, Ben who has
now become the director of the division.

37
06:38:32 --> 06:38:37
So we update Joe's details and
move on to update, Ben's new office for

38
06:38:37 --> 06:38:39
the system crashes.

39
06:38:39 --> 06:38:42
So the files are incompletely updated and

40
06:38:42 --> 06:38:46
there is no way to go back,
and start all over.

41
06:38:46 --> 06:38:52
The term atomicity means that all of
the changes that we need to do for

42
06:38:52 --> 06:38:57
these promotions must happen altogether,
as a single unit.

43
06:38:57 --> 06:39:02
They should either fully go through or
not go through at all.

44
06:39:02 --> 06:39:09
This atomicity is very difficult to handle
when the data reside in one or more files.

45
06:39:11 --> 06:39:17
So, a prime reason for the transition
to a DBMS is to alleviate these and

46
06:39:17 --> 06:39:18
other difficulties.

47
06:39:18 --> 06:39:22
If we look at the current DBMS,
especially relational DBMS,

48
06:39:22 --> 06:39:24
we will notice a number of advantages.

49
06:39:27 --> 06:39:30
DBMSs offer query languages,
which are declarative.

50
06:39:32 --> 06:39:36
Declarative means that we
state what we want to retrieve

51
06:39:36 --> 06:39:39
without telling the DBMS
how exactly to retrieve it.

52
06:39:40 --> 06:39:44
In a relational DBMS, we can say,
find the average set of salary of

53
06:39:44 --> 06:39:49
employees in the R&D division for
every job title and sort from high to low.

54
06:39:49 --> 06:39:53
We don't have to tell the system how
to group these records by job title or

55
06:39:53 --> 06:39:55
how to extract just the salary field.

56
06:39:58 --> 06:40:02
A typical user of a DBMS who issues
queries does not worry about how

57
06:40:02 --> 06:40:07
the relations are structured or whether
they are located in the same machine,

58
06:40:07 --> 06:40:09
or spread across five machines.

59
06:40:09 --> 06:40:13
The goal of data independence is to
isolate the users from the record

60
06:40:13 --> 06:40:16
layout so long as the logical
definition of the data,

61
06:40:16 --> 06:40:21
which means the tables and
their attributes are clearly specified.

62
06:40:23 --> 06:40:27
Now most importantly, relational DBMSs
have developed a very mature and

63
06:40:27 --> 06:40:32
continually improving methodology of
how to answer a query efficiently,

64
06:40:32 --> 06:40:35
even when there are a large
number of cables and

65
06:40:35 --> 06:40:38
the number of records
exceeds hundreds of millions.

66
06:40:39 --> 06:40:44
From a 2009 account,
EB uses the tera data system with

67
06:40:44 --> 06:40:50
72 machines to manage approximately
2.4 terabytes of relational data.

68
06:40:51 --> 06:40:56
These systems have built powerful
data structures, algorithms and

69
06:40:56 --> 06:41:00
sound principles to determine how
a specific array should be onset

70
06:41:00 --> 06:41:06
efficiently despite the size of the data
and the complexity of the tables.

71
06:41:06 --> 06:41:10
Now with any system,
bad things can happen.

72
06:41:10 --> 06:41:13
Systems fail in the middle
of an operation.

73
06:41:13 --> 06:41:17
Malicious processes try to get
unauthorized access to data.

74
06:41:17 --> 06:41:21
One large can often underappreciated
aspect of a DBMS is

75
06:41:21 --> 06:41:27
the implementation of transaction
safety and failure recovery.

76
06:41:27 --> 06:41:28
Now, recall our discussion of atomicity.

77
06:41:30 --> 06:41:35
In databases, a single logical operation
on the data is called a transaction.

78
06:41:35 --> 06:41:39
For example, a transfer of funds
from one bank account to another,

79
06:41:39 --> 06:41:43
even involving multiple changes
like debiting one account and

80
06:41:43 --> 06:41:46
crediting another is a single transaction.

81
06:41:46 --> 06:41:50
Now, atomicity is one of the four
properties that a transaction should

82
06:41:50 --> 06:41:52
provide.

83
06:41:52 --> 06:41:57
The four properties,
collectively called ACID are atomicity,

84
06:41:57 --> 06:42:01
consistency, isolation and durability.

85
06:42:02 --> 06:42:07
Consistency means any data
written to the database must be

86
06:42:07 --> 06:42:12
valid according to all defined
rules including constrains.

87
06:42:12 --> 06:42:17
The durability property ensures that
once a transaction has been committed,

88
06:42:17 --> 06:42:22
it will remain so, even in the event
of power loss, crashes or errors.

89
06:42:24 --> 06:42:28
The isolation property comes
in the context of concurrency,

90
06:42:28 --> 06:42:32
which refers to multiple people
updating a database simultaneously.

91
06:42:33 --> 06:42:38
To understand concurrency, think of
an airline or a railway reservation system

92
06:42:38 --> 06:42:42
where hundreds and thousands of
people are buying, cancelling and

93
06:42:42 --> 06:42:45
changing their reservations and
tickets all at the same time.

94
06:42:46 --> 06:42:50
The DBMS must be sure that
a ticket should no be sold twice.

95
06:42:50 --> 06:42:54
Or if one person is in the middle
of buying the last ticket,

96
06:42:54 --> 06:42:57
another person does not see
that ticket as available.

97
06:42:59 --> 06:43:03
These are guaranteed by
the isolation property that says,

98
06:43:03 --> 06:43:07
not withstanding the number of people
accessing the system at the same time.

99
06:43:07 --> 06:43:14
The transactions must happen as if they're
done serially, that is one after another.

100
06:43:14 --> 06:43:21
Providing these capabilities is
an important part of the M in DBMS.

101
06:43:21 --> 06:43:26
So next, we consider how traditional
databases handle large data volumes.

102
06:43:27 --> 06:43:32
The classical way in which DBMSs
have handled the issue of large

103
06:43:32 --> 06:43:36
volumes is by created parallel and
distributed databases.

104
06:43:36 --> 06:43:40
In a parallel database, for
example, parallel Oracle,

105
06:43:40 --> 06:43:42
parallel DB2 or post SQL XE.

106
06:43:42 --> 06:43:47
The tables are spread across multiple
machines and operations like selection,

107
06:43:47 --> 06:43:51
and join use parallel algorithms
to be more efficient.

108
06:43:51 --> 06:43:55
These systems also allow a user
to create a replication.

109
06:43:55 --> 06:43:58
That is multiple copies of tables.

110
06:43:58 --> 06:44:01
Thus, introducing data redundancy, so

111
06:44:01 --> 06:44:06
that failure on replica can be
compensated for by using another.

112
06:44:07 --> 06:44:11
Further, it replicates in
sync with each other and

113
06:44:11 --> 06:44:15
a query can result into
any of the replicates.

114
06:44:15 --> 06:44:20
This increases the number of simultaneous
that is conquer into queries

115
06:44:20 --> 06:44:22
that can be handled by the system.

116
06:44:22 --> 06:44:27
In contrast, a distributed DBMS, which
we'll not discuss in detail in this course

117
06:44:27 --> 06:44:34
is a network of independently running
DBMSs that communicate with each other.

118
06:44:34 --> 06:44:39
In this case, one component knows some
part of the schema of it is neighboring

119
06:44:39 --> 06:44:43
DBMS and can pass a query or part of
a query to the neighbor when needed.

120
06:44:45 --> 06:44:49
So the important takeaway issue here is,

121
06:44:49 --> 06:44:52
are all of these facilities
offered by a DBMS important for

122
06:44:52 --> 06:44:55
the big data application that
you are planning to build?

123
06:44:56 --> 06:44:59
And the answer in many
cases can be negative.

124
06:44:59 --> 06:45:04
However, if these issues are important,
then the database management

125
06:45:04 --> 06:45:08
systems may offer a viable option for
a big data application.

126
06:45:11 --> 06:45:15
Now, let's take a little more time to
address an issue that's often discussed in

127
06:45:15 --> 06:45:16
the big data word.

128
06:45:16 --> 06:45:20
The question is if DBMSs are so powerful,

129
06:45:20 --> 06:45:25
why do we see the emergence
of MapReduce-style Systems?

130
06:45:25 --> 06:45:29
Unfortunately, the answer to this
question is not straightforward.

131
06:45:31 --> 06:45:37
For a long while now, DBMSs have
effectively used parallelism, specifically

132
06:45:37 --> 06:45:42
parallel databases in addition to
replication would also create partitions.

133
06:45:43 --> 06:45:48
So that different parts of a logical
table can physically reside on different

134
06:45:48 --> 06:45:53
machines,, then different parts of a query
can access the partitions in parallel and

135
06:45:53 --> 06:45:55
speed up creative performance.

136
06:45:55 --> 06:45:59
Now these algorithms not only improve
the operating efficiency, but

137
06:45:59 --> 06:46:04
simultaneously optimize algorithms to
take into account the communication cost.

138
06:46:04 --> 06:46:09
That is the time needed to
exchange data between machines.

139
06:46:09 --> 06:46:16
However, classical parallel DBMSs did
not take into account machine failure.

140
06:46:17 --> 06:46:23
And in contrast, MapReduce was
originally developed not for storage and

141
06:46:23 --> 06:46:26
retrieval, but for distributive
processing of large amounts of data.

142
06:46:27 --> 06:46:32
Specifically, its goal was to support
complex custom computations that could

143
06:46:32 --> 06:46:35
be performed efficiently on many machines.

144
06:46:35 --> 06:46:38
So in a MapReduce or MR setting,

145
06:46:38 --> 06:46:42
the number of machines
could go up to thousands.

146
06:46:42 --> 06:46:46
Now since MR implementations were
done over Hadoop file systems,

147
06:46:46 --> 06:46:50
issues like node failure were
automatically accounted for.

148
06:46:52 --> 06:46:57
So MR effectively used in complex
applications like data mining or

149
06:46:57 --> 06:47:01
data clustering, and these algorithms
are often very complex, and

150
06:47:01 --> 06:47:05
typically require problem
specific techniques.

151
06:47:05 --> 06:47:08
Very often,
these algorithms have multiple stages.

152
06:47:08 --> 06:47:13
That is the output from one processing
stage is the input to the next.

153
06:47:13 --> 06:47:17
It is difficult to develop these
multistage algorithms in a standard

154
06:47:17 --> 06:47:18
relational system.

155
06:47:19 --> 06:47:24
But since these were genetic operations,
many of them were designed to work

156
06:47:24 --> 06:47:27
with unstructured data like text and
nonstandard custom data formats.

157
06:47:29 --> 06:47:34
Now, it's now amply clear that this
mixture of data management requirements

158
06:47:34 --> 06:47:38
and data processing analysis requirements
have created an interesting tension in

159
06:47:38 --> 06:47:40
the data management world.

160
06:47:40 --> 06:47:44
Just look at a few of
these tension points.

161
06:47:44 --> 06:47:48
Now, DBMSs perform storage and
retrieval operations very efficiently.

162
06:47:50 --> 06:47:53
But first,
the data must be loaded into the DBMS.

163
06:47:54 --> 06:47:55
So, how much time does loading take?

164
06:47:56 --> 06:48:00
In one study,
scientists use two CVS files.

165
06:48:00 --> 06:48:05
One had 92 attributes with
about 165 million tuples for

166
06:48:05 --> 06:48:07
a total size of 85 gigabytes.

167
06:48:09 --> 06:48:13
And the other had 227 attributes
with 5 million tuples for

168
06:48:13 --> 06:48:15
a total size of 5 gigabytes.

169
06:48:16 --> 06:48:23
The time to load and index this data in
MySQL and PostgreSQL, took 15 hours each.

170
06:48:24 --> 06:48:29
In a commercial database running on
three machines, it took two hours.

171
06:48:29 --> 06:48:32
Now there are applications like
the quantities in the case we discussed

172
06:48:32 --> 06:48:37
earlier where this kind of loading
time is simply not acceptable,

173
06:48:37 --> 06:48:39
because the analysis on
the data must be performed

174
06:48:39 --> 06:48:42
within a given time limit
after it's arrival.

175
06:48:44 --> 06:48:47
A second problem faced by
some application is that for

176
06:48:47 --> 06:48:51
them, the DBMSs offer
too much functionality.

177
06:48:52 --> 06:48:56
For example, think of an application
that only looks at the price of an item

178
06:48:56 --> 06:48:58
if you provide it with a product name or
product code.

179
06:49:00 --> 06:49:02
The number of products it serves
is let's say, 250 million.

180
06:49:04 --> 06:49:07
This lookup operation happens
only on a single table and

181
06:49:07 --> 06:49:11
does not mean anything
more complex like a join.

182
06:49:11 --> 06:49:15
Further, consider that while there
are several hundred thousand customers

183
06:49:15 --> 06:49:19
who access this data,
none of them really update the tables.

184
06:49:19 --> 06:49:24
So, do we need a full function DBMS for
this read-only application?

185
06:49:24 --> 06:49:29
Or can we get a simpler solution which
can use a cluster of machines, but

186
06:49:29 --> 06:49:32
does not provide all the wonderful
guarantees that a DBMS provides?

187
06:49:34 --> 06:49:39
At the other end of the spectrum,
there is an emerging class of optimization

188
06:49:39 --> 06:49:44
that meets all the nice transactional
guarantees that a DBMS provides.

189
06:49:44 --> 06:49:48
And at the same time, meets the support
for efficient analytical operations.

190
06:49:49 --> 06:49:53
These are often required for
systems like Real-Time Decision Support.

191
06:49:53 --> 06:49:58
That will accept real-time data like
customer purchases on a newly released

192
06:49:58 --> 06:50:01
product will perform some
statistical analysis, so

193
06:50:01 --> 06:50:03
that it can determine buying trends.

194
06:50:03 --> 06:50:05
And then decide whether in real-time,

195
06:50:05 --> 06:50:09
a discount can be offered
to this customer now.

196
06:50:11 --> 06:50:16
It turns out that the combination
of traditional requirements and

197
06:50:16 --> 06:50:20
new requirements is leading
to new capabilities, and

198
06:50:20 --> 06:50:23
products in the big data
management technology.

199
06:50:23 --> 06:50:28
On the one hand, DBMS technologies
are creating new techniques that make

200
06:50:28 --> 06:50:31
use of MapReduce-style data processing.

201
06:50:31 --> 06:50:34
Many of them are being
developed to run on HDFS and

202
06:50:34 --> 06:50:38
take advantage of his data
replication capabilities.

203
06:50:38 --> 06:50:42
More strikingly, DBMSs are beginning
to have a side door for

204
06:50:42 --> 06:50:46
a user to perform and
MR-style operation on HDFS files and

205
06:50:46 --> 06:50:50
exchange data between the Hadoop
subsystem and the DBMS.

206
06:50:50 --> 06:50:54
Thus, giving the user the flexibility
to use both forms of data processing.

207
06:50:57 --> 06:51:00
It has now been recognized
that a simple map and

208
06:51:00 --> 06:51:05
reduce operations are not sufficient for
many data operations leading to

209
06:51:05 --> 06:51:10
a significant expansion in the number
of operations in the MR ecosystems.

210
06:51:10 --> 06:51:14
For example,
Spark has several kinds of join and

211
06:51:14 --> 06:51:17
data grouping operations in
addition to map and reduce.

212
06:51:19 --> 06:51:22
Sound DBMSs are making use of large

213
06:51:22 --> 06:51:26
distributed memory management
operations to accept streaming data.

214
06:51:27 --> 06:51:31
These systems are designed with the idea
that the analysis they need to perform on

215
06:51:31 --> 06:51:33
the data are known before.

216
06:51:33 --> 06:51:37
And as new data records arrive,
they keep a record of the data

217
06:51:37 --> 06:51:41
in the memory long enough to finish
the computation needed on that record.

218
06:51:43 --> 06:51:45
And finally, computer scientists and

219
06:51:45 --> 06:51:48
data scientists are working
towards new solutions

220
06:51:48 --> 06:51:53
where large scale distributed algorithms
are beginning to emerge to solve different

221
06:51:53 --> 06:51:57
kinds of analytics problems like
finding dense regions of a graph.

222
06:51:58 --> 06:52:02
These algorithms use
a MR-style computing and

223
06:52:02 --> 06:52:05
are becoming a part of a new
generation of DBMS products

224
06:52:05 --> 06:52:08
that invoke these algorithms
from inside the database system.

225
06:52:09 --> 06:52:13
In the next video, we'll take a look at
some of the modern day data management

226
06:52:13 --> 06:52:16
systems that have some
of these capabilities.

1
13:28:06 --> 13:28:11
As we mentioned in the last lesson, there
is no single irricuteous solution for

2
13:28:11 --> 13:28:12
big data problems.

3
13:28:12 --> 13:28:14
So in this lesson,

4
13:28:14 --> 13:28:18
our goal will be to explore some existing
solutions in a little more depth.

5
13:28:20 --> 13:28:23
So after this lesson, you'll be able to.

6
13:28:24 --> 13:28:27
Explain the at least five
desirable characteristics of

7
13:28:27 --> 13:28:33
a Big Data Management System,
explain the differences between acid and

8
13:28:33 --> 13:28:36
base, and

9
13:28:36 --> 13:28:41
list examples of BDMSs and describe some
of their similarities and differences.

10
13:28:43 --> 13:28:48
So we start from high level, suppose there
were an ideal big data management system.

11
13:28:50 --> 13:28:52
What capabilities or
features should such a system have?

12
13:28:54 --> 13:28:57
Professor Michael Carey of
the University of California Irvine

13
13:28:57 --> 13:29:00
has described a number of characteristics.

14
13:29:00 --> 13:29:04
We'll go through them and
use them as an idealistic yardstick

15
13:29:04 --> 13:29:07
against which we compare
existing solutions.

16
13:29:08 --> 13:29:14
First, the ideal BDMS would allow for
a semi-structured data model.

17
13:29:14 --> 13:29:19
Now that does not mean it will only
support a specific format like XML.

18
13:29:20 --> 13:29:22
The operative word here is flexible.

19
13:29:23 --> 13:29:26
The flexibility can take many forms,

20
13:29:27 --> 13:29:31
one of which Is the degree to which
schemas should be supported by the system.

21
13:29:33 --> 13:29:35
In a perfect world, it should support

22
13:29:35 --> 13:29:39
a completely traditional application which
requires the development of a schema.

23
13:29:41 --> 13:29:45
At the same time, it should also support
applications which require no schema,

24
13:29:45 --> 13:29:49
because the data can vary in terms
of its attributes and relationships.

25
13:29:51 --> 13:29:57
A different axis of flexibility
is in the data types it supports.

26
13:29:57 --> 13:30:00
For example, it should support
operations on text and documents.

27
13:30:01 --> 13:30:07
It should also permit social media and
other data that have a time component and

28
13:30:07 --> 13:30:10
need temporal operations, like before,
after, during, and so on.

29
13:30:12 --> 13:30:15
Similarly, it should
allow spacial data and

30
13:30:15 --> 13:30:19
allow operations like find all data
within a five mile radius of a landmark.

31
13:30:21 --> 13:30:24
As we saw in a previous lesson,

32
13:30:24 --> 13:30:28
a big advantage of a DBMS is that
is provides a query language.

33
13:30:29 --> 13:30:34
There is a notion that query languages
present a steep learning curve for

34
13:30:34 --> 13:30:37
data science people with no
background in computer science.

35
13:30:37 --> 13:30:41
However, to effectively
manage large volumes of data,

36
13:30:41 --> 13:30:45
it's often more convenient
to use a query language and

37
13:30:45 --> 13:30:49
let the query processor automatically
determine optimal ways to receive data.

38
13:30:51 --> 13:30:54
Now this query language may or
may not look like SQL,

39
13:30:54 --> 13:30:59
which is the standard query language used
by the modern relational systems, but

40
13:30:59 --> 13:31:01
it should at least be equally powerful.

41
13:31:03 --> 13:31:08
Now this is not an unreasonable feature
given that most DBMS vendors offer their

42
13:31:08 --> 13:31:16
own extension of SQL Of course it's not
enough to just have a good query language.

43
13:31:17 --> 13:31:22
Today's big data systems must
have a parallel query engine

44
13:31:22 --> 13:31:24
which will run on multiple machines.

45
13:31:25 --> 13:31:29
The machines can be connected to
a shared nothing architecture, or

46
13:31:29 --> 13:31:32
shared memory architecture,
or a shared cluster.

47
13:31:32 --> 13:31:38
The shared nothing means two machines
do not share a disk or memory.

48
13:31:38 --> 13:31:40
But this is a critical requirement for

49
13:31:40 --> 13:31:45
any BDMS regardless of how complete
the supported query languages is for

50
13:31:45 --> 13:31:50
efficiency sake Continuing with our list,
the next

51
13:31:50 --> 13:31:55
capability of a BDMS is often
not emphasized as much as it should be.

52
13:31:56 --> 13:32:00
Some applications working on top
of a BDMS will issue queries

53
13:32:00 --> 13:32:05
which will only have a few conditions and
a few small objects to return.

54
13:32:05 --> 13:32:10
But some applications, especially those
generated by other software tools or

55
13:32:10 --> 13:32:14
machine learning algorithms can
have many many conditions and

56
13:32:14 --> 13:32:15
can return many large objects.

57
13:32:17 --> 13:32:22
In my own work, we have seen how internet
bots can blast an information system

58
13:32:22 --> 13:32:26
with really large queries that
can potentially choke the system.

59
13:32:26 --> 13:32:27
But that should not happen in a BDMS.

60
13:32:29 --> 13:32:32
Now, we discussed streaming
data in a previous lesson.

61
13:32:33 --> 13:32:37
In many cases,
a BDMS will have both streaming data,

62
13:32:37 --> 13:32:41
which adds to the volume,
as well as to the large data that

63
13:32:41 --> 13:32:44
need to be combined with the streaming
data to solve a problem.

64
13:32:45 --> 13:32:49
An example would be to combine
streaming data from weather stations

65
13:32:49 --> 13:32:52
with historical data to make
better predictions for wild fire.

66
13:32:55 --> 13:32:58
We have discussed the definition,
significance and

67
13:32:58 --> 13:33:01
importance of scalability before.

68
13:33:01 --> 13:33:04
However, what a BDMS needs to guarantee

69
13:33:04 --> 13:33:09
that it is designed to operate over a
cluster, possibly a cluster of hundred or

70
13:33:09 --> 13:33:13
thousand of machines and
that it knows how to handle a failure.

71
13:33:15 --> 13:33:19
Further the system should be able
to handling new machines joining or

72
13:33:19 --> 13:33:20
existing machines leaving the cluster.

73
13:33:22 --> 13:33:27
Finally, our BDMS must have
data management capabilities.

74
13:33:27 --> 13:33:33
It should be easy to install, restart and
configure, provide high availability and

75
13:33:33 --> 13:33:37
make operational management as
simple as possible even when

76
13:33:37 --> 13:33:41
a BDMS is declined across data centers
that are possibly geographically apart

77
13:33:44 --> 13:33:49
In a prior module, we discussed
the ACID properties of transactions and

78
13:33:49 --> 13:33:51
said that BDMSs guarantee them.

79
13:33:53 --> 13:33:57
For big data systems,
there is too much data and

80
13:33:57 --> 13:34:00
too many updates from too many users.

81
13:34:00 --> 13:34:03
So the effort to maintain ACID properties

82
13:34:03 --> 13:34:06
May lead to a significant
slowdown of the system.

83
13:34:07 --> 13:34:11
Now, this lead to the idea, that while
the ACID properties are still desirable,

84
13:34:12 --> 13:34:18
it might be more practical to relax
the ACID conditions and replace them

85
13:34:18 --> 13:34:23
with what's called the BASE properties,
beginning with Basic availability.

86
13:34:25 --> 13:34:28
This states that the system

87
13:34:28 --> 13:34:30
does guarantee the availability
of data in the following sense.

88
13:34:32 --> 13:34:36
If you make a request,
there will be a response to that request.

89
13:34:36 --> 13:34:40
But, the response could still
be failure to obtain data or

90
13:34:40 --> 13:34:43
the data isn't Inconsistent state or
changing state.

91
13:34:45 --> 13:34:48
Well this is not unusual because
it's much like waiting for

92
13:34:48 --> 13:34:50
a check to clear a bank account.

93
13:34:53 --> 13:34:55
Second, there is a soft state.

94
13:34:56 --> 13:35:01
Which means the state of the system
is very likely to change over time.

95
13:35:01 --> 13:35:04
So even during times without input,

96
13:35:04 --> 13:35:08
there may be changes going on through
the system due to eventual consistency.

97
13:35:09 --> 13:35:12
Thus the state of
the system is always soft.

98
13:35:13 --> 13:35:16
And finally there's eventual consistency.

99
13:35:17 --> 13:35:22
This means that the system will
eventually become consistent

100
13:35:22 --> 13:35:23
once it stops receiving input.

101
13:35:25 --> 13:35:27
When it stops receiving input,

102
13:35:27 --> 13:35:32
the data will propagate to everywhere
that it should sooner or later go to but

103
13:35:32 --> 13:35:36
in reality the system will
continue to receive input.

104
13:35:36 --> 13:35:40
And it's not checking the consistency
of every transaction at every moment

105
13:35:40 --> 13:35:44
because there's still lots
of transactions to process.

106
13:35:44 --> 13:35:49
So, if you make a new Facebook post,
your friend in Zambia, who is supposed

107
13:35:49 --> 13:35:55
to see your update but is served by a very
different data center in a different

108
13:35:55 --> 13:36:00
geographic region, will certainly
see it but may not see right away.

109
13:36:02 --> 13:36:06
Now for those of you who have a bit
of computer science background,

110
13:36:06 --> 13:36:10
we just want to mention in passing that
there is actually some theoretical results

111
13:36:10 --> 13:36:12
behind this relaxation.

112
13:36:13 --> 13:36:17
The result comes from what's
called the CAP Theorem.

113
13:36:17 --> 13:36:20
Also named Bauer's theorem after
the computer scientist Eric Bauer.

114
13:36:22 --> 13:36:27
He states, that it is impossible for
a distributed computer system

115
13:36:27 --> 13:36:31
to simultaneously provide all
three of the following guarantees.

116
13:36:32 --> 13:36:34
Consistency.

117
13:36:34 --> 13:36:38
It means all nodes see the
same data at any time.

118
13:36:41 --> 13:36:46
Availability, which is a guarantee
that every request receives a response

119
13:36:46 --> 13:36:47
about whether it succeeded or failed.

120
13:36:47 --> 13:36:52
And Partition Tolerance.

121
13:36:52 --> 13:36:55
Which means the system
continues to operate

122
13:36:55 --> 13:36:59
despite arbitrary partitioning
due to network failures.

123
13:36:59 --> 13:37:03
Now, most of the big data systems
available today will adhere

124
13:37:03 --> 13:37:08
to these BASE properties,
although several modern systems

125
13:37:08 --> 13:37:12
do offer the stricter ACID properties,
or at least several of them.

126
13:37:14 --> 13:37:17
Now given the idealistic background
of what is desirable and

127
13:37:17 --> 13:37:21
achievable in a big data system,
today's marketplace for

128
13:37:21 --> 13:37:24
big data related products
looks somewhat like this.

129
13:37:25 --> 13:37:30
Now this is Matt Turk's depiction of big
data products from a couple of years back.

130
13:37:31 --> 13:37:35
You would notice that the products
are grouped into categories, like no SQL,

131
13:37:35 --> 13:37:40
massively parallel databases, analytic
systems, real time systems, and so forth.

132
13:37:41 --> 13:37:46
In this lesson, we'll do a quick
tour through a few of these products

133
13:37:47 --> 13:37:49
from different areas of this landscape.

134
13:37:51 --> 13:37:57
In each case our goal will to be assess
what aspects of our ideal BDMS they cover,

135
13:37:57 --> 13:38:00
and whether they have obvious limitations.

136
13:38:00 --> 13:38:05
Now we will not cover all features
of every system, but will highlight

137
13:38:05 --> 13:38:09
those aspects of the system that
are relevant to our discussion on BDMS.

1
03:06:15 --> 03:06:20
The first up in our rapid tour
of modern systems is Redis.

2
03:06:21 --> 03:06:25
Redis calls itself an in-memory
data structure store.

3
03:06:25 --> 03:06:31
In simple terms, Redis is not a full blown
DBMS, in the sense we discussed earlier.

4
03:06:32 --> 03:06:37
It can persist data on disks, and
does so to save its state, but

5
03:06:37 --> 03:06:42
it's intended use is to optimally
use memory and memory based methods

6
03:06:42 --> 03:06:46
to make a number of common data
structures very fast for lots of users.

7
03:06:48 --> 03:06:50
Here is a list of data
structures that Redis supports.

8
03:06:54 --> 03:06:59
A good way to think about them is
to think of a data lookup problem.

9
03:06:59 --> 03:07:03
Now, in the simplest case,
a lookup needs a key value pair

10
03:07:03 --> 03:07:07
where the key is a string and
the value is also a string.

11
03:07:07 --> 03:07:12
So for a lookup we provide the key and
get back the value.

12
03:07:12 --> 03:07:13
Simple, right?

13
03:07:13 --> 03:07:14
Let's see.

14
03:07:15 --> 03:07:18
I'm sure you've seen captures like this.

15
03:07:20 --> 03:07:24
These are small images used by websites
to ensure that the user is a human and

16
03:07:24 --> 03:07:25
not a robot.

17
03:07:26 --> 03:07:30
The images presented to the user,
who is supposed to write the text he or

18
03:07:30 --> 03:07:33
she sees in the image, into a text box.

19
03:07:33 --> 03:07:36
Upon success, the user is let in.

20
03:07:37 --> 03:07:41
To implement this,
one obviously needs a key value stored,

21
03:07:41 --> 03:07:44
where the key is the idea of the image.

22
03:07:44 --> 03:07:46
And the value is the desired text.

23
03:07:47 --> 03:07:52
Now what if we wanted to use
the image itself as the key,

24
03:07:52 --> 03:07:55
instead of an ID number that
you generate separately?

25
03:07:57 --> 03:07:59
The content of the image,

26
03:07:59 --> 03:08:04
which let's say is a .jpg file,
can be thought of as a binary string.

27
03:08:04 --> 03:08:05
But can it serve as a key then?

28
03:08:05 --> 03:08:09
According to the Redis specification,
it can.

29
03:08:10 --> 03:08:13
The Redis string can be binary and

30
03:08:13 --> 03:08:18
can have a size of up to 512 megabytes,
although its internal limit is higher.

31
03:08:19 --> 03:08:25
So, small images like this can indeed
be used as binary string keys.

32
03:08:27 --> 03:08:32
In some application scenarios,
keys may have an internal structure.

33
03:08:32 --> 03:08:36
For example,
product codes may have product family,

34
03:08:36 --> 03:08:41
manufacturing batch, and the actual
product ID strung together into one ID.

35
03:08:42 --> 03:08:46
The example shown here is a typical
Twitter style key to store the response

36
03:08:46 --> 03:08:48
to comment one, two, three, four.

37
03:08:50 --> 03:08:53
How long do we want to
keep the comment around,

38
03:08:53 --> 03:08:58
this is a standard big data issue
when it comes to streaming data

39
03:08:58 --> 03:09:01
whose data values have limited
utility beyond the certain period.

40
03:09:03 --> 03:09:05
One typically would not look for

41
03:09:05 --> 03:09:08
this response possibly three
months after the conversation.

42
03:09:09 --> 03:09:13
One would certainly not like to keep
such a value in memory for a long time.

43
03:09:13 --> 03:09:17
Because the memory is being used as
a cache for rapid access to current data.

44
03:09:18 --> 03:09:24
In fact Redis has the ability
to delete an expired key and

45
03:09:24 --> 03:09:27
can be made to call a function
to generate a new key.

46
03:09:29 --> 03:09:32
An interesting side
benefits to structured keys

47
03:09:32 --> 03:09:35
that it can encode
a hierarchy to the structure.

48
03:09:35 --> 03:09:36
In the example,

49
03:09:36 --> 03:09:41
we show keys that represent
increasingly finer subgroups of users.

50
03:09:43 --> 03:09:44
With this structure,

51
03:09:44 --> 03:09:49
a lookup on user.commercial.entertainment
will also retrieve values

52
03:09:49 --> 03:09:53
from user.commercial.entertainment.movie
industry.

53
03:09:56 --> 03:10:02
A slightly more complex case occurs
when the value is not atomic,

54
03:10:02 --> 03:10:07
but a collection object like a list
which by definition is an ordered set.

55
03:10:08 --> 03:10:11
An example of such a list
can come from Twitter,

56
03:10:11 --> 03:10:15
that uses Redis list to store timelines.

57
03:10:15 --> 03:10:18
Now borrowed from a Twitter presentation
about their timeline architecture

58
03:10:19 --> 03:10:23
our timeline service can
take a specific ID, and

59
03:10:23 --> 03:10:27
it quickly identifies all tweets of
this user that are in the cache.

60
03:10:29 --> 03:10:32
These tweets are then populated
by the content of the tweet,

61
03:10:32 --> 03:10:34
then returned as a result.

62
03:10:34 --> 03:10:39
This list can be long, but the insertion
and delete operations on the list

63
03:10:39 --> 03:10:41
can be performed in constant
time which is in milliseconds.

64
03:10:43 --> 03:10:48
If a tweet is retweeted,
the IDs of those tweets are also added

65
03:10:48 --> 03:10:51
to the list of the first tweet, as you can
see for the three cases in the figure.

66
03:10:54 --> 03:10:58
When the lists are long,
space saving becomes an important issue.

67
03:10:59 --> 03:11:03
So Redis employs a method called
Ziplists which essentially

68
03:11:03 --> 03:11:07
compacts the size of the list in
memory without changing the content.

69
03:11:07 --> 03:11:10
Often producing significant
reduction in memory used.

70
03:11:12 --> 03:11:14
Of course,
while Ziplists are very efficient for

71
03:11:14 --> 03:11:19
retrieval, they are a little more complex
for insertion and deletion operations.

72
03:11:21 --> 03:11:24
Since Redis is an open source system,

73
03:11:24 --> 03:11:28
Twitter made a few innovations
on the Redis data structures.

74
03:11:28 --> 03:11:33
One of these innovations is that
they created lists of Ziplists.

75
03:11:33 --> 03:11:37
This gave them the flexibility of
having constant timing insertions and

76
03:11:37 --> 03:11:41
deletions and at the same time used the
compressed representation to save space.

77
03:11:43 --> 03:11:48
In 2012, the timeline service has about

78
03:11:48 --> 03:11:54
40 terabytes of main memory, serving
30 million user queries per second.

79
03:11:54 --> 03:11:57
Running on over 6,000
machines in one data center.

80
03:11:58 --> 03:12:04
For those interested I would like to point
you to two wonderful Twitter presentation

81
03:12:04 --> 03:12:08
explaining Twitter's use of Redis
among other design issue for

82
03:12:08 --> 03:12:09
a realtime data system like Twitter.

83
03:12:11 --> 03:12:14
We also introduce links in
the supplemental readings for this lesson.

84
03:12:18 --> 03:12:23
Now the value to be looked up by the keys
can actually be more complicated and

85
03:12:23 --> 03:12:28
can be records containing
attribute value pairs themselves.

86
03:12:30 --> 03:12:31
Redis values can be hashes

87
03:12:33 --> 03:12:37
which are essentially named containers
of unique fields and their values.

88
03:12:39 --> 03:12:42
In the example, the key, std:101,

89
03:12:42 --> 03:12:46
is associated with five
attributed value pairs.

90
03:12:48 --> 03:12:51
The hashed attributes
are stored very efficiently.

91
03:12:51 --> 03:12:56
And even when the list of attributed
value pairs in a hash is really long,

92
03:12:56 --> 03:12:58
retrieval is efficient.

93
03:13:01 --> 03:13:03
Horizontal scalability or

94
03:13:03 --> 03:13:07
scale out capabilities refers
to the ability of a system

95
03:13:07 --> 03:13:12
to achieve scalability when the number
of machines it operates on is increased.

96
03:13:13 --> 03:13:19
Redis allows data partitioning through
range partitioning and hash partitioning.

97
03:13:20 --> 03:13:26
Rate partitioning takes a numeric key and
breaks up the range of keys into bins.

98
03:13:26 --> 03:13:31
In this case, by bins of 10,000
each of bin is assigned a machine.

99
03:13:33 --> 03:13:37
And ultimately, a partitioning is where
computing a hashing function on a key.

100
03:13:39 --> 03:13:41
Suppose we have 10 machines.

101
03:13:41 --> 03:13:44
We pick the key and use the hash
function to get back a number.

102
03:13:46 --> 03:13:50
We represent the number, modular 10,
and the result, in this case,

103
03:13:50 --> 03:13:53
2, is the machine to which
the record will be allocated.

104
03:13:56 --> 03:14:01
Replication is accomplished in
Redis through master-slave mode.

105
03:14:03 --> 03:14:07
The slaves have a copy of the master node.

106
03:14:07 --> 03:14:08
And can serve read queries.

107
03:14:11 --> 03:14:13
Clients write to the master node.

108
03:14:13 --> 03:14:16
And master node replicates to the slaves.

109
03:14:18 --> 03:14:21
Clients read from the slaves to
scale up the read performance.

110
03:14:22 --> 03:14:24
The replication processes are synchronous.

111
03:14:24 --> 03:14:29
That is that slaves do not get replicated
data, it locks them with each other.

112
03:14:30 --> 03:14:33
However, the replication
process does ensure

113
03:14:33 --> 03:14:35
that they're consistent with each other.

1
06:20:49 --> 06:20:52
The next system we'll explore is Vertica.

2
06:20:52 --> 06:20:56
Which is the relational DBMS
designed to operate on top of HTFS.

3
06:20:58 --> 06:21:01
It belongs to a family of DBMS
architectures called column stores.

4
06:21:03 --> 06:21:06
Other products in
the same family are UCDV,

5
06:21:06 --> 06:21:09
Carrot Cell Xvelocity from Microsoft and
so forth.

6
06:21:11 --> 06:21:13
The primary difference
between a row store and

7
06:21:13 --> 06:21:16
a column store is shown
in the diagram here.

8
06:21:17 --> 06:21:20
Logically, this table has five columns.

9
06:21:20 --> 06:21:23
Emp number, department number,

10
06:21:23 --> 06:21:26
hire date, employee last name and
employee first name.

11
06:21:28 --> 06:21:33
In a row oriented design the database
internally organizes the record

12
06:21:33 --> 06:21:34
two four by two.

13
06:21:36 --> 06:21:40
In a column store,
the data is organized column wise.

14
06:21:41 --> 06:21:46
So the nth number column is stored
separately from the department id

15
06:21:46 --> 06:21:47
column and so forth.

16
06:21:48 --> 06:21:51
Now suppose a query
needs to find the ID and

17
06:21:51 --> 06:21:57
department ID of all employees who were
hired after first of January, 2001.

18
06:21:57 --> 06:22:00
The system only needs to look up

19
06:22:00 --> 06:22:04
the hire date column to figure
out which records qualify.

20
06:22:04 --> 06:22:06
And then pick up the values of the ID,

21
06:22:06 --> 06:22:10
and the department of ID columns for
the qualifying records.

22
06:22:10 --> 06:22:14
The other columns are not touched.

23
06:22:14 --> 06:22:19
So if a table has 30 to 50 columns, very
often only a few of them are needed for

24
06:22:19 --> 06:22:19
any single query.

25
06:22:21 --> 06:22:28
So for tables with 500 million rows and
40 columns a typical query is very fast,

26
06:22:28 --> 06:22:33
and uses much less memory because a full
record is not used most of the time.

27
06:22:34 --> 06:22:39
My own experience is that with
an application that needed a database

28
06:22:39 --> 06:22:41
with 150 billion tuples in a table.

29
06:22:42 --> 06:22:46
Accounting operation took a little
under three minutes to complete.

30
06:22:48 --> 06:22:51
A second advantage of the column store
comes from the nature of the data.

31
06:22:53 --> 06:22:57
In a column store,
data in every column is sorted.

32
06:22:58 --> 06:23:01
The figure on the right shows
three sorted columns of a table

33
06:23:01 --> 06:23:03
with three visible columns.

34
06:23:04 --> 06:23:06
The bottom of the first column shown here

35
06:23:06 --> 06:23:10
has many accurate sixteen
transactions on the same day.

36
06:23:12 --> 06:23:17
The second column has customer ids which
can be numerically close to each other.

37
06:23:17 --> 06:23:19
They are all within seventy
values of the first customer.

38
06:23:21 --> 06:23:26
So in the first case we can just
say that the value is one, one,

39
06:23:26 --> 06:23:29
2007, and
the next 16 records have this value.

40
06:23:30 --> 06:23:36
That means we do not have to store
the next 16 values thus saving space.

41
06:23:38 --> 06:23:42
Now this form of shortened
representation is called compression.

42
06:23:43 --> 06:23:48
And this specific variety is
called run-length encoding or RLE.

43
06:23:49 --> 06:23:53
Another form of encoding can
be seen in the second column.

44
06:23:55 --> 06:23:58
Here the customer ids a long integer,
but for

45
06:23:58 --> 06:24:02
the records shown there
are nearby numbers.

46
06:24:02 --> 06:24:06
So, if we pick a value in the column and
just put the difference between

47
06:24:06 --> 06:24:10
this value and
other values The difference will be small.

48
06:24:10 --> 06:24:13
It means,
we'll need fewer bites to represent them.

49
06:24:15 --> 06:24:18
This one of compression is called
Frame-of-reference encoding.

50
06:24:20 --> 06:24:24
The lesson to remember here is
that compressed data presentation

51
06:24:24 --> 06:24:29
can significantly reduce the total
size of a database and if BDMS.

52
06:24:29 --> 06:24:32
Should use all such tricks
to improve space efficiency.

53
06:24:34 --> 06:24:38
While the space efficiency and
performance of Vertica is impressive for

54
06:24:38 --> 06:24:42
large data, one has to be careful
about how to design a system with it.

55
06:24:43 --> 06:24:45
Like Google's Web Table, and

56
06:24:45 --> 06:24:50
Apache Cassandra, Vertica allows
the declaration of column-groups.

57
06:24:52 --> 06:24:53
These are columns like first name and

58
06:24:53 --> 06:24:56
last name which are very
often accessed together.

59
06:24:57 --> 06:25:01
In Vertica,
a column group is like a mini table

60
06:25:01 --> 06:25:03
which is treated like a little
row storied in the system.

61
06:25:04 --> 06:25:10
But because these groups represent
activists that are frequently co accessed

62
06:25:10 --> 06:25:12
the grouping actually
improves performance.

63
06:25:13 --> 06:25:17
So an application developer is
better off when the nature and

64
06:25:17 --> 06:25:21
the frequency of user queries
are known to some degree.

65
06:25:21 --> 06:25:25
And this knowledge is applies
to designing the column groups.

66
06:25:25 --> 06:25:29
An important side effect of column
structure organization of vertical

67
06:25:29 --> 06:25:32
Is that the writing of data into
Vertica is a little slower.

68
06:25:33 --> 06:25:38
When rows are added to a table,
Vertica initially places

69
06:25:38 --> 06:25:41
them in a row-wise data structure and

70
06:25:41 --> 06:25:46
then converts them into a column-wise
data structure, which is then compressed.

71
06:25:47 --> 06:25:50
This lowness can be perceptible for
large uploads or updates.

72
06:25:52 --> 06:25:56
Vertica belongs to a new breed of
database systems that call themselves

73
06:25:56 --> 06:25:57
analytical databases.

74
06:25:58 --> 06:26:02
This means two slightly different things.

75
06:26:02 --> 06:26:06
First, Vertica offers many more
statistical functions than in

76
06:26:06 --> 06:26:06
classical DBMS.

77
06:26:08 --> 06:26:12
For example one can perform
operations over a Window

78
06:26:13 --> 06:26:20
to see an example consider a table of
stock ticks having a time attribute,

79
06:26:20 --> 06:26:23
a stock name and
a value of the stock called bid here.

80
06:26:24 --> 06:26:29
This shows that data values in
the table now we would like to compute

81
06:26:29 --> 06:26:33
a moving average of
the bit every 40 seconds.

82
06:26:34 --> 06:26:36
We show the query in a frame below.

83
06:26:36 --> 06:26:41
Now we aren't going into the details of
the query just consider the blue part.

84
06:26:41 --> 06:26:45
It says the average,
which is the AVG function in yellow.

85
06:26:45 --> 06:26:48
It must be computed on
the last column which is bit.

86
06:26:49 --> 06:26:51
But, this computation

87
06:26:52 --> 06:26:56
must be over the range that is
computed on the timestamp call.

88
06:26:57 --> 06:27:03
So, the range is defined by a 40
second row before that current row so,

89
06:27:03 --> 06:27:09
here the computation of the average
advances for the stock abc.

90
06:27:09 --> 06:27:14
And for each computation,
the system only considers the rows whose

91
06:27:14 --> 06:27:18
timestamp is within 40 seconds
before the current row.

92
06:27:19 --> 06:27:23
The table on the right shows
the result of the query.

93
06:27:23 --> 06:27:26
The average value, 10.12.

94
06:27:26 --> 06:27:29
Is the same as the actual value, because
there are no other rows within 40 seconds.

95
06:27:29 --> 06:27:33
The next two result rows average
over the preceding rows,

96
06:27:33 --> 06:27:37
was times R within 40
seconds of the current row.

97
06:27:38 --> 06:27:39
When we get to the blue row,

98
06:27:39 --> 06:27:46
that we notice that it occurs 1 minute
16 seconds after the previous row.

99
06:27:46 --> 06:27:48
So we cannot consider the previous
sort in the computation.

100
06:27:48 --> 06:27:52
Instead, the result is just the value
of the bid in the current row.

101
06:27:54 --> 06:27:58
The takeaway from this example is
that analytical computations like

102
06:27:58 --> 06:28:02
this are happening inside the database and
not in an external application.

103
06:28:03 --> 06:28:07
This brings us to the second feature
of Vertica as an analytical database.

104
06:28:09 --> 06:28:13
R is a well known free statistics
package that's used by statisticians,

105
06:28:13 --> 06:28:16
data minors, predictive analytics experts.

106
06:28:16 --> 06:28:21
Today, R can not only
read data from files,

107
06:28:21 --> 06:28:25
but it can go to an SQL database and
grab data to perform statistical analysis.

108
06:28:27 --> 06:28:30
Over time, R has evolved and

109
06:28:30 --> 06:28:35
given rights to distributed R which
is a high performance platform for R.

110
06:28:37 --> 06:28:39
As expected in this distributed setting,

111
06:28:39 --> 06:28:43
the system operates in
a master slave mode.

112
06:28:43 --> 06:28:47
The master node coordinates computations
by sending commands to the workers.

113
06:28:47 --> 06:28:50
The worker nodes maintain
data partitions and

114
06:28:50 --> 06:28:53
apply the computation
functions to the data.

115
06:28:53 --> 06:28:57
Just getting the data parallelly,

116
06:28:57 --> 06:29:00
the essential data structure
is a distributed array.

117
06:29:00 --> 06:29:03
That is an array that is
partitioned as shown here.

118
06:29:03 --> 06:29:06
Now in this diagram
the partitions are equal, but

119
06:29:06 --> 06:29:08
in practice they may be
all different sizes.

120
06:29:08 --> 06:29:14
On which, one can compute a function for
each of these mini-arrays.

121
06:29:14 --> 06:29:18
The bottom diagram, shows a simple
work flow of constructing and

122
06:29:18 --> 06:29:19
deploying a predictive model.

123
06:29:20 --> 06:29:22
The role of Vertica here,

124
06:29:22 --> 06:29:26
is that it's a data supplier to the worker
nodes of R, and a model consumer.

125
06:29:27 --> 06:29:31
The data to be analyzed is
the output of the vertica query,

126
06:29:31 --> 06:29:36
which is transferred in memory through
a protocol called vertica fast transfer

127
06:29:36 --> 06:29:37
through distributed R as a dArray.

128
06:29:38 --> 06:29:40
When the model is created in R,

129
06:29:40 --> 06:29:44
it should come back as a code that
goes through vertica as a function.

130
06:29:46 --> 06:29:48
This function can be
called from inside Vertica

131
06:29:48 --> 06:29:51
as if it was a user defined function.

132
06:29:52 --> 06:29:55
Now in sophisticated applications,
the features of the data needed for

133
06:29:55 --> 06:30:01
predicted modeling will also be
computed inside the DBMS possibly

134
06:30:01 --> 06:30:04
using the new analytical operations
of Vertica that we've just shown.

135
06:30:05 --> 06:30:09
Now this will make future
computation much faster and

136
06:30:09 --> 06:30:12
improve the efficiency of
the entire analytics process.

137
06:30:14 --> 06:30:19
Going forward, we believe that most DBMS's
will want to play in the analytics field,

138
06:30:19 --> 06:30:21
will support similar functions.

1
12:51:11 --> 12:51:15
Most of you have heard of
MongoDB as a dominant store for

2
12:51:15 --> 12:51:16
JSON style semi-structured data.

3
12:51:18 --> 12:51:20
MongoDB is very popular and

4
12:51:20 --> 12:51:22
there are a number of excellent
tutorials on it on the web.

5
12:51:24 --> 12:51:29
In this module we would like to discuss
a relatively new big data management

6
12:51:29 --> 12:51:34
system for semistructured data that's
currently being incubated by Apache.

7
12:51:34 --> 12:51:36
It's called AsterixDB.

8
12:51:37 --> 12:51:41
Originally, AsterixDB was conceived by
the University of California Irvine.

9
12:51:42 --> 12:51:45
Since it is a full fledged DBMS,

10
12:51:45 --> 12:51:51
it provides ACID guarantees to
understand the basic design of AsterixDB,

11
12:51:52 --> 12:51:57
let's consider this incomplete JSON
snippet taken from an actual tweet.

12
12:51:59 --> 12:52:00
We have seen the structure of JSON before.

13
12:52:02 --> 12:52:08
Here we point out that entities and
user, the two parts

14
12:52:08 --> 12:52:12
in blue are nested, that means embedded,
within the structure of the tweet.

15
12:52:14 --> 12:52:19
If we represent a part of the schema of
this abbreviated structure in AsterixDB,

16
12:52:21 --> 12:52:22
it will look like this.

17
12:52:23 --> 12:52:26
Here a dataverse is like a name space for
data.

18
12:52:28 --> 12:52:31
Data is declared in terms of data types.

19
12:52:31 --> 12:52:37
The top type, which looks like
a standard data with stable declaration,

20
12:52:37 --> 12:52:41
represents the user portion of the JSON
object that we highlighted before.

21
12:52:42 --> 12:52:44
The type below represents the message.

22
12:52:45 --> 12:52:48
Now, instead of nesting it like JSON.

23
12:52:48 --> 12:52:53
The user attribute highlighted in
blue is declared to have the type

24
12:52:53 --> 12:52:58
TwitterUserType, thus it captures
the hierarchical structure of JSON.

25
12:53:00 --> 12:53:04
We should also notice that
the first type is declared as open.

26
12:53:06 --> 12:53:11
It means that the actual data can have
more attributes than specified here.

27
12:53:12 --> 12:53:17
In contrast, the TweetMessage
type is declared as closed,

28
12:53:17 --> 12:53:22
meaning that the data instance must have
the same attributes as in the schema.

29
12:53:24 --> 12:53:28
AsterixDB can handle spatial data as given
by the point data types shown in green.

30
12:53:30 --> 12:53:36
The question mark at the end of the point
type says that this attribute is optional.

31
12:53:36 --> 12:53:38
That means all instances need not have it.

32
12:53:40 --> 12:53:46
Finally, the create dataset actually
asks the system to create a dataset

33
12:53:46 --> 12:53:52
called TweetMessages, whose type is
the just declared quick message type.

34
12:53:53 --> 12:53:59
AstrerixDB which runs on HDFS provides
several options for credit support.

35
12:54:02 --> 12:54:07
First it has its own query language
called the Asterix query language

36
12:54:07 --> 12:54:10
which resembles the XML
credit language query.

37
12:54:12 --> 12:54:14
The details of this query language
are not important right now.

38
12:54:15 --> 12:54:20
We are illustrating the structure of
a query just to show what it looks like.

39
12:54:21 --> 12:54:24
This particular query asks for

40
12:54:24 --> 12:54:29
all user objects from the dataset
TwitterUsers in descending order of their

41
12:54:29 --> 12:54:34
follower count and in alphabetical
order of the user's preferred language.

42
12:54:35 --> 12:54:39
What is more interesting and distinctive
is that AsterixDB has a creative

43
12:54:39 --> 12:54:45
processing engine that can process
queries in multiple languages.

44
12:54:46 --> 12:54:50
For its supported language
they've developed a way to

45
12:54:50 --> 12:54:55
transfer the query into a set of low
level operations like select and

46
12:54:55 --> 12:54:58
join which their query
exchange can support.

47
12:54:58 --> 12:55:03
Further, they've determined how
a record described in one of these

48
12:55:03 --> 12:55:07
languages can be transformed
into an Asterix.

49
12:55:07 --> 12:55:11
In this manner, the support hive queries,

50
12:55:11 --> 12:55:15
which is expressed in like this.

51
12:55:15 --> 12:55:20
Xquery, Hadoop map reduce,
as wall as a new language

52
12:55:20 --> 12:55:24
called SQL++ which extends SQL for JSON.

53
12:55:28 --> 12:55:29
Like a typical DB BDms,

54
12:55:29 --> 12:55:34
AsterixDB is designed to operate
on a cluster of machines.

55
12:55:34 --> 12:55:40
The basic idea, not surprisingly,
is to use partition data parallellism.

56
12:55:40 --> 12:55:44
Each data set is divided into
instances of various types

57
12:55:44 --> 12:55:48
which can be decomposed to different
machines by either range partitioning or

58
12:55:48 --> 12:55:50
hash partitioning like
we discussed earlier.

59
12:55:52 --> 12:55:56
A runtime distributed execution
engine called Hyracks is used for

60
12:55:56 --> 12:56:00
partitioned parallel
execution of query plans.

61
12:56:00 --> 12:56:03
For example, let's assume we have
two relations, customers and orders,

62
12:56:03 --> 12:56:04
as you can see here.

63
12:56:06 --> 12:56:09
Our query is find the number of orders for

64
12:56:09 --> 12:56:12
every market segment that
the customers belong to.

65
12:56:14 --> 12:56:19
Now this query need a join operation
between the two relations,

66
12:56:19 --> 12:56:25
using the O_CUSTKEY as a foreign
key of customer into orders.

67
12:56:26 --> 12:56:30
It also needs a grouping operation,
which for

68
12:56:30 --> 12:56:34
each market segment will pull together all
the orders which will then be counted.

69
12:56:35 --> 12:56:38
You don't have to understand the details
of this diagram at this point.

70
12:56:38 --> 12:56:42
We just want to point out that
the different parts of the query

71
12:56:42 --> 12:56:47
that are being marked,
the customer filed here has two partitions

72
12:56:47 --> 12:56:50
that reside on two nodes,
NC one and NC two respectively.

73
12:56:52 --> 12:56:56
The orders file also has two partitions.

74
12:56:56 --> 12:56:58
But each partition is dually replicated.

75
12:57:00 --> 12:57:07
One can be accessed either of nodes NC3 or
NC2 and the other on NC1 and NC5.

76
12:57:10 --> 12:57:15
Hyracks will also break up
the query into a number of jobs and

77
12:57:15 --> 12:57:19
then fill it out which tasks can
be performed in parallel and

78
12:57:19 --> 12:57:22
which ones must be
executed stage by stage.

79
12:57:23 --> 12:57:26
This whole thing will be managed
by the cluster controller.

80
12:57:28 --> 12:57:31
The cluster controller is also
responsible for replanning and

81
12:57:31 --> 12:57:36
reexecuting of a job
if there is a failure.

82
12:57:36 --> 12:57:41
AsteriskDB also has the provision

83
12:57:41 --> 12:57:44
to accept real time data from external
data sources at multiple rates.

84
12:57:46 --> 12:57:49
One way is from files in a directory path.

85
12:57:50 --> 12:57:52
Consider the example of tweets.

86
12:57:53 --> 12:57:55
As you have seen with the hands-on demo,

87
12:57:55 --> 12:58:01
usually people acquire tweets by accessing
data through an api that twitter provides.

88
12:58:01 --> 12:58:04
Very typically a certain volume of tweets,
lets say for

89
12:58:04 --> 12:58:09
every 5 minutes, is accumulated into
a .json file in a specific directory.

90
12:58:09 --> 12:58:11
The next 5 minutes,
in another .json file, and so forth.

91
12:58:13 --> 12:58:16
The way to get this
data into asterisks DB,

92
12:58:16 --> 12:58:19
is to first create an empty
data set called Tweets here.

93
12:58:20 --> 12:58:22
The next task is to create a feed.

94
12:58:23 --> 12:58:25
That is an externally resource.

95
12:58:25 --> 12:58:30
One has to specify that it's coming from
the local file system called local fs here

96
12:58:31 --> 12:58:34
and the location of the directory,
the format and

97
12:58:34 --> 12:58:36
the data type it's going to copy it.

98
12:58:36 --> 12:58:40
Next, the feed is connected
to the data set and

99
12:58:40 --> 12:58:42
the system starts reading unread
files from the directory.

100
12:58:44 --> 12:58:50
Another way for AsteriskDB to access
external data is directly from an API,

101
12:58:50 --> 12:58:52
such as the Twitter API.

102
12:58:52 --> 12:58:56
To do this,
one would create a dataset as before.

103
12:58:56 --> 12:59:00
But this time the data feed is
not on the local file system.

104
12:59:01 --> 12:59:06
Instead it uses the push Twitter
method which invokes the Twitter

105
12:59:06 --> 12:59:10
client with the four authentication
parameters required by the API.

106
12:59:12 --> 12:59:15
Once the feed is defined it is
connected to the data set as before.

1
01:50:26 --> 01:50:29
Now we move to another Apache product for

2
01:50:29 --> 01:50:31
large scale text data
searching called Solr.

3
01:50:33 --> 01:50:34
Systems like Solr, and

4
01:50:34 --> 01:50:40
its underlying text indexing engine, are
typically designed for search problems.

5
01:50:40 --> 01:50:42
So they would typically be
part of a search engine.

6
01:50:43 --> 01:50:45
But before we talk about Solr or

7
01:50:45 --> 01:50:50
large scale text, we need to first
appreciate some fundamental challenges

8
01:50:50 --> 01:50:53
when it comes to storing,
indexing, and matching text data.

9
01:50:55 --> 01:51:00
The basic challenge comes from
the numerous ways in which

10
01:51:00 --> 01:51:04
a text string may vary, making it
hard to define what a good match is.

11
01:51:05 --> 01:51:07
Let us show some of these challenges.

12
01:51:08 --> 01:51:13
Remember in each case,
we're asking whether the strings shown on

13
01:51:13 --> 01:51:16
either side of the double
tilde sign should match.

14
01:51:19 --> 01:51:22
The first issue is about spelling
variations and capitalization.

15
01:51:26 --> 01:51:29
The second issue relates
structured strings.

16
01:51:29 --> 01:51:34
Where different parts of a string
represent different kind of information.

17
01:51:34 --> 01:51:40
We have seen this before for file paths,
URLs, and like in this case, product IDs.

18
01:51:40 --> 01:51:45
The problem is that the searcher may
not always know the structure or

19
01:51:45 --> 01:51:48
my job on misposition,
the internal punctuations.

20
01:51:52 --> 01:51:56
The next problem is very common with
proper nouns which are represented in

21
01:51:56 --> 01:52:02
various ways, including dropping a part of
the string, picking up only the initials.

22
01:52:02 --> 01:52:07
But consider the last variation, should
BH Obama really match the full name?

23
01:52:10 --> 01:52:14
The next example is about
frequently used synonyms.

24
01:52:15 --> 01:52:19
If the document has one of the synonyms,
while the query uses another,

25
01:52:19 --> 01:52:20
should they match?

26
01:52:23 --> 01:52:27
Example 5 illustrates
a very common problem.

27
01:52:27 --> 01:52:29
People use abbreviations all the time.

28
01:52:30 --> 01:52:33
If we look at text and social media and

29
01:52:33 --> 01:52:36
instant messaging, we see a much
wider variety of abbreviations.

30
01:52:38 --> 01:52:41
How well should they met with
the real correct version of the term?

31
01:52:43 --> 01:52:46
Now problem six is a special
case of problem five.

32
01:52:48 --> 01:52:50
Many long nouns are shortened

33
01:52:50 --> 01:52:54
because we take the first initial
letter of each significant word.

34
01:52:55 --> 01:53:01
We say significant because we
drop words like of as shown here.

35
01:53:01 --> 01:53:02
This is called initialism.

36
01:53:04 --> 01:53:08
Just so you know, when an initialism
can be said like a real word,

37
01:53:08 --> 01:53:09
it's called an acronym.

38
01:53:10 --> 01:53:15
Thus IBM is an initialization,
but NATO is an acronym.

39
01:53:17 --> 01:53:22
Now problem seven and
problem eight show two

40
01:53:22 --> 01:53:26
different situation where we first must
decide what to do with the period sign.

41
01:53:28 --> 01:53:33
In the first case, we should not
find a match because students and

42
01:53:33 --> 01:53:37
American are in two different
sentences punctuated by a period sign.

43
01:53:38 --> 01:53:42
But in the second case we should
find a match because the period

44
01:53:42 --> 01:53:44
does not designate a sentence boundary.

45
01:53:47 --> 01:53:52
Lucene, the engine on which Solr is
built is effectively not a database,

46
01:53:52 --> 01:53:54
but a modern inverted index.

47
01:53:55 --> 01:53:56
What's an inverted index?

48
01:53:57 --> 01:54:01
Let's first define a vocabulary
as a collection of terms.

49
01:54:01 --> 01:54:06
Where a term may be a single word or
it can be multiple words.

50
01:54:06 --> 01:54:09
It can a single term or
it can be a collection of synonyms.

51
01:54:10 --> 01:54:12
But how would a search
engine know what a term is?

52
01:54:13 --> 01:54:16
We'll revisit this question
in a couple of slides.

53
01:54:16 --> 01:54:21
For now, let's just say that if we
have a corpus of documents, we can

54
01:54:21 --> 01:54:25
extract most of the terms and construct
a vocabulary for that collection.

55
01:54:27 --> 01:54:32
We can then define occurrence, as a list

56
01:54:32 --> 01:54:36
containing all the information necessary
for each term in the vocabulary.

57
01:54:37 --> 01:54:41
It would include information like,
which documents have the term?

58
01:54:41 --> 01:54:45
The positions in the document
where the term occurs.

59
01:54:45 --> 01:54:49
We can then go on to compute the count of
the term in the document and the corpus.

60
01:54:50 --> 01:54:54
Referring back to a previous module
in this course, we can also compute

61
01:54:54 --> 01:54:58
the term frequency and inverse
document frequency for the collection.

62
01:55:00 --> 01:55:04
An inverted index is
essentially an index which for

63
01:55:04 --> 01:55:09
every term stores at least the ID of
the document where the term occurs.

64
01:55:09 --> 01:55:12
Practically, other computed numbers or

65
01:55:12 --> 01:55:16
properties associated with the terms
will also be included in the index.

66
01:55:18 --> 01:55:22
Solr is an open source
enterprise search platform.

67
01:55:22 --> 01:55:28
The heart of Solr is its search
functionality built for full text search.

68
01:55:28 --> 01:55:31
However, Solr provides much
more than tech search.

69
01:55:32 --> 01:55:38
It can take any structured document,
even CSV files

70
01:55:38 --> 01:55:43
which can be broken up into fields,
and can index each field separately.

71
01:55:44 --> 01:55:48
Full text indexes where text columns
are supplemented by indexes for

72
01:55:48 --> 01:55:53
other types of data, including numeric
data, dates, geographic coordinates, and

73
01:55:53 --> 01:55:57
fields where domains are limited
to an emergent set of values.

74
01:55:59 --> 01:56:04
Solr provides other facilities
like faceted search and

75
01:56:04 --> 01:56:07
highlighting of terms that match a query.

76
01:56:07 --> 01:56:10
Now if you're not familiar
with the term faceted search,

77
01:56:10 --> 01:56:14
let's look at the screenshot
from amazon.com.

78
01:56:14 --> 01:56:16
I performed a search on the string,
Dell laptop.

79
01:56:18 --> 01:56:20
Consider the highlighted
part of the image.

80
01:56:22 --> 01:56:26
Each laptop record carries a lot of
attributes like the display size,

81
01:56:26 --> 01:56:28
the processor speed,
the amount of memory, and so forth.

82
01:56:29 --> 01:56:34
These attributes can be put into builds
like processor type has Intel i5, i7, etc.

83
01:56:35 --> 01:56:41
Faceted search essentially extracts
the individual values of these fields and

84
01:56:41 --> 01:56:46
displays them back to the user, usually
with a count of the number of records.

85
01:56:46 --> 01:56:50
We see this in the upper
part of the marked portion,

86
01:56:50 --> 01:56:55
which says there are 5619
laptops per 411 tablets.

87
01:56:55 --> 01:56:56
These are called facets.

88
01:56:57 --> 01:57:01
If a user clicks on a facet,
documents with only those values,

89
01:57:01 --> 01:57:04
that's just the tablets,
will be presented back to the user.

90
01:57:06 --> 01:57:09
Now let's get back to the question
of what a term is, and

91
01:57:09 --> 01:57:11
how a Solr system should know it.

92
01:57:13 --> 01:57:19
Solr allows the system designer to
specify how to parse a document,

93
01:57:19 --> 01:57:25
by instructing how to tokenize
a document and how to filter it.

94
01:57:25 --> 01:57:30
Tokenization is the process of
breaking down the characters read.

95
01:57:30 --> 01:57:34
For example, one can break
the stream at white spaces, and

96
01:57:34 --> 01:57:37
get all the words as tokens.

97
01:57:37 --> 01:57:41
Then, it can filter out the punctuation
like the period, the apostrophe, and so

98
01:57:41 --> 01:57:43
on, just to get the pure words.

99
01:57:45 --> 01:57:48
The code snippet on the right
essentially achieves this.

100
01:57:49 --> 01:57:53
It uses a standard tokenizer that gets
the words with immediate punctuation.

101
01:57:55 --> 01:57:58
The first filter removes the punctuations.

102
01:57:58 --> 01:58:02
The second filter turns
everything into lowercase.

103
01:58:02 --> 01:58:05
And the third filter uses
a synonym file to ensure that

104
01:58:05 --> 01:58:10
all the synonyms get the same
token after ignoring the case.

105
01:58:10 --> 01:58:14
The last filter removes common
English words like a and the.

106
01:58:16 --> 01:58:20
While a similar process would need to
happen when we get the query string.

107
01:58:22 --> 01:58:25
It will also need to go through a
tokenization and token filtering process.

108
01:58:27 --> 01:58:31
In the query analyzer example, there
are six filters within the tokenizer.

109
01:58:33 --> 01:58:36
We use a pattern tokenizer which
will remove the white spaces and

110
01:58:36 --> 01:58:39
periods and semi-colon.

111
01:58:39 --> 01:58:43
The common grams filter creates
tokens out of pairs of terms, and

112
01:58:43 --> 01:58:47
in doing so, makes sure that words
in the stopword file are used.

113
01:58:47 --> 01:58:48
So if we have the string,

114
01:58:48 --> 01:58:53
the cat, the term the should
not be ignored in this filter.

115
01:58:55 --> 01:58:58
Now, the filters here
are executed in order.

116
01:59:00 --> 01:59:03
After the common grams
filter is already done,

117
01:59:03 --> 01:59:06
the next filter removes all
the stopwords in the file.

118
01:59:08 --> 01:59:13
The fifth filter makes sure that if
the queries coming from a web form,

119
01:59:13 --> 01:59:15
all the HTML characters are stripped off.

120
01:59:16 --> 01:59:21
Finally, the remaining words are stemmed,
so that runs and

121
01:59:21 --> 01:59:25
running in the query would match
the word run in the document.

122
01:59:28 --> 01:59:32
We'll end this section with a discussion
on Solr queries, that is searches.

123
01:59:36 --> 01:59:40
We present a CSV file with nine
records and seven attributes.

124
01:59:42 --> 01:59:47
We issue queries against
the system by posing a web query.

125
01:59:47 --> 01:59:51
In these examples, we show some of
the queries one can post to the system.

126
01:59:53 --> 01:59:56
This will be covered in more detail
during the hands-on session.

127
01:59:57 --> 02:00:03
Just notice that the q equal to is a query
and the fl is what you want to back.

1
03:50:28 --> 03:50:33
Listens on activity who will be performing
queries assessing the Postgres database.

2
03:50:33 --> 03:50:37
First, will open a terminal window and
start the Postgres shell.

3
03:50:37 --> 03:50:41
Next, we will look at table and
column definitions in database.

4
03:50:42 --> 03:50:46
Who’s in query the content
of the buy-clicks table?

5
03:50:46 --> 03:50:51
And see how to do this query, but filter
specific rows and columns from the table.

6
03:50:51 --> 03:50:52
Next, we will perform average and

7
03:50:52 --> 03:50:55
some aggregation operations
on a specific column.

8
03:50:57 --> 03:50:57
And finally,

9
03:50:57 --> 03:51:00
we will see how to combine two tables
by joining them on a single column.

10
03:51:02 --> 03:51:04
Let's begin.

11
03:51:04 --> 03:51:07
First, click on the terminal
icon at the top of the toolbar

12
03:51:07 --> 03:51:08
to open a Terminal window.

13
03:51:10 --> 03:51:15
Next, let's start the Postgres
shell by running psql.

14
03:51:18 --> 03:51:23
The Postgres shell allows us to
enter queries and run commands for

15
03:51:23 --> 03:51:25
the postgres database.

16
03:51:25 --> 03:51:29
We can see what tables are in
the database by running \d.

17
03:51:33 --> 03:51:38
This says that there are three
tables in the database,

18
03:51:38 --> 03:51:41
adclicks, buyclicks and gameclicks.

19
03:51:41 --> 03:51:46
We could use \d table name to see
the definition of one of these tables.

20
03:51:47 --> 03:51:53
Let's look at the definition of buyclicks,

21
03:51:53 --> 03:51:57
we enter \d buyclicks.

22
03:51:59 --> 03:52:02
This shows that there's seven
columns in the database.

23
03:52:03 --> 03:52:08
These are the column names and here's
the data type for each of the columns.

24
03:52:11 --> 03:52:13
Now let's look at the contents
of the buyclicks table.

25
03:52:15 --> 03:52:23
We can query the contents by running
the command select * from buyclicks;.

26
03:52:23 --> 03:52:27
The select says that
we want to do a query.

27
03:52:27 --> 03:52:30
The star means we want to
retrieve all the columns and

28
03:52:30 --> 03:52:34
from buyclicks says,
which table to perform the query from.

29
03:52:35 --> 03:52:41
And finally, all commands in Postgres
shell need to end with a semicolon.

30
03:52:41 --> 03:52:46
When we run this command we see
the contents of the buyclicks table.

31
03:52:48 --> 03:52:51
Column header is at the top and
the contents are below.

32
03:52:53 --> 03:52:58
Again hit Space to scroll through
the contents, and hit Q when we're done.

33
03:53:01 --> 03:53:04
Now, let's view the contents
of only two of the columns.

34
03:53:05 --> 03:53:09
Let's query only the price and
the user id from the buyclicks table.

35
03:53:09 --> 03:53:16
To do this we run select price,
userid from buyclicks.

36
03:53:19 --> 03:53:22
This command says,
we want to query only the price and

37
03:53:22 --> 03:53:24
userid columns from the buyclicks table

38
03:53:27 --> 03:53:32
When we run this,
We only get those two columns.

39
03:53:34 --> 03:53:37
We can also perform queries that
just select certain rows and

40
03:53:37 --> 03:53:39
meet a certain criteria.

41
03:53:40 --> 03:53:41
For example,

42
03:53:41 --> 03:53:46
let's make a query that only shows
the rows containing the price over $10.

43
03:53:47 --> 03:53:52
You can do this by running select price,

44
03:53:52 --> 03:53:57
userid from buyclicks where price > 10.

45
03:53:57 --> 03:54:02
This query says, we only want to see
the price and userid columns from

46
03:54:02 --> 03:54:07
the buyclicks table where the value in the
price column has a value greater than 10.

47
03:54:10 --> 03:54:16
We run this, we see that we always
have price values greater than 10.

48
03:54:18 --> 03:54:22
The SQL language has a number of
aggregation operations built into it.

49
03:54:24 --> 03:54:31
For example,
we can take the average price by running,

50
03:54:31 --> 03:54:36
select avg(price) from buyclicks.

51
03:54:36 --> 03:54:44
This command will show the average price
from all the data in the buyclicks table.

52
03:54:44 --> 03:54:47
Another aggregate operation is sum.

53
03:54:47 --> 03:54:52
We can see the total price by running,

54
03:54:52 --> 03:54:57
select sum(price) from buyclicks.

55
03:55:01 --> 03:55:06
We can also combine two tables
by joining on a common column.

56
03:55:06 --> 03:55:11
If you recall we have three
tables in our database adclicks,

57
03:55:11 --> 03:55:13
buyclicks and gameclicks.

58
03:55:14 --> 03:55:22
We look at the description the definition
of adclicks buy running \d adclicks.

59
03:55:26 --> 03:55:31
You can see that it also
has a column called userid.

60
03:55:31 --> 03:55:36
Let's combine buyclicks and
adclicks based on this common column.

61
03:55:37 --> 03:55:43
You can do this by running, select adid,

62
03:55:43 --> 03:55:50
buyid, adclicks.userid from adclicks join

63
03:55:50 --> 03:55:58
buyclicks on adclicks.userid
= buyclicks.userid

64
03:56:00 --> 03:56:05
This query says,
we want to just see the adid, buyid and

65
03:56:05 --> 03:56:10
userid columns, and
we want to combine the adclicks table,

66
03:56:10 --> 03:56:14
the buyclicks table, and
we will be combining them on

67
03:56:14 --> 03:56:19
the userid column and
it's common in both those tables.

68
03:56:22 --> 03:56:25
When we run it,
you see just the three columns.

1
07:46:54 --> 07:46:57
Next we'll consider queries
where two tables are used.

2
07:46:58 --> 07:47:00
Let's consider the query,

3
07:47:00 --> 07:47:05
find the beers liked by drinkers who
frequent The Great American Bar.

4
07:47:07 --> 07:47:13
For this query, we need
the relation's Frequents and Likes.

5
07:47:13 --> 07:47:16
Now look at the scheme of these
relations in the light blue box.

6
07:47:17 --> 07:47:20
They have a common
attribute called drinker.

7
07:47:21 --> 07:47:24
So if we use the attribute drinker,

8
07:47:24 --> 07:47:27
we need to tell the system
which one we are referring to.

9
07:47:29 --> 07:47:36
Now look at the SQL query, the FROM clause
in the query has these two relations.

10
07:47:36 --> 07:47:38
To handle a common attribute name issue,

11
07:47:38 --> 07:47:43
we need to give nicknames,
aliases to these relations.

12
07:47:43 --> 07:47:48
Therefore in the FROM clause we say,
Likes has the alias L and

13
07:47:48 --> 07:47:50
Frequents has the alias F.

14
07:47:51 --> 07:47:57
Since we want to find beers like before,
we use a SELECT DISTINCT clause for beer.

15
07:47:58 --> 07:48:03
As we saw before, using SELECT DISTINCT
avoids duplicates in the result.

16
07:48:05 --> 07:48:08
The WHERE clause has two
kinds of conditions,

17
07:48:09 --> 07:48:14
the first kind is
a single table condition.

18
07:48:14 --> 07:48:22
In this case, bar = The Great American Bar
on the Frequents relation.

19
07:48:23 --> 07:48:30
The second kind is a joined condition
which says that the drinker's attribute in

20
07:48:30 --> 07:48:37
the frequency relation is the same as the
drinker's attribute of the Likes relation.

21
07:48:38 --> 07:48:42
We encode this in SQL in the last
line of the query using aliases.

22
07:48:44 --> 07:48:49
Why did we not say L.beer in the SELECT
clause or F.bar in the first condition?

23
07:48:49 --> 07:48:52
We could have,
the query would have been equally right.

24
07:48:53 --> 07:48:58
But we are using a shortcut because we
know that these attributes are unique

25
07:48:58 --> 07:48:59
already in the query.

26
07:49:01 --> 07:49:03
Now let's look at the query again,

27
07:49:03 --> 07:49:06
this time from the viewpoint
of evaluating the query.

28
07:49:08 --> 07:49:11
There are many ways to evaluate the query,
but

29
07:49:11 --> 07:49:13
the way it's most likely
to be evaluated is this.

30
07:49:15 --> 07:49:20
The query will first look at the tables
that have single table conditions.

31
07:49:21 --> 07:49:24
So it would perform a select operation

32
07:49:25 --> 07:49:30
on the Frequents table to match
the records of the condition

33
07:49:30 --> 07:49:33
that The Great American Bar
equal to The Great American Bar.

34
07:49:34 --> 07:49:35
Why is this strategy good?

35
07:49:37 --> 07:49:40
It's because the selection operative
reduces the number of triples to consider.

36
07:49:41 --> 07:49:45
Thus, if there are thousand
triples in the relation frequents,

37
07:49:45 --> 07:49:47
maybe 60 of them matches the desired bar.

38
07:49:49 --> 07:49:55
So in the next step, we have to deal with
a fewer number of records than thousand.

39
07:49:56 --> 07:50:01
All right, the next step will
be a Join with a Likes relation.

40
07:50:01 --> 07:50:04
A Join requires two relations
in a Join condition,

41
07:50:06 --> 07:50:07
the Join condition comes from the query.

42
07:50:09 --> 07:50:13
The first relation shown with
an underscore symbol here

43
07:50:16 --> 07:50:19
is a result of the previous operation.

44
07:50:19 --> 07:50:23
Another way of saying this
is that the result of

45
07:50:23 --> 07:50:28
the selection is piped
into the Join operation.

46
07:50:28 --> 07:50:31
That means we do not create

47
07:50:31 --> 07:50:35
an intermediate table from
the result of the selection.

48
07:50:35 --> 07:50:41
The results are directly supplied to the
next operator, which in this case is Join.

49
07:50:43 --> 07:50:48
Now the result of the Join operator is an
intermediate structure with columns beer

50
07:50:48 --> 07:50:49
from Likes relation and

51
07:50:49 --> 07:50:53
the drinker from the Frequents
relation that we've processed.

52
07:50:55 --> 07:51:00
This intermediate set of triples is
piped to the Project operation that

53
07:51:00 --> 07:51:02
picks up the beer column.

54
07:51:04 --> 07:51:09
Now we need to process
the DISTINCT clause for

55
07:51:09 --> 07:51:14
Deduplicate elimination,
which then goes to the Output.

56
07:51:16 --> 07:51:20
We have already seen how the select
project queries on single tables

57
07:51:20 --> 07:51:24
are evaluated when the tables
are partitioned across several machines.

58
07:51:25 --> 07:51:29
We'll now see how we process Join
queries in the same setting.

59
07:51:30 --> 07:51:36
For our case, consider that the Likes and

60
07:51:36 --> 07:51:40
Frequents tables are on
two different machines.

61
07:51:41 --> 07:51:43
In the first part of the query,

62
07:51:43 --> 07:51:46
the selection happens on the machine
with the Frequents table.

63
07:51:47 --> 07:51:52
The output of the query is a smaller
table with the same scheme as Frequents,

64
07:51:52 --> 07:51:55
that is with drinkers and bars.

65
07:51:55 --> 07:51:57
Now we define an operation
called Semijoin,

66
07:51:59 --> 07:52:03
in which we need to move data
from one machine to another.

67
07:52:05 --> 07:52:09
The goal of the Semijoin operation is
to reduce the cost of data movement.

68
07:52:10 --> 07:52:14
That is to move data from
the machine which has

69
07:52:14 --> 07:52:19
the Frequents data to
the machine with the Likes data.

70
07:52:19 --> 07:52:22
The cost is reduced if we ship less data.

71
07:52:22 --> 07:52:30
The way to it is to first find which
data the join operation actually needs.

72
07:52:30 --> 07:52:35
Clearly, it needs only the drinkers
column and not the bars column.

73
07:52:35 --> 07:52:41
So the drinkers column is projected out,
then

74
07:52:41 --> 07:52:44
just this column is transmitted
to the second machine.

75
07:52:46 --> 07:52:51
Finally, the join is performed by looking
at the values in the Likes column

76
07:52:51 --> 07:52:54
that only matches the values
in the shipped data.

77
07:52:56 --> 07:53:01
That means only the data from Likes that
matches the drinkers that are chosen.

78
07:53:03 --> 07:53:07
These are then the join results which
would go to the output of the operation.

79
07:53:08 --> 07:53:11
Now here you can see the Semijoin
operation graphically.

80
07:53:12 --> 07:53:16
The red table on the left is the output
of the selection operations on the left.

81
07:53:19 --> 07:53:23
The white table on the right
is the table to be joined to.

82
07:53:25 --> 07:53:28
Since we need only the Drinkers column,

83
07:53:28 --> 07:53:30
it is projected to create
a one-column relation.

84
07:53:32 --> 07:53:38
Notice that the red table has two entries
for Pete, who frequented two bars.

85
07:53:38 --> 07:53:41
But the output of the project
is condensed in the yellow table

86
07:53:41 --> 07:53:44
to just show the Drinkers,
where Pete appears only once.

87
07:53:46 --> 07:53:49
For those of you with
a background in computer science,

88
07:53:49 --> 07:53:52
this can be done using a hash
map like data structure.

89
07:53:54 --> 07:53:58
This one-column table is now shipped
to Site2, which has the Likes relation.

90
07:53:59 --> 07:54:03
Now at Site2, the Shipped relation
is used to find matches from

91
07:54:03 --> 07:54:08
the Drinkers column and
it finds only one match called Sally.

92
07:54:08 --> 07:54:11
So the corresponding result triples,
in this case,

93
07:54:11 --> 07:54:15
only one triple is produced
at the end of this operation.

94
07:54:15 --> 07:54:21
Now, the original table and
the matching table are shipped

95
07:54:21 --> 07:54:27
to the last of the operation
to finish the Join operation.

96
07:54:27 --> 07:54:31
And more efficient version of
this is shown in the next slide.

97
07:54:32 --> 07:54:39
In this version, the first two
steps this and that are the same.

98
07:54:40 --> 07:54:44
Then the result of the reduce
is also shipped to Site1 to find

99
07:54:44 --> 07:54:47
the matches from the red relation.

100
07:54:48 --> 07:54:52
Another reduce operation
is performed on Site1 now

101
07:54:52 --> 07:54:55
to get the matching records
on the red relation.

102
07:54:55 --> 07:54:58
Finally, these two reduced
relations are shipped to

103
07:54:58 --> 07:55:01
the site where the final join happens.

104
07:55:01 --> 07:55:03
And all of this may seem
like a lot of detail.

105
07:55:05 --> 07:55:07
Let me repeat something I've said before.

106
07:55:09 --> 07:55:15
If we have a system like DB2 or Spark SQL
that implements multi-site joins, it

107
07:55:15 --> 07:55:19
will perform this kind of operation under
the hood, you don't have to know them.

108
07:55:20 --> 07:55:26
However, if we were to implement
a similar operation and all that you have

109
07:55:26 --> 07:55:30
is Hadoop, you may end up implementing
this kind of algorithm yourself.

1
15:42:23 --> 15:42:28
The queries in real life are little more
complex than what we have seen before.

2
15:42:29 --> 15:42:31
So let's consider a more complex query.

3
15:42:33 --> 15:42:39
Let’s find bars where the price
of Miller is the same as or

4
15:42:39 --> 15:42:43
less than what the great American bar
called TGAB here charges for Bud.

5
15:42:45 --> 15:42:50
You may say, but we do not really
know what TGAB charges for Bud.

6
15:42:50 --> 15:42:56
That's correct, so
we can break up the query into two parts.

7
15:42:56 --> 15:43:04
First, we will find this unknown price,
and then we'll use that price

8
15:43:04 --> 15:43:09
to find the bars that would sell Miller
for the same price or better price.

9
15:43:09 --> 15:43:14
Now this is a classic situation where
the result from the first part of

10
15:43:14 --> 15:43:18
the query should be fed as
a parameter to the second query.

11
15:43:18 --> 15:43:21
Now this situation is called a subquery.

12
15:43:23 --> 15:43:26
We write this in SQL as
shown in the slide here.

13
15:43:27 --> 15:43:32
What makes this query different is that
the part where price is less than equal

14
15:43:32 --> 15:43:37
to, instead of specifying
a constant like $8,

15
15:43:37 --> 15:43:43
we actually place another query which
computes the price of a Bud at TGAB.

16
15:43:46 --> 15:43:50
The shaded part is called the inner query,
or the subquery.

17
15:43:51 --> 15:43:54
In this case, both the outer query and

18
15:43:54 --> 15:43:58
the inner query use the same relation,
which is Sells.

19
15:43:59 --> 15:44:05
Now in terms of evaluation,
the inner query is evaluated first,

20
15:44:05 --> 15:44:07
and the outer query uses its output.

21
15:44:08 --> 15:44:12
Now while it may not be
obvious at this time,

22
15:44:12 --> 15:44:15
notice that the inner query is
independent of the outer query.

23
15:44:16 --> 15:44:18
In other words,

24
15:44:18 --> 15:44:22
even if we did not have the outer query,
we can still evaluate the inner query.

25
15:44:24 --> 15:44:28
We say in this case that
the subquery is uncorrelated.

26
15:44:31 --> 15:44:33
Let's look at another
example of a subquery.

27
15:44:34 --> 15:44:37
In this example,
we want to find the name and

28
15:44:37 --> 15:44:41
manufacturer of each beer
that Fred didn't like.

29
15:44:41 --> 15:44:44
So how do we know what Fred didn't like?

30
15:44:44 --> 15:44:49
We do however know that the set of
beers that Fred likes because they

31
15:44:49 --> 15:44:51
are listed in the Likes relation.

32
15:44:51 --> 15:44:56
So we need to subtract this set from the
total set of beers that the company has

33
15:44:56 --> 15:44:57
recorded.

34
15:44:58 --> 15:45:02
This subtraction of sets can
be performed in several ways.

35
15:45:02 --> 15:45:05
One of them is to use
the NOT IN construct.

36
15:45:07 --> 15:45:12
So the query class's job is to take
every name from the Beers table and

37
15:45:12 --> 15:45:17
output it only if it does not appear
in the set produced by the inner query.

38
15:45:19 --> 15:45:23
Similar to the previous query,
the subquery here is also uncorrelated.

39
15:45:26 --> 15:45:28
Now this is a more sophisticated query.

40
15:45:29 --> 15:45:33
The intention is to find
beers that are more expensive

41
15:45:33 --> 15:45:35
than the average price of beer.

42
15:45:35 --> 15:45:39
But since beers have different
prices in different bars,

43
15:45:39 --> 15:45:41
we have to find the average for every bar.

44
15:45:42 --> 15:45:46
Therefore the idea is to find
the average price of beer for

45
15:45:46 --> 15:45:51
every bar and then compare the price of
each beer with respect to this average.

46
15:45:52 --> 15:45:54
Now look at the query and the table.

47
15:45:56 --> 15:45:59
Let's assume we are processing
the first table.

48
15:46:01 --> 15:46:05
The beer is Bud, and the price is $5.

49
15:46:05 --> 15:46:12
Now we need to know if $5 is greater than
the average price of beer sold at HGAT.

50
15:46:12 --> 15:46:17
To do this,
we need to compute the inner query, okay?

51
15:46:17 --> 15:46:19
So now let's look at the fourth row.

52
15:46:19 --> 15:46:25
The price of Guinness needs to be
compared to that average again for HGAT.

53
15:46:26 --> 15:46:29
In fact for
every table processed by the outer query,

54
15:46:29 --> 15:46:32
one needs to compute the inner query for
that bar.

55
15:46:34 --> 15:46:38
This makes the inner subquery
correlated with the outer query.

56
15:46:39 --> 15:46:44
Now a smart query processor will store
the average once it's computed and

57
15:46:44 --> 15:46:47
then reuse the stored value instead
of computing over and over again.

58
15:46:51 --> 15:46:53
What's an aggregate query?

59
15:46:53 --> 15:46:58
Let's use a simple example of
finding the average price of Bud.

60
15:46:59 --> 15:47:04
This is like a simple select project
query with the additional aspect

61
15:47:04 --> 15:47:08
that it takes a list of price values
of Bud from different bars and

62
15:47:08 --> 15:47:09
then computes an average.

63
15:47:10 --> 15:47:14
In the example shown,
the average of the five prices is 4.2.

64
15:47:14 --> 15:47:17
In other words the average function,

65
15:47:17 --> 15:47:22
the AVG function, takes a list of
values and produces a single value.

66
15:47:22 --> 15:47:25
Now there are many functions
that have this behavior.

67
15:47:25 --> 15:47:28
The SUM function takes
a list of values and

68
15:47:28 --> 15:47:30
adds them up to produce a single value.

69
15:47:30 --> 15:47:33
The COUNT function takes
a list of list of values and

70
15:47:33 --> 15:47:35
counts the number of items
in that list and so on.

71
15:47:38 --> 15:47:41
These are called aggregate functions.

72
15:47:43 --> 15:47:48
Now if we wanted to count only the price
values that are different, that is 3,

73
15:47:48 --> 15:47:53
4, and 5 just once, we can write
the SELECT clause a little differently.

74
15:47:55 --> 15:48:00
We would say that the average
is over distinct values

75
15:48:00 --> 15:48:05
of price which in this
case will result in 4.

76
15:48:05 --> 15:48:10
You should recognize that most analytical
operations need to use statistical

77
15:48:10 --> 15:48:12
functions which are aggregates.

78
15:48:14 --> 15:48:16
So another important
analytical requirement

79
15:48:16 --> 15:48:20
is computing the statistical
aggregate by groups.

80
15:48:20 --> 15:48:24
For example, we often compute the average
salaries of employees per department.

81
15:48:25 --> 15:48:31
Now back to our example here,
we want to find the average price paid for

82
15:48:31 --> 15:48:37
Bud per drinker, where we know
that a drinker visits many bars.

83
15:48:37 --> 15:48:40
So the grouping variable here is drinker.

84
15:48:41 --> 15:48:46
So we have three attributes at play,
price which we need to aggregate,

85
15:48:46 --> 15:48:51
drinker which we need to group by,
and bar which is a join attribute.

86
15:48:52 --> 15:48:56
The fourth attribute, namely beer,
is used for selection and

87
15:48:56 --> 15:48:59
does not participate in grouping.

88
15:48:59 --> 15:49:04
So after the selection we will
get an intermediate relation

89
15:49:04 --> 15:49:07
containing drinker, bar, and price.

90
15:49:08 --> 15:49:12
With this, the GROUP BY operation
will create one result row for

91
15:49:12 --> 15:49:17
each drinker and place the average
price over all such rows.

92
15:49:19 --> 15:49:21
Now how does GROUP BY and

93
15:49:21 --> 15:49:23
aggregate computation work
when the data is partitioned?

94
15:49:24 --> 15:49:25
Let's take the same query.

95
15:49:26 --> 15:49:31
We are looking for the average
price of Bud grouped by drinker.

96
15:49:33 --> 15:49:39
But this time the result of the selection
are in two different machines.

97
15:49:39 --> 15:49:42
Imagine that this time they are range
partitioned by row numbers,

98
15:49:42 --> 15:49:44
which we have not shown
to maintain clarity.

99
15:49:45 --> 15:49:49
Now with the GROUP BY operation the data
will get repartitioned by the grouping

100
15:49:49 --> 15:49:50
attribute, that's drinker.

101
15:49:53 --> 15:49:59
And then the aggregate
function is computed locally.

102
15:50:00 --> 15:50:06
To accomplish this repartitioning task,
each machine groups its own data locally,

103
15:50:06 --> 15:50:11
determines which portions of data should
be transmitted to a different machine,

104
15:50:11 --> 15:50:14
and accordingly ships it to that machine.

105
15:50:15 --> 15:50:18
Now there are several variants
of this general scheme

106
15:50:18 --> 15:50:20
which are even more efficient.

107
15:50:20 --> 15:50:24
Now if this reminds you of the map
operation you saw in your previous course,

108
15:50:24 --> 15:50:25
you are exactly right.

109
15:50:26 --> 15:50:32
This fundamental process of grouping,
partitioning, and redistribution of data

110
15:50:32 --> 15:50:37
is inherent in data-parallel computing and
implemented inside database systems.

1
07:33:01 --> 07:33:01
Welcome back.

2
07:33:02 --> 07:33:07
In this video, we will provide you
a quick summary of the main points from

3
07:33:07 --> 07:33:11
our last course on big data modeling and
management.

4
07:33:11 --> 07:33:15
If you had just completed our second
course and do not need a refresher,

5
07:33:15 --> 07:33:18
you may now skip to the next lecture.

6
07:33:18 --> 07:33:23
After this video, you will be able
to recall why big data modeling and

7
07:33:23 --> 07:33:28
management is essential in preparing
to gain insights from your data,

8
07:33:28 --> 07:33:31
summarize different kids of data models.

9
07:33:31 --> 07:33:37
Describe streaming data and the different challenges
it presents, and explain the differences

10
07:33:37 --> 07:33:42
between a database management system and
a big data management system.

11
07:33:45 --> 07:33:50
In the second course, we described
a data model as a specification

12
07:33:50 --> 07:33:54
that precisely characterizes
the structure of the data,

13
07:33:54 --> 07:33:59
the operations on the data, and
the constraints that may apply on data.

14
07:34:00 --> 07:34:04
For example, a data model may state that

15
07:34:04 --> 07:34:08
a data is structured like
a two-dimensional array or a matrix.

16
07:34:09 --> 07:34:15
For this structure,
one may have a data access operation,

17
07:34:15 --> 07:34:20
which given an index of the array,
we use the cell of the array to refer to.

18
07:34:23 --> 07:34:27
A data model may also specify
constraints on the data.

19
07:34:27 --> 07:34:32
For example, while a total
data set may have many arrays,

20
07:34:32 --> 07:34:35
the name of each array must be unique and

21
07:34:35 --> 07:34:40
the values of a specific array
must always be greater than zero.

22
07:34:42 --> 07:34:47
Database management systems handle
low level data management operations,

23
07:34:47 --> 07:34:50
help organization of the data
using a data model, and

24
07:34:50 --> 07:34:54
provide an open programmable
access to data.

25
07:34:56 --> 07:35:00
We covered a number of data models.

26
07:35:00 --> 07:35:03
We showed four models that were
discussed in more details.

27
07:35:06 --> 07:35:09
The relational data to date
is the most used data model.

28
07:35:10 --> 07:35:16
Here, data is structured like tables
which are formally called relations.

29
07:35:17 --> 07:35:20
The relational data model has been
implemented in traditional database

30
07:35:20 --> 07:35:22
systems.

31
07:35:22 --> 07:35:27
But they are being refreshly implemented
in modern data systems over Hadoop and

32
07:35:27 --> 07:35:30
Spark and
are getting deployed on cloud platforms.

33
07:35:31 --> 07:35:36
The second category of data gaining
popularity is semi-structured data,

34
07:35:36 --> 07:35:41
which includes documents like HTML pages,
XML data and

35
07:35:41 --> 07:35:44
JSON data that are used by
many Internet applications.

36
07:35:46 --> 07:35:49
This data can have one element nested or

37
07:35:49 --> 07:35:54
embedded within another data element and
hence can often be modeled as a tree.

38
07:35:57 --> 07:36:02
The third category of data
models is called graph data.

39
07:36:02 --> 07:36:07
A graph is a network where
nodes represent entities and

40
07:36:07 --> 07:36:11
edges represent relationships
between pairs of such entities.

41
07:36:12 --> 07:36:18
For example, in a social network,
nodes may represent users and

42
07:36:18 --> 07:36:22
edges may represent their friendship.

43
07:36:22 --> 07:36:26
The operations performed on graph data
includes traversing the network so

44
07:36:26 --> 07:36:31
that one can find friend of
a friend of a friend if needed.

45
07:36:33 --> 07:36:38
In contrast to the previous three models,
that there is a structure to the data,

46
07:36:38 --> 07:36:44
the text data is much more unstructured
because an entire data item

47
07:36:44 --> 07:36:46
like a new article can
be just a text string.

48
07:36:48 --> 07:36:52
However, text is the primary form of data

49
07:36:52 --> 07:36:56
in information retrieval systems or
search engines like Google.

50
07:36:59 --> 07:37:04
We also discussed streaming data,
or data with velocity, as a special

51
07:37:04 --> 07:37:09
class of data that continually come
to the system at some data rate.

52
07:37:11 --> 07:37:16
Examples can be found in data coming
from road sensors that measure traffic

53
07:37:16 --> 07:37:20
patterns or stock price data from
the stock exchange that may come

54
07:37:20 --> 07:37:24
in volumes from stock
exchanges all over the world.

55
07:37:26 --> 07:37:34
Streaming data is special because a stream
is technically an infinite data source.

56
07:37:34 --> 07:37:37
And therefore,
we keep filling up memory and

57
07:37:37 --> 07:37:41
storage and will eventually go
beyond the capacity of any system.

58
07:37:42 --> 07:37:46
Streaming data, therefore, needs
a different kind of management system.

59
07:37:47 --> 07:37:53
For this reason,
streaming data is processed in memory,

60
07:37:53 --> 07:37:56
in chunks which are also called windows.

61
07:37:56 --> 07:38:00
Often only the necessary
part of the data stream or

62
07:38:00 --> 07:38:04
the results of queries against
the data stream is stored.

63
07:38:06 --> 07:38:12
A typical type of query against streaming
data are alerts or notifications.

64
07:38:12 --> 07:38:16
The system notices an event like multiple
stock price changing within a short time.

65
07:38:20 --> 07:38:23
Streaming data is also used for
prediction.

66
07:38:25 --> 07:38:30
For instance, based on wind direction and
temperature data streams,

67
07:38:30 --> 07:38:33
one can predict how a wildfire
is going to spread.

68
07:38:36 --> 07:38:41
In the last course, we also covered
a number of data systems that we called

69
07:38:41 --> 07:38:43
big data management systems.

70
07:38:45 --> 07:38:48
These systems use
different data models and

71
07:38:48 --> 07:38:54
have different capabilities, but
are characterized by some common features.

72
07:38:55 --> 07:38:59
They are also designed from the start for
parallel and distributed processing.

73
07:39:00 --> 07:39:06
Most of them implement data partition
parallelism, which, if you can recall,

74
07:39:06 --> 07:39:10
refers to the process of segmenting
the data into multiple machines so

75
07:39:10 --> 07:39:15
data retrieval and manipulations can be
performed in parallel on these machines.

76
07:39:17 --> 07:39:23
Many of these systems allow a large
number of users who constantly update and

77
07:39:23 --> 07:39:24
query the system.

78
07:39:26 --> 07:39:31
Some of the systems do not maintain
transaction consistency with every update.

79
07:39:32 --> 07:39:33
That means,

80
07:39:33 --> 07:39:38
not all the machines may have all
the updates guaranteed at every moment.

81
07:39:40 --> 07:39:45
However, most of them provide
a guarantee of eventual consistency,

82
07:39:45 --> 07:39:50
which means all the machines will
get all updates sooner or later.

83
07:39:50 --> 07:39:53
Therefore, providing better accuracy and
time.

84
07:39:56 --> 07:40:01
The third common characteristic
of big data management systems is

85
07:40:01 --> 07:40:05
that they are often built on top of
a Hadoop-like platform that provides

86
07:40:05 --> 07:40:10
automatic replication and
a map-reduce style processing ability.

87
07:40:11 --> 07:40:15
Some of the data operations performed
within these systems make use of these

88
07:40:15 --> 07:40:16
lower level capabilities.

89
07:40:19 --> 07:40:21
After this refresher on data modeling and

90
07:40:21 --> 07:40:25
management, let's start big data
integration and processing.

1
15:13:26 --> 15:13:27
So, hi.

2
15:13:27 --> 15:13:32
In the previous course, we saw
examples of different data models and

3
15:13:32 --> 15:13:36
talked about a few current
data management systems.

4
15:13:36 --> 15:13:39
In this module,
we'll focus on data retrieval.

5
15:13:59 --> 15:14:04
Data retrieval refers to
the way in which data desired

6
15:14:04 --> 15:14:08
by a user is specified and
retrieved from a data store.

7
15:14:10 --> 15:14:14
Note that in this course, we are using
the term data retrieval in two ways.

8
15:14:15 --> 15:14:20
Assume that your data is stored in a data
store that follows a specific data model,

9
15:14:20 --> 15:14:22
like for
example the relational data model.

10
15:14:23 --> 15:14:27
By data retrieval, we will refer to, one,

11
15:14:27 --> 15:14:31
the way you specify how to get
the desired data out of the system,

12
15:14:31 --> 15:14:36
this is called the query
specification method, and two,

13
15:14:36 --> 15:14:42
the internal processing that occurs within
the data management system to compute or

14
15:14:42 --> 15:14:45
evaluate that specified retrieval request.

15
15:14:47 --> 15:14:52
While query specification can apply to
small data stores or large data stores,

16
15:14:52 --> 15:14:56
we'll keep an eye on the nature of
query evaluation when the data is big.

17
15:14:58 --> 15:15:02
Further, we'll consider how
the query specification changes

18
15:15:02 --> 15:15:04
when we deal with faster streaming data.

19
15:15:07 --> 15:15:12
A query language is a language in which
a retrieval request is specified.

20
15:15:14 --> 15:15:20
A query language is often called
declarative, which means it lets you

21
15:15:20 --> 15:15:25
specify what you want to retrieve without
having to tell the system how to retrieve.

22
15:15:26 --> 15:15:28
For example, you can say,

23
15:15:28 --> 15:15:34
find all data from relation employee
where the salary is more than 50k.

24
15:15:34 --> 15:15:38
Now, you don't have to write a program
which will tell the system to open a file,

25
15:15:38 --> 15:15:44
skip the first 250 bytes,
then in a loop pick the next 1024 bytes,

26
15:15:44 --> 15:15:48
probe into the 600th byte and
read an integer, and so forth.

27
15:15:49 --> 15:15:52
Instead of writing such
a complicated procedure,

28
15:15:52 --> 15:15:56
you just specify the data items that
you need and the system does the rest.

29
15:15:57 --> 15:16:02
For example, SQL,
structured query language,

30
15:16:02 --> 15:16:06
is the most used query language for
relational data.

31
15:16:06 --> 15:16:11
Now, in contrast to a query language,
a database programming

32
15:16:11 --> 15:16:16
language like Oracle's PL/SQL or
Postgres's PgSQL are high-level

33
15:16:16 --> 15:16:20
procedural programming languages
that embed query operations.

34
15:16:21 --> 15:16:24
We will look at some query
languages in detail and

35
15:16:24 --> 15:16:27
show examples of database
programming languages.

36
15:16:30 --> 15:16:32
The first query language
we'll look at is SQL,

37
15:16:32 --> 15:16:37
which is the ubiquitous query
language when the data is structured,

38
15:16:37 --> 15:16:42
but has been extended in many ways
to accommodate other types of data.

39
15:16:42 --> 15:16:46
For this course, we'll stick to
the structured aspect of the language.

40
15:16:46 --> 15:16:50
Now, you should know that SQL is used for
classical database management systems

41
15:16:50 --> 15:16:55
like Oracle as well as modern Hadoop
style distributed systems such as Spark.

42
15:16:56 --> 15:16:59
Now, we will work with
an illustrative example.

43
15:17:01 --> 15:17:04
First, we need to define
the schema of the database.

44
15:17:04 --> 15:17:09
Now, think of a business called the
Beer Drinkers Club that owns many bars,

45
15:17:09 --> 15:17:10
and each bar sells beer.

46
15:17:12 --> 15:17:16
Our schema for
this business has six relations of tables.

47
15:17:16 --> 15:17:21
The first table lists these bars,
the names, addresses, and

48
15:17:21 --> 15:17:23
the license number of the bar.

49
15:17:23 --> 15:17:28
Notice that the attribute name is
underlined because it is the primary key

50
15:17:28 --> 15:17:29
of the bars relation.

51
15:17:29 --> 15:17:33
Recall that the primary key
refers to a set of attributes,

52
15:17:33 --> 15:17:37
in this case just the name,
that makes a record unique.

53
15:17:39 --> 15:17:41
Note that the relation bars
with the attribute name

54
15:17:43 --> 15:17:46
within parenthesis is the same
as the table shown on the right.

55
15:17:47 --> 15:17:51
We will use both representations
as we go forward.

56
15:17:51 --> 15:17:56
The second table called Beers, this is
the names and manufacturers of beer.

57
15:17:56 --> 15:18:00
Now, not every bar sells the same
brands of beer, and even when they do,

58
15:18:00 --> 15:18:02
they may have different prices for

59
15:18:02 --> 15:18:06
the same product because of differences
in the establishment costs.

60
15:18:07 --> 15:18:12
So the Sells table records which
bar sells which beer at what price.

61
15:18:14 --> 15:18:17
Now, our business is special.

62
15:18:17 --> 15:18:21
It also keeps information about
the regular member customers.

63
15:18:21 --> 15:18:26
So the Drinkers relation has the name,
address, and phone of these customers.

64
15:18:26 --> 15:18:31
Well, not only that,
it knows which member visits

65
15:18:31 --> 15:18:35
which bars and
which beer each member likes.

66
15:18:35 --> 15:18:38
Clearly, the Beer Drinkers Club
knows its customers.

67
15:18:41 --> 15:18:48
The most basic structure of an SQL
query is a SELECT-FROM-WHERE clause.

68
15:18:49 --> 15:18:52
In this example, we're looking for
beer names that are made by Heineken.

69
15:18:54 --> 15:18:57
So we need to specify
our output attribute,

70
15:18:57 --> 15:18:59
in this case the name of the beer.

71
15:18:59 --> 15:19:05
The logical table which will be used to
answer the query, in this case, Beers.

72
15:19:06 --> 15:19:10
And the condition that all the desired
data items should satisfy,

73
15:19:10 --> 15:19:15
namely, the value of the attribute
called manf is Heineken.

74
15:19:16 --> 15:19:19
Now, there are few things to notice here.

75
15:19:19 --> 15:19:23
First, the literal Heineken
is put within quotes,

76
15:19:23 --> 15:19:26
because it's a single string literal.

77
15:19:27 --> 15:19:28
Remember that in this case,

78
15:19:28 --> 15:19:32
the string is supposed to match exactly,
including the case.

79
15:19:34 --> 15:19:40
Secondly, if you go back to the data
operations discussed in course two,

80
15:19:40 --> 15:19:45
you will recognize that this form
of query can also be represented

81
15:19:45 --> 15:19:51
as a selection operation on the relation
Beers with a condition on the manf

82
15:19:51 --> 15:19:54
attribute, followed by
a projection operation

83
15:19:54 --> 15:19:58
that outputs the name attribute from
the result of the selection operation.

84
15:19:59 --> 15:20:04
So the selection operation finds all
tuples of beer for which the manufacturer

85
15:20:04 --> 15:20:09
is Heineken, and from those tuples
it projects only the name column.

86
15:20:11 --> 15:20:15
The result of the query is a table
with one single attribute called name.

87
15:20:18 --> 15:20:22
We illustrate some more features of SQL,
using two example queries.

88
15:20:22 --> 15:20:26
The first looks for
expensive beer and its price.

89
15:20:27 --> 15:20:30
Let's say we consider a beer to be
expensive if it costs more than $15

90
15:20:30 --> 15:20:31
per bottle.

91
15:20:32 --> 15:20:34
From the schema,

92
15:20:34 --> 15:20:37
we know that the price information is
available in the table called Sells.

93
15:20:38 --> 15:20:41
So the FROM clause should use Sells.

94
15:20:42 --> 15:20:44
The WHERE clause is intuitive and

95
15:20:44 --> 15:20:48
specifies the price of the beer
to be greater than 15.

96
15:20:48 --> 15:20:52
Now notice that the Sells relation
also has a column called bar.

97
15:20:52 --> 15:20:59
Now, if two different bars sell
the same beer at the same price,

98
15:20:59 --> 15:21:01
we'll get both entries in the result.

99
15:21:01 --> 15:21:03
But that's not what we want.

100
15:21:03 --> 15:21:07
Now regardless of the multiplicity
of bars that have the same price for

101
15:21:07 --> 15:21:10
the same beer,
we want the result just once.

102
15:21:12 --> 15:21:17
So this is achieved through
the SELECT DISTINCT statement,

103
15:21:17 --> 15:21:23
which ensures that the result
relation will have no duplicate.

104
15:21:23 --> 15:21:27
The second example shows the case
where more than one condition

105
15:21:27 --> 15:21:30
must be specified by the result.

106
15:21:30 --> 15:21:33
In this query,
the business must be in San Diego and

107
15:21:33 --> 15:21:37
at the same time it must be
a temporary license holder,

108
15:21:37 --> 15:21:41
which means the license
number should start with 32.

109
15:21:41 --> 15:21:47
As we see here, these conditions
are put together by the AND operator.

110
15:21:49 --> 15:21:53
Thus, the query will pick
the third record in the table

111
15:21:53 --> 15:21:57
because the first record satisfy the first
condition and not the second condition.

112
15:21:58 --> 15:22:02
In a few slides, we'll come back to
the evaluation of this type of queries

113
15:22:02 --> 15:22:04
in the context of big data.

114
15:22:04 --> 15:22:11
Now, remember, one can also place a limit
on the number of results to return.

115
15:22:11 --> 15:22:16
If our database is large, and
we need only five results, for

116
15:22:16 --> 15:22:20
example, for a sample to display,
we can say LIMIT 5.

117
15:22:20 --> 15:22:27
Now, the exact syntax of this LIMIT
clause may vary between DBMS vendors.

1
06:35:53 --> 06:35:57
Now if the table of beers was large and
had millions of entries,

2
06:35:58 --> 06:36:02
the table would possibly need
to be split over many machines.

3
06:36:03 --> 06:36:07
Another way of saying that is that
the table will be partitioned

4
06:36:07 --> 06:36:09
across a number of machines.

5
06:36:09 --> 06:36:12
Since a query simply
performs a selection and

6
06:36:12 --> 06:36:15
projection here,
it can be evaluated in parallel.

7
06:36:16 --> 06:36:19
Remember that name is
the primary key of the table.

8
06:36:21 --> 06:36:23
One standard way of partitioning the data

9
06:36:23 --> 06:36:26
is called a range partitioning
by the primary key.

10
06:36:28 --> 06:36:32
This simply means that the rows
of the table are put in groups

11
06:36:32 --> 06:36:36
depending on the alphabetical
order of the name value.

12
06:36:37 --> 06:36:41
So beers with names starting with E and
B here are placed in Machine 1.

13
06:36:41 --> 06:36:44
Those starting with C and
D are in Machine 2.

14
06:36:44 --> 06:36:49
And if there are too many rows for
entries where the name starts with H,

15
06:36:49 --> 06:36:51
maybe H is split into Machines 5 and 6.

16
06:36:51 --> 06:36:53
This is shown in the sketch here.

17
06:36:54 --> 06:36:59
Next, we will show how queries
are performed over partition tables.

18
06:36:59 --> 06:37:04
But before we do that, you should know
that all database management companies,

19
06:37:04 --> 06:37:09
like IBM, Chair Data, Microsoft, and
others, have a solution like this for

20
06:37:09 --> 06:37:12
large volumes of data,
where data partitioning is used.

21
06:37:12 --> 06:37:17
Newer systems, like Spark and SQL,
are naturally distributed, and

22
06:37:17 --> 06:37:18
therefore, offer data partitioning.

23
06:37:21 --> 06:37:24
So, we show the same partition
tables as we saw before.

24
06:37:24 --> 06:37:26
Now we'll ask two queries.

25
06:37:27 --> 06:37:32
The first query asks for
all tuples as records from the beers table

26
06:37:33 --> 06:37:36
where the name of the beer starts with Am.

27
06:37:37 --> 06:37:40
And the second query is
exactly what we asked before.

28
06:37:43 --> 06:37:46
The first query in
the SQL looks like this.

29
06:37:47 --> 06:37:52
We said SELECT* FROM Beers to mean
all attributes from table beers.

30
06:37:52 --> 06:37:56
The WHERE clause shows the syntax for
a partial match query.

31
06:37:57 --> 06:38:01
In this query,
there are two new syntax elements.

32
06:38:01 --> 06:38:03
The first is a predicate called like.

33
06:38:05 --> 06:38:09
When we use like,
we're telling the query engine

34
06:38:09 --> 06:38:12
that we only have partial information
about the string we want to match.

35
06:38:13 --> 06:38:16
This partly specified string
is called a string pattern.

36
06:38:18 --> 06:38:21
That means, there is this part
of the string we know and

37
06:38:21 --> 06:38:22
a part that we do not know.

38
06:38:24 --> 06:38:30
In this case, we know that our design
string starts with Am, so we'd write Am,

39
06:38:30 --> 06:38:35
and then we put % to refer to the part
of the string that we do not know.

40
06:38:35 --> 06:38:39
Putting them together, we get Am%.

41
06:38:39 --> 06:38:43
If we wanted to find, say,
Am somewhere in the middle of the string,

42
06:38:43 --> 06:38:46
we would write the pattern as %Am%.

43
06:38:46 --> 06:38:52
The second query is not new.

44
06:38:52 --> 06:38:54
We saw it in the last slide.

45
06:38:54 --> 06:38:59
However, as we'll see next, evaluating
the second query will be a little more

46
06:38:59 --> 06:39:03
tricky in a partition database than
that we usually see for big data.

47
06:39:08 --> 06:39:11
Let's talk about the first query
in this data partition setting.

48
06:39:11 --> 06:39:18
The question to ask is, do we need to
touch all partitions to answer the query?

49
06:39:18 --> 06:39:24
Of course not, we know that the name is
a primary key for the table of beers.

50
06:39:24 --> 06:39:28
We also know that the system did arrange
partitioning on the name attribute.

51
06:39:29 --> 06:39:34
This means that the evaluation
process should only access Machine 1

52
06:39:34 --> 06:39:38
because no other machine will have
records for names starting with A.

53
06:39:39 --> 06:39:41
Now this is exactly what we, as humans,

54
06:39:41 --> 06:39:46
do when we look up an entry in
a multivolume encyclopedia.

55
06:39:46 --> 06:39:50
We look for the starting words, then
figure out which specific volume would

56
06:39:50 --> 06:39:52
have that entry,
then pick up just that volume.

57
06:39:53 --> 06:39:57
Thus, so long as the system
knows the partitioning strategy,

58
06:39:58 --> 06:40:00
it can make its job much more efficient.

59
06:40:01 --> 06:40:05
When a system processes
thousands of queries per second,

60
06:40:05 --> 06:40:08
this kind of efficiency actually matters.

61
06:40:09 --> 06:40:13
Now raised partitioning is
only one of many partitioning

62
06:40:13 --> 06:40:16
schemes used in a database system, okay.

63
06:40:17 --> 06:40:20
Let's try to answer the second query
in the same partition setting.

64
06:40:21 --> 06:40:25
Now the query condition is on
the second attribute, manf.

65
06:40:26 --> 06:40:28
Now in one sense, it's a simpler query.

66
06:40:28 --> 06:40:30
There is no light pattern here, and

67
06:40:30 --> 06:40:33
we know exactly the string that we are
looking for, namely the string Heineken.

68
06:40:34 --> 06:40:38
However, this time,
we really cannot get away

69
06:40:38 --> 06:40:42
by using the partitioning information
because the partitioning activity is

70
06:40:42 --> 06:40:46
different from the attribute on which
the query condition is applied.

71
06:40:47 --> 06:40:51
So this query will need
to go to all partitions.

72
06:40:51 --> 06:40:56
Technically speaking,
the query needs to be broadcast

73
06:40:56 --> 06:40:59
from the primary machine to all machines,
as shown here.

74
06:41:02 --> 06:41:06
Next, this broadcast query
will be independently, and

75
06:41:06 --> 06:41:10
in parallel,
execute the query on the local machine.

76
06:41:11 --> 06:41:15
Then, these results need to be
brought back into the primary machine.

77
06:41:16 --> 06:41:19
And then,
they need to be unioned together.

78
06:41:19 --> 06:41:22
And only then, the results can be
formed and returned to the client.

79
06:41:24 --> 06:41:26
Now, this might seem
like a lot of extra work.

80
06:41:27 --> 06:41:32
However, remember, the shaded part of
the query is executed in parallel,

81
06:41:32 --> 06:41:34
which is the essence of
dealing with large data.

82
06:41:37 --> 06:41:42
Now, at this point, you might be thinking,
wait a minute, what if I had 100 machines,

83
06:41:43 --> 06:41:46
and the desired data
is only in 20 of them?

84
06:41:48 --> 06:41:52
Should we needlessly go through all 100
machines, find nothing in 80 of them, and

85
06:41:52 --> 06:41:55
return 0 results from those machines?

86
06:41:55 --> 06:41:56
Then why do the extra work?

87
06:41:56 --> 06:41:57
Can it not be avoided?

88
06:41:59 --> 06:42:03
Well, to do this, it would need
one more piece in the solution,

89
06:42:03 --> 06:42:04
it's called an index structure.

90
06:42:05 --> 06:42:07
Very simply,

91
06:42:07 --> 06:42:11
an index can be thought of as a reverse
table, where given the value in a column,

92
06:42:11 --> 06:42:16
you would get back the records where the
value appears as shown in the figure here.

93
06:42:17 --> 06:42:22
Using an index speeds up query
processing significantly.

94
06:42:22 --> 06:42:25
With indexes, we can solve this
problem in many different ways.

95
06:42:28 --> 06:42:32
The top table shows the case where
each machine has its own index for

96
06:42:32 --> 06:42:34
the manf column.

97
06:42:34 --> 06:42:38
This is called a local index because
the index is in every machine

98
06:42:38 --> 06:42:42
that holds the data for
that table on that machine.

99
06:42:44 --> 06:42:46
In this case,
looking up Heineken in the index,

100
06:42:46 --> 06:42:48
we would know which records
would have the data.

101
06:42:50 --> 06:42:55
Since the index is local, the main query
will indeed go to all machines, but

102
06:42:55 --> 06:42:59
the lookup will be really instant, and the
empty results would return very quickly.

103
06:43:02 --> 06:43:04
In the second case,
we adopted a different solution.

104
06:43:05 --> 06:43:11
Here, there is an index on the main
machine, all on a separate index server.

105
06:43:11 --> 06:43:14
Now when we place a data
record in a machine,

106
06:43:14 --> 06:43:18
this index keeps an account of the machine
that contains the record with that value.

107
06:43:20 --> 06:43:21
Look at the second table to the right.

108
06:43:23 --> 06:43:28
Given the value of Heineken,
we know that it is only in three machines,

109
06:43:28 --> 06:43:32
and therefore,
we can avoid going to the other machines.

110
06:43:32 --> 06:43:36
Clearly, we can always use
both indexing schemes.

111
06:43:36 --> 06:43:40
This will use more space,
but queries will be faster.

112
06:43:40 --> 06:43:45
Now this gives you some of the choices
you may need to make with big data,

113
06:43:45 --> 06:43:50
whether you use a parallel DBMS or
a distributed data solution.

1
13:19:42 --> 13:19:46
Welcome to Big Data Integration and
Processing.

2
13:19:46 --> 13:19:49
 Welcome to course three of
the big data specialization.

3
13:19:49 --> 13:19:50
I'm Amarnath Gupta.

4
13:19:50 --> 13:19:52
 And I'm Ilkay Altintas.

5
13:19:52 --> 13:19:55
We are really excited to
work with you in this course

6
13:19:55 --> 13:20:00
to develop your understanding and skills
in big data integration and processing.

7
13:20:00 --> 13:20:05
By now you might have just
finished our first two courses and

8
13:20:05 --> 13:20:08
learned the basics of big data
modelling and management.

9
13:20:08 --> 13:20:11
If you haven't, it's not required.

10
13:20:11 --> 13:20:14
But for those that less background
in the data modelling and

11
13:20:14 --> 13:20:18
data management areas you
might find it valuable.

12
13:20:18 --> 13:20:23
 We understand that you may not have
any background on data management.

13
13:20:23 --> 13:20:28
We are going to introduce Query languages,
we'll first look at SQL in some detail,

14
13:20:28 --> 13:20:32
and then move to Query languages for
MongoDB which is a semi structured

15
13:20:32 --> 13:20:37
data management system, and Aerospike,
which is a key value store.

16
13:20:38 --> 13:20:43
 We will also introduce concepts
related to processing of big data as

17
13:20:43 --> 13:20:44
big data pipelines.

18
13:20:44 --> 13:20:49
We talk about data structures and
transformations related to batch and

19
13:20:49 --> 13:20:52
stream processing as steps in a pipeline.

20
13:20:52 --> 13:20:55
Providing us a way to talk
about big data processing

21
13:20:55 --> 13:20:58
without getting into the details
of the underlying technologies.

22
13:20:59 --> 13:21:04
Once we have reviewed the concepts and
related systems, we will switch gears

23
13:21:04 --> 13:21:09
to hands on exercises with Spark,
one of the most popular big data engines.

24
13:21:10 --> 13:21:15
We will show you examples of patch and
stream processing using Spark.

25
13:21:16 --> 13:21:20
 As you know for
many data science applications

26
13:21:20 --> 13:21:25
one has to use many different databases
and analyze the integrated data.

27
13:21:25 --> 13:21:31
In fact, data integration is one leading
cause leading to the bigness of data.

28
13:21:32 --> 13:21:35
We'll give you a rapid exposure to
information integration systems

29
13:21:35 --> 13:21:40
through use cases and point out the big
data aspects one should pay attention to.

30
13:21:41 --> 13:21:47
 We are also excited to show you
examples of data processing using Splunk.

31
13:21:47 --> 13:21:51
Our goal here is to provide you
with simple hands on exercises

32
13:21:51 --> 13:21:53
that require no programming, but

33
13:21:53 --> 13:21:59
show you how one can use interfaces like
Splunk to manage and process big data.

34
13:21:59 --> 13:22:01
We wish you a fun time learning and

35
13:22:01 --> 13:22:07
hope to hear from you in the discussions
forums and learner stories as usual.

36
13:22:07 --> 13:22:08
 Well, happy learning and think big data

1
02:41:51 --> 02:41:56
In this video, we will talk about
the challenges of ingesting and

2
02:41:56 --> 02:42:01
processing big data and
remind ourselves why need any paradigm and

3
02:42:01 --> 02:42:03
programming models for big data.

4
02:42:04 --> 02:42:09
After this video, you will be able to
summarize the requirements of programming

5
02:42:09 --> 02:42:12
models for big data and
why you should care about them.

6
02:42:13 --> 02:42:16
You will also be able to explain
how the challenges of big

7
02:42:16 --> 02:42:21
data related to its variety, volume and
velocity affects its processing.

8
02:42:25 --> 02:42:30
Before we start,
let's imagine an online gaming newscase,

9
02:42:30 --> 02:42:33
just like the one we have for
Catch the Pink Flamingo.

10
02:42:36 --> 02:42:41
You just introduced the game,
and users started signing up.

11
02:42:41 --> 02:42:44
You start with a traditional
relational database,

12
02:42:44 --> 02:42:46
keeping track of user sessions and
other events.

13
02:42:49 --> 02:42:54
Your game server receives
an event notification every time

14
02:42:54 --> 02:42:57
a user opens his session and
makes a point in the game.

15
02:42:58 --> 02:43:02
Initially, everything is great,
your game is working and

16
02:43:02 --> 02:43:06
the database is able to handle the event
streams coming into the server.

17
02:43:07 --> 02:43:12
However, suddenly your game becomes
highly popular a good problem to have.

18
02:43:14 --> 02:43:18
The database management system in
your game server won't be able to

19
02:43:18 --> 02:43:20
handle the load anymore.

20
02:43:20 --> 02:43:24
You start getting errors that the events
can't be inserted into the database

21
02:43:24 --> 02:43:26
at the speed they are coming in.

22
02:43:27 --> 02:43:34
You decide that you will have a buffer or
a queue to process the advancing chunks.

23
02:43:34 --> 02:43:39
Maybe also at the same time processing
them to be organized in windows of time or

24
02:43:39 --> 02:43:40
game sessions.

25
02:43:42 --> 02:43:47
However, in time as the demand goes up,
you will need more processing nodes and

26
02:43:47 --> 02:43:51
even more database servers
that can handle the load.

27
02:43:51 --> 02:43:57
This is, a typical scenario that
most web sites face when confronted

28
02:43:57 --> 02:44:02
with big data issues related to
volume and velocity of information.

29
02:44:02 --> 02:44:04
As this scenario demonstrates,

30
02:44:04 --> 02:44:09
solving the problem in one step
might be possible initially.

31
02:44:09 --> 02:44:14
But the more reactive fixes
the game developers add, the system

32
02:44:14 --> 02:44:18
becomes less robust and
more complicated to evolve.

33
02:44:20 --> 02:44:24
While the developers initially
started with an application and

34
02:44:24 --> 02:44:25
the database to manage.

35
02:44:26 --> 02:44:30
Now they have to manage a number
of issues related to this

36
02:44:30 --> 02:44:35
infrastructure management just to
keep up with the load on the system.

37
02:44:35 --> 02:44:41
Similarly, the database servers
can be effected and corrupted.

38
02:44:41 --> 02:44:46
The replication and fault tolerance of
them need to be handled separately.

39
02:44:47 --> 02:44:50
Let's start by going through these issues.

40
02:44:50 --> 02:44:54
Let's say,
one of the processing nodes went down.

41
02:44:55 --> 02:45:00
The system needs to manage and
restart the processing and

42
02:45:00 --> 02:45:03
there will be potentially some
data loss in the meantime.

43
02:45:05 --> 02:45:08
The system would need to
check every processing node

44
02:45:08 --> 02:45:10
before it can discard data.

45
02:45:11 --> 02:45:17
Each note and each database has
to be replicated separately.

46
02:45:17 --> 02:45:24
Batch computations that need data from
multiple data servers need to access and

47
02:45:24 --> 02:45:31
maintain use of the data separately which
might end up being quite slow and costly.

48
02:45:31 --> 02:45:35
Big data processing techniques
we will address in this course,

49
02:45:35 --> 02:45:40
will help you to reduce the management
of the mentioned complexities,

50
02:45:40 --> 02:45:44
including failing servers and
breaking compute nodes.

51
02:45:44 --> 02:45:49
While helping with the scalability of the
management and processing infrastructure.

52
02:45:51 --> 02:45:56
We will talk about using big data systems
like Spark to achieve data parallel

53
02:45:56 --> 02:46:01
processing scalability for
data applications on commodity clusters.

54
02:46:02 --> 02:46:07
We will use to Spark Runtime Libraries and
Programming Models to

55
02:46:07 --> 02:46:11
demonstrate how big data systems can
be used for application management.

56
02:46:12 --> 02:46:17
To summarize, what our imaginary game
application needs from big data system.

57
02:46:18 --> 02:46:24
First of all, there needs to be a way
to use common big data operations

58
02:46:24 --> 02:46:28
to manage and split large volumes
of events data streaming in.

59
02:46:28 --> 02:46:32
This means the partitioning and
placement of data in and

60
02:46:32 --> 02:46:38
out of computer memory along with a model
to synchronize the datasets later on.

61
02:46:39 --> 02:46:43
The access to data should
be achieved in a fast way.

62
02:46:45 --> 02:46:47
The game developers need
to be able to deploy

63
02:46:47 --> 02:46:52
many event processing jobs to
distributed processing nodes at once.

64
02:46:52 --> 02:46:56
And these are potentially the data
nodes we move the computations to.

65
02:46:57 --> 02:47:02
It should also enable
reliability of the computing and

66
02:47:02 --> 02:47:05
enable fault tolerance from failures.

67
02:47:05 --> 02:47:08
This means enabling
programmable replications and

68
02:47:08 --> 02:47:11
recovery of event data when needed.

69
02:47:11 --> 02:47:13
It should be easily

70
02:47:13 --> 02:47:18
scalable to a distributed set of
nodes where the data gets produced.

71
02:47:18 --> 02:47:21
It should also enable scaling out.

72
02:47:21 --> 02:47:27
Scaling out is simply adding new
resources like distributed computers to

73
02:47:27 --> 02:47:33
process more or faster data at
scale without losing performance.

74
02:47:33 --> 02:47:36
There are many data
types in an online game.

75
02:47:37 --> 02:47:40
Although, we talked about
time click events and

76
02:47:40 --> 02:47:45
scores, it would be easy to imagine
there are graphs of players,

77
02:47:45 --> 02:47:48
text-based chats, and
images that need to be processed.

78
02:47:50 --> 02:47:53
Our big data system should
enable processing of such

79
02:47:53 --> 02:47:58
a mixed variety of data and
potentially optimize handling of

80
02:47:58 --> 02:48:01
each type separately as well
as together when needed.

81
02:48:04 --> 02:48:08
In addition, our system should
have been able both streaming and

82
02:48:08 --> 02:48:14
batch processing, enabling all
the processing to be debuggable and

83
02:48:14 --> 02:48:16
extensible with minimal effort.

84
02:48:16 --> 02:48:21
That means being able to handle
operations at small chunks of data

85
02:48:21 --> 02:48:25
streams with minimal delay,
that is what we call low latency.

86
02:48:26 --> 02:48:33
While at the same time handle processing
of potentially all available data

87
02:48:33 --> 02:48:38
in batch form and
all through the same system architecture.

88
02:48:40 --> 02:48:45
Latency is a word that we use and
hear a lot in big data processing,

89
02:48:46 --> 02:48:51
here we refer to how fast the data
is being processed, or simply

90
02:48:51 --> 02:48:58
the difference between production or event
time and processing time of a data entry.

91
02:48:58 --> 02:49:02
In other words, latency is quantification

92
02:49:02 --> 02:49:06
of the delay in the processing of
the streaming data in the system.

93
02:49:08 --> 02:49:11
While some big data
systems are good at it.

94
02:49:11 --> 02:49:16
Hadoop for instance is not a great choice
for operations that require low latency.

95
02:49:18 --> 02:49:21
Let's finish by remembering
the real reasons for

96
02:49:21 --> 02:49:25
all these requirements
of big data processing.

97
02:49:25 --> 02:49:30
Making a different from processing
in a traditional data architecture.

98
02:49:30 --> 02:49:36
Big data has varying volume and
velocity requiring the dynamic and

99
02:49:36 --> 02:49:39
scalable batch and stream processing.

100
02:49:39 --> 02:49:44
Big data has a variety requiring
management of data in many

101
02:49:44 --> 02:49:48
different data systems and
integration of it all at scale.

1
05:31:38 --> 05:31:40
Next, we'll describe
aggregation functions.

2
05:31:41 --> 05:31:44
We have seen the first query before.

3
05:31:44 --> 05:31:48
Select count(*) simply
translates to a count function.

4
05:31:49 --> 05:31:55
Now we could also say
db.Drinkers.find.count.

5
05:31:55 --> 05:31:57
But using count directly
is more straightforward.

6
05:32:00 --> 05:32:04
Now, let's ask to count the number
of unique addresses for drinkers.

7
05:32:06 --> 05:32:09
So, we don't care what the address is.

8
05:32:09 --> 05:32:11
We just care if it exists.

9
05:32:12 --> 05:32:18
This is accomplished through
the $exists:true expression.

10
05:32:18 --> 05:32:22
Thus, if an address exists for
a drinker, it will be counted.

11
05:32:24 --> 05:32:29
Another area where we need to count is
when we have an array valued attribute,

12
05:32:29 --> 05:32:30
like places.

13
05:32:31 --> 05:32:36
If we just want the number
of elements in the raw list,

14
05:32:36 --> 05:32:42
we'll write db.country.findplaces.length
and we'll get six.

15
05:32:42 --> 05:32:47
However, if we want distinct values, we'll
use distinct instead of find and then

16
05:32:47 --> 05:32:52
use the length for counting the number
of distinct elements, in this case 4.

17
05:32:54 --> 05:33:01
Now, MongoDB uses an internal machinery
called the aggregation framework,

18
05:33:01 --> 05:33:06
which is modeled on the concept
of data processing pipelines.

19
05:33:06 --> 05:33:11
That means documents enter
a multi-stage pipeline which transforms

20
05:33:11 --> 05:33:17
the documents at each stage until
it becomes an aggregated result.

21
05:33:17 --> 05:33:21
Now we have seeing a similar mechanism for
relational data.

22
05:33:21 --> 05:33:25
The aggregation pipelines starts
by using the aggregate primitive.

23
05:33:26 --> 05:33:32
The most basic pipeline stages provides
filters that operate like queries and

24
05:33:32 --> 05:33:36
the document transformations that
modify the form of the output document.

25
05:33:37 --> 05:33:43
The primary filter operation is $match,
which is followed by a query condition.

26
05:33:43 --> 05:33:47
In this case, status is A.

27
05:33:47 --> 05:33:51
And expectedly, the $match operation
produces a smaller number of documents

28
05:33:51 --> 05:33:53
to be processed at the next stage.

29
05:33:54 --> 05:33:59
This is usually followed
by the $group operation.

30
05:33:59 --> 05:34:03
Now this operation needs to know which
attributes should be grouped together.

31
05:34:04 --> 05:34:08
In the example here cust_id
is the grouping attribute so

32
05:34:08 --> 05:34:12
it is passed as a parameter
to the $group function.

33
05:34:12 --> 05:34:15
Now notice the syntax.

34
05:34:15 --> 05:34:21
_id:$cust_id says that the grouped
data will have an _id attribute,

35
05:34:21 --> 05:34:25
and its value will be
picked from the cust_id

36
05:34:25 --> 05:34:30
attribute from the previous
stage of computation.

37
05:34:30 --> 05:34:34
Thus, the $ before the cust_id
is telling the system that

38
05:34:34 --> 05:34:38
cust_id is a known variable in
the system and not a constant.

39
05:34:38 --> 05:34:41
The $group operation also needs a reducer,

40
05:34:41 --> 05:34:47
which is a function that operates on an
activity to produce an aggregate result.

41
05:34:47 --> 05:34:50
In this case, the reduce function is sum,

42
05:34:51 --> 05:34:54
which operates on the amount
attribute from the previous stage.

43
05:34:55 --> 05:35:00
Like $cust_id, we use $amount to
indicate that it's a variable.

44
05:35:02 --> 05:35:04
As we saw in the relational case,

45
05:35:04 --> 05:35:09
data can be partitioned into chunks on
the same machine or on different machines.

46
05:35:09 --> 05:35:12
These chunks are called chards.

47
05:35:12 --> 05:35:19
The aggregation pipeline of MongoDB
can operate on a charded collection.

48
05:35:19 --> 05:35:24
The grouping operation in MongoDB
can accept multiple attributes like

49
05:35:24 --> 05:35:25
the four shown here.

50
05:35:25 --> 05:35:30
Also shown is a post grouping directive
to sort on the basis of two attributes.

51
05:35:32 --> 05:35:37
The first is a computed count
variable in ascending order.

52
05:35:37 --> 05:35:40
So the one designates the ascending order.

53
05:35:40 --> 05:35:42
The next sorting attribute is secondary.

54
05:35:42 --> 05:35:45
That means if two groups have
the same value for count,

55
05:35:45 --> 05:35:50
then they'll be further sorted
based on the category value.

56
05:35:50 --> 05:35:55
But this time the order is descending
because of the -1 directive

57
05:35:59 --> 05:36:04
In course two we have seen Solar,
a text search engine from Apache.

58
05:36:05 --> 05:36:09
MongoDB has a built in text search engine
which can be invoked through the same

59
05:36:09 --> 05:36:11
aggregation framework we saw before.

60
05:36:12 --> 05:36:17
Imagine that MongoDB documents in this
case are really text documents placed in

61
05:36:17 --> 05:36:18
a collection called articles.

62
05:36:20 --> 05:36:25
In this case, the $match directive of
the aggregate function must be told

63
05:36:25 --> 05:36:28
it's going to perform a text
function on the article's corpus.

64
05:36:30 --> 05:36:32
The actual text function is $search.

65
05:36:32 --> 05:36:38
We set search terms like "Hillary
Democrat" such that having

66
05:36:38 --> 05:36:44
either term in a document will
satisfy the search requirement.

67
05:36:44 --> 05:36:48
As is the case of any text engine,

68
05:36:48 --> 05:36:52
the results of any search returns
a list of documents, each with a score.

69
05:36:53 --> 05:36:58
The next task is to tell MongDB to
sort the results based on textScore.

70
05:37:00 --> 05:37:01
What's the $meta here?

71
05:37:02 --> 05:37:06
Meta stands for metadata,
that is additional information.

72
05:37:06 --> 05:37:10
Remember that the aggregation
operations are executed in a pipeline.

73
05:37:12 --> 05:37:15
Any step in the pipeline can
produce some extra data, or

74
05:37:15 --> 05:37:18
metadata, for each processed document.

75
05:37:18 --> 05:37:22
In this example, the metadata
produced by the search function

76
05:37:22 --> 05:37:25
is a computed attribute called textScore.

77
05:37:25 --> 05:37:31
So this directive tells the system to pick
up this specific metadata attribute and

78
05:37:31 --> 05:37:36
use it to populate the score attribute
which would be used for sorting.

79
05:37:36 --> 05:37:41
Finally, the $project class
does exactly what is expected.

80
05:37:41 --> 05:37:46
It tells the system to output only
the title of each document and

81
05:37:46 --> 05:37:47
suppress its id.

82
05:37:51 --> 05:37:56
The last item in our
discussion of MongoDB is join.

83
05:37:56 --> 05:37:59
We have seen that join is a vital
operation for data management operations.

84
05:38:02 --> 05:38:09
Interestingly, MongoDB introduced this
equivalent of join only in version 3.2.

85
05:38:09 --> 05:38:13
So, the joining in MongoDB also
happens in the aggregation framework.

86
05:38:14 --> 05:38:17
There are a few ways of
expressing joins in MongoDB.

87
05:38:18 --> 05:38:23
We show one here that explicitly performs
a join to a function called look up.

88
05:38:25 --> 05:38:28
We use an example form
the MongoDB documentation.

89
05:38:28 --> 05:38:35
Now here are two document collections,
order and inventory.

90
05:38:35 --> 05:38:40
Notice that the item key in
orders has values abc, jkl, etc.

91
05:38:41 --> 05:38:47
And the sku key in the inventory has
comparable values abc, def, etc.

92
05:38:47 --> 05:38:49
So these two are joinable by value.

93
05:38:50 --> 05:38:56
The way to specify this join,
one can use this query.

94
05:38:56 --> 05:39:01
The db.orders.aggregate
declaration states that orders is

95
05:39:01 --> 05:39:04
sort of the home, or local collection.

96
05:39:04 --> 05:39:09
Now in the aggregate, the function
$lookup needs to know what to look up for

97
05:39:09 --> 05:39:11
each document in orders.

98
05:39:13 --> 05:39:18
The from attribute specifies the name
of the collection as inventory.

99
05:39:19 --> 05:39:21
The next two parameters are the local and

100
05:39:21 --> 05:39:25
foreign matching keys,
which are item and sku, respectively.

101
05:39:26 --> 05:39:31
The last item, as:,
is a construction part of

102
05:39:31 --> 05:39:36
the join operation which says how to
structure the match items into the result.

103
05:39:37 --> 05:39:40
Now before we show you the results,
let's see what should match.

104
05:39:42 --> 05:39:47
The abc item in order should
match the abc in sku.

105
05:39:48 --> 05:39:53
Similarly, the jkl item
should match the jkl in sku.

106
05:39:54 --> 05:39:56
Okay, but there is one more twist.

107
05:39:58 --> 05:39:59
Here is the actual result.

108
05:39:59 --> 05:40:04
The first two records show
exactly what we expect.

109
05:40:04 --> 05:40:06
There is a new field called
inventory-docs in the batching record.

110
05:40:08 --> 05:40:11
The third record however,
shows something interesting.

111
05:40:13 --> 05:40:17
Inventory has two records shown here,
what do they match?

112
05:40:18 --> 05:40:23
Now they match the empty
document in orders because

113
05:40:23 --> 05:40:27
orders has a document
whose item field is null.

114
05:40:28 --> 05:40:34
So it matches documents and
inventory where the sku item is also null,

115
05:40:34 --> 05:40:39
explicitly as in document 5, or
implicitly as in document 6.

116
05:40:41 --> 05:40:45
This concludes our discussion of
queries in the context of MongoDB.

1
11:12:22 --> 11:12:27
In this hands on activity, we'll be
using Pandas to read CSV files and

2
11:12:27 --> 11:12:29
perform various queries on them.

3
11:12:29 --> 11:12:32
Pandas is a data analysis library for
Python.

4
11:12:34 --> 11:12:36
First, we'll create a new
Jupyter Python Notebook.

5
11:12:38 --> 11:12:42
Next, we will use Pandas to read
a CSV file into a DataFrame.

6
11:12:43 --> 11:12:47
We'll then view the contents of the
DataFrame and see how to filter rows and

7
11:12:47 --> 11:12:48
columns of it.

8
11:12:49 --> 11:12:53
Next, we will perform average and
sum operations on the DataFrame.

9
11:12:53 --> 11:12:58
And finally, show how to merge two
DataFrames by joining on a single column.

10
11:13:01 --> 11:13:01
Let's begin.

11
11:13:03 --> 11:13:06
We'll start by creating
a new iPython notebook.

12
11:13:06 --> 11:13:10
Clicking on New and
selecting Python 3 under notebooks.

13
11:13:13 --> 11:13:18
First, we'll import the Pandas
library by writing import pandas.

14
11:13:20 --> 11:13:22
Remember that in iPython notebooks,
to run a command,

15
11:13:22 --> 11:13:24
we hold down the shift key and hit Enter.

16
11:13:28 --> 11:13:34
Next, let's read buyclicks.csv
into a Pandas DataFrame.

17
11:13:34 --> 11:13:36
We'll put it in a variable
called buyclicksDF.

18
11:13:40 --> 11:13:44
We'll read it using pandas.read_csv,

19
11:13:44 --> 11:13:49
and we'll read the buy-clicks.csv file.

20
11:13:52 --> 11:13:56
We can see the contents of the file by
just running the variable by itself.

21
11:14:01 --> 11:14:06
And notice that the file has many rows and
then iPython truncates this,

22
11:14:06 --> 11:14:07
the dot, dot, dot.

23
11:14:11 --> 11:14:14
We can see only the top five
rows by calling .head(5).

24
11:14:15 --> 11:14:20
Next, let's look at only two

25
11:14:20 --> 11:14:26
columns in the buyclicks data.

26
11:14:26 --> 11:14:28
Let's look at price and user ID.

27
11:14:29 --> 11:14:35
We can do these by first entering
buyclicks DataFrame in the same text for

28
11:14:35 --> 11:14:41
specifying only certain columns to
show is open bracket open bracket and

29
11:14:41 --> 11:14:42
then the name of the columns
you want to view.

30
11:14:42 --> 11:14:47
So, want your price and
user ID, and again,

31
11:14:47 --> 11:14:53
we only want to see the first five rows,
so we'll do .head(5).

32
11:14:57 --> 11:15:01
Now, let's query the buyclicks data for
only the prices less than 3.

33
11:15:01 --> 11:15:06
First, we'll enter buyclicksDF,
One square bracket,

34
11:15:06 --> 11:15:11
to filter our particular column,
we enter buyclicksDF and then column name.

35
11:15:12 --> 11:15:16
Now, we specify the limit of
the query by entering <3.

36
11:15:24 --> 11:15:28
This shows first five rows where
the price is less than three.

37
11:15:30 --> 11:15:34
We can also perform aggregate
operations on Panda's DataFrames.

38
11:15:35 --> 11:15:43
We can sum all the price data by
entering buyclicksDF['price'].sum.

39
11:15:47 --> 11:15:51
Another aggregate operation we can
perform is looking at the average.

40
11:15:51 --> 11:15:53
Let's look at the average for price.

41
11:15:53 --> 11:15:55
The function is called mean.

42
11:15:56 --> 11:16:02
So, once your buyclicksDF ['price'].mean.

43
11:16:07 --> 11:16:10
Can also join two DataFrames
on a single column.

44
11:16:11 --> 11:16:16
First, let's read in another
CSV into a different DataFrame.

45
11:16:16 --> 11:16:19
We'll read in adclicks.csv.

46
11:16:19 --> 11:16:26
So we'll says adclicksDF
= pandas.read_csv,

47
11:16:26 --> 11:16:32
we'll say ('ad-clicks.csv').

48
11:16:38 --> 11:16:42
To verify that we read this data
successfully, let's look at the contents.

49
11:16:51 --> 11:16:54
Now, let's combine
the buyclicks DataFrame and

50
11:16:54 --> 11:16:56
the adclicks data frame
on the user ID call.

51
11:16:58 --> 11:17:01
We'll put the result in the new
DataFrame called mergeDF.

52
11:17:01 --> 11:17:08
So we'll say mergeDF = adclicksDF.merge

53
11:17:12 --> 11:17:16
Then, we need to say which
DataFrame we're merging with,

54
11:17:16 --> 11:17:21
buyclicksDF, and
the column that we're merging on.

55
11:17:21 --> 11:17:24
So we'll say on='userid'.

56
11:17:30 --> 11:17:33
Finally, we can look at the contents
with this merged DataFrame.

1
22:29:56 --> 22:30:00
So now from MongoDB we will go to
Aerospike which is a key value store.

2
22:30:02 --> 22:30:04
Key value stores typically offer an API.

3
22:30:05 --> 22:30:09
That is the way to access data using a
programming language like Python or Java.

4
22:30:10 --> 22:30:14
We will take a very brief look
at Aerospike, which offers both

5
22:30:14 --> 22:30:18
a programmatic access and
a limited amount of query access to data.

6
22:30:20 --> 22:30:22
The data model of Aerospike
is illustrated here.

7
22:30:24 --> 22:30:29
Data are organized in lean spaces which
can be in memory or on flash disks.

8
22:30:31 --> 22:30:33
Name spaces are top level data containers.

9
22:30:34 --> 22:30:40
The way you collect data in name spaces
relates to how data is stored and managed.

10
22:30:41 --> 22:30:45
So name space contains records,
indexes, and policies.

11
22:30:45 --> 22:30:50
Now policies dictate name space
behavior like how data is stored,

12
22:30:50 --> 22:30:52
whether it's on RAM or disk, or

13
22:30:52 --> 22:30:57
how how many replicas exist for
a record, and when records expire.

14
22:30:59 --> 22:31:03
A name space can contain sets,
you can think of them as tables.

15
22:31:03 --> 22:31:06
So here there are two sets,
people and places,

16
22:31:06 --> 22:31:10
and a set of records
which are not in any set.

17
22:31:11 --> 22:31:15
Within a record,
data is stored in one or many bins.

18
22:31:16 --> 22:31:20
Bins consist of a name and a value.

19
22:31:22 --> 22:31:26
The example written here is in Java, and

20
22:31:26 --> 22:31:29
you don't have to know Java to
follow the main point here.

21
22:31:31 --> 22:31:35
Here we are creating indexes on key
value data that's handled by Aerospike.

22
22:31:36 --> 22:31:38
This data set comes from Twitter.

23
22:31:39 --> 22:31:45
Each field of a tweet is extracted and
put into Aerospike as a key value pair.

24
22:31:45 --> 22:31:50
So we declare the namespace to be

25
22:31:50 --> 22:31:54
example, and the record set to be tweet.

26
22:31:56 --> 22:32:02
The name of the index to be TestIndex,
and the name of the bin as user name.

27
22:32:04 --> 22:32:09
Since this index is stored
on disk an SQL-like command,

28
22:32:09 --> 22:32:13
like SHOW INDEX, shows the content
of the index as you can see here.

29
22:32:13 --> 22:32:18
This routine shows

30
22:32:18 --> 22:32:22
how data can be inserted into
Aerospike programmatically.

31
22:32:22 --> 22:32:26
Again, the goal is to point out a few
salient aspects of data insertion

32
22:32:26 --> 22:32:28
regardless of the syntax of the language.

33
22:32:28 --> 22:32:33
Now since this is a key value store,
one first needs to define the key.

34
22:32:35 --> 22:32:40
This line here says that the key in
the namespace call example, and set call

35
22:32:40 --> 22:32:45
tweet, is the value of the function getId,
which returns the ID of a tweet.

36
22:32:48 --> 22:32:52
When the data is populated,
we are essentially creating bins.

37
22:32:52 --> 22:32:56
Here, user name is the attribute and

38
22:32:56 --> 22:32:59
the screen name obtained
from the tweet is the value.

39
22:33:00 --> 22:33:04
The actual insertion
happens here in the client.

40
22:33:06 --> 22:33:10
The client.put statement,
where we need to mention the key and

41
22:33:10 --> 22:33:13
the bins we have just created.

42
22:33:13 --> 22:33:16
Now why are we inserting
two bins at a time?

43
22:33:16 --> 22:33:18
Two bins with two ids and user name?

44
22:33:19 --> 22:33:22
This is an idiosyncrasy
of the Aerospike client.

45
22:33:24 --> 22:33:29
After data is inserted one can
create other data using AQL

46
22:33:29 --> 22:33:30
which is very much like SQL.

47
22:33:31 --> 22:33:36
This screenshot shows a part of
the output of a simple select star query.

48
22:33:39 --> 22:33:44
Now in your hands-on exercise, you'll be
able to play with the Aerospike data.

49
22:33:45 --> 22:33:51
This is just a screenshot showing
the basic query syntax of AQL,

50
22:33:51 --> 22:33:55
that is Aerospike Query Language,
and a few examples.

51
22:33:56 --> 22:33:59
The last two lines show a couple of
interesting features of the language.

52
22:34:01 --> 22:34:07
The operation between 0 and 99,
is a nicer way of stating a range query,

53
22:34:07 --> 22:34:09
which gives a lower and
upper limits on a variable.

54
22:34:11 --> 22:34:15
The last line shows the operation cost,

55
22:34:15 --> 22:34:19
which transforms one type
of data to another type.

56
22:34:19 --> 22:34:24
Here it transforms coordinates,
that is latitude and longitude data,

57
22:34:24 --> 22:34:28
to a JSON format called GeoJSON which is
designed to represent geographic data

58
22:34:28 --> 22:34:30
in a JSON structure.

59
22:34:33 --> 22:34:37
We will finish our coverage of queries
with a quick reference to an advanced

60
22:34:37 --> 22:34:39
topic, which is beyond
the scope of this course.

61
22:34:41 --> 22:34:45
Now you have seen in prior courses that
streaming data is complex to process

62
22:34:45 --> 22:34:48
because a stream is infinite in nature.

63
22:34:49 --> 22:34:52
Now does this have any impact on
query languages and evaluation?

64
22:34:53 --> 22:34:56
The answer is that it absolutely does.

65
22:34:56 --> 22:34:58
We'll mention one such impact here.

66
22:35:00 --> 22:35:03
This shows a pictorial
depiction of streaming data.

67
22:35:04 --> 22:35:09
The data is segmented into five pieces,
shown in the white boxes in the upper row.

68
22:35:10 --> 22:35:15
These can be gathered, for example every
few seconds, or every 200 data objects.

69
22:35:16 --> 22:35:19
The query defines a window to select

70
22:35:19 --> 22:35:22
key of these data objects
as a unit of processing.

71
22:35:22 --> 22:35:23
Here case three.

72
22:35:25 --> 22:35:29
So three units of data
are picked up in a window unit.

73
22:35:29 --> 22:35:32
To get the next window it
is moved by two units,

74
22:35:33 --> 22:35:35
this is called a slide of the window.

75
22:35:36 --> 22:35:41
Since the window size is three and
the slide is two,

76
22:35:41 --> 22:35:45
one unit of information overlaps
between the two consecutive windows.

77
22:35:46 --> 22:35:51
The lower line in this diagram shows
the initialized item, let's ignore it,

78
22:35:51 --> 22:35:58
followed by two window sets of
data records for processing.

79
22:36:00 --> 22:36:03
Thus the query language therefore,
will have to specify a query

80
22:36:03 --> 22:36:09
that's an SQL query, over a window,
which is also specified in the query.

81
22:36:09 --> 22:36:13
Now in a traffic data stream example,
the SQL statement might look like this.

82
22:36:15 --> 22:36:18
Where the window size is 30 second, and

83
22:36:18 --> 22:36:21
the slide is the same size as window
giving output every 30 seconds.

84
22:36:22 --> 22:36:27
So streaming data results in changes
in both the query language and

85
22:36:27 --> 22:36:29
the way queries are processed.

1
21:06:24 --> 21:06:28
In this hands-on activity,
we will be querying documents in MongoDB.

2
21:06:30 --> 21:06:34
First, we will start a MongoDB server and
then run the MongoDB Shell.

3
21:06:34 --> 21:06:39
We will see how to show the databases and
collections in the MongoDB Server.

4
21:06:39 --> 21:06:43
We will look at an example
document in the database and

5
21:06:43 --> 21:06:45
see how to find distinct values for
a field.

6
21:06:47 --> 21:06:50
Next, we will search for
specific field values and

7
21:06:50 --> 21:06:52
see how to filter fields
returned in a query.

8
21:06:54 --> 21:06:58
Finally, we will search using
regular expressions and operators.

9
21:07:00 --> 21:07:05
Let's begin,
first we'll start the MongoDB server.

10
21:07:05 --> 21:07:10
Open the terminal window by clicking on
the terminal icon, the top of the toolbar.

11
21:07:10 --> 21:07:17
We'll cd into
Downloads/big-data-3-mongodb.

12
21:07:17 --> 21:07:23
We'll start the MongoDB server by running

13
21:07:23 --> 21:07:29
./mongodb/bin/mongod --dbpath db.

14
21:07:29 --> 21:07:32
The arguments dbpath db

15
21:07:32 --> 21:07:37
specified that the database on
MongoDB is in the directory named db.

16
21:07:39 --> 21:07:40
Run this to start the server.

17
21:07:44 --> 21:07:48
Next, we'll start the MongoDB Shell so
that we can perform queries.

18
21:07:48 --> 21:07:53
We'll open another terminal window.

19
21:07:53 --> 21:08:00
Again, cd into
Downloads/big-data-3-mongodb.

20
21:08:00 --> 21:08:05
We'll start the shell by
running ./mongodb/bin/mongo.

21
21:08:13 --> 21:08:19
We can see what databases are available in
the MongoDB server by running the command,

22
21:08:19 --> 21:08:19
show dbs.

23
21:08:23 --> 21:08:26
We've created the sample database
with JSON data from Twitter.

24
21:08:28 --> 21:08:32
We can use the use command
to change to this database.

25
21:08:32 --> 21:08:34
We'll run use sample.

26
21:08:37 --> 21:08:40
We can see the collections in this
database by running, show collections.

27
21:08:43 --> 21:08:46
There's only one collection called users.

28
21:08:46 --> 21:08:50
So, all of the queries will be
using the users collection.

29
21:08:50 --> 21:08:55
Let's count how many documents there
are in the users collection by writing

30
21:08:55 --> 21:08:57
db.users.count.

31
21:09:02 --> 21:09:07
We can look at one of these documents
by writing db.users.findOne.

32
21:09:15 --> 21:09:19
This document contains
Jason from a Twitter tweet.

33
21:09:19 --> 21:09:22
You can see the field names
here with their values.

34
21:09:22 --> 21:09:26
There are also nested fields
under the user field.

35
21:09:26 --> 21:09:28
And each of these fields also has values.

36
21:09:29 --> 21:09:32
You can find the distinct values for

37
21:09:32 --> 21:09:36
particular field,
by using the distinct command.

38
21:09:36 --> 21:09:39
Let's find the distinct values for
the username field.

39
21:09:39 --> 21:09:45
We'll write
db.users.distinct("user_name").

40
21:09:54 --> 21:09:59
Next, let's find all the documents in
this collection where the field username

41
21:09:59 --> 21:10:00
matches the specific value.

42
21:10:02 --> 21:10:05
The value we'll search for
is ActionSportsJax.

43
21:10:06 --> 21:10:10
We'll run the command db.users.find.

44
21:10:10 --> 21:10:13
And the field name is username, and

45
21:10:13 --> 21:10:18
the value we're searching for
is ActionSportsJax.

46
21:10:28 --> 21:10:31
Results of this query is
compressed all in one line.

47
21:10:32 --> 21:10:35
If we append the .pretty
to the previous query.

48
21:10:35 --> 21:10:37
We can see the formatted output.

49
21:10:37 --> 21:10:40
So we'll run the same
command with append .pretty.

50
21:10:47 --> 21:10:49
We can filter the fields
returned from the queries.

51
21:10:51 --> 21:10:55
Let's perform the same query again but
only show the tweet_id field.

52
21:10:57 --> 21:11:00
We can do this by adding a second
argument to the find command.

53
21:11:01 --> 21:11:06
So we'll run the same command again,

54
21:11:06 --> 21:11:13
but add a second argument,
saying tweet_ID: 1.

55
21:11:13 --> 21:11:19
The underscore ID field is a primary
key used in all documents in MongoDB.

56
21:11:19 --> 21:11:22
You can turn this off by adding
another field to our filter.

57
21:11:24 --> 21:11:27
We'll run the same command again,
but turn of _ID.

58
21:11:35 --> 21:11:40
Next, we use the regular expression search
to find strings containing the document.

59
21:11:41 --> 21:11:45
For example, if we want to find all
the tweets containing the text FIFA,

60
21:11:45 --> 21:11:49
we can run db.users.find tweet_text FIFA.

61
21:11:56 --> 21:11:59
There are no results in this query
because this query is searching for

62
21:11:59 --> 21:12:01
tweet text equals FIFA.

63
21:12:03 --> 21:12:07
That is the entire contents and
the value of tweet_text must be FIFA.

64
21:12:08 --> 21:12:10
Instead, if we want to look for

65
21:12:10 --> 21:12:15
FIFA anywhere in the tweet_text,
we can do a regular expression search.

66
21:12:15 --> 21:12:19
To do this,
replace the double quotes with slashes.

67
21:12:19 --> 21:12:21
So we'll run the same command again.

68
21:12:21 --> 21:12:27
But replacing double quotes with slashes.

69
21:12:27 --> 21:12:31
We can count how many documents are
returned by this query by running the same

70
21:12:31 --> 21:12:33
command again, but appending .count.

71
21:12:37 --> 21:12:41
We can also search for documents in
MongoDB where the field values are greater

72
21:12:41 --> 21:12:44
than or less than a certain value.

73
21:12:45 --> 21:12:50
For example, lets find tweet
mention count greater than six.

74
21:12:50 --> 21:12:56
We'll run db.users.find, and
the field name is tweet_mentioned_count.

75
21:12:59 --> 21:13:05
And we want to look for the value
where this field is greater than six.

76
21:13:05 --> 21:13:11
So I'll enter a { $gt : 6 }.

1
18:19:36 --> 18:19:41
In a prior course we looked at JSON as
an example of semi-structured data,

2
18:19:41 --> 18:19:45
and we demonstrated that JSON
data can be thought of as a tree.

3
18:19:46 --> 18:19:49
In this course,
we'll focus on querying JSON data.

4
18:19:50 --> 18:19:55
Before we start, lets review
the details of the JSON structure, and

5
18:19:55 --> 18:19:59
get an initial sense of how
to query this form of data.

6
18:20:00 --> 18:20:03
Let's consider a simple
JSON collection and

7
18:20:03 --> 18:20:07
look at the structures,
substructures actually, it's composed of.

8
18:20:09 --> 18:20:12
The atomic element in the structure
is a key value pair, for example,

9
18:20:12 --> 18:20:18
name is the key and sue is the value,
in this case, an atomic string value.

10
18:20:20 --> 18:20:22
To query a key value pair,

11
18:20:22 --> 18:20:27
we should be able perform one basic
operation given the key, return the value.

12
18:20:29 --> 18:20:35
Now, the value can also be an array,
an array is a list.

13
18:20:37 --> 18:20:41
So the query operations on it can either
be on its position in the list or

14
18:20:41 --> 18:20:43
on the value.

15
18:20:43 --> 18:20:47
Thus, we should be able to ask for the
second element of the array called badges.

16
18:20:47 --> 18:20:52
Or we should be able to seek objects
of which the key called badges

17
18:20:52 --> 18:20:53
has a value of blue.

18
18:20:55 --> 18:21:00
Notice here that the document
collection here is itself an array,

19
18:21:00 --> 18:21:04
within square brackets and
it's just two elements in it.

20
18:21:04 --> 18:21:08
The top level array does not have a key,
by default it's called db.

21
18:21:11 --> 18:21:16
These key value peers
are structured as tuples,

22
18:21:16 --> 18:21:19
often with a name In the snippet shown,

23
18:21:19 --> 18:21:23
favorites has a tuple
of two key value pairs.

24
18:21:23 --> 18:21:28
Now, tuples can be thought of as
relational records, as the operations

25
18:21:28 --> 18:21:32
would include, projection of an attribute,
and selection over a set of tables.

26
18:21:35 --> 18:21:38
On the other hand,
the area called 'points',

27
18:21:38 --> 18:21:42
has two tuples, these two tuples named.

28
18:21:42 --> 18:21:45
As you will see, we'll address
these tuples by their positions.

29
18:21:47 --> 18:21:51
Finally, this one has nesting,

30
18:21:51 --> 18:21:55
that means a mini structure can be
embedded within another structure.

31
18:21:56 --> 18:21:59
So we need operations
that will let us navigate

32
18:21:59 --> 18:22:02
from one structure to any of
it's embedded structures.

33
18:22:05 --> 18:22:10
Now just like a basic SQL query states,
which parts of which records from one or

34
18:22:10 --> 18:22:14
more people should be reported,
a MongoDB query states

35
18:22:14 --> 18:22:18
which parts of which documents from
a document collection should be returned.

36
18:22:20 --> 18:22:26
The primary query is expressed as a find
function, which contains two arguments and

37
18:22:26 --> 18:22:31
an optional qualifier, there are four
things to notice in this function.

38
18:22:33 --> 18:22:35
The first is the term collection,

39
18:22:35 --> 18:22:40
this tells the system which
document collection to use, and

40
18:22:40 --> 18:22:46
therefore is roughly similar to the From
clause when restricted to one table.

41
18:22:46 --> 18:22:53
So if the name of the collection is beers,
the first part would say db.beers.find.

42
18:22:53 --> 18:22:58
The second item is a query filter
which lists all conditions that

43
18:22:58 --> 18:23:03
the retrieved documents should satisfy,
so it's like a Where clause.

44
18:23:05 --> 18:23:09
Now, if we want to return everything,
then this filter is left blank.

45
18:23:10 --> 18:23:14
Otherwise, we'll fill it in a couple
of ways shown in the next few slides.

46
18:23:16 --> 18:23:20
The third term is a projection class
which is essentially a list of variables

47
18:23:20 --> 18:23:22
that we want to see in the output.

48
18:23:24 --> 18:23:28
The fourth and last item sits
after the find function ends and

49
18:23:28 --> 18:23:32
is separated by a dot,
it's called a cursor modifier.

50
18:23:33 --> 18:23:37
The word cursor relates back
to SQL where cursor is defined

51
18:23:37 --> 18:23:41
as a block of results that is
returned to the user in one chunk.

52
18:23:42 --> 18:23:46
This becomes important when the set of
results is too large to be returned all

53
18:23:46 --> 18:23:49
together, and
the user may need to specify how much, or

54
18:23:49 --> 18:23:51
what portion of results
they actually want.

55
18:23:53 --> 18:23:56
So, we'll start out with a few queries,

56
18:23:56 --> 18:24:01
where we show how the same query
can be expressed in SQL, and

57
18:24:01 --> 18:24:06
in MongoDB The first query
wants everything from Beers.

58
18:24:06 --> 18:24:09
The SQL query is structured
on the table Beers, and

59
18:24:09 --> 18:24:12
the SELECT * asks to return all rows.

60
18:24:13 --> 18:24:18
In MongoDB the same query
is more succincted,

61
18:24:18 --> 18:24:20
since the name of the collection
is already specified in

62
18:24:20 --> 18:24:24
calling the find function,
the body of the find function is empty.

63
18:24:25 --> 18:24:30
That means there are no query conditions
and no projection clauses in it.

64
18:24:30 --> 18:24:34
The second query

65
18:24:34 --> 18:24:39
needs to return the variables beer and
price for all records.

66
18:24:40 --> 18:24:47
So the find function here needs an empty
query condition denoted by the open and

67
18:24:47 --> 18:24:52
closed brace symbols, but the projection
clauses are specifically identified.

68
18:24:52 --> 18:24:56
There is a 1 if an attribute is output and
a 0 if it is not.

69
18:24:56 --> 18:25:01
As a shortcut,
only variables with 1 are required.

70
18:25:03 --> 18:25:04
Okay, so when do you use 0?

71
18:25:05 --> 18:25:06
A common situation is the following.

72
18:25:08 --> 18:25:12
Every MongoDB document has
an identifier named _id.

73
18:25:15 --> 18:25:19
By default every query will
return the id of the document.

74
18:25:20 --> 18:25:24
If you don't want it to return
this designated attribute,

75
18:25:24 --> 18:25:30
you should explicitly say, _id:0.

76
18:25:30 --> 18:25:33
Next we will add query conditions.

77
18:25:33 --> 18:25:37
That is the equivalent of
the Where clause in SQL.

78
18:25:37 --> 18:25:42
Our query number three has the query
condition where name is equal to a value.

79
18:25:43 --> 18:25:50
In MongoDB, that equal to translate
to a variable colon value form.

80
18:25:51 --> 18:25:56
Notice the symbol used for
a string is quotes.

81
18:25:57 --> 18:26:00
Query four is more interesting for
two reasons.

82
18:26:01 --> 18:26:05
First, we see a way in which
the distinct operation is specified.

83
18:26:07 --> 18:26:13
Notice here that the primary query
function is not find any more but

84
18:26:13 --> 18:26:15
a new function called distinct.

85
18:26:16 --> 18:26:20
As we'll see later again in our slides,

86
18:26:20 --> 18:26:24
MongoDB uses a few special query
functions for some of the operations.

87
18:26:26 --> 18:26:30
So, you need to know, which function
should be used in what context,

88
18:26:30 --> 18:26:32
when you write MogoDB queries.

89
18:26:33 --> 18:26:38
Secondly, in this query,
we have a non-equality condition,

90
18:26:38 --> 18:26:41
namely, the price is greater than 15.

91
18:26:43 --> 18:26:47
This example shows MongoDB style
of using operators in a query.

92
18:26:47 --> 18:26:52
It's always variable:

93
18:26:52 --> 18:26:56
followed by MongoDB's name for the
operator, and then the comparison value.

94
18:26:57 --> 18:26:59
So where would you find
MongoDB's operators?

95
18:27:01 --> 18:27:03
Here are some of the operators
supported in MongoDB.

96
18:27:05 --> 18:27:08
These operators and others are listed
in the URL shown at the bottom.

97
18:27:10 --> 18:27:12
The operators shown here are color coded.

98
18:27:13 --> 18:27:16
The top blue set
are the comparison operators.

99
18:27:16 --> 18:27:22
We see the $gt, greater than,
operation that we used in the last slide.

100
18:27:25 --> 18:27:29
The green colored operations are array
operations which we'll see shortly.

101
18:27:29 --> 18:27:33
And the yellow operators at the bottom
are logical operations that

102
18:27:33 --> 18:27:37
combine two conditions in different ways
like the AND operation we saw in SQL.

103
18:27:37 --> 18:27:42
Now, the last operator $nor,
is interesting,

104
18:27:42 --> 18:27:48
because it is used to specify queries
when neither of two conditions must hold.

105
18:27:48 --> 18:27:51
For example, find all beer
whose name is neither bad nor

106
18:27:51 --> 18:27:55
is the price less than $6 per bottle.

107
18:27:55 --> 18:27:58
Now I would strongly encourage you
to play with these operators in

108
18:27:58 --> 18:27:59
your hands on session.

109
18:28:02 --> 18:28:04
Now I'm sure you remember
the like query in SQL.

110
18:28:06 --> 18:28:12
MongoDB uses regular expressions
to specify partial string matches.

111
18:28:12 --> 18:28:14
Now some of you may not know
what a regular expression is.

112
18:28:14 --> 18:28:16
Let's first use some examples.

113
18:28:18 --> 18:28:23
The first example is the same query
we saw before when we're asking for

114
18:28:23 --> 18:28:28
beer manufacturers,
whose name has a sub string A-M in it,

115
18:28:28 --> 18:28:31
so A-M can appear
anywhere within the name.

116
18:28:32 --> 18:28:33
To do this,

117
18:28:33 --> 18:28:38
the query condition first states that
it is going to use a $regex operation.

118
18:28:39 --> 18:28:45
And then we have to give
the partial string as /am/.

119
18:28:45 --> 18:28:51
Then it gives the directive
that this match

120
18:28:51 --> 18:28:56
should be case insensitive by placing
an i after the partial string.

121
18:28:56 --> 18:29:03
And if we just wanted to do names we
would stop right after the find function.

122
18:29:03 --> 18:29:08
But here we also want to do a count,
which is a post operation

123
18:29:08 --> 18:29:13
after the find, so we use .count
at the end of the find function.

124
18:29:14 --> 18:29:21
Now, what if we have the same query,
but we want the partial string A-m?

125
18:29:21 --> 18:29:24
To appear at the beginning of the name and

126
18:29:24 --> 18:29:27
you'd like the A to really
be a capital letter.

127
18:29:28 --> 18:29:32
In this case we use
the caret sign to indicate

128
18:29:32 --> 18:29:34
that the partial string is at
the beginning of the name.

129
18:29:36 --> 18:29:40
Naturally we also drop the i at the end
because the match is no longer case

130
18:29:40 --> 18:29:45
insensitive A more complex

131
18:29:45 --> 18:29:50
partial string pattern will be a case
where our name starts with capital A-m,

132
18:29:50 --> 18:29:54
then has a number of characters
in the middle and ends with corp.

133
18:29:55 --> 18:30:01
So for the first part,
the string pattern is ^Am.

134
18:30:01 --> 18:30:06
For the second part, that is,
any character in the middle, we use dot

135
18:30:06 --> 18:30:12
to represent any character, and star
to represent zero or more occurrences.

136
18:30:12 --> 18:30:15
For the third part, we say corp but

137
18:30:15 --> 18:30:19
put a dollar at the end to say that it
must appear at the end of the string.

138
18:30:20 --> 18:30:24
The regular expression pattern is
a sub-language, in itself, and

139
18:30:24 --> 18:30:27
is supported by most
programming languages today.

140
18:30:27 --> 18:30:31
We will refer you to the following
URL to learn more about it.

141
18:30:33 --> 18:30:36
Also, note an example that,
instead of saying, find.count,

142
18:30:36 --> 18:30:41
we can directly use the count function,
natively defined in MongoDB.

143
18:30:43 --> 18:30:48
One important feature of JSON is that
everything contain arrays, as a type of

144
18:30:48 --> 18:30:54
collection objects, this enables us
to query arrays in multiple ways.

145
18:30:54 --> 18:30:58
One of them,
is to consider an array as a list and

146
18:30:58 --> 18:31:03
perform intersection operations,
the first query shows this.

147
18:31:03 --> 18:31:04
The data item is shown on the right.

148
18:31:06 --> 18:31:10
It has the area value attribute
called tags with three entries.

149
18:31:10 --> 18:31:15
The first query asks if two specific
strings belong to the array.

150
18:31:16 --> 18:31:21
In other words, it wants to get
the document whose tagged attribute

151
18:31:21 --> 18:31:23
intersects with the query supplied array.

152
18:31:24 --> 18:31:28
In this case, there is an intersection and
the document is returned.

153
18:31:30 --> 18:31:32
In the second case, it is asking for

154
18:31:32 --> 18:31:36
a documents who's tags
attribute has no intersection.

155
18:31:37 --> 18:31:43
Now notice the $nin operator so
there is no intersection with this list.

156
18:31:45 --> 18:31:49
So in this document there exists and
intersection so nothing will be returned.

157
18:31:51 --> 18:31:56
A different kind of array query uses
the positions of the list elements and

158
18:31:56 --> 18:31:58
wants to extract a portion of the array.

159
18:31:59 --> 18:32:03
This is illustrated in
the third query which asks for

160
18:32:03 --> 18:32:06
the second and third items of the array.

161
18:32:06 --> 18:32:10
To encode this in MongoDB,
we use the $slice operator

162
18:32:10 --> 18:32:16
which needs two parameters,
the number of variable limits to skip,

163
18:32:16 --> 18:32:19
and the number of variable limits
to extract after skipping.

164
18:32:20 --> 18:32:25
In this case, we need to extract items two
and three, so the skip value is one and

165
18:32:25 --> 18:32:30
the number of items is two,
thus returning summer and Japanese.

166
18:32:31 --> 18:32:35
Now, we could get the same result if we
pose the query using the last statement.

167
18:32:37 --> 18:32:41
In this case, the minus says that
the system should count from the end and

168
18:32:42 --> 18:32:45
the true says that it should
extract two elements.

169
18:32:46 --> 18:32:51
Now if we omitted the minus sign,
it will come from the beginning and

170
18:32:51 --> 18:32:52
fetch the first two elements.

171
18:32:56 --> 18:33:02
Finally, we can also ask for a document
who's second element in tags is summer.

172
18:33:02 --> 18:33:08
In this case we use an array index
tags.1 to denote the second element.

173
18:33:12 --> 18:33:17
Compound statements are queries
with multiple query conditions that

174
18:33:17 --> 18:33:20
are combined using logical operations.

175
18:33:20 --> 18:33:26
The query shown here has one condition,
which is the and,are in terms of MongoDB,

176
18:33:26 --> 18:33:28
the $and of three different clauses.

177
18:33:29 --> 18:33:32
The last clause is
the most straight forward,

178
18:33:32 --> 18:33:35
it states that the desired
item should not be Coors.

179
18:33:35 --> 18:33:42
The first clause is an or, that is,
a $or, between two sub-conditions,

180
18:33:42 --> 18:33:48
A the prices either 3.99, or B it is 4.99.

181
18:33:48 --> 18:33:53
The second clause is also an or
of two sub conditions,

182
18:33:53 --> 18:33:58
A the rating is good, and
B the quantity is less than 20.

183
18:33:59 --> 18:34:02
This query shows that the $and and

184
18:34:02 --> 18:34:07
the $or operators need a list
that is an array of arguments.

185
18:34:07 --> 18:34:09
To draw a quick comparison,

186
18:34:09 --> 18:34:13
here's the example of the same
query imposed with SQL.

187
18:34:18 --> 18:34:22
Now, an important feature of
semi-structured data is that

188
18:34:22 --> 18:34:24
it allows nesting.

189
18:34:25 --> 18:34:29
We showed three documents here,
where there is an area named points,

190
18:34:29 --> 18:34:33
which in turn has two tuples with
the elements points and bonus.

191
18:34:33 --> 18:34:39
Let's assume that these three
documents are part of a collection, so

192
18:34:39 --> 18:34:42
they form three items in
an area called users.

193
18:34:43 --> 18:34:48
Our goal is to show how we can
write queries to extract data

194
18:34:48 --> 18:34:50
from these documents with nesting.

195
18:34:51 --> 18:34:54
The first query wants
to find documents for

196
18:34:54 --> 18:34:58
which, the value of points should
be less than or equal to 80.

197
18:34:58 --> 18:35:01
Now, which ones?

198
18:35:01 --> 18:35:04
Now, points.0,

199
18:35:04 --> 18:35:09
refers to the first tuple
under the outer points, and

200
18:35:09 --> 18:35:14
points.0.points, refers to
the first element of that tuple.

201
18:35:14 --> 18:35:17
Clearly, only the second
documents satisfies this query.

202
18:35:20 --> 18:35:23
Now, what happens if we have
the same query but we drop the zero?

203
18:35:24 --> 18:35:29
Now, we are looking for points.points
without specifying the array index.

204
18:35:29 --> 18:35:31
This means that the points element in

205
18:35:31 --> 18:35:35
any of the tuples should
have a value of 80 or less.

206
18:35:36 --> 18:35:40
So now, the first and the second
document will satisfy the query.

207
18:35:42 --> 18:35:46
We can put multiple conditions
as seen in the third query.

208
18:35:47 --> 18:35:51
It looks for a document where the points
element of a tuple, should be utmost 81,

209
18:35:51 --> 18:35:58
and the bonus should be exactly 20, and
clearly the second document qualifies.

210
18:35:58 --> 18:36:00
But does the third document qualify?

211
18:36:01 --> 18:36:07
In this case, the first tuple
satisfies points greater than 81 and

212
18:36:07 --> 18:36:09
the second tuple satisfies
bonus equal to 20.

213
18:36:09 --> 18:36:16
The answer is no, because the comma
is treated as an implicit and

214
18:36:16 --> 18:36:20
condition within the same double,
as shown in the yellow braces.

215
18:36:22 --> 18:36:25
Now remember that we said in course
two that all semi-structured

216
18:36:25 --> 18:36:27
data can be viewed as a tree.

217
18:36:28 --> 18:36:32
Now, what if I pick a node of a tree and
ask for all the descendents of that node?

218
18:36:33 --> 18:36:37
That would require the system
to recursively get chideNodes,

219
18:36:37 --> 18:36:39
over increasing depth from the given node.

220
18:36:40 --> 18:36:46
Unfortunately, at this time,
MongoDB does not support recursive search.

1
12:56:23 --> 12:56:28
A different kind of scaling problem arises
when we try to answer queries over a large

2
12:56:28 --> 12:56:33
number of data sources, but before we do
that let's see how a query is answered in

3
12:56:33 --> 12:56:35
the virtual data integration setting.

4
12:56:36 --> 12:56:39
We are going to use a toy
scenario in a medical setting

5
12:56:40 --> 12:56:43
simple as we have four data sources.

6
12:56:43 --> 12:56:45
Each with one table for
the sake of simplicity.

7
12:56:46 --> 12:56:51
Notice that two sources, S1 and
S3, have the same schema.

8
12:56:51 --> 12:56:55
Now this is entirely possible because
sources may be independent of each other.

9
12:56:56 --> 12:57:00
Further, there is no guarantee that
they would have the same exact content.

10
12:57:01 --> 12:57:05
Maybe these two sources represent
clinics at different locations.

11
12:57:07 --> 12:57:09
So next, we look at the target schema.

12
12:57:11 --> 12:57:15
For simplicity, let's consider that
it's not an algorithmically creative

13
12:57:15 --> 12:57:20
probabilistic mediator schema but just a
manually designed schema with five tables.

14
12:57:21 --> 12:57:26
But while we assume that
the target schema is fixed

15
12:57:26 --> 12:57:29
we want the possibility that
we can add more sources.

16
12:57:29 --> 12:57:32
That means more clinics
as the system grows.

17
12:57:34 --> 12:57:37
Now I'm beginning to
add the schema mapping.

18
12:57:38 --> 12:57:42
Now there are several techniques
of specifying schema mappings.

19
12:57:42 --> 12:57:45
One of them is called Local-as-View.

20
12:57:45 --> 12:57:50
This means we write the relations in each
source as a view over the target schema.

21
12:57:52 --> 12:57:56
But this way of writing the query,
as we can see here, may seem odd to you.

22
12:57:56 --> 12:58:00
It's called syntax, but
you don't need to know it.

23
12:58:00 --> 12:58:06
Just as an example, the first few things
the treats relation in S1 maps to,

24
12:58:06 --> 12:58:08
so you see that arrow, that means maps to.

25
12:58:09 --> 12:58:14
So it maps to the query select doctor,

26
12:58:14 --> 12:58:19
chronic disease from treats patient,
has chronic disease.

27
12:58:19 --> 12:58:25
Where treatspatient.patient Is equal
to has chronic disease dot patient.

28
12:58:28 --> 12:58:29
We see the query here in the yellow box.

29
12:58:30 --> 12:58:35
The only thing we should notice here
is that the select class on the query

30
12:58:35 --> 12:58:36
has two attributes doctor and

31
12:58:36 --> 12:58:42
chronic disease which are exactly the same
attributes of the treats relation in S1.

32
12:58:44 --> 12:58:46
Now let's ask a query that
gives the target schema.

33
12:58:47 --> 12:58:51
Which doctors are responsible for
discharging patients?

34
12:58:51 --> 12:58:53
Which translates to
the SQL query shown here.

35
12:58:55 --> 12:59:00
Now, the problem is how to translate
this query to a query that can

36
12:59:00 --> 12:59:02
be sent to the sources.

37
12:59:03 --> 12:59:08
Now ideally this should be simplest query
with no extra operations as shown here.

38
12:59:08 --> 12:59:16
S3 treats means treats
relation in sources 3.

39
12:59:16 --> 12:59:18
Now you can see the ideal answer.

40
12:59:19 --> 12:59:24
To find such an optimal query
reformulation, it turns out that this

41
12:59:24 --> 12:59:29
process is very complex and becomes
worse as a number of sources increases.

42
12:59:31 --> 12:59:33
Thus query reformulation

43
12:59:33 --> 12:59:38
becomes a significant scalability problem
in a big data integration scenario.

44
12:59:42 --> 12:59:43
Let's look at the second use case

45
12:59:44 --> 12:59:48
public health is a significant
component of our healthcare system.

46
12:59:49 --> 12:59:54
Public health systems monitor detect and
take action when epidemics strike.

47
12:59:54 --> 12:59:59
Not so long ago we have witnessed public
health concerns due to Anthrax virus,

48
12:59:59 --> 13:00:01
the swine flu and the bird flu.

49
13:00:02 --> 13:00:07
But these epidemics caused a group
called WADDS to develop a system for

50
13:00:07 --> 13:00:08
disease surveillance.

51
13:00:09 --> 13:00:13
This system would connect all local
hospitals in the Washington, DC area, and

52
13:00:13 --> 13:00:16
is designed to exchange
disease information.

53
13:00:16 --> 13:00:21
For example, if a hospital lab has
identified a new strain of a virus,

54
13:00:21 --> 13:00:23
other hospitals and the Centers for
Disease Control, CDC,

55
13:00:23 --> 13:00:26
in the network,
should be able to know about it.

56
13:00:28 --> 13:00:33
It should be clear that this needs a data
integration solution where the data

57
13:00:33 --> 13:00:38
sources would be the labs, the data would
be the lab tests medical records and

58
13:00:38 --> 13:00:42
even genetic profiles of the virus and
the subjects who might be infected.

59
13:00:43 --> 13:00:47
The table here shows the different
components with this architecture.

60
13:00:47 --> 13:00:50
We will just digest the necessary
parts for our requirement.

61
13:00:51 --> 13:00:56
Just know that RIM which stands for
Reference Information Model is global

62
13:00:56 --> 13:01:01
schema that this industry has developed
and expects to use as a standard.

63
13:01:04 --> 13:01:08
Why we want to exchange and combine new
information from different hospitals?

64
13:01:09 --> 13:01:11
Every hospital is independent and

65
13:01:11 --> 13:01:15
can implement their own information
system any way they see fit.

66
13:01:16 --> 13:01:20
Therefore even when there
are standards like HL-7 that

67
13:01:20 --> 13:01:24
specify what kind of data a held
cache system should have an exchange.

68
13:01:24 --> 13:01:29
There are considerable variations in
the implementation of the standard itself.

69
13:01:29 --> 13:01:33
For example the two wide boxes show
a difference in representation

70
13:01:33 --> 13:01:38
of the same kind of data, this should
remind you of the data variety problem.

71
13:01:39 --> 13:01:44
Let's say, we have a patient
with ID 19590520 whose lab

72
13:01:46 --> 13:01:50
reports containing her plasma protein
measurements are required for

73
13:01:50 --> 13:01:51
analyzing her health condition.

74
13:01:52 --> 13:01:56
The problem is that the patient
went to three different clinics and

75
13:01:56 --> 13:01:59
four different labs which all
implement the standards differently.

76
13:02:01 --> 13:02:02
On top of it?

77
13:02:02 --> 13:02:06
Each clinic uses its own
electronic medical record system

78
13:02:06 --> 13:02:08
which we have a very large amount of data.

79
13:02:09 --> 13:02:14
So the data integration system's job is to
transform the data from the source schema

80
13:02:14 --> 13:02:19
to the schema of the receiving
system in this case the rim system.

81
13:02:20 --> 13:02:23
This is sometimes called
the data exchange problem.

82
13:02:26 --> 13:02:30
Informally a data exchange
problem can be defined like this.

83
13:02:31 --> 13:02:35
Suppose we have a given database
whose relations are known.

84
13:02:36 --> 13:02:39
Let us say we also know
the target database's schema and

85
13:02:39 --> 13:02:41
the constraints the schema will satisfy.

86
13:02:43 --> 13:02:47
Further we know the desired schema
mappings between the source and

87
13:02:47 --> 13:02:49
this target schema.

88
13:02:49 --> 13:02:55
What we do not know is how to populate
the tuples in the target database.

89
13:02:55 --> 13:02:59
From the tuples in the socialization in
such a way that both schema mappings and

90
13:02:59 --> 13:03:02
target constraints
are simultaneously satisfied.

91
13:03:05 --> 13:03:09
In many domains like healthcare,
a significant amount of effort has been

92
13:03:09 --> 13:03:13
spend by the industry in
standardizing schemas and values.

93
13:03:14 --> 13:03:18
For example LOINC is a standard for
medical lab observations.

94
13:03:19 --> 13:03:22
Here item like systolic blood pressure or

95
13:03:22 --> 13:03:26
gene mutation are encoded in this
specific way as given by this standard.

96
13:03:27 --> 13:03:32
So, if we want to write that
the systolic/diastolic pressure of

97
13:03:32 --> 13:03:38
a individual is 132 by 90, we'll not write
out the string systolic blood pressure,

98
13:03:38 --> 13:03:40
but use the code for it.

99
13:03:40 --> 13:03:45
The ability to use standard code
is not unique to healthcare data.

100
13:03:45 --> 13:03:47
The 50 states of the US all
have two letter abbreviations.

101
13:03:49 --> 13:03:55
Generalizing therefore, whenever we have
data such as the domain is finite and have

102
13:03:55 --> 13:04:00
a standard set of code available, we give
a new opportunity of handling big deal.

103
13:04:02 --> 13:04:06
Mainly, reducing the data
size through compression.

104
13:04:08 --> 13:04:10
The compression refers to a way of

105
13:04:10 --> 13:04:13
creating an encoded
representation of data.

106
13:04:13 --> 13:04:17
So that this encoder form is smaller
than the original representation.

107
13:04:19 --> 13:04:23
A common encoding method is
called dictionary encoding.

108
13:04:23 --> 13:04:26
Consider a database with 10 million
record of patient visits a lab.

109
13:04:28 --> 13:04:32
Each record indicates a test and
its results.

110
13:04:32 --> 13:04:35
Now we show it this way like
in a columnar structure

111
13:04:36 --> 13:04:41
to make the point that the data is kept
in a column stored relational database

112
13:04:41 --> 13:04:43
rather than a row store
relational database.

113
13:04:44 --> 13:04:47
Now consider the column for test code.

114
13:04:47 --> 13:04:50
Where the type of test is codified
according to the standard.

115
13:04:52 --> 13:04:56
We replace a string representation
of the standard by a number.

116
13:04:58 --> 13:05:01
The mapping between
the original test code and

117
13:05:01 --> 13:05:04
the encoded number are also
stored separately.

118
13:05:04 --> 13:05:06
Now suppose there
are a total of 500 tests.

119
13:05:08 --> 13:05:13
So this separate table called
the dictionary here has 500 rows,

120
13:05:13 --> 13:05:18
which is clearly much smaller
than ten million right?

121
13:05:18 --> 13:05:23
Now 500 distinct values can be
represented by encoding them in 9 bits,

122
13:05:23 --> 13:05:25
because 2 to the power of 9 is 512.

123
13:05:27 --> 13:05:30
Other encoding techniques would be applied
to attributes like date and patient ID.

124
13:05:31 --> 13:05:36
That's full large data we cannot reduce
the number of total actual rules.

125
13:05:36 --> 13:05:39
So we have to store all ten million rules.

126
13:05:39 --> 13:05:44
But we can reduce the amount of space
required by storing data in a column

127
13:05:44 --> 13:05:49
oriented data store and
by using compression, indeed modern

128
13:05:49 --> 13:05:56
systems use credit processing algorithms
to operate directly on compress data.

129
13:06:00 --> 13:06:03
Data compression is an important
technology for big data.

130
13:06:06 --> 13:06:11
And just like is a set of qualified
terms for lab tests, clinical data

131
13:06:11 --> 13:06:16
also uses SNOMED which stands for
systematized nomenclature of medicine.

132
13:06:17 --> 13:06:20
SNOMED is a little more
than just a vocabulary.

133
13:06:21 --> 13:06:23
It does have a vocabulary of course.

134
13:06:24 --> 13:06:29
The vocabulary is the collection
of medical terms in human and

135
13:06:29 --> 13:06:35
medicine to provide codes, terms, synonyms
and definitions that cover anatomy,

136
13:06:35 --> 13:06:39
diseases, findings, procedures,
micro organisms, substances etcetera.

137
13:06:40 --> 13:06:42
But it also has relationships.

138
13:06:44 --> 13:06:48
As you can see,
a renal cyst is related to kidney because

139
13:06:48 --> 13:06:51
kidney's the finding site of a renal cyst.

140
13:06:53 --> 13:06:58
If we query against an ontology,
it would look like a graph grid.

141
13:06:59 --> 13:06:59
In this box,

142
13:06:59 --> 13:07:04
we are asking to find all patient
findings with a benign tumor morphology.

143
13:07:05 --> 13:07:10
In terms of querying, we are looking for
edges of a graph where one noticed

144
13:07:10 --> 13:07:15
the concept that we need to find which is
connected to a node called benign neoplasm

145
13:07:15 --> 13:07:20
that is benign tumor through an edge
called associated morphology

146
13:07:21 --> 13:07:25
that applying this query against
the data here produces all benign tumors

147
13:07:25 --> 13:07:29
of specific organs as you can
see by the orange rounded group.

148
13:07:32 --> 13:07:34
But now that we have these terms,

149
13:07:34 --> 13:07:38
we can use these terms to search
outpatient records with these terms would

150
13:07:38 --> 13:07:43
have been used so
what's the essence of this used case.

151
13:07:45 --> 13:07:50
This is can shows that an in division
system in a public health domain and

152
13:07:50 --> 13:07:53
in many other domains must
be able to handle variety.

153
13:07:54 --> 13:08:01
In this case there's a Global Schema
called RiM shown here all queries and

154
13:08:01 --> 13:08:06
analyses performed by a data analysts
should be against this global schema.

155
13:08:07 --> 13:08:11
However, the actual data which is
generated by different medical facilities

156
13:08:11 --> 13:08:16
would need to be transformed
into data in this schema.

157
13:08:16 --> 13:08:21
This would require not only
format conversions, but

158
13:08:21 --> 13:08:26
it would need to respect all constraints
imposed by the source and by the target.

159
13:08:26 --> 13:08:31
For example, a source may not distinguish
between an emergency surgical procedure

160
13:08:31 --> 13:08:33
and a regular surgical procedure.

161
13:08:33 --> 13:08:37
But the target may want to
put them in different tables.

162
13:08:39 --> 13:08:41
We also saw that
the integration system for

163
13:08:41 --> 13:08:45
this used case would need to use
quantified data but this gives

164
13:08:45 --> 13:08:49
us the opportunity to use data compression
to gauge story and query efficiency.

165
13:08:51 --> 13:08:56
In terms of variety, we saw how
relational data like patient records XML

166
13:08:56 --> 13:09:01
data like HL7 events, and
graph data like ontologies, are co-used.

167
13:09:03 --> 13:09:08
To support this, the integration
system must be able to do both model

168
13:09:08 --> 13:09:11
transformation and query transformation.

169
13:09:12 --> 13:09:16
Query transformation is the process of
taking a query on the target schema.

170
13:09:17 --> 13:09:21
Converting it to a query
against a different data model.

171
13:09:21 --> 13:09:27
For example, part of an SQL query against
the RIM may need to go to snowmad and

172
13:09:27 --> 13:09:30
hence when you to be converted to
a graph query in the snowmad system.

173
13:09:31 --> 13:09:34
Model transformation is
a process of taking data

174
13:09:34 --> 13:09:38
represented in one model
in one source system and

175
13:09:38 --> 13:09:43
converting it to an equivalent data
in another model the target system.

1
02:06:04 --> 02:06:06
Hello, my name is Victor Lou.

2
02:06:06 --> 02:06:11
I am the Solutions Engineer of
Global Philanthropy of Daameer.org.

3
02:06:11 --> 02:06:12
Helping researchers, academia,

4
02:06:12 --> 02:06:15
and non-profits find the insights
that matter using data.

5
02:06:15 --> 02:06:19
Today I am here to give you
a production environment,

6
02:06:19 --> 02:06:21
personalized music recommendation project.

7
02:06:21 --> 02:06:25
That was done for PonoMusic and
so what is Pono Music?

8
02:06:27 --> 02:06:30
Pono Music is revolutionizing music
listening by bringing back native,

9
02:06:30 --> 02:06:32
high-resolution audio.

10
02:06:32 --> 02:06:35
It's the way that artists intended and
recorded their music.

11
02:06:35 --> 02:06:40
Neil Young, the iconic artist and
musician is also the founder and CEO.

12
02:06:40 --> 02:06:44
Pono Music is the only complete
high-resolution music ecosystem.

13
02:06:44 --> 02:06:47
That includes the Pono player, the Pono
Music store, the Pono Music community.

14
02:06:49 --> 02:06:52
They have been super
generous in allowing us to

15
02:06:52 --> 02:06:55
take a look behind the scenes as how
we built out the recommendations.

16
02:06:57 --> 02:07:00
So, David Meer supports their mission
by providing a music recommendation

17
02:07:00 --> 02:07:04
engine that is both scalable and
flexible as Pono grows their user base.

18
02:07:04 --> 02:07:09
Let's begin by visiting Pono
music's website as you can see,

19
02:07:09 --> 02:07:11
they have a few shelves that focus on

20
02:07:11 --> 02:07:15
various recommendations that are not
tailored to each individual user.

21
02:07:15 --> 02:07:18
If you login,
then an additional shelf would appear

22
02:07:18 --> 02:07:21
that would deliver the recommendations
that are created in data gear.

23
02:07:21 --> 02:07:23
So how does DataMirror accomplish this.

24
02:07:23 --> 02:07:28
In a nutshell, DataMirror is a end to end,
antalis platform which contains ingestion,

25
02:07:28 --> 02:07:33
preparation analysis, visualization and
operationalization all the same platform.

26
02:07:33 --> 02:07:36
And having all the capabilities
in one platform is superior to

27
02:07:36 --> 02:07:38
traditional technology stack.

28
02:07:38 --> 02:07:43
Integration between disparate technologies
brings a lot of unnecessary challenges.

29
02:07:43 --> 02:07:45
We take advantage of open
source big data technologies.

30
02:07:45 --> 02:07:50
Specifically, we run natively and
Hadoop and Spark for our back end.

31
02:07:50 --> 02:07:53
And we leverage D3JS on the front end for
visualizations.

32
02:07:53 --> 02:07:55
Our Excel-like interface makes it easy for

33
02:07:55 --> 02:07:59
folks who don't know how to code to
also get in with the data modeling.

34
02:07:59 --> 02:08:01
It is very much self-service.

35
02:08:01 --> 02:08:04
In the case of Pono Music's
data mirror deployment,

36
02:08:04 --> 02:08:06
we have deployed a Hadoop
on Amazon on web services.

37
02:08:06 --> 02:08:08
But we can also be deployed off premise if

38
02:08:08 --> 02:08:10
needed it doesn't have
to be in the clouds.

39
02:08:10 --> 02:08:13
In fact, we could work with many many
different distributions of Hadoop.

40
02:08:13 --> 02:08:16
We can access dean rear through
any modern web browser.

41
02:08:16 --> 02:08:20
As you can see here we're using Google
Chrome but it works on any browser, right?

42
02:08:20 --> 02:08:22
So lets go ahead and take a look at
the a; let's log into data grip,

43
02:08:22 --> 02:08:25
so we're going to go ahead and
log in here..

44
02:08:27 --> 02:08:32
So what you're seeing here is the browser
tab and we can see various artifacts

45
02:08:32 --> 02:08:36
which include connections such
as connections to the pull nose,

46
02:08:36 --> 02:08:40
salesforce.com instance or
a connection to AWS S3 instance.

47
02:08:40 --> 02:08:42
Because DataMirror comes with so

48
02:08:42 --> 02:08:46
many prebuilt connectors which include
Connections to salesforce.com and S3.

49
02:08:46 --> 02:08:50
We can easily grant access to
those systems in order to pull or

50
02:08:50 --> 02:08:52
push data in and out of the dev.

51
02:08:52 --> 02:08:55
For example, let's take a quick look at
the salesforce.com configuration here.

52
02:08:55 --> 02:09:03
As you can see, we've already configured
salesforce as the connector type.

53
02:09:03 --> 02:09:04
When we hit next here,

54
02:09:04 --> 02:09:08
we'll we that you can authorize
a data manager to retrieve data.

55
02:09:08 --> 02:09:10
We're not going to do it because
this is a production environment, so

56
02:09:10 --> 02:09:13
I don't want to disrupt the pools.

57
02:09:13 --> 02:09:17
But if we were to click it, it'll give
us a salesfloor.com blogging stream.

58
02:09:17 --> 02:09:21
And so as soon as we login, the Datameer
would have access via the O-Off token.

59
02:09:21 --> 02:09:24
And so this is valid for a period of time.

60
02:09:24 --> 02:09:24
I'm going to go ahead and

61
02:09:24 --> 02:09:27
hit the cancel button as this
is a production environment.

62
02:09:27 --> 02:09:32
And so now we're ready to look
at an import job artifact.

63
02:09:32 --> 02:09:35
So this is the sales force import job and

64
02:09:35 --> 02:09:40
as you can see we go in here and
we configure it.

65
02:09:40 --> 02:09:45
We can see that it's connected to
the Pono Music SMDC connection.

66
02:09:48 --> 02:09:53
I'm going to go ahead and
hit next here.with salesforce.com,

67
02:09:53 --> 02:09:56
we just have to simply define the SOQL,

68
02:09:56 --> 02:10:00
which is basically a querying
language that's based on SQL.

69
02:10:00 --> 02:10:03
To locate data contained within
the salesforce.com objects.

70
02:10:03 --> 02:10:08
You can see the select statement here,
you can also see the familiar

71
02:10:08 --> 02:10:14
from statement, and you can also
see the where statement as well.

72
02:10:14 --> 02:10:16
So in order to protect
the privacy of the users,

73
02:10:16 --> 02:10:20
Datameer has the capability to obfuscate
sort of columns that are sensitive.

74
02:10:20 --> 02:10:23
And so in other words, we can scramble
sensitive fields such as email or

75
02:10:23 --> 02:10:25
physical addresses.

76
02:10:25 --> 02:10:29
And at this point in time, I'm going to
go ahead and cancel out, and I'm going to

77
02:10:29 --> 02:10:32
show the next part of this demo in
an obfuscated workbook environment.

78
02:10:32 --> 02:10:37
The artifacts contained in the obfuscated
folders are duplicates of the import job

79
02:10:37 --> 02:10:42
of the workbook from their production
counterparts as you can see here.

80
02:10:44 --> 02:10:48
So as you can see in the import
job settings we have configured

81
02:10:48 --> 02:10:55
the same connections here but
for the obfuscated columns here.

82
02:10:55 --> 02:10:58
Now we're obfuscating the owner
email as well as the street address.

83
02:10:58 --> 02:10:59
Now we hit Next here.

84
02:11:03 --> 02:11:07
And so you can see, various fields are
being pulled here from Salesforce, right.

85
02:11:07 --> 02:11:09
And in particular,
I want to highlight the.

86
02:11:10 --> 02:11:13
The owner email, that field, so as you
can see these are the obfuscated fields.

87
02:11:13 --> 02:11:17
Similarly, the actual street address,
that's also masked as well.

88
02:11:17 --> 02:11:21
I'm going to go ahead and hit Cancel.

89
02:11:21 --> 02:11:24
I will go back to, rather we're going to
go check out the workbook environment,

90
02:11:24 --> 02:11:27
which is where we do the preparation and
analysis.

91
02:11:32 --> 02:11:35
To the obfuscated workbook environment.

92
02:11:35 --> 02:11:38
As you can see we're starting
with a raw source of data here.

93
02:11:38 --> 02:11:42
This was the data that was
contained in the data sheet.

94
02:11:42 --> 02:11:47
Take note that there's a variety of icons
that indicate different types of sheets.

95
02:11:47 --> 02:11:49
So you could see here this
is the raw data type.

96
02:11:49 --> 02:11:52
This is a regular worksheet.

97
02:11:52 --> 02:11:55
You have a union sheet and
you have a joint sheet and

98
02:11:55 --> 02:11:58
we'll go into a little bit
more detail as we get to them.

99
02:11:58 --> 02:12:01
So, as you can see,
the interface is very much like Excel.

100
02:12:01 --> 02:12:06
Looking at the second sheet pairs, and
it's using a function called group by.

101
02:12:06 --> 02:12:11
So the group by function is a type of
aggregation function in Datameer speak.

102
02:12:11 --> 02:12:14
As you can see, you can build
the functions by pointing and clicking.

103
02:12:14 --> 02:12:18
But you can also enter the formula
to create nested functions or

104
02:12:18 --> 02:12:23
Apply arithmetic
operations just as a cell.

105
02:12:23 --> 02:12:25
We take all the unique combinations
of albums purchased for

106
02:12:25 --> 02:12:28
each of the unique email
using a group pair function.

107
02:12:29 --> 02:12:32
Then we pull out each of the elements
of the pairings using a list element

108
02:12:32 --> 02:12:34
function, so let's see here.

109
02:12:35 --> 02:12:38
If you were to build a function
from scratch, you can see this.

110
02:12:38 --> 02:12:43
So then you create a group by function,
and then direct it at a column.

111
02:12:43 --> 02:12:45
You basically pass it to argument.

112
02:12:47 --> 02:12:51
To find out the occurrence
of each pairing of albums,

113
02:12:51 --> 02:12:56
we're going to go ahead and
look at this LR sheet.

114
02:12:56 --> 02:12:59
And so these are, we're using group
by functions as well here, so

115
02:12:59 --> 02:13:02
we're grouping by the first item one,
and then item two.

116
02:13:02 --> 02:13:04
And then, we're doing a group count,

117
02:13:04 --> 02:13:07
which is counting the frequency
of occurrences of those cells.

118
02:13:07 --> 02:13:10
Similarly, we're going to
do a similar thing but

119
02:13:10 --> 02:13:15
with the item two first now, and then
item one, and then doing a group count.

120
02:13:15 --> 02:13:17
And the reason is,
we don't care about the order.

121
02:13:17 --> 02:13:21
And so, we repeat this on the next sheet
reversing the order of the albums to make

122
02:13:21 --> 02:13:25
sure that we capture all the combinations
since the order doesn't matter to us.

123
02:13:28 --> 02:13:32
And then we use our unit sheet function
to append the second frequency

124
02:13:32 --> 02:13:38
counting sheet to the first frequency
counting sheet, as you can see here.

125
02:13:38 --> 02:13:40
It's drag and drop, of course, and
so we've connected them together.

126
02:13:40 --> 02:13:43
You can do multi-sheets but
in this case we only have two, right.

127
02:13:43 --> 02:13:46
So it's, okay.

128
02:13:46 --> 02:13:51
We ignore the co-occurrent set sheet as it
is currently not used in the final output.

129
02:13:51 --> 02:13:55
We were kind of experimenting
with other recommendation models.

130
02:13:55 --> 02:13:56
We're rebuilding those workbooks.

131
02:13:56 --> 02:14:00
So one of the really cool features for
we can execute the workbook, but

132
02:14:00 --> 02:14:03
deselect the storage of
intermediary sheets.

133
02:14:03 --> 02:14:06
This means that we only need
relevant functions are executed

134
02:14:06 --> 02:14:11
at a to the data lineage of columns that
are contained in the selector sheet.

135
02:14:11 --> 02:14:16
Moving on, we have to create
a separate sheet from the raw data,

136
02:14:16 --> 02:14:20
that counts the number of times an album
occurs across all of the purchases.

137
02:14:20 --> 02:14:24
So you can see here, which students will
buy an album and then we do a count.

138
02:14:28 --> 02:14:33
We bring the code occurrences and
the frequency together using the join

139
02:14:33 --> 02:14:37
feature as you can see,
it is a drag and drop interface.

140
02:14:37 --> 02:14:40
You got the obfuscated workbook
each of the worksheets here and

141
02:14:40 --> 02:14:43
within each worksheets,
you got various columns.

142
02:14:43 --> 02:14:49
You can bring those, you can drag and
drop them into here to do the joins.

143
02:14:49 --> 02:14:54
You can do different types of joins,
you got the inner, outer, etc.

144
02:14:54 --> 02:14:57
You can also do multi column joins.

145
02:14:57 --> 02:14:58
You can do multi sheet joins.

146
02:14:58 --> 02:15:03
You can also select which columns
you want to keep during the join.

147
02:15:03 --> 02:15:06
Go ahead and cancel out here.

148
02:15:06 --> 02:15:10
Datameer does not allow us to add
additional functions to a joint sheet so

149
02:15:10 --> 02:15:13
we duplicate it by right clicking and
then hitting the duplicate button.

150
02:15:15 --> 02:15:18
And then we create the next sheet,
which contains a link to columns, so

151
02:15:18 --> 02:15:21
it will be linked to the rating sheet.

152
02:15:21 --> 02:15:26
This is, see the link columns back
to the joint sheets and then.

153
02:15:27 --> 02:15:32
We simply add a calculation
where it's the number of times

154
02:15:32 --> 02:15:36
the co-occurrence occurs divided by
the number of times that the first album

155
02:15:36 --> 02:15:39
in the column appears
throughout the data set.

156
02:15:39 --> 02:15:44
The idea is to give a high recommendation
for albums that appear frequently

157
02:15:44 --> 02:15:48
together While simultaneously
penalizing albums that are too common.

158
02:15:48 --> 02:15:52
And so at this point,
I would like to write a few more joins

159
02:15:52 --> 02:15:55
that bring together the album ID,
the email,

160
02:15:55 --> 02:16:00
recommended album ID based on the album
ID and email, so take a look here.

161
02:16:03 --> 02:16:07
So, as you can see here and then,

162
02:16:07 --> 02:16:12
before we finish,
we also do an anti joint by performing

163
02:16:12 --> 02:16:18
the outer left joint with
the almighty in the email.

164
02:16:18 --> 02:16:22
And then filtering, so this is,
by the way, this is a double column joint.

165
02:16:24 --> 02:16:28
And then we do a filter, so
then we filter out all the, for

166
02:16:28 --> 02:16:30
the obfuscated owner
fields that are empty.

167
02:16:30 --> 02:16:34
We apply a filter to it.

168
02:16:34 --> 02:16:39
So finally, we use a few group I
functions to do some deep duplication,

169
02:16:39 --> 02:16:42
so we use group by functions again.

170
02:16:42 --> 02:16:46
And then, so we make sure that the album
functions are unique for each user and

171
02:16:46 --> 02:16:48
then we use a group max function.

172
02:16:49 --> 02:16:53
In order to preserve the highest
rating for each of the unique

173
02:16:54 --> 02:16:59
recommendations the final desired
output is on the recommenders sheet.

174
02:16:59 --> 02:17:05
It leverages the group top end function,
so you can see here group top numbers.

175
02:17:05 --> 02:17:09
So we only want to look at the to
20 recommendations for each user.

176
02:17:09 --> 02:17:13
And in addition,
we also create a email row number field,

177
02:17:13 --> 02:17:17
which is a key that needed for
salesforce.com, and

178
02:17:17 --> 02:17:22
it is generated by a simple
concatenation function here.

179
02:17:22 --> 02:17:26
Something that note that [SOUND] is
big on data governance, data lineage.

180
02:17:26 --> 02:17:30
As you can see, we can keep track of
how all the raw data flows through

181
02:17:30 --> 02:17:33
from the raw data set all
the way to the end product.

182
02:17:35 --> 02:17:40
So each workbook functions can be
also exported in JSON format to keep

183
02:17:40 --> 02:17:42
track of revision history.

184
02:17:42 --> 02:17:45
We're not ready to operationalize
things and so how do we do it?

185
02:17:45 --> 02:17:49
Get our workbook configurations.

186
02:17:49 --> 02:17:50
Go ahead and exit out of the workbook.

187
02:17:53 --> 02:17:54
Let's go back to
the production environment.

188
02:18:00 --> 02:18:01
As you can see,

189
02:18:01 --> 02:18:05
the workbook recalculation retriggers
when the import job is completed.

190
02:18:05 --> 02:18:07
We also only retain the latest results, so

191
02:18:07 --> 02:18:13
we select purge historical results,
as you can see here.

192
02:18:13 --> 02:18:18
As I mentioned earlier, a data mirror
makes it easy to operationalize a workbook

193
02:18:18 --> 02:18:21
because we only select
the final output sheets.

194
02:18:21 --> 02:18:25
And, so you see everything's unchecked
except for the recommended sheet.

195
02:18:26 --> 02:18:30
And Datameer basically will automatically
determine what is a critical path

196
02:18:30 --> 02:18:32
of the intermediary sheets to
produce the desired output sheet.

197
02:18:34 --> 02:18:37
Once the recommendation
workbook is calculated,

198
02:18:37 --> 02:18:39
then this triggers the export job run.

199
02:18:39 --> 02:18:42
I'm going to go ahead and
move to the export job artifact.

200
02:18:42 --> 02:18:44
Exit out of this.

201
02:18:44 --> 02:18:46
Go ahead and save, or not save.

202
02:18:46 --> 02:18:51
We'll see the export job,
let's go ahead and configure that.

203
02:18:56 --> 02:18:59
Hit Next, Share.

204
02:18:59 --> 02:19:04
CSV outputs, so.

205
02:19:06 --> 02:19:08
This triggers the export job to run,

206
02:19:08 --> 02:19:12
which is how we basically push
the results of the recommendations to S3.

207
02:19:12 --> 02:19:18
We elected to use S3 because we
are already hosting at services and

208
02:19:18 --> 02:19:20
S3 is an affordable storage solution.

209
02:19:20 --> 02:19:23
We cannot push directly to
salesforce.com at the present because

210
02:19:23 --> 02:19:26
It's a only connection.

211
02:19:26 --> 02:19:30
Therefore, we need to push
data to the storage area,

212
02:19:30 --> 02:19:34
which can be scheduled for using Apex.

213
02:19:34 --> 02:19:37
A general challenge for
is the role limitation on each pool.

214
02:19:37 --> 02:19:42
Fortunately, DataMirror comes
with the option to push

215
02:19:42 --> 02:19:45
files that are broken
into one megabyte chunks.

216
02:19:45 --> 02:19:50
So as you can see here in
advanced settings 1 megabyte, and

217
02:19:50 --> 02:19:55
then we set a type of
consecutive numbering scheme.

218
02:19:55 --> 02:19:57
And so it's dynamic naming conventions
using time stamps and numbering for

219
02:19:57 --> 02:19:59
each of the chunks.

220
02:19:59 --> 02:20:02
So that sums up the current
co-curds/d deployment.

221
02:20:02 --> 02:20:07
As you've seen, we have easily
integrated data from Salesforce.com,

222
02:20:07 --> 02:20:11
created a recommendation model, and
set up a push to S3 automatically for

223
02:20:11 --> 02:20:13
easy consumption back to Salesforce.com.

224
02:20:13 --> 02:20:17
Finally, we operationalized this by
triggering each of the steps sequentially

225
02:20:17 --> 02:20:20
all in sitting on top of
the powerful Hadoop platform.

226
02:20:20 --> 02:20:23
What's next?

227
02:20:23 --> 02:20:27
Well, Pono Music is working on
implementing Google Analytics tracking for

228
02:20:27 --> 02:20:30
each user at the Apple pages.

229
02:20:30 --> 02:20:33
So let's go back to
the Apple Music Store here.

230
02:20:33 --> 02:20:36
Just for an example, let's take a look
at the Train Led Zeppelin Two Album.

231
02:20:37 --> 02:20:40
As you can see all of those album
IDs that we were looking at earlier,

232
02:20:40 --> 02:20:42
these are actually embedded in the URL.

233
02:20:44 --> 02:20:48
This means that in the near future, we
can actually adapt recommendations based

234
02:20:48 --> 02:20:53
on the buying behavior to recommendations
based on both buying and browser behavior.

235
02:20:53 --> 02:20:57
We can look at recommendations based on
genre, perhaps we can try to look at most

236
02:20:57 --> 02:21:01
recent purchases or browsing behavior in
the last three to six to nine months.

237
02:21:01 --> 02:21:05
Or we could use album metadata such as
album release date to give additional

238
02:21:05 --> 02:21:07
recommendations.

239
02:21:07 --> 02:21:11
Also, it is quite simple to just
duplicate the recommendations workbook in

240
02:21:11 --> 02:21:14
[INAUDIBLE] to try any
number of these options.

241
02:21:14 --> 02:21:17
And so,
that means we can do a lot of AB testing,

242
02:21:17 --> 02:21:22
as we track how users react to each of
the modified recommendation algorithms.

243
02:21:22 --> 02:21:25
The possibilities are literally endless.

244
02:21:25 --> 02:21:28
So anyhow, thanks again for
checking out how Data Mirror

245
02:21:28 --> 02:21:31
builds a simple code current
recommendation engine for music.

246
02:21:31 --> 02:21:33
Please feel free to direction questions or

247
02:21:33 --> 02:21:36
comments to me at
victor.liu@datamirror.com.

248
02:21:36 --> 02:21:38
Thanks again.

1
04:27:43 --> 04:27:51
Amarnath just finished overviewing
querying and integrational big data.

2
04:27:51 --> 04:27:55
Although, fundamental understanding
of these concepts are important.

3
04:27:56 --> 04:27:59
Some of these tasks can be accomplished

4
04:27:59 --> 04:28:03
using graphical user interface
based tools or software products.

5
04:28:04 --> 04:28:09
In this short module,
we introduce you to two of those tools.

6
04:28:09 --> 04:28:11
Splunk and Datameer.

7
04:28:13 --> 04:28:19
We have selected a few videos from
the site of our sponsors Splunk,

8
04:28:19 --> 04:28:23
to give you an overview of using such
tools for different applications.

9
04:28:24 --> 04:28:27
You will also have a short
hands-on activity on Splunk.

10
04:28:29 --> 04:28:34
In addition,
we provide you a comprehensive video on

11
04:28:34 --> 04:28:38
using Datamere in
the digital music industry.

12
04:28:38 --> 04:28:43
Keep in mind, Splunk and
Datamere are just two of the many

13
04:28:43 --> 04:28:48
tools in this category, but
they represent a huge industry.

1
08:56:45 --> 08:56:50
Open XC is an open source hardware-software platform.

2
08:56:50 --> 08:56:55
We wanted to see what correlations we could actually create within this model dashboards.

3
08:56:55 --> 08:57:00
It plugs into your car, and it gives you all the data that you can possibly want.

4
08:57:00 --> 08:57:03
What we're interested in is looking at how you

5
08:57:03 --> 08:57:06
can make the car much more modular and customizable,

6
08:57:06 --> 08:57:08
getting a lot of this data and afford cars

7
08:57:08 --> 08:57:10
out and letting developers do whatever they want with it.

8
08:57:10 --> 08:57:14
You could kind of like update technology as technology progresses.

9
08:57:14 --> 08:57:15
So what we're really trying to do is like catch up with

10
08:57:15 --> 08:57:20
the consumer electronics design cycle not the automotive design cycle types.

11
08:57:20 --> 08:57:22
The clock hasn't started yet.

12
08:57:22 --> 08:57:26
One of the tests that we did was a gas car versus an electric car.

13
08:57:26 --> 08:57:28
So we're trying to see which is faster,

14
08:57:28 --> 08:57:29
which is expecting the most energy,

15
08:57:29 --> 08:57:32
which is the most cost efficient.

16
08:57:32 --> 08:57:35
And we wanted to

17
08:57:35 --> 08:57:39
measure doesn't electric car driver drive different than a gas car driver.

18
08:57:39 --> 08:57:41
So actually by using the Open XC data,

19
08:57:41 --> 08:57:43
and by splunking that creating links in

20
08:57:43 --> 08:57:47
dashboards will give it to match ups and correlations.

21
08:57:53 --> 08:57:59
It all comes down to how much are they mashing the gas.

22
08:57:59 --> 08:58:05
We've been internally calling that dashboard the "Lead Foot Dashboard".

23
08:58:15 --> 08:58:23
Who's the most efficient driver?

24
08:58:23 --> 08:58:26
There's one of our drivers actually going low and

25
08:58:26 --> 08:58:30
slow and actually using into the brakes not really gnashing on the gas,

26
08:58:30 --> 08:58:32
and driving more like a normal person would.

27
08:58:32 --> 08:58:37
When you drive an electric car, it's a lot quicker off the line than a gas powered car.

28
08:58:37 --> 08:58:40
So, we find people kind of smashing

29
08:58:40 --> 08:58:43
on the gas pedal but sort of easing up on the electric pedal.

30
08:58:43 --> 08:58:45
So I've got some data for you here.

31
08:58:45 --> 08:58:47
We see. What's this I'm doing?

32
08:58:47 --> 08:58:49
It says you're doing.

33
08:58:49 --> 08:58:50
There we go.

34
08:58:50 --> 08:58:54
Battery car is high. There you go.

35
08:58:54 --> 08:58:57
Hurry up.

36
08:58:57 --> 08:58:59
There we go. We don't even need because Open XC, We just cut it.

37
08:58:59 --> 08:59:01
These are basically all of

38
08:59:01 --> 08:59:05
the sensor readings that are being broadcast from that Firefly box.

39
08:59:05 --> 08:59:08
It's plugged into the diagnostics port of the car.

40
08:59:08 --> 08:59:11
It's about how much power can we kind of give

41
08:59:11 --> 08:59:15
away in order to incentivize a community of makers.

42
08:59:15 --> 08:59:17
We want to be able to actually harness that and create

43
08:59:17 --> 08:59:20
an environment where they can much more quickly,

44
08:59:20 --> 08:59:21
experiment with our ideas,

45
08:59:21 --> 08:59:24
enrich the platform in general,

46
08:59:24 --> 08:59:29
make our vehicles in that sense more valuable.

1
17:56:01 --> 17:56:04
In this hands on activity,
we will be performing queries in Splunk.

2
17:56:05 --> 17:56:08
First, we will open a browser and
login to Splunk.

3
17:56:08 --> 17:56:12
Next, we will import a CSV file and
search its contents.

4
17:56:12 --> 17:56:17
We will see how to filter fields for
specific values and

5
17:56:17 --> 17:56:19
also perform statistical
calculations on the data.

6
17:56:19 --> 17:56:22
Let's begin.

7
17:56:22 --> 17:56:27
First, open a web browser and
navigate to the Splunk web page.

8
17:56:27 --> 17:56:32
We'll enter localhost:8000.

9
17:56:35 --> 17:56:40
Next we'll log in to Splunk using
admin and the default password.

10
17:56:50 --> 17:56:54
Next, we'll import a CSV file into Splunk.

11
17:56:54 --> 17:56:55
We'll click on Add Data.

12
17:56:59 --> 17:57:04
Upload We'll click on Select File.

13
17:57:07 --> 17:57:10
And we'll choose the census.csv
file that we downloaded.

14
17:57:15 --> 17:57:16
Click Next.

15
17:57:21 --> 17:57:24
On the left,
it should say source type csv.

16
17:57:24 --> 17:57:29
If it does not, click on the button and

17
17:57:29 --> 17:57:34
go down to Structured and select csv.

18
17:57:34 --> 17:57:36
In this table,
we see a preview of the data.

19
17:57:38 --> 17:57:41
You should see the column names
of the CSV file at the top.

20
17:57:43 --> 17:57:44
Click on Next.

21
17:57:47 --> 17:57:48
Review.

22
17:57:50 --> 17:57:51
And Submit.

23
17:57:53 --> 17:57:57
Now that the file has been imported
successfully, click on Start Searching.

24
17:58:03 --> 17:58:08
In the search box,
it fills in the default query,

25
17:58:08 --> 17:58:15
source="cencus.csv" the host name and
sourcetype="csv"

26
17:58:17 --> 17:58:21
We could change these fields
to search other files or

27
17:58:21 --> 17:58:24
other data types if we
imported those into Splunk.

28
17:58:24 --> 17:58:30
Now let's search census.csv for
particular values in the fields.

29
17:58:30 --> 17:58:33
Let's search for all the data
where the state is California.

30
17:58:36 --> 17:58:39
We'll enter STNAME="California".

31
17:58:45 --> 17:58:49
You'll see the results down here.

32
17:58:49 --> 17:58:52
You could search for
other states by using or.

33
17:58:52 --> 17:58:56
For example, we can add OR
STNAME="Alaska".

34
17:58:56 --> 17:59:00
This will search for
state names equal to California or Alaska.

35
17:59:04 --> 17:59:06
We can add conditions
to our query as well.

36
17:59:06 --> 17:59:13
Let's search for state equals California
whose population was over one million.

37
17:59:13 --> 17:59:20
We'll do this by saying
STNAME = "California"

38
17:59:20 --> 17:59:25
CENSUS2010POP > 1000000.

39
17:59:31 --> 17:59:34
Can limit the results to one or
more columns.

40
17:59:34 --> 17:59:41
You do this by adding pipe table
CTYNAME to the end of our query.

41
17:59:46 --> 17:59:51
In Spunk queries, the pipe command is used
to send the outputs from the first part of

42
17:59:51 --> 17:59:52
the query into the next.

43
17:59:55 --> 17:59:57
You can also show more than
one column from the output.

44
17:59:57 --> 18:00:01
If we add a comma CENSUS2010POP
to the end of this,

45
18:00:01 --> 18:00:06
we'll see both the city name and
the population.

46
18:00:06 --> 18:00:09
You can also see a visualization of this
data by clicking on the Visualization tab.

47
18:00:12 --> 18:00:17
At the bottom, on the x-axis,
we see the county names, and

48
18:00:17 --> 18:00:20
the y values are the population numbers.

49
18:00:23 --> 18:00:26
Now let's perform some
statistics on this data.

50
18:00:26 --> 18:00:30
We'll begin by counting the number of
records where the state is California.

51
18:00:32 --> 18:00:38
You can do this by saying
STNAME="California" pipe stats count.

52
18:00:43 --> 18:00:48
Now switch back to the statistics tab,
see the result here

53
18:00:51 --> 18:00:54
Now let's see the total population for
California.

54
18:00:54 --> 18:01:00
You can replace count
with sum(CENUS2010POP)

55
18:01:04 --> 18:01:06
You can also calculate
the average population.

56
18:01:06 --> 18:01:08
We'll replace sum with mean.

1
11:57:08 --> 11:57:13
Hello, My name is Mitch Fleichmann,
senior instructor here at Splunk.

2
11:57:13 --> 11:57:15
Today we are going to
install Splunk on Linux.

3
11:57:17 --> 11:57:20
I'm using one of
the Splunk Education Linux servers.

4
11:57:20 --> 11:57:21
Let's examine the environment.

5
11:57:24 --> 11:57:28
First of all,
notice I'm logged in as the user splunker.

6
11:57:28 --> 11:57:31
As a best practice,
do not install Splunk as the root user.

7
11:57:34 --> 11:57:40
I'm currently in the /opt directory,
where I've already downloaded

8
11:57:40 --> 11:57:45
the tarball from splunk.com/download for
this platform.

9
11:57:47 --> 11:57:52
As a final check, let's check the system
to make sure we have the correct operating

10
11:57:52 --> 11:57:53
system and kernel.

11
11:57:56 --> 11:58:03
This is indeed a Linux machine, let's
get confirmation that it is also 64 bit.

12
11:58:09 --> 11:58:11
So we are good to go.

13
11:58:11 --> 11:58:14
The next step is to unzip and
untar the installer and

14
11:58:14 --> 11:58:17
that we can do with the gunzip and
tar commands.

15
11:58:25 --> 11:58:30
When everything untucks, you'll notice
a new sub directory created named splunk.

16
11:58:31 --> 11:58:35
And we can navigate to the splunk/bin
directory to start up Splunk.

17
11:58:38 --> 11:58:43
Couple of ways to start
Splunk with no switches.

18
11:58:44 --> 11:58:48
On the first startup, you'll be prompted
to read and agree to the software license.

19
11:58:50 --> 11:58:54
Or as a shortcut, you can start
up Splunk and accept the license.

20
11:59:02 --> 11:59:07
And notice a couple of port numbers
being grabbed, port 8000 for

21
11:59:07 --> 11:59:10
splunk web and port 8089 for
splunk management.

22
11:59:15 --> 11:59:16
The splunk daemon has started.

23
11:59:19 --> 11:59:21
Splunk is generating its own keys.

24
11:59:24 --> 11:59:28
And we can see the splunk web interface,
the URL.

25
11:59:33 --> 11:59:33
As a best practice,

26
11:59:33 --> 11:59:38
you may also want to consider automating
splunk to start when the machine boots.

27
11:59:39 --> 11:59:44
That you can do as the root user.

28
11:59:44 --> 11:59:48
By issuing the splunk enable command,

29
11:59:48 --> 11:59:53
enable boot-start as the -user splunker,

30
11:59:53 --> 11:59:58
the same user that we
just consult splunk with.

31
12:00:07 --> 12:00:10
So lets log in to splunk web and
see how the system looks.

32
12:00:10 --> 12:00:12
And for that we'll go back to the browser.

33
12:00:14 --> 12:00:16
Go to the appropriate URL.

34
12:00:20 --> 12:00:22
And we can see upon first login,

35
12:00:22 --> 12:00:27
you are prompted to login with
the credentials admin, password changeme.

36
12:00:35 --> 12:00:36
Since this is the first login,

37
12:00:36 --> 12:00:40
you also coach to change your
password to something more secure.

38
12:00:40 --> 12:00:43
And it's highly recommended to
follow this best practice as well.

39
12:00:53 --> 12:00:55
And also on first connection,

40
12:00:55 --> 12:00:58
you'll also see a splash screen
showing you what's new in version 6.

41
12:01:00 --> 12:01:01
In this case, Powerful Analytics.

42
12:01:03 --> 12:01:05
Some changes to the UI to
make it more intuitive.

43
12:01:08 --> 12:01:11
Simplified component management for
cluster management,

44
12:01:11 --> 12:01:13
folder management and so on.

45
12:01:15 --> 12:01:17
Also a richer developer experience.

46
12:01:21 --> 12:01:24
Close down this window and
you can explore the navigation options.

47
12:01:25 --> 12:01:27
And notice in the left side,

48
12:01:27 --> 12:01:30
you see a panel showing you
the apps to navigate to and manage.

49
12:01:31 --> 12:01:36
And on the right side, you see some
panel showing you data in the system and

50
12:01:36 --> 12:01:37
various links for help.

51
12:01:39 --> 12:01:41
Let's go to the search and
reporting app by clicking here.

52
12:01:46 --> 12:01:50
And you can see the search far up top,

53
12:01:50 --> 12:01:56
some tips on how to search and
then data to search.

54
12:01:56 --> 12:02:00
Since this is a fresh install,
there is no data to search, so

55
12:02:00 --> 12:02:04
the next step is to index data and
begin searching.

56
12:02:04 --> 12:02:04
Good luck.

1
23:59:11 --> 23:59:15
[SOUND] Hello, this is Chris Busheers,

2
23:59:15 --> 23:59:19
part of the Splunk education team.

3
23:59:19 --> 23:59:24
In this video, I'll show you how
install Splunk onto a Window's server.

4
23:59:24 --> 23:59:27
First we need to get the software
from the splunk.com download page.

5
23:59:29 --> 23:59:33
We will need to select that
the platform is 32 or 64-bit.

6
23:59:33 --> 23:59:36
If you aren't sure if your system is 32 or

7
23:59:36 --> 23:59:39
64-bit, you can check
your system properties.

8
23:59:40 --> 23:59:45
As you can see, this server is 64-bit,
so we can install that version.

9
23:59:45 --> 23:59:50
If we saw a system type of 32-bit,
we would download the 32-bit version.

10
23:59:51 --> 23:59:54
We run the installer by
double clicking on it.

11
23:59:55 --> 23:59:59
There is button to view the license
agreement, and a check box to accept it.

12
00:00:01 --> 00:00:04
At this point we can either install
Splunk with the defaults, or

13
00:00:04 --> 00:00:06
customize our installation.

14
00:00:07 --> 00:00:11
Let's click on the customize options
to see what settings can be selected.

15
00:00:12 --> 00:00:16
The first option is to change
the installation location of Splunk.

16
00:00:16 --> 00:00:20
We are fine with this location,
so we click Next.

17
00:00:20 --> 00:00:26
Now we must choose what account type
to install Splunk as, Local System or

18
00:00:26 --> 00:00:26
Domain Account.

19
00:00:28 --> 00:00:32
A Local System account will allow
Splunk to access all data on, or

20
00:00:32 --> 00:00:34
forwarded, to this machine.

21
00:00:34 --> 00:00:37
A Domain Account will allow
you to collect logs and

22
00:00:37 --> 00:00:42
metrics from remote machines as
well as local and forwarded data.

23
00:00:42 --> 00:00:46
You are required to provide a Domain
Account with the proper domain rights

24
00:00:46 --> 00:00:47
to use this type.

25
00:00:47 --> 00:00:51
Local System works well for
us, so we click Next.

26
00:00:51 --> 00:00:56
We can select to have a shortcut to Splunk
added, and click Install to continue.

27
00:00:56 --> 00:00:59
[MUSIC]

28
00:00:59 --> 00:01:03
Once installed, we can select to have
Splunk launch, and click Finish.

29
00:01:03 --> 00:01:06
This Splunk web interface
opens in our default browser.

30
00:01:07 --> 00:01:12
We enter the default user name of admin,
and a password of change made.

31
00:01:12 --> 00:01:15
A dialog box appears asking
us to change our password.

32
00:01:15 --> 00:01:18
It is always best practice to do this.

33
00:01:18 --> 00:01:21
Once logged in, we are taken to
the Splunk launcher homepage.

34
00:01:21 --> 00:01:24
And that's all it takes to get
Splunk installed on Windows.

35
00:01:24 --> 00:01:26
Now dig in, and start exploring.

36
00:01:26 --> 00:01:32
[SOUND]

1
00:00:46 --> 00:00:47
Our third and

2
00:00:47 --> 00:00:52
final case is applicable to most companies
that create customer-focused products.

3
00:00:54 --> 00:00:58
They want to understand how their
customers are responding to the products,

4
00:00:58 --> 00:01:00
how the product marketing
efforts are performing,

5
00:01:00 --> 00:01:04
what kind of problems customers
are encountering, and what new features or

6
00:01:04 --> 00:01:07
feature improvements the customers
are seeking, and so forth.

7
00:01:08 --> 00:01:11
But how does the company
get this information?

8
00:01:11 --> 00:01:14
What kind of data sources
would carry this information?

9
00:01:14 --> 00:01:16
The figure show some of these sources.

10
00:01:16 --> 00:01:23
They are in focused user surveys,
emails sent by the customers, in blogs and

11
00:01:23 --> 00:01:29
product review forums, specialized
groups on social media and user forums.

12
00:01:29 --> 00:01:35
In short, they are on the Internet or
in material received through the Internet.

13
00:01:35 --> 00:01:37
Now, how many sources are there?

14
00:01:38 --> 00:01:40
Two.

15
00:01:40 --> 00:01:41
The number would vary.

16
00:01:41 --> 00:01:44
A new sites, a new postings, and

17
00:01:44 --> 00:01:45
new discussion threads
would come up all the time.

18
00:01:47 --> 00:01:50
In all of these,
the goal is to identify information that

19
00:01:50 --> 00:01:55
truly relates to the companies product,
its features and its utility.

20
00:01:57 --> 00:02:00
To cast this as a type
of big data problem,

21
00:02:00 --> 00:02:03
we look at a task that computer
scientists called Data Fusion.

22
00:02:05 --> 00:02:10
Consider a set of data sources, S,
as we mentioned on the last slide and

23
00:02:10 --> 00:02:12
a set of data items, D.

24
00:02:13 --> 00:02:18
A data item represents a particular
aspect of a real world entity

25
00:02:18 --> 00:02:20
which in our case is
a product of the company.

26
00:02:22 --> 00:02:27
For each data item, a source can, but
not necessarily will, provide a value.

27
00:02:27 --> 00:02:29
For example,

28
00:02:29 --> 00:02:34
the usability of an ergonomically
split keyboard can have a value good.

29
00:02:35 --> 00:02:40
The value can be atomic,
like good, or a set, or a list or

30
00:02:40 --> 00:02:41
sometimes embedded in the string.

31
00:02:43 --> 00:02:47
For example, the cursor sometimes
freezes when using the touchpad,

32
00:02:48 --> 00:02:52
is a string which has
a value about the touchpad.

33
00:02:54 --> 00:02:59
The goal of Data Fusion is to find
the values of Data Items from a source.

34
00:03:01 --> 00:03:06
In many cases, the system would find
a unique true value of an item.

35
00:03:06 --> 00:03:10
For example, the launch data of a product
in Europe should be the same true value

36
00:03:10 --> 00:03:12
regardless of the data
source one looks at.

37
00:03:13 --> 00:03:17
In other cases, we could find
a value distribution of an item.

38
00:03:17 --> 00:03:20
For example, the usability of our
keyboard may have a value distribution.

39
00:03:22 --> 00:03:26
That's with Data Fusion, we should be
able to collect the values of real world

40
00:03:26 --> 00:03:29
items from a subset of data sources.

41
00:03:29 --> 00:03:32
It is a subset because
not all Data Sources

42
00:03:32 --> 00:03:34
will have relevant information
about the Data Item.

43
00:03:36 --> 00:03:39
There are some other versions
of what a Data Fusion is but for

44
00:03:39 --> 00:03:41
our purposes we'll stick with
this general description.

45
00:03:44 --> 00:03:48
Now one obvious problem with the Internet
is that there are too many data

46
00:03:48 --> 00:03:52
sources at any time,
these lead to many difficulties.

47
00:03:53 --> 00:03:57
First, it is to be understood
that with too many data sources

48
00:03:57 --> 00:04:00
there will be many values for
the same item.

49
00:04:01 --> 00:04:04
Often these will differ and
sometimes they will conflict.

50
00:04:05 --> 00:04:08
A standard technique in this case
is to use a voting mechanism.

51
00:04:10 --> 00:04:14
However, even a voting
mechanism can be complex

52
00:04:14 --> 00:04:16
due to problems with the data source.

53
00:04:18 --> 00:04:21
One of the problems is to estimate
the trustworthiness of the source.

54
00:04:23 --> 00:04:25
For each data source,

55
00:04:25 --> 00:04:31
we need to evaluate whether it's reporting
some basic or known facts correctly.

56
00:04:31 --> 00:04:34
If a source mentions details
about a rainbow colored iPhone,

57
00:04:34 --> 00:04:38
which does not exist,
it's trustworthiness reduces

58
00:04:38 --> 00:04:40
because of the falsity of
the provided value of this data item.

59
00:04:42 --> 00:04:46
Accordingly, a higher vote count can be
assigned to a more trustworthy source.

60
00:04:47 --> 00:04:50
And then, this can be used in voting.

61
00:04:52 --> 00:04:54
The second aspect is Copy Detection.

62
00:04:55 --> 00:04:59
Detecting weather once was has copied
information from another can be very

63
00:04:59 --> 00:05:02
important for
detail fusion task in customer analytics.

64
00:05:03 --> 00:05:05
If a source has copied information,

65
00:05:06 --> 00:05:11
it's such that discounted vote count
can be assigned to a copy value and

66
00:05:11 --> 00:05:17
voting that means the copy in
source will have less weight.

67
00:05:18 --> 00:05:23
Now this is especially relevant when we
compute value distributions, because if we

68
00:05:23 --> 00:05:28
treat copies as genuine information, we
will statistically bias the distribution.

69
00:05:28 --> 00:05:33
Now here is active research on how to
detect copies, how to determine bias and

70
00:05:33 --> 00:05:37
then arrive at a statistically sound
estimation of value distribution.

71
00:05:37 --> 00:05:42
But to our knowledge, these methods are
yet to be applied to existing software for

72
00:05:42 --> 00:05:44
big data integration.

73
00:05:47 --> 00:05:49
It should be very clear by now but

74
00:05:49 --> 00:05:53
there are two kinds of big data
situations when it comes to information.

75
00:05:54 --> 00:05:59
The first two uses cases that we
saw requires an integration system

76
00:05:59 --> 00:06:03
to consider all sources because
the application demand so.

77
00:06:04 --> 00:06:10
In contrast, problems where data comes
from too many redundant, potentially

78
00:06:10 --> 00:06:15
unreliable sources like the Internet, the
best results can be obtained if we have

79
00:06:15 --> 00:06:19
a way of evaluating the worthiness of
sources before information integration.

80
00:06:20 --> 00:06:23
But this problem is
called Source Selection.

81
00:06:23 --> 00:06:27
The picture on the right shows the result
of a cost benefit analysis for

82
00:06:27 --> 00:06:29
data fusion.

83
00:06:29 --> 00:06:32
The x-axis indicates the number
of sources used, and

84
00:06:32 --> 00:06:36
the y-axis measures the proportion
of true results that were returned.

85
00:06:38 --> 00:06:42
We can clearly see that the plot peaks
around six-to-eight sources, and

86
00:06:42 --> 00:06:44
that the efficiency falls
as more sources are added.

87
00:06:46 --> 00:06:51
In a cost benefit analysis,
the cost must include both the human and

88
00:06:51 --> 00:06:53
the computational costs,

89
00:06:53 --> 00:06:57
while the benefit is a function of
the accuracy of the fusion result.

90
00:06:57 --> 00:07:02
The technique for
solving this problem comes from economics.

91
00:07:03 --> 00:07:06
Assuming that cost and
benefits are measure in the same unit, for

92
00:07:06 --> 00:07:07
example, dollars.

93
00:07:08 --> 00:07:11
They proposed to continue
selecting sources

94
00:07:11 --> 00:07:15
until the marginal benefit is
less than the marginal cost.

95
00:07:17 --> 00:07:21
Now recent techniques were performing
this computation at quite scalable.

96
00:07:21 --> 00:07:24
In one setting,
selecting the most beneficial sources

97
00:07:24 --> 00:07:28
from a total of one million
sources took less than one hour.

98
00:07:30 --> 00:07:34
This completes our coverage of
the big data integration problems.

1
00:08:17 --> 00:08:19
Hello and welcome.

2
00:08:19 --> 00:08:24
My name is Rene and I'll be sharing with
you, how to create a report using Pivot.

3
00:08:24 --> 00:08:28
To access the Pivot interface,
click Pivot on the navigation menu.

4
00:08:30 --> 00:08:34
The first step is to select
a prebuilt data model.

5
00:08:34 --> 00:08:38
Now the data model allows you
to create compelling reports and

6
00:08:38 --> 00:08:43
dashboards without having to know
how to write complex search queries.

7
00:08:43 --> 00:08:48
As a side note data models are typically
created by a knowledge manager that

8
00:08:48 --> 00:08:51
understands their
organization's index data.

9
00:08:51 --> 00:08:55
Grasps the search language and
are familiar with lookups,

10
00:08:55 --> 00:08:59
transactions, field extractions and
calculated fields.

11
00:09:00 --> 00:09:05
For our example, we're going to
use Buttercup Games Online Sales.

12
00:09:07 --> 00:09:11
Now, this data model includes
the online sales activities.

13
00:09:11 --> 00:09:15
It's made up of nine objects.

14
00:09:15 --> 00:09:20
Each object represents a specific set
of events in a hierarchical structure.

15
00:09:22 --> 00:09:29
The http request would include the largest
number of events in this data model.

16
00:09:29 --> 00:09:32
The next object, successful request,

17
00:09:32 --> 00:09:37
would be a subset of the http
request events and so on.

18
00:09:37 --> 00:09:42
Let's say that we want to create a report
for purchases over the last seven days.

19
00:09:42 --> 00:09:45
We would select the most
appropriate object.

20
00:09:45 --> 00:09:49
In this case we're going to
select successful purchase.

21
00:09:50 --> 00:09:54
After you have selected the object
you will notice that a new

22
00:09:54 --> 00:09:56
Pivot view will display.

23
00:09:57 --> 00:10:02
If you notice, the count of
successful purchase is the total

24
00:10:02 --> 00:10:07
number of events for this specific object.

25
00:10:07 --> 00:10:09
Now, it is based off of all time.

26
00:10:11 --> 00:10:16
Before you begin, you should have an idea
of what type of report you want to create.

27
00:10:16 --> 00:10:19
So let's say that we
want to create a bar chart.

28
00:10:19 --> 00:10:25
We're going to select that
from the left navigation pane.

29
00:10:25 --> 00:10:29
So the third icon will allow me
to click and select Bar Chart.

30
00:10:31 --> 00:10:35
So at this point we are ready
to start building our report.

31
00:10:36 --> 00:10:38
The first selection is Time Range.

32
00:10:38 --> 00:10:41
Let's set that to the last seven days.

33
00:10:43 --> 00:10:47
The next one is Filter, now we don't need
to define Filter, because we've already

34
00:10:47 --> 00:10:53
selected an object of successful purchase
and that's what we're reporting against.

35
00:10:54 --> 00:10:59
Now, for my x axis,
I'm going to select product name.

36
00:11:00 --> 00:11:03
You'll notice that I can set the label.

37
00:11:03 --> 00:11:05
I can set the sort order.

38
00:11:05 --> 00:11:08
I can even set how many
maximum bars that I want.

39
00:11:09 --> 00:11:13
The next thing to define is the y axis.

40
00:11:13 --> 00:11:18
At this time it's set to
count of successful purchase.

41
00:11:18 --> 00:11:22
But I want the total sales by product.

42
00:11:22 --> 00:11:24
So I'm going to select price and

43
00:11:24 --> 00:11:29
then you'll notice that my
value is already set to sum.

44
00:11:29 --> 00:11:31
So I'm going to keep it as sum.

45
00:11:33 --> 00:11:35
My report is starting to look really good.

46
00:11:35 --> 00:11:42
But after reviewing it I might decide that
I want to see values segmented by host.

47
00:11:42 --> 00:11:47
So let's do that,
in order to set the segmentation

48
00:11:47 --> 00:11:55
of a specific product name I could go and
set the color to the field that I want.

49
00:11:55 --> 00:11:56
So I'm going to select Host.

50
00:11:59 --> 00:12:06
Now, you'll see that my total
sales are segmented by host.

51
00:12:07 --> 00:12:09
So we really like this report.

52
00:12:09 --> 00:12:11
Let's go ahead and save it.

53
00:12:12 --> 00:12:14
So we're going to click on Save As.

54
00:12:16 --> 00:12:18
Select report and give it a title.

55
00:12:18 --> 00:12:23
Let's call it Product Sales by Host.

56
00:12:23 --> 00:12:30
We're going to click Save and
then we're going to view our report.

57
00:12:33 --> 00:12:36
It was that easy to create
a report through Pivot.

58
00:12:38 --> 00:12:43
Now, let's just say that we want to go and
work with another Pivot.

59
00:12:43 --> 00:12:45
This time we're going to just go and

60
00:12:45 --> 00:12:49
create a statistic table
to view some of that data.

61
00:12:49 --> 00:12:51
Let's go ahead and click Pivot again.

62
00:12:53 --> 00:12:56
We'll select the same data model.

63
00:12:56 --> 00:13:00
Buttercup games online sales and
let's keep to the object that

64
00:13:00 --> 00:13:04
we're familiar with right now,
successful purchases.

65
00:13:11 --> 00:13:16
As I mentioned, I want to just
simply create a statistic table.

66
00:13:17 --> 00:13:20
I want to show you how
we'd go about doing that.

67
00:13:21 --> 00:13:28
So the first step is to think about what
are some of the row data that you want?

68
00:13:28 --> 00:13:35
Let's just assume that we want to view
the product names within their categories.

69
00:13:35 --> 00:13:41
So, let's go ahead, under Split Rows,
we're going to select category first.

70
00:13:42 --> 00:13:45
Notice that I can set a label if I want.

71
00:13:45 --> 00:13:49
Let's go ahead and
set it capitalized category.

72
00:13:49 --> 00:13:50
I'm going to add that to my table.

73
00:13:50 --> 00:13:55
You'll see that it
becomes the first column.

74
00:13:55 --> 00:13:59
Now I want to add another column.

75
00:13:59 --> 00:14:03
Let's go ahead and
click the plus right next to categories.

76
00:14:03 --> 00:14:06
Again we're still working
with the split rows.

77
00:14:08 --> 00:14:13
I'm going to select product name, again if
you want to set a label you can do that.

78
00:14:13 --> 00:14:15
Now this is the label of our column.

79
00:14:17 --> 00:14:17
Add to table.

80
00:14:21 --> 00:14:26
And I want you to notice that
now we are viewing each product

81
00:14:26 --> 00:14:28
within their category.

82
00:14:29 --> 00:14:33
Now at this point if you look at
the statistic that is being defined,

83
00:14:33 --> 00:14:36
it's the count of successful purchases.

84
00:14:36 --> 00:14:39
That is defined as column values.

85
00:14:40 --> 00:14:45
Now if I want to add to this,
I can append and have another column.

86
00:14:45 --> 00:14:51
Let's go ahead and click the plus next
to the Count of successful purchase.

87
00:14:52 --> 00:14:54
This time I want to add the price.

88
00:14:55 --> 00:14:58
And you'll notice that I have
different values available.

89
00:14:58 --> 00:15:01
I could do a Sum, a Count,
an Average, Max, Min.

90
00:15:01 --> 00:15:08
Let's keep it as a Sum and
let's add a label of Total Sales.

91
00:15:08 --> 00:15:10
And let's Add To Table.

92
00:15:12 --> 00:15:16
So this is just to show you
how quick it is to go and

93
00:15:16 --> 00:15:20
build a table with the statistics
that you would need.

94
00:15:20 --> 00:15:24
Now at this point, if ever you decide,
I only want to view the data for

95
00:15:24 --> 00:15:26
the last seven days.

96
00:15:26 --> 00:15:30
Just note that you could go
back up to your filter and

97
00:15:30 --> 00:15:33
your first filter is
based off of all time.

98
00:15:33 --> 00:15:37
So we could set that to
the last seven days.

99
00:15:37 --> 00:15:40
So of course that the data
in our table will change.

100
00:15:41 --> 00:15:46
Now, the last thing I want to show you
here is if ever you did want to see

101
00:15:47 --> 00:15:54
the count and the total sales per host,
as we've defined in our previous example.

102
00:15:54 --> 00:15:58
That is where you could go
to the split columns and

103
00:15:58 --> 00:16:01
define how you wish to
split that information.

104
00:16:01 --> 00:16:04
So let's go ahead and select host again.

105
00:16:04 --> 00:16:07
And I'm just going to
click on add to table.

106
00:16:12 --> 00:16:19
Now I could see that the information
has been split by host.

107
00:16:19 --> 00:16:25
I have www1 count of successful
purchases as well as total sales.

108
00:16:25 --> 00:16:32
And then I have the split of www2 I have
the count and the total sales and etc.

109
00:16:32 --> 00:16:37
So this concludes our short
video hope you enjoy it.

110
00:16:37 --> 00:16:38
Thank you.

1
00:24:56 --> 00:24:58
Welcome.

2
00:24:58 --> 00:25:03
In this short module we'll talk about
information integration which refers

3
00:25:03 --> 00:25:08
to the problem of using many different
information sources to accomplish a task.

4
00:25:09 --> 00:25:14
In this module, we'll look at the problems
and solutions through a few use cases.

5
00:25:17 --> 00:25:22
So after this video, you'll be able to
explain the data integration problem,

6
00:25:24 --> 00:25:29
define integrated views and schema
mapping, describe the impact of increasing

7
00:25:29 --> 00:25:35
the number of data sources, appreciate
the need to use data compression,

8
00:25:35 --> 00:25:41
And describe record linking,
data exchange, and data fusion tasks.

9
00:25:45 --> 00:25:49
Our first use case starts with
an example given at an IBM website for

10
00:25:49 --> 00:25:50
their information integration products.

11
00:25:52 --> 00:25:55
It represents a very common
scenario in today's business world.

12
00:25:57 --> 00:26:01
Due to the changing market dynamics,
companies

13
00:26:01 --> 00:26:04
are always selling off a part of their
company or acquiring another company.

14
00:26:06 --> 00:26:10
As these mergers and acquisitions happen,
databases which were developed and

15
00:26:10 --> 00:26:14
stored separately in different companies
would now need to be brought together.

16
00:26:16 --> 00:26:18
Now take a minute to read this case.

17
00:26:23 --> 00:26:27
This is the case of an expanding financial
services group that's growing its customer

18
00:26:27 --> 00:26:29
base in different countries.

19
00:26:31 --> 00:26:36
And all they want is a single view
of their entire customer base.

20
00:26:36 --> 00:26:41
In other words, it does not matter
which previous company originally had

21
00:26:41 --> 00:26:46
the customers, Suncorp-Metway
want to consolidate all customer

22
00:26:46 --> 00:26:53
information as if they were
in one single database.

23
00:26:53 --> 00:26:57
And in reality, of course, they may
not want to buy a huge machine and

24
00:26:57 --> 00:27:00
migrate every subsidiary
company's data into it.

25
00:27:01 --> 00:27:07
What they're looking to create is possibly
a software solution which would make all

26
00:27:07 --> 00:27:13
customer-related data to appear as though
they were together as a single database.

27
00:27:13 --> 00:27:17
This software solution is called
an information integration system.

28
00:27:18 --> 00:27:22
This will help them ensure that they have
a uniform set of marketing campaigns for

29
00:27:22 --> 00:27:24
all their customers.

30
00:27:26 --> 00:27:31
Let's try to see, hypothetically, of
course, what might be involved in creating

31
00:27:31 --> 00:27:36
this combined data and what kind of use
the integrated data might result in.

32
00:27:36 --> 00:27:39
So we first create
a hypothetical scenario.

33
00:27:40 --> 00:27:43
Although Suncorp have a large
number of data sources,

34
00:27:43 --> 00:27:46
we will take a much simpler situation and

35
00:27:46 --> 00:27:50
have only two data sources from two
different financial service companies.

36
00:27:52 --> 00:27:53
The first data source,

37
00:27:53 --> 00:27:58
which is an insurance company that manages
it's data with a relation of DBMS,

38
00:27:58 --> 00:28:03
this database has nine tables where the
primary object of information is a policy.

39
00:28:05 --> 00:28:08
The company offers many
different types of policies

40
00:28:08 --> 00:28:11
sold to individual people by their agents.

41
00:28:11 --> 00:28:15
Now as it's true for all insurance
companies, policyholders pay their monthly

42
00:28:15 --> 00:28:21
dues, and sometimes people make claims
against their insurance policies.

43
00:28:21 --> 00:28:25
When they do, the details of the claims
are maintained in the database.

44
00:28:26 --> 00:28:30
These claims can belong to different
categories, and when the claims have

45
00:28:30 --> 00:28:34
paid to the claimants, the transaction
is recorded in the transactions table.

46
00:28:36 --> 00:28:38
As we have done several times now,

47
00:28:38 --> 00:28:41
the primary keys of the table
are the underlined attributes here.

48
00:28:43 --> 00:28:46
The second company in
our example is a bank,

49
00:28:47 --> 00:28:48
which also uses a relational database.

50
00:28:50 --> 00:28:54
In this bank, both individuals and
businesses called corporations here,

51
00:28:54 --> 00:28:55
can have accounts.

52
00:28:57 --> 00:29:00
Now accounts can be of different types.

53
00:29:00 --> 00:29:02
For example, a money market account
is different from a savings account.

54
00:29:04 --> 00:29:08
A bank also maintains its transactions
in a table, which can be really large.

55
00:29:10 --> 00:29:15
But the dispute in a bank record
case happens when the bank is

56
00:29:15 --> 00:29:19
charged a customer, or the customer has
declined responsibility of the charge.

57
00:29:19 --> 00:29:24
This can happen, for example, if a
customer's Internet account was hacked or

58
00:29:24 --> 00:29:25
a debit card got stolen.

59
00:29:26 --> 00:29:28
The bank keeps a record
of these anomalies and

60
00:29:28 --> 00:29:33
fraudulent events in a disputes table,
all right.

61
00:29:33 --> 00:29:37
Let's see what happens after the data
from these two subsidiary companies

62
00:29:37 --> 00:29:38
are integrated.

63
00:29:41 --> 00:29:45
After the merger the company wants to
do a promotional goodwill activity.

64
00:29:46 --> 00:29:49
They would like to offer
a small discount to their

65
00:29:49 --> 00:29:54
insurance policyholders if they're also
customers of the newly acquired bank.

66
00:29:55 --> 00:29:57
How do you identify these customers?

67
00:29:57 --> 00:29:58
Let's see.

68
00:29:58 --> 00:30:03
In other words, we need to use the table
shown on the left to create the table

69
00:30:03 --> 00:30:07
shown on the right called
discount candidates.

70
00:30:07 --> 00:30:11
One is to create a yellow tables from
the insurance company database, and

71
00:30:11 --> 00:30:16
the blue table from the bank database,
and then join them to construct the table

72
00:30:16 --> 00:30:20
with a common customer ID, and both
the policyKey and bank account number.

73
00:30:22 --> 00:30:26
Now, this relation, which is derived that
is computed by querying two different

74
00:30:26 --> 00:30:31
data sources and combining their results,
is called an integrated view.

75
00:30:32 --> 00:30:37
It is integrated because the data is
retrieved from different data sources,

76
00:30:38 --> 00:30:42
and it's called a view because
in database terminology

77
00:30:42 --> 00:30:45
it is a relation computed
from other relations.

78
00:30:48 --> 00:30:51
To populate the integrated
view discount candidates,

79
00:30:51 --> 00:30:54
we need to go through a step
called schema mapping.

80
00:30:55 --> 00:30:59
The term mapping means to
establish correspondence between

81
00:30:59 --> 00:31:03
the attributes of the view, which is
also called a target relation, and

82
00:31:03 --> 00:31:05
that of the source relations.

83
00:31:06 --> 00:31:11
For example, we can map the full address
from individuals to the address attribute

84
00:31:11 --> 00:31:16
in discountCandidates, but
this would only be true for

85
00:31:16 --> 00:31:20
customers whose names and
addresses match in the two databases.

86
00:31:22 --> 00:31:27
As you can see, policyholders uses
the full name of a customer, whereas

87
00:31:27 --> 00:31:31
individuals has it broken down into first
name, middle initial, and last name.

88
00:31:32 --> 00:31:37
On the other hand, full address is
a single field in individuals, but

89
00:31:37 --> 00:31:41
represented in full attributes
in the policyholders relation.

90
00:31:43 --> 00:31:46
The mappings of account number and
policyKey are more straightforward.

91
00:31:47 --> 00:31:50
Well, what about customer ID which doesn't
correspond to anything in the four

92
00:31:50 --> 00:31:52
input relations?

93
00:31:52 --> 00:31:53
We'll come back to this later on.

94
00:31:56 --> 00:31:59
Okay, now we'll define
an integrated relation.

95
00:31:59 --> 00:32:01
How do we query?

96
00:32:01 --> 00:32:02
For example,

97
00:32:02 --> 00:32:06
how do you find the bank account number
of a person whose policyKey is known?

98
00:32:07 --> 00:32:10
You might think, what's the problem here?

99
00:32:10 --> 00:32:11
We have a table.

100
00:32:11 --> 00:32:17
Just say select account number from
discount candidates where policyKey

101
00:32:17 --> 00:32:21
is equal to 4-937528734, and we're done.

102
00:32:23 --> 00:32:30
Well, yes, you can write this query,
but how the query be evaluated?

103
00:32:30 --> 00:32:32
That depends on what's called the query

104
00:32:32 --> 00:32:35
architecture of the data
integration system.

105
00:32:35 --> 00:32:38
The figure on the left shows
the elements of this architecture.

106
00:32:39 --> 00:32:42
We'll discover it in more detail,
but on this slide,

107
00:32:42 --> 00:32:45
we'll just describe
the three axes of this cube.

108
00:32:46 --> 00:32:52
The vertical z axis specifies
whether we have one data source or

109
00:32:52 --> 00:32:53
multiple data sources.

110
00:32:54 --> 00:32:57
Our interest is in the case where
there are multiple data sources.

111
00:32:59 --> 00:33:05
The x axis asks whether the integrated
data is actually stored physically

112
00:33:05 --> 00:33:11
in some place or whether it is computed
on the fly, each time a query is asked.

113
00:33:11 --> 00:33:17
If it is all precomputed and stored,
we say that the data is materialized.

114
00:33:17 --> 00:33:19
And if it is computed on the fly,
we say it's virtual.

115
00:33:21 --> 00:33:25
The y axis asks whether
there is a single schema or

116
00:33:25 --> 00:33:29
global schema defined all
over the data integrated for

117
00:33:29 --> 00:33:34
an application or whether the data
stay in different computers and

118
00:33:34 --> 00:33:38
it is accessed in a peer-to-peer
manner at runtime.

119
00:33:38 --> 00:33:43
Thus, the seemingly simple select
project query will be evaluated

120
00:33:43 --> 00:33:48
depending on which part of the cube
our architecture implements.

121
00:33:48 --> 00:33:51
But for now,
let's return to our example use case.

122
00:33:54 --> 00:33:58
An obvious goal of an information
integration system

123
00:33:58 --> 00:34:00
is to be complete and accurate.

124
00:34:01 --> 00:34:05
Complete means no eligible record
from the source should be absent in

125
00:34:05 --> 00:34:06
the target relation.

126
00:34:07 --> 00:34:13
Accurate means all the entries in
the integrated relation should be correct.

127
00:34:13 --> 00:34:18
Now we said on the previous
slide that a matching

128
00:34:18 --> 00:34:22
customer is a person who
was in both databases and

129
00:34:22 --> 00:34:27
has the same name and
address in the two databases.

130
00:34:27 --> 00:34:29
Now let's look at some example records.

131
00:34:30 --> 00:34:34
Specifically, consider the records
marked by the three arrows.

132
00:34:35 --> 00:34:40
The two bank accounts and
the policy record do not match for name or

133
00:34:40 --> 00:34:42
for address.

134
00:34:42 --> 00:34:44
So our previous method would discard them.

135
00:34:45 --> 00:34:47
But look at the records closely.

136
00:34:48 --> 00:34:51
Do you think they might all
belong to the same customer?

137
00:34:52 --> 00:34:54
Maybe this lady has a maiden name and

138
00:34:54 --> 00:34:58
a married name, and
has moved from one address to another.

139
00:34:59 --> 00:35:02
Maybe she changed her Social Security
number somewhere along the way.

140
00:35:03 --> 00:35:06
So this is called
a record linkage problem.

141
00:35:07 --> 00:35:12
That means we would like to ensure that
the set of data records that belong to

142
00:35:12 --> 00:35:17
a single entity are recognized,
perhaps by clustering

143
00:35:17 --> 00:35:21
the values of different attributes or
by using a set of matching rules so

144
00:35:21 --> 00:35:26
that we know how to deal with it
during the integration process.

145
00:35:26 --> 00:35:28
For example, we need to determine

146
00:35:28 --> 00:35:31
which of the addresses should be
used in the integrated relation.

147
00:35:32 --> 00:35:33
Which of the two bank accounts?

148
00:35:35 --> 00:35:39
If the answer is both accounts 102 and
103, we will need to change

149
00:35:39 --> 00:35:45
the schema of the target's relation
to a list instead of an atomic number

150
00:35:45 --> 00:35:48
to avoid creating multiple tuples for
the same entity.

151
00:35:50 --> 00:35:55
As we saw, the schema of adding
process is a task of figuring out

152
00:35:55 --> 00:35:59
how elements of the schema from two
sources would relate to each other and

153
00:35:59 --> 00:36:02
determining how they would
map the target schema.

154
00:36:03 --> 00:36:08
You also saw that this is not really
a simple process, that we are trying to

155
00:36:08 --> 00:36:13
produce one integrated relation using
a couple of relations from each source.

156
00:36:15 --> 00:36:20
In a Big Data situation,
there are dozens of data sources, or

157
00:36:20 --> 00:36:26
more because the company's growing and
each source may have a few hundred tables.

158
00:36:26 --> 00:36:30
So it becomes very hard to actually solve

159
00:36:30 --> 00:36:35
this correspondence-making problem
completely and accurately just because

160
00:36:35 --> 00:36:39
the number of combinations one has to
go through is really, really high.

161
00:36:42 --> 00:36:47
One practical way to tackle this problem
is not to do a full-scale detail

162
00:36:47 --> 00:36:52
integration in the beginning but
adopt what's called a pay-as-you-go model.

163
00:36:52 --> 00:36:55
The pay-as-you-go data
management principle is simple.

164
00:36:56 --> 00:37:01
The system should provide some basic
integration services at the outset and

165
00:37:01 --> 00:37:06
then evolve the schema mappings between
the different sources on an as needed

166
00:37:06 --> 00:37:07
basis.

167
00:37:07 --> 00:37:12
So given a query, the system should
generate a best effort or approximate

168
00:37:12 --> 00:37:16
answers from the data sources for
a perfect schema mappings do not exist.

169
00:37:17 --> 00:37:21
When it discovers a large number
of sophisticated queries or

170
00:37:21 --> 00:37:25
data mining tasks over certain sources,
it will guide the users to make

171
00:37:25 --> 00:37:29
additional efforts to integrate
these sources more precisely.

172
00:37:31 --> 00:37:35
Okay, so how does the first
approximate schema mapping performed?

173
00:37:36 --> 00:37:41
One approach to do this is called
Probabilistic Schema Mapping.

174
00:37:41 --> 00:37:43
We'll describe it in more detail next.

175
00:37:46 --> 00:37:47
In the previous step,

176
00:37:47 --> 00:37:53
we just decided to create the disk count
candidates rrelation in an ad hoc way.

177
00:37:53 --> 00:37:54
And in a Big Data situation,

178
00:37:54 --> 00:37:58
we need to carefully determine
what the integrated schema,

179
00:37:58 --> 00:38:03
also called mediated schemas, should be,
and we should evaluate them properly.

180
00:38:04 --> 00:38:10
Since our toy company is trying
to create a single customer view,

181
00:38:10 --> 00:38:14
it's natural to create an integrated
table called customers.

182
00:38:14 --> 00:38:16
But how can we design this table?

183
00:38:16 --> 00:38:17
Here are some options.

184
00:38:18 --> 00:38:24
We can create the customer table to
include individuals and corporations and

185
00:38:24 --> 00:38:29
then use a flag called customer
type to distinguish between them.

186
00:38:29 --> 00:38:34
Now in the mediated schema then,
the individuals first name,

187
00:38:34 --> 00:38:38
middle initial, last name,
policyholder's name and

188
00:38:38 --> 00:38:43
corporation's name would all
map to Customer_Name similarly.

189
00:38:44 --> 00:38:48
The individual's full address,
the corporation's registered address,

190
00:38:48 --> 00:38:52
and the policyholder's address,
plus city, plus state,

191
00:38:52 --> 00:38:56
plus zip would all map
to customer address.

192
00:38:56 --> 00:39:01
Now we can enumerate all such choices of
which attributes to group together and

193
00:39:01 --> 00:39:04
map for each single attribute
in the target schema.

194
00:39:05 --> 00:39:10
But no matter how you'll do it,
it will never be a perfect fit because

195
00:39:10 --> 00:39:13
not all these combinations
would go well together.

196
00:39:13 --> 00:39:17
For example, should that date of
birth be included in this table?

197
00:39:18 --> 00:39:19
Would it make sense for corporations?

198
00:39:21 --> 00:39:27
In Probabilistic Mediated Schema Design,
we answer this question by

199
00:39:27 --> 00:39:30
associating probability values
with each of these options.

200
00:39:33 --> 00:39:37
To compute these values, we need to
quantify the relationships between

201
00:39:37 --> 00:39:41
attributes by figuring out which
attributes should be grouped or

202
00:39:41 --> 00:39:43
clustered together?

203
00:39:43 --> 00:39:47
Now two pieces of information
available in the source schemas

204
00:39:47 --> 00:39:50
can serve as evidence for
attribute clustering.

205
00:39:50 --> 00:39:54
One, the parallel similarity
of source attributes, and two,

206
00:39:54 --> 00:39:59
statistical properties
of service attributes.

207
00:40:00 --> 00:40:05
The first piece of information indicates
when two attributes are likely to be

208
00:40:05 --> 00:40:09
similar and is used for
creating multiple mediated schemas.

209
00:40:10 --> 00:40:14
One can apply a collection of
attribute matching modules

210
00:40:14 --> 00:40:16
to compute pairwise similarity.

211
00:40:16 --> 00:40:18
For example, individual names and

212
00:40:18 --> 00:40:21
policyholder names
are possibly quite similar.

213
00:40:22 --> 00:40:27
Are individual names versus
corporation names similar?

214
00:40:27 --> 00:40:32
Now, the similarity between two
source attributes, EIN and EZ,

215
00:40:32 --> 00:40:37
measure how closely the two attributes
represent the same real world concept.

216
00:40:39 --> 00:40:42
The second piece of information indicates

217
00:40:42 --> 00:40:46
when two attributes are likely
to be different, and is used for

218
00:40:46 --> 00:40:49
assigning probabilities to
each of the mediated schemas.

219
00:40:51 --> 00:40:54
For example, date of birth and

220
00:40:54 --> 00:40:57
corporation name possibly
will never co-occur together.

221
00:40:59 --> 00:41:02
But for
large schemas with large data volumes,

222
00:41:02 --> 00:41:06
one can estimate these measures by
taking samples from the actual database

223
00:41:06 --> 00:41:11
to come up with reasonable
similarity co-occurrence scores.

224
00:41:11 --> 00:41:17
To illustrate attribute regrouping, we
take a significant re-simplified example.

225
00:41:17 --> 00:41:22
Here we want to create customer
transactions as a mediated relation

226
00:41:22 --> 00:41:27
based upon the bank transactions and
insurance transactions.

227
00:41:28 --> 00:41:31
Each attribute is given
an abbreviation for simplicity.

228
00:41:33 --> 00:41:36
Below, you can see three
possible mediated schemas.

229
00:41:37 --> 00:41:42
In the first one,
the transaction begin and end times

230
00:41:42 --> 00:41:47
from bank transactions are grouped into
the same cluster as transaction date time

231
00:41:47 --> 00:41:51
from the insurance transactions,
because all of them are the same type.

232
00:41:52 --> 00:41:57
Similarly, transaction party, that
means who is giving or receiving money,

233
00:41:58 --> 00:42:01
and transaction description are grouped
together with transaction details.

234
00:42:03 --> 00:42:06
The second schema keeps
all of them separate.

235
00:42:07 --> 00:42:11
And the third candidate schema
groups some of them and not others.

236
00:42:14 --> 00:42:18
Now that we have multiple mediated
schemas, which one should we choose?

237
00:42:20 --> 00:42:22
Now I'm presenting here
a qualitative account of the method.

238
00:42:24 --> 00:42:27
The primary goal is to look for
what we can call consistency.

239
00:42:29 --> 00:42:33
A source schema is consistent
with a mediated schema if

240
00:42:33 --> 00:42:38
two different attributes of the source
schema do not occur in one cluster.

241
00:42:39 --> 00:42:43
And in the example, Med3,
that means related schema three,

242
00:42:43 --> 00:42:49
is more consistent with bank
transactions because, unlike Med1,

243
00:42:49 --> 00:42:53
it keeps TBT,
TET in two different clusters.

244
00:42:54 --> 00:43:01
Once this is done, we can count the number
of consistent sources for each candidate

245
00:43:01 --> 00:43:05
mediated schema and then use this count
to come up with a probability estimate.

246
00:43:06 --> 00:43:11
This estimate can then be used
to choose the k best schemas.

247
00:43:12 --> 00:43:17
Should one ever choose more
than one just best schema?

248
00:43:17 --> 00:43:18
Well, that's a hard question
to answer in general.

249
00:43:20 --> 00:43:24
It is done when the top capability
estimates are very close to each other.

1
01:08:19 --> 01:08:23
[SOUND] The Splunk platform for
operational intelligence is

2
01:08:23 --> 01:08:29
a revolutionary suite of products that
is unlocking unprecedented value for

3
01:08:29 --> 01:08:32
thousands of customers around the world.

4
01:08:32 --> 01:08:34
Why Splunk?

5
01:08:34 --> 01:08:37
It all starts with machine data.

6
01:08:37 --> 01:08:39
Machine data is the big data generated by

7
01:08:39 --> 01:08:42
all the technologies that
power our businesses.

8
01:08:42 --> 01:08:47
From the applications, servers, websites,
and network devices in the data center and

9
01:08:47 --> 01:08:50
the cloud, to the mobile device
in the palm of your hand.

10
01:08:51 --> 01:08:56
Thermostats, train sensors, electric cars,
and the Internet of things.

11
01:08:56 --> 01:08:58
Machine data is everywhere.

12
01:08:58 --> 01:09:00
It's fast-growing and complex.

13
01:09:00 --> 01:09:03
It's also incredibly valuable, why?

14
01:09:03 --> 01:09:07
Because it contains a definitive
record of all activity and behavior.

15
01:09:08 --> 01:09:13
Splunk software collects and
indexes this data at massive scale,

16
01:09:13 --> 01:09:17
from wherever it's generated,
regardless of format or source.

17
01:09:17 --> 01:09:21
Users can quickly and
easily monitor, search, analyze, and

18
01:09:21 --> 01:09:24
report on their data, all in real time.

19
01:09:24 --> 01:09:26
Machine data is different.

20
01:09:26 --> 01:09:30
It can't be processed and
analyzed using traditional methods.

21
01:09:30 --> 01:09:35
Splunk software does not rely on brittle
schemas and inflexible databases.

22
01:09:35 --> 01:09:38
Splunk is easy to deploy, easy to use, and

23
01:09:38 --> 01:09:44
easy to scale, whether on premises or
in a public, private, or hybrid cloud.

24
01:09:44 --> 01:09:46
Splunk is also available
as a cloud service.

25
01:09:46 --> 01:09:50
And for big data environments that
used Hadoop for cheap app storage,

26
01:09:50 --> 01:09:53
we have Hunk, Splunk analytics for Hadoop.

27
01:09:53 --> 01:09:55
[MUSIC]

28
01:09:55 --> 01:09:58
Our customers from around
the world illustrate why so

29
01:09:58 --> 01:10:01
many organizations use Splunk.

30
01:10:01 --> 01:10:05
Intuit has standardized on Splunk,
delivering operational visibility for

31
01:10:05 --> 01:10:11
their leading online products, including
QuickBooks, Quicken, and TurboTax.

32
01:10:11 --> 01:10:14
Intuit considers Splunk one of
their cornerstone technologies

33
01:10:14 --> 01:10:18
that is helping them innovate and
deliver better service to their customers.

34
01:10:18 --> 01:10:22
Cisco, one of the world's
largest technology providers,

35
01:10:22 --> 01:10:25
empowers their global security
team with Splunk enterprise

36
01:10:25 --> 01:10:30
to gain a centralized view into
end user and system activities.

37
01:10:30 --> 01:10:34
Splunk has dramatically helped
improve their incident detection and

38
01:10:34 --> 01:10:34
response rate.

39
01:10:36 --> 01:10:39
Splunk was key to Domino's Pizza's
success during the Super Bowl,

40
01:10:39 --> 01:10:43
by monitoring database uptime and
order response.

41
01:10:43 --> 01:10:45
Armed with new levels of
customer understanding,

42
01:10:45 --> 01:10:49
Domino's was able to strengthen their
online business, in addition to saving

43
01:10:49 --> 01:10:53
hundreds of thousands of dollars replacing
legacy technologies with Splunk.

44
01:10:53 --> 01:10:58
We are thrilled to hear Domino's
call Splunk their secret sauce.

45
01:10:58 --> 01:11:02
Another great Splunk story includes
cars and the Internet of things.

46
01:11:03 --> 01:11:07
Working together with the Ford Motor
Company and Ford's OpenXC platform,

47
01:11:07 --> 01:11:11
Splunk delivered connected car dashboards
to examine driving behavior and

48
01:11:11 --> 01:11:13
vehicle performance.

49
01:11:13 --> 01:11:17
UK-based Tesco is one of
the world's largest retailers,

50
01:11:17 --> 01:11:23
with nearly 1 million customers and
a half billion online orders per week.

51
01:11:23 --> 01:11:26
Customer satisfaction
is a critical metric.

52
01:11:26 --> 01:11:31
Tesco deployed Splunk to gain a unified
view across their websites, transactions,

53
01:11:31 --> 01:11:36
and business, gaining valuable digital
intelligence about their customers.

54
01:11:36 --> 01:11:41
Splunk was founded to pursue a disruptive
vision to make machine data accessible,

55
01:11:41 --> 01:11:44
usable, and valuable to everyone.

56
01:11:44 --> 01:11:46
Find out more about Splunk and

57
01:11:46 --> 01:11:49
operational intelligence by
downloading the executive summary.

58
01:11:49 --> 01:11:50
Or better yet,

59
01:11:50 --> 01:11:55
experience the value firsthand by
downloading Splunk software for free.

60
01:11:55 --> 01:11:58
Chances are someone in your
organization already has.

61
01:11:58 --> 01:12:04
[SOUND]

1
02:20:25 --> 02:20:27
Aggregations in Big Data Pipelines.

2
02:20:30 --> 02:20:33
After this video you will
be able to compare and

3
02:20:33 --> 02:20:38
select the aggregation operation that
you require to solve your problem.

4
02:20:38 --> 02:20:44
Explain how you can use aggregations to
compact your dataset and reduce volume.

5
02:20:44 --> 02:20:45
That is in many cases.

6
02:20:47 --> 02:20:52
And design complex operations in your
pipeline, using a series of aggregations.

7
02:20:55 --> 02:21:02
Aggregation is any operation on a data set
that performs a specific transformation,

8
02:21:02 --> 02:21:06
taking all the related data
elements into consideration.

9
02:21:08 --> 02:21:12
Let's say we have a bunch of stars
which are of different colors.

10
02:21:13 --> 02:21:17
Different colors denote diversity or
variety in the data.

11
02:21:19 --> 02:21:25
To keep things simple, we will use
letter 'f' to denote a transformation.

12
02:21:26 --> 02:21:27
In the following slides,

13
02:21:27 --> 02:21:33
we will see examples of how 'f' can take
the shape of different transformations.

14
02:21:36 --> 02:21:41
If we apply a transformation that does
something using the information of

15
02:21:41 --> 02:21:45
all the stars here,
we are performing an aggregation.

16
02:21:47 --> 02:21:52
Loosely speaking, we can say
that applying a transformation 'f'

17
02:21:52 --> 02:21:57
that takes all the elements of data
as input is called 'aggregation'.

18
02:22:01 --> 02:22:07
One of the simplest aggregations is
summation over all the data elements.

19
02:22:07 --> 02:22:11
In this case,
let's say every star counted as 1.

20
02:22:11 --> 02:22:15
Summing over all the stars gives 14,

21
02:22:15 --> 02:22:20
which is the summation of 3 stars for
yellow,

22
02:22:20 --> 02:22:24
5 stars for green, and
6 stars for color pink.

23
02:22:27 --> 02:22:33
Another aggregation that you could perform
is summation of individual star colors,

24
02:22:33 --> 02:22:36
that is, grouping the sums by color.

25
02:22:37 --> 02:22:43
So, If each star is a 1,
adding each group will result in 3 for

26
02:22:43 --> 02:22:48
yellow stars, 5 for green stars,
and 6 for pink stars.

27
02:22:49 --> 02:22:50
In this case,

28
02:22:50 --> 02:22:56
the aggregation function 'f' will output
3 tuples of star colors and counts.

29
02:22:59 --> 02:23:04
In a sales scenario, each color could
denote a different product type.

30
02:23:05 --> 02:23:10
And the number 1 could be replaced
by revenue generated by a product

31
02:23:10 --> 02:23:12
in each city the product is sold.

32
02:23:13 --> 02:23:17
In fact,
we will keep coming back to this analogy.

33
02:23:19 --> 02:23:24
You can also perform average
over items of similar kind,

34
02:23:24 --> 02:23:29
such as sums grouped by color.

35
02:23:31 --> 02:23:33
Continuing the example earlier,

36
02:23:33 --> 02:23:38
you can calculate average revenue per
product type using this aggregation.

37
02:23:40 --> 02:23:45
Other simple yet useful aggregational
operations to help you extract meaning

38
02:23:45 --> 02:23:52
from large data sets are maximum,
minimum, and standard deviation.

39
02:23:52 --> 02:23:57
Remember, you can always perform
aggregation as a series of operations,

40
02:23:57 --> 02:24:01
such as maximum of the sums per product.

41
02:24:01 --> 02:24:04
That is, summation followed by maximum.

42
02:24:05 --> 02:24:09
If you first sum sales for
each city, that is for

43
02:24:09 --> 02:24:14
each product,
then you can take the maximum of it

44
02:24:14 --> 02:24:20
by applying maximum function to
the result of the summation function.

45
02:24:20 --> 02:24:24
In this case, you get the product,
which has maximum sales in the country.

46
02:24:27 --> 02:24:31
Aggregation over Boolean data
sets that can have true-false or

47
02:24:31 --> 02:24:37
one-zero values could be a complex mixture
of AND, OR, and NOT logical operations.

48
02:24:39 --> 02:24:43
A lot of problems become easy
to manipulate using sets.

49
02:24:43 --> 02:24:47
Because sets don't allow duplicate values.

50
02:24:47 --> 02:24:50
Depending on your application,
this could be very useful.

51
02:24:51 --> 02:24:56
For example, to count the number
of products from a sales table,

52
02:24:56 --> 02:25:01
you can simply take all
the sales tables and create sets

53
02:25:01 --> 02:25:07
of these products in those tables,
and take a union of these sets.

54
02:25:09 --> 02:25:15
To summarize, by choosing the right
aggregation, you can generate compact and

55
02:25:15 --> 02:25:21
meaningful insights that enable faster and
effective decision making in business.

56
02:25:21 --> 02:25:23
You will find that in most cases,

57
02:25:23 --> 02:25:27
aggregation results in
smaller output data sets.

58
02:25:28 --> 02:25:33
Hence, aggregation is an important tool
set to keep in pocket when dealing with

59
02:25:33 --> 02:25:35
large data sets, and big data pipelines.

1
04:46:00 --> 04:46:03
Big Data Processing Pipelines:
A Dataflow Approach.

2
04:46:05 --> 04:46:09
Most big data applications
are composed of a set of operations

3
04:46:09 --> 04:46:11
executed one after another as a pipeline.

4
04:46:12 --> 04:46:15
Data flows through these operations,

5
04:46:15 --> 04:46:18
going through various
transformations along the way.

6
04:46:18 --> 04:46:22
We also call this dataflow graphs.

7
04:46:22 --> 04:46:25
So to understand big data
processing we should start by

8
04:46:25 --> 04:46:28
understanding what dataflow means.

9
04:46:29 --> 04:46:34
After this video you will be able to
summarize what dataflow means and

10
04:46:34 --> 04:46:36
it's role in data science.

11
04:46:36 --> 04:46:42
Explain split->do->merge as a big
data pipeline with examples,

12
04:46:42 --> 04:46:44
and define the term data parallel.

13
04:46:46 --> 04:46:51
Let's consider the hello
world MapReduce example for

14
04:46:51 --> 04:46:55
WordCount which reads one or
more text files and

15
04:46:55 --> 04:47:01
counts the number of occurrences
of each word in these text files.

16
04:47:01 --> 04:47:05
You are by now very familiar with
this example, but as a reminder,

17
04:47:05 --> 04:47:08
the output will be a text
file with a list of words and

18
04:47:08 --> 04:47:12
their occurrence frequencies
in the input data.

19
04:47:14 --> 04:47:18
In this application,
the files were first split into HDFS

20
04:47:18 --> 04:47:23
cluster nodes as partitions of
the same file or multiple files.

21
04:47:25 --> 04:47:28
Then a map operation, in this case,

22
04:47:28 --> 04:47:34
a user defined function to count words
was executed on each of these nodes.

23
04:47:34 --> 04:47:40
And all the key values that were output
from map were sorted based on the key.

24
04:47:40 --> 04:47:45
And the key values with the same word
were moved or shuffled to the same node.

25
04:47:46 --> 04:47:52
Finally, the reduce operation
was executed on these nodes

26
04:47:52 --> 04:47:55
to add the values for
key-value pairs with the same keys.

27
04:47:57 --> 04:48:03
If you look back at this example, we see
that there were four distinct steps,

28
04:48:03 --> 04:48:09
namely the data split step,
the map step, the shuffle and

29
04:48:09 --> 04:48:11
sort step, and the reduce step.

30
04:48:12 --> 04:48:16
Although, the word count
example is pretty simple it

31
04:48:16 --> 04:48:20
represents a large number of
applications that these three

32
04:48:20 --> 04:48:25
steps can be applied to achieve
data parallel scalability.

33
04:48:25 --> 04:48:31
We refer in general to this
pattern as "split-do-merge".

34
04:48:33 --> 04:48:39
In these applications, data flows
through a number of steps, going through

35
04:48:39 --> 04:48:44
transformations with various scalability
needs, leading to a final product.

36
04:48:46 --> 04:48:48
The data first gets partitioned.

37
04:48:49 --> 04:48:54
The split data goes through a set of
user-defined functions to do something,

38
04:48:55 --> 04:49:00
ranging from statistical operations to
data joins to machine learning functions.

39
04:49:01 --> 04:49:06
Depending on the application's
data processing needs,

40
04:49:06 --> 04:49:11
these "do something" operations can
differ and can be chained together.

41
04:49:12 --> 04:49:18
In the end results can be combined
using a merging algorithm or

42
04:49:18 --> 04:49:20
a higher-order function like reduce.

43
04:49:21 --> 04:49:25
We call the stitched-together
version of these sets of steps for

44
04:49:25 --> 04:49:28
big data processing "big data pipelines".

45
04:49:31 --> 04:49:36
The term pipe comes from
a UNIX separation that

46
04:49:36 --> 04:49:41
the output of one running program gets
piped into the next program as an input.

47
04:49:41 --> 04:49:45
As you might imagine,
one can string multiple programs together

48
04:49:45 --> 04:49:50
to make longer pipelines with various
scalability needs at each step.

49
04:49:51 --> 04:49:54
However, for big data processing,

50
04:49:54 --> 04:50:00
the parallelism of each step in
the pipeline is mainly data parallelism.

51
04:50:00 --> 04:50:05
We can simply define data parallelism
as running the same functions

52
04:50:05 --> 04:50:11
simultaneously for the elements or
partitions of a dataset on multiple cores.

53
04:50:11 --> 04:50:15
For example,
in our word count example, data

54
04:50:15 --> 04:50:19
parallelism occurs in every
step of the pipeline.

55
04:50:20 --> 04:50:24
There's definitely parallelization
during map over the input

56
04:50:24 --> 04:50:28
as each partition gets
processed as a line at a time.

57
04:50:28 --> 04:50:30
To achieve this type of data parallelism,

58
04:50:30 --> 04:50:35
we must decide on the data granularity
of each parallel computation.

59
04:50:35 --> 04:50:36
In this case, it is a line.

60
04:50:39 --> 04:50:44
We also see a parallel grouping of
data in the shuffle and sort phase.

61
04:50:44 --> 04:50:48
This time, the parallelization is
over the intermediate products,

62
04:50:48 --> 04:50:51
that is, the individual key-value pairs.

63
04:50:53 --> 04:50:56
And after the grouping of
the intermediate products

64
04:50:56 --> 04:51:01
the reduce step gets parallelized
to construct one output file.

65
04:51:02 --> 04:51:06
You have probably noticed that
the data gets reduced to a smaller set

66
04:51:06 --> 04:51:07
at each step.

67
04:51:09 --> 04:51:11
Although, the example we have given is for

68
04:51:11 --> 04:51:16
batch processing, similar techniques
apply to stream processing.

69
04:51:17 --> 04:51:18
Let's discuss this for

70
04:51:18 --> 04:51:22
our simplified advanced stream
data from an online game example.

71
04:51:23 --> 04:51:28
In this case, your event gets ingested

72
04:51:28 --> 04:51:32
through a real time big data ingestion
engine, like Kafka or Flume.

73
04:51:34 --> 04:51:38
Then they get passed into
a Streaming Data Platform for

74
04:51:38 --> 04:51:42
processing like Samza,
Storm or Spark streaming.

75
04:51:43 --> 04:51:49
This is a valid choice for processing
data one event at a time or chunking

76
04:51:49 --> 04:51:55
the data into Windows or Microbatches
of time or other features.

77
04:51:56 --> 04:52:02
Any pipeline processing of data can
be applied to the streaming data here

78
04:52:02 --> 04:52:05
as we wrote in a batch-
processing Big Data engine.

79
04:52:07 --> 04:52:13
The process stream data can then be
served through a real-time view or

80
04:52:13 --> 04:52:15
a batch-processing view.

81
04:52:15 --> 04:52:20
Real-time view is often subject to change
as potentially delayed new data comes in.

82
04:52:21 --> 04:52:26
The storage of the data can be
accomplished using H-Base, Cassandra,

83
04:52:26 --> 04:52:30
HDFS, or
many other persistent storage systems.

84
04:52:32 --> 04:52:37
To summarize, big data pipelines
get created to process data

85
04:52:37 --> 04:52:41
through an aggregated set of steps that
can be represented with the split-

86
04:52:41 --> 04:52:46
do-merge pattern with
data parallel scalability.

87
04:52:46 --> 04:52:49
This pattern can be
applied to many batch and

88
04:52:49 --> 04:52:52
streaming data processing applications.

89
04:52:52 --> 04:52:56
Next we will go through some processing
steps in a big data pipeline in

90
04:52:56 --> 04:53:01
more detail, first conceptually,
then practically in Spark.

1
09:39:00 --> 09:39:04
Now that we went through an overview
of the Spark ecosystem and

2
09:39:04 --> 09:39:08
the components of the Spark stack, it is
time for us to start learning more about

3
09:39:08 --> 09:39:12
its architecture and run our first
Spark program in the cloud VM.

4
09:39:14 --> 09:39:18
After this video you will be
able to describe how Spark

5
09:39:18 --> 09:39:23
does in-memory processing using the
Resilient Distributed Dataset abstraction,

6
09:39:24 --> 09:39:28
explain the inner workings of
the Spark architecture, and

7
09:39:28 --> 09:39:33
summarize how Spark manages and
executes code on Clusters.

8
09:39:34 --> 09:39:38
I mentioned a few times that
Spark is efficient because it

9
09:39:38 --> 09:39:42
uses an abstraction called RDDs for
in memory processing.

10
09:39:44 --> 09:39:47
What this means might not be
clear to some of you yet.

11
09:39:49 --> 09:39:51
Let's remember the alternative.

12
09:39:51 --> 09:39:56
In Hadoop MapReduce,
each step, also pipeline,

13
09:39:56 --> 09:40:01
reads from disk to memory,
performs the computations and

14
09:40:01 --> 09:40:05
writes back its output from
the memory to the disk.

15
09:40:07 --> 09:40:12
However, writing data to disk
is a costly operation and

16
09:40:12 --> 09:40:16
this cost becomes even more
with large volumes of data.

17
09:40:17 --> 09:40:19
Here is an interesting fact.

18
09:40:20 --> 09:40:24
Memory operations can
be up to 100,000 times

19
09:40:24 --> 09:40:27
faster than disk operations in some cases.

20
09:40:28 --> 09:40:32
Spark instead takes advantage of this and
allows for

21
09:40:32 --> 09:40:36
immediate results of transformations
in different stages of the pipeline and

22
09:40:36 --> 09:40:39
memory, like MAP and REDUCE here.

23
09:40:40 --> 09:40:44
Here, we see that the outputs
of MAP operations

24
09:40:44 --> 09:40:48
are shared with reduce operations
without being written to the disk.

25
09:40:49 --> 09:40:54
The containers where the data
gets stored in memory

26
09:40:54 --> 09:40:59
are called resilient distributed
datasets or RDDs for short.

27
09:41:01 --> 09:41:05
RDDs are how Spark distributes data and
computations across

28
09:41:05 --> 09:41:11
the nodes of a commodity cluster,
preferably with large memory.

29
09:41:11 --> 09:41:13
Thanks to this abstraction,

30
09:41:13 --> 09:41:17
Spark has proven to be 100 times
faster for some applications.

31
09:41:19 --> 09:41:22
Let's define all the words
in this interesting name

32
09:41:22 --> 09:41:25
beginning with the last one, datasets.

33
09:41:27 --> 09:41:35
Datasets that RDD distributes comes
from a batch data storage like HDFS,

34
09:41:35 --> 09:41:41
no SQL databases, text files or streaming
data ingestion systems like Cafco.

35
09:41:42 --> 09:41:46
It can even conveniently read and
distribute the data from your local disc

36
09:41:46 --> 09:41:51
like text files into Spark, or
even a hierarchy of folders.

37
09:41:53 --> 09:42:00
When Spark reads data from these sources,
it generates RDDs for them.

38
09:42:00 --> 09:42:06
The Spark operations can transform RDDs
into other RDDs like any other data.

39
09:42:06 --> 09:42:12
Here it's important to mention
that RDDs are immutable.

40
09:42:12 --> 09:42:16
This means that you cannot
change them partially.

41
09:42:16 --> 09:42:21
However, you can create new RDDs by
a series of one or many transformations.

42
09:42:23 --> 09:42:27
Next, let's look at what distributed
means in the RDD context.

43
09:42:28 --> 09:42:33
As I mentioned before, RDDs distribute
partitioned data collections and

44
09:42:33 --> 09:42:39
computations on clusters even
across a number of machines.

45
09:42:40 --> 09:42:43
For example, running on the Amazon Cloud.

46
09:42:45 --> 09:42:49
The complexity of this operation is
hidden by this very simple interface.

47
09:42:51 --> 09:42:56
Computations are a diverse set of
transformations of RDDs like map,

48
09:42:56 --> 09:42:58
filter and join.

49
09:42:58 --> 09:43:04
And also actions on the RDDs like counting
and saving them persistently on disk.

50
09:43:04 --> 09:43:09
The partitioning of data can be changed
dynamically to optimize Spark's

51
09:43:09 --> 09:43:10
performance.

52
09:43:11 --> 09:43:16
The last element is resilient, and
it's very important because in a large

53
09:43:16 --> 09:43:20
scale computing environment it is
pretty common to have node failures.

54
09:43:21 --> 09:43:24
It's very important to be
able to recover from these

55
09:43:24 --> 09:43:27
situations without losing
any work already done.

56
09:43:28 --> 09:43:33
For full tolerance in such situations,
Spark tracks the history of each

57
09:43:33 --> 09:43:38
partition, keeping a lineage
over RDDs over time,

58
09:43:38 --> 09:43:43
so every point in your calculations,
Spark knows which are the partitions

59
09:43:43 --> 09:43:46
needed to recreate the partition
in case it gets lost.

60
09:43:47 --> 09:43:49
And if that happens,

61
09:43:49 --> 09:43:54
then Spark automatically figures out
where it can start the recompute from and

62
09:43:54 --> 09:43:58
optimizes the amount of processing
needed to recover from the failure.

63
09:44:00 --> 09:44:05
Before we create our first Spark program
using RDDs in Pi Spark in the cloud

64
09:44:05 --> 09:44:08
layer VM,
let's review Spark's architecture.

65
09:44:11 --> 09:44:15
From a bird's eye view,
Spark has two main components,

66
09:44:15 --> 09:44:18
a driver program and worker nodes.

67
09:44:20 --> 09:44:24
The driver program is where
your application starts.

68
09:44:25 --> 09:44:29
It distributes RDDs on your
computational cluster and

69
09:44:29 --> 09:44:34
makes sure the transformations and
actions on these RDDs are performed.

70
09:44:36 --> 09:44:41
Driver programs create
a connection to a Spark cluster or

71
09:44:41 --> 09:44:44
your local Spark through
a Spark context object.

72
09:44:46 --> 09:44:49
The default Spark context
in the Spark shell is

73
09:44:49 --> 09:44:53
an object called SC for Spark context.

74
09:44:54 --> 09:45:00
For example, in the upcoming reading for
creating word counts in Spark,

75
09:45:00 --> 09:45:04
we will use SC as the context to

76
09:45:04 --> 09:45:09
generate RDDs for a text file
using the line of code shown here.

77
09:45:10 --> 09:45:16
The driver program manages a potentially
large number of nodes called worker nodes.

78
09:45:17 --> 09:45:22
On a local computer, we can assume
that there's only one worker node and

79
09:45:22 --> 09:45:24
it is where the Spark operations execute.

80
09:45:26 --> 09:45:31
A worker node in Spark keeps
a running Java virtual machine,

81
09:45:31 --> 09:45:35
called JVM commonly, called the executor.

82
09:45:37 --> 09:45:39
Depending on the illustration,

83
09:45:39 --> 09:45:44
executor can execute task
related to mapping stages or

84
09:45:44 --> 09:45:49
reducing stages or
other Spark specific pipelines.

85
09:45:49 --> 09:45:55
This Java virtual machine is the core
that all the computation is executed,

86
09:45:55 --> 09:46:02
and this is the interface also to the rest
of the Big Data storage systems and tools.

87
09:46:04 --> 09:46:09
For example, if we ever had the Hadoop
file system, HTFS, as the storage system,

88
09:46:09 --> 09:46:13
then on each worker node,
some of the data will be stored locally.

89
09:46:13 --> 09:46:18
As you know, the most important point
of this computing framework is to bring

90
09:46:18 --> 09:46:20
the computation to data.

91
09:46:20 --> 09:46:23
So Spark will send some
computational jobs to be executed

92
09:46:23 --> 09:46:29
on the data that are already available
on the machine thanks to HDFS.

93
09:46:29 --> 09:46:31
Data will be read from HDFS and

94
09:46:31 --> 09:46:37
get processed in memory,
the results will be stored as one RDD.

95
09:46:37 --> 09:46:44
The actual computation is running
straight in the executor,

96
09:46:44 --> 09:46:49
that is the JVM that runs your Scala or
Java codes.

97
09:46:49 --> 09:46:53
Instead, if you are using PySpark
then there will be several Python

98
09:46:53 --> 09:46:56
processes generally, one for task but

99
09:46:56 --> 09:47:00
you can configure it depending
on your application.

100
09:47:02 --> 09:47:08
In a real Big Data scenario, we have many
worker nodes running tasks internally.

101
09:47:09 --> 09:47:14
It is important to have a system that can
automatically manage provisioning and

102
09:47:14 --> 09:47:15
restarting of these nodes.

103
09:47:16 --> 09:47:19
The cluster manager in
Spark has this capability.

104
09:47:20 --> 09:47:26
Spark currently supports mainly three
interfaces for cluster management,

105
09:47:26 --> 09:47:33
namely Spark's standalone cluster manager,
the Apache Mesos, and Hadoop YARN.

106
09:47:35 --> 09:47:39
Standalone means that there's a special
Spark process that takes care of

107
09:47:39 --> 09:47:41
restarting nodes that are failing or

108
09:47:41 --> 09:47:44
starting nodes at the beginning
of the computation.

109
09:47:44 --> 09:47:49
YARN and Mesos are two external research
measures that can be used also for

110
09:47:49 --> 09:47:50
these purposes.

111
09:47:53 --> 09:47:56
Choosing a cluster manager
to fit your application and

112
09:47:56 --> 09:47:59
infrastructure can be quite confusing.

113
09:47:59 --> 09:48:03
Here, we give you a good
article as a starting point on

114
09:48:03 --> 09:48:06
how to pick the right cluster manager for
your organization.

115
09:48:07 --> 09:48:12
To summarize, the Spark architecture
includes a driver program.

116
09:48:13 --> 09:48:17
The driver program communicates
with the cluster manager for

117
09:48:17 --> 09:48:21
monitoring and
provisioning of resources and

118
09:48:21 --> 09:48:26
communicates directly with worker
nodes to submit and execute tasks.

119
09:48:26 --> 09:48:32
RDDs get created and passed within
transformations running in the executable.

120
09:48:33 --> 09:48:38
Finally, let's see how this
setup works on the Cloudera VM.

121
09:48:38 --> 09:48:42
In the Cloudera VM,
we are using Spark in standalone mode and

122
09:48:42 --> 09:48:45
everything is running locally.

123
09:48:45 --> 09:48:52
So it's a single machine and on the same
machine we have our driver program,

124
09:48:52 --> 09:48:57
the executor JVM and
our single PySpark process.

125
09:48:57 --> 09:49:01
With that, we are ready to start with
our first reading to install Spark and

126
09:49:01 --> 09:49:05
then running our word count program
using the Spark environment.

1
19:28:05 --> 19:28:09
After a brief overview of some
of the processing systems

2
19:28:09 --> 19:28:13
in the Big Data Landscape, it is time for
us to dive deeper into Spark.

3
19:28:14 --> 19:28:19
Spark was initiated at
UC Berkeley in 2009 and

4
19:28:19 --> 19:28:23
was transferred to
Apache Software Foundation in 2013.

5
19:28:23 --> 19:28:28
Since then, Spark has become a top
level project with many users and

6
19:28:28 --> 19:28:29
contributors worldwide.

7
19:28:31 --> 19:28:36
After this video, you will be able
to list the main motivations for

8
19:28:36 --> 19:28:40
the development of Spark,
draw the Spark stack as a layer diagram,

9
19:28:42 --> 19:28:46
And explain the functionality of
the components in the Spark stack.

10
19:28:48 --> 19:28:53
As we have discussed in our earlier
discussions, while Hadoop is great for

11
19:28:53 --> 19:28:57
batch processing using
the MapReduce programming module,

12
19:28:57 --> 19:28:59
it has shortcomings in a number of ways.

13
19:29:01 --> 19:29:06
First of all, since it is limited to
Map and Reduce based transformations,

14
19:29:06 --> 19:29:11
one has to restrict their big data
pipeline to map and reduce steps.

15
19:29:12 --> 19:29:15
But the number of applications
can be implemented using Map and

16
19:29:15 --> 19:29:18
Reduce, it's not always possible and

17
19:29:18 --> 19:29:22
it is often not the most efficient
way to express a big data pipeline.

18
19:29:24 --> 19:29:29
For example, you might want to do a join
operation between different data sets or

19
19:29:29 --> 19:29:32
you might want to filter or
sample your data.

20
19:29:33 --> 19:29:36
Or you might have a more
complicated data pipeline with

21
19:29:36 --> 19:29:40
several steps including joins and
group byes.

22
19:29:40 --> 19:29:45
It might have a Map and Reduce face,
but maybe another map face after that.

23
19:29:46 --> 19:29:52
These types of operations are hard or
impossible to express using MapReduce and

24
19:29:52 --> 19:29:55
cannot be accommodated by
the MapReduce framework in Hadoop.

25
19:29:57 --> 19:30:00
Another important bottleneck in
Hadoop MapReduce that is critical for

26
19:30:00 --> 19:30:05
performance, is that MapReduce relies
heavily on reading data from disc.

27
19:30:07 --> 19:30:12
This is especially a problem for iterative
algorithms that require taking several

28
19:30:12 --> 19:30:16
passes through the data using
a number of transformations.

29
19:30:16 --> 19:30:20
Since each transformation will need
to read its inputs from the disk,

30
19:30:20 --> 19:30:24
this will end up in a performance
bottleneck due to IO.

31
19:30:26 --> 19:30:29
Most machine learning pipelines
are in this category,

32
19:30:29 --> 19:30:32
making Hadoop MapReduce not ideal for
machine learning.

33
19:30:33 --> 19:30:38
And as I mentioned in the system overview,
the only programming language

34
19:30:38 --> 19:30:42
that MapReduce provides
a native interface for is Java.

35
19:30:42 --> 19:30:47
Although, it's possible to run Python code
to implementation for it is more complex

36
19:30:47 --> 19:30:52
and not very efficient especially when
you are running not with text data, but

37
19:30:52 --> 19:30:53
with floating point numbers.

38
19:30:54 --> 19:30:58
The programming language issue
also affects how interactive

39
19:30:58 --> 19:30:59
the environment is.

40
19:30:59 --> 19:31:02
Most data scientist
prefer to use scripting

41
19:31:02 --> 19:31:06
languages due to their
interactive shell capabilities.

42
19:31:06 --> 19:31:10
Not having such an interface in Hadoop
really makes it difficult to use and

43
19:31:10 --> 19:31:11
adapt my many in the field.

44
19:31:13 --> 19:31:17
In addition in the big data
era having support for

45
19:31:17 --> 19:31:20
streaming data processing is a key for

46
19:31:20 --> 19:31:24
being able to run similar analysis on
both real time and historical data.

47
19:31:26 --> 19:31:30
Spark came out of the need to
extend the MapReduce framework

48
19:31:30 --> 19:31:32
to overcome this shortcomings and

49
19:31:32 --> 19:31:37
provide an expressive cluster computing
environment that can provide interactive

50
19:31:37 --> 19:31:41
querying, efficient iterative analytics
and streaming data processing.

51
19:31:43 --> 19:31:46
So, how does Apache Spark provide
solutions for these problems?

52
19:31:48 --> 19:31:52
Spark provides a very rich and
expressive programming module that

53
19:31:52 --> 19:31:57
gives you more than 20 highly efficient
distributed operations or transformations.

54
19:31:58 --> 19:32:02
Pipe-lining any of these steps in Spark
simply takes a few lines of code.

55
19:32:04 --> 19:32:09
Another important feature of Spark is
the ability to run these computations

56
19:32:09 --> 19:32:09
in memory.

57
19:32:10 --> 19:32:14
It's ability to cache and
process data in memory,

58
19:32:14 --> 19:32:18
makes it significantly faster for
iterative applications.

59
19:32:18 --> 19:32:23
This is proven to provide a factor
of ten or even 100 speed-up

60
19:32:23 --> 19:32:27
in the performance of some algorithms,
especially using large data sets.

61
19:32:29 --> 19:32:34
Additionally, Spark provides support for
batch and streaming workloads at once.

62
19:32:35 --> 19:32:40
Last but not least,
Spark provides simple APIs for Python,

63
19:32:40 --> 19:32:45
Scala, Java and SQL programming
through an interactive shell to

64
19:32:45 --> 19:32:50
accomplish analytical tasks through both
external and its built-in libraries.

65
19:32:52 --> 19:32:56
The Spark layer diagram,
also called Stack,

66
19:32:56 --> 19:33:01
consists of components that build on
top of the Spark computational engine.

67
19:33:02 --> 19:33:08
This engine distributes and monitors tasks
across the nodes of a commodity cluster.

68
19:33:09 --> 19:33:14
The components built on top of this
engine are designed to interact and

69
19:33:14 --> 19:33:16
communicate through this common engine.

70
19:33:17 --> 19:33:21
Any improvements to
the underlying engine becomes

71
19:33:21 --> 19:33:25
an improvement in the other components,
thanks to such close interaction.

72
19:33:27 --> 19:33:32
This also enables building applications
that's span across these different

73
19:33:32 --> 19:33:37
components like querying data using
Spark SQL and applying machine learning

74
19:33:37 --> 19:33:43
algorithms, the query results using Sparks
machine learning library and MLlib.

75
19:33:45 --> 19:33:49
The Spark Core is where
the core capability is

76
19:33:49 --> 19:33:52
of the Spark Framework are implemented.

77
19:33:52 --> 19:33:54
This includes support for

78
19:33:54 --> 19:33:58
distributed scheduling,
memory management and full tolerance.

79
19:33:59 --> 19:34:03
Interaction with different schedulers,
like YARN and Mesos and

80
19:34:03 --> 19:34:08
various NoSQL storage systems like
HBase also happen through Spark Core.

81
19:34:10 --> 19:34:14
A very important part of
Spark Core is the APIs for

82
19:34:14 --> 19:34:19
defining resilient distributed data sets,
or RDDs for short.

83
19:34:19 --> 19:34:22
RDDs are the main programming
abstraction in Spark,

84
19:34:22 --> 19:34:27
which carry data across many computing
nodes in parallel, and transform it.

85
19:34:27 --> 19:34:33
Spark SQL is the component of Spark
that provides querying structured and

86
19:34:33 --> 19:34:37
unstructured data through
a common query language.

87
19:34:37 --> 19:34:42
It can connect to many data sources and
provide APIs to convert

88
19:34:42 --> 19:34:47
query results to RDDs in Python,
Scala and Java programs.

89
19:34:47 --> 19:34:53
Spark Streaming is where data
manipulations take place in Spark.

90
19:34:55 --> 19:35:01
Although, not a native real-time interface
to datastreams, Spark streaming enables

91
19:35:01 --> 19:35:05
creating small aggregates of data coming
from streaming data ingestion systems.

92
19:35:07 --> 19:35:12
These aggregate datasets
are called micro-batches and

93
19:35:12 --> 19:35:15
they can be converted into RDBs in
Spark Streaming for processing.

94
19:35:17 --> 19:35:20
MLlib is Sparks native library for

95
19:35:20 --> 19:35:25
machine learning algorithms
as well as model evaluation.

96
19:35:25 --> 19:35:30
All of the functionality is potentially
ported to any programming language Sparks

97
19:35:30 --> 19:35:34
supports and
is designed to scale out using Spark.

98
19:35:35 --> 19:35:39
GraphX is the graph analytics
library of Spark and

99
19:35:39 --> 19:35:45
enables the Vertex edge data model of
graphs to be converted into RDDs as

100
19:35:45 --> 19:35:49
well as providing scalable implementations
of graph processing algorithms.

101
19:35:51 --> 19:35:56
To summarize, through these
layers Spark provides diverse,

102
19:35:56 --> 19:36:01
scalable interactive management and
analyses of big data.

103
19:36:01 --> 19:36:07
The interactive shell enables data
scientists to conduct exploratory

104
19:36:07 --> 19:36:12
analysis and create big data pipelines,
while also enabling the big

105
19:36:12 --> 19:36:17
data system integration engineers
to scale these analytical pipelines

106
19:36:17 --> 19:36:22
across commodity computing clusters and
cloud environments.

1
15:04:27 --> 15:04:31
There are many big data
processing systems, but

2
15:04:31 --> 15:04:35
how do we make sense of them in order to
take full advantage of these systems?

3
15:04:36 --> 15:04:39
In this video,
we will review some of them, and

4
15:04:39 --> 15:04:43
a way to categorize big data processing
systems as we go through our review.

5
15:04:45 --> 15:04:50
After this video, you will be able
to recall the Hadoop ecosystem,

6
15:04:51 --> 15:04:56
draw a layer diagram with three layers for
data storage, data processing and

7
15:04:56 --> 15:04:57
workflow management.

8
15:04:57 --> 15:05:02
Summarize an evaluation criteria for
big data processing systems and

9
15:05:02 --> 15:05:07
explain the properties of Hadoop,
Spark, Flink,

10
15:05:07 --> 15:05:11
Beam, and Storm as major big
data processing systems.

11
15:05:12 --> 15:05:15
In our introduction to big data course,

12
15:05:15 --> 15:05:19
we talked about a version of
the layer diagram for the tools

13
15:05:19 --> 15:05:25
in the Hadoop ecosystem organized
vertically based on the interface.

14
15:05:25 --> 15:05:29
Lower level interface is the storage and
scheduling on the bottom and

15
15:05:29 --> 15:05:32
higher level languages and
interactivity at the top.

16
15:05:33 --> 15:05:38
Most of the tools in the Hadoop ecosystem
are initially built to complement

17
15:05:38 --> 15:05:43
the capabilities of Hadoop for distributed
filesystem management using HDFS.

18
15:05:44 --> 15:05:49
Data processing using the MapReduce
engine, and resource scheduling and

19
15:05:49 --> 15:05:51
negotiation using the YARN engine.

20
15:05:52 --> 15:05:56
Over time,
a number of new projects were built

21
15:05:56 --> 15:06:01
either to add to these
complimentary tools or to handle

22
15:06:01 --> 15:06:05
additional types of big data management
and processing not available in Hadoop.

23
15:06:07 --> 15:06:12
Arguably, the most important
change to Hadoop over time

24
15:06:12 --> 15:06:17
was the separation of YARN from
the MapReduce programming model

25
15:06:17 --> 15:06:20
to solely handle resource
management concerns.

26
15:06:21 --> 15:06:26
This allowed for Hadoop to be extensible
to different programming models.

27
15:06:26 --> 15:06:30
And enabled the development of
a number of processing engines for

28
15:06:30 --> 15:06:32
batch and stream processing.

29
15:06:34 --> 15:06:38
Another way to look at the vast number of
tools that have been added to the Hadoop

30
15:06:38 --> 15:06:41
ecosystem, is from the point of view of

31
15:06:41 --> 15:06:43
their functionality in the big
data processing pipeline.

32
15:06:44 --> 15:06:50
Simply put, these associate to three
distinct layers for data management and

33
15:06:50 --> 15:06:57
storage, data processing, and resource
coordination and workflow management.

34
15:06:58 --> 15:07:03
In our second course,
we talked about the bottom layer

35
15:07:03 --> 15:07:08
in this diagram in detail,
namely the data management and storage.

36
15:07:10 --> 15:07:16
While this layer includs Hadoop's HDFS
there are a number of other systems that

37
15:07:16 --> 15:07:22
rely on HDFS as a file system or implement
their own no SQL storage options.

38
15:07:24 --> 15:07:28
As big data can have a variety of
structure, semi structured and

39
15:07:28 --> 15:07:33
unstructured formats, and
gets analyzed through a variety of tools,

40
15:07:33 --> 15:07:38
many tools were introduced to
fit this variety of needs.

41
15:07:38 --> 15:07:41
We call these big data management systems.

42
15:07:44 --> 15:07:48
We reviewed redis and
Aerospike as key value stores,

43
15:07:48 --> 15:07:51
where each data item is
identified with a unique key.

44
15:07:53 --> 15:07:56
We got some practical
experience with Lucene and

45
15:07:56 --> 15:08:01
Gephi as vector and
graph data stores, respectively.

46
15:08:02 --> 15:08:06
We also talked about Vertica
as a column store database,

47
15:08:06 --> 15:08:10
where information is stored
in columns rather than rows.

48
15:08:11 --> 15:08:15
Cassandra and
Hbase are also in this category.

49
15:08:16 --> 15:08:22
Finally, we introduce Solr and
Asterisk DB for managing unstructured and

50
15:08:22 --> 15:08:28
semi-structured text, and
mongoDB as a document store.

51
15:08:28 --> 15:08:33
The processing layer is where these
different varieties of data gets

52
15:08:33 --> 15:08:40
retrieved, integrated, and analyzed,
which is the primary focus of this class.

53
15:08:42 --> 15:08:45
In the integration and processing layer

54
15:08:45 --> 15:08:49
we roughly refer to the tools that
are built on top of the HDFS and YARN,

55
15:08:49 --> 15:08:54
although some of them work with
other storage and file systems.

56
15:08:56 --> 15:09:03
YARN is a significant enabler of many of
these tools making a number of batch and

57
15:09:03 --> 15:09:08
stream processing engines like Storm,
Spark, Flink and being possible.

58
15:09:09 --> 15:09:13
We will revisit these processing
engines and explain why we have so

59
15:09:13 --> 15:09:14
many later in this lecture.

60
15:09:15 --> 15:09:20
This layer also includes tools like Hive,
or Spark SQL, for

61
15:09:20 --> 15:09:25
bringing a query interface
on top of the storage layer.

62
15:09:25 --> 15:09:30
Pig, for scripting simple big data
pipelines using the MapReduce framework.

63
15:09:30 --> 15:09:35
And a number of specialized analytical
libraries for machine learning and

64
15:09:35 --> 15:09:40
graph analytics, like Giraph as GraphX of
Spark are examples of such libraries for

65
15:09:40 --> 15:09:41
graph processing.

66
15:09:41 --> 15:09:46
And Mahout on top of the Hadoop stack and
MLlib of Spark are two options for

67
15:09:46 --> 15:09:47
machine learning.

68
15:09:48 --> 15:09:52
Although we have a basic overview of
graph processing and machine learning for

69
15:09:52 --> 15:09:57
big data analytics later in this course,
we won't go into the details here.

70
15:09:57 --> 15:10:02
Instead, we will have a dedicated
course on each of them

71
15:10:02 --> 15:10:04
later in this specialization.

72
15:10:04 --> 15:10:12
The third and top layer in our diagram is
the coordination and management layer.

73
15:10:12 --> 15:10:16
This is where integration,
scheduling, coordination, and

74
15:10:16 --> 15:10:22
monitoring of applications across many
tools in the bottom two layers take place.

75
15:10:23 --> 15:10:27
This layer is also where the results of
the big data analysis gets communicated to

76
15:10:27 --> 15:10:33
other programs, websites, visualization
tools, and business intelligence tools.

77
15:10:33 --> 15:10:38
Workflow management systems help to
develop automated solutions that can

78
15:10:38 --> 15:10:43
manage and coordinate the process of
combining data management and analytical

79
15:10:43 --> 15:10:49
tests in a big data pipeline, as
a configurable, structured set of steps.

80
15:10:51 --> 15:10:55
The workflow driven thinking also
matches this basic process of data

81
15:10:55 --> 15:10:57
science that we overviewed before.

82
15:10:59 --> 15:11:03
Oozie is an example of a workflow
scheduler that can interact with

83
15:11:03 --> 15:11:06
many of the tools in the integration and
processing layer.

84
15:11:07 --> 15:11:12
Zookeeper is the resource coordination and
monitoring tool and

85
15:11:12 --> 15:11:16
manages and coordinates all these tools
and middleware named after animals.

86
15:11:18 --> 15:11:22
Although virtual management is
my personal research area and

87
15:11:22 --> 15:11:24
I talk more about it in other venues,

88
15:11:24 --> 15:11:29
in this specialization we focus mainly
on big data integration and processing.

89
15:11:29 --> 15:11:35
And we will not have a specific
lecture on this layer in this course.

90
15:11:35 --> 15:11:39
We give you a reading on big data
workflows after this video as

91
15:11:39 --> 15:11:42
further information and
a starting point for the subject.

1
06:16:10 --> 06:16:14
In data integration and
processing pipelines,

2
06:16:14 --> 06:16:19
data goes through a number of operations,
which can apply

3
06:16:19 --> 06:16:25
a specific function to it, can work
the data from one format to another,

4
06:16:25 --> 06:16:32
join data with other data sets, or
filter some values out of a data set.

5
06:16:33 --> 06:16:39
We generally refer to these as
transformations, some of which can also

6
06:16:39 --> 06:16:44
be specially named aggregations as you
have seen in Amarnath's earlier lectures.

7
06:16:44 --> 06:16:51
In this video we will reveal some common
transformation operations that we see

8
06:16:51 --> 06:16:56
in these pipelines, some of which,
we refer to as data parallel patterns.

9
06:16:58 --> 06:17:03
After this video you will
be able to list common data

10
06:17:03 --> 06:17:08
transformations within big data pipelines,
and design

11
06:17:08 --> 06:17:13
a conceptual data processing pipeline
using the basic data transformations.

12
06:17:15 --> 06:17:20
Simply speaking, transformations
are higher order functions or

13
06:17:20 --> 06:17:25
tools to convert your data from
one form to another, just like

14
06:17:25 --> 06:17:31
we would use tools at the wood shop
to transform logs into furniture.

15
06:17:32 --> 06:17:35
When we look at big data
pipelines used today,

16
06:17:36 --> 06:17:40
map is probably the most
common transformation we find.

17
06:17:42 --> 06:17:47
The map operation is one of the basic
building blocks of the big data pipeline.

18
06:17:48 --> 06:17:54
When you want to apply a process
to each member of a collection,

19
06:17:54 --> 06:17:57
such as adding 10% bonus to each

20
06:17:57 --> 06:18:01
person's salary on a given month a map
operation comes in very handy.

21
06:18:03 --> 06:18:08
It takes your process and
understand that it is required to perform

22
06:18:08 --> 06:18:12
the same operation or
process to each member of the set.

23
06:18:14 --> 06:18:18
The figure on the left
here shows the application

24
06:18:18 --> 06:18:23
of a map function to data
depicted in grey color.

25
06:18:23 --> 06:18:30
Here colors red, blue, and
yellow are keys to identify each data set.

26
06:18:32 --> 06:18:39
As you see, each data set is executed
separately even for the same colored key.

27
06:18:43 --> 06:18:47
The reduce operation helps you then
to collectively apply the same

28
06:18:47 --> 06:18:50
process to objects of similar nature.

29
06:18:52 --> 06:18:58
For example, when you want to add your
monthly spending in different categories,

30
06:18:58 --> 06:19:04
like grocery, fuel, and dining out,
the reduce operation is very useful.

31
06:19:06 --> 06:19:09
In our figure here on the top left,

32
06:19:09 --> 06:19:14
we see that data sets in
grey with the same color

33
06:19:14 --> 06:19:19
are keys grouped together
using a reduced function.

34
06:19:20 --> 06:19:24
Reds together, blues together,
and yellows together.

35
06:19:26 --> 06:19:31
It would be a good idea to check out
the Spark word count hands-on to see

36
06:19:31 --> 06:19:36
how map and reduce can be used
effectively for getting things done.

37
06:19:37 --> 06:19:42
Map and reduce are types of
transformations that work on a single

38
06:19:42 --> 06:19:47
list of key and data pairings just
like we see on the left of our figure.

39
06:19:50 --> 06:19:55
Now let's consider a scenario where
we have two data sets identified

40
06:19:55 --> 06:20:01
by the same keys just like the two
sets and colors in our diagram.

41
06:20:03 --> 06:20:08
Many operations have such needs where
we have to look at all the pairings of

42
06:20:08 --> 06:20:12
all key value pairs,
just like crossing two matrices.

43
06:20:14 --> 06:20:19
For a practical example
Imagine you have two teams,

44
06:20:19 --> 06:20:24
a sales team with two people, and
an operations team with four people.

45
06:20:26 --> 06:20:30
In an event you would want each
person to meet every other person.

46
06:20:32 --> 06:20:35
In this case, a cross product, or

47
06:20:35 --> 06:20:40
a cartesian product, becomes a good
choice for organizing the event and

48
06:20:40 --> 06:20:44
sharing each pairs' meeting location and
travel time to them.

49
06:20:45 --> 06:20:51
In a cross or cartesian product operation,
each data partition gets

50
06:20:51 --> 06:20:57
paired with all other data partitions,
regardless of its key.

51
06:20:57 --> 06:21:00
This sometimes gets
referred to as all pairs.

52
06:21:02 --> 06:21:08
Now add to the cross product by
just grouping together the data

53
06:21:08 --> 06:21:13
partitions with the same key,
just like the red data.

54
06:21:15 --> 06:21:17
And the yellow data partitions here.

55
06:21:20 --> 06:21:24
This is a typical match or join operation.

56
06:21:24 --> 06:21:30
As we see in the figure here, match
is very similar to the cross product,

57
06:21:30 --> 06:21:33
except that it is more
selective in forming pairs.

58
06:21:34 --> 06:21:37
Every pair must have something in common.

59
06:21:38 --> 06:21:42
This something in common is
usually referred to as a key.

60
06:21:44 --> 06:21:48
For example,
each person in your operations team and

61
06:21:48 --> 06:21:51
sales team is assigned
to a different product.

62
06:21:51 --> 06:21:56
You only want those people to meet
who are working on the same product.

63
06:21:56 --> 06:22:00
In this case your key is product.

64
06:22:00 --> 06:22:03
And you can perform and
match operation and

65
06:22:03 --> 06:22:07
send e-mails to those people
who share a common product.

66
06:22:09 --> 06:22:14
The number of e-mails is likely to be less
than when you performed a cartesian or

67
06:22:14 --> 06:22:18
a cross product, therefore reducing
the cost of the operation.

68
06:22:20 --> 06:22:27
In a match operation, only the keys
with data in both sets get joined,

69
06:22:28 --> 06:22:31
and become a part of the final
output of the transformation.

70
06:22:34 --> 06:22:39
Now let's consider listing
the data sets with all the keys,

71
06:22:39 --> 06:22:41
even if they don't exist in both sets.

72
06:22:43 --> 06:22:48
Consider a scenario where you
want to do brainstorming sessions

73
06:22:48 --> 06:22:51
of people from operations and sales, and

74
06:22:51 --> 06:22:55
get people who work on the same
products in the same rooms.

75
06:22:58 --> 06:23:00
A co-group operation will do this for you.

76
06:23:02 --> 06:23:06
You give it a product name
as they key to work with and

77
06:23:06 --> 06:23:09
the two tables, the sales team and
operations team.

78
06:23:10 --> 06:23:15
The co-group will create groups
which contain team members

79
06:23:15 --> 06:23:21
working on common products even if a
product doesn't exist in one of the sets.

80
06:23:24 --> 06:23:28
The last operation we will
see is the filter operation.

81
06:23:28 --> 06:23:31
Filter works much like a test

82
06:23:31 --> 06:23:36
where only elements that pass
a test are shown in the output.

83
06:23:38 --> 06:23:42
Consider as a set that contains teams and
a number of members in their teams.

84
06:23:43 --> 06:23:46
If your game requires people to pair up,

85
06:23:46 --> 06:23:50
you may want to select teams which
have an even number of members.

86
06:23:51 --> 06:23:57
In this case, you can create a test
that only passes the teams which have

87
06:23:57 --> 06:24:04
an even number of team members shown as
divided by 2 with 0 in the remainder.

88
06:24:07 --> 06:24:12
The real effectiveness of the basic
transformation we saw here

89
06:24:12 --> 06:24:17
is in pipelining them in a way that
helps you to solve your specific

90
06:24:17 --> 06:24:21
problem just as you would perform
a series of tasks on a real block

91
06:24:21 --> 06:24:26
of wood to make a fine piece of
woodwork that you can use to steer your

92
06:24:26 --> 06:24:30
ship, which in this case is
your business or research.

1
12:40:38 --> 12:40:41
Now that we revealed all three layers,

2
12:40:41 --> 12:40:46
we are ready to come back to
the Integration and Processing layer.

3
12:40:46 --> 12:40:50
Just a simple Google search for
Big Data Processing Pipelines

4
12:40:50 --> 12:40:55
will bring a vast number of pipelines
with large number of technologies

5
12:40:55 --> 12:41:00
that support scalable data cleaning,
preparation, and analysis.

6
12:41:02 --> 12:41:06
How do we make sense of it all to
make sure we use the right tools for

7
12:41:06 --> 12:41:07
our application?

8
12:41:08 --> 12:41:13
We will continue our lecture to review
a set of evaluation criteria for

9
12:41:13 --> 12:41:18
these systems and some of the big data
processing systems based on this criteria.

10
12:41:20 --> 12:41:25
Depending on the resources we have access
to and characteristics of our application,

11
12:41:25 --> 12:41:29
we apply several
considerations to evaluate and

12
12:41:29 --> 12:41:30
pick a software stack for big data.

13
12:41:32 --> 12:41:37
Of these, first one we consider
is the Execution Model,

14
12:41:37 --> 12:41:42
and the expressivity of it to support for
various transformations of batch or

15
12:41:42 --> 12:41:45
streaming data, or
sometimes interactive computing.

16
12:41:46 --> 12:41:50
Semantics of streaming,
including exactly once or

17
12:41:50 --> 12:41:55
at least one processing for each event, or
being able to keep the state of the data,

18
12:41:55 --> 12:41:59
is an important concern for
this execution model.

19
12:41:59 --> 12:42:04
Latency is another important criteria,
depending on the application.

20
12:42:05 --> 12:42:08
Having a low latency system
is very important for

21
12:42:08 --> 12:42:12
applications like online gaming and
hazards management.

22
12:42:12 --> 12:42:16
Whereas most applications
are less time critical,

23
12:42:16 --> 12:42:21
like search engine indexing, and would
be fine with a batch processing ability.

24
12:42:22 --> 12:42:27
Scalability for both small and
large datasets and different

25
12:42:27 --> 12:42:32
analytical methods and algorithms,
is also an important evaluation criteria.

26
12:42:33 --> 12:42:37
As well as support for
different programming language

27
12:42:37 --> 12:42:41
of the libraries used by the analytical
tools that we have access to.

28
12:42:42 --> 12:42:47
Finally, while all big data
tools provide fault tolerance,

29
12:42:47 --> 12:42:52
the mechanics of how the fault tolerance is
handled is an important issue to consider.

30
12:42:53 --> 12:42:56
Let's review five of
the big data processing

31
12:42:56 --> 12:43:01
engines supported by the Apache Foundation
using this evaluation criteria.

32
12:43:03 --> 12:43:08
The MapReduce implementation of
Hadoop provides a batch execution

33
12:43:08 --> 12:43:13
model where the data from HDFS gets
loaded into mappers before processing.

34
12:43:14 --> 12:43:17
There is no in-memory processing support,

35
12:43:17 --> 12:43:22
meaning the mappers write the data on
files before the reducers can read it,

36
12:43:22 --> 12:43:26
resulting in a high-latency and
less scalable execution.

37
12:43:27 --> 12:43:31
This also hinders
the performance of iterative and

38
12:43:31 --> 12:43:36
interactive applications that require many
steps of transformations using MapReduce.

39
12:43:39 --> 12:43:42
Although the only native
programming interface for

40
12:43:42 --> 12:43:47
MapReduce is in Java, other programming
languages like Python provide modules or

41
12:43:47 --> 12:43:52
libraries for Hadoop MapReduce
programming, however with less efficiency.

42
12:43:54 --> 12:43:58
Data replication is the primary
method of fault tolerance,

43
12:43:58 --> 12:44:04
which in turn affects the scalability and
execution speed further.

44
12:44:04 --> 12:44:07
Spark was built to support iterative and

45
12:44:07 --> 12:44:12
interactive big data processing
pipelines efficiently using an in-memory

46
12:44:12 --> 12:44:17
structure called Resilient Distributed
Datasets, or shortly, RDDs.

47
12:44:19 --> 12:44:23
In addition to map and reduce operations,
it provides support for

48
12:44:23 --> 12:44:27
a range of transformation
operations like join and filter.

49
12:44:27 --> 12:44:33
Any pipeline of transformations can
be applied to these RDD's in-memory,

50
12:44:33 --> 12:44:38
making Spark's performance very high for
iterative processing.

51
12:44:40 --> 12:44:45
The RDD extraction is also designed
to handle fault tolerance with

52
12:44:45 --> 12:44:47
less impact on performance.

53
12:44:48 --> 12:44:51
In addition to HDFS,

54
12:44:51 --> 12:44:56
Spark can read data from many storage
platforms and it provides support for

55
12:44:56 --> 12:45:02
streaming data applications using
a technique called micro-batching.

56
12:45:02 --> 12:45:08
Its latency can be on the order of
seconds depending on the batch size,

57
12:45:08 --> 12:45:12
which is relatively slower compared
to native streaming platforms.

58
12:45:13 --> 12:45:18
Spark has support for a number of
programming languages, including Scala and

59
12:45:18 --> 12:45:22
Python as the most popular ones,
as well as built-in libraries for

60
12:45:22 --> 12:45:24
graph processing and machine learning.

61
12:45:25 --> 12:45:29
Although Flink has very
similar transformations and

62
12:45:29 --> 12:45:33
in-memory data extractions with Spark,
it provides direct support for

63
12:45:33 --> 12:45:37
streaming data,
making it a lower-latency framework.

64
12:45:38 --> 12:45:42
It provides connection interfaces to
streaming data ingestion engines like

65
12:45:42 --> 12:45:44
Kafka and Flume.

66
12:45:45 --> 12:45:49
Flink supports application
programming interfaces in Java and

67
12:45:49 --> 12:45:50
Scala just like Spark.

68
12:45:50 --> 12:45:55
Starting with it's original
version called Stratosphere,

69
12:45:55 --> 12:46:01
Flink had it's own execution engine
called Nephele, and had an ability

70
12:46:01 --> 12:46:05
to run both on Hadoop and also separately
in its own execution environment.

71
12:46:07 --> 12:46:11
In addition to map and reduce,
Flink provides abstractions for

72
12:46:11 --> 12:46:15
other data parallel database
patterns like join and group by.

73
12:46:17 --> 12:46:22
One of the biggest advantage of using
Flink comes from it's optimizer

74
12:46:22 --> 12:46:25
to pick and apply the best pattern and
execution strategy.

75
12:46:26 --> 12:46:30
There has been experiments comparing
fault tolerance features of Flink

76
12:46:30 --> 12:46:34
to those of Sparks, which conclude
that Sparks slightly better for Spark.

77
12:46:35 --> 12:46:41
The Beam system, from Google,
is a relatively new system for

78
12:46:41 --> 12:46:44
batch and stream processing with
a data flow programming model.

79
12:46:46 --> 12:46:51
It initially used Google's own Cloud data
flow as an execution environment, but

80
12:46:51 --> 12:46:55
Spark and Flink backends for
it have been implemented recently.

81
12:46:55 --> 12:47:00
It's a low-latency environment with
high reviews on fault tolerance.

82
12:47:01 --> 12:47:06
It currently provides application
programming interfaces in Java and

83
12:47:06 --> 12:47:09
Scala, and a Python SDK is in the works.

84
12:47:09 --> 12:47:12
SDK means software development kit.

85
12:47:12 --> 12:47:18
Beam provides a very strong streaming and
windowing framework for streaming data.

86
12:47:18 --> 12:47:23
And it is highly scalable and reliable,
allowing it to make trade-off

87
12:47:23 --> 12:47:27
decisions between accuracy,
speed, and cost of processing.

88
12:47:29 --> 12:47:31
Storm has been designed for

89
12:47:31 --> 12:47:36
stream processing in real
time with very low-latency.

90
12:47:36 --> 12:47:41
It defined input stream interface
abstractions called spouts, and

91
12:47:41 --> 12:47:44
computation abstractions called bolts.

92
12:47:45 --> 12:47:49
Spouts and bolts can be pipelined
together using a data flow approach.

93
12:47:49 --> 12:47:53
That data gets queued until
the computation acknowledges

94
12:47:53 --> 12:47:54
the receipt of it.

95
12:47:56 --> 12:47:59
A master node tracks running jobs and

96
12:47:59 --> 12:48:02
ensures all data is processed
by the computations on workers.

97
12:48:04 --> 12:48:09
Nathan Mars, the lead developer for
Storm, built the Lambda Architecture

98
12:48:09 --> 12:48:15
using Storm for stream processing and
Hadoop MapReduce for batch processing.

99
12:48:18 --> 12:48:23
The Lambda Architecture
originally used Storm for

100
12:48:23 --> 12:48:28
speed layer and Hadoop and
HBase for batch and

101
12:48:28 --> 12:48:32
serving layers, as seen in this diagram.

102
12:48:33 --> 12:48:38
However, it was later used as a more
general framework that can combine

103
12:48:38 --> 12:48:44
the results of stream and batch processing
executed in multiple big data systems.

104
12:48:44 --> 12:48:50
This diagram shows a generalized Lambda
Architecture containing some of the tools

105
12:48:50 --> 12:48:57
we discussed earlier, including using
Spark for both batch and speed layers.

106
12:48:59 --> 12:49:03
In this course, we picked Spark
as a big data integration and

107
12:49:03 --> 12:49:08
processing environment since it supports
most of our evaluation criteria.

108
12:49:08 --> 12:49:13
And this hybrid data processing
architecture using built-in data querying,

109
12:49:13 --> 12:49:16
streaming, and analytical libraries.

110
12:49:16 --> 12:49:21
We will continue our discussion with
Spark and hands-on exercises in Spark.

1
01:30:00 --> 01:30:03
Analytical operations
in big data pipelines.

2
01:30:04 --> 01:30:06
After this video,

3
01:30:06 --> 01:30:12
you will be able to list common analytical
operations within big data pipelines and

4
01:30:12 --> 01:30:15
describe sample applications for
these analytical operations.

5
01:30:17 --> 01:30:23
In this lesson, we will be
looking at analytical operations.

6
01:30:23 --> 01:30:27
These are operations used in analytics,
which is the process of

7
01:30:27 --> 01:30:32
transforming data into insights for
making more informed decisions.

8
01:30:33 --> 01:30:38
The purpose of analytical operations is to
analyze the data to discover meaningful

9
01:30:38 --> 01:30:43
trends and patterns, in order to gain
insights into the problem being studied.

10
01:30:44 --> 01:30:46
The knowledge gained from these insights

11
01:30:46 --> 01:30:50
ultimately lead to more informed
decisions driven by data.

12
01:30:53 --> 01:30:57
Here are some common analytical operations
that we will discuss in this lecture.

13
01:30:57 --> 01:31:01
Classification, clustering,

14
01:31:01 --> 01:31:05
path analysis and connectivity analysis.

15
01:31:06 --> 01:31:08
Let's start with classification.

16
01:31:09 --> 01:31:17
In classification, the goal is to predict
a categorical target from the input data.

17
01:31:17 --> 01:31:20
A categorical target is one
with discreet values or

18
01:31:20 --> 01:31:22
categories, instead of continuous values.

19
01:31:24 --> 01:31:27
For example, this diagram shows

20
01:31:27 --> 01:31:33
a classification task to determine the
risk associated with a loan application.

21
01:31:33 --> 01:31:36
The input consists of the loan amount,

22
01:31:36 --> 01:31:43
applicant information such as income,
age, debts, and a down payment.

23
01:31:45 --> 01:31:49
From this input data,
the task is to determine whether

24
01:31:49 --> 01:31:53
the loan application is low risk or
high risk.

25
01:31:56 --> 01:31:59
There are many classification techniques
or algorithms that can be used for

26
01:31:59 --> 01:32:01
this problem.

27
01:32:01 --> 01:32:06
We will discuss a specific one, namely,
decision tree in the next slide.

28
01:32:08 --> 01:32:12
The decision tree algorithm is
one technique for classification.

29
01:32:12 --> 01:32:14
With this technique,

30
01:32:14 --> 01:32:19
decisions to perform the classification
task are modeled as a tree structure.

31
01:32:21 --> 01:32:25
For the loan risk assessment problem,
a simple decision tree is shown here,

32
01:32:26 --> 01:32:31
where the loan application is
classified as being either low risk, or

33
01:32:31 --> 01:32:33
high risk, based on the loan amount.

34
01:32:33 --> 01:32:36
The applicant's income,
and the applicant's age.

35
01:32:39 --> 01:32:43
The decision tree algorithm is implemented
in many machine learning tools.

36
01:32:44 --> 01:32:50
This diagram shows how to specify
decision tree from input data, KNIME.

37
01:32:50 --> 01:32:54
A graphical user-interface-based
machine learning platform.

38
01:32:56 --> 01:33:01
Some examples of classification
are the prediction of whether cells from

39
01:33:01 --> 01:33:06
a tumor are benign or
malignant, categorization of

40
01:33:06 --> 01:33:11
handwritten digits as being zero,
one, two, etc, up to nine.

41
01:33:13 --> 01:33:17
And determining whether a credit card
transaction is legitimate or fraudulent,

42
01:33:19 --> 01:33:24
and classification of a loan application
as being low-risk, medium-risk or

43
01:33:24 --> 01:33:25
high-risk, as you've seen.

44
01:33:27 --> 01:33:30
Another common analytical
operation is cluster analysis.

45
01:33:32 --> 01:33:34
In cluster analysis, or clustering,

46
01:33:34 --> 01:33:39
the goal is to organize similar
items in to groups of association.

47
01:33:40 --> 01:33:46
This diagram shows an example of cluster
analysis in which customers are clustered

48
01:33:46 --> 01:33:50
into groups according to their
preferences of movie genre.

49
01:33:52 --> 01:33:55
So, customers who like Sci-Fi
movies are grouped together.

50
01:33:57 --> 01:34:00
Those who like drama movies
are grouped together,

51
01:34:00 --> 01:34:05
and customers who like horror
movies are grouped together.

52
01:34:05 --> 01:34:09
With this grouping, new movies,
as well as other products,

53
01:34:09 --> 01:34:14
such as books, can be offered to
the right type of costumers in order to

54
01:34:14 --> 01:34:17
generate interest and increase revenue.

55
01:34:19 --> 01:34:24
A simple and commonly used algorithm for
cluster analysis is k-means.

56
01:34:26 --> 01:34:30
With k-means,
samples are divided into k clusters.

57
01:34:30 --> 01:34:34
This clustering is done in order
to minimize the variance or

58
01:34:34 --> 01:34:37
similarity between samples
within the same cluster

59
01:34:37 --> 01:34:40
using some similarity
measures such as distance.

60
01:34:41 --> 01:34:46
In this example, k is equal to three, and

61
01:34:46 --> 01:34:52
k-means divides the original data shown
on the left into three clusters,

62
01:34:52 --> 01:34:56
shown as blue, green, and
red on the chart on the right.

63
01:34:58 --> 01:35:02
The k-means clustering algorithm is
implemented on many machine-learning

64
01:35:02 --> 01:35:03
platforms.

65
01:35:04 --> 01:35:07
The code here shows how to read in and

66
01:35:07 --> 01:35:12
parse input data, and
perform k-means clustering on the data.

67
01:35:12 --> 01:35:17
Other examples of cluster analysis
are grouping a company’s customer base

68
01:35:17 --> 01:35:23
into distinct segments for more effective
targeted marketing, finding articles or

69
01:35:23 --> 01:35:27
webpages with similar topics for
retrieving relevant information.

70
01:35:29 --> 01:35:34
Identification of areas in the city with
rates of particular types of crimes for

71
01:35:34 --> 01:35:39
effective management of law
enforcement resources, and

72
01:35:39 --> 01:35:45
determining different groups of weather
patterns such as rainy, cold or snowy.

73
01:35:47 --> 01:35:50
Classification and cluster analysis
are considered machine learning and

74
01:35:50 --> 01:35:52
analytical operations.

75
01:35:52 --> 01:35:55
There are also analytical
operations from graph analytics,

76
01:35:55 --> 01:36:00
which is the field of analytics where
the underlying data is structured as, or

77
01:36:00 --> 01:36:02
can be modeled as the set of graphs.

78
01:36:04 --> 01:36:08
One analytical operation using
graphs as path analysis,

79
01:36:08 --> 01:36:12
which analyzes sequences of nodes and
edges in a graph.

80
01:36:13 --> 01:36:16
A common application of path analysis

81
01:36:16 --> 01:36:20
is to find routes from one
location to another location.

82
01:36:20 --> 01:36:26
For example, you might want to find the
shortest path from your home to your work.

83
01:36:26 --> 01:36:31
This path may be different depending on
conditions such as the day of the week,

84
01:36:31 --> 01:36:35
time of day, traffic congestion,
weather and etc.

85
01:36:37 --> 01:36:42
This code shows some operations for
path analysis on neo4j,

86
01:36:42 --> 01:36:47
which is a graph database system
using a query language called Cypher.

87
01:36:48 --> 01:36:53
The first operation finds the shortest
path between specific nodes in a graph.

88
01:36:54 --> 01:36:58
The second operation finds all
the shortest paths in a graph.

89
01:36:59 --> 01:37:03
Connectivity analysis of graphs
has to do with finding and

90
01:37:03 --> 01:37:06
tracking groups to determine
interactions between entities.

91
01:37:08 --> 01:37:12
Entities in highly interacting
groups are more connected

92
01:37:12 --> 01:37:16
to each other than to entities
of other groups in a graph.

93
01:37:17 --> 01:37:20
These groups are called communities, and

94
01:37:20 --> 01:37:25
are interesting to analyze as they
give insights into the degree and

95
01:37:25 --> 01:37:30
patterns of the interaction between
entities, and also between communities.

96
01:37:31 --> 01:37:38
Some applications of connectivity analysis
are to extract conversation threads.

97
01:37:38 --> 01:37:41
For example,
by looking at tweets and retweets.

98
01:37:42 --> 01:37:46
To find interacting groups, for example,
to determine which users are interacting

99
01:37:46 --> 01:37:52
with each other users, to find
influencers, for example, to understand

100
01:37:52 --> 01:37:57
who are the main users leading to
the conversation about a particular topic.

101
01:37:57 --> 01:38:00
Or, who do people pay attention to?

102
01:38:00 --> 01:38:04
This information can be used to
identify the fewest number of

103
01:38:04 --> 01:38:05
people with the greatest influence.

104
01:38:05 --> 01:38:10
For example, for political campaigns,
or marketing on social media.

105
01:38:12 --> 01:38:14
This code shows some operations for

106
01:38:14 --> 01:38:19
connectivity analysis on neo4j using
the query language, Cypher, again.

107
01:38:20 --> 01:38:24
The first operation finds the degree
of all the nodes in a graph,

108
01:38:24 --> 01:38:29
and the second creates a histogram
of degrees for all nodes in a graph

109
01:38:29 --> 01:38:34
to determine how connected a node in
a graph is, we need to look at its degree.

110
01:38:34 --> 01:38:38
The degree of a node is the number
of edges connected to the node.

111
01:38:40 --> 01:38:44
A degree histogram shows the distribution
of node degrees in the graph and

112
01:38:44 --> 01:38:49
is useful in comparing graphs and
identifying types of users, for

113
01:38:49 --> 01:38:54
example, those who follow, versus those
who are followed in social networks.

114
01:38:55 --> 01:39:00
To summarize and add to these techniques,
the decision tree algorithm for

115
01:39:00 --> 01:39:05
classification and k-means algorithm for
cluster analysis that we covered in this

116
01:39:05 --> 01:39:08
lecture are techniques
from machine learning.

117
01:39:09 --> 01:39:14
Machine learning is a field of analytics
focused on the study and construction of

118
01:39:14 --> 01:39:19
computer systems that can learn from data
without being explicitly programmed.

119
01:39:21 --> 01:39:24
Our course on machine learning in
this specialization will cover these

120
01:39:24 --> 01:39:28
algorithms in more detail,
along with other algorithms used for

121
01:39:28 --> 01:39:30
classification and cluster analysis.

122
01:39:30 --> 01:39:33
As well as algorithms for
other machine learning tasks,

123
01:39:33 --> 01:39:38
such as regression,
association analysis, and tools for

124
01:39:38 --> 01:39:41
implementing and
executing machine learning algorithms.

125
01:39:43 --> 01:39:48
As a summary of the Graph Analytics,
the Path Analytics technique for finding

126
01:39:48 --> 01:39:52
the shortest path and the connectivity
analysis technique for analyzing communities

127
01:39:52 --> 01:39:57
that we discussed earlier,
are techniques used in graph analytics.

128
01:39:57 --> 01:40:01
As explained earlier,
graph analytics is the field of analytics,

129
01:40:01 --> 01:40:06
where the underlying data is structured or
can be modeled as a set of graphs.

130
01:40:07 --> 01:40:12
Our graph analytics course in the
specialization will cover these and other

131
01:40:12 --> 01:40:17
graph techniques, and we'll also cover
tools and platforms for graph analytics.

132
01:40:17 --> 01:40:24
In summary, analytic operations
are used to discover meaningful

133
01:40:24 --> 01:40:30
patterns in the data in order to provide
insights into the problem being studied.

134
01:40:30 --> 01:40:35
We looked at some of the examples of
analytical operations for classification,

135
01:40:35 --> 01:40:40
cluster analysis, path analysis and
connectivity analysis in this lecture.

1
03:10:40 --> 03:10:41
In this hands-on activity,

2
03:10:41 --> 03:10:44
we'll be performing word count on
the complete works of Shakespeare.

3
03:10:46 --> 03:10:50
First, we will copy the Shakespeare
text into the Hadoop file system.

4
03:10:50 --> 03:10:53
Next, we will create a new
Jupyter Notebook, and

5
03:10:53 --> 03:10:56
read the Shakespeare
text into a Spark RDD.

6
03:10:56 --> 03:11:00
We will then perform WordCount
using map and reduce,

7
03:11:00 --> 03:11:03
and write the results to HDFS and
view the contents.

8
03:11:06 --> 03:11:08
Let's begin.

9
03:11:08 --> 03:11:12
In the intro to big data course,
we copy the Shakespeare text into HDFS.

10
03:11:12 --> 03:11:15
Let's see if it's still there.

11
03:11:15 --> 03:11:17
If not, we can copy it now.

12
03:11:17 --> 03:11:21
Click on the terminal icon
at the top of the toolbar.

13
03:11:22 --> 03:11:28
Now we can run hadoop fs- ls to see what's
in our hadoop filesystem directory.

14
03:11:30 --> 03:11:33
There are no files in HTFS,
so let's copy it.

15
03:11:33 --> 03:11:40
If you already have words.txt in your HTFS
directory, you can skip this next step.

16
03:11:41 --> 03:11:45
Cd into downloads,

17
03:11:45 --> 03:11:54
big-data-3/spark-wordcount.

18
03:11:54 --> 03:11:56
We can do ls to see the file.

19
03:11:57 --> 03:12:00
Let's copy this file to HTFS.

20
03:12:00 --> 03:12:07
We run Hadoop,
fs copy from local, words.txt.

21
03:12:12 --> 03:12:16
We can write Hadoop fs -ls again
to verify that the file is there.

22
03:12:20 --> 03:12:21
Now let's do work count in spark.

23
03:12:23 --> 03:12:26
We will do this in an iPython
notebook using Jupyter server.

24
03:12:28 --> 03:12:30
Look on the web browser icon,
the top of the toolbar.

25
03:12:34 --> 03:12:38
And go to the Jupyter server URL,
which is local host port 8889.

26
03:12:41 --> 03:12:43
Next, let's create a new iPython notebook

27
03:12:49 --> 03:12:55
The first step is to read the words.txt
files in HTFS into a spark RDD.

28
03:12:55 --> 03:12:57
We'll call the RDD, lines.

29
03:13:01 --> 03:13:06
We can read it using the spark context SC,
in calling the text file method.

30
03:13:11 --> 03:13:16
The argument is the URL of
the word set TXT file and HDFS.

31
03:13:26 --> 03:13:30
Let's run this We can

32
03:13:30 --> 03:13:35
view the contents of this RDD
by calling lines.take(5).

33
03:13:40 --> 03:13:44
The argument 5 says how many
lines to show of the RDD.

34
03:13:45 --> 03:13:50
Next, we'll transform this RDD
of lines into an RDD of words.

35
03:13:50 --> 03:13:54
We'll say, words = lines.flatmap,

36
03:13:57 --> 03:13:59
lambda line:

37
03:14:03 --> 03:14:07
line.split Double quote
space double quote.

38
03:14:10 --> 03:14:17
This creates a new RDD called, words,
by running flatMap over the line RDD.

39
03:14:17 --> 03:14:20
The argument is this lambda expression.

40
03:14:22 --> 03:14:26
A lambda in Python is a simple way
to declare a one line expression.

41
03:14:27 --> 03:14:30
In this case,
there's one argument called line and

42
03:14:30 --> 03:14:34
we called it split method on this line and
we split on spaces.

43
03:14:36 --> 03:14:38
We can run this and
look at the contents of words.

44
03:14:48 --> 03:14:51
We can see that each element
now is an individual word.

45
03:14:53 --> 03:14:56
Next, we'll create tuples of these words.

46
03:14:57 --> 03:15:02
We'll put them in a new RDD called tuples.

47
03:15:02 --> 03:15:05
Enter, tuples

48
03:15:05 --> 03:15:10
= words.map:lambda

49
03:15:10 --> 03:15:15
word; (word, 1).

50
03:15:20 --> 03:15:24
This creates the tuples
by transforming words.

51
03:15:24 --> 03:15:26
This uses map and another lambda function.

52
03:15:26 --> 03:15:32
In this case, the lambda takes
one argument and returns a tuple.

53
03:15:32 --> 03:15:34
Where the first value of
the tuple is the word.

54
03:15:35 --> 03:15:37
The second value, is the number 1.

55
03:15:37 --> 03:15:43
Not that in this case, we use map,
whereas before, we used flat map.

56
03:15:45 --> 03:15:48
In this case, we want a tuple for
every word in the words.

57
03:15:48 --> 03:15:52
So we have a one to one mapping
between inputs and outputs.

58
03:15:52 --> 03:15:58
Previously, while we were splitting lines
into word, each line had multiple words.

59
03:16:00 --> 03:16:01
In general,

60
03:16:01 --> 03:16:05
you want to use map when you have a one to
one mapping between inputs and outputs.

61
03:16:05 --> 03:16:10
In flatMap you have a one to many or
none mapping between inputs and

62
03:16:10 --> 03:16:16
outputs Let's run this and look at tuples.

63
03:16:24 --> 03:16:29
We can see that each word now has
a tuple initialized with the count of 1.

64
03:16:29 --> 03:16:34
We can now count all the words by
combining or reducing these tuples.

65
03:16:34 --> 03:16:36
We'll put this in a new RDD called counts.

66
03:16:37 --> 03:16:42
So we'll say counts equals
tuples.reduce by key.

67
03:16:46 --> 03:16:50
Lambda a,b:.

68
03:16:50 --> 03:16:52
a + b.

69
03:16:55 --> 03:16:59
In this case, the lambda function
takes two arguments, a and be, and

70
03:16:59 --> 03:17:01
will return the result of adding a and b.

71
03:17:03 --> 03:17:05
To view the result,

72
03:17:12 --> 03:17:16
You can see now that the counts for
each words have been created.

73
03:17:16 --> 03:17:19
We can write this result back to HDFS.

74
03:17:19 --> 03:17:29
Let's say
counts.coalesce(1).saveAsTextFile,

75
03:17:29 --> 03:17:32
and then the URL.

76
03:17:43 --> 03:17:48
The coalesce means we only
want a single output file.

77
03:17:48 --> 03:17:51
Let's go back to our shell and
view the results.

78
03:17:54 --> 03:17:57
We'll run hadoop fs -ls
to see the directory.

79
03:18:00 --> 03:18:03
And run it again to look inside
the wordcount directory.

80
03:18:08 --> 03:18:11
And once more,
to look inside wordcount/outputDir.

81
03:18:17 --> 03:18:24
As you recall,
the output from hadoop jobs is part-0000.

82
03:18:24 --> 03:18:27
This is also true for spark jobs.

83
03:18:27 --> 03:18:30
Let's copy this file to
the local file system.

84
03:18:30 --> 03:18:34
We'll run hadoop fs CopyToLocal

85
03:18:34 --> 03:18:41
wordcount/outputDir/part-00000.

86
03:18:48 --> 03:18:50
You can view the results with more.

1
06:29:30 --> 06:29:31
In this hands on activity,

2
06:29:31 --> 06:29:34
we will be using Spark Streaming
to read weather data.

3
06:29:35 --> 06:29:38
First, we open
the Spark Streaming Jupyter Notebook.

4
06:29:39 --> 06:29:42
Next, we will look at sensor format and
measurement types.

5
06:29:43 --> 06:29:48
We'll then create a Spark DStream of
weather data, read the measurements, and

6
06:29:48 --> 06:29:49
create a sliding window of the data.

7
06:29:49 --> 06:29:55
We will define a function to display the
maximum and minimum values in the window.

8
06:29:55 --> 06:29:57
We start to stream processing
to give their results.

9
06:30:00 --> 06:30:04
Before we begin this activity, we need
to change the virtual box settings for

10
06:30:04 --> 06:30:06
our carder virtual machine.

11
06:30:08 --> 06:30:11
Start streaming needs more
than one thread of execution.

12
06:30:11 --> 06:30:14
So we need to change the settings to
add more than one virtual processor.

13
06:30:16 --> 06:30:21
First, shut down your cloudera virtual
machine and go to the virtual box manager.

14
06:30:23 --> 06:30:27
Select the cloudera virtual box and
click on settings.

15
06:30:29 --> 06:30:34
Next, click on system, click on Processor.

16
06:30:36 --> 06:30:43
And change the number of
CPU's to be two or more.

17
06:30:43 --> 06:30:48
When you're done, click okay,
and start the machine as usual.

18
06:30:52 --> 06:30:53
Let's begin.

19
06:30:53 --> 06:30:56
First, click on the browser icon
at the top of the tool bar.

20
06:30:58 --> 06:31:04
Navigate to the Jupyter Notebook server,
monitoring local host calling 8889.

21
06:31:04 --> 06:31:08
We'll then go in to downloads.

22
06:31:08 --> 06:31:12
Big data 3.

23
06:31:12 --> 06:31:15
Spark-streaming.

24
06:31:15 --> 06:31:17
Let's then open Spark-Streaming notebook.

25
06:31:20 --> 06:31:24
This first line, shows the example
data we get from the weather station.

26
06:31:25 --> 06:31:28
Each line has a time stamp and
a set of measurements.

27
06:31:30 --> 06:31:34
Each of these abbreviations is
a particular type of measurement,

28
06:31:34 --> 06:31:35
followed by the actual value.

29
06:31:38 --> 06:31:40
The next cell shows the key for
these measurements.

30
06:31:40 --> 06:31:45
For this hands-on, we are interested
in the average wind direction.

31
06:31:45 --> 06:31:47
Which is abbreviated as DM.

32
06:31:49 --> 06:31:53
This next cell, defines a function
that parses each line of text and

33
06:31:53 --> 06:31:55
pulls out the average wind speed.

34
06:31:56 --> 06:31:58
We define it here, so
we don't have to type it in later.

35
06:32:00 --> 06:32:01
Let's run this cell.

36
06:32:04 --> 06:32:07
Next, let's create
a streaming spark context.

37
06:32:07 --> 06:32:09
First, we'll need to import the module.

38
06:32:09 --> 06:32:15
We'll enter from pyspark.streaming
import StreamingContext.

39
06:32:15 --> 06:32:20
We can create a new streaming context.

40
06:32:20 --> 06:32:21
We'll put in in a variable called ssc.

41
06:32:23 --> 06:32:28
We'll enter ssc = StreamingContext(sc,1).

42
06:32:28 --> 06:32:31
The SC is a StreamingContext.

43
06:32:31 --> 06:32:36
The 1 specifies the batch interval,
1 second in this case.

44
06:32:36 --> 06:32:36
Let's run this.

45
06:32:39 --> 06:32:40
Next, we'll create a dstream.

46
06:32:42 --> 06:32:46
We'll import the streaming weather data,
over a TCP connection.

47
06:32:46 --> 06:32:48
We'll put this in a dstream called, Lines.

48
06:32:50 --> 06:32:55
Let's say lines = ssc.socketTextStream,

49
06:32:55 --> 06:33:00
we'll enter the host name in
port of the weather station,

50
06:33:00 --> 06:33:04
rtd.hpwren.ucsd.edu for 12028.

51
06:33:04 --> 06:33:05
Let's run this.

52
06:33:09 --> 06:33:15
Next, we'll create a new d-stream called
vals that would hold the measurements.

53
06:33:15 --> 06:33:21
We'll say vals = lines.flatMap parse.

54
06:33:21 --> 06:33:24
This calls the parse function,
we defined above for

55
06:33:24 --> 06:33:26
each of the lines coming
from the weather station.

56
06:33:26 --> 06:33:30
The resulting D-Stream will have just
the average wind direction values.

57
06:33:31 --> 06:33:32
We'll run this.

58
06:33:36 --> 06:33:40
Next, we'll create a window that
will aggregate the D-Stream values.

59
06:33:42 --> 06:33:46
We'll say, window = vals.window(10,5).

60
06:33:46 --> 06:33:51
The first argument specifies that the
length of the window should be 10 seconds.

61
06:33:51 --> 06:33:56
The second argument specifies that
the window should move every 5 seconds.

62
06:33:56 --> 06:33:58
Let's run this.

63
06:34:00 --> 06:34:03
Next, we'll define a function
that prints the minimum and

64
06:34:03 --> 06:34:05
maximum values that we see.

65
06:34:05 --> 06:34:06
We'll start by entering the definition.

66
06:34:08 --> 06:34:13
Def stats,
this will take an rdd as an argument.

67
06:34:16 --> 06:34:19
Next, let's print the entire
contents of the rdd.

68
06:34:19 --> 06:34:23
Print, parenthesis rdd.collect,

69
06:34:23 --> 06:34:28
this'll print the entire
content of the rdd.

70
06:34:28 --> 06:34:30
In a real big data application,

71
06:34:30 --> 06:34:33
this will be impractical due
to the size of the data.

72
06:34:34 --> 06:34:36
However, for this hands on,
the rdd is small, and so

73
06:34:36 --> 06:34:39
we can use this to see
the contents of the rdd.

74
06:34:41 --> 06:34:43
Next, we'll print the min and max.

75
06:34:44 --> 06:34:46
Before we do that however,

76
06:34:46 --> 06:34:50
we should check to make sure that
the size of the rdd is greater than zero.

77
06:34:50 --> 06:34:53
We'll check that rdd.count
is greater than 0.

78
06:34:58 --> 06:35:03
Finally, we'll print the MinID, MAX.

79
06:35:03 --> 06:35:08
We'll enter print (“max = {} min =

80
06:35:08 --> 06:35:13
{}”) Outside of the quote we'll do

81
06:35:13 --> 06:35:19
.format(rdd.max,rdd.min())).

82
06:35:19 --> 06:35:23
Let's run this, next,

83
06:35:23 --> 06:35:28
let's call this function stats.

84
06:35:28 --> 06:35:30
So all the rdds in our sliding window.

85
06:35:30 --> 06:35:37
I'll enter window.foreachRDD(stats).

86
06:35:37 --> 06:35:42
Run this.

87
06:35:42 --> 06:35:45
We're now ready to start
our streaming processing.

88
06:35:45 --> 06:35:48
We can do this by entering ssc.start.

89
06:35:48 --> 06:35:50
We'll run this to start the streaming.

90
06:35:58 --> 06:36:01
When we want to stop this streaming,
we'll run ssc.stop

91
06:36:07 --> 06:36:09
Please scroll up and
look at the beginning of the output.

92
06:36:12 --> 06:36:16
We'll see that it's printing the full
window and the min and max values.

93
06:36:19 --> 06:36:21
Notice that in the beginning,
the window is not yet filled.

94
06:36:21 --> 06:36:23
In this case, there's only three entries.

95
06:36:24 --> 06:36:28
We count to see that the window
is moving by five measurements.

96
06:36:29 --> 06:36:32
For example, the last five
measurements in the second window,

97
06:36:33 --> 06:36:35
are the first five measurements
in the third window.

1
13:06:04 --> 13:06:08
In this hands on activity we
will be using SparkSQL to

2
13:06:08 --> 13:06:10
query data from an SQL database.

3
13:06:11 --> 13:06:15
First we will open
the SparkSQL Jupyter Notebook.

4
13:06:15 --> 13:06:18
We will connect Spark to a Postgres table.

5
13:06:19 --> 13:06:24
And then view the Spark DataFrame
schema and count the rows.

6
13:06:24 --> 13:06:26
We will view the contents
of the data frame.

7
13:06:27 --> 13:06:29
See how to filter rows and columns.

8
13:06:30 --> 13:06:33
And finally perform aggregate
operation on a column.

9
13:06:37 --> 13:06:38
Let's begin.

10
13:06:38 --> 13:06:42
First, click on the browser icon,
the top of the toolbar.

11
13:06:42 --> 13:06:45
>> [SOUND]
>> Next,

12
13:06:45 --> 13:06:49
navigate to the Jupyter Notebook server.

13
13:06:49 --> 13:06:52
It's localhost:8889.

14
13:06:55 --> 13:06:58
Go to Downloads,

15
13:06:58 --> 13:07:02
Big Data 3, Spark SQL.

16
13:07:04 --> 13:07:06
To open the SparkSQL Notebook.

17
13:07:06 --> 13:07:11
The first three cells have
already been entered for you.

18
13:07:13 --> 13:07:20
First, we import the SQLContext, run this.

19
13:07:20 --> 13:07:28
Next, we create an SQLContext
from the SparkContext run this.

20
13:07:28 --> 13:07:33
And next, we'll create a Spark DataFrame
from a Postgres table.

21
13:07:36 --> 13:07:38
We used the read attribute format.

22
13:07:40 --> 13:07:44
The jdbc argument means that we're
using a Java database connection.

23
13:07:46 --> 13:07:48
The next line sets the URL option.

24
13:07:48 --> 13:07:53
It says we're using Postgres
database running on the local host.

25
13:07:53 --> 13:07:58
The database name is Cloudera and
the username is Cloudera.

26
13:07:58 --> 13:08:00
The second option, DB table,

27
13:08:00 --> 13:08:04
says we want our data frame
to be the game clicks table.

28
13:08:04 --> 13:08:05
And finally we call load.

29
13:08:07 --> 13:08:08
Let's execute this.

30
13:08:10 --> 13:08:15
You can see the schema of the data
frame by calling df.printschema.

31
13:08:18 --> 13:08:21
This shows the name of each
column along with the data type.

32
13:08:24 --> 13:08:28
We can count the rows in this
df frame by calling df.count.

33
13:08:35 --> 13:08:39
We can look at the first five
rows by calling df.show(5)

34
13:08:43 --> 13:08:45
This shows all the columns
in the data frame.

35
13:08:47 --> 13:08:50
We can select specific columns
by using the select method.

36
13:08:52 --> 13:08:56
Let's select just the User ID and
Team Level columns.

37
13:08:56 --> 13:09:01
I'll enter df.select("userid",teamlevel.

38
13:09:01 --> 13:09:02
Parenthesis.

39
13:09:05 --> 13:09:08
And finally we only want to
see the top five rows.

40
13:09:08 --> 13:09:10
So we'll do .show(5).

41
13:09:14 --> 13:09:19
We can also select rows
that have a specific value.

42
13:09:19 --> 13:09:23
Let's look for the rows where
the team level is greater than one.

43
13:09:23 --> 13:09:27
We'll enter df.filter.

44
13:09:27 --> 13:09:31
We'll specify that we want team level
greater than one by entering df,

45
13:09:31 --> 13:09:34
square bracket, team level,
greater than one.

46
13:09:38 --> 13:09:42
And again, we only want the user ID and
team level columns.

47
13:09:47 --> 13:09:49
And finally only the first five rows.

48
13:09:54 --> 13:09:59
We can use the group by method to
aggregate a particular column.

49
13:09:59 --> 13:10:03
For example, the ishit column
has a value of zero or one.

50
13:10:03 --> 13:10:08
And we can use group by to count how
many times each of these values occurs.

51
13:10:08 --> 13:10:16
Or a df.groupby ishit, and
we'll call count to count the values

52
13:10:25 --> 13:10:29
We can also perform aggregate statistical
operations on the data in a data frame.

53
13:10:31 --> 13:10:35
Let's compute the mean and
sum values for ishit.

54
13:10:35 --> 13:10:40
First we need to import
the statistical functions we'll

55
13:10:40 --> 13:10:45
run from.pyspark.sql.functions
import star.

56
13:10:45 --> 13:10:51
Next we'll run df.select
(mean) ishit,sum ishit

57
13:10:59 --> 13:11:02
We can also join two data
frames on a particular column.

58
13:11:03 --> 13:11:08
Let's join the existing data frame of the
game clicks table with the adclicks table.

59
13:11:09 --> 13:11:12
First, we need to create data frame for
the adclicks table.

60
13:11:13 --> 13:11:14
Let's go back up.

61
13:11:19 --> 13:11:20
Copy the content of this cell,

62
13:11:27 --> 13:11:30
Paste it.

63
13:11:30 --> 13:11:35
We put the adclicks table
the data frame called df2.

64
13:11:35 --> 13:11:38
And we'll change the db table
option to the adclicks.

65
13:11:42 --> 13:11:43
Run it.

66
13:11:45 --> 13:11:48
Let's print the schema of df2.

67
13:11:56 --> 13:12:00
You can see that it also has
a column called user id.

68
13:12:00 --> 13:12:04
So let's join the game clicks
data frame with the add

69
13:12:04 --> 13:12:06
clicks data frame on this column.

70
13:12:06 --> 13:12:10
We put the result in a new
data frame called merged.

71
13:12:10 --> 13:12:19
We'll say merge = df.join.df2 "userid".

72
13:12:19 --> 13:12:19
We'll run it.

73
13:12:21 --> 13:12:23
Let's look at the schema.

74
13:12:23 --> 13:12:25
We'll call merge.printschema.

75
13:12:29 --> 13:12:33
We can see that this merged data frame
has the column for both game clicks and

76
13:12:33 --> 13:12:34
adclicks.

77
13:12:35 --> 13:12:39
Finally we'll look at the top five
rows in this merged data frame.

78
13:12:39 --> 13:12:42
We'll run merge.show.

1
02:18:48 --> 02:18:51
We have now seen some
simple transformations and

2
02:18:51 --> 02:18:56
how Spark can create RDDs from
each other using transformations.

3
02:18:56 --> 02:19:01
We learned that transformations are
evaluated after an action is performed.

4
02:19:02 --> 02:19:07
So we can simply define actions as RDD
operations that trigger the evaluation of

5
02:19:07 --> 02:19:13
the transformation pipeline and return
the final result to the driver program or

6
02:19:13 --> 02:19:15
save the results to a persistent storage.

7
02:19:17 --> 02:19:22
We can also call them the last
step in a Spark pipeline.

8
02:19:22 --> 02:19:24
Let's now look at a few action operations.

9
02:19:26 --> 02:19:32
After this video, you will be able to
explain the steps of a Spark pipeline

10
02:19:32 --> 02:19:38
ending with a collect action and list
four common action operations in Spark.

11
02:19:40 --> 02:19:43
A very common action in Spark is collect.

12
02:19:45 --> 02:19:49
In this example, we can imagine that
initially we are reading from HDFS.

13
02:19:51 --> 02:19:56
The RDD partitions that go through
the transformation steps in our big data

14
02:19:56 --> 02:20:00
pipeline are defined as flatMap and
groupbyKey.

15
02:20:02 --> 02:20:07
When the final step is done,
the collect action is called and

16
02:20:07 --> 02:20:11
Spark sends all the tasks for
execution to the worker notes.

17
02:20:14 --> 02:20:18
Collect will send all the resulting
RDDs from the workers and

18
02:20:18 --> 02:20:23
copy them to the Java virtual
machine on the driver program.

19
02:20:23 --> 02:20:27
And then, this will be piped
also to our Python shell.

20
02:20:29 --> 02:20:34
While collect copies all the data,
another action, take,

21
02:20:34 --> 02:20:37
copies the first n results of the driver.

22
02:20:39 --> 02:20:43
If the results are too large
to fit in the driver memory,

23
02:20:43 --> 02:20:47
then there's an opportunity to write
them directly to HDFS instead.

24
02:20:49 --> 02:20:53
Among many other actions,
reduce is probably the most famous one.

25
02:20:54 --> 02:20:59
Reduce takes two elements and
returns a result, like sum.

26
02:20:59 --> 02:21:05
But in this case, we don't have a key,
we just have a large area of some values.

27
02:21:05 --> 02:21:08
And we are running this function over and

28
02:21:08 --> 02:21:12
over again to reduce everything
to one single value.

29
02:21:12 --> 02:21:15
For example,
to the global sum of everything.

30
02:21:16 --> 02:21:20
Another very useful action Is saveAsText,

31
02:21:20 --> 02:21:24
to save the results to local disk or
HDFS, and

32
02:21:24 --> 02:21:30
this is very useful if the output of
the power computation is pretty large.

1
04:40:18 --> 04:40:22
Hello, I hope you enjoyed your first
programming experience with Spark.

2
04:40:23 --> 04:40:26
Although the words count
example is simple,

3
04:40:26 --> 04:40:30
it is useful in starting to
understand how to work with RDDs.

4
04:40:32 --> 04:40:38
After this video, you'll be able to use
two methods to create RDDs in Spark,

5
04:40:38 --> 04:40:44
explain what immutable means,
interpret a Spark program as a pipeline

6
04:40:44 --> 04:40:49
of transformations and actions, and
list the steps to create a Spark program.

7
04:40:52 --> 04:40:53
So let's remember where we are.

8
04:40:55 --> 04:40:59
We have a Driver Program that
defines the Spark context.

9
04:41:00 --> 04:41:04
This is the entry point
to your application.

10
04:41:04 --> 04:41:09
The driver converts all the data to RDDs,
and

11
04:41:09 --> 04:41:15
everything from this point on
gets managed using the RDDs.

12
04:41:15 --> 04:41:19
RDDs can be constructed from files or
any other storage.

13
04:41:20 --> 04:41:24
They can also be constructed from
data structures for collections and

14
04:41:24 --> 04:41:26
programs, like lists.

15
04:41:27 --> 04:41:34
All the transformations and actions on
these RDDs take place either locally,

16
04:41:34 --> 04:41:38
or on the Worker Nodes
managed by a Cluster Manager.

17
04:41:41 --> 04:41:45
Each transformation results in
a new updated version of the RDD.

18
04:41:45 --> 04:41:49
The RDDs at the end get converted and

19
04:41:49 --> 04:41:53
saved in a persistent storage like HDFS or
your local drive.

20
04:41:56 --> 04:42:02
As we mentioned before,
RDDs get created in the Driver Program.

21
04:42:02 --> 04:42:04
The developer of the Driver Program,

22
04:42:04 --> 04:42:08
who in this case is you,
is responsible for creating them.

23
04:42:11 --> 04:42:14
You can just read in a file
through your Spark Context, or

24
04:42:14 --> 04:42:20
as we have in this example,
you can provide an existing collection,

25
04:42:20 --> 04:42:23
like a list to be turned into
a distributed collection.

26
04:42:26 --> 04:42:31
You can also create an integer
RDD using parallelize,

27
04:42:32 --> 04:42:34
and provide a number of partitions for

28
04:42:34 --> 04:42:39
distribution as we do create
the numbers RDD in this line.

29
04:42:41 --> 04:42:45
Here, the range function in Python

30
04:42:46 --> 04:42:49
will give us a list of
numbers starting from 0 to 9.

31
04:42:49 --> 04:42:56
The parallelize function
will create three partitions

32
04:42:56 --> 04:43:01
of the RDD to be distributed, based on
the parameter that was provided to it.

33
04:43:02 --> 04:43:07
Spark will decide how to assign partitions
to our executors and worker nodes.

34
04:43:09 --> 04:43:14
The distributed RDDs can in the end be
gathered into a single partition on

35
04:43:14 --> 04:43:17
the driver using
the collect transformation.

36
04:43:23 --> 04:43:28
Now let's think of a scenario were we
start processing the created RDDs.

37
04:43:30 --> 04:43:35
There are two types of operations
that help with processing in Spark,

38
04:43:35 --> 04:43:38
namely Transformations and Actions.

39
04:43:40 --> 04:43:45
All partitions written in RDD,
go through the same transformation in

40
04:43:45 --> 04:43:50
the worker node, executors when
a transformation is applied to an RDD.

41
04:43:52 --> 04:43:56
Spark uses lazy evaluation for
transformations.

42
04:43:57 --> 04:44:01
That means they will not be
immediately executed, but

43
04:44:01 --> 04:44:03
instead wait for
an action to be performed.

44
04:44:05 --> 04:44:09
The transformations get computed
when an action is executed.

45
04:44:09 --> 04:44:12
For this reason,
a lot of times you will see run

46
04:44:12 --> 04:44:17
time errors showing up at the action stage
and not at the transformation stages.

47
04:44:18 --> 04:44:20
It is very similar to Haskell or Erlang,

48
04:44:20 --> 04:44:23
if any of you are familiar
with these languages.

49
04:44:25 --> 04:44:28
Let's put some names on
these transformations.

50
04:44:28 --> 04:44:34
We can have a pipeline by converting a
text file into an RDD with two partitions.

51
04:44:35 --> 04:44:41
Filter some values out of it, and
maybe apply a map function to it.

52
04:44:41 --> 04:44:46
In the end, the run,
the collect action on the mapped RDDs

53
04:44:46 --> 04:44:50
to evaluate the results of the pipeline
and convert the outputs into results.

54
04:44:51 --> 04:44:57
Here, filter and map are transformations,
and collect is the action.

55
04:44:59 --> 04:45:03
Although the RDDs are in memory,
and they are not persistent,

56
04:45:03 --> 04:45:07
we can use the cash function
to make them persistent cash.

57
04:45:09 --> 04:45:14
For example, in order to reuse the RDD
created from a database query that could

58
04:45:14 --> 04:45:19
otherwise be costly to re-execute,
we can instead cache these RDDs.

59
04:45:22 --> 04:45:25
We need to use caution when
using the cache option,

60
04:45:25 --> 04:45:30
as it can consume too much memory and
generate a bottleneck itself.

61
04:45:33 --> 04:45:41
As a part of the Word Count example, we
mapped the words RDD to generate tuples.

62
04:45:41 --> 04:45:45
We then applied reduceByKey
to tuples to generate counts.

63
04:45:46 --> 04:45:50
In the end, we convert the number
of partitions to one so

64
04:45:50 --> 04:45:54
that output is one file
when written to this later.

65
04:45:54 --> 04:45:57
Otherwise, output will be spread
over multiple files on disk.

66
04:45:59 --> 04:46:05
Finally, saveAsTextFile is an action
that kickstarts the computation and

67
04:46:05 --> 04:46:06
writes to disk.

68
04:46:08 --> 04:46:11
To summarize, in a typical Spark program

69
04:46:11 --> 04:46:16
we create RDDs from external storage or
local collections like lists.

70
04:46:17 --> 04:46:21
Then we apply transformations
to these RDDs,

71
04:46:21 --> 04:46:25
like filter, map, and reduceByKey.

72
04:46:25 --> 04:46:30
These transformations get lazily
evaluated until an action is performed.

73
04:46:31 --> 04:46:38
Actions are performed both for local and
parallel computation to generate results.

74
04:46:38 --> 04:46:42
Next, we will talk more about
transformation and actions in Spark.

1
09:26:59 --> 09:27:03
In the last video,
we talked about the programming model for

2
09:27:03 --> 09:27:08
Spark where RDD's get generated from
external datasets and gets partitioned.

3
09:27:10 --> 09:27:16
We said, RDDs are immutable meaning they
can't be changed in place even partially.

4
09:27:17 --> 09:27:21
They need a transformation
operation applied to them and

5
09:27:21 --> 09:27:23
get converted into a new RDD.

6
09:27:25 --> 09:27:30
This is essential for keeping track
of all the processing that has been

7
09:27:30 --> 09:27:36
applied to our dataset providing the
ability to keep a linear chain of RDDs.

8
09:27:36 --> 09:27:42
In addition, as a part of a big data
pipeline, we start with an RDD.

9
09:27:42 --> 09:27:46
And through several transformation steps,
many other RDDs as

10
09:27:46 --> 09:27:51
intermediate products get executed
until we get to our final result.

11
09:27:53 --> 09:27:56
We also mention that
an important feature of Spark

12
09:27:56 --> 09:28:00
is that all these transformation are lazy.

13
09:28:01 --> 09:28:05
This means they don't execute
immediately when applied to an RDD.

14
09:28:07 --> 09:28:11
So when we apply a transformation,
nothing happens right away.

15
09:28:11 --> 09:28:15
We are basically preparing our big
data pipeline to be executed later.

16
09:28:16 --> 09:28:21
When we are done defining all
the transformations and perform an action,

17
09:28:21 --> 09:28:26
Spark will take care of finding the best
way to execute this computation and

18
09:28:26 --> 09:28:30
then start all the necessary
tasks in our worker nodes.

19
09:28:30 --> 09:28:35
In this video, we will explain some
common transformation in Spark.

20
09:28:36 --> 09:28:41
After this video, you will be able
to explain the difference between

21
09:28:41 --> 09:28:45
a narrow transformation and
wide transformation.

22
09:28:45 --> 09:28:49
Describe map, flatmap,
filter and coalesce as narrow

23
09:28:49 --> 09:28:54
transformations and
list two wide transformations.

24
09:28:57 --> 09:29:02
Let's take at look at, probably the
simplest transformation, which is a map.

25
09:29:04 --> 09:29:08
By now,
you're well versed in home networks.

26
09:29:08 --> 09:29:14
It applies the function to each
partition or element of an RDD.

27
09:29:14 --> 09:29:17
This is a one to one transformation.

28
09:29:17 --> 09:29:22
It is also in the category of element-wise
transformations since it transforms

29
09:29:22 --> 09:29:24
every element of an RDD separately.

30
09:29:28 --> 09:29:32
The code example in the blue
box here applies a function

31
09:29:32 --> 09:29:36
called lower to all
the elements in a text_RDD.

32
09:29:37 --> 09:29:38
The lower function

33
09:29:40 --> 09:29:43
turns all the characters in
a line to lower case letters.

34
09:29:44 --> 09:29:49
So the input is one line
of text with any kind of

35
09:29:49 --> 09:29:54
capitalization and the outfit is going
to be the same line, all lower case.

36
09:29:56 --> 09:30:01
In this example, we have two worker
nodes drawn as orange boxes.

37
09:30:02 --> 09:30:06
The black boxes are partitions
of our dataset.

38
09:30:06 --> 09:30:09
We work by partition and not by element.

39
09:30:09 --> 09:30:15
As you would remember this, it is the
difference between Spark and MapReduce.

40
09:30:17 --> 09:30:22
The partition is just a chunk of our data
with some number of elements in it and

41
09:30:22 --> 09:30:26
the map function gets applied to all
elements in that partition in each

42
09:30:26 --> 09:30:28
worker node locally.

43
09:30:29 --> 09:30:33
Each node applies the map
function to the data or

44
09:30:33 --> 09:30:37
RDD partition they received independently.

45
09:30:37 --> 09:30:41
Let's look at a few more in
element-wise transformation category.

46
09:30:44 --> 09:30:47
FlatMap is very similar to map.

47
09:30:48 --> 09:30:53
However, instead of returning
an individual element for each map,

48
09:30:53 --> 09:30:59
it returns an RDD with an aggregate of
all the results for all the elements.

49
09:30:59 --> 09:31:01
In the example in the blue box,

50
09:31:01 --> 09:31:06
the split_words fuction
takes a line as an input,

51
09:31:06 --> 09:31:11
which is one element and it's output
is each word as a single element.

52
09:31:11 --> 09:31:16
So, it splits a line to words.

53
09:31:17 --> 09:31:19
The same thing gets done for each line.

54
09:31:21 --> 09:31:24
When the output for
all the lines is flattened,

55
09:31:24 --> 09:31:27
we get a simple
one-dimensional list of words.

56
09:31:29 --> 09:31:34
So, we'll get all the words in
all the lines in just one list.

57
09:31:36 --> 09:31:41
Depending on the line length the output
partitions might be of different sizes.

58
09:31:41 --> 09:31:45
Detected here by the height of
each black box being different.

59
09:31:47 --> 09:31:52
In Spark terms, map and
flatMap are narrow transformations.

60
09:31:54 --> 09:31:59
Narrow transformation refers to
the processing where the processing

61
09:31:59 --> 09:32:04
logic depends only on data that is
already residing in the partition and

62
09:32:04 --> 09:32:07
data shuffling is not necessary.

63
09:32:10 --> 09:32:14
Another very important
transformation is filter.

64
09:32:14 --> 09:32:19
Often, we're interested just
in a subset of our data or

65
09:32:19 --> 09:32:23
we want to get rid of bad data.

66
09:32:23 --> 09:32:28
Filter transformation takes the function
take executes on each element

67
09:32:28 --> 09:32:29
of a RDD partition and

68
09:32:29 --> 09:32:34
returns only the elements that
the transformation element returns true.

69
09:32:37 --> 09:32:42
The example code in the blue box here,
applies a filter

70
09:32:42 --> 09:32:48
function that filters out words
that start with the letter a.

71
09:32:48 --> 09:32:52
The function starts with a,
takes the input word,

72
09:32:52 --> 09:32:58
then transforms it to lowercase and
then checks if the word starts with a.

73
09:33:00 --> 09:33:06
So, the output of this operation will be
a list with only words that start with a.

74
09:33:08 --> 09:33:10
This is another narrow transformation.

75
09:33:12 --> 09:33:15
So, it only gets executed locally

76
09:33:15 --> 09:33:20
without the need to shuffle any RDD
partitions across the word kernels.

77
09:33:22 --> 09:33:28
The output of filter depends on
the input and the filter functions.

78
09:33:28 --> 09:33:29
In some cases,

79
09:33:29 --> 09:33:35
even if you started with even RDD
partitions within the worker nodes,

80
09:33:35 --> 09:33:41
the RDD size can significantly vary across
the workers after a filter operation,

81
09:33:41 --> 09:33:46
then this happens is a pretty good
idea to join some of those partitions

82
09:33:46 --> 09:33:51
to increase performance and
even out processing across clusters.

83
09:33:51 --> 09:33:58
This transformation is called coalesce.

84
09:33:58 --> 09:34:04
Coalesce simply helps with balancing
the data partition numbers and sizes.

85
09:34:05 --> 09:34:10
When you have significantly reduced
your initial data after some filters and

86
09:34:10 --> 09:34:11
other transformations,

87
09:34:11 --> 09:34:16
having a large number of partitions
might not be very useful anymore.

88
09:34:16 --> 09:34:17
In this case,

89
09:34:17 --> 09:34:21
you can use coalesce to reduce the number
of partitions to a more manageable number.

90
09:34:24 --> 09:34:29
Until now, we talked about narrow
transformations that happen in a worker

91
09:34:29 --> 09:34:34
node locally without having to
transfer data through the network.

92
09:34:34 --> 09:34:38
Now, let's start talking
about wide transformations.

93
09:34:41 --> 09:34:43
Let's remember our Word Count example.

94
09:34:43 --> 09:34:50
As a part of the Word Count example, we
map the words RDD could generate tuples.

95
09:34:50 --> 09:34:56
The output of map is a key value pair
list where the key is the word and

96
09:34:56 --> 09:34:57
the value is always one.

97
09:34:59 --> 09:35:05
We then apply it reduceByKey
to tuples to generate counts,

98
09:35:05 --> 09:35:08
which simply sums the values for
each key or word.

99
09:35:09 --> 09:35:16
Let's imagine for a second that we use
groupByKey instead of reduceByKey.

100
09:35:16 --> 09:35:21
We will come back to reduceByKey
in just a little bit.

101
09:35:21 --> 09:35:24
Remember, mapped outputs tuples,

102
09:35:24 --> 09:35:29
which is a list of key value
pairs in the forms of word one.

103
09:35:31 --> 09:35:37
At each worker node, we will have
tuples that have the same word as key.

104
09:35:37 --> 09:35:43
In this example, apple as the key and
1 as the count and 2 worker nodes.

105
09:35:45 --> 09:35:50
Trying to group together, all the counts
of a word across worker nodes

106
09:35:50 --> 09:35:53
requires shuffling of
data between these nodes.

107
09:35:54 --> 09:35:58
Just like we do for the word apple here.

108
09:35:58 --> 09:36:03
GroupByKey is the transformation
that helps us combine values with

109
09:36:03 --> 09:36:09
the same key into a list without applying
a special user define function to it.

110
09:36:09 --> 09:36:15
As you see on the right, the result
of a groupByKey transformation on all

111
09:36:15 --> 09:36:21
the map outputs by the word apple is
the key ends up in a list with all ones.

112
09:36:24 --> 09:36:30
If you instead apply the function to
list like summing up all the values,

113
09:36:30 --> 09:36:33
then we could have had
the word count results.

114
09:36:33 --> 09:36:34
In this case, 2.

115
09:36:34 --> 09:36:41
If we instead applied a function to
the list like summing up all the values,

116
09:36:41 --> 09:36:45
then we would have had
the word count results.

117
09:36:45 --> 09:36:49
If we need to apply such functions to
a group of values related to a key like

118
09:36:49 --> 09:36:52
this, we use the reduceByKey operation.

119
09:36:54 --> 09:36:59
ReduceByKey helps us to combine
the value using a reduce function,

120
09:36:59 --> 09:37:02
which in the word count
case is a simple summation.

121
09:37:04 --> 09:37:08
In groupByKey and
reduceByKey transformations,

122
09:37:09 --> 09:37:13
we observe the behavior that require
shuffling of the data across work nodes,

123
09:37:15 --> 09:37:18
we call such transformations
wide transformations.

124
09:37:20 --> 09:37:25
In wide transformation operations,
processing depends on data

125
09:37:25 --> 09:37:30
residing in multiple partitions
distributed across worker nodes and this

126
09:37:30 --> 09:37:35
requires data shuffling over the network
to bring related datasets together.

127
09:37:37 --> 09:37:42
As a summary, we have listed a small
number of transformations in Spark with

128
09:37:42 --> 09:37:47
some examples and distinguished between
them as narrow and wide transformations.

129
09:37:48 --> 09:37:53
Although this is a good start,
I advise you to go through the list

130
09:37:53 --> 09:37:59
provided at the link shown here after
you complete this beginner course.

131
09:37:59 --> 09:38:04
Read about the rest of the transformations
in Spark before you start programming in

132
09:38:04 --> 09:38:07
Spark and have fun with transformations.

1
19:05:07 --> 19:05:10
Lastly, we will introduce
you to Spark GraphX.

2
19:05:12 --> 19:05:18
After this video,
you will be able to describe GraphX is,

3
19:05:18 --> 19:05:22
explain how vertices and
edges are stored in GraphX, and

4
19:05:22 --> 19:05:26
describe how Pregel works at a high level.

5
19:05:29 --> 19:05:34
GraphX is Apache Spark's Application
Programming Interface for

6
19:05:34 --> 19:05:38
graphs and graph-parallel computation.

7
19:05:38 --> 19:05:42
GraphX uses a property graph model.

8
19:05:42 --> 19:05:45
This means, both nodes.

9
19:05:45 --> 19:05:49
And edges in a graph can
have attributes and values.

10
19:05:51 --> 19:05:57
In GraphX, the node properties
are stored in a vertex table and

11
19:05:57 --> 19:06:02
edge properties are stored
in an edge table.

12
19:06:03 --> 19:06:09
The connectivity information, that is,
which edge connects which nodes,

13
19:06:09 --> 19:06:13
is stored separately from the node and
edge properties.

14
19:06:16 --> 19:06:21
GraphX is built on special RDDs for
vertices and edges.

15
19:06:22 --> 19:06:28
VertexRDD represents a set of vertices,

16
19:06:28 --> 19:06:33
all of which have an attribute called A.

17
19:06:33 --> 19:06:38
The EdgeRDD here extends
this basic edge storing by

18
19:06:38 --> 19:06:45
the edges in columnar format on
each partition for performance.

19
19:06:45 --> 19:06:51
Note that VertexID are defined
to be unique by design.

20
19:06:52 --> 19:06:57
The edge class is an object
with a source vertex and

21
19:06:57 --> 19:07:01
destination vertex and an edge attribute.

22
19:07:03 --> 19:07:09
In addition to the vortex and
edge views of the property graph,

23
19:07:09 --> 19:07:12
GraphX also has triplet view.

24
19:07:12 --> 19:07:19
The triplet view logically joins
vortex and edge properties.

25
19:07:22 --> 19:07:26
GraphX has an operator that
can execute operations

26
19:07:26 --> 19:07:29
from the Pregel library for
graph analytics.

27
19:07:31 --> 19:07:37
This Pregel operator executes
in a series of super steps

28
19:07:37 --> 19:07:40
which defines a messaging protocol for
vertices.

29
19:07:42 --> 19:07:44
We will revisit graph analytics and

30
19:07:44 --> 19:07:48
using GraphX in more detail in
course five of the specialization.

31
19:07:50 --> 19:07:56
In summary, Spark can be used for
graph parallel computations.

32
19:07:57 --> 19:08:03
GraphX uses special RDDs for
storing vertex and edge information.

33
19:08:05 --> 19:08:09
And the pregel operator works
in a series of super steps.

1
14:13:15 --> 14:13:18
Now, we will introduce you to Spark MLlib.

2
14:13:20 --> 14:13:25
After this video, you will be
able to describe what MLlib is,

3
14:13:25 --> 14:13:29
list the main categories of
techniques available in MLlib,

4
14:13:29 --> 14:13:34
and explain code segments
containing MLlib algorithms.

5
14:13:34 --> 14:13:40
MLlib is a scalable machine learning
library that runs on top of Spark Core.

6
14:13:40 --> 14:13:45
It provides distributed implementations
of commonly used machine learning

7
14:13:45 --> 14:13:46
algorithms and utilities.

8
14:13:48 --> 14:13:54
As with Spark Core, MLlib has APIs for
Scala, Java, Python, and R.

9
14:13:56 --> 14:13:59
MLlib offers many algorithms and

10
14:13:59 --> 14:14:02
techniques commonly used in
a machine learning process.

11
14:14:03 --> 14:14:05
The main categories are machine learning,

12
14:14:05 --> 14:14:09
statistics and some common utility
tools for common techniques.

13
14:14:11 --> 14:14:12
As the name suggests,

14
14:14:12 --> 14:14:16
many machine learning algorithms
are available in MLlib.

15
14:14:17 --> 14:14:22
These are algorithms to build models for
classification, regression, and

16
14:14:22 --> 14:14:22
clustering.

17
14:14:24 --> 14:14:28
There are also techniques for
evaluating the resulting models.

18
14:14:28 --> 14:14:31
For example,
you can compute the values for

19
14:14:31 --> 14:14:36
a receiver of creating characteristic
that we call an ROC curve.

20
14:14:36 --> 14:14:39
A common statistical technique for

21
14:14:39 --> 14:14:42
plotting the performance
of a binary classifier.

22
14:14:44 --> 14:14:48
Statistical functions
are also provided in MLlib.

23
14:14:48 --> 14:14:52
Examples are summary statistics,
means, standard deviation, etc.

24
14:14:53 --> 14:14:57
Correlations and
methods to sample a dataset.

25
14:14:59 --> 14:15:04
MLlib also has techniques commonly
used in the machine learning process,

26
14:15:04 --> 14:15:07
such as dimensionality reduction and

27
14:15:07 --> 14:15:10
feature transformation methods for
preprocessing the data.

28
14:15:11 --> 14:15:15
In short, Spark MLlib offers

29
14:15:15 --> 14:15:19
many techniques often used in
a machine learning pipeline.

30
14:15:21 --> 14:15:27
Let's take a look at an example to
compute summary statistics using MLlib.

31
14:15:27 --> 14:15:31
Note that we will use the spark pipe
of API similar to the ones used for

32
14:15:31 --> 14:15:33
our other examples in this course.

33
14:15:34 --> 14:15:39
Here is the code segment to
compute summary statistics for

34
14:15:39 --> 14:15:43
a data set consisting
of columns of numbers.

35
14:15:43 --> 14:15:47
Lines of code are in white, and
the comments are in orange.

36
14:15:47 --> 14:15:52
The first line imports statistics
functions from the stat module.

37
14:15:52 --> 14:15:58
The second line creates an RDD
of Vectors with the data.

38
14:15:58 --> 14:16:02
You can think of each vector
as a column in a data matrix.

39
14:16:03 --> 14:16:07
The next line denoted with three invokes

40
14:16:07 --> 14:16:12
the column stats function to compute
summary statistics for each column.

41
14:16:13 --> 14:16:18
The last three lines show
by four print out the mean,

42
14:16:18 --> 14:16:22
variance and number of non-zero
entries for each column.

43
14:16:24 --> 14:16:28
As you can see from this example,
computing the summary statistics for

44
14:16:28 --> 14:16:31
a data set is very
straightforward using a MLlib.

45
14:16:34 --> 14:16:36
Here is another example.

46
14:16:36 --> 14:16:40
Although we will go through the ratio
learning details in our next course,

47
14:16:40 --> 14:16:44
here we give you a hint of how to
use two ratio learning techniques.

48
14:16:44 --> 14:16:46
One for Classification,
and one for Clustering.

49
14:16:48 --> 14:16:54
This code segment, shows the six steps to
build a DecisionTree for classification.

50
14:16:54 --> 14:16:57
The first line imports
the DecisionTree module,

51
14:16:57 --> 14:17:00
the second line imports MLUtils module,

52
14:17:02 --> 14:17:07
the next line fails the DecisionTree
to classify the data for two classes.

53
14:17:07 --> 14:17:12
Then, the model is printed out and
finally the model is saved in a file.

54
14:17:15 --> 14:17:19
Here is another MLlib example,
this time for clustering.

55
14:17:19 --> 14:17:24
This code segment shows the 5-step
code to build a k-means clustering.

56
14:17:26 --> 14:17:31
The first line imports the k-means module,
the second line

57
14:17:31 --> 14:17:36
imports an array module from numpy,

58
14:17:36 --> 14:17:41
the next two lines read in the data and
parses it using space as the limiter,

59
14:17:41 --> 14:17:45
then the k-means model

60
14:17:45 --> 14:17:50
is built by dividing the parsedData
into three clusters.

61
14:17:50 --> 14:17:53
Finally, the cluster centers
are printed out for each.

62
14:17:57 --> 14:18:01
In summary,
MLlib is Spark's machine learning library.

63
14:18:03 --> 14:18:04
It provides algorithms and

64
14:18:04 --> 14:18:07
techniques that are implemented
using distributors processing.

65
14:18:08 --> 14:18:11
The main categories of algorithms and

66
14:18:11 --> 14:18:15
techniques available in machine
learning library are machine learning,

67
14:18:15 --> 14:18:20
statistics and utility functions for
the machine learning process.

1
04:31:36 --> 04:31:38
Over the next couple of videos,

2
04:31:38 --> 04:31:43
we will introduce you to the basic
components of the Spark stack.

3
04:31:44 --> 04:31:47
In this lecture, we start with Spark SQL.

4
04:31:49 --> 04:31:54
After this video, you will be able to
process structured data using Spark's SQL

5
04:31:54 --> 04:31:59
module and explain the numerous
benefits of Spark SQL.

6
04:32:02 --> 04:32:08
Spark SQL is the component of Spark
that enables querying structured and

7
04:32:08 --> 04:32:11
unstructured data through
a common query language.

8
04:32:12 --> 04:32:17
It can connect to many data sources and
provides APIs to convert

9
04:32:17 --> 04:32:22
the query results to RDDs in Python,
Scala, and Java programs.

10
04:32:24 --> 04:32:27
Spark SQL gives a mechanism for

11
04:32:27 --> 04:32:32
SQL users to deploy SQL queries on Spark.

12
04:32:35 --> 04:32:39
Spark SQL enables business
intelligence tools to connect to

13
04:32:39 --> 04:32:44
Spark using standard connection
protocols like JDBC and ODBC.

14
04:32:47 --> 04:32:52
Spark SQL also provides APIs to
convert the query data into DataFrames

15
04:32:52 --> 04:32:54
to hold distributed data.

16
04:32:55 --> 04:33:00
DataFrames are organized as named
columns and basically look like tables.

17
04:33:04 --> 04:33:10
The first step to run any SQL Spark
is to create a SQLContext.

18
04:33:12 --> 04:33:17
Once you have an SQLContext,
you want to leverage it

19
04:33:17 --> 04:33:22
to create a DataFrame so you can deploy
complex operations on the data set.

20
04:33:23 --> 04:33:26
DataFrames can be created
from existing RDDs,

21
04:33:26 --> 04:33:29
Hive tables or many other data sources.

22
04:33:31 --> 04:33:39
A file can be read and converted into
a DataFrame using a single command.

23
04:33:40 --> 04:33:48
The show function here will display
the DataFrame in your Spark show.

24
04:33:48 --> 04:33:52
RDDs can be converted to DataFrames but
require a little more work.

25
04:33:53 --> 04:33:57
First you will have to
convert each line into a row.

26
04:33:58 --> 04:34:02
Once your data is in a DataFrame,
you can perform all sorts of

27
04:34:02 --> 04:34:07
transformation operations on it
as shown here, including show,

28
04:34:07 --> 04:34:11
printSchema, select, filter and groupBy.

29
04:34:13 --> 04:34:18
To summarize, Spark SQL lets you
run relational queries on Spark.

30
04:34:20 --> 04:34:25
It also lets you connect to
a variety of databases, and

31
04:34:25 --> 04:34:28
deploy business intelligence
tools over Spark.

32
04:34:28 --> 04:34:32
We will go through some of this
functionality in one of the readings and

33
04:34:32 --> 04:34:34
in the upcoming hands-on session.

1
09:06:09 --> 09:06:13
Next we will talk about Spark streaming.

2
09:06:14 --> 09:06:20
After this video you will be able to
summarize how Spark reads streaming data,

3
09:06:20 --> 09:06:25
list several sources of streaming
data supported by Spark, and

4
09:06:25 --> 09:06:28
describe Spark's sliding windows.

5
09:06:28 --> 09:06:33
Spark streaming provides
scalable processing for

6
09:06:33 --> 09:06:38
real-time data and
runs on top of Spark Core.

7
09:06:40 --> 09:06:44
Continuous data streams are converted or

8
09:06:44 --> 09:06:50
grouped into discrete RDDs which
can then be processed in parallel.

9
09:06:51 --> 09:06:55
Spark Streaming provides APIs for Scala,

10
09:06:55 --> 09:06:59
Java, and Python,
like other Spark products.

11
09:07:03 --> 09:07:08
Spark Streaming can read
data from many different

12
09:07:08 --> 09:07:14
types of resources,
including Kafka and Flume.

13
09:07:14 --> 09:07:20
Kafka is a high throughput published
subscribed messaging system,

14
09:07:20 --> 09:07:25
and Flume collects and
aggregates log data.

15
09:07:25 --> 09:07:30
Spark Streaming can also read from batch

16
09:07:30 --> 09:07:34
input data sources, such as HDFS,

17
09:07:34 --> 09:07:39
S3, and many other non SQL databases.

18
09:07:39 --> 09:07:47
Additionally, Spark Streaming can read
directly from Twitter, raw TCP sockets,

19
09:07:47 --> 09:07:53
and many other data sources that
are real-time data providers.

20
09:07:56 --> 09:07:57
So how does it all work?

21
09:07:58 --> 09:08:03
Here we show you a flow of
transformations and actions

22
09:08:03 --> 09:08:09
which you will try in the upcoming reading
and hands-on exercises on Spark Streaming.

23
09:08:09 --> 09:08:13
Spark streaming reads

24
09:08:13 --> 09:08:18
streaming data and
converts it into micro batches

25
09:08:18 --> 09:08:24
which we call DStreams which is short for
discretized stream.

26
09:08:27 --> 09:08:32
In this example a 10 second
stream gets converted

27
09:08:32 --> 09:08:37
into five RDDs using a batch
length of 2 seconds.

28
09:08:39 --> 09:08:44
Similar to other RDDs,
transformations such as map, reduce,

29
09:08:44 --> 09:08:47
and filter can be applied to DStreams.

30
09:08:47 --> 09:08:53
DStreams can be aggregated

31
09:08:53 --> 09:09:00
into Windows allowing you to apply
computations on sliding window of data.

32
09:09:01 --> 09:09:06
In this example the Window
size is 4 seconds and

33
09:09:06 --> 09:09:09
the sliding interval is 2 seconds.

34
09:09:12 --> 09:09:17
In summary, Spark Streaming is
Spark's library to work with

35
09:09:17 --> 09:09:20
streaming data in near real time.

36
09:09:22 --> 09:09:27
DStreams can be used just
like any other RDD and

37
09:09:27 --> 09:09:30
can go through the same
transformation as batch datasets.

38
09:09:32 --> 09:09:39
DStreams can create a sliding window to
perform calculations on a window of time.

1
18:15:49 --> 18:15:52
Now that we know what machine learning
is and have seen some examples of it,

2
18:15:52 --> 18:15:56
let's talk about how we
do machine learning.

3
18:15:56 --> 18:16:00
In this lecture we will get an overview of
the main categories of machine learning

4
18:16:00 --> 18:16:00
techniques.

5
18:16:02 --> 18:16:05
After this video you will be able to

6
18:16:05 --> 18:16:08
describe the main categories of
machine learning techniques and

7
18:16:08 --> 18:16:12
summarize how supervised learning
differs from unsupervised learning.

8
18:16:14 --> 18:16:17
There are different categories of
machine learning techniques for

9
18:16:17 --> 18:16:19
different types of problems.

10
18:16:19 --> 18:16:21
The main categories are listed here.

11
18:16:21 --> 18:16:25
They are classification,
regression, cluster analysis, and

12
18:16:25 --> 18:16:27
association analysis.

13
18:16:27 --> 18:16:30
We will cover each one in
detail in the following slides.

14
18:16:31 --> 18:16:36
In classification, the goal is to
predict the category of the input data.

15
18:16:36 --> 18:16:41
An example of this is predicting the
weather as being sunny, rainy, windy, or

16
18:16:41 --> 18:16:43
cloudy.

17
18:16:43 --> 18:16:47
The input data in this case would be
sensor data specifying the temperature,

18
18:16:47 --> 18:16:52
relative humidity, atmospheric pressure,
wind speed, wind direction, etc.

19
18:16:52 --> 18:16:57
The target or what you're trying to
predict would be the different weather

20
18:16:57 --> 18:17:01
categories, sunny, windy,
rainy, and cloudy.

21
18:17:01 --> 18:17:06
Another example is to classify
a tumor as either benign or malignant.

22
18:17:06 --> 18:17:09
In this case,
the classification is referred to as

23
18:17:09 --> 18:17:13
binary classification since
there are only two categories.

24
18:17:13 --> 18:17:15
But you can have many categories as well.

25
18:17:15 --> 18:17:17
As the weather prediction
problem shown here.

26
18:17:18 --> 18:17:22
Another example is to identify hand
written digits as being in one of ten

27
18:17:22 --> 18:17:23
categories, zero to nine.

28
18:17:25 --> 18:17:30
Some more examples of classification
are classifying a tumor from a medical

29
18:17:30 --> 18:17:32
image as being benign or malignant.

30
18:17:32 --> 18:17:35
Predicting whether it
will rain the next day.

31
18:17:35 --> 18:17:40
Determining if a loan application is
high-risk, medium-risk or low-risk.

32
18:17:40 --> 18:17:42
Identifying the sentiment of a tweet or

33
18:17:42 --> 18:17:45
review as being positive,
negative, or neutral.

34
18:17:47 --> 18:17:51
When your model has to predict
a numeric value instead of a category,

35
18:17:51 --> 18:17:54
then the task becomes
a regression problem.

36
18:17:56 --> 18:17:59
An example of regression is to
predict the price of a stock.

37
18:17:59 --> 18:18:03
The stock price is a numeric value,
not a category.

38
18:18:03 --> 18:18:06
So this is a regression task
instead of a classification task.

39
18:18:07 --> 18:18:10
If you were to predict whether
the stock price will rise or

40
18:18:10 --> 18:18:13
fall, then that would be
a classification problem.

41
18:18:13 --> 18:18:16
But if you're predicting
the actual price of the stock,

42
18:18:16 --> 18:18:17
then that is a regression problem.

43
18:18:18 --> 18:18:22
That is the main difference between
classification and regression.

44
18:18:22 --> 18:18:25
In classification,
you're predicting a category and

45
18:18:25 --> 18:18:27
in regression,
you're predicting a numeric value.

46
18:18:29 --> 18:18:34
Some other examples of regression
are estimating the demand of a product

47
18:18:34 --> 18:18:36
based on time or season of the year.

48
18:18:36 --> 18:18:39
Predicting a score on a test,

49
18:18:39 --> 18:18:42
determining the likelihood of
how effective a drug will be for

50
18:18:42 --> 18:18:47
a particular patient, predicting
the amount of rain for a region.

51
18:18:47 --> 18:18:48
In cluster analysis,

52
18:18:48 --> 18:18:53
the goal is to organize similar
items in your data set into groups.

53
18:18:53 --> 18:18:57
A very common application of
cluster analysis is referred to as

54
18:18:57 --> 18:18:59
customer segmentation.

55
18:18:59 --> 18:19:03
This means that you're separating your
customer base into different groups or

56
18:19:03 --> 18:19:05
segments based on customer types.

57
18:19:05 --> 18:19:09
For example it would be very beneficial
to segment your customers into seniors,

58
18:19:09 --> 18:19:12
adults and teenagers.

59
18:19:12 --> 18:19:14
These groups have different likes and
dislikes and

60
18:19:14 --> 18:19:16
have different purchasing behaviors.

61
18:19:16 --> 18:19:20
By segmenting your customers to different
groups you can more effectively provide

62
18:19:20 --> 18:19:24
marketing adds targeted for
each groups particular interests.

63
18:19:24 --> 18:19:27
Note that cluster analysis is
also referred to as clustering.

64
18:19:29 --> 18:19:33
Some other examples of cluster
analysis are, identifying areas of

65
18:19:33 --> 18:19:38
similar topography, such as desert region,
grassy areas, mountains etc.

66
18:19:38 --> 18:19:42
Categorizing different types of
tissues from medical images.

67
18:19:42 --> 18:19:47
Determining different groups of weather
patterns, such as snowy, dry, monsoon.

68
18:19:48 --> 18:19:51
And discovering hot spots for different
types of crime from police reports.

69
18:19:53 --> 18:19:57
The goal in association analysis is
to come up with a set of rules to

70
18:19:57 --> 18:20:01
capture associations between items or
events.

71
18:20:01 --> 18:20:05
The rules are used to determine when
items or events occur together.

72
18:20:05 --> 18:20:09
A common application of association
analysis is known as market

73
18:20:09 --> 18:20:11
basket analysis.

74
18:20:11 --> 18:20:15
Which is used to understand
customer purchasing behavior.

75
18:20:15 --> 18:20:19
For example, association analysis can
reveal that banking customers who have

76
18:20:19 --> 18:20:23
CDs, or Certificates of Deposits,
also tend to be interested in

77
18:20:23 --> 18:20:26
other investment vehicles such
as money market accounts.

78
18:20:26 --> 18:20:29
This information can be used for
cross selling.

79
18:20:29 --> 18:20:34
If you advertise money market accounts to
your customers with CDs they are likely to

80
18:20:34 --> 18:20:35
open such an account.

81
18:20:37 --> 18:20:41
According to data mining folklore
a supermarket chain used association

82
18:20:41 --> 18:20:46
analysis to discover a connection between
two seemingly unrelated products.

83
18:20:46 --> 18:20:49
They discovered that many
customers who go to the store

84
18:20:49 --> 18:20:53
late on Sunday night to buy
diapers also tend to buy beer.

85
18:20:53 --> 18:20:55
This information was then
used to place beer and

86
18:20:55 --> 18:20:59
diapers close together and
they saw a jump in sales of both items.

87
18:20:59 --> 18:21:01
This is the famous diaper,
beer connection.

88
18:21:03 --> 18:21:08
Some other applications of association
analysis are recommending similar

89
18:21:08 --> 18:21:13
items based on the purchasing behavior or
browsing histories of customers.

90
18:21:14 --> 18:21:17
Finding items that are often purchased
together, such as garden hose and

91
18:21:17 --> 18:21:19
potting soil, and

92
18:21:19 --> 18:21:22
offer sales on these related items at the
same time to drive sales of both items.

93
18:21:23 --> 18:21:26
Identifying web pages that
are often accessed together so

94
18:21:26 --> 18:21:31
that you can more efficiently offer up
these related web pages at the same time.

95
18:21:32 --> 18:21:36
We have now looked at the different
categories of machine learning techniques.

96
18:21:36 --> 18:21:41
They are classification,
regression, cluster analysis, and

97
18:21:41 --> 18:21:42
association analysis.

98
18:21:42 --> 18:21:45
We have also seen some
examples of each category

99
18:21:47 --> 18:21:51
There is also another categorization
of machine learning techniques, and

100
18:21:51 --> 18:21:54
that is supervised versus
unsupervised approaches.

101
18:21:55 --> 18:21:57
In supervised approaches the target,

102
18:21:57 --> 18:22:00
which is what the model is predicting,
is provided.

103
18:22:00 --> 18:22:05
This is referred to as having labeled
data because the target is labeled for

104
18:22:05 --> 18:22:07
every sample that you
have in your data set.

105
18:22:08 --> 18:22:12
Referring back to our example of
predicting a weather category of sunny,

106
18:22:12 --> 18:22:14
windy, rainy or cloudy,

107
18:22:14 --> 18:22:19
every sample in the data set is labeled
as being one of these four categories.

108
18:22:19 --> 18:22:24
So the data is labeled and predicting the
weather categories is a supervised task.

109
18:22:24 --> 18:22:28
In general, classification and
regression are supervised approaches.

110
18:22:30 --> 18:22:33
In unsupervised approaches
on the other hand,

111
18:22:33 --> 18:22:37
the target that the model is
predicting is unknown or unavailable.

112
18:22:37 --> 18:22:40
This means that you have unlabeled data.

113
18:22:40 --> 18:22:44
Going back to our cluster analysis
example of segmenting customers into

114
18:22:44 --> 18:22:45
different groups.

115
18:22:45 --> 18:22:49
The samples in your data are not
labeled with the correct group.

116
18:22:49 --> 18:22:53
Instead, the segmentation is performed
using a clustering technique to group

117
18:22:53 --> 18:22:56
items based on characteristics
that they have in common.

118
18:22:56 --> 18:22:58
Thus, the data is unlabeled and

119
18:22:58 --> 18:23:03
the task of grouping customers into
different segments is an unsupervised one.

120
18:23:03 --> 18:23:05
In general, cluster analysis and

121
18:23:05 --> 18:23:08
association analysis
are unsupervised approaches.

122
18:23:10 --> 18:23:11
In summary,

123
18:23:11 --> 18:23:16
in this lecture we looked at the different
categories of machine learning techniques.

124
18:23:16 --> 18:23:22
We discussed classification, regression,
cluster analysis and association analysis.

125
18:23:22 --> 18:23:24
We also defined what supervised and

126
18:23:24 --> 18:23:26
unsupervised approaches are,
in machine learning.

1
12:39:14 --> 12:39:17
In this lecture,
we will review each of the steps in

2
12:39:17 --> 12:39:19
the machine learning
process in greater detail.

3
12:39:20 --> 12:39:25
After this video, you will be able
to explain the goals of each step in

4
12:39:25 --> 12:39:30
the machine learning process, and list key
activities in each step in the process.

5
12:39:32 --> 12:39:36
This is the machine learning process
that we saw in the last lecture.

6
12:39:36 --> 12:39:40
In this lecture,
we will cover each step in more detail.

7
12:39:40 --> 12:39:42
We will describe the goals
of each step and

8
12:39:42 --> 12:39:44
the key activities performed in each step.

9
12:39:46 --> 12:39:50
The first step in the data science
process is to acquire the data.

10
12:39:50 --> 12:39:52
The goal of the step is to identify and

11
12:39:52 --> 12:39:55
obtain all data related
to the problem at hand.

12
12:39:56 --> 12:40:00
First, we need to identify all
related data and the sources.

13
12:40:00 --> 12:40:05
Keep in mind, that data can come from
different sources such as files,

14
12:40:05 --> 12:40:08
databases, the internet, mobile devices.

15
12:40:08 --> 12:40:12
So remember to include all data related
to the problem you're addressing.

16
12:40:13 --> 12:40:16
After you've identified your data and
data sources,

17
12:40:16 --> 12:40:21
the next step is to collect the data and
integrate data from the different sources.

18
12:40:21 --> 12:40:25
This may require conversion,
as data can come in different formats.

19
12:40:25 --> 12:40:28
And it may also require aligning the data,

20
12:40:28 --> 12:40:32
as data from different sources may have
different time or spacial resolutions.

21
12:40:34 --> 12:40:39
Once you’ve collected and integrated your
data, you now have a coherent data set for

22
12:40:39 --> 12:40:40
your analysis.

23
12:40:41 --> 12:40:46
The next step after acquiring data is
to prepare it to make it suitable for

24
12:40:46 --> 12:40:47
analysis.

25
12:40:47 --> 12:40:52
There are two parts to this step,
explore data and preprocess data.

26
12:40:52 --> 12:40:54
We will discuss data exploration first.

27
12:40:56 --> 12:41:00
In data exploration, you want to
do some preliminary investigation

28
12:41:00 --> 12:41:05
in order to gain a better understanding of
the specific characteristics of your data.

29
12:41:05 --> 12:41:08
This in turn will guide
the rest of the process.

30
12:41:08 --> 12:41:11
With data exploration,
you'll want to look for

31
12:41:11 --> 12:41:16
things like correlations,
general trends, outliers, etc.

32
12:41:18 --> 12:41:20
Correlations provide information
about the relationship

33
12:41:20 --> 12:41:22
between variables in your data.

34
12:41:23 --> 12:41:28
Trends in your data will reveal if the
variable is moving in a certain direction,

35
12:41:28 --> 12:41:31
such as transaction volume
increasing throughout the year.

36
12:41:33 --> 12:41:36
Outliers indicate potential
problems with the data, or

37
12:41:36 --> 12:41:40
may indicate an interesting data
point that needs further examination.

38
12:41:40 --> 12:41:42
Without this data exploration activity,

39
12:41:42 --> 12:41:45
you will not be able to
use your data effectively.

40
12:41:47 --> 12:41:51
One way to explore your data is
to calculate summary statistics

41
12:41:51 --> 12:41:53
to numerically describe the data.

42
12:41:55 --> 12:41:59
Summary statistics are quantities
that capture various characteristics

43
12:41:59 --> 12:42:03
of a set of values with a single number,
or a small set of numbers.

44
12:42:03 --> 12:42:08
Some basic summary statistics that you
should compute for your data set are mean,

45
12:42:08 --> 12:42:13
median, mode, range and
standard deviation.

46
12:42:13 --> 12:42:16
Mean and median are measures of
the location of a set of values.

47
12:42:16 --> 12:42:20
Mode is the value that occurs most
frequently in your data set, and

48
12:42:20 --> 12:42:24
range and standard deviation
are measures of spread in your data.

49
12:42:24 --> 12:42:29
Looking at these measures will give you
an idea of the nature of your data.

50
12:42:29 --> 12:42:32
They can tell you if there's
something wrong with your data.

51
12:42:32 --> 12:42:36
For example, if the range of
the values for age in your data

52
12:42:36 --> 12:42:40
includes negative numbers, or
a number much greater than a hundred,

53
12:42:40 --> 12:42:42
there's something suspicious in
the data that needs to be examined.

54
12:42:44 --> 12:42:47
Visualization techniques
also provide quick and

55
12:42:47 --> 12:42:49
effective ways to explore your data.

56
12:42:49 --> 12:42:52
Some examples are, a histogram,

57
12:42:52 --> 12:42:56
such as the plot shown here,
shows the distribution of the data and

58
12:42:56 --> 12:43:00
can show skewness or
unusual dispersion in outliers.

59
12:43:01 --> 12:43:06
A line plot, like the one in the lower
left, can be used to look at trends in

60
12:43:06 --> 12:43:09
the data, such as,
the change in the price of a stock.

61
12:43:10 --> 12:43:13
A heat map can give you an idea
of where the hot spots are.

62
12:43:15 --> 12:43:20
A scatter plot effectively shows
correlation between two variables.

63
12:43:20 --> 12:43:23
Overall, there are many types
of plots to visualize data.

64
12:43:23 --> 12:43:26
They are very useful in helping
you understand the data you have.

65
12:43:28 --> 12:43:32
The second part of the prepare
step is preprocess.

66
12:43:32 --> 12:43:36
So, after we've explored the data, we need
to preprocess the data to prepare it for

67
12:43:36 --> 12:43:37
analysis.

68
12:43:39 --> 12:43:42
The goal here is to create the data
that will be used for analysis.

69
12:43:42 --> 12:43:46
The main activities on this
part are to clean the data,

70
12:43:46 --> 12:43:50
select the appropriate variables to
use and transform the data as needed.

71
12:43:51 --> 12:43:54
A very important part of
data preparation is to

72
12:43:54 --> 12:43:56
clean the data to address quality issues.

73
12:43:56 --> 12:43:58
Real world data is nothing.

74
12:43:58 --> 12:44:04
There are many examples of quality issues
with data from real applications including

75
12:44:04 --> 12:44:09
missing values, such as income
in a survey, duplicate data,

76
12:44:09 --> 12:44:12
such as two different records for the same
customer with different addresses.

77
12:44:14 --> 12:44:17
Inconsistent or invalid data,
such as a six digit zip code.

78
12:44:19 --> 12:44:22
Noise in the collection of data
that distorts the true values.

79
12:44:23 --> 12:44:28
Outliers, such as a number much
larger than 100 for someone's age.

80
12:44:28 --> 12:44:29
It is essential to detect and

81
12:44:29 --> 12:44:33
address these issues that can negatively
affect the quality of the data.

82
12:44:35 --> 12:44:39
Other activities in data preprocessing
can be broadly categorized as

83
12:44:39 --> 12:44:43
feature selection and
feature transformation.

84
12:44:43 --> 12:44:46
Feature selection refers to
choosing the set of features to

85
12:44:46 --> 12:44:49
use that is appropriate for
the application.

86
12:44:49 --> 12:44:52
Feature selection can
involve removing redundant or

87
12:44:52 --> 12:44:57
irrelevant features, combining features,
or creating new features.

88
12:44:58 --> 12:45:00
During the exploring data step,

89
12:45:00 --> 12:45:03
you may have discovered that two
features are very correlated.

90
12:45:03 --> 12:45:06
In that case,
one of these features can be removed

91
12:45:06 --> 12:45:09
without negatively effecting
the analysis results.

92
12:45:09 --> 12:45:12
For example,
the purchase price of a product and

93
12:45:12 --> 12:45:15
the amount of sales tax pain
are very likely to be correlated.

94
12:45:15 --> 12:45:19
Eliminating the sales tax
amount then will be beneficial.

95
12:45:19 --> 12:45:24
Removing redundant or irrelevant features
will make the subsequent analysis simpler.

96
12:45:25 --> 12:45:28
You may also want to combine features or
create new ones.

97
12:45:28 --> 12:45:31
For example,
adding the applicants education level

98
12:45:31 --> 12:45:34
as a feature to a loan approval
application would make sense.

99
12:45:35 --> 12:45:38
There are also algorithms to
automatically determine the most relevant

100
12:45:38 --> 12:45:40
features based on various
mathematical properties.

101
12:45:43 --> 12:45:47
Feature transformation maps the data
from one format to another.

102
12:45:47 --> 12:45:49
Various transformation operations exist.

103
12:45:49 --> 12:45:54
For example, scaling maps
the data values to a specified

104
12:45:54 --> 12:45:58
range to prevent any one feature from
dominating the analysis results.

105
12:46:00 --> 12:46:04
Filtering or aggregation can be used to
reduce noise and variability in the data.

106
12:46:06 --> 12:46:11
Dimensionality reduction maps the data
to a smaller subset of dimensions

107
12:46:11 --> 12:46:14
to simplify the subsequent analysis.

108
12:46:14 --> 12:46:18
We will discuss techniques to prepare
data in more detail later in this course.

109
12:46:20 --> 12:46:23
After preparing the data to
address data quality issues and

110
12:46:23 --> 12:46:26
preprocess it to get it in
the appropriate format,

111
12:46:26 --> 12:46:30
the next step in the machine learning
process is to analyze the data.

112
12:46:30 --> 12:46:33
The goals of the staff are to
build a machine learning model,

113
12:46:33 --> 12:46:37
to analyze the data and to evaluate
the results that you get from the model.

114
12:46:40 --> 12:46:45
The analyze steps starts with this
determining the type of problem you have.

115
12:46:45 --> 12:46:48
You begin by selecting appropriate machine
learning techniques to analyze the data.

116
12:46:50 --> 12:46:53
Then you construct the model using
the data that you've prepared.

117
12:46:54 --> 12:46:59
Once the model is built, you will
want to apply it to new data samples

118
12:46:59 --> 12:47:01
to evaluate how well the model performs.

119
12:47:02 --> 12:47:06
Thus data analysis involves selecting
the appropriate technique for

120
12:47:06 --> 12:47:10
your problem, building the model,
then evaluating the results.

121
12:47:10 --> 12:47:11
As there are a different
types of problems,

122
12:47:11 --> 12:47:14
there are also different
types of analysis techniques.

123
12:47:14 --> 12:47:16
We will cover algorithms for

124
12:47:16 --> 12:47:19
building machine learning models in a much
more detail in week three of this course.

125
12:47:21 --> 12:47:25
The next step in the machine learning
process is reporting results from your

126
12:47:25 --> 12:47:25
analysis.

127
12:47:25 --> 12:47:30
In reporting your results, it is important
to communicate your insights and

128
12:47:30 --> 12:47:32
make a case for
what actions should follow.

129
12:47:34 --> 12:47:35
In reporting your results,

130
12:47:35 --> 12:47:39
you will want to think about what to
present, as well as how to present.

131
12:47:39 --> 12:47:43
In deciding what to present, you should
consider what the main results are,

132
12:47:43 --> 12:47:46
what insights were gained
from your analysis, and

133
12:47:46 --> 12:47:49
what added value do these insights
bring to the application.

134
12:47:50 --> 12:47:54
Keep in mind that even negative results
are valuable lessons learned, and

135
12:47:54 --> 12:47:57
suggest further avenues for
additional analysis.

136
12:47:57 --> 12:48:00
Remember that all findings
must be presented so

137
12:48:00 --> 12:48:02
that informs decisions can be made for
next steps.

138
12:48:03 --> 12:48:05
In deciding how to present,

139
12:48:05 --> 12:48:09
remember that visualization is an
important tool in presenting your results.

140
12:48:11 --> 12:48:14
Plots and summary statistics
discussing the explore step

141
12:48:14 --> 12:48:16
can be used effectively here as well.

142
12:48:16 --> 12:48:20
You should also have tables with
details from your analysis as backup,

143
12:48:20 --> 12:48:22
if someone wants to take
a deeper dive into the results.

144
12:48:23 --> 12:48:27
In summary, you want to report your
findings by presenting your results and

145
12:48:27 --> 12:48:30
the value added with graphs
using visualization tools.

146
12:48:33 --> 12:48:35
The final step in
the machine loading process

147
12:48:35 --> 12:48:39
is to determine what action should be
taken based on the insights gained.

148
12:48:41 --> 12:48:44
What action should be taken based
on the results of your analysis?

149
12:48:44 --> 12:48:48
Should you market certain products
to a specific customer segment to

150
12:48:48 --> 12:48:49
increase sales?

151
12:48:49 --> 12:48:52
What inefficiency is can be
removed from your process?

152
12:48:52 --> 12:48:55
What incentives would be effective
in attracting new customers?

153
12:48:57 --> 12:49:00
Once a specific action
has been determined,

154
12:49:00 --> 12:49:02
the next step is to implement the action.

155
12:49:03 --> 12:49:08
Things to consider here include, how can
the action be added to your application?

156
12:49:08 --> 12:49:10
How will end users be affected?

157
12:49:11 --> 12:49:16
Assessing the impact of the implemented
action is then necessary to evaluate

158
12:49:16 --> 12:49:17
the benefits gained.

159
12:49:17 --> 12:49:20
The results of this assessment
determine next steps,

160
12:49:20 --> 12:49:24
which could suggest additional analysis or
further opportunities,

161
12:49:24 --> 12:49:28
which would begin another cycle
of the machine learning process.

162
12:49:30 --> 12:49:33
In summary, we've looked at the steps in
the machine learning process in detail.

163
12:49:35 --> 12:49:39
The goals or main activities of each
step were discussed in this lecture.

164
12:49:39 --> 12:49:42
Remember that this is
an iterative process,

165
12:49:42 --> 12:49:46
in which each step can be repeated
more than once, and findings from

166
12:49:46 --> 12:49:50
each step may require a previous step
to be repeated with new information

1
01:29:04 --> 01:29:06
Hello, everyone.
You have probably heard of machine

2
01:29:06 --> 01:29:10
learning before, enough to grab your
attention and bring you to this class.

3
01:29:10 --> 01:29:13
However, you might be wondering
what machine learning is.

4
01:29:13 --> 01:29:14
We will talk about that
in this lecture and

5
01:29:14 --> 01:29:18
discuss how we can see machine learning
in action in our day-to-day life.

6
01:29:20 --> 01:29:24
After this video you will be able to
explain what machine learning is.

7
01:29:24 --> 01:29:28
And list three applications of machine
learning encountered in everyday life.

8
01:29:30 --> 01:29:35
Since this course is about machine
learning, lets define what that means.

9
01:29:35 --> 01:29:38
We hear this term a lot these
days used in many contexts.

10
01:29:38 --> 01:29:42
So it's good to start out with a solid
definition of what machine learning means.

11
01:29:43 --> 01:29:48
Machine learning is the field of study
that focuses on computer systems that can

12
01:29:48 --> 01:29:49
learn from data.

13
01:29:49 --> 01:29:54
That is the system's often called
models can learn to perform a specific

14
01:29:54 --> 01:29:58
task by analyzing lots of examples for
a particular problem.

15
01:29:58 --> 01:30:02
For example, a machine learning
model can learn to recognize

16
01:30:02 --> 01:30:06
an image of a cat by being shown lots and
lots of images of cats.

17
01:30:08 --> 01:30:10
This notion of learning from data
means that a machine learning

18
01:30:10 --> 01:30:15
model can learn a specific task
without being explicitly programmed.

19
01:30:15 --> 01:30:19
In other words, the machine learning
model is not given the step

20
01:30:19 --> 01:30:22
by step instructions on how to
recognize the image of a cat.

21
01:30:23 --> 01:30:26
Instead, the model learns
what features are important

22
01:30:26 --> 01:30:32
in determining whether it picture contains
a cat from the data that has analyzed.

23
01:30:32 --> 01:30:35
Because the model learns to
perform this task from data

24
01:30:35 --> 01:30:39
it's good to know that the amount and
quality of data available for

25
01:30:39 --> 01:30:43
building the model are important factors
in how well the model learns the task.

26
01:30:44 --> 01:30:47
Because machine learning
models can learn from data

27
01:30:47 --> 01:30:50
that can be used to discover hidden
patterns and trends in the data.

28
01:30:52 --> 01:30:56
These patterns and trends lead to
valuable insights into the data.

29
01:30:56 --> 01:31:00
Thus the use of machine learning allows
for data driven decisions to be made for

30
01:31:00 --> 01:31:02
a particular problem.

31
01:31:02 --> 01:31:05
So to summarize, the field of machine
learning focuses on the study and

32
01:31:05 --> 01:31:09
construction of computer systems
that can learn from data

33
01:31:09 --> 01:31:11
without being explicitly programmed.

34
01:31:11 --> 01:31:13
Machine learning algorithms and

35
01:31:13 --> 01:31:16
techniques are used to build models,
to discover hidden patterns and

36
01:31:16 --> 01:31:19
trends in the data allowing for
data-driven decisions to be made.

37
01:31:22 --> 01:31:25
You may have heard that machine learning
is an inter-disciplinary field.

38
01:31:25 --> 01:31:27
This is very true.

39
01:31:27 --> 01:31:31
Machine learning combines concepts and
methods from many disciplines, including

40
01:31:31 --> 01:31:36
math, statistics, computer science,
artificial intelligence, and optimization.

41
01:31:37 --> 01:31:39
In applying machine learning to a problem,

42
01:31:39 --> 01:31:44
domain knowledge is essential
to the success of end results.

43
01:31:44 --> 01:31:47
By domain knowledge we mean
an understanding of the application or

44
01:31:47 --> 01:31:48
business domain.

45
01:31:49 --> 01:31:53
Knowledge about the application,
the data related to the application, and

46
01:31:53 --> 01:31:56
how the outcomes will be used are crucial
to driving the process of building

47
01:31:56 --> 01:31:58
the machine learning model.

48
01:31:58 --> 01:32:02
So domain knowledge is also an integral
part of a machine learning solution.

49
01:32:04 --> 01:32:07
Machine learning has been used in many
different learning application, many of

50
01:32:07 --> 01:32:11
which you'll probably encounter in your
daily life, perhaps without realizing it.

51
01:32:13 --> 01:32:17
One application of machine learning that
you likely used this past weekend, or

52
01:32:17 --> 01:32:20
even just today,
is credit card fraud detection.

53
01:32:20 --> 01:32:24
Every time you use your credit card,
the current purchase is analyzed

54
01:32:24 --> 01:32:28
against your history of credit card
transactions to determine if the current

55
01:32:28 --> 01:32:33
purchase is a legitimate transaction or
a potentially fraudulent one.

56
01:32:33 --> 01:32:36
If the purchase is very different
from your past purchases, such as for

57
01:32:36 --> 01:32:40
a big ticket item in a category that
you had never shown an interest in or

58
01:32:40 --> 01:32:43
when the point of sales location
is from another country,

59
01:32:43 --> 01:32:45
then it will be flagged
as a suspicious activity.

60
01:32:46 --> 01:32:49
In that case,
the transaction may be denied.

61
01:32:49 --> 01:32:51
Or you may get a call from your
credit card company to confirm that

62
01:32:51 --> 01:32:54
the purchase was indeed made by you.

63
01:32:54 --> 01:32:57
This is a very common use of machine
learning that is encountered in

64
01:32:57 --> 01:32:58
everyday life.

65
01:33:00 --> 01:33:03
Another example application of machine
learning encountered in daily life is

66
01:33:03 --> 01:33:06
handwritten digit recognition.

67
01:33:06 --> 01:33:10
When you deposit a hand-written check into
an ATM, a machine learning process is used

68
01:33:10 --> 01:33:15
to read the numbers written on the check
to determine the amount of the deposit.

69
01:33:15 --> 01:33:19
Handwritten digits are trickier
to decipher than typed digits

70
01:33:19 --> 01:33:22
due to the many variations
in people's handwriting.

71
01:33:23 --> 01:33:27
A machine learning system can sift through
the different variations to find similar

72
01:33:27 --> 01:33:30
patterns to distinguish a one from a nine,
for example.

73
01:33:32 --> 01:33:35
Recommendations in what sites is
another example application of

74
01:33:35 --> 01:33:39
machine learning that most people
have experienced first hand.

75
01:33:39 --> 01:33:44
After you buy an item on a website you
will often get a list of related items.

76
01:33:44 --> 01:33:47
Often this will be displayed
as customers who bought this

77
01:33:47 --> 01:33:50
item also bought these items,
or you may also like.

78
01:33:51 --> 01:33:55
These related items have been associated
with the item you purchased by

79
01:33:55 --> 01:33:56
a machine learning model, and

80
01:33:56 --> 01:34:00
are now being shown to you since
you may also be interested in them.

81
01:34:00 --> 01:34:03
This is a common application of
machine learning used often in

82
01:34:03 --> 01:34:04
sales and marketing.

83
01:34:04 --> 01:34:09
Here are some other examples of where
machine learning has been used.

84
01:34:09 --> 01:34:12
Like targeted ads on mobile devices.

85
01:34:12 --> 01:34:15
Sentiment analysis of social media data,

86
01:34:15 --> 01:34:20
climate monitoring to detect seasonal
patterns, crime pattern detection, and

87
01:34:20 --> 01:34:23
a healthiness analysis of drugs
among many other applications.

88
01:34:25 --> 01:34:26
As you can see from this short list,

89
01:34:26 --> 01:34:30
machine learning has been used in
various applications including science,

90
01:34:30 --> 01:34:34
medicine, retail, law enforcement,
education and many others.

91
01:34:36 --> 01:34:41
Let's take a few minutes to discuss the
different terms which refer to this field.

92
01:34:41 --> 01:34:45
The term we are using for this course is
machine learning, but you may have heard

93
01:34:45 --> 01:34:50
other terms such as data mining,
predictive analytics and data slangs.

94
01:34:50 --> 01:34:52
So what is the difference
between these different terms?

95
01:34:54 --> 01:34:57
As we have discussed, machine learning
has its roots since statistics,

96
01:34:57 --> 01:35:01
artificial intelligence, and
computer science among other fields.

97
01:35:01 --> 01:35:04
Machine learning encompasses
the algorithms and

98
01:35:04 --> 01:35:05
techniques used to learn from data.

99
01:35:07 --> 01:35:11
The term data mining became popular
around the time that the use

100
01:35:11 --> 01:35:13
databases became common place.

101
01:35:13 --> 01:35:17
So data mining was used to refer to
activities related to finding patterns in

102
01:35:17 --> 01:35:20
databases and data warehouses.

103
01:35:20 --> 01:35:24
There are some practical data management
aspects to data mining related to

104
01:35:24 --> 01:35:26
accessing data from databases.

105
01:35:26 --> 01:35:29
But the process of finding
patterns in data is similar, and

106
01:35:29 --> 01:35:32
can use the same algorithms and
techniques as machine learning.

107
01:35:34 --> 01:35:40
Predictive analytics refers to analyzing
data in order to predict future outcomes.

108
01:35:40 --> 01:35:44
This term is usually used in the business
context to describe activities such as

109
01:35:44 --> 01:35:48
sales forecasting or predicting
the purchasing behavior of a customer.

110
01:35:48 --> 01:35:52
But again the techniques used to make
these predictions are the same techniques

111
01:35:52 --> 01:35:52
from machine learning.

112
01:35:55 --> 01:35:58
Data science is a new term that is
used to describe processing and

113
01:35:58 --> 01:36:00
analyzing data to extract meaning.

114
01:36:00 --> 01:36:03
Again machine learning techniques
can also be used here.

115
01:36:03 --> 01:36:07
Because the term data science became
popular at the same time that big

116
01:36:07 --> 01:36:12
data began appearing, data science usually
refers to extracting meaning from big data

117
01:36:12 --> 01:36:16
and so includes approaches for collecting,
storing and managing big data.

118
01:36:17 --> 01:36:19
These terms evolved at different times and

119
01:36:19 --> 01:36:22
may have encompassed
different sets of activities.

120
01:36:22 --> 01:36:25
But there have always been more
similarities than differences between

121
01:36:25 --> 01:36:26
them.

122
01:36:26 --> 01:36:28
Now they are often used
interchangeably and

123
01:36:28 --> 01:36:31
have come to mean
essentially the same thing.

124
01:36:31 --> 01:36:36
The process of extracting valuable insight
from data, the core algorithms and

125
01:36:36 --> 01:36:39
techniques for doing this do not
change with different terms.

126
01:36:41 --> 01:36:45
To summarize, in this lecture we've
discussed what machine learning is and

127
01:36:45 --> 01:36:47
how it is being used.

128
01:36:47 --> 01:36:51
Machine learning models learn from data to
perform a task without being explicitly

129
01:36:51 --> 01:36:52
programmed.

130
01:36:52 --> 01:36:55
They are used to discover patterns and
trends in the data.

131
01:36:55 --> 01:37:00
And a lab for data driven decisions to
be made for the problems being studied.

132
01:37:00 --> 01:37:04
We also discuss in this lecture examples
of how machine learning is being used.

133
01:37:04 --> 01:37:07
We see how the example applications
that machine learning can be applied

134
01:37:07 --> 01:37:09
to many different areas.

1
03:06:13 --> 03:06:15
What are the steps in
the Machine Learning Process?

2
03:06:15 --> 03:06:17
We will discuss those in this lecture.

3
03:06:18 --> 03:06:23
After this video, you will be able
to identify the steps in the machine

4
03:06:23 --> 03:06:27
learning process and discuss why
the machine learning process is iterative.

5
03:06:29 --> 03:06:33
This diagram illustrates the steps
in the machine learning process.

6
03:06:33 --> 03:06:36
In this lecture,
we will cover an overview of these steps.

7
03:06:36 --> 03:06:39
In the next lecture,
we will cover each step in more detail.

8
03:06:41 --> 03:06:45
It should be kept in mind that all of
these steps need to be carried out with

9
03:06:45 --> 03:06:47
a clear purpose in mind.

10
03:06:47 --> 03:06:51
That is, the problem or opportunity that
is being addressed must be defined with

11
03:06:51 --> 03:06:54
clearly stated goals and objectives.

12
03:06:54 --> 03:06:55
For example,

13
03:06:55 --> 03:06:59
the purpose of a project may be to study
customer purchasing behavior to come up

14
03:06:59 --> 03:07:04
with a more effective marketing strategy
in order to increase sales revenue.

15
03:07:04 --> 03:07:08
The purpose behind the project will
drive the machine learning process.

16
03:07:10 --> 03:07:12
The first step in the machine
learning process is to get

17
03:07:12 --> 03:07:16
all available data related
to the problem at hand.

18
03:07:16 --> 03:07:20
Here, we need to identify all data
sources, collect the data, and

19
03:07:20 --> 03:07:22
finally integrate data from
these multiple sources.

20
03:07:24 --> 03:07:28
The next step in the machine learning
process is to prepare the data.

21
03:07:29 --> 03:07:33
This step is further divided into two
parts, explore data and pre-process data.

22
03:07:35 --> 03:07:39
The first part of data preparation
involves preliminary exploration

23
03:07:39 --> 03:07:43
of the data to understand the nature
of the data that we have to work with.

24
03:07:44 --> 03:07:48
Things we want to understand about
the data are its characteristics, format,

25
03:07:48 --> 03:07:49
and quality.

26
03:07:50 --> 03:07:54
A good understanding of the data
leads to a more informed analysis and

27
03:07:54 --> 03:07:55
a more successful outcome.

28
03:07:57 --> 03:08:01
Once we know more about the data
through exploratory analysis,

29
03:08:01 --> 03:08:05
the next part is pre-processing
of the data for analysis.

30
03:08:05 --> 03:08:09
This includes cleaning data,
selecting the variables to use, and

31
03:08:09 --> 03:08:13
transforming data to make the data more
suitable for analysis in the next step.

32
03:08:14 --> 03:08:18
The prepared data then would be
passed on to the analysis step.

33
03:08:18 --> 03:08:22
This step involves selecting
the analytical techniques to use,

34
03:08:22 --> 03:08:25
building a model using the data,
and assessing the results.

35
03:08:26 --> 03:08:30
Step four in the machine learning
process it to communicate results.

36
03:08:30 --> 03:08:35
This includes evaluating the results with
respect to the goal set for the project.

37
03:08:35 --> 03:08:38
presenting the results in
a easy to understand way and

38
03:08:38 --> 03:08:40
communicating the results to others.

39
03:08:40 --> 03:08:44
The last step is to apply the results.

40
03:08:44 --> 03:08:47
This brings us back to
the purpose of the project.

41
03:08:47 --> 03:08:52
How can the insights from our analysis
be used to provide effective marketing

42
03:08:52 --> 03:08:53
to increase sales revenue?

43
03:08:53 --> 03:08:57
Determining actions from insights gained

44
03:08:57 --> 03:09:00
from analysis is the main
focus of the act step.

45
03:09:00 --> 03:09:04
Note that the machine learning
process is a very iterative one.

46
03:09:04 --> 03:09:07
Findings from one step may require
a previous step to be repeated

47
03:09:07 --> 03:09:09
with new information.

48
03:09:09 --> 03:09:14
For example, during the prepare step, we
may find some data quality issues that may

49
03:09:14 --> 03:09:18
require us to go back to the acquire
step to address some issues with data

50
03:09:18 --> 03:09:22
collection or to get additional data that
we didn't include in the first go around.

51
03:09:23 --> 03:09:26
Each step may also require
several iterations.

52
03:09:26 --> 03:09:31
For example, it is common to try different
analysis techniques in the analyze step

53
03:09:31 --> 03:09:33
in order to get reasonable
results from the model.

54
03:09:34 --> 03:09:38
So, it is important to recognize that
this is a highly iterative process, and

55
03:09:38 --> 03:09:39
not a linear one.

1
06:15:51 --> 06:15:56
Now that Maya has explained
the basics of machine learning,

2
06:15:56 --> 06:16:01
let's remember how big data
influences analytical applications.

3
06:16:01 --> 06:16:06
And how we can take advantage of
the existing big data tools and

4
06:16:06 --> 06:16:08
techniques in machine learning.

5
06:16:08 --> 06:16:11
How can machine learning
algorithms be scaled up

6
06:16:11 --> 06:16:15
to process large volumes of data?

7
06:16:15 --> 06:16:16
Let's talk about that now.

8
06:16:18 --> 06:16:21
After this video, you will be able to

9
06:16:21 --> 06:16:25
explain how machine learning
techniques can scale up to big data.

10
06:16:26 --> 06:16:31
And discuss the role of distributed
computing platforms like Hadoop and

11
06:16:31 --> 06:16:34
Spark in applying machine
learning to big data.

12
06:16:35 --> 06:16:39
With the massive amounts of data
that need to be processed for

13
06:16:39 --> 06:16:43
applications, such as drug
effectiveness analysis,

14
06:16:43 --> 06:16:48
climate monitoring, and
website recommendations to name a few.

15
06:16:49 --> 06:16:54
We need to be able to add scalability
to machine learning techniques.

16
06:16:54 --> 06:16:56
How do we apply machine learning at scale?

17
06:16:57 --> 06:17:02
One way, is to scale up by
adding more memory, processors,

18
06:17:02 --> 06:17:08
and storage to our system so
that it can store and process more data.

19
06:17:08 --> 06:17:10
This is not the big data approach.

20
06:17:11 --> 06:17:16
Specialized hardware such as
graphical processing units, or

21
06:17:16 --> 06:17:20
GPUs for short,
can also be added to speed up the miracle

22
06:17:20 --> 06:17:24
operations common in machine
learning algorithms.

23
06:17:24 --> 06:17:28
Although this is a good approach,
this is also not the big data approach.

24
06:17:29 --> 06:17:35
As we learned in our introductory course,
one problem with this approach

25
06:17:35 --> 06:17:39
is that larger specialized
hardware can be very costly.

26
06:17:41 --> 06:17:46
Another problem is that we
eventually will hit a limit.

27
06:17:46 --> 06:17:49
There's only so
much hardware you can add to a machine.

28
06:17:50 --> 06:17:54
An alternative approach is to scale out.

29
06:17:55 --> 06:18:01
This means using many local commodity
distribution systems together.

30
06:18:02 --> 06:18:07
Data is distributed over these
systems to gain processing speed up.

31
06:18:09 --> 06:18:16
As shown in this illustration the idea is
to divide the data into smaller subsets.

32
06:18:16 --> 06:18:22
The same processing is applied to
each subset, or map, and the results

33
06:18:22 --> 06:18:27
are merged at the end to come up with the
overall results for the original dataset.

34
06:18:28 --> 06:18:34
Let's consider an example,
where we want to apply the same operation

35
06:18:34 --> 06:18:39
to all the samples in
a dataset of N samples.

36
06:18:39 --> 06:18:42
In this case, N is four.

37
06:18:42 --> 06:18:47
If it takes T time units to perform this

38
06:18:47 --> 06:18:52
operation on each sample,
then with sequential processing

39
06:18:52 --> 06:18:57
the time to apply that operation
to all samples, is N times T.

40
06:18:58 --> 06:19:01
If we have a cluster of four processors,

41
06:19:01 --> 06:19:05
we can distribute the data
across the four processors.

42
06:19:07 --> 06:19:14
Each process performs the operation on
the dataset subset of N over four samples.

43
06:19:16 --> 06:19:21
Processing of the four subsets
of the data is done in parallel.

44
06:19:21 --> 06:19:25
That is, the subsets
are processed at the same time.

45
06:19:27 --> 06:19:33
The processing time for the distributed
approach is approximately N over 4 times T

46
06:19:35 --> 06:19:41
plus any overhead required to merge
the subset results and maybe shuffle them.

47
06:19:43 --> 06:19:48
This is a speedup of nearly four
times over the sequential approach.

48
06:19:50 --> 06:19:54
On a distributed computing
platform such as Spark or Hadoop,

49
06:19:54 --> 06:19:59
scalable machine learning algorithms
use the same scale out approach.

50
06:19:59 --> 06:20:03
Data is distributed across
different processors,

51
06:20:03 --> 06:20:08
which operate on the data subsets
in parallel using map, reduce, and

52
06:20:08 --> 06:20:11
other distributed
parallel transformations.

53
06:20:12 --> 06:20:13
This allows for

54
06:20:13 --> 06:20:17
machine learning techniques to be
applied to large volumes of data.

55
06:20:18 --> 06:20:24
In this course, we will use Spark and
its scalable machine learning library,

56
06:20:24 --> 06:20:29
MLF, to show you how machine
learning can be applied to big data.

57
06:20:29 --> 06:20:30
And don't forget,

58
06:20:30 --> 06:20:35
this is the processing of the machine
learning on where the data resides.

59
06:20:35 --> 06:20:38
And we call this, the Big Data Approach.

60
06:20:39 --> 06:20:45
However, you can also imagine
a scenario where you also

61
06:20:45 --> 06:20:50
update the machine learning
algorithms to scale up.

62
06:20:50 --> 06:20:54
So you can paralyze the machine
learning algorithms themselves,

63
06:20:54 --> 06:20:58
and also use processing of big
data together with this approach.

1
12:36:49 --> 12:36:54
In this video, we will provide you with
a quick summary of the main points

2
12:36:54 --> 12:36:58
from our first three courses to
recall what you have learned.

3
12:36:58 --> 12:37:02
If you have just completed our third
course and do not need a refresher,

4
12:37:02 --> 12:37:04
you might skip to the next lecture.

5
12:37:07 --> 12:37:12
We started our first course explaining
how a new torrent of big data

6
12:37:12 --> 12:37:17
combined with cloud computing
capabilities to process data anytime and

7
12:37:17 --> 12:37:22
anywhere has been at the core of
the launch of the big data era.

8
12:37:22 --> 12:37:26
Such capabilities enable or
present opportunities for

9
12:37:26 --> 12:37:32
many dynamic data-driven applications,
including energy management,

10
12:37:32 --> 12:37:37
smart cities, precision medicine,
and smart manufacturing.

11
12:37:38 --> 12:37:43
These applications are increasingly
more data-driven, dynamic and

12
12:37:43 --> 12:37:47
heterogeneous in terms of
their technology needs.

13
12:37:47 --> 12:37:50
They're also more process-driven and

14
12:37:50 --> 12:37:55
need to be tackled using
a collaborative approach by a team that

15
12:37:55 --> 12:37:59
puts value on accountability and
reproducibility of the results.

16
12:38:00 --> 12:38:06
Overall, by modeling,
managing, integrating diverse

17
12:38:06 --> 12:38:11
data streams we add value
to our big data and

18
12:38:11 --> 12:38:15
improve our business even more
before we start analyzing it.

19
12:38:16 --> 12:38:18
A part of modeling and

20
12:38:18 --> 12:38:23
managing big data is focusing on
the dimensions of the scalability and

21
12:38:23 --> 12:38:28
considering the challenges associated with
these dimensions to pick the right tools.

22
12:38:29 --> 12:38:34
We also talked about characteristics
of big data, referring to some Vs

23
12:38:34 --> 12:38:40
like volume, variety, velocity,
veracity and valence.

24
12:38:41 --> 12:38:47
Each week presents a challenging
dimension of big data,

25
12:38:47 --> 12:38:55
namely size, complexity, speed,
quality and connectedness.

26
12:38:55 --> 12:38:57
We also added a sixth V,

27
12:38:57 --> 12:39:01
value, referring to the real reason
we are interested in big data.

28
12:39:03 --> 12:39:07
To turn it into an advantage in the
context of a problem using data science

29
12:39:07 --> 12:39:10
techniques, big data needs to be analyzed.

30
12:39:12 --> 12:39:19
We explained a five steps process for data
science that includes data acquisition,

31
12:39:19 --> 12:39:23
modeling, management,
integration, and analysis.

32
12:39:24 --> 12:39:27
The influence of big data pushes for

33
12:39:27 --> 12:39:31
alternative scalability approaches
at each step of the process.

34
12:39:33 --> 12:39:39
If we just focus on the scalability
challenges related to the three Vs,

35
12:39:39 --> 12:39:42
we can say big data has varying volume and

36
12:39:42 --> 12:39:48
velocity, requiring dynamic and
scalable batch and stream processing.

37
12:39:49 --> 12:39:54
Big data has variety, requiring management
of data in many different data systems,

38
12:39:54 --> 12:39:56
and integration of it at scale.

39
12:39:58 --> 12:40:01
In our introduction to
the big data course,

40
12:40:01 --> 12:40:05
we talked about the version of a layer
diagram for the tools in the Hadoop

41
12:40:05 --> 12:40:09
ecosystem, organized vertically
based on the interface.

42
12:40:11 --> 12:40:16
Low level interfaces for storage and
scheduling on the bottom

43
12:40:17 --> 12:40:21
and high level languages and
interactivity at the top.

44
12:40:22 --> 12:40:27
Most of the tools in the Hadoop ecosystem
were initially built to compliment

45
12:40:27 --> 12:40:32
the capabilities of Hadoop for distributed
file system management using HDFS.

46
12:40:34 --> 12:40:37
Data processing using
the MapReduce engine, and

47
12:40:37 --> 12:40:41
resource scheduling, and
negotiation using the YARN engine.

48
12:40:43 --> 12:40:46
Over time,
a number of new projects were built,

49
12:40:46 --> 12:40:52
either to add to these complementary
tools or to handle additional types of

50
12:40:52 --> 12:40:57
big data management and processing not
available in Hadoop, just like Spark.

51
12:40:59 --> 12:41:04
Arguably, the most important
change to Hadoop over time

52
12:41:04 --> 12:41:08
was the separation of YARN from
the MapReduce programming model

53
12:41:08 --> 12:41:11
to solely handle resource
management concerns.

54
12:41:13 --> 12:41:18
This allowed for Hadoop to be extensible
to different programming models and enable

55
12:41:18 --> 12:41:23
the development of a number of processing
engines for batch and stream processing.

56
12:41:25 --> 12:41:29
Another way to look at the vast number of
tools that have been added to the Hadoop

57
12:41:29 --> 12:41:33
ecosystem is from the point of
view of their functionality

58
12:41:33 --> 12:41:34
in the big data processing pipeline.

59
12:41:36 --> 12:41:41
Simply put, these are associated
with three distinct layers for

60
12:41:41 --> 12:41:46
data management and storage,
for data processing and

61
12:41:46 --> 12:41:49
for resource coordination and
workflow management.

62
12:41:51 --> 12:41:57
In our second course, we talked in detail
about the bottom layer in this diagram,

63
12:41:57 --> 12:41:59
namely data management and storage.

64
12:42:01 --> 12:42:07
While this layer includes Hadoop's HDFS,
there are a number of other systems

65
12:42:07 --> 12:42:14
that rely on HDFS as a file system or
implement their own no-SQL storage option.

66
12:42:14 --> 12:42:19
As big data can have a variety of
structured, semi-structured, and

67
12:42:19 --> 12:42:24
unstructured formats and
gets analyzed through a variety of tools,

68
12:42:24 --> 12:42:27
many tools were introduced to
fit this variety of needs.

69
12:42:29 --> 12:42:32
We call these big data management systems.

70
12:42:33 --> 12:42:37
We reviewed Redis and Aerospike as

71
12:42:37 --> 12:42:42
key value stores where each data item
is identified with a unique key.

72
12:42:44 --> 12:42:49
We also got some practical
experience with Lucene and

73
12:42:49 --> 12:42:53
Gephi as vector and
graph-stores respectively.

74
12:42:55 --> 12:42:59
We also talked about Vertica as
a column-store database where

75
12:42:59 --> 12:43:03
information is stored in
columns rather than rows.

76
12:43:05 --> 12:43:09
Cassandra and
HBase are also in this category.

77
12:43:09 --> 12:43:16
Finally, we introduced Solr and
Asterisk DB for managing unstructured and

78
12:43:16 --> 12:43:21
semi-structured text and
MongoDB as a document store.

79
12:43:24 --> 12:43:29
The processing layer is where
all these different types

80
12:43:29 --> 12:43:34
of data get retrieved,
integrated, and analyzed,

81
12:43:34 --> 12:43:38
which was the primary
focus of our third course.

82
12:43:38 --> 12:43:41
In the integration and processing layer,

83
12:43:41 --> 12:43:46
we roughly refer to the tools that
are built on top of HTFS and YARN,

84
12:43:46 --> 12:43:50
although some of them were with
other storage and file systems.

85
12:43:52 --> 12:43:59
YARN is a significant enable of many of
these tools making a number of batch and

86
12:43:59 --> 12:44:05
stream processing engines like Storm,
Spark, Flink and Beam possible.

87
12:44:05 --> 12:44:09
This layer also includes tools
like Hive and Spark SQL for

88
12:44:09 --> 12:44:13
bringing a query interface on top
of the storage layer, Pig for

89
12:44:13 --> 12:44:19
scripting simple big data pipelines
using the MapReduce framework and

90
12:44:19 --> 12:44:22
a number of specialized
analytical libraries,

91
12:44:22 --> 12:44:25
formation learning, and graph analytics.

92
12:44:25 --> 12:44:32
Giraph and GraphX of Spark are examples
of such libraries for graph processing.

93
12:44:32 --> 12:44:35
Mahout on top of the Hadoop stack and

94
12:44:35 --> 12:44:39
MLlib of Spark Are two options for
machine learning.

95
12:44:40 --> 12:44:44
Although we had a basic overview
of graph processing and

96
12:44:44 --> 12:44:48
machine learning for big data
analytics earlier in our second and

97
12:44:48 --> 12:44:52
third courses,
we haven't gone into the details there.

98
12:44:52 --> 12:44:58
In this course, we will use Spark's MLlib
as one of our two main tools,

99
12:44:58 --> 12:45:04
providing a deeper introduction to
the machine learning library of Spark.

100
12:45:04 --> 12:45:11
The third and top layer in our diagram is
the coordination and management layer.

101
12:45:11 --> 12:45:14
This is where integrations,
scheduling, coordination, and

102
12:45:14 --> 12:45:19
monitoring of applications across many
tools in the bottom two layers take place.

103
12:45:20 --> 12:45:25
This layer is also where the results of
the big data analysis get communicated

104
12:45:25 --> 12:45:31
to other programs, websites, visualization
tools and business intelligence tools.

105
12:45:32 --> 12:45:37
Workflow management systems help to
develop automated solutions that

106
12:45:37 --> 12:45:42
can manage and coordinate the process
of combining data management and

107
12:45:42 --> 12:45:49
analytical tasks in a big data pipeline as
a configurable, structured set of steps.

108
12:45:50 --> 12:45:53
Workflow driven thinking also matches

109
12:45:53 --> 12:45:57
this basic process of data science
that we overviewed before.

110
12:45:57 --> 12:46:01
Oozie is an example workflow
scheduler that can interact with

111
12:46:01 --> 12:46:04
many of the tools in the integration and
processing layer.

112
12:46:05 --> 12:46:10
Zookeeper is the resource
coordination tool which monitors and

113
12:46:10 --> 12:46:15
manages and coordinates all these
tools and named after animal.

114
12:46:17 --> 12:46:21
Now that we've reviewed all three
layers we are ready to come back to

115
12:46:21 --> 12:46:27
the integration and processing layer, but
now in the context of machine learning.

116
12:46:27 --> 12:46:31
In which we will use machine
learning techniques to

117
12:46:31 --> 12:46:35
apply to our five step data science
process and analyze big data.

118
12:46:37 --> 12:46:43
Just a simple Google search for
big data processing pipelines will bring

119
12:46:43 --> 12:46:47
a vast number of pipelines with
a large number of technologies

120
12:46:47 --> 12:46:52
that support scalable data cleaning,
preparation, and analysis.

121
12:46:54 --> 12:46:58
How do we make sense of all of it to
make sure we use the right tools for

122
12:46:58 --> 12:46:59
our application?

123
12:47:00 --> 12:47:03
How do we pick the right
pre-processing and

124
12:47:03 --> 12:47:06
machine learning techniques to
start doing predictive modeling?

125
12:47:08 --> 12:47:13
Over the next few weeks Dr.
Mei will walk you through some of the most

126
12:47:13 --> 12:47:18
fundamental machine learning techniques,
along with introductory hands

127
12:47:18 --> 12:47:24
on exercises we designed for you to ease
you into the world of machine learning.

128
12:47:24 --> 12:47:25
Let's get started.

1
01:24:15 --> 01:24:18
Let's go through an overview of the tools
we will be using in this course.

2
01:24:20 --> 01:24:24
After this video you will be
able to describe what KNIME is.

3
01:24:24 --> 01:24:30
Describe what Spark MLlib is, and contrast
KNIME and MLlib as machine learning tools.

4
01:24:32 --> 01:24:35
The machine learning tools that we will
be using in this course are KNIME and

5
01:24:35 --> 01:24:37
Spark MLlib.

6
01:24:37 --> 01:24:39
These are both open source tools.

7
01:24:39 --> 01:24:42
This lecture will introduce
these tools to you.

8
01:24:42 --> 01:24:45
You will need to use them for
the hands on activities in this course.

9
01:24:47 --> 01:24:53
KNIME Analytics is a platform for data
analytics, reporting, and visualization.

10
01:24:53 --> 01:24:58
The KNIME platform uses a graphical user
interface based approach with drag and

11
01:24:58 --> 01:25:02
drop features to facilitate
constructing an analytic solution.

12
01:25:02 --> 01:25:05
The basic components in KNIME
are referred to as nodes.

13
01:25:06 --> 01:25:09
Each node provides
a specific functionalities,

14
01:25:09 --> 01:25:11
such as reading in a file,

15
01:25:11 --> 01:25:15
creating a specific type of machine
running model and generating a plot.

16
01:25:17 --> 01:25:20
Nodes can be connected to create
machine running workflows or pipelines.

17
01:25:22 --> 01:25:25
KNIME stands for
Konstanz Information Miner.

18
01:25:25 --> 01:25:29
The Konstanz is for
the University of Konstanz in Germany.

19
01:25:29 --> 01:25:32
And note that the K in KNIME is
silent in the pronunciation.

20
01:25:35 --> 01:25:38
In KNIME you assemble the steps that
need to be performed in a machine

21
01:25:38 --> 01:25:42
learning process by connecting
nodes to create a workflow.

22
01:25:43 --> 01:25:47
To create a workflow the user
chooses the appropriate nodes

23
01:25:47 --> 01:25:51
from the node repository and
assembles them into a workflow.

24
01:25:51 --> 01:25:55
The workflow can then be
executed in the KNIME work bench.

25
01:25:57 --> 01:26:01
A node implements a specific
operation in a workflow.

26
01:26:01 --> 01:26:03
In this screenshot we see two nodes.

27
01:26:03 --> 01:26:07
The file reader node is used to
read data from a text file or

28
01:26:07 --> 01:26:09
a URL or a web address.

29
01:26:09 --> 01:26:13
The decision tree learner node
builds a decision tree model.

30
01:26:14 --> 01:26:20
Each node can have input and output ports
and can be connected to other nodes.

31
01:26:20 --> 01:26:23
When a node is executed,
it takes data from its input port,

32
01:26:23 --> 01:26:29
performs some operations on the data and
writes the results to the output port.

33
01:26:29 --> 01:26:32
Data is transferred between
nodes that are connected.

34
01:26:32 --> 01:26:37
A node can be configured by opening
up its configuration dialog.

35
01:26:37 --> 01:26:39
This is where the parameters for
the node can be set.

36
01:26:41 --> 01:26:45
The Node Repository is where you will find
all the nodes available in your KNIME

37
01:26:45 --> 01:26:47
installation.

38
01:26:47 --> 01:26:50
The nodes are organized by categories.

39
01:26:50 --> 01:26:55
KNIME provides an array of nodes to
perform operations for data access,

40
01:26:55 --> 01:26:59
data manipulation, analysis,
visualization, and reporting.

41
01:27:01 --> 01:27:06
As you can see, KNIME provides
a visual approach to machine learning.

42
01:27:06 --> 01:27:11
It's GUI-based, drag-and-drop approach
provides an easy way to create and

43
01:27:11 --> 01:27:13
execute a machine learning workflow.

44
01:27:14 --> 01:27:18
The open source version of KNIME however
is limited in how large of a dataset

45
01:27:18 --> 01:27:19
it can handle.

46
01:27:20 --> 01:27:24
There are commercial extensions to
KNIME to manage large datasets and

47
01:27:24 --> 01:27:28
offer other extra functionalities, but
these extensions are not open source.

48
01:27:30 --> 01:27:32
Now let's talk about Spark MLlib.

49
01:27:32 --> 01:27:35
You've worked with Spark before if
you took the third course in this

50
01:27:35 --> 01:27:36
specialization on big data.

51
01:27:37 --> 01:27:40
Spark is a distributed computing platform.

52
01:27:41 --> 01:27:44
MLlib is a scalable machine learning
library that runs on top of Spark.

53
01:27:46 --> 01:27:50
It provides distributed implementations of
commonly used machine learning algorithms

54
01:27:50 --> 01:27:51
and utilities.

55
01:27:52 --> 01:27:55
The ML and MLlib of course stands for
machine learning.

56
01:27:57 --> 01:28:03
To implement machine learning operations
in Spark MLlib, you need to write code.

57
01:28:03 --> 01:28:06
So MLlib is not a GUI-based approach.

58
01:28:06 --> 01:28:10
This segment of code reads and
parses data from a file,

59
01:28:10 --> 01:28:14
then builds a decision
tree classification model.

60
01:28:14 --> 01:28:19
MLlib, as with the base Spark core,
provides an application programming

61
01:28:19 --> 01:28:25
interface, or API, for
Java, Python, Scala and R.

62
01:28:25 --> 01:28:29
This means that you can write code in
these programming languages to execute

63
01:28:29 --> 01:28:31
the base operations provided in Spark.

64
01:28:33 --> 01:28:36
Spark MLlib runs on
a distributed platform.

65
01:28:36 --> 01:28:38
It provides machine
learning algorithms and

66
01:28:38 --> 01:28:42
techniques that are implemented
using distributed processing.

67
01:28:42 --> 01:28:46
So MLlib is used for processing and
analyzing large datasets.

68
01:28:47 --> 01:28:51
And as we have discussed, writing code is
required to implement operations in MLlib.

69
01:28:53 --> 01:28:58
In summary, KNIME is a GUI-based machine
learning tool, while Spark MLlib provides

70
01:28:58 --> 01:29:04
a programming-based scalable platform for
processing very large datasets.

71
01:29:04 --> 01:29:08
You will be using both Spark MLlib and
KNIME throughout this course.

72
01:29:08 --> 01:29:12
We have readings and hands-on exercises to
help you get familiar with these popular

73
01:29:12 --> 01:29:15
open source tools for machine learning.

74
01:29:15 --> 01:29:18
I think you will find it very informative
and fun to work with these tools.

1
02:53:33 --> 02:53:37
Welcome to course four of
the big data specialization.

2
02:53:37 --> 02:53:42
I'm Ilkay Altintas, for the new learners
I'm the Chief Data Science Officer

3
02:53:42 --> 02:53:49
at the San Diego Supercomputer Center at
the University of California, San Diego.

4
02:53:49 --> 02:53:53
I feel honored to teach you
the basics of big data modeling,

5
02:53:53 --> 02:53:57
management, and
analysis in this specialization.

6
02:53:57 --> 02:54:01
And to work with Dr.
Mai Nguyen on this class.

7
02:54:01 --> 02:54:05
>> And I'm Mai Nguyen, while most of
you might be familiar with Ilkay,

8
02:54:05 --> 02:54:06
I'm a new face.

9
02:54:06 --> 02:54:11
I'm excited to be here to teach you
what I love doing, machine learning.

10
02:54:11 --> 02:54:14
I received my PhD in computer science
with a focus on machine learning

11
02:54:14 --> 02:54:17
from the University of
California in San Diego.

12
02:54:17 --> 02:54:19
Since then,
I have worked as a data scientist,

13
02:54:19 --> 02:54:22
and instructor of machine
learning in various venues.

14
02:54:22 --> 02:54:25
I am the Lead for Data Analytics at SDFC.

15
02:54:25 --> 02:54:28
In this role, I work on data
science projects doing research

16
02:54:28 --> 02:54:31
on scalability on machine
learning methods to big data.

17
02:54:32 --> 02:54:37
>> We are really happy to have you in this
course to develop your understanding and

18
02:54:37 --> 02:54:40
skills in machine learning.

19
02:54:40 --> 02:54:43
>> And
give you an introductory level experience

20
02:54:43 --> 02:54:45
with application of machine
learning to big data.

21
02:54:46 --> 02:54:51
By now you might have just finished
our first three courses and

22
02:54:51 --> 02:54:58
learned the basics of big data modeling,
management, integration, and processing.

23
02:54:58 --> 02:55:00
If you haven't, it's not required.

24
02:55:00 --> 02:55:04
But for those with less background
in big data management and

25
02:55:04 --> 02:55:06
systems, you might find it valuable.

26
02:55:07 --> 02:55:10
>> We understand that you may
not even have heard anything on

27
02:55:10 --> 02:55:12
machine learning yet.

28
02:55:12 --> 02:55:16
That's why we will start by
discussing what machine learning is.

29
02:55:16 --> 02:55:20
Describing some sample applications and
presenting the typical process

30
02:55:20 --> 02:55:24
of a machine learning project to give
you a sense of what machine learning is.

31
02:55:24 --> 02:55:27
Then we will delve into some
commonly used machine learning

32
02:55:27 --> 02:55:29
techniques like classification and
clustering.

33
02:55:30 --> 02:55:35
>> We are also going to show you how
to explore your data, prepare it for

34
02:55:35 --> 02:55:40
analysis, and evaluate the results you
get with your machine learning model.

35
02:55:42 --> 02:55:46
These are all necessary steps for
a successful machine learning solution.

36
02:55:48 --> 02:55:50
>> As you know, for
many data science applications

37
02:55:50 --> 02:55:55
one has to use many different tools and
methods to analyze data.

38
02:55:55 --> 02:55:58
In fact, keeping up with the rapid
development of new tools is

39
02:55:58 --> 02:56:01
one of the challenges of
today's big data environment.

40
02:56:01 --> 02:56:04
In this course, we will introduce you to
two different types of machine learning

41
02:56:04 --> 02:56:07
tools namely Nime and Spark MNL.

42
02:56:08 --> 02:56:13
Nime is a graphical user interface based
tool that requires no programming.

43
02:56:13 --> 02:56:16
And as representative of
a set of tools used in visual

44
02:56:16 --> 02:56:18
workflow approach to machine learning.

45
02:56:18 --> 02:56:21
You will have hand on practice with
Nime as you go through the exercises in

46
02:56:21 --> 02:56:22
this course.

47
02:56:23 --> 02:56:28
We are also excited to show you
examples of data processing

48
02:56:28 --> 02:56:32
using Sparks Machine Learning library and
OwlWeb.

49
02:56:32 --> 02:56:34
Our goal here is to provide you

50
02:56:34 --> 02:56:39
with simple hands-on exercises that
require introductory level programming,

51
02:56:39 --> 02:56:43
to inspire you on how big data machine
learning tools can be operated.

52
02:56:45 --> 02:56:47
We wish you a fun time learning and

53
02:56:47 --> 02:56:52
hope to hear from you in the discussion
forums and learner stories as always.

54
02:56:53 --> 02:56:56
>> We have suggested time estimates
each week for the course.

55
02:56:56 --> 02:56:59
But feel free to take the course
at a faster or slower pace.

56
02:56:59 --> 02:57:02
And don't forget to connect to other
learners through the forums to

57
02:57:02 --> 02:57:05
enhance your learning experience.

58
02:57:05 --> 02:57:05
>> Happy learning.

59
02:57:05 --> 02:57:06
>> Happy learning.

1
05:50:38 --> 05:50:41
In the last lecture we
discussed data quality issues.

2
05:50:41 --> 05:50:46
We will now discuss some common techniques
for addressing those quality issues.

3
05:50:46 --> 05:50:51
After this video, you will be able
to define what imputation means,

4
05:50:51 --> 05:50:54
illustrate three ways to
handle missing values, and

5
05:50:54 --> 05:50:59
describe the role of domain knowledge
in addressing data quality issues.

6
05:50:59 --> 05:51:03
As we discussed in the last lecture,
real world data is messy.

7
05:51:03 --> 05:51:06
Some data quality issues that you can
find in your data are missing values,

8
05:51:06 --> 05:51:12
duplicate data, invalid data,
noise and outliers.

9
05:51:12 --> 05:51:15
You will need to clean your data if you
want to perform any meaningful analysis

10
05:51:15 --> 05:51:16
on that data.

11
05:51:17 --> 05:51:20
Recall that missing data occurs
when you don't have a value for

12
05:51:20 --> 05:51:23
certain variables in some samples.

13
05:51:23 --> 05:51:27
A simple way to handle missing data is
to simply drop any samples with missing

14
05:51:27 --> 05:51:28
values or NAs.

15
05:51:29 --> 05:51:32
All machine learning tools provide
a mechanism or command for

16
05:51:32 --> 05:51:35
filtering out rows with
any missing values.

17
05:51:35 --> 05:51:38
The advantage of this approach
is that it is very simple.

18
05:51:38 --> 05:51:42
The caveat is that you are removing
data when you filter out examples.

19
05:51:42 --> 05:51:46
If the number of samples dropped is large,
then you end up losing a lot of your data.

20
05:51:47 --> 05:51:50
An alternative to dropping
samples with missing data is to

21
05:51:50 --> 05:51:52
impute the missing values.

22
05:51:52 --> 05:51:56
Imputing means to replace the missing
values with some reasonable values.

23
05:51:57 --> 05:52:02
The advantage of this approach is that
you're making use of all your data.

24
05:52:02 --> 05:52:05
Oc course, imputing is more complicated
than simply dropping samples.

25
05:52:06 --> 05:52:09
There are several ways to
impute missing values.

26
05:52:09 --> 05:52:12
One strategy is to replace
the missing values with the mean or

27
05:52:12 --> 05:52:15
median value of the variable.

28
05:52:15 --> 05:52:20
For example, a missing value for years of
employment can be replaced by the mean or

29
05:52:20 --> 05:52:25
median value for years of employment for
all current employees.

30
05:52:25 --> 05:52:28
Another approach is to use
the most frequent value

31
05:52:28 --> 05:52:30
in place of the missing value.

32
05:52:30 --> 05:52:34
For example, the most frequently
recorded age of customers

33
05:52:34 --> 05:52:39
associated with the specific item can
be used if that value is missing.

34
05:52:39 --> 05:52:43
Alternatively, a sensible value can
be derived as a replacement for

35
05:52:43 --> 05:52:44
a missing value.

36
05:52:44 --> 05:52:47
For example, a missing value for
income can be set to zero for

37
05:52:47 --> 05:52:50
customers less then 18 years old, or

38
05:52:50 --> 05:52:55
it can be replaced with an average
value based on occupation and location.

39
05:52:55 --> 05:52:58
Note that this approach requires
knowledge about the application and

40
05:52:58 --> 05:53:02
the variable with missing values in
order to make reasonable choices

41
05:53:02 --> 05:53:05
about what valuables would be sensible
to replace the missing values.

42
05:53:06 --> 05:53:11
In the case of duplicate data one
approach is to delete the older record.

43
05:53:11 --> 05:53:14
Another approach is to
merge duplicate records.

44
05:53:14 --> 05:53:19
This often requires a way to determine
how to resolve conflicting values.

45
05:53:19 --> 05:53:23
For example, in the case of multiple
addresses for the same customer,

46
05:53:23 --> 05:53:28
some logic for determining similarities
between addresses might be necessary.

47
05:53:28 --> 05:53:31
For example,
St period is the same as Street.

48
05:53:32 --> 05:53:37
To address invalid data, consulting
another data source may be necessary.

49
05:53:37 --> 05:53:40
For example,
an invalid zip code can be corrected

50
05:53:40 --> 05:53:44
by looking up the correct zip
code based on city and state.

51
05:53:44 --> 05:53:49
A best estimate for a reasonable value
can also be used as a replacement.

52
05:53:49 --> 05:53:52
For example, for
a missing age value for an employee,

53
05:53:52 --> 05:53:56
a reasonable value can be estimated based
on the employee's length of employment.

54
05:53:58 --> 05:54:01
Noise that distorts the data
values can be addressed by

55
05:54:01 --> 05:54:04
filtering out the source of the noise.

56
05:54:04 --> 05:54:08
For example, filtering out the frequency
of a constant background noise

57
05:54:08 --> 05:54:11
will remove that noise
component from a recording.

58
05:54:11 --> 05:54:14
This filtering must be
done with care however,

59
05:54:14 --> 05:54:18
as it can also remove some components
of the true data in the process.

60
05:54:19 --> 05:54:23
Outliers can be detected through
the use of summary statistics and

61
05:54:23 --> 05:54:24
plots of the data.

62
05:54:24 --> 05:54:27
Outliers can significantly skew
the distribution of your data and

63
05:54:27 --> 05:54:30
thus the results of your analysis.

64
05:54:30 --> 05:54:33
In cases where outliers are not
the focus of your analysis,

65
05:54:33 --> 05:54:37
you will want to remove these
outlier samples from your data set.

66
05:54:37 --> 05:54:39
For example,
when a thermostat malfunctions and

67
05:54:39 --> 05:54:43
causes values to fluctuate wildly,
or to be much higher or

68
05:54:43 --> 05:54:46
lower than normal,
these samples should be filtered out.

69
05:54:46 --> 05:54:51
In some applications, however, outliers
are exactly what you're looking for.

70
05:54:51 --> 05:54:54
So when you detect outliers,
you don't want to throw them out.

71
05:54:54 --> 05:54:57
Instead, you want to
examine them more closely.

72
05:54:57 --> 05:55:01
A classic example of this is in fraud
detection, where outliers represent

73
05:55:01 --> 05:55:05
potential fraudulent use and
those samples should be analyzed closely.

74
05:55:06 --> 05:55:09
In order to address data
quality issues effectively

75
05:55:09 --> 05:55:12
knowledge about
the application is crucial.

76
05:55:12 --> 05:55:15
Things such as how the data was collected,

77
05:55:15 --> 05:55:20
the user population, the intended use
of the application etc, are important.

78
05:55:20 --> 05:55:25
This domain knowledge is essential
to making informed decisions on how

79
05:55:25 --> 05:55:28
to best impute missing values,
how to handle duplicate records and

80
05:55:28 --> 05:55:32
invalid data and what to do about
noise and outliers in your data.

1
11:46:10 --> 11:46:14
This activity we'll be exploring
weather data in Spark.

2
11:46:14 --> 11:46:19
First, we will load weather data from
a CSV file into a Spark DataFrame.

3
11:46:19 --> 11:46:25
Next, we will examine the columns and
schema of the DataFrame.

4
11:46:25 --> 11:46:30
We will then view the summary statistics
and drop rows with missing values.

5
11:46:30 --> 11:46:33
Finally, we will compute
the correlation between two columns.

6
11:46:36 --> 11:46:38
Let's begin.

7
11:46:38 --> 11:46:42
First, we'll create a new
Jupyter Python notebook.

8
11:46:42 --> 11:46:46
You do this by clicking on New and
choosing Python 3.

9
11:46:50 --> 11:46:54
Next, we'll import SQL context.

10
11:46:54 --> 11:46:58
This is done by entering from

11
11:46:58 --> 11:47:02
pyspark.sql import SQLContext.

12
11:47:02 --> 11:47:05
Next, we'll create an instance
of the SQLContext.

13
11:47:06 --> 11:47:13
We'll enter sqlContext = SQLContext(sc).

14
11:47:13 --> 11:47:16
Now let's read our weather
data into a DataFrame.

15
11:47:16 --> 11:47:22
We'll call the DataFrame df and
we'll read it using the sqlContext.

16
11:47:22 --> 11:47:27
We'll enter sqlContext.read.load.

17
11:47:27 --> 11:47:30
The first argument is the URL of the file.

18
11:47:30 --> 11:47:38
That's
file:///home/cloudera/Downloads/big-data--

19
11:47:38 --> 11:47:44
4/daily_weather.csv.

20
11:47:44 --> 11:47:49
The second argument specifies
the format of how to read the file.

21
11:47:49 --> 11:47:54
In this case, we're going to use the
Spark CSV package from Databricks to load

22
11:47:54 --> 11:47:56
the CSV directly into the DataFrame.

23
11:47:56 --> 11:48:01
We need to use this because
the Cloudera image only has Spark 1.

24
11:48:01 --> 11:48:04
In Spark 2 and later,
this package is included so

25
11:48:04 --> 11:48:07
we don't have to use this argument.

26
11:48:07 --> 11:48:11
We'll enter ,format= and

27
11:48:11 --> 11:48:19
the name of the package is
com.databricks.spark.csv.

28
11:48:19 --> 11:48:23
The next argument specifies that the first
line in the CSV file is the header,

29
11:48:23 --> 11:48:29
header='true'.

30
11:48:29 --> 11:48:34
The last argument tells Spark
to try to infer the schema

31
11:48:34 --> 11:48:39
from the CSV header, inferSchema='true'.

32
11:48:39 --> 11:48:41
Run this.

33
11:48:41 --> 11:48:43
Now let's look at our DataFrame.

34
11:48:43 --> 11:48:48
We can run a df.columns to see
the names of all the columns.

35
11:48:48 --> 11:48:52
We can also run df.printSchema to
see the schema of the DataFrame.

36
11:48:56 --> 11:48:59
Next, let's look at the summary
statistics for the data.

37
11:48:59 --> 11:49:02
We can do this using the describe method.

38
11:49:02 --> 11:49:08
We'll run df.describe().show().

39
11:49:12 --> 11:49:16
This shows summary statistics for
all the columns in the DataFrame.

40
11:49:16 --> 11:49:20
There's a lot of information here,
so lets just choose one column.

41
11:49:20 --> 11:49:23
Let's look at air pressure at 9 AM.

42
11:49:23 --> 11:49:28
We can see the summary statistics for
air pressure 9 AM by running

43
11:49:28 --> 11:49:33
df.describe('air_pressure_9am').show().

44
11:49:38 --> 11:49:44
There are five statistics in this output,
the count, the number of rows,

45
11:49:44 --> 11:49:50
the mean, the standard deviation,
and the min and max values.

46
11:49:50 --> 11:49:56
We can see the total number of columns in
the DataFrame by running len(df.columns).

47
11:50:00 --> 11:50:06
We can also see the total number of rows
in the DataFrame by writing df.count.

48
11:50:06 --> 11:50:10
This says that there are 1,095
rows in the DataFrame.

49
11:50:10 --> 11:50:15
However, the summary statistics for
air_pressure_9am,

50
11:50:15 --> 11:50:17
we see the count is 1,092.

51
11:50:19 --> 11:50:22
Summary statistics do not
include rows of missing values.

52
11:50:22 --> 11:50:27
This means that there are three rows in
the air_pressure_9am column that have

53
11:50:27 --> 11:50:28
missing values.

54
11:50:28 --> 11:50:31
We can drop these missing values.

55
11:50:31 --> 11:50:34
Let's create a new DataFrame where
we've dropped these missing values.

56
11:50:36 --> 11:50:39
We'll call the new DataFrame df2.

57
11:50:39 --> 11:50:44
To drop the missing values, we'll enter

58
11:50:44 --> 11:50:52
df.na.drop(subset=['air_pressure_9am']).

59
11:50:52 --> 11:50:57
We can then count the total number of
rows in the new DataFrame, df2.count.

60
11:51:00 --> 11:51:05
We see this value agrees with
our earlier value of 1092.

61
11:51:05 --> 11:51:10
Next, let's compute the correlation
between two columns in the DataFrame.

62
11:51:10 --> 11:51:15
We'll compute the correlation between
rain accumulation and rain duration.

63
11:51:15 --> 11:51:21
To do this, we'll enter df2.stat.corr and

64
11:51:21 --> 11:51:25
then the names of the two columns,

65
11:51:25 --> 11:51:32
rain_accumulation_9am and
rain_duration_9am.

1
23:37:43 --> 23:37:47
Visualizing your data is a very
effective way to explore your data.

2
23:37:47 --> 23:37:50
We'll look at different ways to
visualize your data in this lecture.

3
23:37:50 --> 23:37:51
After this video,

4
23:37:51 --> 23:37:57
you will be able to discuss how plots
can be useful in exploring data,

5
23:37:57 --> 23:38:01
describe how you would use a scatter plot,
and summarize what a boxplot shows.

6
23:38:02 --> 23:38:06
Visualizing data,
that is looking at data graphically,

7
23:38:06 --> 23:38:08
is a great way to explore your data set.

8
23:38:08 --> 23:38:12
Data visualization is a nice complement
to using summary statistics for

9
23:38:12 --> 23:38:13
exploring data.

10
23:38:13 --> 23:38:17
We will cover several ways to
visualize your data in this lecture.

11
23:38:17 --> 23:38:22
There are several types of plots that
you can use to visualize your data.

12
23:38:22 --> 23:38:29
We will go over histogram, line plot,
scatter plot, bar plot, and box plot.

13
23:38:29 --> 23:38:32
These are the most commonly used plots,
but there are many others as well.

14
23:38:33 --> 23:38:37
A histogram is used to display
the distribution of a variable.

15
23:38:37 --> 23:38:42
The range of values for the variable
is divided into the number of bins, and

16
23:38:42 --> 23:38:45
the number of values that fall
into each bin is counted.

17
23:38:45 --> 23:38:47
Which determines the height of each bin.

18
23:38:49 --> 23:38:53
A histogram can reveal many things
about a variable in your data, for

19
23:38:53 --> 23:38:58
example, you can usually determine
the central tendency of a variable,

20
23:38:58 --> 23:39:01
that is where the majority
of the values lie.

21
23:39:01 --> 23:39:03
You can also see the most frequent
value of values for that variable.

22
23:39:05 --> 23:39:09
A histogram also shows whether the values
for that variable are skewed and

23
23:39:09 --> 23:39:12
whether the skewness is to
the left towards smaller values or

24
23:39:12 --> 23:39:14
to the right towards larger values.

25
23:39:15 --> 23:39:19
You can also pick outliers in
the histogram as shown on the bottom plot.

26
23:39:20 --> 23:39:24
A line plot shows how data
values change over time.

27
23:39:24 --> 23:39:28
The values of a variable or
variables are shown on the Y axis and

28
23:39:28 --> 23:39:31
the X axis shows the motion of time.

29
23:39:31 --> 23:39:34
The resulting line displays
the data values over time.

30
23:39:36 --> 23:39:39
A line plot can show
patterns in your variables.

31
23:39:39 --> 23:39:44
For example, a cyclical pattern
can be detected as in this plot,

32
23:39:44 --> 23:39:48
where the values start high,
then decrease and go back up again.

33
23:39:49 --> 23:39:54
Trends can also be detected as
shown in the upper-right plot

34
23:39:54 --> 23:39:58
where the values fluctuate but
show a general upward trend over time.

35
23:39:59 --> 23:40:04
It is also easy to compare how multiple
variables change over time on a single

36
23:40:04 --> 23:40:06
line plot as displayed in
the center bottom plot.

37
23:40:08 --> 23:40:13
A scatter plot is a great way to visualize
the relationship between two variables.

38
23:40:13 --> 23:40:15
One variable is on the x axis.

39
23:40:15 --> 23:40:19
The other variable is on
the y axis Each sample is

40
23:40:19 --> 23:40:24
a product using the values of the 2
variables aspects and Y coordinates.

41
23:40:24 --> 23:40:28
The resulting plot shows how one variable
changes as the other is changed.

42
23:40:29 --> 23:40:34
A scatter plot can be used to display
the correlation between 2 variables.

43
23:40:34 --> 23:40:38
For example, 2 variables such as
the high temperature of the day, and

44
23:40:38 --> 23:40:39
the low temperature of the day,

45
23:40:39 --> 23:40:41
can have a positive correlation
as shown in this plot.

46
23:40:43 --> 23:40:47
A positive correlation means that as
the value of one variable increases,

47
23:40:47 --> 23:40:52
the value of the other variable
also increases by a similar amount.

48
23:40:52 --> 23:40:55
The upper right scatter plot shows
a negative correlation between two

49
23:40:55 --> 23:40:57
variables.

50
23:40:57 --> 23:41:00
This means that as the value
of one variable increases,

51
23:41:00 --> 23:41:04
there is a corresponding decrease in
the other variable, two variables

52
23:41:04 --> 23:41:09
can also have a non-linear correlation
as shown in the lower left plot.

53
23:41:09 --> 23:41:12
This means that a change
in one variable will not

54
23:41:12 --> 23:41:15
always correspond to the same
change in the other variable.

55
23:41:15 --> 23:41:19
This is indicated by the curve in
the scatter plot as opposed to something

56
23:41:19 --> 23:41:21
closer to a straight line for
linear correlation.

57
23:41:22 --> 23:41:25
There can also be no correlation
between two variables.

58
23:41:25 --> 23:41:28
In this case, you will see something
like randomly placed dots as

59
23:41:28 --> 23:41:31
displayed in the lower right plot,
indicating no

60
23:41:31 --> 23:41:35
relationship between how the two variables
change with respect to each other.

61
23:41:36 --> 23:41:40
A bar plot is used to show
the distribution of categorical variables.

62
23:41:40 --> 23:41:44
Recall that a histogram is also used to
look at the distribution of the values

63
23:41:44 --> 23:41:46
of the variable.

64
23:41:46 --> 23:41:49
The difference is that in general,
a histogram is used for

65
23:41:49 --> 23:41:54
numeric variables whereas a bar plot
is used for categorical variables.

66
23:41:54 --> 23:41:59
In a bar chart, the different categories
of a categorical variable is shown along

67
23:41:59 --> 23:42:05
the x-axis, and the count of instances for
each category is displayed on the y-axis.

68
23:42:05 --> 23:42:09
This is an effective way to
compare the different categories.

69
23:42:09 --> 23:42:12
For example, the most frequent
category can be easily determined.

70
23:42:14 --> 23:42:18
A bar plot is also a great way to
compare two categorical variables.

71
23:42:18 --> 23:42:22
For example, this plot compares
two categorical variables.

72
23:42:22 --> 23:42:27
One in blue and the other in orange,
each with three different categories.

73
23:42:27 --> 23:42:29
Here you can see that for
the first category,

74
23:42:29 --> 23:42:31
the blue variable has the higher count,

75
23:42:31 --> 23:42:35
while the orange variable has a higher
count for the second and third category.

76
23:42:35 --> 23:42:39
This type of Bar Plot is
called a Grouped Bar Chart.

77
23:42:39 --> 23:42:42
And the different variables
of products side by side.

78
23:42:43 --> 23:42:47
A different kind of comparison can be
performed using a Stacked Bar chart

79
23:42:47 --> 23:42:50
as seen in a lower right quad.

80
23:42:50 --> 23:42:53
Here, the accounts for the two variables
are stacked on top of each other for

81
23:42:53 --> 23:42:54
each category.

82
23:42:55 --> 23:42:58
With this bar chart, you can
determine that the combined count for

83
23:42:58 --> 23:43:03
the first category is about equal to
the combine count for the second category,

84
23:43:03 --> 23:43:07
while the compliant count for
the third category is much larger.

85
23:43:08 --> 23:43:13
A box plot is another plot that shows
the distribution of a numeric variable,

86
23:43:13 --> 23:43:16
it shows the distribution in a different
format than the histogram, however.

87
23:43:17 --> 23:43:20
This is how a box plot displays
the distribution of values for

88
23:43:20 --> 23:43:25
a variable, the gray portion
in the figure is the box part.

89
23:43:25 --> 23:43:30
The lower and upper boundaries of
the box represent the 25th and

90
23:43:30 --> 23:43:32
75th percentiles respectively.

91
23:43:32 --> 23:43:36
This means that the box represents
the middle 50% of the data,

92
23:43:36 --> 23:43:41
the median is the 50th percentile,
meaning that 50% of

93
23:43:41 --> 23:43:46
the data is greater than its value and
50% of the data is less than this value.

94
23:43:47 --> 23:43:52
The top and bottom lines are the Whiskers
and represent the 10th and

95
23:43:52 --> 23:43:54
90th percentiles respectively.

96
23:43:54 --> 23:43:59
So, 80% of the data are in the region
indicated by the upper extreme and

97
23:43:59 --> 23:44:00
lower extreme.

98
23:44:00 --> 23:44:03
Any data values outside of
this region are outliers and

99
23:44:03 --> 23:44:06
are indicated as single
point on the box plot.

100
23:44:07 --> 23:44:10
Note that there are different
variations of the box plot,

101
23:44:10 --> 23:44:14
with the whiskers representing
different types of extreme values.

102
23:44:14 --> 23:44:18
Box plots provide a compact way to
show how variables are distributed, so

103
23:44:18 --> 23:44:21
they are often used to compare variables.

104
23:44:21 --> 23:44:22
The box plot on the left for

105
23:44:22 --> 23:44:26
example compares the base salary for
two different roles.

106
23:44:26 --> 23:44:30
This plot can quickly provide information
regarding the median value, the range and

107
23:44:30 --> 23:44:33
the spread of the two different variables.

108
23:44:33 --> 23:44:36
We can quickly see that
the median salary for

109
23:44:36 --> 23:44:39
the marketing role is higher
than the research role.

110
23:44:40 --> 23:44:43
We can also see that the variation or
spread of the values for

111
23:44:43 --> 23:44:48
marketing is greater than for research,
due to the larger area of the purple box.

112
23:44:49 --> 23:44:53
A box plot can also show you if
the distribution of the data values is

113
23:44:53 --> 23:44:57
symmetrical, positively skewed or
negatively skewed.

114
23:44:57 --> 23:45:01
Here we see that a box plot can
also be displayed on its side.

115
23:45:01 --> 23:45:04
A symmetric distribution is
indicated if the line in the box

116
23:45:04 --> 23:45:09
which specifies the median,
is in the center of the box.

117
23:45:09 --> 23:45:12
A negative skew is indicated
when the median is to the right

118
23:45:12 --> 23:45:14
of the center of the box.

119
23:45:14 --> 23:45:17
This means that there are more
values that are less than the median

120
23:45:17 --> 23:45:19
than there are values
greater than the median.

121
23:45:20 --> 23:45:24
Similarly, a positive skew is indicated
when the median is to the left

122
23:45:24 --> 23:45:25
of the center of the box.

123
23:45:26 --> 23:45:29
To summarize,
data visualization provides a quick and

124
23:45:29 --> 23:45:31
intuitive way to examine your data.

125
23:45:31 --> 23:45:34
Data visualization should be
used in conjunction with summary

126
23:45:34 --> 23:45:38
statistics that we discussed in
the last lecture to explore data.

127
23:45:38 --> 23:45:42
The different types of plots that we have
covered in this lecture will also be very

128
23:45:42 --> 23:45:45
helpful in communicating your results
throughout your machine learning project.

1
23:23:27 --> 23:23:31
Let's look at how we can use summary
statistics to explore data in more detail.

2
23:23:33 --> 23:23:38
After this video you will be able to
define what a summary statistic is,

3
23:23:38 --> 23:23:41
list three common summary statistics and

4
23:23:41 --> 23:23:44
explain how summary statistics
are useful in exploring data.

5
23:23:46 --> 23:23:51
Summary statistics are quantities
that describe a set of data values.

6
23:23:51 --> 23:23:56
Summary statistics provide a simple and
quick way to summarize a dataset.

7
23:23:56 --> 23:24:00
We will discuss three main
categories of summary statistics.

8
23:24:00 --> 23:24:05
Measures of location or centrality,
measures of spread, and measures of shape.

9
23:24:07 --> 23:24:11
Measures of location are summary
statistics that describe the central or

10
23:24:11 --> 23:24:14
typical value in your dataset.

11
23:24:14 --> 23:24:18
These statistics give a sense of
the middle or center of the dataset.

12
23:24:19 --> 23:24:23
Examples of these are mean,
median and mode.

13
23:24:23 --> 23:24:27
The mean is just the average
of the values in a dataset.

14
23:24:27 --> 23:24:32
The median is the value in the middle if
you sorted the values in your dataset.

15
23:24:32 --> 23:24:37
In a sorted list, half of the values
will be less than the median and

16
23:24:37 --> 23:24:38
half will be greater than the median.

17
23:24:40 --> 23:24:42
If the number of data values is even,

18
23:24:42 --> 23:24:45
then the median is the mean
of the two middle values.

19
23:24:46 --> 23:24:50
The mode is a value that is repeated
more often than any other value.

20
23:24:52 --> 23:24:56
In this example we have
a dataset with ten values.

21
23:24:56 --> 23:24:57
For this dataset,

22
23:24:57 --> 23:25:03
the mean is 51.1 which is the number
of all the values divided by 10.

23
23:25:03 --> 23:25:10
The median is 46, if you sort these
numbers, the middle numbers are 42 and 50.

24
23:25:10 --> 23:25:14
The average of these two numbers is 46.

25
23:25:14 --> 23:25:18
There are two modes for
the this dataset, 42 and 78,

26
23:25:18 --> 23:25:22
since each occurs twice,
more than any other value in the dataset.

27
23:25:24 --> 23:25:29
Measures of spread describe how
dispersed or varied your dataset is.

28
23:25:29 --> 23:25:33
Common measures of spread are minimum,
maximum, range,

29
23:25:33 --> 23:25:37
standard deviation and variance.

30
23:25:37 --> 23:25:40
Minimum and
maximum are of course the smallest and

31
23:25:40 --> 23:25:43
largest values in your
dataset respectively.

32
23:25:43 --> 23:25:48
The range is simply the difference
between the maximum and minimum and

33
23:25:48 --> 23:25:51
tells you how spread out your data is.

34
23:25:51 --> 23:25:54
Standard deviation describes the amount
of variation in your dataset.

35
23:25:55 --> 23:25:59
A low standard deviation value means
that the samples in your dataset

36
23:25:59 --> 23:26:01
tend to be close to the mean.

37
23:26:01 --> 23:26:06
And a high standard deviation value means
that the data samples are spread out.

38
23:26:06 --> 23:26:09
Variance is closely related
to standard deviation.

39
23:26:09 --> 23:26:13
In fact the variance is the square
of the standard deviation.

40
23:26:13 --> 23:26:18
So it also indicates how spread out
the data samples are from the mean.

41
23:26:18 --> 23:26:23
For the same dataset, the range is 66
which is a difference between the largest

42
23:26:23 --> 23:26:27
number which is 87 and
the smallest number which is 21.

43
23:26:27 --> 23:26:31
The variance is 548.767,

44
23:26:31 --> 23:26:36
you can calculate this using
a calculator or a spreadsheet.

45
23:26:36 --> 23:26:42
And the standard deviation is 23.426
which is the square root of the variance.

46
23:26:43 --> 23:26:48
Measures of shape describe the shape
of the distribution of a set of values.

47
23:26:48 --> 23:26:51
Common members of shape are skewness and
kurtosis.

48
23:26:52 --> 23:26:57
Skewness indicates whether the data
values are asymmetrically distributed.

49
23:26:58 --> 23:27:02
A skewness value of around zero
indicates that the data distribution

50
23:27:02 --> 23:27:05
is approximately normal, as shown in
the middle figure in the top diagram.

51
23:27:06 --> 23:27:11
A negative skewness value indicates that
the distribution is skewed to the left,

52
23:27:11 --> 23:27:14
as indicated in the left
figure in the top diagram.

53
23:27:14 --> 23:27:17
A positive skewness
value on the other hand

54
23:27:17 --> 23:27:20
indicates that the data distribution
is skewed to the right.

55
23:27:21 --> 23:27:25
Kurtosis measures the tailedness
of the data distribution or

56
23:27:25 --> 23:27:29
how heavy or
fat the tails of the distribution are.

57
23:27:29 --> 23:27:34
A high kurtosis value describes a
distribution with longer and fatter tails

58
23:27:34 --> 23:27:39
and a higher and sharper central peak,
indicating the presence of outliers.

59
23:27:39 --> 23:27:41
A low kurtosis value on the other hand,

60
23:27:41 --> 23:27:46
describes a distribution with shorter and
lighter tails and lower and

61
23:27:46 --> 23:27:50
broader central peak,
suggesting the lack of outliers.

62
23:27:52 --> 23:27:58
In our age example, the skewness is about
0.3 indicating a slight positive skew.

63
23:27:58 --> 23:28:04
And the kurtosis is -1.2 indicating
a distribution with a low and

64
23:28:04 --> 23:28:06
broad central peak and
shorter and lighter tails.

65
23:28:08 --> 23:28:13
Measures of dependence determine if any
relationship exists between variables.

66
23:28:13 --> 23:28:16
Pairwise correlation is a commonly
used measure of dependence.

67
23:28:17 --> 23:28:22
This is a table that shows pairwise
correlation for a set of variables.

68
23:28:22 --> 23:28:25
Note that correlation applies
only to numerical variables.

69
23:28:26 --> 23:28:31
Correlations is between zero and one,
with zero indicating no correlation,

70
23:28:31 --> 23:28:34
and one indicating a one
to one correlation.

71
23:28:34 --> 23:28:37
So a correlation of
0.89 is very strong and

72
23:28:37 --> 23:28:42
this is expected since a person's height
and weight should be very correlated.

73
23:28:43 --> 23:28:48
The summary statistics we just covered
are useful for numerical variables.

74
23:28:48 --> 23:28:52
For categorical variables, we want to look
at statistics that describe the number of

75
23:28:52 --> 23:28:56
categories and
the frequency of each category.

76
23:28:56 --> 23:28:58
This is done using a contingency table.

77
23:28:59 --> 23:29:02
Here's an example that shows
a distribution of people's pets and

78
23:29:02 --> 23:29:04
their colors.

79
23:29:04 --> 23:29:09
We can see the most common pet is
a dog and least common's a fish.

80
23:29:09 --> 23:29:12
Similarly, black is the most common
color and orange the least common.

81
23:29:12 --> 23:29:18
The contingency table also shows
the distribution between the categories.

82
23:29:18 --> 23:29:23
For example, only fish are orange
while most of the brown pets are dogs.

83
23:29:23 --> 23:29:26
In addition to looking at
the traditional summary statistics for

84
23:29:26 --> 23:29:32
numerical variables, and
category count for categorical variables.

85
23:29:32 --> 23:29:33
For machine learning problems,

86
23:29:33 --> 23:29:37
we also want to examine some additional
statistics to quickly validate the data.

87
23:29:38 --> 23:29:42
One of the first things to
check is the number of rows and

88
23:29:42 --> 23:29:44
the number of columns in your dataset.

89
23:29:44 --> 23:29:48
Does the number of rows match
the expected number of samples?

90
23:29:48 --> 23:29:52
Does the number of columns match
the expected number of variables?

91
23:29:52 --> 23:29:54
These should be very quick and
easy checks.

92
23:29:55 --> 23:29:59
Another easy data validation check is
to look at the values in the first and

93
23:29:59 --> 23:30:03
last few samples in your dataset
to see if they're reasonable.

94
23:30:04 --> 23:30:08
For example, do the temperature values
looks to be in the right units of measure.

95
23:30:10 --> 23:30:12
Do the values for rainfall look correct or

96
23:30:12 --> 23:30:15
are there some values
that look out of place?

97
23:30:15 --> 23:30:18
Are the data types for
your variables correct, for example,

98
23:30:18 --> 23:30:22
is the date field captured as dates or
timestamp.

99
23:30:22 --> 23:30:24
Or is it capture as a string or
numerical value?

100
23:30:24 --> 23:30:28
These will have consequences in how
these fields should be processed.

101
23:30:30 --> 23:30:33
Another important step is to check for
missing values.

102
23:30:33 --> 23:30:37
You need to determine the number
of samples with missing values.

103
23:30:37 --> 23:30:40
You also need to determine
if there are any variables

104
23:30:40 --> 23:30:41
with a high percentage of missing values.

105
23:30:43 --> 23:30:46
Handling missing values is a very
important step in data preparation

106
23:30:46 --> 23:30:48
which we will cover in the next module.

107
23:30:48 --> 23:30:52
Having this information will be very
helpful in determining how missing values

108
23:30:52 --> 23:30:54
should be handled in data preparation.

109
23:30:56 --> 23:31:00
We covered several types of summary
statistics useful for exploring data and

110
23:31:00 --> 23:31:01
machine learning.

111
23:31:01 --> 23:31:06
The statistics provide useful information
about your dataset and should be

112
23:31:06 --> 23:31:09
thoroughly examine if you want to get
a better understanding of your data.

1
22:54:37 --> 22:54:41
Now that we are familiar with some
commonly use terms to describe data,

2
22:54:41 --> 22:54:44
let's look at what data exploration is and
why it's important.

3
22:54:45 --> 22:54:49
After this video,
you will be able to explain why data

4
22:54:49 --> 22:54:54
exploration is necessary, articulate
the objectives of data exploration,

5
22:54:54 --> 22:54:56
list the categories of techniques for
exploring data.

6
22:54:58 --> 22:55:01
Data exploration means
doing some preliminary

7
22:55:01 --> 22:55:03
investigation of your data set.

8
22:55:03 --> 22:55:07
The goal is to gain a better understanding
of the data that you have to work with.

9
22:55:07 --> 22:55:12
If you understand the characteristics of
your data, you can make optimal use of it

10
22:55:12 --> 22:55:15
in whatever subsequent processing and
analysis you do with the data.

11
22:55:16 --> 22:55:21
Note that data exploration is also
called exploratory data analysis, or

12
22:55:21 --> 22:55:22
EDA for short.

13
22:55:23 --> 22:55:25
How do you go about exploring data?

14
22:55:25 --> 22:55:29
There are two main categories of
techniques to explore your data,

15
22:55:29 --> 22:55:33
one based on summary statistics and
the other based on visualization methods.

16
22:55:34 --> 22:55:38
Summary statistics provide important
information that summarizes a set

17
22:55:38 --> 22:55:40
of data values.

18
22:55:40 --> 22:55:42
There are many such statistics.

19
22:55:42 --> 22:55:44
Many of them you have
probably heard of before,

20
22:55:44 --> 22:55:50
such as mean, median,
and standard deviation.

21
22:55:50 --> 22:55:54
These are some very commonly
used summary statistics.

22
22:55:54 --> 22:55:58
A summary statistic provides a single
quantity that summarizes some aspects of

23
22:55:58 --> 22:56:00
the dataset.

24
22:56:00 --> 22:56:04
For example, the mean, is a single
value that describes the average value

25
22:56:04 --> 22:56:07
of the dataset,
no matter how large that dataset is.

26
22:56:07 --> 22:56:12
You can think of the mean as an indicator
of where your dataset is centrally located

27
22:56:12 --> 22:56:15
on a number line, thus summary
statistics provide a simple and

28
22:56:15 --> 22:56:17
quick way to summarize a dataset.

29
22:56:18 --> 22:56:23
Data visualization techniques allow
you to look at your data, graphically.

30
22:56:23 --> 22:56:26
There are several types of plots that
you can use to visualize your data.

31
22:56:26 --> 22:56:31
Some examples are histogram,
line plot, and scatter plot.

32
22:56:33 --> 22:56:37
Each type of plot serves a different
purpose, we will cover the use of plots to

33
22:56:37 --> 22:56:39
visualize your data in
an upcoming lecture.

34
22:56:41 --> 22:56:44
What should you look for
when exploring your data?

35
22:56:44 --> 22:56:46
You use statistics and
visual methods to summarize and

36
22:56:46 --> 22:56:50
describe your dataset, and
some of the things you'll want to look for

37
22:56:50 --> 22:56:53
are correlations,
general trends and outliers.

38
22:56:54 --> 22:56:57
Correlations provide information
about the relation took between

39
22:56:57 --> 22:56:59
variables in your data.

40
22:56:59 --> 22:57:01
By looking at correlations,

41
22:57:01 --> 22:57:05
you may be able to determine that
two variables are very correlated.

42
22:57:05 --> 22:57:09
This means they provide the same or
similar information about your data.

43
22:57:09 --> 22:57:13
Since this contain redundant information,
this suggest that you may want to

44
22:57:13 --> 22:57:16
remove one of the variables
to make the analysis simpler.

45
22:57:17 --> 22:57:21
Trends in your data will reveal
characteristics in your data.

46
22:57:21 --> 22:57:25
For example, you can see where
the majority of the data values lie,

47
22:57:25 --> 22:57:27
whether your data is skilled or

48
22:57:27 --> 22:57:31
not, what the most frequent value or
values are in a date set, etc.

49
22:57:32 --> 22:57:36
Looking at trends in your data can also
reveal that a variable is moving in

50
22:57:36 --> 22:57:41
a certain direction, such as sales revenue
increasing or decreasing over the years.

51
22:57:42 --> 22:57:44
Calculating the minimum, the maximum and

52
22:57:44 --> 22:57:48
range of the data values are basic
steps in exploring your data.

53
22:57:48 --> 22:57:52
Determining outliers is
a also very important.

54
22:57:52 --> 22:57:55
Outliers indicate potential
problems with the data and

55
22:57:55 --> 22:57:57
may need to be eliminated
in some applications.

56
22:57:58 --> 22:57:59
In other applications,

57
22:57:59 --> 22:58:04
outliers represent interesting data points
that should be looked at more closely.

58
22:58:04 --> 22:58:07
In either case, outliers usually
require further examination.

59
22:58:08 --> 22:58:12
In summary, what you get by exploring
your data is a better understanding of

60
22:58:12 --> 22:58:16
the complexity of the data so
you can work with it more effectively.

61
22:58:16 --> 22:58:20
Better understanding in turn will
guide the rest of the process and

62
22:58:20 --> 22:58:23
lead to more informed analysis.

63
22:58:23 --> 22:58:24
Summary statistics and

64
22:58:24 --> 22:58:28
visualization techniques
are essential in exploring your data.

65
22:58:28 --> 22:58:31
This should be used together
to examined a dataset.

66
22:58:31 --> 22:58:32
In the next two lectures,

67
22:58:32 --> 22:58:35
we will look at a specific methods that
you can apply to explore your data.

1
21:53:12 --> 21:53:15
In this lesson you will learn how
to prepare data for analysis.

2
21:53:16 --> 21:53:20
After this video, you will be able
to articulate the importance of data

3
21:53:20 --> 21:53:25
preparation, define the objectives
of data preparation, and

4
21:53:25 --> 21:53:27
list some activities in preparing data.

5
21:53:29 --> 21:53:33
The raw data that you get directly from
your sources is rarely in the format

6
21:53:33 --> 21:53:35
that you can use to perform analysis on.

7
21:53:35 --> 21:53:40
The goal of data preparation is to create
the data that will be used for analysis.

8
21:53:41 --> 21:53:47
This means cleaning the data, and putting
the data in the right format for analysis.

9
21:53:47 --> 21:53:51
The latter generally involves selecting
the appropriate features to use and

10
21:53:51 --> 21:53:53
transforming the data as needed.

11
21:53:54 --> 21:53:57
The data that you've acquired
will likely have many problems.

12
21:53:57 --> 21:54:01
An important part of data preparation is
to clean the data that you have to work

13
21:54:01 --> 21:54:08
with to address what are referred
to as data quality issues.

14
21:54:08 --> 21:54:12
There are many types of
data quality issues,

15
21:54:12 --> 21:54:16
including missing values, duplicate data,

16
21:54:16 --> 21:54:21
inconsistent or invalid data,
noise, and outliers.

17
21:54:21 --> 21:54:25
The problems listed here can negatively
affect the quality of the data and

18
21:54:25 --> 21:54:28
compromise the analysis process and
results.

19
21:54:28 --> 21:54:32
So it is very important to detect and
address these data quality issues.

20
21:54:33 --> 21:54:38
Some techniques to address data quality
issues include removing data records

21
21:54:38 --> 21:54:42
with missing values,
merging duplicate records,

22
21:54:42 --> 21:54:47
generating a best, or at most reasonable,
estimate for invalid values.

23
21:54:47 --> 21:54:51
We will discuss these techniques in
more detail in the next lecture.

24
21:54:51 --> 21:54:56
Since the goal of the step of data
preparation is cleaning the data,

25
21:54:56 --> 21:54:59
it is referred to as Data Cleaning or
Data Cleansing.

26
21:55:01 --> 21:55:04
After the data has been cleaned,
another goal of data preparation is to

27
21:55:04 --> 21:55:07
get the data into the format needed for
the analysis.

28
21:55:07 --> 21:55:12
This step is referred to by many names,
Data Munching, Data Wrangling and

29
21:55:12 --> 21:55:14
Data Preprocessing.

30
21:55:15 --> 21:55:19
The two broad categories of data
wrangling are feature selection and

31
21:55:19 --> 21:55:20
feature transformation.

32
21:55:22 --> 21:55:24
Feature selection involves
deciding on which features

33
21:55:24 --> 21:55:27
to use from the existing
ones available in your data.

34
21:55:28 --> 21:55:31
Features can be removed,
added or combined.

35
21:55:31 --> 21:55:34
Feature transformation involves
changing the format of the data

36
21:55:34 --> 21:55:40
in some way to reduce noise or variability
or make the data easier to analyze.

37
21:55:40 --> 21:55:43
Two common feature transformations
are scaling the data so

38
21:55:43 --> 21:55:48
that all features have the same value
range and reducing the dimensionality,

39
21:55:48 --> 21:55:51
which is effectively the number
of features of the data.

40
21:55:51 --> 21:55:53
We will discuss these techniques
later in this lesson.

41
21:55:55 --> 21:55:59
Data preparation is a very important
part of the machine learning process.

42
21:55:59 --> 21:56:02
It can be a tedious process,
but it is a crucial step.

43
21:56:02 --> 21:56:05
If you do not spend the time and
effort to create good data for

44
21:56:05 --> 21:56:08
the analysis,
you will not get good results,

45
21:56:08 --> 21:56:12
no matter how sophisticated the analysis
technique you are using is.

46
21:56:12 --> 21:56:15
Always remember, garbage in, garbage out.

47
21:56:15 --> 21:56:19
So take the time to prepare your data
if you want good analysis results.

1
19:49:31 --> 19:49:35
A very important part of data preparation
is to assess the quality of your data.

2
19:49:35 --> 19:49:39
We will look at some common data
quality issues in this lecture.

3
19:49:39 --> 19:49:44
After this video, you will be able to
describe three data quality issues,

4
19:49:44 --> 19:49:47
name three reasons for
poor data quality and

5
19:49:47 --> 19:49:51
explain why data quality
issues need to be addressed.

6
19:49:51 --> 19:49:53
Real world data is often very messy, so

7
19:49:53 --> 19:49:57
it's a given fact that you will need
to clean your data by identifying and

8
19:49:57 --> 19:50:00
addressing many issues that
affect the quality of your data.

9
19:50:00 --> 19:50:03
Let's take a closer look at what
these data quality issues are.

10
19:50:04 --> 19:50:07
A very common data quality
issue is missing data.

11
19:50:07 --> 19:50:11
Recall that a sample in your dataset
typically contains several variables or

12
19:50:11 --> 19:50:15
features like name, age and income.

13
19:50:15 --> 19:50:18
For some samples, some of these
variables may not have a value.

14
19:50:18 --> 19:50:23
These are referred to as
missing values in the data.

15
19:50:23 --> 19:50:27
Missing values are also referred
to as N/A for not available.

16
19:50:27 --> 19:50:30
So you will see N/A and
missing values used interchangeably.

17
19:50:31 --> 19:50:35
You may have missing values in your data
if you have an optional field in your

18
19:50:35 --> 19:50:36
data set.

19
19:50:36 --> 19:50:41
For example, the field age is often
an optional field on a survey.

20
19:50:41 --> 19:50:45
Also many people may choose not
to provide a response for income.

21
19:50:45 --> 19:50:50
And so you will end up with missing values
for the variable income in your data set.

22
19:50:50 --> 19:50:54
In some cases, a variable may
not be applicable to all cases.

23
19:50:54 --> 19:50:59
For example, income may not be
applicable to people who are retired or

24
19:50:59 --> 19:51:01
unemployed or to children.

25
19:51:01 --> 19:51:04
So you will not have an entry for
income in all of your samples.

26
19:51:05 --> 19:51:09
You can also have missing values, due to a
data collecting device that malfunctions,

27
19:51:09 --> 19:51:13
a network problem that affects
how the data was transmitted, or

28
19:51:13 --> 19:51:17
something else that goes wrong during
the data collection process itself, or

29
19:51:17 --> 19:51:19
the process of transmitting the data or
storing the data.

30
19:51:21 --> 19:51:25
Duplicate data occurs when your data set
has data objects that are duplicates or

31
19:51:25 --> 19:51:27
new duplicates of one another.

32
19:51:28 --> 19:51:31
An example of this is when there
are two different records for

33
19:51:31 --> 19:51:33
the same customer with
different addresses.

34
19:51:34 --> 19:51:36
This can come about, for example,

35
19:51:36 --> 19:51:40
if a customer's address has changed,
but the second address was simply

36
19:51:40 --> 19:51:45
added to this customer's records instead
of used to update the first address.

37
19:51:45 --> 19:51:48
Duplicate data can occur when
merging data from multiple sources.

38
19:51:49 --> 19:51:50
Invalid or

39
19:51:50 --> 19:51:54
inconsistent data occurs when you have
an impossible value for a variable.

40
19:51:54 --> 19:51:59
Some common examples are when you have
a six digit zip code, the letters AB for

41
19:51:59 --> 19:52:03
state abbreviations, or
a negative numbers for age.

42
19:52:03 --> 19:52:06
These invalid data values can
come about when there is a data

43
19:52:06 --> 19:52:08
entry error during data collection.

44
19:52:08 --> 19:52:11
For example, if you allow people
to type in their zip code and

45
19:52:11 --> 19:52:15
someone accidentally includes an extra
digit to their five digit zip code,

46
19:52:15 --> 19:52:17
then you will end up with
an invalid six digit zipcode.

47
19:52:19 --> 19:52:22
Noise refers to anything
that can distort your data.

48
19:52:22 --> 19:52:25
Noise can be introduced during
the data collection process or

49
19:52:25 --> 19:52:27
data transmission process.

50
19:52:27 --> 19:52:31
An example is buzzing in the background
when an audio message is recorded

51
19:52:31 --> 19:52:34
due to background noise or
a faulty microphone.

52
19:52:34 --> 19:52:38
Another example is an overly bright
image due to an incorrect light

53
19:52:38 --> 19:52:39
exposure setting.

54
19:52:40 --> 19:52:44
An Outlier is a data sample with
values that are considerably different

55
19:52:44 --> 19:52:46
than the rest of the other
data samples in a data set.

56
19:52:47 --> 19:52:51
An example scenario, that can create
Outliers is when there's a sense of

57
19:52:51 --> 19:52:56
failure that causes values being recorded
to be much higher or lower than normal.

58
19:52:56 --> 19:53:00
In this case, you want to remove
the Outliers from your data.

59
19:53:00 --> 19:53:03
In other applications however,
such as fraud detection,

60
19:53:03 --> 19:53:08
outliers are the important samples
that should be examined more closely.

61
19:53:08 --> 19:53:11
So depending on the application,
outliers may need to be removed or

62
19:53:11 --> 19:53:13
be kept for further analysis.

63
19:53:14 --> 19:53:17
If you simply ignore these
data quality issues,

64
19:53:17 --> 19:53:21
any analysis that is performed
will produce misleading results.

65
19:53:21 --> 19:53:24
In addition, some implementations
of analysis techniques

66
19:53:24 --> 19:53:27
cannot handle some of these
problems such as missing values.

67
19:53:28 --> 19:53:31
So, problems that we've discussed
in this lecture need to be

68
19:53:31 --> 19:53:34
addressed before any meaningful
analysis can be performed.

69
19:53:34 --> 19:53:35
We will discuss some techniques for

70
19:53:35 --> 19:53:37
handling data quality
issues in the next lecture.

1
15:43:07 --> 15:43:10
If you have been in a conversation
on machine learning,

2
15:43:10 --> 15:43:14
you have probably heard terms like
feature, sample, and variable.

3
15:43:14 --> 15:43:16
We will be defining some of
those terms in this lecture.

4
15:43:18 --> 15:43:23
After this video, you will be able
to describe what a feature is, and

5
15:43:23 --> 15:43:25
how it relates to a sample.

6
15:43:25 --> 15:43:28
Name some alternative terms for feature.

7
15:43:28 --> 15:43:32
Summarize how a categorical feature
differs from a numerical feature.

8
15:43:35 --> 15:43:37
Before we delve into the methods for
processing and

9
15:43:37 --> 15:43:41
analyzing data, let's first start with
defining some term used to describe data,

10
15:43:41 --> 15:43:43
starting with sample and variable.

11
15:43:45 --> 15:43:49
A sample is an instance or
example of an entity in your data.

12
15:43:49 --> 15:43:52
This is typically a row in your dataset.

13
15:43:52 --> 15:43:55
This figure shows part of a dataset
of values related to weather.

14
15:43:56 --> 15:44:01
Each row is a sample representing
weather data for particular day.

15
15:44:01 --> 15:44:06
The table in the figure shows four samples
of weather data, each for different day.

16
15:44:07 --> 15:44:12
In this table, each sample has
five values associated with it.

17
15:44:12 --> 15:44:16
These values are different
information pieces about the sample

18
15:44:16 --> 15:44:20
such as the sample ID, sample date,

19
15:44:20 --> 15:44:24
minimum temperature, maximum temperature,
and rainfall on that day.

20
15:44:25 --> 15:44:28
We call these different values
variables of the sample.

21
15:44:31 --> 15:44:33
There are many names for
sample and variable.

22
15:44:33 --> 15:44:38
Some other terms for sample that you
might hear in a machine learning

23
15:44:38 --> 15:44:44
context include record, example,
row, instance and observation.

24
15:44:44 --> 15:44:47
It is helpful to realize that all of
these terms mean the same thing in

25
15:44:47 --> 15:44:48
machine learning.

26
15:44:48 --> 15:44:53
That is, they all refer to a specific
example of an entity in your dataset.

27
15:44:54 --> 15:45:00
There are also many names for the term
variable, such as feature, column,

28
15:45:00 --> 15:45:02
dimension, attribute, and field.

29
15:45:02 --> 15:45:06
All of these terms refer to
specific characteristics for

30
15:45:06 --> 15:45:07
each sample in your dataset.

31
15:45:09 --> 15:45:12
An important point to emphasize
about variable is that,

32
15:45:12 --> 15:45:15
they are additional
values with a data type.

33
15:45:15 --> 15:45:18
Each variable has a data
type associated with it.

34
15:45:18 --> 15:45:21
The most common data types are numeric and
categorical.

35
15:45:22 --> 15:45:26
There are other data types as
well such as string and date but

36
15:45:26 --> 15:45:30
we will focus on two of the more common
data types, numeric and categorical.

37
15:45:32 --> 15:45:37
As the name implies, numeric variables
are variables that take on number values.

38
15:45:37 --> 15:45:42
Numeric variables can be measured, and
their values can be sorted in some way.

39
15:45:42 --> 15:45:46
Note that a numeric variable can
take on just integer values or

40
15:45:46 --> 15:45:47
be continuous valued.

41
15:45:47 --> 15:45:51
It can also have just positive numbers,
negative numbers or both.

42
15:45:52 --> 15:45:55
Let's go over some examples
of various numeric variables.

43
15:45:56 --> 15:46:01
A person's height is a positive,
continuous valued number.

44
15:46:01 --> 15:46:07
The score in an exam is a positive number
that range between zero and a 100%.

45
15:46:07 --> 15:46:11
The number of transactions per
hour is a positive integer,

46
15:46:11 --> 15:46:15
whereas the change in a stock price
can be either positive or negative.

47
15:46:17 --> 15:46:20
A variable with labels,
names, or categories for

48
15:46:20 --> 15:46:24
values instead of numbers
are called categorical variables.

49
15:46:25 --> 15:46:29
For example a variable that describes
the color of an item, such as the color of

50
15:46:29 --> 15:46:35
a car, can have values such as red,
silver, blue, white and black.

51
15:46:35 --> 15:46:39
These are non-numeric values
that describes some quality or

52
15:46:39 --> 15:46:40
characteristic of an entity.

53
15:46:41 --> 15:46:46
These values can be thought of as names or
labels that can be sorted into categories.

54
15:46:46 --> 15:46:51
Therefore, categorical variables are also
referred to as qualitative variables, or

55
15:46:51 --> 15:46:52
nominal variables.

56
15:46:54 --> 15:46:59
Some examples of categorical
variables are gender, marital status,

57
15:46:59 --> 15:47:02
type of customer, for example,
teenager, adult, senior.

58
15:47:03 --> 15:47:08
Product categories, for example,
electronics, kitchen, bathroom and

59
15:47:08 --> 15:47:09
color of an item.

60
15:47:11 --> 15:47:15
To summarize, a sample is an instance or
example of an entity in your data.

61
15:47:15 --> 15:47:20
A variable captures a specific
characteristic of each entity.

62
15:47:20 --> 15:47:23
So a sample has many
variables to describe it.

63
15:47:23 --> 15:47:26
Data from real applications
are often multidimensional,

64
15:47:26 --> 15:47:31
meaning that there are many dimensions or
variables describing each sample.

65
15:47:31 --> 15:47:34
Each variable has a data
type associated with it,

66
15:47:34 --> 15:47:38
the most common data types are numeric and
categorical.

67
15:47:38 --> 15:47:41
Note that there are many terms to
describe these data related concepts.

1
07:30:48 --> 07:30:53
The data used in machine learning
processes often have many variables.

2
07:30:53 --> 07:30:56
This is what we call
highly dimensional data.

3
07:30:56 --> 07:31:00
Most of these dimensions may or may not
matter in the context of our application

4
07:31:00 --> 07:31:02
with the questions we are asking.

5
07:31:02 --> 07:31:07
Reducing such high dimensions to
a more manageable set of related and

6
07:31:07 --> 07:31:12
useful variables improves the performance
and accuracy of our analysis.

7
07:31:12 --> 07:31:18
After this video, you will be able to
explain what dimensionality reduction is,

8
07:31:18 --> 07:31:22
discuss the benefits of
dimensionality reduction, and

9
07:31:22 --> 07:31:25
describe how PCA transforms your data.

10
07:31:25 --> 07:31:30
The number of features or variables
you have in your data set determines

11
07:31:30 --> 07:31:34
the number of dimensions or
dimensionality of your data.

12
07:31:34 --> 07:31:38
If your dataset has two features,
than it is two dimensional data.

13
07:31:38 --> 07:31:43
If it has three features than
it has three features and so on.

14
07:31:43 --> 07:31:48
You want to use as many features as
possible to capture the characteristics of

15
07:31:48 --> 07:31:49
your data, but

16
07:31:49 --> 07:31:53
you also don't want the dimension
audio of your data to be too high.

17
07:31:53 --> 07:31:58
As the dimensionality increases, the
problem spaces you're looking at increases

18
07:31:58 --> 07:32:04
requiring substantially more instances
to adequately sample of that space.

19
07:32:04 --> 07:32:06
So as the dimensionality increases,

20
07:32:06 --> 07:32:10
the space that you are looking
at grows exponentially.

21
07:32:10 --> 07:32:14
As the space grows data
becomes increasingly sparse.

22
07:32:14 --> 07:32:18
In this diagram we see how
the problem space grows as

23
07:32:18 --> 07:32:23
the dimensionality
increases from 1 to 2 to 3.

24
07:32:23 --> 07:32:27
In the left plot, we have a one
dimensional space partitioned into four

25
07:32:27 --> 07:32:31
regions each with size of 5 units.

26
07:32:31 --> 07:32:36
The middle plot shows a two
dimensional space with 5x5 regions.

27
07:32:36 --> 07:32:40
The number of regions has
now going from 4 to 16.

28
07:32:40 --> 07:32:46
In the third plot, the problem space is
three dimensional with 5x5x5 regions.

29
07:32:46 --> 07:32:51
The number of regions
increased even more to 64.

30
07:32:51 --> 07:32:56
We see that as the number of dimensions
increases, the number of regions

31
07:32:56 --> 07:33:01
increases exponentially and
the data becomes increasingly sparse.

32
07:33:01 --> 07:33:07
With a small dataset relative to the
problem space, analysis results degrade.

33
07:33:07 --> 07:33:11
In addition, certain calculations used in
analysis become much more difficult to

34
07:33:11 --> 07:33:14
define and calculate effectively.

35
07:33:14 --> 07:33:17
For example, distances between samples
are harder to compare since all samples

36
07:33:17 --> 07:33:20
are far away from each other.

37
07:33:20 --> 07:33:24
All of these challenges represent
the difficulty of dealing with high

38
07:33:24 --> 07:33:29
dimensional data and as referred
to as the curse of dimensionality.

39
07:33:29 --> 07:33:31
To avoid the curse of dimensionality,

40
07:33:31 --> 07:33:34
you want to reduce
the dimensionality of your data.

41
07:33:34 --> 07:33:39
This means finding a smaller subset of
features that can effectively capture

42
07:33:39 --> 07:33:41
the characteristics of your data.

43
07:33:42 --> 07:33:46
Recall from the lecture on feature
selection part of data preparation is to

44
07:33:46 --> 07:33:47
select the features to use.

45
07:33:47 --> 07:33:53
For example, you can a feature that is
very correlated with another feature.

46
07:33:53 --> 07:33:56
Using feature selection
techniques to select assessitive

47
07:33:56 --> 07:33:58
features is one approach to
dimensionality reduction.

48
07:34:00 --> 07:34:04
Another approach to dimensionality
reduction is to mathematically determine

49
07:34:04 --> 07:34:08
the most important dimension to keep and
ignore the rest.

50
07:34:08 --> 07:34:11
The idea is to find a smallest
subset of dimensions that

51
07:34:11 --> 07:34:13
capture the most variation in your data.

52
07:34:14 --> 07:34:19
This reduces the dimensions of the data
while eliminating the relevant features

53
07:34:19 --> 07:34:22
making the subsequent analysis simple.

54
07:34:22 --> 07:34:27
A technique commonly use to find
the subset of most important dimensions is

55
07:34:27 --> 07:34:31
called principal component analysis,
or PCA for short.

56
07:34:31 --> 07:34:35
The goal of PCA is to map the data from
the original high dimensional space

57
07:34:35 --> 07:34:40
to a lower dimensional space that
captures as much of the variation in

58
07:34:40 --> 07:34:42
the data as possible.

59
07:34:42 --> 07:34:43
In other words,

60
07:34:43 --> 07:34:48
PCA aims to find the most useful subset
of dimensions to summarize the data.

61
07:34:50 --> 07:34:52
This plot illustrates the idea behind PCA.

62
07:34:53 --> 07:34:58
Here, we have data samples in a two
dimensional space that is defined

63
07:34:58 --> 07:35:00
by the x axis and the y axis.

64
07:35:00 --> 07:35:05
You can see that most of the variation in
the data lies along the red diagonal line.

65
07:35:05 --> 07:35:09
This means that the dat samples are best
differentiated along this dimension

66
07:35:09 --> 07:35:14
because they're spread out,
not clumped together along this dimension.

67
07:35:14 --> 07:35:19
This dimension indicated by the red
line is the first principle component

68
07:35:19 --> 07:35:21
labelled as PC1 in the part.

69
07:35:21 --> 07:35:27
It captures the large amount of variance
along a single dimension in data.

70
07:35:27 --> 07:35:32
PC1, indicated by the red line does
not correspond to either axis.

71
07:35:32 --> 07:35:36
The next principle component is determined
by looking in the direction that is

72
07:35:36 --> 07:35:40
orthogonal, in other words perpendicular,
to the first principle component which

73
07:35:40 --> 07:35:44
captures the next largest
amount of variance in the data.

74
07:35:44 --> 07:35:48
This is the second
principal component PC2 and

75
07:35:48 --> 07:35:51
it's indicated by
the green line in the plot.

76
07:35:51 --> 07:35:56
This process can be repeated to find as
many principal components as desired.

77
07:35:56 --> 07:36:00
Note that the principal components do
not align with either the x-axis or

78
07:36:00 --> 07:36:01
the y-axis.

79
07:36:01 --> 07:36:05
And that they are orthogonal, in other
words, perpendicular to each other.

80
07:36:05 --> 07:36:07
This is what PCA does.

81
07:36:07 --> 07:36:10
It finds the underlined dimensions,
the principal

82
07:36:10 --> 07:36:15
components that capture as much of
the variation in the data as possible.

83
07:36:15 --> 07:36:19
These principal components form a new
coordinates system to transform

84
07:36:19 --> 07:36:24
the data to, instead of the conventional
dimensions like X, Y, and Z.

85
07:36:24 --> 07:36:28
So how does PCA help with
dimensionality reduction?

86
07:36:28 --> 07:36:31
Let's look again in this plot with
the first principle component.

87
07:36:31 --> 07:36:35
Since the first principle component
captures most of the variations in

88
07:36:35 --> 07:36:40
the data, the original data sample can
be mapped to this dimension indicated by

89
07:36:40 --> 07:36:43
the red line with minimum
loss of information.

90
07:36:43 --> 07:36:46
In this case then,
we map a two-dimensional dataset to

91
07:36:46 --> 07:36:51
a one-dimensional space while keeping
as much information as possible.

92
07:36:51 --> 07:36:54
Here are some main points about
principal components analysis.

93
07:36:54 --> 07:36:58
PCA finds a new coordinate system for
your data,

94
07:36:58 --> 07:37:02
such that the first coordinate
defined by the first principal

95
07:37:02 --> 07:37:05
component Captures the greatest
variance in your data.

96
07:37:05 --> 07:37:10
The second coordinate defined by
the second principal component captures

97
07:37:10 --> 07:37:13
the second greatest variance in a data,
etc..

98
07:37:13 --> 07:37:18
The first few principle components that
capture most of the variance in a data

99
07:37:18 --> 07:37:22
can be used to define
a lower-dimensional space for your data.

100
07:37:22 --> 07:37:26
PCA can be a very useful technique for
dimensionality reduction,

101
07:37:26 --> 07:37:30
especially when working
with high-dimensional data.

102
07:37:30 --> 07:37:34
While PCA is a useful technique for
reducing the dimensionality of your

103
07:37:34 --> 07:37:36
data which can help with
the downstream analysis,

104
07:37:36 --> 07:37:41
it can also make the resulting analysis
models more difficult to interpret.

105
07:37:41 --> 07:37:46
The original features in your data set
have specific meanings such as income,

106
07:37:46 --> 07:37:48
age and occupation.

107
07:37:48 --> 07:37:53
By mapping the data to a new coordinate
system defined by principal components,

108
07:37:53 --> 07:37:58
the dimensions in your transformed
data no longer have natural meanings.

109
07:37:58 --> 07:38:02
This should be kept in mind when using
PCA for dimensionality reduction.

1
15:08:51 --> 15:08:55
In this activity, we will use KNIME to
perform data exploration of weather data.

2
15:08:57 --> 15:09:00
First, we will create
a new KNIME workflow.

3
15:09:01 --> 15:09:02
We'll then input the weather data.

4
15:09:02 --> 15:09:07
Next, we will create
a histogram of air temperature.

5
15:09:07 --> 15:09:11
Create a scatter plot
between two variables.

6
15:09:11 --> 15:09:15
Create a bar chart to show the
distribution of a categorical variable.

7
15:09:15 --> 15:09:19
And finally, create a box plot to
compare two different distributions.

8
15:09:21 --> 15:09:24
Let's begin.

9
15:09:24 --> 15:09:26
First, we will create
a new workflow in KNIME.

10
15:09:28 --> 15:09:30
To do this we go to the File menu.

11
15:09:31 --> 15:09:38
Select new, select new KNIME workflow and
click on next.

12
15:09:40 --> 15:09:42
Then we type the name of
the workflow we want to create.

13
15:09:43 --> 15:09:45
We'll call this one plots.

14
15:09:46 --> 15:09:47
Click on finish.

15
15:09:49 --> 15:09:54
Next we want to load
weather data into KNIME.

16
15:09:54 --> 15:09:56
To do this we'll use a file reader node.

17
15:09:57 --> 15:10:01
To add this node,
go to the bottom left node repository.

18
15:10:01 --> 15:10:04
And in the box type in file reader.

19
15:10:05 --> 15:10:09
Drag File Reader to the canvas.

20
15:10:09 --> 15:10:13
Next, double click on File Reader to
configure it with the weather data.

21
15:10:14 --> 15:10:15
Click on the Browse button.

22
15:10:17 --> 15:10:21
We'll choose the weather file.

23
15:10:21 --> 15:10:24
That's daily_weather.csv.

24
15:10:25 --> 15:10:28
We can see preview of
the data at the bottom half

25
15:10:28 --> 15:10:31
of the file reader configure dialog.

26
15:10:31 --> 15:10:35
You can see values for
each column in the CSV file.

27
15:10:35 --> 15:10:38
Click OK to close the dialog.

28
15:10:39 --> 15:10:43
Next, we want to create
a histogram of air temperature.

29
15:10:43 --> 15:10:46
We'll add the histogram
node to the workflow.

30
15:10:46 --> 15:10:50
Again, we go to the node repository and
type in histogram.

31
15:10:51 --> 15:10:54
We'll drag and
drop histogram to the workflow and

32
15:10:56 --> 15:10:58
we'll connect File Reader to histogram.

33
15:10:59 --> 15:11:03
We'll click and
hold on output to File Reader and

34
15:11:03 --> 15:11:06
drag to the input of histogram and
release.

35
15:11:08 --> 15:11:12
Before we configure the histogram node,
we need to run the file reader node.

36
15:11:12 --> 15:11:16
We can do that by selecting
the file reader and

37
15:11:16 --> 15:11:20
either clicking on the green
arrow at the top or

38
15:11:20 --> 15:11:24
right clicking and choosing execute here.

39
15:11:27 --> 15:11:32
Once we've done that,
double-click on histogram.

40
15:11:32 --> 15:11:38
We'll select error_temp_9AM
as the biding column and

41
15:11:38 --> 15:11:44
also add error_temp_9AM to
the aggregation column.

42
15:11:44 --> 15:11:47
We'll leave the default
number of bins as ten.

43
15:11:47 --> 15:11:49
Click on OK.

44
15:11:51 --> 15:11:52
Now we'll run the workflow.

45
15:11:52 --> 15:11:57
You can click on the two arrow green
button at the top to run all the nodes.

46
15:11:58 --> 15:12:01
Now let's view the histogram.

47
15:12:01 --> 15:12:03
Right click on the histogram node.

48
15:12:03 --> 15:12:07
And choose View, Histogram View.

49
15:12:07 --> 15:12:11
The x-axis shows the value for
each bin in the histogram.

50
15:12:11 --> 15:12:14
And the y-axis is the count, or frequency.

51
15:12:16 --> 15:12:20
We can see the most frequent
values are between 60 and 73.

52
15:12:20 --> 15:12:25
On the right, we also see that
there's some with missing values.

53
15:12:26 --> 15:12:28
Next close the window.

54
15:12:29 --> 15:12:33
Now let's create a scatter plot to show
the relationship between two variables.

55
15:12:34 --> 15:12:37
First, we will add the scatter
plot node to the work flow.

56
15:12:40 --> 15:12:44
We'll connect the the output of
File Reader to the input of Scatter Plot.

57
15:12:46 --> 15:12:51
Execute the workflow, right-click on

58
15:12:51 --> 15:12:57
Scatter Plot and choose View Scatter Plot.

59
15:12:57 --> 15:13:04
We'll click on the Column Selection and
choose air_temp_9am for the X Column.

60
15:13:04 --> 15:13:07
And for the Y Column,
choose relative humidity 9 AM.

61
15:13:07 --> 15:13:10
In this plot,

62
15:13:10 --> 15:13:14
we can see a negative correlation
between temperature and humidity.

63
15:13:14 --> 15:13:17
As the temperature increases,
the humidity goes down.

64
15:13:19 --> 15:13:20
We'll close this window.

65
15:13:22 --> 15:13:27
Next, we'll create a bar chart to show the
distribution of a categorical variable.

66
15:13:27 --> 15:13:30
We'll visualize the wind
direction at 9 AM.

67
15:13:30 --> 15:13:35
We will begin by creating the categorical
variable by using the numeric binner node.

68
15:13:36 --> 15:13:38
Let's add numeric binner to the work flow.

69
15:13:41 --> 15:13:46
We'll connect the output of file
reader to the input of numeric binner.

70
15:13:47 --> 15:13:51
Double-click on Numeric Binner
to configure it.

71
15:13:51 --> 15:13:55
Select max_wind_direction_9am and

72
15:13:55 --> 15:13:59
click on add five times to add five bins.

73
15:14:01 --> 15:14:03
Now let's give a name for
each of these bins.

74
15:14:04 --> 15:14:07
Select a bin and now choose a name.

75
15:14:07 --> 15:14:09
The first will be for the North direction.

76
15:14:09 --> 15:14:12
So we'll call that 1-N.

77
15:14:12 --> 15:14:19
Next, 2-E for East, 3-S for

78
15:14:19 --> 15:14:24
South, 4-W for West and

79
15:14:24 --> 15:14:28
finally 1-N for north again.

80
15:14:29 --> 15:14:33
Now we need to specify the values for
each of these bins.

81
15:14:33 --> 15:14:38
We'll select the first one and
set the endpoint to be 45 degrees.

82
15:14:38 --> 15:14:43
Next, choose east and
set that maximum to be 135.

83
15:14:45 --> 15:14:49
Select the third one and
choose the max to be 225.

84
15:14:49 --> 15:14:55
Finally, select the fourth one and
select the max to be 315.

85
15:14:55 --> 15:14:58
Next, click on a pin new column.

86
15:14:58 --> 15:15:04
Set the name for categorical variable to
be categorical underscore max underscore.

87
15:15:04 --> 15:15:08
Wind_direction_9AM.

88
15:15:10 --> 15:15:13
Click OK.

89
15:15:13 --> 15:15:15
Now add a histogram note to the work flow.

90
15:15:20 --> 15:15:24
Connect the output of
Numeric Binner to histogram.

91
15:15:24 --> 15:15:28
Double-click on Histogram to configure it.

92
15:15:28 --> 15:15:34
Make sure they binning column is
categorical_max_wind_direction_9AM.

93
15:15:34 --> 15:15:37
And also there are no aggregation columns.

94
15:15:38 --> 15:15:42
Click OK and execute the work flow.

95
15:15:44 --> 15:15:45
Let's view the chart.

96
15:15:46 --> 15:15:51
Right click on Histogram and
choose View, Histogram View.

97
15:15:53 --> 15:15:57
This tells us that most of the wind
comes from the east and the south.

98
15:15:59 --> 15:16:00
And not many measurements from the north.

99
15:16:02 --> 15:16:03
Next, close this.

100
15:16:05 --> 15:16:09
Now lets create a box plot to
compare two different distributions.

101
15:16:09 --> 15:16:12
We w'll examine the distribution,
air pressure for

102
15:16:12 --> 15:16:16
low humidity days versus normal or
high humidity days.

103
15:16:16 --> 15:16:20
To do this, we'll first need to creat
a categorical variable for humidity.

104
15:16:20 --> 15:16:23
Lets add another numeric
binner actor to the workflow.

105
15:16:26 --> 15:16:30
Connect the output,
a file reader to this actor.

106
15:16:32 --> 15:16:34
Double click to configure.

107
15:16:34 --> 15:16:36
Select relative humidity, 9 AM.

108
15:16:36 --> 15:16:39
And click on add twice to add two bins.

109
15:16:41 --> 15:16:46
Select the first bin and
we'll call that humidity_low.

110
15:16:46 --> 15:16:53
Select the second bin and
we'll call that humidity_not_low.

111
15:16:53 --> 15:16:56
Select the first bin, and

112
15:16:56 --> 15:17:01
we'll set the maximum value to 25.

113
15:17:01 --> 15:17:02
Check append new column.

114
15:17:04 --> 15:17:11
And set the name to low_humidity_day,
click on OK.

115
15:17:11 --> 15:17:14
Next, add the conditional box
plot node to the workflow.

116
15:17:18 --> 15:17:23
Right click on conditional
box plot to configure it.

117
15:17:23 --> 15:17:27
Make sure the nominal column
is low_humidity_day and

118
15:17:27 --> 15:17:31
the numeric column is air_pressure_9am.

119
15:17:31 --> 15:17:32
Click OK.

120
15:17:32 --> 15:17:36
Run the workflow.

121
15:17:38 --> 15:17:43
Right click on conditional box plot and
choose few additional box plot.

122
15:17:45 --> 15:17:49
And the x axis, we see the two values or
a categorical variable, humidity low and

123
15:17:49 --> 15:17:51
humidity not low.

124
15:17:51 --> 15:17:54
The y axis shows the air pressure values.

125
15:17:55 --> 15:17:59
I can see that on average,
lower humidity means higher air pressure.

126
15:18:01 --> 15:18:03
Close this.

127
15:18:03 --> 15:18:07
Finally, save the workflow when
we're done by clicking on the disk

128
15:18:07 --> 15:18:08
at the top left of the toolbar.

1
06:26:59 --> 06:27:02
In addition to cleaning your data
to address data quality issues,

2
06:27:02 --> 06:27:07
data preparation also includes
selecting features to use for analysis.

3
06:27:07 --> 06:27:12
After this video, you will be able to
explain what feature selection involves,

4
06:27:12 --> 06:27:16
discuss the goal of feature selection,
and list three approaches for

5
06:27:16 --> 06:27:18
selecting features.

6
06:27:18 --> 06:27:21
Feature selection refers to
choosing the set of features to

7
06:27:21 --> 06:27:25
use that is appropriate for
the subsequent analysis.

8
06:27:25 --> 06:27:28
The goal of feature selection is to come
up with the smallest set of features

9
06:27:28 --> 06:27:33
that best captures the characteristics
of the problem being addressed.

10
06:27:33 --> 06:27:37
The smaller the number of features used,
the simpler the analysis will be.

11
06:27:37 --> 06:27:37
But of course,

12
06:27:37 --> 06:27:42
the set of features used must include
all features relevant to the problem.

13
06:27:42 --> 06:27:45
So, there must be a balance
between expressiveness and

14
06:27:45 --> 06:27:46
compactness of the feature set.

15
06:27:46 --> 06:27:51
There are several methods to
consider in selecting features.

16
06:27:51 --> 06:27:53
New features can be added,

17
06:27:53 --> 06:27:58
some features can be removed, features can
be re-coded or features can be combined.

18
06:27:58 --> 06:28:02
All these operations affect the final
set of features that will be used for

19
06:28:02 --> 06:28:03
analysis.

20
06:28:03 --> 06:28:05
Of course some features
can be kept as is as well.

21
06:28:07 --> 06:28:10
New features can be derived
from existing features.

22
06:28:10 --> 06:28:15
For example, a new feature to specify
whether a student is in state or

23
06:28:15 --> 06:28:19
out of state can be added based on
the student's state of residence.

24
06:28:19 --> 06:28:22
For an application such
as college admissions,

25
06:28:22 --> 06:28:26
this new feature represents an important
aspect of an application and so

26
06:28:26 --> 06:28:28
would be very helpful
as a separate feature.

27
06:28:29 --> 06:28:33
Another example is adding a feature
to indicate the color of a vehicle,

28
06:28:33 --> 06:28:36
which can play an important role
in an auto insurance application.

29
06:28:37 --> 06:28:41
Features can also be removed,
candidates for

30
06:28:41 --> 06:28:43
removal are features
that are very correlated.

31
06:28:43 --> 06:28:46
During data exploration,
you may have discovered that

32
06:28:46 --> 06:28:51
two features are very correlated,
that is they change in very similar ways.

33
06:28:51 --> 06:28:53
For example,
the purchase price of a product and

34
06:28:53 --> 06:28:58
the amount of sales tax paid
are likely to be very correlated.

35
06:28:58 --> 06:29:02
The higher the purchase price
the higher the sales tax.

36
06:29:02 --> 06:29:05
In this case, you might want
to drop one of these features,

37
06:29:05 --> 06:29:08
since these features have
essentially duplicate information.

38
06:29:08 --> 06:29:11
And keeping both features makes
the feature set larger and

39
06:29:11 --> 06:29:14
the analysis unnecessarily more complex.

40
06:29:14 --> 06:29:17
Features with a high percentage of
missing values may also be good

41
06:29:17 --> 06:29:19
candidates for removal.

42
06:29:19 --> 06:29:20
The validity and

43
06:29:20 --> 06:29:24
usefulness of features with a lot
of missing values are in question.

44
06:29:24 --> 06:29:27
So removing may not result
in any loss of information.

45
06:29:27 --> 06:29:30
Again these features would have
been discovered during the data

46
06:29:30 --> 06:29:31
exploration step.

47
06:29:32 --> 06:29:35
Irrelevant features should also
be removed from the data set.

48
06:29:35 --> 06:29:39
Irrelevant features are those that
contain no information that is useful for

49
06:29:39 --> 06:29:41
the analysis task.

50
06:29:41 --> 06:29:44
An example of this is employee
ID in predicting income.

51
06:29:44 --> 06:29:49
Other fields used simply for
identification such as row number,

52
06:29:49 --> 06:29:52
person's ID, etc.,
are good candidates for removal.

53
06:29:53 --> 06:29:57
Features can also be combined if the new
feature presents important information

54
06:29:57 --> 06:30:01
that is not represented by looking at
the original features individually.

55
06:30:03 --> 06:30:07
For example, BMI, which is body
mass index is an indicator of

56
06:30:07 --> 06:30:11
whether a person is underweight,
average weight, or overweight.

57
06:30:12 --> 06:30:15
This is an important feature to have for
a weight loss application.

58
06:30:15 --> 06:30:20
It represents information about how much
a person weighs relative to their height

59
06:30:20 --> 06:30:24
that is not available by looking at just
the person's height or weight alone.

60
06:30:25 --> 06:30:29
A feature can be re-coded as
appropriate for the application.

61
06:30:29 --> 06:30:33
A common example of this is when you
want to turn a continuous feature in to

62
06:30:33 --> 06:30:35
a categorical one.

63
06:30:35 --> 06:30:40
For example for a marketing application
you might want to re-code customer's age

64
06:30:40 --> 06:30:46
into customer categories such as teenager,
young adult, adult and senior citizen.

65
06:30:46 --> 06:30:53
So you would map ages 13 to 19 to
teenager, ages 20 to 25 to young adult,

66
06:30:53 --> 06:30:58
26 to 55 as adult and over 55 as senior.

67
06:30:59 --> 06:31:03
For some applications you may want
to make use of finery features.

68
06:31:03 --> 06:31:07
As an example, you might want a feature to
capture whether a customer tends to buy

69
06:31:07 --> 06:31:09
expensive items or not.

70
06:31:09 --> 06:31:14
In this case, you would want a feature
that maps to one for a customer with

71
06:31:14 --> 06:31:18
an average purchase price over a certain
amount and maps to zero otherwise.

72
06:31:18 --> 06:31:22
Re-coding features can also
result in breaking one feature

73
06:31:22 --> 06:31:24
into multiple features.

74
06:31:24 --> 06:31:28
A common example of this is to
separate an address feature

75
06:31:28 --> 06:31:34
into its constituent parts street address,
city, state and zip code.

76
06:31:34 --> 06:31:37
This way you can more easily
group records by state, for

77
06:31:37 --> 06:31:40
example, to provide
a state by state analysis.

78
06:31:41 --> 06:31:46
Future selection aims to select
the smallest set of features to best

79
06:31:46 --> 06:31:49
capture the characteristics of
the data for your application.

80
06:31:50 --> 06:31:54
Know from the examples represented
that domain knowledge once again place

81
06:31:54 --> 06:31:58
a key role in choosing
the appropriate features to use.

82
06:31:58 --> 06:32:02
Good understanding of the application is
essential in deciding which features to

83
06:32:02 --> 06:32:04
add, drop or modify.

84
06:32:05 --> 06:32:09
It should also be noted that feature
selection can be referred to as feature

85
06:32:09 --> 06:32:13
engineering, since what you're doing here
is to engineer the best feature set for

86
06:32:13 --> 06:32:14
your application.

1
12:59:13 --> 12:59:16
Let's now discuss what
feature transformation is.

2
12:59:17 --> 12:59:21
After this video you will be able
to articulate the purpose of

3
12:59:21 --> 12:59:26
feature transformation, list three
feature transformation operations, and

4
12:59:26 --> 12:59:28
discuss when scaling is important.

5
12:59:29 --> 12:59:31
In addition to feature selection,

6
12:59:31 --> 12:59:35
data pre-processing can also
include feature transformation.

7
12:59:35 --> 12:59:38
Feature transformation involves
mapping a set of values for

8
12:59:38 --> 12:59:42
the feature to a new set of values to
make the representation of the data more

9
12:59:42 --> 12:59:46
suitable or easier to process for
the downstream analysis.

10
12:59:48 --> 12:59:52
A common feature transformation
operation is scaling.

11
12:59:52 --> 12:59:54
This involves changing
the range of values for

12
12:59:54 --> 12:59:58
a feature of features to
another specified range.

13
12:59:58 --> 13:00:01
This is done to avoid allowing
features with large values to

14
13:00:01 --> 13:00:03
dominate the analysis results.

15
13:00:04 --> 13:00:08
For example, if your dataset has
both width and height as features,

16
13:00:08 --> 13:00:11
the magnitude of the weight values,
which are in pounds, will be much

17
13:00:11 --> 13:00:15
larger than the magnitude of the height
values which are in feet and inches.

18
13:00:15 --> 13:00:18
So scaling both features
to a common value range

19
13:00:18 --> 13:00:21
will make the contributions from
both weight and height equal.

20
13:00:23 --> 13:00:28
One way to perform scaling is to map all
values of a feature to a specific range

21
13:00:28 --> 13:00:30
such as between zero and one.

22
13:00:30 --> 13:00:33
For example,
let's say you have a feature for

23
13:00:33 --> 13:00:36
income that ranges from 30,000 to 100,000.

24
13:00:36 --> 13:00:41
And you have another feature for years
of employment that ranges from 0 to 50.

25
13:00:41 --> 13:00:44
These features have very different scales.

26
13:00:44 --> 13:00:47
If you want both features to have equal
weighting when you compare the data

27
13:00:47 --> 13:00:53
samples, then you can scale the range
of both features to be between 0 and 1.

28
13:00:53 --> 13:00:56
That way the income feature which is
on much largest scale than the years

29
13:00:56 --> 13:01:00
of employment feature will not
dominate the compares result.

30
13:01:00 --> 13:01:05
Alternatively, scaling can be perform
by transforming the features such that

31
13:01:05 --> 13:01:09
the results have zero mean,
and unit standard deviation.

32
13:01:09 --> 13:01:12
The steps to perform the scaling
is to first calculate the mean and

33
13:01:12 --> 13:01:15
standard deviation values for
the feature to be scaled.

34
13:01:15 --> 13:01:18
Then for each value, for this feature,

35
13:01:18 --> 13:01:23
subtract the mean value from that value,
and divide by the standard deviation.

36
13:01:23 --> 13:01:27
The transformed feature will end
up with a mean value of zero, and

37
13:01:27 --> 13:01:28
standard deviation of one.

38
13:01:29 --> 13:01:32
This effectively removes
the units of the features and

39
13:01:32 --> 13:01:37
converts each future value to number
of standard deviations from the mean.

40
13:01:38 --> 13:01:42
This scaling method is used when
the min and max values are known.

41
13:01:42 --> 13:01:46
This is also useful when there are
outliers which will skew the calculation

42
13:01:46 --> 13:01:50
for the range as the max value is
determined by the furthest outlier.

43
13:01:51 --> 13:01:55
This scaling operation is often
referred to as zero-normalization or

44
13:01:55 --> 13:01:57
as standardization.

45
13:01:58 --> 13:02:01
Filtering is another feature
transformation operation.

46
13:02:02 --> 13:02:07
This is commonly applied to time series
data, such as speech or audio signals.

47
13:02:07 --> 13:02:10
A low pass filter can be
used to filter out noise.

48
13:02:10 --> 13:02:14
Which usually manifests as the high
frequency component in the signal.

49
13:02:14 --> 13:02:19
A low pass filter removes components
above a certain frequency

50
13:02:19 --> 13:02:21
allowing the rest to
pass through unaltered.

51
13:02:22 --> 13:02:26
Filtering can also be used
to remove noise in images.

52
13:02:26 --> 13:02:31
Noise in an image is random variation
in intensity or color in the pixels.

53
13:02:31 --> 13:02:35
For example, a noise can cause
an image to appear grainy.

54
13:02:35 --> 13:02:40
A mean or median filter can be used to
replace the pixel value with the mean or

55
13:02:40 --> 13:02:42
median value of its neighboring pixels.

56
13:02:43 --> 13:02:45
This has the effect of
smoothing out the image,

57
13:02:45 --> 13:02:48
removing the noise that causes
the graininess in the image.

58
13:02:49 --> 13:02:53
Aggregation combines values for
a feature in order to summarize data or

59
13:02:53 --> 13:02:55
reduce variability.

60
13:02:56 --> 13:03:00
Aggregation combines values for a feature
in order to summarize the data or

61
13:03:00 --> 13:03:02
to reduce variability.

62
13:03:02 --> 13:03:07
Aggregation is done by summing or
averaging data values at a higher level.

63
13:03:07 --> 13:03:11
For example, hourly values can be
aggregated to the daily level.

64
13:03:11 --> 13:03:16
Or values within a city region can
be aggregated to a state level.

65
13:03:16 --> 13:03:19
These plots show
the results of aggregation.

66
13:03:19 --> 13:03:23
The left plot shows the wind
speed values in miles per hour

67
13:03:23 --> 13:03:26
averaged every 10 minutes
over a period of seven days.

68
13:03:26 --> 13:03:31
Notice that there's a lot of variability,
that is the values fluctuate a lot.

69
13:03:32 --> 13:03:35
The right plot shows wind speed
values averaged every hour so

70
13:03:35 --> 13:03:38
every 60 minutes instead
of every 10 minutes.

71
13:03:38 --> 13:03:43
Notice that the line is much smoother
here because the values are aggregated at

72
13:03:43 --> 13:03:44
a higher time scale level.

73
13:03:45 --> 13:03:48
So aggregation can have
the effect of removing noise

74
13:03:48 --> 13:03:52
to provide a clear representation
of the structure of your data.

75
13:03:53 --> 13:03:56
Another example is tracking stock prices.

76
13:03:56 --> 13:03:59
Hourly deviations of a stock
may be difficult to track but,

77
13:03:59 --> 13:04:04
aggregated daily changes may better reveal
any upward or downward trend of the stock.

78
13:04:06 --> 13:04:10
In summary, feature transformation
involves mapping the set of values for

79
13:04:10 --> 13:04:14
a feature to a new set of values to
make the representation of the data

80
13:04:14 --> 13:04:18
more suitable for the downstream analysis.

81
13:04:18 --> 13:04:22
Feature transformation should be used with
cautions since they change the nature of

82
13:04:22 --> 13:04:28
the data and unintentionally remove some
important characteristics of the data.

83
13:04:28 --> 13:04:31
So it's important to look at the effects
of the transformation you're applying

84
13:04:31 --> 13:04:34
to your data to make certain that
it has the intended consequences.

1
02:03:46 --> 02:03:50
In this activity we will be using
KNIME to handle missing data.

2
02:03:50 --> 02:03:55
First, we will create new workflow and
import our weather data.

3
02:03:55 --> 02:04:01
Next, we will remove missing values for
specific measurement in the data.

4
02:04:01 --> 02:04:04
We will then impute missing
values with the mean.

5
02:04:04 --> 02:04:07
And finally,
remove all the rows with missing values.

6
02:04:10 --> 02:04:14
Let's begin, first,
let's create a new workflow in KNIME.

7
02:04:20 --> 02:04:22
We'll name it, Handling Missing Values.

8
02:04:28 --> 02:04:33
Next, let's import the weather
data using the File Reader node.

9
02:04:33 --> 02:04:35
We'll add the File Reader
node to the canvas.

10
02:04:44 --> 02:04:48
We'll configure File Reader
to read daily_weather.csv.

11
02:04:54 --> 02:04:59
Next, we'll connect the Histogram
node to File Reader.

12
02:05:06 --> 02:05:10
Before we configure the Histogram node,
we need to run the File Reader node.

13
02:05:10 --> 02:05:14
We can do this by clicking on File Reader,
and

14
02:05:14 --> 02:05:17
clicking on the green
arrow in the toolbar.

15
02:05:21 --> 02:05:24
Next, double-click on
Histogram to configure it.

16
02:05:24 --> 02:05:29
We'll set both the binning column and
aggregation columns to air_temp_9am.

17
02:05:38 --> 02:05:40
Next, we'll add a missing value node.

18
02:05:48 --> 02:05:52
We'll connect this to
the file reader node.

19
02:05:52 --> 02:05:55
Double click on missing
value to configuring it.

20
02:05:55 --> 02:05:59
Go to column settings,

21
02:05:59 --> 02:06:07
select air_temp_9am, and click on Add.

22
02:06:07 --> 02:06:09
We'll then choose remove row.

23
02:06:12 --> 02:06:18
This will remove the measurements of
air_temp_9am that had missing values.

24
02:06:18 --> 02:06:18
Click OK.

25
02:06:21 --> 02:06:27
Next, we'll add another histogram
node to the output of missing value.

26
02:06:27 --> 02:06:31
We can do this easily by copying and
pasting the existing histogram node.

27
02:06:40 --> 02:06:43
Now let's run the workflow by
clicking on the double green arrows.

28
02:06:46 --> 02:06:50
Let's view the output in
the histogram nodes before and

29
02:06:50 --> 02:06:52
after we remove missing values.

30
02:06:52 --> 02:06:56
In this histogram view,
we can see that there are missing values.

31
02:06:56 --> 02:06:58
If we go and
look at the second histogram node,

32
02:06:58 --> 02:07:01
the one after we removed
the missing values.

33
02:07:04 --> 02:07:06
We can see that there are no
missing values in this chart.

34
02:07:09 --> 02:07:14
Next, instead of removing missing values,
let's impute the values to the mean.

35
02:07:14 --> 02:07:18
We could do this by configuring
the missing value node.

36
02:07:20 --> 02:07:25
And instead of remove row,
change that to mean.

37
02:07:25 --> 02:07:33
Click OK, and rerun the work flow.

38
02:07:33 --> 02:07:36
We could see the difference by comparing
the graphs and the two histogram nodes.

39
02:07:42 --> 02:07:43
If we go to visualization settings.

40
02:07:46 --> 02:07:50
And change labels to all elements, we'll
see the number of elements in each band.

41
02:07:57 --> 02:08:00
In the histogram,
before we removed the missing values,

42
02:08:00 --> 02:08:04
we can see the fifth column
has 216 measurements in it.

43
02:08:04 --> 02:08:11
Where as the fifth column after we imputed
the missing values has 221 values in it.

44
02:08:11 --> 02:08:12
Let's close these.

45
02:08:17 --> 02:08:22
Next, let's remove all the rows that
have missing values in the data.

46
02:08:22 --> 02:08:25
We can do this by double
clicking on missing value.

47
02:08:28 --> 02:08:30
Removing air_temp_ 9am.

48
02:08:32 --> 02:08:34
Clicking on the default tab.

49
02:08:36 --> 02:08:39
Changing this to remove row.

50
02:08:39 --> 02:08:45
I'll click OK, now we rerun the workflow.

51
02:08:45 --> 02:08:49
Again, we'll look at the histograms before
and after we remove the missing values.

52
02:09:01 --> 02:09:06
Again, we'll go to visualization settings
and change the labels to all elements so

53
02:09:06 --> 02:09:08
we can see the number of
elements in each bin.

54
02:09:13 --> 02:09:16
We can see the number of elements in
each bin in the different histograms

55
02:09:16 --> 02:09:17
has changed.

1
04:13:03 --> 04:13:08
In this activity we will see how
to handle missing values in Spark.

2
04:13:08 --> 04:13:11
First, we will load weather
data into a Spark DataFrame.

3
04:13:12 --> 04:13:17
We'll then examine the summary
statistics for air temperature, remove

4
04:13:17 --> 04:13:22
the rows with missing values, and finally
impute missing values with the mean.

5
04:13:25 --> 04:13:26
Let's begin.

6
04:13:26 --> 04:13:30
First, we'll open the notebook
called handling missing values.

7
04:13:31 --> 04:13:34
The first cell creates an SQLContext and

8
04:13:34 --> 04:13:37
then loads the weather data
csv into a data frame.

9
04:13:39 --> 04:13:44
Let's execute this,
we can view the summary statistics for

10
04:13:44 --> 04:13:50
the DataFrame by running
df.describe().show().

11
04:13:50 --> 04:13:53
Let's just look at the summary
statistics for the air temperature.

12
04:13:53 --> 04:13:54
We'll run

13
04:13:54 --> 04:14:03
df.describe(['air_score_9am']) .show.

14
04:14:06 --> 04:14:09
This says that there are 1090 rows.

15
04:14:10 --> 04:14:14
This does not include the rows of
missing values for the air temperature.

16
04:14:15 --> 04:14:20
We can count the total number of rows
in the DataFrame by running df.count.()

17
04:14:22 --> 04:14:26
Since there are 1095 total
rows in the DataFrame, but

18
04:14:26 --> 04:14:28
only 1090 in the air_temp column,

19
04:14:28 --> 04:14:33
that means there are five rows in
air_temp that have missing values.

20
04:14:35 --> 04:14:39
Next, let's remove all the rows in
the DataFrame that have missing values.

21
04:14:40 --> 04:14:43
We'll put these in a new data
frame called removeAllDF.

22
04:14:44 --> 04:14:49
To drop the missing values
we'll run df.na.drop.

23
04:14:51 --> 04:14:53
Let's look at the summary statistics for

24
04:14:53 --> 04:14:57
air_temp 9AM with
the missing values dropped.

25
04:14:57 --> 04:15:03
We'll run removeAllDF .describe([
air_temp_9am]) show.()

26
04:15:06 --> 04:15:09
We can see that the mean and
standard deviation values

27
04:15:09 --> 04:15:13
are close to the original values before
we removed the rows with missing values.

28
04:15:15 --> 04:15:19
Additionally, the count of
the number of rows is 1064.

29
04:15:21 --> 04:15:25
We can verify that this is the total
number of rows in the new DataFrame by

30
04:15:25 --> 04:15:27
running removeAllDF.count.

31
04:15:32 --> 04:15:35
Next, let's impute the missing
values with the mean,

32
04:15:35 --> 04:15:37
instead of removing them
from the DataFrame.

33
04:15:38 --> 04:15:42
First, we'll need to load
the average function from pyspark.

34
04:15:44 --> 04:15:53
We'll do this by running from
pyspark.sql.functions import avg.

35
04:15:53 --> 04:15:57
Next, we'll create a copy of the DataFrame
in which we will input the missing values.

36
04:15:58 --> 04:16:01
We'll call the new DataFrame imputeDF.

37
04:16:05 --> 04:16:09
To impute the missing values we'll
iterate through each column of

38
04:16:09 --> 04:16:14
the original DataFrame, first computing
the mean value for that column and

39
04:16:14 --> 04:16:18
then replacing the missing values
in that column with the mean value.

40
04:16:18 --> 04:16:23
To move through the columns
in the data frame,

41
04:16:23 --> 04:16:27
we'll enter for
x in imputeDF.columns: next,

42
04:16:27 --> 04:16:32
we'll compute the mean value for
that column.

43
04:16:35 --> 04:16:39
We'll use the data frame in which
we removed all the missing values,

44
04:16:39 --> 04:16:42
we'll call the agg function
to compute an aggregate.

45
04:16:42 --> 04:16:45
And the argument that we give it is avg.

46
04:16:45 --> 04:16:51
The argument avg is x, which is the column
we are trying to compute the average of.

47
04:16:53 --> 04:16:57
The agg function returns to DataFrame and

48
04:16:57 --> 04:17:00
we want to get the first
row of that data frame.

49
04:17:00 --> 04:17:05
We can do this by calling .first and
then you get the first value in

50
04:17:05 --> 04:17:10
this row or say [0].

51
04:17:10 --> 04:17:13
Next let's print the column
name in mean value.

52
04:17:14 --> 04:17:19
print(x, meanValue)

53
04:17:22 --> 04:17:26
Now let's update our new DataFrame,

54
04:17:26 --> 04:17:31
replacing the missing
values with the mean value.

55
04:17:31 --> 04:17:40
imputeDF = imputeDF.na.fill(meanValue,
[x]).

56
04:17:41 --> 04:17:42
Let's run this.

57
04:17:47 --> 04:17:49
We can see that the mean value for

58
04:17:49 --> 04:17:54
air_temp 9am matches the mean value
computed in the summary statistics of

59
04:17:54 --> 04:17:57
the data frame where the missing
values were removed.

60
04:17:59 --> 04:18:03
Finally, let's print the imputed
data summary statistics.

61
04:18:03 --> 04:18:07
First we'll show the summary
statistics for the original DataFrame.

62
04:18:07 --> 04:18:10
And then the summary statistics for
the imputed DataFrame.

63
04:18:10 --> 04:18:17
We'll enter df.describe([
air_temp_ 9am]) .show () and

64
04:18:17 --> 04:18:25
imputeDF.describe(['air_temp_9am']).sho-
w().

65
04:18:25 --> 04:18:27
Run this.

66
04:18:27 --> 04:18:32
We can see that the number of rows
in the imputed DataFrame is larger

67
04:18:32 --> 04:18:36
than the number of rows in
the original DataFrame.

68
04:18:37 --> 04:18:41
There were five rows in the original
DataFrame with missing values.

69
04:18:41 --> 04:18:43
And these have now been
replaced with the mean.

1
08:31:46 --> 08:31:50
Let's talk about what it means to
build a classification model and

2
08:31:50 --> 08:31:52
how building a model differs
from applying a model.

3
08:31:54 --> 08:31:55
After this video,

4
08:31:55 --> 08:32:00
you will be able to discuss what
building a classification model means.

5
08:32:00 --> 08:32:03
Explain the difference between
building and applying a model.

6
08:32:03 --> 08:32:06
And summarize why the parameters
of a model needs to be adjusted.

7
08:32:08 --> 08:32:11
A machine learning model
is a mathematical model.

8
08:32:11 --> 08:32:15
In the general sense, this means that
the model has parameters and uses

9
08:32:15 --> 08:32:19
equations to determine the relationship
between its inputs and outputs.

10
08:32:20 --> 08:32:25
The parameters are used by the model to
modify the inputs to generate the outputs.

11
08:32:25 --> 08:32:28
The model adjusts its parameters
in order to correct or

12
08:32:28 --> 08:32:31
refine this input, output relationship.

13
08:32:33 --> 08:32:35
Here's an example of a simple model.

14
08:32:35 --> 08:32:38
This mathematical model represents a line.

15
08:32:38 --> 08:32:44
y is the output, x is the input,
m determines the slope of the line and

16
08:32:44 --> 08:32:51
b determines the y-intercept or
where the line crosses the y-axis.

17
08:32:51 --> 08:32:53
m and b are the model's parameters.

18
08:32:53 --> 08:32:55
Given a specific value for

19
08:32:55 --> 08:33:00
x, the model uses as parameters
along with x to determine y.

20
08:33:00 --> 08:33:05
By adjusting the values for
the parameters m and b,

21
08:33:05 --> 08:33:10
the model can adjust how the input
x matched to the output y.

22
08:33:12 --> 08:33:15
Here we see how the output y changes for

23
08:33:15 --> 08:33:19
the same value of input x,
when parameter b changes.

24
08:33:19 --> 08:33:25
Recall that b is the y-intercept, or
where the line crosses the y-axis.

25
08:33:26 --> 08:33:34
The value of b is +1 for
the red line and -1 for the blue line.

26
08:33:34 --> 08:33:37
For the input x=1, the value of y is 3 for

27
08:33:37 --> 08:33:42
the red line,
as indicated by the red arrow.

28
08:33:42 --> 08:33:48
For the blue line, when the parameter
b changes from +1 to -1,

29
08:33:48 --> 08:33:53
for x=1, the value of y is 1,
as indicated by the blue arrow.

30
08:33:54 --> 08:33:58
So we see that with just a simple
change in one model parameter,

31
08:33:58 --> 08:34:01
the input to output mapping changes.

32
08:34:02 --> 08:34:05
A machine learning model
works in a similar way.

33
08:34:05 --> 08:34:08
It maps input values to output values.

34
08:34:08 --> 08:34:11
And it adjusts the parameters
in order to correct or

35
08:34:11 --> 08:34:14
refine this input-output mapping.

36
08:34:15 --> 08:34:18
The parameters of a machine
learning model are adjusted or

37
08:34:18 --> 08:34:22
estimated from the data
using a learning algorithm.

38
08:34:23 --> 08:34:27
This, in essence,
is what is involved in building a model.

39
08:34:27 --> 08:34:32
This process is referred to by many terms,
such as model building,

40
08:34:32 --> 08:34:37
model creation,
model training and model fitting.

41
08:34:37 --> 08:34:38
In building a model,

42
08:34:38 --> 08:34:43
we want to adjust the parameters in
order to reduce the model's error.

43
08:34:43 --> 08:34:48
In the case of supervised tasks,
such as classification, this means getting

44
08:34:48 --> 08:34:53
the model's outputs to match the targets
or desired outputs as much as possible.

45
08:34:54 --> 08:34:58
Since the classification task is
to predict the correct category or

46
08:34:58 --> 08:35:01
class, given the input variables,

47
08:35:01 --> 08:35:06
you can think of the classification
problem visually as carving out the input

48
08:35:06 --> 08:35:10
space as regions corresponding
to the different class labels.

49
08:35:10 --> 08:35:15
In this diagram for example,
the classification model needs to form

50
08:35:15 --> 08:35:20
the boundaries to define the regions
separating red triangles

51
08:35:20 --> 08:35:24
from blue diamonds, from green circles,
from yellow squares.

52
08:35:25 --> 08:35:31
In this example, if a sample falls within
the region in the upper right corner,

53
08:35:31 --> 08:35:33
it will be classified as a blue diamond.

54
08:35:34 --> 08:35:39
Classification decisions are based on
these regions, and the regions are defined

55
08:35:39 --> 08:35:43
by the boundaries, as indicated by
the dashed lines in the diagram.

56
08:35:43 --> 08:35:46
So these boundaries are referred
to as decision boundaries.

57
08:35:47 --> 08:35:52
Building a classification then means using
the data to adjust the model's parameters

58
08:35:52 --> 08:35:56
in order to form decision boundaries
to separate the target classes.

59
08:35:57 --> 08:36:01
Note that the term classifier is often
used to mean classification model.

60
08:36:03 --> 08:36:06
In general,
building a classification model,

61
08:36:06 --> 08:36:10
as well as other machine learning models,
involves two phases.

62
08:36:11 --> 08:36:16
The first is the training phase,
in which the model is constructed and

63
08:36:16 --> 08:36:20
its parameters adjusted using as
what referred to as training data.

64
08:36:21 --> 08:36:25
Training data is the data set
used to train or create a model.

65
08:36:26 --> 08:36:28
The second is the testing phase.

66
08:36:28 --> 08:36:31
This is where the learned
model is applied to new data.

67
08:36:31 --> 08:36:34
That is,
data not used in training the model.

68
08:36:35 --> 08:36:37
Here's another way to
look at the two phases.

69
08:36:39 --> 08:36:43
In a training phase, the learning
algorithm uses the training data

70
08:36:43 --> 08:36:46
to adjust the model's
parameters to minimize errors.

71
08:36:47 --> 08:36:49
At the end of the training phase,
you get the trained model.

72
08:36:51 --> 08:36:56
In the testing phase,
the trained model is applied to test data.

73
08:36:56 --> 08:37:02
Test data is separate from training data
and is previously unseen by the model.

74
08:37:02 --> 08:37:05
The model is then evaluated on
how it performs on the test data.

75
08:37:06 --> 08:37:10
The goal in building a classifier
model is to have the model

76
08:37:10 --> 08:37:13
perform well on training,
as well as test data.

77
08:37:13 --> 08:37:16
We will discuss in more detail
the use of training and

78
08:37:16 --> 08:37:21
test data sets in the next module,
when we discuss model evaluation.

79
08:37:21 --> 08:37:26
To adjust a model's parameters,
we need to apply a learning algorithm.

80
08:37:26 --> 08:37:30
We will discuss the specific algorithms
to build a classification model in

81
08:37:30 --> 08:37:31
the next few lectures.

1
17:09:18 --> 17:09:21
In this video, we will outline
some commonly used algorithms for

2
17:09:21 --> 17:09:24
building a classification model.

3
17:09:24 --> 17:09:28
After this video,
you will be able to describe the goal of

4
17:09:28 --> 17:09:33
a classification algorithm and name some
common algorithms for classification.

5
17:09:34 --> 17:09:39
Recall that a classification task is
to predict the category from the input

6
17:09:39 --> 17:09:40
variables.

7
17:09:40 --> 17:09:46
A classification model processes the input
data it receives and provides an output.

8
17:09:46 --> 17:09:50
Since classification is a supervised task,
a target or

9
17:09:50 --> 17:09:54
desired output is provided for
each sample.

10
17:09:54 --> 17:09:59
The goal is to get the model outputs to
match the targets as much as possible.

11
17:10:01 --> 17:10:04
A classification model
adjusts its parameters

12
17:10:04 --> 17:10:07
to get its outputs to match the targets.

13
17:10:07 --> 17:10:11
To adjust a model's parameters,
a learning algorithm is applied.

14
17:10:12 --> 17:10:16
This occurs in a training phase
when the model is constructed.

15
17:10:17 --> 17:10:20
There are many algorithms to
build a classification model.

16
17:10:20 --> 17:10:25
In this course,
we will cover the algorithms listed here,

17
17:10:25 --> 17:10:30
kNN or k Nearest Neighbors,
decision tree, and naive Bayes.

18
17:10:30 --> 17:10:34
kNN stands for k Nearest Neighbors.

19
17:10:34 --> 17:10:39
This technique relies on the notion that
samples with similar characteristics,

20
17:10:39 --> 17:10:45
that is samples with similar values for
input, likely belong to the same class.

21
17:10:45 --> 17:10:48
So classification of a sample is dependent

22
17:10:48 --> 17:10:50
on the target values of
the neighboring points.

23
17:10:52 --> 17:10:56
Another classification technique
is referred to as decision tree.

24
17:10:56 --> 17:11:01
A decision tree is a classification
model that uses a treelike structure

25
17:11:01 --> 17:11:04
to represent multiple decision paths.

26
17:11:04 --> 17:11:09
Traversing each path leads to a different
way to classify an input sample.

27
17:11:10 --> 17:11:14
A naive Bayes model uses a probabilistic
approach to classification.

28
17:11:15 --> 17:11:19
Baye's Theorem is used to capture the
relationship between the input data and

29
17:11:19 --> 17:11:20
the output class.

30
17:11:21 --> 17:11:26
Simply put, the Baye's Theorem
compares the probability of an event

31
17:11:26 --> 17:11:28
in the presence of another event.

32
17:11:28 --> 17:11:33
We see here the probability
of A if B is present.

33
17:11:33 --> 17:11:37
For example, probability of having
a fire if the weather is hot.

34
17:11:37 --> 17:11:41
You can imagine event B depending
on more than one variable.

35
17:11:41 --> 17:11:43
For example, weather is hot and windy.

36
17:11:43 --> 17:11:51
We will cover kNN, decision tree and naive
Bayes in detail in the next few lectures.

37
17:11:51 --> 17:11:55
There are many other classification
techniques, but we will focus on these

38
17:11:55 --> 17:11:58
since they are fundamental algorithms
that are commonly used and

39
17:11:58 --> 17:12:01
form the basis of other algorithms for
classification.

1
10:21:19 --> 10:21:23
In this activity we will perform
classification in Spark is in

2
10:21:23 --> 10:21:24
decision tree.

3
10:21:24 --> 10:21:28
First, we will load weather
data into DataFrame and

4
10:21:28 --> 10:21:31
drop unused and missing data.

5
10:21:31 --> 10:21:34
We'll then,
create a categorical variable for

6
10:21:34 --> 10:21:39
low humidity days and aggregate
features used to make predictions.

7
10:21:39 --> 10:21:43
We will then split our data
into training and test sets.

8
10:21:44 --> 10:21:46
And then create and
train the decision tree.

9
10:21:47 --> 10:21:52
Finally, we will save our
predictions to a CSV file.

10
10:21:52 --> 10:21:56
Let's begin, first, we'll open
the notebook called classification.

11
10:21:56 --> 10:22:04
The first cell contains the classes
we need to load to run this exercise.

12
10:22:04 --> 10:22:05
Let's run this.

13
10:22:08 --> 10:22:13
Next we create SQL context and load
the weather data CSV into a data frame.

14
10:22:13 --> 10:22:16
The second cell also prints all
the columns in this data frame.

15
10:22:17 --> 10:22:18
Run this.

16
10:22:23 --> 10:22:26
The third cell defines the columns
in the weather data we will use for

17
10:22:26 --> 10:22:28
the decision tree classifier.

18
10:22:29 --> 10:22:29
Let's run it.

19
10:22:32 --> 10:22:34
We will now use the column name number.

20
10:22:34 --> 10:22:41
So, let's drop that from the data frame,
df = df.drop Number.

21
10:22:42 --> 10:22:49
Now, let's revolve the rows with
missing data, df = df.na.drop.

22
10:22:51 --> 10:22:58
Now, let's print the number of rows and
columns in our resulting data frame,

23
10:22:58 --> 10:23:01
df.count(), len(df.columns).

24
10:23:05 --> 10:23:10
Next, let's create a categorical variable
to denote if the humidity is low.

25
10:23:12 --> 10:23:17
We'll enter binarizer = Binarizer ().

26
10:23:17 --> 10:23:21
The first argument specifies
a threshold value for the variable.

27
10:23:21 --> 10:23:27
We want the categorical variable to be 1,
if the humidity is greater than 25%.

28
10:23:27 --> 10:23:33
So we'll enter a threshold=24.9999.

29
10:23:33 --> 10:23:38
The next argument specifies the column to
use to create the categorical variable.

30
10:23:38 --> 10:23:44
We'll input,
inputCol = relative_humidity_3pm.

31
10:23:44 --> 10:23:51
The final argument specifies the new
column name, outputCol = label.

32
10:23:51 --> 10:23:54
Now, let's create a new data frame
with this categorical variable.

33
10:23:54 --> 10:24:00
binarizeredDF = binarizer.transform df.

34
10:24:00 --> 10:24:01
Let's run this.

35
10:24:03 --> 10:24:06
Let's look at the first four
rows in this new data frame.

36
10:24:06 --> 10:24:12
We'll run binarizedDF.select
('relative_humidity_3pm',

37
10:24:12 --> 10:24:14
'label').show(4).

38
10:24:18 --> 10:24:24
The relative humidity in the first row
is greater than 25% and the label is 1.

39
10:24:24 --> 10:24:28
The relative humidity in the second,
third, and fourth rows are less than 25%,

40
10:24:28 --> 10:24:32
and the label is 0.

41
10:24:32 --> 10:24:38
Next, let's aggregate the features we will
use to make predictions into a single col,

42
10:24:38 --> 10:24:40
assembler = VectorAssember( ).

43
10:24:40 --> 10:24:44
The first argument is a list of
the columns to be aggregated.

44
10:24:44 --> 10:24:49
inputCols=featureColumns, and
the second argument is the name

45
10:24:49 --> 10:24:56
of the new column containing the
aggregated features, outputCol=features.

46
10:24:56 --> 10:25:00
We can create the new
data frame by running

47
10:25:00 --> 10:25:05
assembled=assembler.transform binarizedDF.

48
10:25:05 --> 10:25:07
Let's run this.

49
10:25:09 --> 10:25:13
Next we will split our data
set into two parts, one for

50
10:25:13 --> 10:25:15
training data and one for test data.

51
10:25:15 --> 10:25:21
You can do this by entering (training

52
10:25:21 --> 10:25:29
Data,
testData)=assembled.randomSplit([0.8,

53
10:25:29 --> 10:25:34
0.2], seed=13234).

54
10:25:34 --> 10:25:39
We can see the size of the two
sets by running count,

55
10:25:39 --> 10:25:43
trainingData.count(), testData.count ().

56
10:25:46 --> 10:25:48
Next let's create and
train the decision tree.

57
10:25:48 --> 10:25:53
We'll enter dt = DecisionTreeClassifier.

58
10:25:54 --> 10:25:59
The first argument is the column we're
trying to predict, labelCol='label'.

59
10:25:59 --> 10:26:04
The second argument is the name
of the column containing your

60
10:26:04 --> 10:26:07
aggregated features,
featuresCol='features'.

61
10:26:09 --> 10:26:11
The third argument is
the stopping criteria for

62
10:26:11 --> 10:26:16
tree induction based on the maximum
depth of the tree, maxDepth=5.

63
10:26:16 --> 10:26:22
The fourth argument is the stopping
criteria for tree induction based

64
10:26:22 --> 10:26:27
on the minimum number of samples
in a node, minInstancesPerNode=20.

65
10:26:27 --> 10:26:32
And finally, the last argument
specifies the impurity measure

66
10:26:32 --> 10:26:38
used to split the nodes,
impurity="gini", let's run this.

67
10:26:40 --> 10:26:43
Next, we can create a model by
training the decision tree.

68
10:26:43 --> 10:26:46
We'll do this by executing
it in a pipeline.

69
10:26:46 --> 10:26:54
We'll enter pipeline=Pipeline
(stages=[dt]).

70
10:26:54 --> 10:26:58
We'll create them all by putting
a training data, model =

71
10:26:58 --> 10:27:03
pipeline.fit trainingData.

72
10:27:03 --> 10:27:10
Let's run this Now, we can make
predictions using our test data.

73
10:27:10 --> 10:27:16
We'll enter predictions =
model.transform(testData).

74
10:27:16 --> 10:27:20
You can look at the first 10 rows
of the prediction by running,

75
10:27:20 --> 10:27:25
predictions.select('prediction', label')
show(10).

76
10:27:28 --> 10:27:31
You can see in the first ten rows,
the prediction matches the label.

77
10:27:31 --> 10:27:34
Now let's save our
predictions to a CSV file.

78
10:27:34 --> 10:27:38
In the next Spark hands-on activity
we will evaluate the accuracy.

79
10:27:38 --> 10:27:43
You can save it by running
predictions.select('prediction',

80
10:27:43 --> 10:27:48
'label').write.save(path='file File:///

81
10:27:48 --> 10:27:54
home/cloudera/downloads/big-data-4/predic-
tions.csv.

82
10:27:54 --> 10:27:58
We'll specify the format to use Spark csv.

83
10:27:58 --> 10:28:04
Format='com.databricks.spark.csv.

84
10:28:04 --> 10:28:07
Finally, we'll enter header='true'.

85
10:28:07 --> 10:28:10
Run this to save
the predictions to a .csv file.

1
20:49:29 --> 20:49:32
In this activity, we will be
performing classification in KNIME,

2
20:49:33 --> 20:49:37
first we will create a new workflow and
import our weather data.

3
20:49:37 --> 20:49:42
Next, we will remove the missing data and
then create a categorical value for

4
20:49:42 --> 20:49:44
the humidity measurements.

5
20:49:45 --> 20:49:49
We will then examine summary
statistics of the data before and

6
20:49:49 --> 20:49:54
after the missing data was removed, and
finally build a decision tree workflow.

7
20:49:58 --> 20:49:59
Let's begin.

8
20:49:59 --> 20:50:01
First, let's create a New Workflow.

9
20:50:05 --> 20:50:06
We'll call it Classification.

10
20:50:09 --> 20:50:14
Next, we'll import the daily weather data,
using the File Reader node.

11
20:50:17 --> 20:50:24
Configure the file reader node,
to use daily_weather.csv.

12
20:50:24 --> 20:50:29
Next, we'll add a missing value node,

13
20:50:29 --> 20:50:32
to remove the missing values
in the daily weather data.

14
20:50:37 --> 20:50:41
We'll configure missing value to
remove all the missing values.

15
20:50:49 --> 20:50:54
Next we'll use a numeric binner node
to create a categorical variable for

16
20:50:54 --> 20:50:55
the humidity at 3pm.

17
20:50:57 --> 20:51:00
We'll add the numeric binner node,
we'll connect it to missing value.

18
20:51:00 --> 20:51:06
We'll select the relative
humidity 3pm column.

19
20:51:07 --> 20:51:13
We'll create two bins,
the first bin we'll call humidity_low.

20
20:51:17 --> 20:51:19
And that value will go up to 25.

21
20:51:19 --> 20:51:24
The second bin, we'll

22
20:51:24 --> 20:51:31
call humidity_not_low.

23
20:51:31 --> 20:51:34
We'll check the Append new column and

24
20:51:34 --> 20:51:38
we'll call the new
column low_humidity_day.

25
20:51:47 --> 20:51:51
Next, we'll examine some summary
statistics, both before, and

26
20:51:51 --> 20:51:54
after we've removed the missing values.

27
20:51:54 --> 20:51:57
We'll add the statistics
node to the canvas.

28
20:52:02 --> 20:52:06
We'll connect the first one to
the output of the File Reader actor.

29
20:52:07 --> 20:52:10
We'll add another one to
the output of Numeric Binner.

30
20:52:15 --> 20:52:19
We'll change the name of the first
one to be BEFORE filtering.

31
20:52:23 --> 20:52:26
And change the name of the second
one to AFTER filtering.

32
20:52:30 --> 20:52:31
Lets configure the first one.

33
20:52:33 --> 20:52:38
We'll change the maximum number of
possible values per column to 1500.

34
20:52:40 --> 20:52:42
We'll also add all the columns.

35
20:52:46 --> 20:52:48
Let's configure the second statistics now.

36
20:52:48 --> 20:52:52
Again, we'll change the maximum number of
possible values per column to 1500, and

37
20:52:52 --> 20:52:53
we'll add all the columns.

38
20:52:57 --> 20:52:58
Now let's run our workflow.

39
20:53:02 --> 20:53:06
Let's view both Statistic
nodes to compare the outputs.

40
20:53:10 --> 20:53:15
In the BEFORE filtering statistics,
we can see that there are missing values.

41
20:53:17 --> 20:53:22
However, there are no missing values
in the AFTER filtering statistics.

42
20:53:26 --> 20:53:30
We can also compare the summary
statistics, for different measurements,

43
20:53:30 --> 20:53:32
to see that they are similar values.

44
20:53:32 --> 20:53:35
For example,
let's take a look at air_temp_9am.

45
20:53:46 --> 20:53:49
We could see that most of
the statistics are the same.

46
20:53:50 --> 20:53:52
As well as the distribution of values.

47
20:53:54 --> 20:53:58
In the AFTER filtering statistics,
click on the Nominal tab,

48
20:53:58 --> 20:54:03
and we'll look at the low humidity
day categorical variable we created.

49
20:54:11 --> 20:54:15
We can see that the samples are equally
distributed between the two values.

50
20:54:19 --> 20:54:21
Let's close these Statistics views.

51
20:54:22 --> 20:54:24
Next, let's add a Column Filter node.

52
20:54:32 --> 20:54:35
We'll connect this to
the output of Numeric Binner.

53
20:54:35 --> 20:54:37
Double-click to configure the node.

54
20:54:37 --> 20:54:43
And we will exclude
relative_humidity_9am and

55
20:54:43 --> 20:54:48
relative_humidity_3pm columns, click OK.

56
20:54:48 --> 20:54:52
Next, we'll add a Color Manager
node to the canvas.

57
20:54:56 --> 20:55:01
Will connect this to the output of Column
Filter, double-click to configure it.

58
20:55:03 --> 20:55:10
We will make sure the humidly_low is red,
and humidity_not_low is blue.

59
20:55:10 --> 20:55:12
Click OK to close.

60
20:55:12 --> 20:55:17
Next, we'll add the Partitioning
node to the campus.

61
20:55:22 --> 20:55:25
We'll connect this to
the output of Color Manager.

62
20:55:25 --> 20:55:26
Double-click to configure.

63
20:55:26 --> 20:55:31
We want to split the data
into two partitions.

64
20:55:31 --> 20:55:33
The first partition should
have 80% of the data.

65
20:55:33 --> 20:55:35
The second partition should have 20%.

66
20:55:35 --> 20:55:40
To do this click on Relative[%] and
change this to 80.

67
20:55:40 --> 20:55:44
We'll make sure Draw randomly is selected.

68
20:55:47 --> 20:55:53
And check Use random seed and
change the seed to 12345.

69
20:55:53 --> 20:55:56
Normally we would not
specific the random seed.

70
20:55:56 --> 20:56:01
However, since we want repeatable results,
we use a specific seed value here.

71
20:56:02 --> 20:56:03
Click OK to continue.

72
20:56:04 --> 20:56:07
Next, we'll add a Decision Tree Learner
to the work flow.

73
20:56:10 --> 20:56:13
We'll connect this to the top
output of the Partitioning node.

74
20:56:15 --> 20:56:18
Double-click to configure, and

75
20:56:18 --> 20:56:23
change the Min number of
records per node to 20.

76
20:56:23 --> 20:56:24
Click OK.

77
20:56:26 --> 20:56:31
Next, we'll add a Decision Tree Predictor
node to the canvas, and we will connect

78
20:56:31 --> 20:56:36
the bottom output of Partitioning to
the bottom input of the predictor.

79
20:56:37 --> 20:56:41
Next we'll connect the output
of Decision Tree Learner

80
20:56:41 --> 20:56:44
to the top input of
Decision Tree Predictor.

81
20:56:45 --> 20:56:47
Now let's execute our workflow.

82
20:56:48 --> 20:56:51
We can view the resulting
classification rules

83
20:56:51 --> 20:56:53
by right clicking on
Decision Tree Predictor.

84
20:56:55 --> 20:57:00
And choosing either View: Decision Tree
View or View: Decision Tree View (simple).

85
20:57:01 --> 20:57:02
Let's do the first one.

86
20:57:03 --> 20:57:07
This shows the first two
splits in our decision tree.

87
20:57:07 --> 20:57:09
We can expand each branch
by clicking on the plus.

88
20:57:09 --> 20:57:11
Let's close this.

89
20:57:11 --> 20:57:15
Now let's look at the simplified view.

90
20:57:17 --> 20:57:21
Again, we can expand the branches to
see the splits in the Decision Tree.

91
20:57:22 --> 20:57:27
Close this,
finally let's save this workflow.

92
20:57:28 --> 20:57:33
We'll analyze the results of the Decision
Tree model in the next KNIME Hands On.

93
20:57:33 --> 20:57:35
Save the workflow by
clicking on the disk icon.

1
17:47:04 --> 17:47:08
Welcome back, we already discussing
Classification models and techniques for

2
17:47:08 --> 17:47:10
next few lectures.

3
17:47:10 --> 17:47:12
Let's first define what
the classification task is,

4
17:47:13 --> 17:47:18
after this video you will be able
to define what classification is.

5
17:47:18 --> 17:47:23
Explain whether classification is
supervised or unsupervised and

6
17:47:23 --> 17:47:27
describe how binomial classification
differs from multinomial classification.

7
17:47:29 --> 17:47:32
Classification is one type of
machine learning problems.

8
17:47:32 --> 17:47:36
In the classification problem, the input
data is presented to the machine learning

9
17:47:36 --> 17:47:42
model and the task is to predict the
target corresponding to the input data.

10
17:47:42 --> 17:47:47
The target is a categorical variable,
so the classification task is

11
17:47:47 --> 17:47:52
to predict the category or
label of the target given the input data.

12
17:47:52 --> 17:47:56
For example, the classification
problem illustrated in this image

13
17:47:56 --> 17:47:59
is to predict the type of weather.

14
17:47:59 --> 17:48:04
The target that the model has to predict
is the weather and the possible values for

15
17:48:04 --> 17:48:09
weather in this case is Sunny,
Windy, Rainy, or Cloudy.

16
17:48:10 --> 17:48:15
The input data can consist of measurements
like temperature, relative humidity,

17
17:48:15 --> 17:48:19
atmospheric pressure,
wind speed, wind direction, etc.

18
17:48:20 --> 17:48:25
So, given specific values for temperature,
relative humidity, atmospheric pressure,

19
17:48:25 --> 17:48:30
etc., the task for the model is to
predict if the weather will be sunny.

20
17:48:30 --> 17:48:33
Windy, rainy, or cloudy for the day,

21
17:48:35 --> 17:48:39
this is what the data set might look like
for the weather classification problem.

22
17:48:39 --> 17:48:44
Each row is a sample with input
variables temperature, humidity, and

23
17:48:44 --> 17:48:46
pressure and target variable weather.

24
17:48:48 --> 17:48:51
Each row has specific values for
the input variables and

25
17:48:51 --> 17:48:53
a corresponding value for
the target variable.

26
17:48:55 --> 17:48:59
The classification task is to predict
the value of the target variable

27
17:48:59 --> 17:49:01
from the values of the input variables.

28
17:49:03 --> 17:49:06
Since a target is provided,
we have labeled data and so

29
17:49:06 --> 17:49:10
classification is a supervised task.

30
17:49:10 --> 17:49:13
Recall that in a supervised task,
the target or

31
17:49:13 --> 17:49:17
desired output for each sample is given.

32
17:49:17 --> 17:49:21
Note that the target variable goes
by many names such as target,

33
17:49:21 --> 17:49:26
label, output, class variable,
category, and class.

34
17:49:28 --> 17:49:33
A classification problem can be binary or
multi-class with binary

35
17:49:33 --> 17:49:39
classification the target variable has two
possible values, for example yes and no.

36
17:49:39 --> 17:49:43
With multi-class classification
the target variable has more than

37
17:49:43 --> 17:49:45
two possible values.

38
17:49:45 --> 17:49:49
For example, the target can be short,
medium and tall.

39
17:49:49 --> 17:49:54
Multi-class classification is
also referred to multinomial or

40
17:49:54 --> 17:49:56
multi-label classification.

41
17:49:57 --> 17:50:01
Remember though that the target is always
a categorical variable in classification.

42
17:50:03 --> 17:50:07
Some examples of binary
classification are predicting

43
17:50:07 --> 17:50:12
whether it will rain tomorrow or not,
here there are two possible outcomes,

44
17:50:12 --> 17:50:15
yes it will rain tomorrow or
no, it will not rain tomorrow.

45
17:50:16 --> 17:50:21
Identifying whether a credit card
transaction is legitimate or fraudulent,

46
17:50:21 --> 17:50:25
again, there are only two possible values
for the target, legitimate or fraudulent.

47
17:50:26 --> 17:50:30
Some examples of multi-class
classification include

48
17:50:30 --> 17:50:34
predicting what type of product
that a customer will buy.

49
17:50:34 --> 17:50:37
The possible values for
the target variables would be product

50
17:50:37 --> 17:50:41
categories such as kitchen,
electronics, clothes, etc.

51
17:50:41 --> 17:50:45
There is more than one
category of products, so

52
17:50:45 --> 17:50:48
this is a multi-class
classification problem.

53
17:50:49 --> 17:50:54
Another example is categorizing a tweet
as having a positive, negative, or

54
17:50:54 --> 17:50:58
neutral sentiment, again,
the number of possible values for

55
17:50:58 --> 17:50:59
the target is more than two here.

56
17:50:59 --> 17:51:04
So, this is also a multi-class
classification task

57
17:51:04 --> 17:51:06
to summarize in classification,

58
17:51:06 --> 17:51:10
the model has to predict the category
corresponding to the input data.

59
17:51:10 --> 17:51:15
Since the target is provided for each
sample, classification is a supervised

60
17:51:15 --> 17:51:20
task, the target variable is always
categorical in classification.

1
11:38:24 --> 11:38:25
In this lecture,

2
11:38:25 --> 11:38:30
we will look at the decision tree model,
a popular method used for classification.

3
11:38:30 --> 11:38:35
After this video, you will be able to
explain how a decision tree is used for

4
11:38:35 --> 11:38:37
classification.

5
11:38:37 --> 11:38:41
Describe the process of constructing
a decision tree for classification.

6
11:38:41 --> 11:38:45
And interpret how a decision tree comes
up with a classification decision.

7
11:38:46 --> 11:38:50
The idea behind decision trees for
classification is to split the data

8
11:38:50 --> 11:38:56
into subsets where each subset
belongs to only one class.

9
11:38:56 --> 11:39:00
This is accomplished by dividing
the input space into pure regions,

10
11:39:00 --> 11:39:04
that is regions with samples
from only one class.

11
11:39:05 --> 11:39:09
With real data completely pure
subsets may not be possible.

12
11:39:09 --> 11:39:15
So the goal is to divide the data into
subsets that are as pure as possible.

13
11:39:15 --> 11:39:20
That is each subset contains as many
samples as possible from a single class.

14
11:39:21 --> 11:39:26
Graphically this is equivalent to dividing
the input space into regions that are as

15
11:39:26 --> 11:39:27
pure as possible.

16
11:39:27 --> 11:39:32
Boundaries separating these regions
are called decision boundaries.

17
11:39:32 --> 11:39:36
And the decision tree model
makes classification decisions

18
11:39:36 --> 11:39:38
based on these decision boundaries.

19
11:39:39 --> 11:39:46
A decision tree is a hierarchical
structure with nodes and directed edges.

20
11:39:46 --> 11:39:48
The node at the top is
called the root node.

21
11:39:49 --> 11:39:51
The nodes at the bottom
are called the leaf nodes.

22
11:39:53 --> 11:39:58
Nodes that are neither the root node or
the leaf nodes are called internal nodes.

23
11:39:58 --> 11:40:01
The root and
internal nodes have test conditions,

24
11:40:02 --> 11:40:05
each leaf node has a class
label associated with it.

25
11:40:06 --> 11:40:11
A classification decision is made
by traversing the decision tree

26
11:40:11 --> 11:40:12
starting with the root node.

27
11:40:13 --> 11:40:17
At each node the answer to the test
condition determines which

28
11:40:17 --> 11:40:19
branch to traverse to.

29
11:40:19 --> 11:40:23
When a leaf node is reached
the category at the leaf node

30
11:40:23 --> 11:40:25
determines the classification decision.

31
11:40:26 --> 11:40:32
The depth of a node is the number of
edges from the root node to that node.

32
11:40:32 --> 11:40:34
The depth of the root node is 0.

33
11:40:34 --> 11:40:38
The depth of a decision
tree is the number of edges

34
11:40:38 --> 11:40:42
in the longest path from
the root node to the leaf node.

35
11:40:42 --> 11:40:46
The size of a decision tree is
the number of nodes in the tree.

36
11:40:47 --> 11:40:50
This is an example of a decision tree.

37
11:40:50 --> 11:40:55
It can be used to classify an animal
as a mammal or not a mammal.

38
11:40:55 --> 11:40:59
According to this decision tree,
if an animal is warm-blooded,

39
11:40:59 --> 11:41:04
gives live birth, and
is a vertebrate, then it is a mammal.

40
11:41:04 --> 11:41:08
If an animal does not have all
of these three characteristics,

41
11:41:08 --> 11:41:09
then it is not a mammal.

42
11:41:10 --> 11:41:14
A decision tree is built by starting
with all samples at a single node,

43
11:41:14 --> 11:41:16
the root node.

44
11:41:16 --> 11:41:21
Additional nodes are added when
the data is split into subsets.

45
11:41:21 --> 11:41:25
At a high level, constructing a decision
tree consists of the following steps.

46
11:41:26 --> 11:41:28
Start with all samples and a node,

47
11:41:30 --> 11:41:34
partition the samples into subsets
based in the input variables.

48
11:41:34 --> 11:41:39
The goal is to create subsets of
records that are purest, that is

49
11:41:39 --> 11:41:43
each subset contains as many samples as
possible, belonging to just one class.

50
11:41:45 --> 11:41:49
Another way to say this is that
the subsets should be as homogeneous or

51
11:41:49 --> 11:41:50
as pure as possible.

52
11:41:52 --> 11:41:56
Repeatedly partition data into
successively purer subsets

53
11:41:56 --> 11:41:58
until some stopping
criterion is satisfied.

54
11:41:59 --> 11:42:00
An algorithm for

55
11:42:00 --> 11:42:05
constructing a decision tree model is
referred to as an induction algorithm.

56
11:42:05 --> 11:42:09
So you may hear the term tree induction
used to describe the process of

57
11:42:09 --> 11:42:10
building a decision tree.

58
11:42:12 --> 11:42:15
Note that at each split
the induction algorithm

59
11:42:15 --> 11:42:19
only considers the best way to split
that particular portion of the data.

60
11:42:19 --> 11:42:22
This is referred to as a greedy approach.

61
11:42:22 --> 11:42:26
Greedy algorithms solve a subset
of the problem at a time, and

62
11:42:26 --> 11:42:31
as a necessary approach when solving
the entire problem is not feasible.

63
11:42:31 --> 11:42:34
This is the case with decision trees.

64
11:42:34 --> 11:42:38
It is not feasible to determine
the best tree given a data set, so

65
11:42:38 --> 11:42:41
the tree has to be built
in piecemeal fashion

66
11:42:41 --> 11:42:44
by determining the best way to split
the current node at each step.

67
11:42:44 --> 11:42:49
And combining these decisions together
to form the final decision tree,

68
11:42:49 --> 11:42:54
in constructing a decision tree
how is the data partitioned?

69
11:42:54 --> 11:42:57
How does a decision tree
determine the best way to split

70
11:42:57 --> 11:42:59
the set of samples at a node?

71
11:42:59 --> 11:43:04
Again the goal is to partition data
at a node into subsets that are as

72
11:43:04 --> 11:43:05
pure as possible.

73
11:43:06 --> 11:43:08
In this example,

74
11:43:08 --> 11:43:12
the partition shown on the right
results in more homogeneous subsets.

75
11:43:12 --> 11:43:17
Since these subsets contain more
samples belonging to a single class

76
11:43:17 --> 11:43:20
than the resulting subsets
shown on the left.

77
11:43:20 --> 11:43:24
So the partition on the right
results in purer subsets and

78
11:43:24 --> 11:43:26
is the preferred partition.

79
11:43:27 --> 11:43:31
Therefore, we need a way to
measure the purity of a split

80
11:43:31 --> 11:43:35
in order to compare different
ways to partition a set of data.

81
11:43:35 --> 11:43:40
It turns out that it works out better
mathematically if we measure the impurity

82
11:43:40 --> 11:43:42
rather than the purity of a split.

83
11:43:43 --> 11:43:46
So the impurity measure of a node

84
11:43:46 --> 11:43:49
specifies how mixed
the resulting subsets are.

85
11:43:49 --> 11:43:53
Since we want the resulting subsets
to have homogeneous class labels,

86
11:43:53 --> 11:43:59
not mixed class labels, we want the split
that minimizes the impurity measure.

87
11:44:00 --> 11:44:02
A common impurity measure used for

88
11:44:02 --> 11:44:04
determining the best
split is the Gini Index.

89
11:44:05 --> 11:44:10
The lower the Gini Index the higher
the purity of the split.

90
11:44:10 --> 11:44:15
So the decision tree will select
the split that minimizes the Gini Index.

91
11:44:15 --> 11:44:19
Besides the Gini Index, other
impurity measures include entropy, or

92
11:44:19 --> 11:44:22
information gain, and
misclassification rate.

93
11:44:24 --> 11:44:27
The other factor in determining
the best way to partition a node

94
11:44:27 --> 11:44:30
is which variable to split on.

95
11:44:30 --> 11:44:35
The decision tree will test all variables
to determine the best way to split a node

96
11:44:35 --> 11:44:39
using a purity measure such as
the Gini index to compare the various

97
11:44:39 --> 11:44:40
possibilities.

98
11:44:41 --> 11:44:46
Recall that the tree induction algorithm
repeatedly splits nodes to get more and

99
11:44:46 --> 11:44:48
more homogeneous subsets.

100
11:44:48 --> 11:44:50
So when does this process stop?

101
11:44:50 --> 11:44:53
When does the algorithm
stop growing the tree?

102
11:44:54 --> 11:44:58
There's several criteria that can be used
to determine when a node should no longer

103
11:44:58 --> 11:44:59
be split into subsets.

104
11:45:01 --> 11:45:04
The induction algorithm can
stop expanding a node when

105
11:45:04 --> 11:45:08
all samples in the node
have the same class label.

106
11:45:08 --> 11:45:12
This means that this set of data
is as pure as possible, and

107
11:45:12 --> 11:45:16
further splitting will not result in
any better partition of the data.

108
11:45:17 --> 11:45:21
Since getting completely pure subsets
is difficult to achieve with real data,

109
11:45:21 --> 11:45:24
this stopping criterion can be modified.

110
11:45:24 --> 11:45:28
To when a certain percentage of
the samples in the node, say 90% for

111
11:45:28 --> 11:45:31
example, have the same class labels.

112
11:45:32 --> 11:45:36
The algorithm can stop expanding a node
when the number of samples in the node

113
11:45:36 --> 11:45:38
falls below a certain minimum value.

114
11:45:39 --> 11:45:42
A this point the number of
samples is too small to make

115
11:45:42 --> 11:45:46
much difference in the classification
results with the further splitting.

116
11:45:47 --> 11:45:51
The induction algorithm can stop expanding
a node when the improvement in impurity

117
11:45:51 --> 11:45:56
measure is too small to make much of
a difference in classification results.

118
11:45:58 --> 11:46:02
The algorithm can stop expanding a node
when the maximum tree depth is reached.

119
11:46:02 --> 11:46:06
This is to control the complexity
of the resulting tree.

120
11:46:08 --> 11:46:11
There can be other criteria that can be
used to determine when tree induction

121
11:46:11 --> 11:46:12
should stop.

122
11:46:13 --> 11:46:18
Let's take a look at an example to
illustrate the tree induction process.

123
11:46:18 --> 11:46:22
Let's say that we want to classify loan
applicants as being likely to repay a loan

124
11:46:22 --> 11:46:26
or not likely to repay a loan
based on their income and

125
11:46:26 --> 11:46:27
amount of debt they already have.

126
11:46:29 --> 11:46:30
Building a decision tree for

127
11:46:30 --> 11:46:33
this classification problem
could proceed as follows.

128
11:46:33 --> 11:46:37
Consider the input space of this
problem as shown in the left figure.

129
11:46:37 --> 11:46:41
One way to split this
data set into homogeneous

130
11:46:41 --> 11:46:46
subsets is to consider the decision
boundary where income equals t1.

131
11:46:47 --> 11:46:51
To the right of this decision
boundary are mostly red samples and

132
11:46:51 --> 11:46:53
to the left are mostly blue samples.

133
11:46:54 --> 11:46:56
The subsets are not
completely homogeneous but

134
11:46:56 --> 11:47:01
that is the best way to split the original
data set based on the variable income.

135
11:47:03 --> 11:47:07
This decision boundary is represented
in the decision tree by the condition

136
11:47:07 --> 11:47:11
Income is greater than
t1 at the root node.

137
11:47:11 --> 11:47:14
This is the condition used to
split the original data set.

138
11:47:14 --> 11:47:18
Samples with income greater
than the threshold value

139
11:47:18 --> 11:47:22
of t1 are placed in the right subset and
samples with income less than or

140
11:47:22 --> 11:47:27
equal to t1 are placed in the left subset,
just as shown in the right diagram.

141
11:47:29 --> 11:47:31
The right subset is now labeled as red,

142
11:47:31 --> 11:47:35
meaning that the loan applicant
is likely to be paid alone.

143
11:47:36 --> 11:47:41
The second step then is to determine how
to split the region outlined in red.

144
11:47:41 --> 11:47:47
As shown in the left diagram, in input
space, the best way to split this data

145
11:47:47 --> 11:47:50
is specified by the second decision
boundary with debt equals t2.

146
11:47:52 --> 11:47:55
This is represented in
the decision tree on the right

147
11:47:55 --> 11:47:59
with the addition of the node with
condition debt greater than t2.

148
11:48:01 --> 11:48:04
Samples with the value of debt greater
than t2 are shown in the region

149
11:48:04 --> 11:48:07
above the decision boundary.

150
11:48:07 --> 11:48:12
This region contains all blue samples and
so the corresponding node is labeled blue.

151
11:48:12 --> 11:48:15
Meaning that the loan applicant
is not likely to repay the loan.

152
11:48:17 --> 11:48:21
The third and final split looks at how
to split the region outlined in red

153
11:48:21 --> 11:48:22
in the left diagram.

154
11:48:23 --> 11:48:27
The best split is specified by
the boundary with income equals T3.

155
11:48:27 --> 11:48:31
This splits the red region
into two pure subsets.

156
11:48:32 --> 11:48:36
This split is represented in the decision
tree by adding a node with condition,

157
11:48:36 --> 11:48:38
Income is greater than t3.

158
11:48:38 --> 11:48:42
The left resulting node is labeled blue,
and

159
11:48:42 --> 11:48:44
the right resulting node is labeled red,

160
11:48:44 --> 11:48:49
corresponding to the resulting subsets
within the red border in the left diagram.

161
11:48:50 --> 11:48:53
We end with the final
decision tree on the right,

162
11:48:53 --> 11:48:58
which implements the decision boundaries
shown as dash lines in the left diagram.

163
11:48:59 --> 11:49:03
These decision boundaries
partition the data set as shown.

164
11:49:03 --> 11:49:08
The label for each region is determined by
the label of the majority of the samples.

165
11:49:08 --> 11:49:12
These labels are reflected in
the leaf nodes of the decision tree.

166
11:49:12 --> 11:49:12
Shown on the right.

167
11:49:14 --> 11:49:18
You may have noticed that the decision
boundaries of a decision tree are parallel

168
11:49:18 --> 11:49:25
to the axes formed by the variables,
this is referred to as being rectilinear.

169
11:49:25 --> 11:49:29
The boundaries are rectilinear
because each split considers only

170
11:49:29 --> 11:49:31
a single variable.

171
11:49:31 --> 11:49:35
There are variance of the tree induction
algorithm that consider more than one

172
11:49:35 --> 11:49:37
attribute when splitting a note.

173
11:49:37 --> 11:49:42
However, each split has to consider all
combinations of combined variables and

174
11:49:42 --> 11:49:47
so such induction algorithms are much
more computationally expensive.

175
11:49:48 --> 11:49:51
There are a few important things to
note about the decision tree classifier.

176
11:49:53 --> 11:49:56
The resulting tree is often simple
to understand and interpret.

177
11:49:56 --> 11:50:01
This is one of the biggest advantages
of decision trees for classification.

178
11:50:01 --> 11:50:04
It is often possible to look
at the resulting tree to see

179
11:50:04 --> 11:50:08
which variables are important to
the classification problem and

180
11:50:08 --> 11:50:11
understand how
the classification is performed.

181
11:50:11 --> 11:50:15
For this reason, many people will start
out with the decision tree classifier

182
11:50:15 --> 11:50:18
to get a feel for
the classification problem.

183
11:50:18 --> 11:50:21
Even if they end up using more
sophisticated models later on.

184
11:50:23 --> 11:50:26
The tree induction algorithm
as described in this lesson

185
11:50:26 --> 11:50:29
is relatively computationally inexpensive.

186
11:50:29 --> 11:50:33
So training a decision tree for
classification can be relatively fast.

187
11:50:35 --> 11:50:40
The greedy approach used by tree induction
algorithm determines the best way to split

188
11:50:40 --> 11:50:42
the portion of the data at a node but

189
11:50:42 --> 11:50:46
does not guarantee the best solution
overall for the entire data set.

190
11:50:49 --> 11:50:51
Decision boundaries are rectilinear.

191
11:50:51 --> 11:50:55
This can limit the expressiveness
of the resulting model which means

192
11:50:55 --> 11:50:59
that it may not be able to solve
complicated classification problems that

193
11:50:59 --> 11:51:02
require more complex decision
boundaries to be formed.

194
11:51:04 --> 11:51:09
In summary, the decision tree classifier
uses a tree like structure to specify

195
11:51:09 --> 11:51:14
a series of conditions that are tested to
determine the class label for a sample.

196
11:51:15 --> 11:51:20
The decision tree is constructed by
repeatedly splitting a data partition

197
11:51:20 --> 11:51:23
into successively more
homogeneous subsets.

198
11:51:23 --> 11:51:26
The resulting tree can
often be easy to interpret.

1
23:29:49 --> 23:29:54
We will start with a very simple
classification technique called k-Nearest

2
23:29:54 --> 23:29:54
Neighbors.

3
23:29:54 --> 23:29:59
After this video, you will be able
to describe how kNN is used for

4
23:29:59 --> 23:30:01
classification.

5
23:30:01 --> 23:30:06
Discuss the assumption behind kNN and
explain what the k stands for in kNN.

6
23:30:06 --> 23:30:10
kNN stands for k-Nearest Neighbors.

7
23:30:10 --> 23:30:14
This is one of the simplest techniques
to build a classification model.

8
23:30:14 --> 23:30:19
The basic idea is to classify
a sample based on its neighbors.

9
23:30:19 --> 23:30:24
So when you get a new sample as shown by
the green circle in the figure, the class

10
23:30:24 --> 23:30:29
label for that sample is determined by
looking at the labels of its neighbors.

11
23:30:29 --> 23:30:33
KNN relies on the notion
of the so-called duck test.

12
23:30:33 --> 23:30:37
That is if it looks like a duck,
swims like a duck and quacks like a duck,

13
23:30:37 --> 23:30:40
then it probably is a duck.

14
23:30:40 --> 23:30:42
In the classification context,

15
23:30:42 --> 23:30:48
this means that samples with similar input
values likely belong to the same class.

16
23:30:48 --> 23:30:52
So, samples with similar input values
should be labeled with the same

17
23:30:52 --> 23:30:54
target label.

18
23:30:54 --> 23:30:57
This means that classification
of a sample is dependent

19
23:30:57 --> 23:31:01
on the target labels of
the neighboring points.

20
23:31:01 --> 23:31:04
In more detail then,
this is how kNN works.

21
23:31:04 --> 23:31:06
Given a new sample, look for

22
23:31:06 --> 23:31:10
the samples in the training data
that are closest to the new sample.

23
23:31:10 --> 23:31:12
These are the neighbors.

24
23:31:12 --> 23:31:17
Use the labels of this neighboring points
to determine the label for the new sample.

25
23:31:18 --> 23:31:21
This figure illustrate how kNN works.

26
23:31:21 --> 23:31:25
The problem here is to determine if
a sample should be classified as

27
23:31:25 --> 23:31:26
a blue square or red triangle.

28
23:31:27 --> 23:31:29
The green circle is the new sample.

29
23:31:29 --> 23:31:34
To determine a class label for this new
sample, look at its closest neighbors.

30
23:31:34 --> 23:31:38
These neighbors are the samples
within the dashed circle.

31
23:31:39 --> 23:31:42
Two blue squares and one red triangle.

32
23:31:42 --> 23:31:45
The class labels of the neighboring
samples determine the label for

33
23:31:45 --> 23:31:46
the new sample.

34
23:31:47 --> 23:31:52
The value of k determines the number
of nearest neighbor to consider.

35
23:31:52 --> 23:31:57
So if k equals 1,
then only the closest neighbor is examined

36
23:31:57 --> 23:32:01
to determine the class of the new
sample as shown in the left figure.

37
23:32:02 --> 23:32:03
If k equals 2,

38
23:32:03 --> 23:32:08
then the 2 nearest neighbors are
considered as seen in the middle figure.

39
23:32:08 --> 23:32:12
If k equal 3, then the 3 nearest
neighbors are considered as

40
23:32:12 --> 23:32:15
in the right figure and so on.

41
23:32:15 --> 23:32:19
If k equal 1 and only 1 neighbor is used,
then the label for

42
23:32:19 --> 23:32:23
the new sample is simpler
the label of the neighbor.

43
23:32:23 --> 23:32:26
This is shown in the left figure.

44
23:32:26 --> 23:32:28
The label of the new sample is then A,

45
23:32:28 --> 23:32:31
since that is the label of
its one nearest neighbor.

46
23:32:32 --> 23:32:37
When multiple neighbors are considered,
then a voting scheme is used.

47
23:32:37 --> 23:32:41
Majority of vote is commonly used,
so the label associated with

48
23:32:41 --> 23:32:46
the majority of the neighbors is
used as the label of the new sample.

49
23:32:46 --> 23:32:49
This is what we see in the right figure.

50
23:32:49 --> 23:32:53
With k equals 3,
3 nearest neighbors are considered.

51
23:32:54 --> 23:32:57
With two neighbors labeled as A and
one as B,

52
23:32:57 --> 23:33:01
the majority of vote determines that
the new sample should be labeled as A.

53
23:33:02 --> 23:33:07
In case of a tie which could be
possible if the value of k is even,

54
23:33:07 --> 23:33:10
then some tight breaking rule is needed.

55
23:33:10 --> 23:33:13
For example, the label of
the closer neighbor is used or

56
23:33:13 --> 23:33:17
the label is chosen randomly
among the neighbors.

57
23:33:17 --> 23:33:19
This is seen in the middle figure.

58
23:33:20 --> 23:33:24
With two nearest neighbors and each with
a different class label, the label for

59
23:33:24 --> 23:33:27
the new sample is randomly
chosen here to be B.

60
23:33:29 --> 23:33:33
With kNN, some measure of similarity
is needed to determine how

61
23:33:33 --> 23:33:36
close two samples are together.

62
23:33:36 --> 23:33:39
This is necessary to determine which
samples are the nearest neighbors.

63
23:33:40 --> 23:33:44
Distance measures such as
distance are commonly used.

64
23:33:44 --> 23:33:48
Other distance measures that can be used,
include Manhattan and hemming distance.

65
23:33:49 --> 23:33:54
To summarize, kNN is a very
simple classification technique.

66
23:33:54 --> 23:33:57
Note that there is no
separate training phase.

67
23:33:57 --> 23:34:02
There is no separate part where a model is
constructed and its parameter is adjusted.

68
23:34:02 --> 23:34:05
This is unlike most other
classification algorithms.

69
23:34:07 --> 23:34:11
KNN can generate complex
decision boundaries allowing for

70
23:34:11 --> 23:34:13
complex classification
decisions to be made.

71
23:34:15 --> 23:34:17
It can be susceptible to noise,however,

72
23:34:17 --> 23:34:21
because classification decisions
are made using only information about

73
23:34:21 --> 23:34:24
a few neighboring points
instead of the entire dataset.

74
23:34:26 --> 23:34:30
KNN can be slow, however, since
the distance between a new sample and

75
23:34:30 --> 23:34:34
all sample points in the data must
be calculated in order to determine

76
23:34:34 --> 23:34:38
the k-Nearest Neighbors.

1
23:04:27 --> 23:04:32
In this lecture, we will discuss
the Naive Bayes classifier.

2
23:04:32 --> 23:04:36
After this video, you will be able
to discuss how a Naive Bayes model

3
23:04:36 --> 23:04:41
works fro classification,
define the components of Bayes' Rule and

4
23:04:41 --> 23:04:44
explain what the naive
means in Naive Bayes.

5
23:04:45 --> 23:04:51
A Naive Bayes classification model uses a
probabilistic approach to classification.

6
23:04:51 --> 23:04:55
What this means is that the relationships
between the input features and

7
23:04:55 --> 23:04:58
the class is expressed as probabilities.

8
23:04:58 --> 23:05:03
So given the input features for
a sample, the probability for

9
23:05:03 --> 23:05:05
each class is estimated.

10
23:05:05 --> 23:05:10
The class with the highest probability
then, determines the label for the sample.

11
23:05:12 --> 23:05:17
In addition to using a probabilistic
framework for classification,

12
23:05:17 --> 23:05:22
the Naive Bayes classifier also uses
what is known as Bayes' theorem.

13
23:05:22 --> 23:05:25
The application of Bayes' theorem makes
estimating the probabilities easier.

14
23:05:27 --> 23:05:31
In addition, Naive Bayes assumes that
the input features are statistically

15
23:05:31 --> 23:05:34
independent of one another.

16
23:05:34 --> 23:05:37
This means that, for a given class,

17
23:05:37 --> 23:05:42
the value of one feature does not
affect the value of any other feature.

18
23:05:42 --> 23:05:46
This independence assumption is
an oversimplified one that does not always

19
23:05:46 --> 23:05:50
hold true, and so
is considered a naive assumption.

20
23:05:50 --> 23:05:52
The naive independence assumption and

21
23:05:52 --> 23:05:55
the use of Bayes theorem gives this
classification model its name.

22
23:05:56 --> 23:05:58
We will cover Bayes theorem and

23
23:05:58 --> 23:06:01
the independence assumption in
more detail in this lecture.

24
23:06:02 --> 23:06:05
Before we look at naive
Bayes in more detail,

25
23:06:05 --> 23:06:07
let's first start with some
background on probability.

26
23:06:08 --> 23:06:12
Probability is the measure
of how likely an event is,

27
23:06:12 --> 23:06:18
the probability of an event A occurring
is denoted P and in parenthesis A.

28
23:06:19 --> 23:06:24
It is calculated by dividing
the number of ways event A can occur,

29
23:06:24 --> 23:06:26
by the total number of possible outcomes.

30
23:06:28 --> 23:06:34
For example, what is the probability
of rolling a die and getting six?

31
23:06:34 --> 23:06:37
When you roll a die you can get
a number from one to six, so

32
23:06:37 --> 23:06:40
the number of possible outcomes is six.

33
23:06:41 --> 23:06:44
The number of ways fro getting six is one,

34
23:06:44 --> 23:06:48
since the way you can get six is if
the die shows six when it stops rolling.

35
23:06:49 --> 23:06:54
That means that the probability of getting
the number six when you roll a die

36
23:06:54 --> 23:06:57
is one over six.

37
23:06:57 --> 23:07:01
This is denoted p of six and
that's equal to one over six and

38
23:07:01 --> 23:07:05
is read as probability
of six is one over six.

39
23:07:07 --> 23:07:12
There's also a joint probability,
the joint probability specifies

40
23:07:12 --> 23:07:16
the probability of event A and
event B occurring together.

41
23:07:18 --> 23:07:23
In this diagram, the probability of event
A occurring is shown as the blue circle

42
23:07:24 --> 23:07:28
and the probability of event B
occurring is shown as the green circle.

43
23:07:29 --> 23:07:34
Then the joint probability,
that is the probability of A and

44
23:07:34 --> 23:07:39
B occurring together is shown as
the overlap of these two circles.

45
23:07:40 --> 23:07:45
The joint probability of A and
B is denoted,

46
23:07:45 --> 23:07:48
P(A,B) for

47
23:07:48 --> 23:07:53
an example of joint probability,
let's consider rolling 2 dice together.

48
23:07:53 --> 23:07:58
What is the probability in getting
2 sixes or a six from each die.

49
23:07:59 --> 23:08:04
If the two events are independent, then
the joint probability is simply the result

50
23:08:04 --> 23:08:07
of multiplying the probabilities
of the individual events together.

51
23:08:09 --> 23:08:12
In this case then, we have
the probability of rolling a six for

52
23:08:12 --> 23:08:18
each die is one over six so
the joint probability is one over 36,

53
23:08:18 --> 23:08:23
this leads us to conditional probability.

54
23:08:23 --> 23:08:28
The conditional probability is
the probability of event A occurring

55
23:08:28 --> 23:08:31
Given that event B has already occurred.

56
23:08:32 --> 23:08:38
Another way to say this is that
event A is conditioned on event B.

57
23:08:38 --> 23:08:43
The conditional probability is
the noted P and in parentheses A,

58
23:08:43 --> 23:08:48
vertical line B and is read as,
probability of A Given B.

59
23:08:49 --> 23:08:54
This diagram gives a graphical
definition of conditional probability.

60
23:08:54 --> 23:09:00
As before, the blue circle is
the probability of event A occurring,

61
23:09:00 --> 23:09:04
the green circle is the probability
of event B occurring.

62
23:09:04 --> 23:09:07
The overlap is a joint
probability of A and B.

63
23:09:08 --> 23:09:13
The conditional probability,
P(A given B) then is

64
23:09:13 --> 23:09:18
calculated as the join probability
divided by the probability of B.

65
23:09:20 --> 23:09:23
The conditional probability is
an important concept in classification

66
23:09:23 --> 23:09:25
as we will see later.

67
23:09:25 --> 23:09:30
It provides the means to specify
the probability of a class label,

68
23:09:30 --> 23:09:31
given the input values.

69
23:09:32 --> 23:09:39
The relationship between conditional
probabilities P of B given A and

70
23:09:39 --> 23:09:44
P of A given B can be expressed
through Bayes' Theorem.

71
23:09:44 --> 23:09:48
This theorem is named after a reverend
named Thomas Bayes who lived in the 1700s.

72
23:09:48 --> 23:09:53
It is a way to look at how the probability
of a hypothesis is affected by

73
23:09:53 --> 23:09:56
new evidence gathered from data.

74
23:09:56 --> 23:10:01
Bayes' theorem expresses the relationship
between probability of B

75
23:10:01 --> 23:10:07
given A and probability of A given
B as shown in this equation.

76
23:10:07 --> 23:10:13
Bayes' Theorem is also known
as Bayes' Rule or Bayes' Law.

77
23:10:13 --> 23:10:16
Now that we have reviewed some
background on probability

78
23:10:16 --> 23:10:19
let's see how all this relates
to the classification problem.

79
23:10:20 --> 23:10:25
With the probabilistic framework the
classification task is defined as follows.

80
23:10:26 --> 23:10:30
Capital X is the set of values for
the input features in the sample,

81
23:10:32 --> 23:10:36
given a sample with features X,
predict the corresponding class C.

82
23:10:37 --> 23:10:42
Another way to state this is,
what is the class label associated with

83
23:10:42 --> 23:10:48
the feature vector X or how should
the feature vector x be classified?

84
23:10:49 --> 23:10:54
To find the class label C we need to
calculate the conditional probability of

85
23:10:54 --> 23:10:58
class C, given X for all classes and

86
23:10:58 --> 23:11:00
select a class with
the highest probability.

87
23:11:02 --> 23:11:04
So for classification,

88
23:11:04 --> 23:11:10
we want to find the value of C that
maximizes the probability of C given X.

89
23:11:10 --> 23:11:14
The problem is that it is difficult
to estimate this probability,

90
23:11:14 --> 23:11:19
because we would need to enumerate every
possible combination of feature values and

91
23:11:19 --> 23:11:21
to know the conditional probability.

92
23:11:21 --> 23:11:26
For each class given every
possible feature combination and

93
23:11:26 --> 23:11:29
here is where Bayes'
theorem comes into play.

94
23:11:29 --> 23:11:33
The classification problem can be
reformulated using Bayes' theorem

95
23:11:33 --> 23:11:35
to simplify the classification problem.

96
23:11:35 --> 23:11:43
Specifically, using Bayes' theorem, the
probability of c given x, can be expressed

97
23:11:43 --> 23:11:47
using other probability quantities,
which can be estimated from the data.

98
23:11:49 --> 23:11:53
Here's Bayes' theorem again, but
some additional terms defined.

99
23:11:53 --> 23:11:58
Probability of C I X is referred to
as the posterior probability since

100
23:11:58 --> 23:12:04
it is the probability of the class label
being C after observing input features X.

101
23:12:06 --> 23:12:10
Probability of X given
C is the probability of

102
23:12:10 --> 23:12:15
observing input features X given
that c is the class label.

103
23:12:16 --> 23:12:21
This is the class conditional probability
since it is conditioned on the class.

104
23:12:22 --> 23:12:27
Probability of c is the probability
of the class label being C,

105
23:12:28 --> 23:12:33
this is the probability of each class
prior to observing any input data.

106
23:12:33 --> 23:12:36
And so
is referred to as the prior probability.

107
23:12:37 --> 23:12:41
The probability of X is the probability

108
23:12:41 --> 23:12:45
of observing input features X
regardless of what the class label is.

109
23:12:47 --> 23:12:51
So for classification we want
to calculate the posterior

110
23:12:51 --> 23:12:54
probability P(C | X) for each class C.

111
23:12:54 --> 23:13:01
From Bayes' theorem P(C | X) is related to

112
23:13:01 --> 23:13:07
the P(X | C) P(C And probability of X.

113
23:13:08 --> 23:13:12
Probability of X does not depend
on the class C, therefore,

114
23:13:12 --> 23:13:16
it is a constant value, given the input X.

115
23:13:16 --> 23:13:19
Since it the same value for
all classes, the probability

116
23:13:19 --> 23:13:24
of X can be removed from the calculation
of probability of C, given X.

117
23:13:25 --> 23:13:30
What's left then are probability of
X given C and the probability of C.

118
23:13:32 --> 23:13:36
So estimating the probability
of C given X boils down to

119
23:13:36 --> 23:13:41
estimating the probability of X
given C and probability of C.

120
23:13:41 --> 23:13:45
The nice thing is that
probability of X given C and

121
23:13:45 --> 23:13:49
probability of C can be
estimated from the data.

122
23:13:50 --> 23:13:55
So now we have a way to calculate probably
of C given X which is what we need for

123
23:13:55 --> 23:13:55
classification.

124
23:13:57 --> 23:13:58
To the estimate the probability of C

125
23:13:59 --> 23:14:04
which is the probability of the class
of C before observing any input data.

126
23:14:04 --> 23:14:08
We simply calculate the fraction
of samples with that class label

127
23:14:08 --> 23:14:09
C in the training data.

128
23:14:11 --> 23:14:15
For this example, there are four samples
labeled as green circles out of 10

129
23:14:15 --> 23:14:22
samples, so probability of green
circle is 4 out of 10, or 0.4.

130
23:14:22 --> 23:14:28
Similarly, the fraction of samples labeled
as red triangles is 6 out of 10, or 0.6.

131
23:14:28 --> 23:14:33
So estimating the prior
probabilities is a simple count

132
23:14:33 --> 23:14:36
of number of samples with each class label

133
23:14:36 --> 23:14:39
divided by the total number of
samples in the training data center.

134
23:14:41 --> 23:14:46
In estimating probability of X
given C which is the probability

135
23:14:46 --> 23:14:50
of observing feature factor
X given that the class is C,

136
23:14:50 --> 23:14:53
we can use the independent
assumption to simplify the problem.

137
23:14:54 --> 23:14:58
The Independence Assumption of
the Naive Bayes classifier assumes

138
23:14:58 --> 23:15:03
that each feature X sub I
in the featured vector X

139
23:15:03 --> 23:15:08
is conditionally independent of every
other feature, given the class C.

140
23:15:09 --> 23:15:14
This means that we only need to
estimate the probability of X

141
23:15:14 --> 23:15:18
sub i given C, instead of having to

142
23:15:18 --> 23:15:23
estimate the probability of
the entire feature X given C.

143
23:15:23 --> 23:15:25
For every combination of values for

144
23:15:25 --> 23:15:30
the features in X then we would simply
multiply these individual probabilities

145
23:15:30 --> 23:15:35
together to get the probability
of the entire feature vector X.

146
23:15:35 --> 23:15:41
Given the class C to estimate
the probability of X sub I,

147
23:15:41 --> 23:15:46
given C, we count up the number
of times a particular input

148
23:15:46 --> 23:15:50
value is observed for
the class c in the training data.

149
23:15:50 --> 23:15:55
For example, the number of times that we
see the value of yes for the future home

150
23:15:55 --> 23:16:02
owner, when the class label is no it's
three as indicated by the green arrows.

151
23:16:02 --> 23:16:07
This is divided by the number of samples
with no as the class label which is seven.

152
23:16:07 --> 23:16:10
This fraction, three out of seven,

153
23:16:10 --> 23:16:14
is the probability that home owner
is Yes given that the class is No.

154
23:16:17 --> 23:16:20
Similarly, the samples with
the value of Single for

155
23:16:20 --> 23:16:25
the feature Marital Status when it crosses
Yes are indicated by the red arrows.

156
23:16:25 --> 23:16:29
And the probability that
Marital Status is Single,

157
23:16:29 --> 23:16:34
given that the class label
is Yes is 2/3 or 0.67.

158
23:16:36 --> 23:16:41
Some things to know about the Naive Bayes
classification model are it is a fast and

159
23:16:41 --> 23:16:42
simple algorithm.

160
23:16:42 --> 23:16:46
The algorithm boils down to calculating
counts for probabilities and

161
23:16:46 --> 23:16:51
performing some multiplication, so
it is very simple to implement.

162
23:16:51 --> 23:16:55
And the probabilities that are needed
can be calculated with a single

163
23:16:55 --> 23:16:57
scan of the data set and
stored in a table.

164
23:16:58 --> 23:17:02
Either two processing of the data
is not necessary as with many other

165
23:17:02 --> 23:17:03
machine learning algorithms.

166
23:17:04 --> 23:17:09
So model building and
testing of both task, it scales well.

167
23:17:09 --> 23:17:11
Due today independent assumption,
the probability for

168
23:17:11 --> 23:17:15
each feature can be
independently estimated.

169
23:17:15 --> 23:17:19
These means that featured probability
is can be calculated in parallel,

170
23:17:19 --> 23:17:23
this also means that the data set size
does not have to grow exponentially

171
23:17:23 --> 23:17:24
with a number of features.

172
23:17:25 --> 23:17:29
This avoids the many problems associated
with the curse of dimensionality,

173
23:17:30 --> 23:17:33
this also means that you do not need
a lot of data to build the model.

174
23:17:34 --> 23:17:38
The number of parameters scales
linearly with the number of features.

175
23:17:39 --> 23:17:43
The Independence assumption does
not hold true in many cases.

176
23:17:43 --> 23:17:47
In practice however, the Naive Bayes
classifier still tends to perform very

177
23:17:47 --> 23:17:51
well this is because even
though Naive Bayes may

178
23:17:51 --> 23:17:55
not provide good estimates of
the correct class probabilities.

179
23:17:55 --> 23:17:59
As long as the correct class is
more probable than any other class,

180
23:17:59 --> 23:18:02
the correct classification
results will be reached.

181
23:18:03 --> 23:18:07
The independence assumption also prevents
the naive base classifier to model

182
23:18:07 --> 23:18:12
interactions between features which
limits its classification power.

183
23:18:12 --> 23:18:17
The increased risk of smoking in a history
of cancer would not be captured,

184
23:18:17 --> 23:18:17
for example.

185
23:18:19 --> 23:18:23
The Naive Bays classifier has been
applied to many real world problems

186
23:18:23 --> 23:18:27
including spam filtering, document
classification, and sentiment analysis.

187
23:18:28 --> 23:18:33
To summarize, the Naive Bayes classifier
uses a probabilistic framework for

188
23:18:33 --> 23:18:35
classification.

189
23:18:35 --> 23:18:38
It applies Bayes Theorem and
the Feature Independence Assumption,

190
23:18:38 --> 23:18:43
to simplify the problem of estimating
probabilities for the classification task.

1
22:23:11 --> 22:23:14
In addition to the evaluation
matrix covered n the last lecture.

2
22:23:14 --> 22:23:18
The performance of a classification
model can also be evaluated

3
22:23:18 --> 22:23:20
using a Confusion Matrix.

4
22:23:20 --> 22:23:22
We will introduce the Confusion Matrix,
in this lecture.

5
22:23:24 --> 22:23:26
After this video you will be able to,

6
22:23:26 --> 22:23:31
describe how a confusion matrix can
be used to evaluate a classifier.

7
22:23:31 --> 22:23:34
Interpret the confusion matrix of a model.

8
22:23:34 --> 22:23:37
And relate accuracy to values
in a confusion matrix.

9
22:23:38 --> 22:23:44
Let's use our example again of predicting
whether a given animal is a mammal or not.

10
22:23:44 --> 22:23:48
Recall that this is a binary
classification task, with the Class Label

11
22:23:48 --> 22:23:52
being either Yes, indicating mammal or
No indicating non-mammal.

12
22:23:54 --> 22:23:57
Now let's review the different Types
of Errors that you can get with

13
22:23:57 --> 22:23:58
Classification.

14
22:23:58 --> 22:24:00
If the True Label is Yes and

15
22:24:00 --> 22:24:05
the Predicted Label is Yes, then this
is a True Positive, abbreviated as TP.

16
22:24:06 --> 22:24:10
This is a case where the label is
correctly predicted as positive.

17
22:24:11 --> 22:24:13
If the True Label is No and

18
22:24:13 --> 22:24:18
the Predicted Label is No, then this
is a True Negative, abbreviated as TN.

19
22:24:19 --> 22:24:22
This is the case where the label is
correctly predicted as negative.

20
22:24:23 --> 22:24:25
If the True Label is No and

21
22:24:25 --> 22:24:30
the Predicted Label is Yes, then this
is a False Positive, abbreviated as FP.

22
22:24:32 --> 22:24:36
This is the case where the label is
incorrectly predicted as positive

23
22:24:36 --> 22:24:37
when it should be negative.

24
22:24:39 --> 22:24:42
If the True Label is Yes and
the Predicted Label is No,

25
22:24:42 --> 22:24:46
then this is a False Negative
abbreviated as FN.

26
22:24:47 --> 22:24:51
This is the case where the label is
incorrectly predicted as negative,

27
22:24:51 --> 22:24:52
when it should be positive.

28
22:24:54 --> 22:24:57
A Confusion Matrix can be used to
summarize the different types of

29
22:24:57 --> 22:24:59
classification errors.

30
22:25:00 --> 22:25:04
The True Positive cell corresponds to the
samples that are correctly predicted as

31
22:25:04 --> 22:25:06
positive by the model.

32
22:25:07 --> 22:25:11
The True Negative cell corresponds to
the samples that are correctly predicted

33
22:25:11 --> 22:25:12
as negative.

34
22:25:13 --> 22:25:17
The False Positive cell corresponds to
samples that are incorrectly predicted

35
22:25:17 --> 22:25:17
as positive.

36
22:25:18 --> 22:25:23
The False Negative cell corresponds to
samples that are incorrectly predicted

37
22:25:23 --> 22:25:23
as negative.

38
22:25:25 --> 22:25:29
Each cell has the count, or percentage
of samples, with each type of errors.

39
22:25:30 --> 22:25:35
Let's look at an example to see how
a Confusion Matrix is filled in.

40
22:25:35 --> 22:25:39
The table on the lists True Labels
along with the models prediction for

41
22:25:39 --> 22:25:41
a data set of ten samples.

42
22:25:41 --> 22:25:44
We'll summarize this results
in a Confusion Matrix.

43
22:25:46 --> 22:25:49
First let's figure out
the number of true positives.

44
22:25:49 --> 22:25:52
We call that a true positive occurrence
when the output is correctly predicted

45
22:25:52 --> 22:25:54
as positive.

46
22:25:54 --> 22:25:59
In other words, the true label is Yes and
the model's prediction is also Yes.

47
22:25:59 --> 22:26:04
In this example, there are three true
positives as indicated by the red arrows.

48
22:26:04 --> 22:26:08
We enter three and the true positive
cell in the Confusion Matrix.

49
22:26:09 --> 22:26:12
Now let's look at the true negatives.

50
22:26:12 --> 22:26:16
A true negative occurs when the output
is correctly predicted as negative.

51
22:26:16 --> 22:26:22
In other words, the true label is No and
the models prediction is also No.

52
22:26:22 --> 22:26:23
In this example,

53
22:26:23 --> 22:26:27
there are four true negatives as
indicated by the green arrows.

54
22:26:27 --> 22:26:31
We enter four in the true negative
cell in the Confusion Matrix.

55
22:26:32 --> 22:26:34
What about false negatives?

56
22:26:34 --> 22:26:39
A false negative occurs when the output
is incorrectly predicted as negative,

57
22:26:39 --> 22:26:41
when it should be positive.

58
22:26:41 --> 22:26:44
That is the true label is Yes,
and the model's prediction is No.

59
22:26:45 --> 22:26:46
In this example,

60
22:26:46 --> 22:26:50
there are two false negatives as
indicated by the purple arrows.

61
22:26:50 --> 22:26:54
We enter two in the false negative
cell in the Confusion Matrix.

62
22:26:55 --> 22:26:59
Finally we need to look
at false positives.

63
22:26:59 --> 22:27:00
A false positive occurs,

64
22:27:00 --> 22:27:05
when the input is incorrectly predicted
as positive, when it should be negative.

65
22:27:05 --> 22:27:10
That is the true label is No and
models prediction is Yes.

66
22:27:10 --> 22:27:14
In this example there is one false
positive as indicated by the yellow arrow.

67
22:27:15 --> 22:27:18
We enter one in the false positive
cell in the Confusion Matrix.

68
22:27:20 --> 22:27:23
This is our complete Confusion Matrix for
this example.

69
22:27:23 --> 22:27:26
We see that the sum of the numbers
in the cells add up to ten,

70
22:27:26 --> 22:27:28
which is the number of
samples in our dataset.

71
22:27:30 --> 22:27:33
Note that the diagonal values,
true positives and

72
22:27:33 --> 22:27:37
true negatives are samples
with Correct Predictions.

73
22:27:37 --> 22:27:40
In our example, these values sum up to 7.

74
22:27:40 --> 22:27:44
Meaning that 7 out of 10 samples were
correctly predicted by the model.

75
22:27:45 --> 22:27:49
The higher the sum of the diagonal values,
the better the performance of the model.

76
22:27:51 --> 22:27:56
The off diagonal values capture
the misclassified samples.

77
22:27:56 --> 22:27:59
Where the model's predictions
do not match the true labels.

78
22:28:00 --> 22:28:05
In this example these values indicate
that there were three misclassifications.

79
22:28:05 --> 22:28:06
Smaller values for

80
22:28:06 --> 22:28:11
the off diagonal cells in the confusion
matrix indicate better model performance.

81
22:28:13 --> 22:28:16
Note that the diagonal values,
true positives and

82
22:28:16 --> 22:28:20
true negatives are samples
with correct predictions.

83
22:28:20 --> 22:28:23
In our example,
these values sum up to 7 meaning

84
22:28:23 --> 22:28:28
that 7 out of 10 samples were
correctly predicted by the model.

85
22:28:28 --> 22:28:32
The higher the sum of the diagonal values,
the better the performance of the model.

86
22:28:33 --> 22:28:37
You may have noticed that the diagonal
values are related to Accuracy Rate.

87
22:28:37 --> 22:28:41
Recall that accuracy is defined
as the sum of two positives and

88
22:28:41 --> 22:28:44
true negatives, divided by all samples.

89
22:28:44 --> 22:28:46
The sum of two positives and

90
22:28:46 --> 22:28:50
two negatives is the sum of the diagonal
values in a confusion matrix.

91
22:28:50 --> 22:28:54
The sum of the diagonal
values in a confusion matrix,

92
22:28:54 --> 22:28:59
divided by the total number of
samples gives you the Accuracy Rate.

93
22:28:59 --> 22:29:04
Similarly, the off diagonal values
are related to the Error Rate.

94
22:29:04 --> 22:29:08
Recall that the Error Rate is
the opposite of the Accuracy Rate.

95
22:29:08 --> 22:29:12
The sum of the off diagonal
values in a confusion matrix,

96
22:29:12 --> 22:29:16
divided by the total number of samples,
gives you the Error Rate.

97
22:29:16 --> 22:29:20
Looking at the values in the Confusion
Matrix can help you understand

98
22:29:20 --> 22:29:23
the kind of Misclassifications
that your model is making.

99
22:29:23 --> 22:29:26
A high value for
this cell indicated by the yellow arrow,

100
22:29:26 --> 22:29:32
means that classifying the Positive
class is problematic for the model.

101
22:29:32 --> 22:29:36
A high value for the cell indicated by
the orange arrow on the other hand,

102
22:29:36 --> 22:29:40
means that classifying the Negative
class is problematic for the model.

103
22:29:41 --> 22:29:44
In summary,
the Confusion Matrix is a table

104
22:29:44 --> 22:29:47
used to summarize the different
types of errors for a classifier.

105
22:29:47 --> 22:29:51
The values in a Confusion Matrix can
be used to evaluate the performance of

106
22:29:51 --> 22:29:57
a classifier and are related to evaluation
metrics, such as accuracy and error rates.

107
22:29:57 --> 22:30:01
They also indicate what other types of
misclassifications the model is making.

108
22:30:02 --> 22:30:06
Note that in some implementations
of a Confusion Matrix, the true and

109
22:30:06 --> 22:30:08
predictive labels are switched.

110
22:30:08 --> 22:30:13
Be sure to review the documentation for
the software you're using to generate

111
22:30:13 --> 22:30:16
a Confusion Matrix to understand
what each cell specifies.

1
20:53:27 --> 20:53:31
In this activity, we will be
evaluating the decision tree model

2
20:53:31 --> 20:53:34
we created in the KNIME
classification hands-on.

3
20:53:34 --> 20:53:38
First, we will create a confusion
matrix to determine the accuracy

4
20:53:38 --> 20:53:39
of the decision tree model.

5
20:53:41 --> 20:53:42
Next, we will use highlighting and

6
20:53:42 --> 20:53:45
a scatter plot to analyze
the classification errors.

7
20:53:48 --> 20:53:50
Let's begin.

8
20:53:50 --> 20:53:55
First, let's open the Classification
workflow that we built in the previous

9
20:53:55 --> 20:53:56
hands-on.

10
20:53:56 --> 20:54:00
In the top-left of KNIME
is a KNIME Explorer.

11
20:54:00 --> 20:54:02
Double-click on the Classification,
under LOCAL.

12
20:54:05 --> 20:54:09
Next, we'll create a confusion
matrix to analyze the accuracy

13
20:54:09 --> 20:54:11
of our decision tree model.

14
20:54:13 --> 20:54:16
To do this,
we'll add the Scorer node to the canvas.

15
20:54:23 --> 20:54:28
Connect the output of Decision Tree
Predictor, to the input of Scorer.

16
20:54:29 --> 20:54:30
Double-click on Scorer.

17
20:54:32 --> 20:54:34
We'll use the default values, so click OK.

18
20:54:37 --> 20:54:39
Next, run the workflow.

19
20:54:40 --> 20:54:41
Now, let's view the confusion matrix.

20
20:54:46 --> 20:54:49
I can see the accuracy as 80.282%.

21
20:54:49 --> 20:54:56
At the top, I can see that it
accurately predicted 76 values for

22
20:54:56 --> 20:55:02
humidity_low and 95 for humidity_not_low.

23
20:55:02 --> 20:55:08
It inaccurately predicted 24 values for
humidty_low and

24
20:55:08 --> 20:55:14
18 for humidty_not_low,
for an error of 19.718%.

25
20:55:14 --> 20:55:18
Close this.

26
20:55:18 --> 20:55:21
Next, we use an interactive table to
look at the values that were incorrectly

27
20:55:21 --> 20:55:22
predicted.

28
20:55:23 --> 20:55:25
We'll add the Interactive Table
to the canvas.

29
20:55:34 --> 20:55:39
We'll connect this to the output
of Decision Tree Predictor,

30
20:55:39 --> 20:55:42
run the workflow and view the table.

31
20:55:49 --> 20:55:53
The right two columns have the real value
for low_humidity and the prediction.

32
20:55:54 --> 20:55:58
For some of these rows, we can tell
that the prediction was not correct.

33
20:55:59 --> 20:56:05
For example, in row ten, And in row 17.

34
20:56:09 --> 20:56:13
Let's leave the table view open,
and go back to the workflow.

35
20:56:13 --> 20:56:17
Next, we'll add the Scatter Plot
nodes to the workflow.

36
20:56:26 --> 20:56:28
Connect this to the output
of Decision Tree Predictor.

37
20:56:32 --> 20:56:35
Execute the workflow, and
view the scatter plot.

38
20:56:39 --> 20:56:43
We'll select row 17, and
choose, Hilite Selected.

39
20:56:45 --> 20:56:46
Go back to the scatter plot,

40
20:56:46 --> 20:56:51
and we see this particular value
is highlighted in the plot.

41
20:56:52 --> 20:56:54
We could choose another
row from the table, and

42
20:56:54 --> 20:56:57
highlight it again to see
its place in the plot.

43
20:56:58 --> 20:56:59
Let's choose row ten.

44
20:57:02 --> 20:57:06
We can do this for
values that were incorrectly predicted,

45
20:57:06 --> 20:57:08
to find any patterns for further analysis.

1
17:50:34 --> 17:50:38
In this activity we will use Spark
to evaluate our decision tree.

2
17:50:38 --> 17:50:41
First, we load
the classification predictions,

3
17:50:41 --> 17:50:44
created during the last
Spark hands on activity.

4
17:50:45 --> 17:50:48
We then compute the accuracy
of these predictions.

5
17:50:48 --> 17:50:50
And then, generate a confusion matrix.

6
17:50:52 --> 17:50:53
Let's begin.

7
17:50:53 --> 17:50:56
First, let's open the model
evaluation notebook.

8
17:50:58 --> 17:51:02
Next, let's execute the first
cell to load the classes.

9
17:51:03 --> 17:51:06
Then, execute the second cell
to load the predictions we saved

10
17:51:06 --> 17:51:07
during the previous hands on.

11
17:51:09 --> 17:51:12
We could then complete
the accuracy of these predictions

12
17:51:12 --> 17:51:15
by using a multi-class
classification evaluator.

13
17:51:16 --> 17:51:24
Let's enter evaluator =
MulticlassClassificationEvaluator(labelCo-

14
17:51:24 --> 17:51:29
l="label", predictionCol="prediction") and

15
17:51:29 --> 17:51:34
finally metricName="precision" and
execute this.

16
17:51:38 --> 17:51:42
We can then compute the accuracy by
calling evaluate on the evaluator.

17
17:51:42 --> 17:51:47
We'll enter
evaluator.evaluate[predictions].

18
17:51:49 --> 17:51:52
This says that the accuracy is about 81%.

19
17:51:53 --> 17:51:58
Next, let's use multi-class metrics
to compute a confusion matrix.

20
17:51:58 --> 17:52:02
Multi-class metrics takes
an RDD of numbers, and

21
17:52:02 --> 17:52:05
our data is currently in a data frame.

22
17:52:05 --> 17:52:10
We can access the RDD of the underlying
data frame by using the RDD

23
17:52:10 --> 17:52:12
attribute of predictions.

24
17:52:12 --> 17:52:18
If we look at predictions.rdd.take(2)
we see the RDD

25
17:52:18 --> 17:52:24
is the RDD of rows, or
each row has a prediction and label.

26
17:52:24 --> 17:52:28
However multi-class metrics
wants an RDD of numbers.

27
17:52:28 --> 17:52:30
We could do this using a map.

28
17:52:30 --> 17:52:35
We'll enter predictions.rdd.map and
we'll use a key word,

29
17:52:35 --> 17:52:40
tuple, and we'll look at the first
two elements in this RDD.

30
17:52:40 --> 17:52:40
Run this.

31
17:52:43 --> 17:52:45
We can see now this RDD is just numbers.

32
17:52:46 --> 17:52:51
Now we'll create a new instance of
a multiclass metrics using this RDD.

33
17:52:51 --> 17:52:58
Metrics =
MulticlassMetrics(predrictions.rdd.map(tu-

34
17:52:58 --> 17:52:59
ple)).

35
17:53:02 --> 17:53:06
We can then display
the confusion matrix by running

36
17:53:06 --> 17:53:12
metrics.confusionMatrix().toArray().trans-
pose().

37
17:53:12 --> 17:53:19
We can see these results are similar
to the confusion matrix in nine.

1
11:43:54 --> 11:43:58
Generalization and overfitting are very
important concepts in machine learning.

2
11:43:58 --> 11:44:00
We will cover them in
the next three lectures.

3
11:44:01 --> 11:44:06
After this video you will be able
to define what overfitting is,

4
11:44:06 --> 11:44:10
describe how overfitting is
related to generalization, and

5
11:44:10 --> 11:44:12
explain why overfitting should be avoided.

6
11:44:14 --> 11:44:18
Before we look at generalization and
overfitting, let's first define some terms

7
11:44:18 --> 11:44:21
that we will need to know to
discuss errors in classification.

8
11:44:23 --> 11:44:28
Recall that a machine learning model
maps the input it receives to an output.

9
11:44:28 --> 11:44:32
For a classification model, the model's
output is the predicted class label for

10
11:44:32 --> 11:44:36
the input variables and
the true class label is the target.

11
11:44:38 --> 11:44:41
Then if the classifier predicts
the correct classes label for

12
11:44:41 --> 11:44:44
a sample, that is a success.

13
11:44:44 --> 11:44:48
If the predicted class label is
different from the true class label,

14
11:44:48 --> 11:44:49
then that is an error.

15
11:44:50 --> 11:44:55
The error rate, then, is the percentage
of errors made over the entire data set.

16
11:44:55 --> 11:45:00
That is, it is the number of errors
divided by the total number of samples

17
11:45:00 --> 11:45:00
in a data set.

18
11:45:01 --> 11:45:06
Error rate is also known as
misclassification rate, or simply error.

19
11:45:07 --> 11:45:12
In our lesson on classification we discuss
that there is a training phase in which

20
11:45:12 --> 11:45:17
the model is built, and a testing phase in
which the model is applied to new data.

21
11:45:17 --> 11:45:23
The model is built using training data and
evaluated on test data.

22
11:45:23 --> 11:45:26
The training and
test data are two different data sets.

23
11:45:26 --> 11:45:30
The goal in building a machine learning
model is to have the model perform well

24
11:45:30 --> 11:45:34
on training, as well as test data.

25
11:45:34 --> 11:45:39
Error rate, or simply error, on the
training data is refered to as training

26
11:45:39 --> 11:45:43
error, and the error on test data
is referred to as test error.

27
11:45:45 --> 11:45:49
The error on the test data is
an indication of how well the classifier

28
11:45:49 --> 11:45:50
will perform on new data.

29
11:45:51 --> 11:45:54
This is known as generalization.

30
11:45:54 --> 11:45:58
Generalization refers to how well
your model performs on new data,

31
11:45:58 --> 11:46:00
that is data not used to train the model.

32
11:46:01 --> 11:46:04
You want your model to
generalize well to new data.

33
11:46:04 --> 11:46:08
If your model generalizes well, then
it will perform well on data sets that

34
11:46:08 --> 11:46:11
are similar in structure
to the training data, but

35
11:46:11 --> 11:46:14
doesn't contain exactly the same
samples as in the training set.

36
11:46:15 --> 11:46:20
Since the test error indicates how well
your model generalizes to new data,

37
11:46:20 --> 11:46:23
note that the test error is also
called generalization error.

38
11:46:24 --> 11:46:28
A related concept to
generalization is overfitting.

39
11:46:28 --> 11:46:30
If your model has very
low training error but

40
11:46:30 --> 11:46:34
high generalization error,
then it is overfitting.

41
11:46:34 --> 11:46:38
This means that the model has learned to
model the noise in the training data,

42
11:46:38 --> 11:46:41
instead of learning the underlying
structure of the data.

43
11:46:42 --> 11:46:45
These plots illustrate what
happens when a model overfits.

44
11:46:46 --> 11:46:48
Training samples are shown as points, and

45
11:46:48 --> 11:46:53
the input to output mapping that the model
has learned is indicated as a curve.

46
11:46:53 --> 11:46:57
The plot on the left shows that the model
has learned the underlying structure of

47
11:46:57 --> 11:47:02
the data, as the curve follows
the trend of the sample point as well.

48
11:47:02 --> 11:47:03
The plot on the right,

49
11:47:03 --> 11:47:07
however, shows that the model has learned
to model the noise in a data set.

50
11:47:08 --> 11:47:11
The model tries to capture
every sample point,

51
11:47:11 --> 11:47:14
instead of the general trend
of the samples together.

52
11:47:15 --> 11:47:16
The training error and

53
11:47:16 --> 11:47:19
the generalization error are plotted
together, during model training.

54
11:47:21 --> 11:47:25
What is the connection between
overfitting and generalization?

55
11:47:25 --> 11:47:28
A model that overfits will not
generalize well to new data.

56
11:47:28 --> 11:47:32
So the model will do well on just
the data it was trained on, but

57
11:47:32 --> 11:47:35
given a new data set,
it will perform poorly.

58
11:47:36 --> 11:47:39
A classifier that performs well
on just the training data set

59
11:47:39 --> 11:47:41
will not be very useful.

60
11:47:41 --> 11:47:45
So it is essential that the goal of good
generalization performance is kept in mind

61
11:47:45 --> 11:47:46
when building a model.

62
11:47:47 --> 11:47:50
A problem related to
overfitting is underfitting.

63
11:47:51 --> 11:47:56
Overfitting occurs when the model is
fitting to the noise in the training data.

64
11:47:56 --> 11:47:59
This results in low training error and
high test error.

65
11:48:01 --> 11:48:02
Underfitting on the other hand,

66
11:48:02 --> 11:48:06
occurs when the model has not
learned the structure of the data.

67
11:48:06 --> 11:48:09
This results in high training error and
high test error.

68
11:48:11 --> 11:48:13
Both are undesirable,

69
11:48:13 --> 11:48:17
since both mean that the model will
not generalize well to new data.

70
11:48:17 --> 11:48:21
Overfitting generally occurs when
a model is too complex, that is,

71
11:48:21 --> 11:48:26
it has too many parameters relative
to the number of training samples.

72
11:48:26 --> 11:48:30
So to avoid overfitting, the model needs
to be kept as simple as possible, and yet

73
11:48:30 --> 11:48:35
still solve the input/output mapping for
the given data set.

74
11:48:35 --> 11:48:36
We will discuss methods for

75
11:48:36 --> 11:48:39
avoiding overfitting in
the next couple of lectures.

76
11:48:40 --> 11:48:46
In summary, overfitting is when your model
has learned the noise in the training data

77
11:48:46 --> 11:48:48
instead of the underlying
structure of the data.

78
11:48:48 --> 11:48:53
You want to avoid overfitting so that your
model will generalize well to new data.

1
23:32:46 --> 23:32:49
How do you evaluate
your model performance?

2
23:32:49 --> 23:32:52
In this lecture, we will look at
different metrics that can be used to

3
23:32:52 --> 23:32:55
evaluate the performance of
your classification model.

4
23:32:56 --> 23:32:59
After this video, you will be able to

5
23:32:59 --> 23:33:03
discuss how performance metrics
can be used to evaluate models.

6
23:33:03 --> 23:33:08
Name three model evaluation metrics, and
explain why accuracy may be misleading.

7
23:33:10 --> 23:33:14
For the classification task, an error
occurs when the model's prediction of

8
23:33:14 --> 23:33:18
the class label is different
from the true class label.

9
23:33:18 --> 23:33:22
We can also define the different types
of errors in classification depending on

10
23:33:22 --> 23:33:23
the predicted and true labels.

11
23:33:24 --> 23:33:29
Let's take the case with the task is
to predict whether a given animal is

12
23:33:29 --> 23:33:30
a mammal or not.

13
23:33:30 --> 23:33:36
This is a binary classification task
with the class label being either yes,

14
23:33:36 --> 23:33:40
indicating mammal, or
no indicating non-mammal.

15
23:33:40 --> 23:33:43
Then the different types
of errors are as follows.

16
23:33:44 --> 23:33:49
If the true label is yes and
the predicted label is yes,

17
23:33:49 --> 23:33:54
then this is a true positive,
abbreviated as TP.

18
23:33:54 --> 23:33:58
This is the case where the label is
correctly predicted as positive.

19
23:34:00 --> 23:34:04
If the true label is no and
the predicted label is no,

20
23:34:04 --> 23:34:08
then this is a true negative,
abbreviated as TN.

21
23:34:10 --> 23:34:13
This is the case where the label is
correctly predicted as negative.

22
23:34:15 --> 23:34:20
If the true label is no and
the predicted label is yes,

23
23:34:20 --> 23:34:25
then this is a false positive,
abbreviated as FP.

24
23:34:25 --> 23:34:29
This is the case with the label is
incorrectly predicted as positive,

25
23:34:29 --> 23:34:30
when it should be negative.

26
23:34:32 --> 23:34:34
If the true label is yes and

27
23:34:34 --> 23:34:39
the predicted label is no, then this
is a false negative abbreviated as FN.

28
23:34:40 --> 23:34:45
This is the case where the label is
incorrectly predicted as negative,

29
23:34:45 --> 23:34:46
when it should be positive.

30
23:34:47 --> 23:34:52
These definitions can take a while to sink
in, so feel free to hit the pause button

31
23:34:52 --> 23:34:55
and replay button several times
here to review this part.

32
23:34:57 --> 23:35:00
These four different types of errors
are used in calculating many evaluation

33
23:35:00 --> 23:35:03
metrics for classifiers.

34
23:35:03 --> 23:35:07
The most commonly used evaluation
metric is the accuracy rate, or

35
23:35:07 --> 23:35:09
accuracy for short.

36
23:35:09 --> 23:35:12
For classication,
accuracy is calculated as the number of

37
23:35:12 --> 23:35:16
correct predictions divided by
the total number of predictions.

38
23:35:17 --> 23:35:20
Note that the number of correct
predictions is the sum of the true

39
23:35:20 --> 23:35:25
positives, and the true negatives, since
the true and predicted labels match for

40
23:35:25 --> 23:35:27
those cases.

41
23:35:27 --> 23:35:30
The accuracy rate is an intuitive
way to measure the performance

42
23:35:30 --> 23:35:31
of a classification model.

43
23:35:33 --> 23:35:37
Model performance can also be
expressed in terms of error rate.

44
23:35:37 --> 23:35:40
Error rate is the opposite
of accuracy rate.

45
23:35:41 --> 23:35:46
Let's look at an example to see how
accuracy and error rates are calculated.

46
23:35:46 --> 23:35:50
The table on the left, lists the true
label along with the model's prediction

47
23:35:50 --> 23:35:52
for a data set of ten samples.

48
23:35:54 --> 23:35:57
First, let’s figure out
the number of true positives.

49
23:35:57 --> 23:36:01
Recall that a true positive occurs when
the output is correctly predicted as

50
23:36:01 --> 23:36:03
positive.

51
23:36:03 --> 23:36:07
In other words the true label is yes,
and the model's prediction is yes.

52
23:36:07 --> 23:36:12
In this example there are three true
positives as indicated by the red arrows.

53
23:36:12 --> 23:36:16
So, TP=3, remember that value
as we'll need it later.

54
23:36:18 --> 23:36:22
Now, let's figure out
the number of true negatives.

55
23:36:22 --> 23:36:26
A true negative occurs when the output
is correctly predicted as negative.

56
23:36:26 --> 23:36:30
In other words, the true label is no and
the model's prediction is no.

57
23:36:30 --> 23:36:34
In this example there are four true
negatives as indicated by the green

58
23:36:34 --> 23:36:35
arrows.

59
23:36:35 --> 23:36:39
So TN = 4,
we'll need to remember this value as well.

60
23:36:41 --> 23:36:46
Now we use the values for TP and
TN to calculate the accuracy rate.

61
23:36:46 --> 23:36:52
Using the equation for accuracy rate,
we plug in three for TP and four for TN.

62
23:36:52 --> 23:36:56
We get seven correct predictions for
the numerator.

63
23:36:56 --> 23:37:01
The denominator is simply the total number
of samples in our data set, which is ten.

64
23:37:01 --> 23:37:04
So the accuracy rate for

65
23:37:04 --> 23:37:10
example is 7 out of 10 which is 0.7 or
70%.

66
23:37:10 --> 23:37:13
The error rate is the exact
opposite of the accuracy rate.

67
23:37:13 --> 23:37:19
To calculate the error rate, we simply
subtract the accuracy rate from 1.

68
23:37:19 --> 23:37:24
For our example that is
1- 0.7 which is 0.3.

69
23:37:24 --> 23:37:29
So the error rate for
this example is 0.3 or 30%.

70
23:37:29 --> 23:37:33
There's a limitation with accuracy and

71
23:37:33 --> 23:37:36
error rates when you have
a class imbalance problem.

72
23:37:37 --> 23:37:41
This is when there are very few
samples of the class of interest, and

73
23:37:41 --> 23:37:42
the majority are negative examples.

74
23:37:43 --> 23:37:48
An example of this is identifying
if a tumor is cancerous or not.

75
23:37:48 --> 23:37:52
What is of interest is identifying
samples with cancerous tumors, but

76
23:37:52 --> 23:37:57
these positive cases where the tumor
is cancerous are very rare.

77
23:37:57 --> 23:38:01
So, you end up with a very small
fraction of positive samples, and

78
23:38:01 --> 23:38:03
most of the samples are negative.

79
23:38:03 --> 23:38:05
Thus the name, class imbalance problem.

80
23:38:06 --> 23:38:10
What could be the problem with using
accuracy for a class imbalance problem?

81
23:38:11 --> 23:38:16
Consider the situation where only 3%
of the cases are cancerous tumors.

82
23:38:17 --> 23:38:21
If the classification model
always predicts non-cancer,

83
23:38:21 --> 23:38:25
it will have an accuracy rate of 97%,

84
23:38:25 --> 23:38:29
since 97% of the samples will
have non-cancerous tumors.

85
23:38:29 --> 23:38:35
But note that in this case, the model
fails to detect any cancer cases at all.

86
23:38:35 --> 23:38:38
So the accuracy rate is
very misleading here.

87
23:38:38 --> 23:38:41
You may think that your model is
performing very well with such

88
23:38:41 --> 23:38:42
a high accuracy rate.

89
23:38:42 --> 23:38:48
But in fact it cannot identify any of
the cases in the class of interest.

90
23:38:48 --> 23:38:51
In these cases we need evaluation
metrics that can capture how

91
23:38:51 --> 23:38:55
well the model classifies positive,
versus negative classes.

92
23:38:57 --> 23:39:00
A pair of evaluations metrics that
are commonly used when there is a class

93
23:39:00 --> 23:39:04
imbalance are precision and recall.

94
23:39:04 --> 23:39:08
Precision is defined as the number of
true positives divided by the sum of

95
23:39:08 --> 23:39:11
true positives and false positives.

96
23:39:11 --> 23:39:15
In other words, it is the number of true
positives divided by the total number

97
23:39:15 --> 23:39:18
of samples predicted as being positive.

98
23:39:20 --> 23:39:24
Recall is defined as the number of
true positives divided by the sum of

99
23:39:24 --> 23:39:26
true positives and false negatives.

100
23:39:26 --> 23:39:31
It is the number of true positives
divided by the total number of samples,

101
23:39:31 --> 23:39:33
actually belonging to the true class.

102
23:39:34 --> 23:39:38
Here's an illustration that
shows precision and recall.

103
23:39:38 --> 23:39:43
The selected elements indicated by the
green half circle are the true positives.

104
23:39:43 --> 23:39:47
That is samples predicted as positive and
are actually positive.

105
23:39:48 --> 23:39:51
The relevant elements indicated
by the green half circle and

106
23:39:51 --> 23:39:57
the green half rectangle, are the true
positives, plus the false negatives.

107
23:39:57 --> 23:40:01
That is samples that are actually
positive, but some are correctly predicted

108
23:40:01 --> 23:40:04
as positive, and some are incorrectly
predicted as negative.

109
23:40:05 --> 23:40:10
Recall then is the number of samples
correctly predicted as positive,

110
23:40:10 --> 23:40:13
divided by all samples that
are actually positive.

111
23:40:15 --> 23:40:18
The entire circle indicated
by the green half circle and

112
23:40:18 --> 23:40:23
the pink half circle, are the true
positives plus the false positives.

113
23:40:23 --> 23:40:25
That is samples that were
predicted as positive

114
23:40:25 --> 23:40:28
although some were actually positive and
some were actually negative.

115
23:40:30 --> 23:40:34
Then precision is the number of samples
correctly predicted as positive,

116
23:40:34 --> 23:40:38
divided by the number of all
samples predicted as positive.

117
23:40:40 --> 23:40:43
Precision is considered a measure
of exactness because it calculates

118
23:40:43 --> 23:40:46
the percentage of samples
predicted as positive,

119
23:40:46 --> 23:40:48
which are actually in a positive class.

120
23:40:49 --> 23:40:53
Recall is considered a measure of
completeness, because it calculates

121
23:40:53 --> 23:40:56
the percentage of positive samples
that the model correctly identified.

122
23:40:58 --> 23:41:01
There is a trade off between precision and
recall.

123
23:41:01 --> 23:41:06
A perfect precision score of one for
a class C means that every sample

124
23:41:06 --> 23:41:11
predicted as belonging to class C,
does indeed belong to class C.

125
23:41:11 --> 23:41:15
But this says nothing about the number of
samples from class C that were predicted

126
23:41:15 --> 23:41:16
incorrectly.

127
23:41:17 --> 23:41:19
A perfect recall score of one for

128
23:41:19 --> 23:41:24
a class C, means that every sample
from class C was correctly labeled.

129
23:41:24 --> 23:41:27
But this doesn't say anything
about how many other samples were

130
23:41:27 --> 23:41:30
incorrectly labeled as
belonging to class C.

131
23:41:30 --> 23:41:32
So they are used together.

132
23:41:32 --> 23:41:37
For example, precision values can be
compared for a fixed value of recall or

133
23:41:37 --> 23:41:38
vice versa.

134
23:41:38 --> 23:41:42
The goal for classification is to
maximize both precision and recall.

135
23:41:43 --> 23:41:49
Precision and recall can be combined into
a single metric called the F-measure.

136
23:41:49 --> 23:41:52
The equation for that is 2 times
the product of precision and

137
23:41:52 --> 23:41:55
recall divided by their sum.

138
23:41:55 --> 23:41:58
There are different
versions of the F-measure.

139
23:41:58 --> 23:41:59
The equation on this side is for

140
23:41:59 --> 23:42:04
the F1 measure which is the most
commonly used variant of the F measure.

141
23:42:04 --> 23:42:08
With the F1 measure, precision and
recall are equally weighted.

142
23:42:08 --> 23:42:12
The F2 measure weights recall
higher than precision.

143
23:42:12 --> 23:42:17
And the F0.5 measure weights
precision higher than recall.

144
23:42:18 --> 23:42:21
The value for
the F1 measure ranges from zero to one,

145
23:42:21 --> 23:42:24
with higher values giving better
classification performance.

146
23:42:26 --> 23:42:28
In summary, there are several metrics for

147
23:42:28 --> 23:42:31
evaluating the performance
of a classification model.

148
23:42:31 --> 23:42:35
They are defined in terms of
the types of errors you can get in

149
23:42:35 --> 23:42:36
a classification problem.

150
23:42:37 --> 23:42:42
We covered some of the most commonly
used evaluation metrics in this lecture,

151
23:42:42 --> 23:42:46
namely accuracy and error rates,
precision and recall and F1 measure.

1
23:15:32 --> 23:15:37
In this lecture, we will discuss how
overfitting occurs with decision trees and

2
23:15:37 --> 23:15:38
how it can be avoided.

3
23:15:39 --> 23:15:40
After this video,

4
23:15:40 --> 23:15:46
you will be able to discuss overfitting
in the context of decision tree models.

5
23:15:46 --> 23:15:49
Explain how overfitting is addressed
in decision tree induction.

6
23:15:49 --> 23:15:53
Define pre-pruning and post-pruning.

7
23:15:53 --> 23:15:57
In our lecture on decision trees, we
discussed that during the construction of

8
23:15:57 --> 23:16:02
a decision tree, also referred to as
tree induction, the tree repeatedly

9
23:16:02 --> 23:16:07
splits the data in a node in order to
get successively paired subsets of data.

10
23:16:08 --> 23:16:12
Note that a decision tree classifier can
potentially expand its nodes until it can

11
23:16:12 --> 23:16:16
perfectly classify samples
in the training data.

12
23:16:16 --> 23:16:20
But if the tree grows nodes to fit
the noise in the training data,

13
23:16:20 --> 23:16:23
then it will not classify
a new sample well.

14
23:16:23 --> 23:16:27
This is because the tree has partitioned
the input space according to the noise in

15
23:16:27 --> 23:16:31
the data instead of to
the true structure of a data.

16
23:16:31 --> 23:16:32
In other words, it has overfit.

17
23:16:34 --> 23:16:37
How can overfitting be
avoided in decision trees?

18
23:16:37 --> 23:16:39
There are two ways.

19
23:16:39 --> 23:16:42
One is to stop growing the tree
before the tree is fully grown

20
23:16:42 --> 23:16:44
to perfectly fit the training data.

21
23:16:44 --> 23:16:47
This is referred to as pre-pruning.

22
23:16:47 --> 23:16:51
The other way to avoid overfitting in
decision trees is to grow the tree to its

23
23:16:51 --> 23:16:56
maximum size and then prune the tree
back by removing parts of the tree.

24
23:16:56 --> 23:17:00
This is referred to as post-pruning.

25
23:17:00 --> 23:17:04
In general, overfitting occurs
because the model is too complex.

26
23:17:04 --> 23:17:06
For a decision tree model,

27
23:17:06 --> 23:17:10
model complexity is determined by
the number of nodes in the tree.

28
23:17:10 --> 23:17:15
Addressing overfitting in decision trees
means controlling the number of nodes.

29
23:17:15 --> 23:17:18
Both methods of pruning control
the growth of the tree and consequently,

30
23:17:18 --> 23:17:20
the complexity of the resulting model.

31
23:17:22 --> 23:17:27
With pre-pruning, the idea is to stop tree
induction before a fully grown tree is

32
23:17:27 --> 23:17:30
built that perfectly
fits the training data.

33
23:17:30 --> 23:17:35
To do this, restrictive stopping
conditions for growing nodes must be used.

34
23:17:35 --> 23:17:40
For example, a nose stops expanding if
the number of samples in the node is less

35
23:17:40 --> 23:17:42
than some minimum threshold.

36
23:17:42 --> 23:17:46
Another example is to stop expanding
a note if the improvement in the impurity

37
23:17:46 --> 23:17:49
measure falls below a certain threshold.

38
23:17:50 --> 23:17:54
In post-pruning,
the tree is grown to its maximum size,

39
23:17:54 --> 23:17:59
then the tree is pruned by removing
nodes using a bottom up approach.

40
23:17:59 --> 23:18:03
That is, the tree is trimmed
starting with the leaf nodes.

41
23:18:03 --> 23:18:06
The pruning is done by replacing
a subtree with a leaf node if

42
23:18:06 --> 23:18:10
this improves the generalization error,
or if there is no

43
23:18:10 --> 23:18:13
change to the generalization
error with this replacement.

44
23:18:13 --> 23:18:17
In other words, if removing a subtree
does not have a negative effect

45
23:18:17 --> 23:18:21
on the generalization error,
then the nodes in that subtree only

46
23:18:21 --> 23:18:24
add to the complexity of the tree,
and not to its overall performance.

47
23:18:24 --> 23:18:26
So those nodes should be removed.

48
23:18:26 --> 23:18:31
In practice,
post-pruning tends to give better results.

49
23:18:31 --> 23:18:35
This is because pruning decisions are
based on information from the full tree.

50
23:18:35 --> 23:18:40
Pre-pruning, on the other hand, may stop
the tree growing process prematurely.

51
23:18:40 --> 23:18:44
However, post-pruning is more
computationally expensive since the tree

52
23:18:44 --> 23:18:47
has to be expanded to its full size.

53
23:18:48 --> 23:18:53
In summary, to address overfitting in
decision trees, tree pruning is used.

54
23:18:53 --> 23:18:58
There are two pruning methods,
pre-pruning and post-pruning.

55
23:18:58 --> 23:19:00
Both methods control
the complexity of the tree model.

1
22:34:32 --> 22:34:36
In this lecture, we will discuss
what a validation set is and

2
22:34:36 --> 22:34:39
how it relates to overfitting and
model performance evaluation.

3
22:34:41 --> 22:34:45
After this video, you will be able to
describe how validation sets can be

4
22:34:45 --> 22:34:47
used to avoid overfitting.

5
22:34:47 --> 22:34:51
Articulate how training,
validation, and test sets are used.

6
22:34:51 --> 22:34:54
And list three ways that
validation can be performed.

7
22:34:55 --> 22:34:57
In our lesson on classification,

8
22:34:57 --> 22:35:01
we discussed that there is a training
phase in which the model is built, and

9
22:35:01 --> 22:35:05
a testing phase in which
the model is applied to new data.

10
22:35:05 --> 22:35:10
The model is built using training data and
evaluated on test data.

11
22:35:10 --> 22:35:14
The training and
test data are two different datasets.

12
22:35:14 --> 22:35:18
The goal in building a machine learning
model is to have the model perform well

13
22:35:18 --> 22:35:23
on the training set, as well as generalize
well on new data in the test set.

14
22:35:24 --> 22:35:29
Recall that a model that overfits
does not generalize well to new data.

15
22:35:29 --> 22:35:35
Recall also that overfitting generally
occurs when a model is too complex.

16
22:35:35 --> 22:35:39
So to have a model with good
generalization performance,

17
22:35:39 --> 22:35:43
model training has to stop before
the model gets too complex.

18
22:35:43 --> 22:35:45
How do you determine
when this should occur?

19
22:35:46 --> 22:35:51
A validation set can be used to guide the
training process to avoid overfitting and

20
22:35:51 --> 22:35:54
deliver good generalization performance.

21
22:35:54 --> 22:35:58
We have discussed having a training
set and a separate test set.

22
22:35:58 --> 22:36:01
The training set is used
to build a model and

23
22:36:01 --> 22:36:03
the test set is used to see how
the model performs a new data.

24
22:36:04 --> 22:36:09
Now we want to further divide up
the training data into a training set and

25
22:36:09 --> 22:36:11
a validation set.

26
22:36:11 --> 22:36:14
The training set is used to
train the model as before and

27
22:36:14 --> 22:36:18
the validation set is used to determine
when to stop training the model

28
22:36:18 --> 22:36:22
to avoid overfitting, in order to get
the best generalization performance.

29
22:36:24 --> 22:36:27
The idea is to look at the errors
on both training set and

30
22:36:27 --> 22:36:30
validation set during model
training as shown here.

31
22:36:30 --> 22:36:34
The orange solid line on the plot
is the training error and

32
22:36:34 --> 22:36:37
the green line is the validation error.

33
22:36:37 --> 22:36:41
We see that as model building
progresses along the x-axis,

34
22:36:41 --> 22:36:43
the number of nodes increases.

35
22:36:43 --> 22:36:47
That is the complexity
of the model increases.

36
22:36:47 --> 22:36:53
We can see that as the model complexity
increases, the training error decreases.

37
22:36:53 --> 22:36:57
On the other hand, the validation
error initially decreases but

38
22:36:57 --> 22:36:58
then starts to increase.

39
22:36:59 --> 22:37:04
When the validation error increases, this
indicates that the model is overfitting,

40
22:37:04 --> 22:37:07
resulting in decreased
generalization performance.

41
22:37:09 --> 22:37:12
This can be used to determine
when to stop training.

42
22:37:12 --> 22:37:17
Where validation error starts to increase
is when you get the best generalization

43
22:37:17 --> 22:37:20
performance, so
training should stop there.

44
22:37:20 --> 22:37:24
This method of using a validation set
to determine when to stop training

45
22:37:24 --> 22:37:26
is referred to as model selection

46
22:37:26 --> 22:37:30
since you're selecting one from
many of varying complexities.

47
22:37:30 --> 22:37:34
Note that this was illustrated for
a decision tree classifier, but

48
22:37:34 --> 22:37:39
the same method can be applied to
any type of machine learning model.

49
22:37:39 --> 22:37:44
There are several ways to create and use
the validation set to avoid overfitting.

50
22:37:44 --> 22:37:48
The different methods are holdout method,

51
22:37:48 --> 22:37:53
random subsampling,
k-fold cross-validation,

52
22:37:53 --> 22:37:57
and leave-one-out cross-validation.

53
22:37:57 --> 22:38:01
The first way to use a validation
set is the holdout method.

54
22:38:01 --> 22:38:03
This describes the scenario
that we have been discussing,

55
22:38:03 --> 22:38:08
where part of the training data
is reserved as a validation set.

56
22:38:08 --> 22:38:10
The validation set is
then the holdout set.

57
22:38:11 --> 22:38:14
Errors on the training set and the holdout
set are calculated at each step

58
22:38:14 --> 22:38:19
during model training and
plotted together as we've seen before.

59
22:38:19 --> 22:38:22
And the lowest error on the holdout
set is when training should stop.

60
22:38:22 --> 22:38:26
This is the just the process that
we have described here before.

61
22:38:26 --> 22:38:28
There's some limitations to
the holdout method however.

62
22:38:29 --> 22:38:33
First, since some samples are reserved for
the holdout validation set,

63
22:38:33 --> 22:38:37
the training set now has less data
than it originally started out with.

64
22:38:38 --> 22:38:43
Secondly, if the training and holdout sets
do not have the same data distributions,

65
22:38:43 --> 22:38:46
then the results will be misleading.

66
22:38:46 --> 22:38:50
For example, if the training data has
many more samples of one class and

67
22:38:50 --> 22:38:54
the holdout dataset has many
more samples of another class.

68
22:38:54 --> 22:38:59
The next method for using
a validation set is repeated holdout.

69
22:38:59 --> 22:39:00
As the name implies,

70
22:39:00 --> 22:39:04
this is essentially repeating
the holdout method several times.

71
22:39:04 --> 22:39:09
With each iteration, samples are randomly
selected from the original training data

72
22:39:09 --> 22:39:12
to create the holdout validation set.

73
22:39:12 --> 22:39:15
This is repeated several times with
different training and validation sets.

74
22:39:16 --> 22:39:20
Then the iterates on the holdout set for
the different iterations

75
22:39:20 --> 22:39:25
are averaged together to get the overall
iterate for model selection.

76
22:39:25 --> 22:39:29
A potential problem with repeated holdout
is that you could end up with some samples

77
22:39:29 --> 22:39:31
being used more than others for training.

78
22:39:32 --> 22:39:37
Since a sample can be used for either
testing or training any number of times,

79
22:39:37 --> 22:39:42
some samples may be put in the training
set more times than other samples.

80
22:39:42 --> 22:39:46
So you might end up with some
samples being overrepresented while

81
22:39:46 --> 22:39:49
other samples are underrepresented
in training or testing.

82
22:39:51 --> 22:39:56
A way to improve on the repeated
holdout method is use cross-validation.

83
22:39:56 --> 22:39:58
Cross-validation works as follows.

84
22:39:58 --> 22:40:03
Segment the data into k number
of disjoint partitions.

85
22:40:03 --> 22:40:08
During each iteration, one partition
is used as the validation set.

86
22:40:08 --> 22:40:10
Repeat the process k times.

87
22:40:10 --> 22:40:14
Each time using a different partition for
validation.

88
22:40:14 --> 22:40:17
So each partition is used for
validation exactly once.

89
22:40:17 --> 22:40:20
This is illustrated in this figure.

90
22:40:20 --> 22:40:24
In the fist iteration, the first
partition, specified in green, is used for

91
22:40:24 --> 22:40:25
validation.

92
22:40:25 --> 22:40:28
In the second iteration,
the second partition is used for

93
22:40:28 --> 22:40:30
validation and so on.

94
22:40:31 --> 22:40:35
The overall validation error is calculated
by averaging the validation errors for

95
22:40:35 --> 22:40:36
all k iterations.

96
22:40:38 --> 22:40:42
The model with the smallest average
validation error then is selected.

97
22:40:42 --> 22:40:47
The process we just described is
referred to as k-fold cross-validation.

98
22:40:47 --> 22:40:51
This is a very commonly used approach
to model selection in practice.

99
22:40:52 --> 22:40:57
This approach gives you a more structured
way to divide available data up between

100
22:40:57 --> 22:41:02
training and validation datasets and
provides a way to overcome the variability

101
22:41:02 --> 22:41:07
in performance that you can get when
using a single partitioning of the data.

102
22:41:07 --> 22:41:11
Leave-one-out cross-validation
is a special case of k-fold

103
22:41:11 --> 22:41:16
cross-validation where k equals N,
where N is the size of your dataset.

104
22:41:17 --> 22:41:21
Here, for each iteration the validation
set has exactly one sample.

105
22:41:21 --> 22:41:25
So the model is trained to
using N minus one samples and

106
22:41:25 --> 22:41:27
is validated on the remaining sample.

107
22:41:27 --> 22:41:33
The rest of the process works the same
way as regular k-fold cross-validation.

108
22:41:33 --> 22:41:38
Note that cross-validation is often
abbreviated CV and leave-one-out

109
22:41:38 --> 22:41:43
cross-validation is in abbreviated
L-O-O-C-V and pronounced LOOCV.

110
22:41:45 --> 22:41:49
We have described several ways to use
a validation set to address overfitting.

111
22:41:49 --> 22:41:53
Error on the validation set is used
to determine when to stop training so

112
22:41:53 --> 22:41:55
that the model does not overfit.

113
22:41:56 --> 22:42:00
Note that the validation error that comes
out of this process can also be used

114
22:42:00 --> 22:42:03
to estimate generalization
performance of the model.

115
22:42:03 --> 22:42:04
In other words,

116
22:42:04 --> 22:42:09
the error on the validation set provides
an estimate of the error on the test set.

117
22:42:10 --> 22:42:12
With the addition of the validation set,

118
22:42:12 --> 22:42:16
you really need three distinct
datasets when you build a model.

119
22:42:16 --> 22:42:17
Let's review these datasets.

120
22:42:19 --> 22:42:23
The training dataset is used to train the
model, that is to adjust the parameters of

121
22:42:23 --> 22:42:25
the model to learn the input
to output mapping.

122
22:42:27 --> 22:42:31
The validation dataset is used to
determine when training should stop

123
22:42:31 --> 22:42:33
in order to avoid overfitting.

124
22:42:35 --> 22:42:39
The test data set is used to evaluate
the performance of the model on new data.

125
22:42:41 --> 22:42:46
Note that the test data set should never,
ever be used in any way to create or

126
22:42:46 --> 22:42:47
tune the model.

127
22:42:47 --> 22:42:49
It should not be used, for

128
22:42:49 --> 22:42:53
example, in a cross-validation process
to determine when to stop training.

129
22:42:54 --> 22:42:58
The test dataset must always remain
independent from model training and

130
22:42:58 --> 22:43:03
remain untouched until the very end
when all training has been completed.

131
22:43:03 --> 22:43:08
Note that in sampling the original dataset
to create the training, validation, and

132
22:43:08 --> 22:43:12
test sets, all datasets must contain the
same distribution of the target classes.

133
22:43:12 --> 22:43:18
For example, if in the original dataset,
70% of the samples belong to one class and

134
22:43:18 --> 22:43:23
30% to the other class, then this same
distribution should approximately

135
22:43:23 --> 22:43:27
be present in each of the training,
validation, and test sets.

136
22:43:27 --> 22:43:30
Otherwise, analysis results
will be misleading.

137
22:43:30 --> 22:43:33
To summarize, we have discuss the need for

138
22:43:33 --> 22:43:36
three different datasets
in building model.

139
22:43:36 --> 22:43:41
A training set to train the model,
a validation set to determine when to stop

140
22:43:41 --> 22:43:44
training, and a test to evaluate
performance on new data.

141
22:43:46 --> 22:43:49
We learned how a validation set can
be used to avoid overfitting and

142
22:43:49 --> 22:43:54
in the process, provide an estimate
of generalization performance.

143
22:43:54 --> 22:43:56
And we covered different
ways to create and

144
22:43:56 --> 22:44:00
use a validation set such
as k-fold cross-validation.

1
21:18:32 --> 21:18:36
Now that we have seen what
association analysis is,

2
21:18:36 --> 21:18:39
let's go over the association
analysis process in more detail.

3
21:18:40 --> 21:18:45
After this video, you will be able
to define the terms support and

4
21:18:45 --> 21:18:50
confidence, describe the steps
in association analysis, and

5
21:18:50 --> 21:18:54
explain how association rules
are formed from item sets.

6
21:18:55 --> 21:18:58
Let's review the set in
association analysis.

7
21:18:58 --> 21:19:03
They are create item the sets,
then identify the frequent item sets.

8
21:19:03 --> 21:19:05
Finally, generate the rules.

9
21:19:06 --> 21:19:09
We will continue with
this example dataset,

10
21:19:09 --> 21:19:11
there are five transactions
in the dataset.

11
21:19:11 --> 21:19:14
Each with a set of items
purchased together.

12
21:19:14 --> 21:19:18
The goal is to come up with rules
describing associations between items.

13
21:19:20 --> 21:19:22
The first step is to create item sets.

14
21:19:23 --> 21:19:26
Item sets have different sizes
which need to be created.

15
21:19:26 --> 21:19:28
We will color code the items so

16
21:19:28 --> 21:19:31
that each one is easier to pick
out from the transactions table.

17
21:19:32 --> 21:19:37
We start out with just 1-item sets,
that is, sets with just one item.

18
21:19:38 --> 21:19:41
The left table is
the dataset of transactions.

19
21:19:41 --> 21:19:45
The right table contains the 1-item sets
that can be created from this dataset.

20
21:19:47 --> 21:19:51
As each item set is created, we also need
to keep track of the frequency at which

21
21:19:51 --> 21:19:53
these item set occurs in the dataset.

22
21:19:55 --> 21:19:59
This is referred to support for
the item set and

23
21:19:59 --> 21:20:04
is calculated by dividing the number of
times the item set occurs in the dataset

24
21:20:04 --> 21:20:07
by the total number of transactions.

25
21:20:07 --> 21:20:11
This is what is in the Support
column in the right table.

26
21:20:11 --> 21:20:14
For example, eggs the last
item in the right table occurs

27
21:20:14 --> 21:20:18
just occurs just once in the dataset,
in the transaction two.

28
21:20:18 --> 21:20:23
So if Support is 1/5 or
one fifth,the item set with

29
21:20:23 --> 21:20:31
diaper occurs in all transactions,
so if Support is 5/5 or 1.

30
21:20:31 --> 21:20:35
The Support for each item set will be
used to identify frequent item sets

31
21:20:35 --> 21:20:39
in the next step, specifically,
the Support issues to prune, or

32
21:20:39 --> 21:20:42
remove, item sets that
do not occur frequently.

33
21:20:44 --> 21:20:47
The support of each item set
will be used to identify

34
21:20:47 --> 21:20:50
frequent item sets in the next step.

35
21:20:50 --> 21:20:52
Specifically, the support
is used to prune or

36
21:20:52 --> 21:20:56
remove item sets that do
not occur frequently.

37
21:20:56 --> 21:21:00
For example, the minimum support
threshold is set to 3/5.

38
21:21:00 --> 21:21:05
So looking at the 1-item sets table
We can remove any item set with

39
21:21:05 --> 21:21:07
the support of less than 3/5.

40
21:21:07 --> 21:21:12
These item sets are highlighted in pink,
they will be removed before the sets for

41
21:21:12 --> 21:21:14
two items are created.

42
21:21:14 --> 21:21:20
The final one item sets are then the item
sets with bread, milk, beer and diaper.

43
21:21:21 --> 21:21:25
We only consider items that were in
the one item sets, that were not pruned.

44
21:21:26 --> 21:21:28
The two item sets are shown
in the right table.

45
21:21:30 --> 21:21:32
We, again,
need to keep track of the support for

46
21:21:32 --> 21:21:35
these item sets,
just as we did with the one item sets.

47
21:21:36 --> 21:21:38
For example, for
the last item set, with beer, and

48
21:21:38 --> 21:21:42
diaper, we see, by looking at
the left table, that beer and

49
21:21:42 --> 21:21:48
diaper occur together three times In
transactions two, three and four.

50
21:21:48 --> 21:21:50
So with support is 3/5.

51
21:21:50 --> 21:21:53
Again, we need to prune
item sets with low support.

52
21:21:53 --> 21:21:57
The ones highlighted in pink
in the two item sets table.

53
21:21:57 --> 21:22:01
Those would be the item set with bread and
beer and the item set with milk and beer.

54
21:22:02 --> 21:22:05
The remaining two items that end.

55
21:22:05 --> 21:22:08
One item such or
then use to create the three item sets.

56
21:22:09 --> 21:22:11
Let's look now at
creating three item sets.

57
21:22:12 --> 21:22:17
The only three item sets that has a
support value greater than minimum support

58
21:22:17 --> 21:22:19
is the one shown in the right table.

59
21:22:19 --> 21:22:22
Namely the items start with bread,
milk and diaper.

60
21:22:23 --> 21:22:29
The second step in association analysis
is to identify the frequent item sets.

61
21:22:29 --> 21:22:31
But note that the process
that we just described for

62
21:22:31 --> 21:22:35
creating item sets already
identifies frequent item sets.

63
21:22:36 --> 21:22:39
A frequent item set is one whose
support is greater than or

64
21:22:39 --> 21:22:42
equal to the minimum support.

65
21:22:42 --> 21:22:47
So by keeping track of the support of
each item set as it is being created and

66
21:22:47 --> 21:22:50
removing item sets with low support,

67
21:22:50 --> 21:22:52
we are already identifying
frequent item sets.

68
21:22:53 --> 21:22:58
For our example, the frequent one,
two and three item sets are shown here.

69
21:23:00 --> 21:23:03
Now that we identified the frequent
item sets, the last step is to

70
21:23:03 --> 21:23:07
generate the rules to capture
associations that we see in the data.

71
21:23:09 --> 21:23:13
Let's first define some terms we'll
need to discuss association rules.

72
21:23:13 --> 21:23:17
The format of an association
rule is shown at the top.

73
21:23:17 --> 21:23:23
It's written as X arrow Y and
is read as if X, then Y.

74
21:23:23 --> 21:23:27
The X part is called the antecedent and

75
21:23:27 --> 21:23:29
the Y part is called
the consequent of the rule.

76
21:23:30 --> 21:23:32
X and Y are item sets.

77
21:23:33 --> 21:23:36
An important term in rule
generation is the rule confidence.

78
21:23:38 --> 21:23:40
This is to find as a support for X and

79
21:23:40 --> 21:23:46
Y together divided by the support for
X only.

80
21:23:46 --> 21:23:48
So rule confidence
calculates the frequency of

81
21:23:48 --> 21:23:50
instances to which the rule applies.

82
21:23:52 --> 21:23:56
Recall that the support for
X is the frequency of item set X and

83
21:23:56 --> 21:24:00
is defined as the number of
transactions containing items in X

84
21:24:00 --> 21:24:03
divided by the total
number of transactions.

85
21:24:03 --> 21:24:06
The rule confidence measures
how frequently items in

86
21:24:06 --> 21:24:10
Y appear in the transaction
that contain X.

87
21:24:10 --> 21:24:15
In other words, the confidence measures
the reliability of the rule by determining

88
21:24:15 --> 21:24:20
how often, if X and
Y is found to be true in the data.

89
21:24:20 --> 21:24:22
How is rule confidence
used in rule generation?

90
21:24:24 --> 21:24:29
Association rules are generated from the
frequent item sets created from the data.

91
21:24:29 --> 21:24:33
Each item in an item set can be
used as a part of the antecedent or

92
21:24:33 --> 21:24:35
consequent of the rule.

93
21:24:35 --> 21:24:38
And you can have many ways to combine
items to form the antecedent and

94
21:24:38 --> 21:24:39
consequent.

95
21:24:40 --> 21:24:44
So if we just simply generate
rules from each frequent item set,

96
21:24:44 --> 21:24:46
we would end up with lots and
lots of rules.

97
21:24:47 --> 21:24:52
Each item set with k items can
generate 2 to the k-2 rules.

98
21:24:52 --> 21:24:54
That's a lot of rules.

99
21:24:54 --> 21:24:57
And the majority of those rules
would not be found in the data.

100
21:24:58 --> 21:25:00
This is where rule confidence comes in.

101
21:25:01 --> 21:25:05
We can use rule confidence to
constrain the number of rules to keep.

102
21:25:06 --> 21:25:10
Specifically, a minimum
confidence threshold is set and

103
21:25:10 --> 21:25:12
only rules with confidence greater than or

104
21:25:12 --> 21:25:17
equal to the minimum confidence are
significant and only those will be kept.

105
21:25:18 --> 21:25:21
Let's look at how this works
with our example dataset.

106
21:25:22 --> 21:25:27
We call that only one three item set
was created from the transactions.

107
21:25:27 --> 21:25:30
That three items that contains
items bread, milk and

108
21:25:30 --> 21:25:31
diaper as shown at the top.

109
21:25:32 --> 21:25:37
With these three item set let's see
how we can generate rules from it and

110
21:25:37 --> 21:25:39
determine which rules to keep and
which one to prune.

111
21:25:41 --> 21:25:44
Let's set the minimum confidence to 0.95.

112
21:25:44 --> 21:25:47
And here again is the definition for
confidence.

113
21:25:48 --> 21:25:53
For candidate rule if bread and
milk then diaper,

114
21:25:53 --> 21:25:56
we can calculate it's confidence
as follows the support for

115
21:25:56 --> 21:26:01
both antecedent and consequent is
the number of times we see bread, milk and

116
21:26:01 --> 21:26:07
diaper together in the data, divided
by the total number of transactions.

117
21:26:07 --> 21:26:12
Items bread, milk and diaper appear
together in transaction 1, 4 and

118
21:26:12 --> 21:26:15
5 so the support is 3/5.

119
21:26:15 --> 21:26:19
The support for just the antecedent is
the number of times we see bread and

120
21:26:19 --> 21:26:22
milk together divided by
the total number of transactions.

121
21:26:23 --> 21:26:28
Items bread and milk appear together
also in transactions 1, 4, and 5.

122
21:26:28 --> 21:26:31
So the support is 3/5.

123
21:26:31 --> 21:26:36
The confidence of this rule is then 1,
or 100%.

124
21:26:36 --> 21:26:39
This means that the rule is correct 100%.

125
21:26:39 --> 21:26:43
Every time bread and milk are bought
together, diaper is bought as well.

126
21:26:44 --> 21:26:46
For candidate rule if bread and

127
21:26:46 --> 21:26:51
diaper than milk,
we calculate its confidence the same way.

128
21:26:51 --> 21:26:55
The support for bread, diaper and
milk is 3/5 as before.

129
21:26:55 --> 21:27:01
Items bread and diaper are paired
together in transactions 1, 2, 4 and 5.

130
21:27:01 --> 21:27:06
So the support for
the items set with bread and milk is 4/5.

131
21:27:06 --> 21:27:11
Then the confidence with
this rule is 0.75 or 75%.

132
21:27:11 --> 21:27:14
Since the minimum confidence is 0.95 or
95%,

133
21:27:14 --> 21:27:19
the first rule is kept and the second
rule is removed from consideration.

134
21:27:21 --> 21:27:24
There are several algorithms for
association analysis.

135
21:27:24 --> 21:27:28
Each uses a different set of methods
to make frequent items set creation and

136
21:27:28 --> 21:27:30
rule generation efficient.

137
21:27:30 --> 21:27:35
The more popular algorithms are Apriori,
FP Growth and Eclat.

138
21:27:36 --> 21:27:41
As a summary, we just looked at the steps
in association analysis in more detail.

139
21:27:41 --> 21:27:46
We saw how items sets can be created from
a dataset, how frequent items sets can be

140
21:27:46 --> 21:27:50
identified, and how association rules can
be created from frequent item sets and

141
21:27:50 --> 21:27:52
pruned using rule confidence.

1
18:46:24 --> 18:46:28
We've looked at classification,
regression and cluster analysis.

2
18:46:28 --> 18:46:33
Let's now discuss association
analysis as a machine learning task.

3
18:46:33 --> 18:46:39
After this video, you will be able to
explain what association analysis entails,

4
18:46:39 --> 18:46:44
list some applications of association
analysis, define what an item set is.

5
18:46:45 --> 18:46:49
In association analysis, the goal is to
come up with a set of rules to capture

6
18:46:49 --> 18:46:53
associations between items or events.

7
18:46:53 --> 18:46:57
The rules are used to determine when
items or events occur together.

8
18:46:57 --> 18:47:02
You may remember seeing these images
earlier in the course, where we introduced

9
18:47:02 --> 18:47:05
the different categories of machine
learning tasks and techniques.

10
18:47:05 --> 18:47:09
But do you remember what
the association is between these items?

11
18:47:09 --> 18:47:12
Well, let's recap that story,
in case you don't remember.

12
18:47:12 --> 18:47:14
The story goes like this.

13
18:47:14 --> 18:47:18
A supermarket chain used
association analysis to discover

14
18:47:18 --> 18:47:22
a connection between two
seemingly unrelated products.

15
18:47:22 --> 18:47:25
They discovered that many
customers who go to the store

16
18:47:25 --> 18:47:29
late on Sunday night to buy
diapers also tend to buy beer.

17
18:47:29 --> 18:47:33
This information was then used to
place beer and diapers close together.

18
18:47:33 --> 18:47:36
And they saw a jump in
sales of both items.

19
18:47:36 --> 18:47:39
This illustrates that you
can uncover unexpected and

20
18:47:39 --> 18:47:43
useful relationships with
association analysis.

21
18:47:43 --> 18:47:47
This diaper and beer story has become
part of the data mining folklore.

22
18:47:47 --> 18:47:51
It's unclear how much of it true, but it
has become the prime example of what you

23
18:47:51 --> 18:47:55
can discover with association analysis and
machine learning in general.

24
18:47:56 --> 18:48:01
A common application of association
analysis is referred to as

25
18:48:01 --> 18:48:03
market basket analysis.

26
18:48:03 --> 18:48:07
This is used to understand
the purchasing behavior of customers.

27
18:48:07 --> 18:48:11
The idea is that you're looking into
the shopping basket of customers

28
18:48:11 --> 18:48:12
when they are at the market, and

29
18:48:12 --> 18:48:17
analyzing that data to understand
what items are purchased together.

30
18:48:17 --> 18:48:21
This information can be used to
place related items together, or

31
18:48:21 --> 18:48:24
to have sales on items that
are often purchased together.

32
18:48:25 --> 18:48:29
Another application of association
analysis is to recommend items that

33
18:48:29 --> 18:48:34
a customer may be interested in, based
on their purchasing or browsing history.

34
18:48:34 --> 18:48:37
This is very commonly used
on companies' websites

35
18:48:37 --> 18:48:39
to get customers to buy more items.

36
18:48:40 --> 18:48:42
There are medical applications as well.

37
18:48:42 --> 18:48:47
Analysis of patients and treatments may
reveal associations to identify effective

38
18:48:47 --> 18:48:51
treatments for
patients with certain medical histories.

39
18:48:51 --> 18:48:54
this diagram illustrates how
association analysis works.

40
18:48:54 --> 18:48:58
The data set is a collection
of transactions.

41
18:48:58 --> 18:49:00
Each transaction contains one or
more items.

42
18:49:01 --> 18:49:04
This is referred to as the item set.

43
18:49:04 --> 18:49:06
From the given items sets,

44
18:49:06 --> 18:49:10
generate association rules that capture
which item tend occur together.

45
18:49:11 --> 18:49:15
In our example, the data set
consists of five transactions.

46
18:49:15 --> 18:49:20
In the first transaction,
the items are diaper, bread and milk.

47
18:49:20 --> 18:49:25
The second transaction has items bread,
diaper, beer, and eggs, and so on.

48
18:49:27 --> 18:49:31
Rules that could be generated from
this data set are shown at the bottom.

49
18:49:31 --> 18:49:34
For example,
the first rule states that if bread and

50
18:49:34 --> 18:49:37
milk are bought together,
then diaper is also bought.

51
18:49:37 --> 18:49:41
The second rule state that if milk is
bought, then bread is also bought.

52
18:49:43 --> 18:49:46
The association analysis process
consist of the following steps.

53
18:49:48 --> 18:49:51
The first step is to create item sets.

54
18:49:51 --> 18:49:55
Item sets are generated for sets with one
item, two items, three items and so on.

55
18:49:57 --> 18:50:00
Then frequent item sets are identified.

56
18:50:00 --> 18:50:04
Frequent item sets are those that occur
at least a minimum number of times.

57
18:50:05 --> 18:50:09
From the frequent item sets,
association rules are generated.

58
18:50:09 --> 18:50:12
We will take a more detailed look
at these steps in the next lecture.

59
18:50:13 --> 18:50:16
Some things to note about
association analysis.

60
18:50:16 --> 18:50:21
Like cluster analysis, each transaction
does not have a label to specify which

61
18:50:21 --> 18:50:23
item set or rule it belongs to.

62
18:50:23 --> 18:50:27
So, association analysis
is an unsupervised task.

63
18:50:28 --> 18:50:31
You may end up with many rules
at the end of the analysis.

64
18:50:31 --> 18:50:34
But, whether those rules are interesting,
useful,

65
18:50:34 --> 18:50:39
or applicable, requires interpretation
using domain knowledge of the application.

66
18:50:40 --> 18:50:41
In addition,

67
18:50:41 --> 18:50:46
the association analysis process will
not tell you how to apply the rules.

68
18:50:46 --> 18:50:49
This also requires knowledge
of the application.

69
18:50:49 --> 18:50:54
So as with cluster analysis,
interpretation and analysis are required

70
18:50:54 --> 18:50:58
to make sense of the resulting rules
that you get from association analysis.

71
18:50:59 --> 18:51:01
In summary,

72
18:51:01 --> 18:51:06
association analysis finds rules to
capture associations between items.

73
18:51:06 --> 18:51:10
The association rules have intuitive
appeal because they are in the form of

74
18:51:10 --> 18:51:14
if this, then that,
which is easy to understand.

75
18:51:14 --> 18:51:18
The results of association
analysis require analysis and

76
18:51:18 --> 18:51:22
interpretation using domain knowledge
to determine the usefulness and

77
18:51:22 --> 18:51:24
applicability of the resulting rules.

78
18:51:25 --> 18:51:28
Next we will cover the steps in
the association analysis process in

79
18:51:28 --> 18:51:29
more detail.

1
13:37:53 --> 13:37:58
In this activity, we will use
Spark to perform cluster analysis.

2
13:37:58 --> 13:38:00
First, we will load
the minute weather data.

3
13:38:01 --> 13:38:05
Next, we will remove the unused and
missing data and

4
13:38:05 --> 13:38:08
then scale the data so
that the mean is zero.

5
13:38:08 --> 13:38:11
We will then create an elbow plot,
a subset of the data,

6
13:38:11 --> 13:38:15
to determine the optimal
number of clusters.

7
13:38:15 --> 13:38:19
And then cluster the full
data set using k-means.

8
13:38:19 --> 13:38:23
Finally, we will generate parallel plots
to analyze the individual clusters.

9
13:38:25 --> 13:38:25
Let's begin.

10
13:38:26 --> 13:38:28
First, let's open the clustering notebook.

11
13:38:31 --> 13:38:34
Execute the first cell
to load the libraries.

12
13:38:36 --> 13:38:38
Execute the second cell
to load the data set.

13
13:38:40 --> 13:38:44
This data set contains weather station
measurements that were taken every minute.

14
13:38:44 --> 13:38:46
So there's a lot of measurements.

15
13:38:46 --> 13:38:53
We can count how many rows there are in
the data frame by running df.count.

16
13:38:54 --> 13:38:58
This says that there are over 1.5
million rows in the data frame.

17
13:38:58 --> 13:39:03
Clustering this much data on a single
Cloudera VM can take a lot of time.

18
13:39:03 --> 13:39:05
So let's only work with
one-tenth of the data.

19
13:39:05 --> 13:39:08
Let's subset the data
into the new data frame.

20
13:39:08 --> 13:39:13
We'll enter filteredDF

21
13:39:13 --> 13:39:18
= df.filter((df.rowID

22
13:39:18 --> 13:39:22
% 10)) == 0.

23
13:39:22 --> 13:39:31
We then count the rows of the new data
frame by calling filteredDF.count.

24
13:39:33 --> 13:39:38
The new data frame has one-tenth as
many rows as the original data set.

25
13:39:39 --> 13:39:41
Let's compute the summary
statistics using describe.

26
13:39:41 --> 13:39:47
filteredDF.describe, and
to display it nicely,

27
13:39:47 --> 13:39:52
we'll enter toPandas().transpose.

28
13:39:52 --> 13:39:52
Run this.

29
13:40:00 --> 13:40:03
These weather measurements were taken
during the period of a long drought.

30
13:40:05 --> 13:40:08
As we can see from the mean values
of the two rain measurements,

31
13:40:08 --> 13:40:12
rain accumulation, the rain duration,
the measurements are close to zero.

32
13:40:14 --> 13:40:17
Let's count how many rain
values are equal to 0.

33
13:40:17 --> 13:40:25
FilterDF.filter(filterDF.rain_accumulation
==

34
13:40:25 --> 13:40:28
0.0).count().

35
13:40:31 --> 13:40:33
Now let's do rain duration.

36
13:40:33 --> 13:40:40
filteredDF.filter(filteredDF.rain_duration
==

37
13:40:40 --> 13:40:44
0.0).count().

38
13:40:44 --> 13:40:50
We can see from these counts that most
in these two measurements are zero.

39
13:40:50 --> 13:40:53
So let's remove them from our data frame.

40
13:40:53 --> 13:40:59
workingDF =
filterDF.drop('rain_accumulation').drop('-

41
13:40:59 --> 13:41:01
rain_duration').

42
13:41:01 --> 13:41:04
And we'll also drop the column
called hpwren_timestamp,

43
13:41:04 --> 13:41:06
since we will not use it.

44
13:41:06 --> 13:41:09
So .drop('hpwren_timestamp').

45
13:41:13 --> 13:41:18
Next, let's drop rows with missing values,
and count how many rows were dropped.

46
13:41:18 --> 13:41:23
before = workingDF.count() workingDF =

47
13:41:23 --> 13:41:29
workingDF.na.drop() after
= workingDF.count(),

48
13:41:29 --> 13:41:35
and finally, we'll print the difference,
before- after.

49
13:41:39 --> 13:41:42
So only 46 rows had missing values and
were dropped.

50
13:41:43 --> 13:41:47
Next, let's scale the data so that each
feature will have a value of 0 for

51
13:41:47 --> 13:41:51
the mean and a value of 1 for
the standard deviation.

52
13:41:51 --> 13:41:54
First, we need to combine the columns
into a single vector column.

53
13:41:54 --> 13:41:59
We can look at the existing columns
by entering workingDF.columns.

54
13:41:59 --> 13:42:03
We do not want to include rowID
since it is a row number.

55
13:42:03 --> 13:42:06
Additionally, the minimum wind
measurements have a high correlation

56
13:42:06 --> 13:42:09
to the average wind measurements,
so we will not include this either.

57
13:42:10 --> 13:42:13
Let's create an array of
the columns we want to combine and

58
13:42:13 --> 13:42:16
then use vector assembler to
create the vector column.

59
13:42:16 --> 13:42:21
featuresUsed = [.

60
13:42:21 --> 13:42:24
Now, let's copy and
paste the columns we want to use.

61
13:42:43 --> 13:42:50
Assembler = VectorAssembler(inputCols
= featuresUsed,

62
13:42:50 --> 13:42:54
outputCol = "features_unscaled").

63
13:42:54 --> 13:43:00
And finally, assembled =
assembler.transform(workingDF).

64
13:43:00 --> 13:43:02
Run this.

65
13:43:02 --> 13:43:08
Now we'll use standard
scaler to scale the data.

66
13:43:08 --> 13:43:14
Scaler = standard scaler
inputCol=”features _unscaler”,

67
13:43:14 --> 13:43:18
outputCol=“features”, withStd = true,

68
13:43:18 --> 13:43:24
withMean = True,
scalerModel = scaler.fit(assembled),

69
13:43:24 --> 13:43:30
scaledData =
scalerModel.transform[assembled].

70
13:43:36 --> 13:43:39
Next we will create an elbow plot
to determine the value k, for

71
13:43:39 --> 13:43:41
the number of clusters.

72
13:43:41 --> 13:43:45
To create the elbow plot,
we will calculate the within cluster,

73
13:43:45 --> 13:43:49
sum of squared error, or WSSE,
for different values of k.

74
13:43:49 --> 13:43:53
So since some valves running
K-means many times, let's do it for

75
13:43:53 --> 13:43:56
a smaller subset of the data
since it'll be faster.

76
13:43:58 --> 13:44:01
First, let's choose the data to work with.

77
13:44:01 --> 13:44:03
Let's subset the data.

78
13:44:03 --> 13:44:08
So scaledData =
scaledData.select("features"), ("rowID").

79
13:44:08 --> 13:44:15
elbowset =
scaledData.filter((scaledData.rowID

80
13:44:15 --> 13:44:21
%3) == 0).select("features").

81
13:44:21 --> 13:44:25
And finally,
we'll call the persist method on elbowset.

82
13:44:25 --> 13:44:29
Persist will keep it in memory,
and make the calculations faster.

83
13:44:29 --> 13:44:33
Run this.

84
13:44:33 --> 13:44:39
Now, let's compute the WSSE values for
different values of k.

85
13:44:39 --> 13:44:42
We'll do for k, 2 to 30.

86
13:44:42 --> 13:44:46
Clusters equals range(2,31).

87
13:44:46 --> 13:44:55
wsseList = utils.elbow(elbowset,
clusters).

88
13:44:55 --> 13:45:02
Run this, This will print out
the value of wsse for each value of k.

89
13:45:02 --> 13:45:06
As you can tell, this'll take some
time to run, let's skip to the end.

90
13:45:09 --> 13:45:12
Now let's display the plot of our data.

91
13:45:12 --> 13:45:22
utils.elbow_plot(wsseList, clusters).

92
13:45:22 --> 13:45:29
The x axis is k, the number of clusters,
and the y axis is the WSSE value.

93
13:45:29 --> 13:45:33
You can see that the graph flattens
out between 10 and 15 for k.

94
13:45:34 --> 13:45:38
So let's choose k equals 12 as
the midpoint for our number of clusters.

95
13:45:40 --> 13:45:45
We'll now cluster the data into
12 clusters using k-means.

96
13:45:45 --> 13:45:47
First, let's select the data
we want to cluster.

97
13:45:47 --> 13:45:54
scaledDataFeat =
scaledData.select("features"),

98
13:45:54 --> 13:45:58
scaledDataFeat.persist.

99
13:46:00 --> 13:46:03
Now I'll perform
the clustering using kmeans.

100
13:46:03 --> 13:46:08
kmeans = KMeans(k=12, seed=1).

101
13:46:08 --> 13:46:14
model = KMeans.fit(scaledDataFeat),
and finally,

102
13:46:14 --> 13:46:20
transformed =
model.transform(scaleDataFeat).

103
13:46:20 --> 13:46:22
Run this.

104
13:46:22 --> 13:46:26
We can now see the centre
measurement of each

105
13:46:26 --> 13:46:31
cluster by calling model.clusterCenters.

106
13:46:35 --> 13:46:40
It is difficult to compare the cluster
centers just by looking at these numbers.

107
13:46:40 --> 13:46:45
So let's use parallel
plots to visualize them.

108
13:46:45 --> 13:46:51
P = utils.pd_centers(featuresUsed,

109
13:46:51 --> 13:46:55
model.clusterCenters).

110
13:46:56 --> 13:46:57
Let's show the clusters for

111
13:46:57 --> 13:47:01
dry days where the weather samples
have low relative humidity.

112
13:47:02 --> 13:47:12
utils.parallel_plot(P[P['relative_humidi-
ty']

113
13:47:13 --> 13:47:17
< -0.5], P).

114
13:47:21 --> 13:47:24
The x axis of this chart shows
the different measurement types.

115
13:47:25 --> 13:47:29
The y values show the standard deviations,
with zero being the mean.

116
13:47:31 --> 13:47:35
Each line is a different cluster, and
there are five clusters in this graph.

117
13:47:37 --> 13:47:40
We can see that they all have
a low relative humidity.

118
13:47:40 --> 13:47:45
Notice that cluster four, the red one,
has a high average wind speed and

119
13:47:45 --> 13:47:46
a high maximum wind speed.

120
13:47:48 --> 13:47:51
Additionally, it has a low
average wind direction.

121
13:47:51 --> 13:47:55
Which means it was coming from
the north and northeast directions.

122
13:47:55 --> 13:47:58
So this cluster probably
represents Santa Ana conditions.

123
13:48:00 --> 13:48:03
Next, let's show the plot for warm days,

124
13:48:03 --> 13:48:06
the weather samples with
high air temperature.

125
13:48:06 --> 13:48:14
utils.parallel_plot(P[P['air_temp'] >

126
13:48:14 --> 13:48:17
0.5], P).

127
13:48:23 --> 13:48:27
Other clusters in this plot have air
temperature greater than 0.5 standard

128
13:48:27 --> 13:48:28
deviations away from the mean.

129
13:48:29 --> 13:48:33
However, they have different values for
the other features.

130
13:48:33 --> 13:48:35
Now let's show the clusters for cool days.

131
13:48:35 --> 13:48:39
Weather samples with high relative
humidity and low air temperatures.

132
13:48:40 --> 13:48:50
utils.parallel_plot(P[(P['relative_humidi-
ty']

133
13:48:52 --> 13:49:00
> 0.5) & (P['air_temp']

134
13:49:00 --> 13:49:04
< 0.5)], P).

135
13:49:04 --> 13:49:09
All the clusters in this plot have
relative humidity greater than 0.5

136
13:49:09 --> 13:49:14
standard deviations and air temp
less than 0.5 standard deviations.

137
13:49:14 --> 13:49:17
These clusters represent cool
temperature with high humidity and

138
13:49:17 --> 13:49:20
possibly rainy weather patterns.

139
13:49:20 --> 13:49:22
So far we have seen all
the clusters except two.

140
13:49:22 --> 13:49:27
Since it is not falling to any other
categories let's plot this cluster,

141
13:49:27 --> 13:49:36
utils.parallel_plot(P.iloc[[2]],

142
13:49:36 --> 13:49:37
P).

143
13:49:40 --> 13:49:42
Cluster two captures
days with mild weather.

1
03:27:36 --> 03:27:39
In addition to classification and
regression, machine learning tasks and

2
03:27:39 --> 03:27:44
techniques can also fall into another
category known as cluster analysis.

3
03:27:44 --> 03:27:50
After this video, you will be able to
articulate the goal of cluster analysis,

4
03:27:50 --> 03:27:55
discuss whether cluster analysis
is supervised or unsupervised, and

5
03:27:55 --> 03:27:57
list some ways that cluster
results can be applied.

6
03:27:59 --> 03:28:00
In cluster analysis,

7
03:28:00 --> 03:28:05
the goal is to organize similar items in
your data set into groups or clusters.

8
03:28:05 --> 03:28:11
By segmenting your data into clusters, you
can analyze each cluster more carefully.

9
03:28:11 --> 03:28:14
Note that cluster analysis is
also referred to as clustering.

10
03:28:16 --> 03:28:21
A very common application of cluster
analysis that we have discussed before is

11
03:28:21 --> 03:28:25
to divide your customer base into segments
based on their purchasing histories.

12
03:28:25 --> 03:28:29
For example, you can segment customers
into those who have purchased science

13
03:28:29 --> 03:28:33
fiction books and videos, versus those
who tend to buy nonfiction books,

14
03:28:33 --> 03:28:37
versus those who have bought
many children's books.

15
03:28:37 --> 03:28:41
This way, you can provide more targeted
suggestions to each different group.

16
03:28:41 --> 03:28:46
Some other examples of cluster analysis
are characterizing different weather

17
03:28:46 --> 03:28:50
patterns for a region,
grouping the latest news articles into

18
03:28:50 --> 03:28:54
topics to identify the trending
topics of the day, and

19
03:28:54 --> 03:28:58
discovering hot spots for different
types of crime from police reports

20
03:28:58 --> 03:29:01
in order to provide sufficient
police presence for problem areas.

21
03:29:03 --> 03:29:07
Cluster analysis divides all
the samples in a data set into groups.

22
03:29:08 --> 03:29:11
In this diagram,
we see that the red, green, and

23
03:29:11 --> 03:29:13
purple data points are clustered together.

24
03:29:14 --> 03:29:18
Which group a sample is placed in is
based on some measure of similarity.

25
03:29:20 --> 03:29:25
The goal of cluster analysis is to segment
data so that differences between samples

26
03:29:25 --> 03:29:30
in the same cluster are minimized, as
shown by the yellow arrow, and differences

27
03:29:30 --> 03:29:35
between samples of different clusters are
maximized, as shown by the orange arrow.

28
03:29:35 --> 03:29:38
Visually, you can think of
this as getting samples in

29
03:29:38 --> 03:29:41
each cluster to be as close
together as possible, and

30
03:29:41 --> 03:29:45
the samples from different clusters
to be as far apart as possible.

31
03:29:46 --> 03:29:49
Cluster analysis requires
some sort of metric to

32
03:29:49 --> 03:29:52
measure similarity between two samples.

33
03:29:52 --> 03:29:57
Some common similarity measures are
Euclidean distance, which is the distance

34
03:29:57 --> 03:30:01
along a straight line between two points,
A and B, as shown in this plot.

35
03:30:02 --> 03:30:06
Manhattan distance, which is
calculated on a strictly horizontal and

36
03:30:06 --> 03:30:10
vertical path, as shown in the right plot.

37
03:30:10 --> 03:30:14
To go from point A to point B, you can
only step along either the x-axis or

38
03:30:14 --> 03:30:18
the y-axis in a two-dimensional case.

39
03:30:18 --> 03:30:22
So the path to calculate the Manhattan
distance consists of segments

40
03:30:22 --> 03:30:27
along the axes instead of along a diagonal
path, as with Euclidean distance.

41
03:30:28 --> 03:30:33
Cosine similarity measures the cosine
of the angle between points A and

42
03:30:33 --> 03:30:35
B, as shown in the bottom plot.

43
03:30:35 --> 03:30:40
Since distance measures such as Euclidean
distance are often used to measure

44
03:30:40 --> 03:30:44
similarity between samples
in clustering algorithms,

45
03:30:44 --> 03:30:48
note that it may be necessary to
normalize the input variables so

46
03:30:48 --> 03:30:51
that no one value dominates
the similarity calculation.

47
03:30:51 --> 03:30:53
We discussed scaling and

48
03:30:53 --> 03:30:57
normalizing variables in the lecture
on feature transformation.

49
03:30:57 --> 03:31:00
Normalizing is one method
to scale variables.

50
03:31:00 --> 03:31:05
Essentially, scaling the input variables
puts the variables on the same scale so

51
03:31:05 --> 03:31:08
that all variables have equal
weighting in the calculation

52
03:31:08 --> 03:31:10
to determine similarity between samples.

53
03:31:11 --> 03:31:16
Scaling is necessary when you have
variables that have very different scales,

54
03:31:16 --> 03:31:18
such as weight and height.

55
03:31:18 --> 03:31:22
The magnitude of the height values,
which are in feet and inches,

56
03:31:22 --> 03:31:26
will be much smaller than the magnitude of
the weight values, which are in pounds.

57
03:31:26 --> 03:31:30
So scaling both variables to
a common value range will

58
03:31:30 --> 03:31:33
make the contributions from
both weight and height equal.

59
03:31:34 --> 03:31:36
Here are some things to note
about cluster analysis.

60
03:31:37 --> 03:31:41
First, unlike classification or
regression, in general,

61
03:31:41 --> 03:31:44
cluster analysis is an unsupervised task.

62
03:31:44 --> 03:31:48
This means that there is no target
label for any sample in the data set.

63
03:31:50 --> 03:31:53
In general,
there is no correct clustering results.

64
03:31:53 --> 03:31:56
The best set of clusters
is highly dependent

65
03:31:56 --> 03:31:59
on how the resulting
clusters will be used.

66
03:31:59 --> 03:32:03
There are numerical measures to
compare two different clusters, but

67
03:32:03 --> 03:32:06
since there are no labels to
determine whether a sample has

68
03:32:06 --> 03:32:09
been correctly clustered,
there is no ground truth

69
03:32:09 --> 03:32:13
to determine if a set of clustering
results are truly correct or incorrect.

70
03:32:15 --> 03:32:17
Clusters don't come with labels.

71
03:32:17 --> 03:32:20
You may end up with five different
clusters at the end of a cluster

72
03:32:20 --> 03:32:24
analysis process, but you don't
know what each cluster represents.

73
03:32:24 --> 03:32:27
Only by analyzing
the samples in each cluster

74
03:32:27 --> 03:32:31
can you come out with reasonable
labels for your clusters.

75
03:32:31 --> 03:32:35
Given all this, it is important to
keep in mind that interpretation and

76
03:32:35 --> 03:32:40
analysis of the clusters
are required to make sense of and

77
03:32:40 --> 03:32:42
make use of the results
of cluster analysis.

78
03:32:44 --> 03:32:48
There are several ways that the results
of cluster analysis can be used.

79
03:32:48 --> 03:32:51
The most obvious is data segmentation and
the benefits that come from that.

80
03:32:52 --> 03:32:56
If you segment your customer base
into different types of readers,

81
03:32:56 --> 03:33:00
the resulting insights can be used
to provide more effective marketing

82
03:33:00 --> 03:33:03
to the different customer groups
based on their preferences.

83
03:33:03 --> 03:33:07
For example, analyzing each segment
separately can provide valuable insights

84
03:33:07 --> 03:33:12
into each group's likes, dislikes and
purchasing behavior, just like we see

85
03:33:12 --> 03:33:15
science fiction, non-fiction and
children's books preferences here.

86
03:33:17 --> 03:33:20
Clusters can also be used to
classify new data samples.

87
03:33:20 --> 03:33:23
When a new sample is received,

88
03:33:23 --> 03:33:27
like the orange sample here, compute
the similarity measure between it and

89
03:33:27 --> 03:33:32
the centers of all clusters, and assign
a new sample to the closest cluster.

90
03:33:32 --> 03:33:33
The label of that cluster,

91
03:33:33 --> 03:33:39
manually determined through analysis,
is then used to classify the new sample.

92
03:33:39 --> 03:33:43
In our book buyers' preferences example,
a new customer can be classified as being

93
03:33:43 --> 03:33:47
either a science fiction, non-fiction or
children's books customer

94
03:33:47 --> 03:33:51
depending on which cluster the new
customer is most similar to.

95
03:33:52 --> 03:33:54
Once cluster labels have been determined,

96
03:33:54 --> 03:33:59
samples in each cluster can be used as
labeled data for a classification task.

97
03:33:59 --> 03:34:03
The samples would be the input
to the classification model.

98
03:34:03 --> 03:34:06
And the cluster label would be
the target class for each sample.

99
03:34:06 --> 03:34:10
This process can be used to provide much
needed labeled data for classification.

100
03:34:12 --> 03:34:17
Yet another use of cluster results
is as a basis for anomaly detection.

101
03:34:17 --> 03:34:19
If a sample is very far away, or

102
03:34:19 --> 03:34:24
very different from any of the cluster
centers, like the yellow sample here,

103
03:34:24 --> 03:34:28
then that sample is a cluster outlier and
can be flagged as an anomaly.

104
03:34:29 --> 03:34:33
However, these anomalies
require further analysis.

105
03:34:33 --> 03:34:37
Depending on the application, these
anomalies can be considered noise, and

106
03:34:37 --> 03:34:39
should be removed from the data set.

107
03:34:39 --> 03:34:43
An example of this would be a sample
with a value of 150 for age.

108
03:34:44 --> 03:34:49
For other cases, these anomalous cases
should be studied more carefully.

109
03:34:49 --> 03:34:53
Examples of this are in a credit
card fraud detection, or,

110
03:34:53 --> 03:34:56
a network intrusion detection application.

111
03:34:56 --> 03:35:00
In these applications, examples outside
of the norm are the interesting cases

112
03:35:00 --> 03:35:04
that should be looked at to determine
if they represent potential problems.

113
03:35:05 --> 03:35:11
To summarize, cluster analysis is used to
organize similar data items into groups or

114
03:35:11 --> 03:35:12
clusters.

115
03:35:12 --> 03:35:16
Analyzing the resulting clusters
often leads to useful insights

116
03:35:16 --> 03:35:19
about the characteristics of each group,

117
03:35:19 --> 03:35:22
as well as the underlying
structure of the entire data set.

118
03:35:23 --> 03:35:25
Clusters require analysis and

119
03:35:25 --> 03:35:28
interpretation to make
sense of the results,

120
03:35:28 --> 03:35:33
since there are no labels associated with
samples or clusters in a clustering task.

121
03:35:33 --> 03:35:37
In the next lecture, we will discuss
a specific algorithm for cluster analysis.

1
07:03:12 --> 07:03:16
k-means clustering is a simple yet
effective algorithm for

2
07:03:16 --> 07:03:18
cluster analysis that is
commonly used in practice.

3
07:03:19 --> 07:03:20
After this video,

4
07:03:20 --> 07:03:25
you will be able to describe
the steps in the k-means algorithm,

5
07:03:25 --> 07:03:30
explain what the k stands for in k-means
and define what a cluster centroid is.

6
07:03:31 --> 07:03:36
Recall a cluster analysis divides samples
in a data set into groups or clusters.

7
07:03:36 --> 07:03:41
The idea is to group similar items in
the same cluster, where similar is defined

8
07:03:41 --> 07:03:45
by some metric that measures
similarity between data samples.

9
07:03:45 --> 07:03:48
So the goal of cluster analysis
is to divide data sample

10
07:03:48 --> 07:03:53
such that sample within a cluster
are as close together as possible.

11
07:03:53 --> 07:03:58
And samples from different clusters
are as far apart as possible.

12
07:03:58 --> 07:04:02
k-means is a classic algorithm used for
cluster analysis.

13
07:04:02 --> 07:04:03
The algorithm is very simple.

14
07:04:05 --> 07:04:08
The first step is to select
k initial centroids.

15
07:04:09 --> 07:04:13
A centroid is simply the center of
a cluster, as you see in the diagram here.

16
07:04:15 --> 07:04:19
Next, assign each sample in
a dataset to the closest centroid.

17
07:04:19 --> 07:04:22
This means you calculate
the distance between the sample and

18
07:04:22 --> 07:04:28
each cluster center and assign a sample
to the cluster with the closest centroid.

19
07:04:28 --> 07:04:30
Then you calculate the mean,

20
07:04:30 --> 07:04:33
or average, of each cluster
to determine a new centroid.

21
07:04:35 --> 07:04:39
These two steps are then repeated until
some stopping criterion are reached.

22
07:04:40 --> 07:04:44
Here's an illustration
of how k-Means works.

23
07:04:44 --> 07:04:46
(a) shows the original data
set with some samples.

24
07:04:48 --> 07:04:50
And (b) illustrates centroids
initially selected.

25
07:04:52 --> 07:04:54
(c) shows the first iteration.

26
07:04:54 --> 07:04:58
Here, samples are assigned
to the closest centroid.

27
07:04:58 --> 07:05:01
In (d), the centroids are recalculated.

28
07:05:02 --> 07:05:04
(e) shows the second iteration.

29
07:05:04 --> 07:05:07
Samples are assigned to
the closer centroid.

30
07:05:07 --> 07:05:11
Note that some samples changed
their cluster assignments.

31
07:05:11 --> 07:05:14
And in (f),
the centroids are recalculated again.

32
07:05:14 --> 07:05:15
Cluster assignments and

33
07:05:15 --> 07:05:20
centroid calculations are repeated until
some stopping criteria is reached.

34
07:05:20 --> 07:05:22
And you get your final clusters,
as shown in (f).

35
07:05:24 --> 07:05:26
How are the initial centroids selected?

36
07:05:26 --> 07:05:30
The issue is that the final
cluster results are sensitive

37
07:05:30 --> 07:05:31
to initial centroids.

38
07:05:31 --> 07:05:34
This means that cluster results with
one set of initial centroids can be

39
07:05:34 --> 07:05:39
very different from results with
another set of initial centroids.

40
07:05:39 --> 07:05:42
There are many approaches to
selecting the initial centroids for

41
07:05:42 --> 07:05:45
k-means varying in levels
of sophistication.

42
07:05:45 --> 07:05:50
The easiest and most widely used approach
is to apply k-means several times

43
07:05:50 --> 07:05:55
with different initial centroids
randomly chosen to cluster you dataset.

44
07:05:55 --> 07:05:59
And then select the centroids that
give the best clustering results.

45
07:06:00 --> 07:06:02
To evaluate the cluster results,
an error measure,

46
07:06:02 --> 07:06:07
known as the within cluster sum
of squared error, can be used.

47
07:06:07 --> 07:06:10
The error associated with a sample

48
07:06:10 --> 07:06:15
within a cluster is the distance between
the sample and the cluster centroid.

49
07:06:15 --> 07:06:19
The squared error of the sample then,
is the squared of that distance.

50
07:06:21 --> 07:06:24
We sum up all the squared errors for
all samples for

51
07:06:24 --> 07:06:27
a cluster to the get the squared error for
that cluster.

52
07:06:28 --> 07:06:33
We then do the same thing for all
clusters to get the final calculation for

53
07:06:33 --> 07:06:36
the within-cluster sum
of squared error for

54
07:06:36 --> 07:06:39
all clusters in the results
of a cluster analysis run.

55
07:06:40 --> 07:06:45
Given two clustering results, the one with
the smaller within-cluster sum of squared

56
07:06:45 --> 07:06:51
error, or WSSE for short,
provides the better solution numerically.

57
07:06:51 --> 07:06:54
However, as we've discussed before,
there is no ground truth

58
07:06:54 --> 07:06:59
to mathematically determine which set of
clusters is more correct than the other.

59
07:07:00 --> 07:07:04
In addition, note that increasing
the number of clusters,

60
07:07:04 --> 07:07:08
that is, increasing the value for
k, always reduces WSSE.

61
07:07:08 --> 07:07:12
So WSSE should be used with caution.

62
07:07:12 --> 07:07:16
It only makes sense to use WSSE
to compare two sets of clusters

63
07:07:16 --> 07:07:20
with the same value for k and
generate it from the same dataset.

64
07:07:22 --> 07:07:25
Also the set of clusters with
the smallest WSSE may not

65
07:07:25 --> 07:07:28
always be the best solution for
the application at hand.

66
07:07:28 --> 07:07:30
Again, interpretation and

67
07:07:30 --> 07:07:34
domain knowledge about what the cluster
should represent and how they will be used

68
07:07:34 --> 07:07:38
are crucial in determining
which cluster results are best.

69
07:07:38 --> 07:07:42
Now that there are several metrics that are
used to evaluate cluster results as well.

70
07:07:43 --> 07:07:48
Choosing the optimal value for k is
always a big question in using k-means.

71
07:07:48 --> 07:07:51
There are several methods to
determine the value for k.

72
07:07:51 --> 07:07:53
We will discuss a few here.

73
07:07:54 --> 07:07:57
Visualization techniques can be used
to determine the dataset to see if

74
07:07:57 --> 07:08:00
there are natural
groupings of the samples.

75
07:08:00 --> 07:08:01
Scatter plots and

76
07:08:01 --> 07:08:05
the use of dimensionality reduction
are useful here, to visualize the data.

77
07:08:06 --> 07:08:09
A good value for
k is application-dependent.

78
07:08:09 --> 07:08:14
So domain knowledge of the application can
drive the selection for the value of k.

79
07:08:14 --> 07:08:15
For example,

80
07:08:15 --> 07:08:18
if you want to cluster the types of
products customers are purchasing,

81
07:08:18 --> 07:08:23
a natural choice for k might be the number
of product categories that you offer.

82
07:08:23 --> 07:08:28
Or k might be selected to represent the
geographical locations of respondents to

83
07:08:28 --> 07:08:29
a survey.

84
07:08:29 --> 07:08:31
In which case, a good value for

85
07:08:31 --> 07:08:34
k would be the number of regions
your interested in analyzing.

86
07:08:36 --> 07:08:40
There are also data-driven method for
determining the value of k.

87
07:08:40 --> 07:08:42
These methods calculate symmetric for

88
07:08:42 --> 07:08:46
different values of k to determine
the best selections of k.

89
07:08:46 --> 07:08:48
One such method is the elbow method.

90
07:08:49 --> 07:08:53
The elbow method for determining
the value of k is shown on this plot.

91
07:08:53 --> 07:08:56
As we saw in the previous slide,

92
07:08:56 --> 07:09:00
WSSE, or within-cluster sum of
squared error, measures how much

93
07:09:00 --> 07:09:05
data samples deviate from their respective
centroids in a set of clustering results.

94
07:09:05 --> 07:09:10
If we plot WSSE for different values for
k, we can see how this

95
07:09:10 --> 07:09:16
error measure changes as a value
of k changes as seen in the plot.

96
07:09:16 --> 07:09:22
The bend in this error curve indicates
a drop in gain by adding more clusters.

97
07:09:22 --> 07:09:26
So this elbow in the curve provides
a suggestion for a good value of k.

98
07:09:27 --> 07:09:31
Note that the elbow can not always be
unambiguously determined, especially for

99
07:09:31 --> 07:09:33
complex data.

100
07:09:33 --> 07:09:36
And in many cases, the error curve
will not have a clear suggestion for

101
07:09:36 --> 07:09:39
one value, but for multiple values.

102
07:09:39 --> 07:09:42
This can be used as a guideline for
the range of values to try for k.

103
07:09:44 --> 07:09:46
We've discussed choosing
the initial centroids and

104
07:09:46 --> 07:09:50
looked at ways to select a value for
k, the number of clusters.

105
07:09:50 --> 07:09:52
Let's now look at when to stop.

106
07:09:52 --> 07:09:54
How do you know when to stop
iterating when using k-means?

107
07:09:54 --> 07:10:00
One obviously stopping criterion is when
there are no changes to the centroids.

108
07:10:00 --> 07:10:04
This means that no samples would
change cluster assignments.

109
07:10:04 --> 07:10:08
And recalculating the centroids
will not result in any changes.

110
07:10:08 --> 07:10:11
So additional iterations will
not bring about any more changes

111
07:10:11 --> 07:10:12
to the cluster results.

112
07:10:13 --> 07:10:18
The stopping criterion can be relaxed to
the second stopping criterion listed here.

113
07:10:18 --> 07:10:21
Which is when the number of
sample changing clusters is below

114
07:10:21 --> 07:10:25
a certain threshold, say 1% for example.

115
07:10:25 --> 07:10:28
At this point, the clusters
are changing by only a few samples,

116
07:10:28 --> 07:10:32
resulting in only minimal changes
to the final cluster results.

117
07:10:32 --> 07:10:34
So the algorithm can be stopped here.

118
07:10:35 --> 07:10:39
At the end of k-means we have a set
of clusters, each with a centroid.

119
07:10:40 --> 07:10:44
Each centroid is the mean of
the samples assigned to that cluster.

120
07:10:44 --> 07:10:49
You can think of the centroid as
a representative sample for that cluster.

121
07:10:49 --> 07:10:51
So to interpret the cluster
analysis results,

122
07:10:51 --> 07:10:54
we can examine the cluster centroids.

123
07:10:54 --> 07:10:58
Comparing the values of the variables
between the centroids will reveal

124
07:10:58 --> 07:11:01
how different or alike clusters are and

125
07:11:01 --> 07:11:04
provide insights into what
each cluster represents.

126
07:11:04 --> 07:11:09
For example, if the value for age is
different for different customer clusters,

127
07:11:09 --> 07:11:12
this indicates that the clusters
are encoding different

128
07:11:12 --> 07:11:15
customer segments by age,
among other variables.

129
07:11:15 --> 07:11:20
In summary, k-means is a classic algorithm
for performing cluster analysis.

130
07:11:20 --> 07:11:23
It is an algorithm that is simple
to understand and implement, and

131
07:11:23 --> 07:11:25
is also efficient.

132
07:11:25 --> 07:11:29
The value of k, the number of clusters,
must be specified.

133
07:11:29 --> 07:11:32
And final clusters are sensitive
to initial centroids.

1
14:14:45 --> 14:14:48
Linear regression is a very common
algorithm to build regression models.

2
14:14:50 --> 14:14:55
After this video you will be able to
describe how linear regression works,

3
14:14:55 --> 14:15:00
discuss how least squares is used in
linear regression, define simple and

4
14:15:00 --> 14:15:01
multiple linear regression.

5
14:15:03 --> 14:15:08
A linear regression model captures the
relationship between a numerical output

6
14:15:08 --> 14:15:10
and the input variables.

7
14:15:10 --> 14:15:14
The relationship is modeled as
a linear relationship hence the linear

8
14:15:14 --> 14:15:15
in linear regression.

9
14:15:16 --> 14:15:21
To see how linear regression works, let's
take a look at an example from the Iris

10
14:15:21 --> 14:15:25
flower dataset, which is a commonly
used dataset for machine learning.

11
14:15:25 --> 14:15:30
This dataset has samples of
different species of iris flowers

12
14:15:30 --> 14:15:33
along with measurements such as
petal width and petal length.

13
14:15:33 --> 14:15:38
Here we have a plot with petal width
measurements in centimeters on the x axis,

14
14:15:38 --> 14:15:41
and petal length
measurements on the y axis.

15
14:15:41 --> 14:15:45
Let's say that we want to predict
petal length based on petal width.

16
14:15:46 --> 14:15:50
Then the regression task is this,
given a measurement for

17
14:15:50 --> 14:15:52
petal width predict the petal length.

18
14:15:53 --> 14:15:57
We can build a linear regression model to
capture this linear relationship between

19
14:15:57 --> 14:16:01
the input petal width and
the output petal length.

20
14:16:01 --> 14:16:05
The linear relationship for this samples
is shown as the red line on the plot.

21
14:16:06 --> 14:16:10
From this example we see that linear
regression works by finding the best

22
14:16:10 --> 14:16:13
fitting straight line,
through the samples.

23
14:16:13 --> 14:16:15
This is called the regression line.

24
14:16:16 --> 14:16:19
In the simple case with
just one input variable,

25
14:16:19 --> 14:16:22
the regression line is simply a line.

26
14:16:22 --> 14:16:27
The equation for a line is y = mx + b,

27
14:16:27 --> 14:16:31
where m determines
the slope of the line and

28
14:16:31 --> 14:16:35
b is the y intercept or
where the line crosses the y axis.

29
14:16:36 --> 14:16:38
M and b are the parameters of the model.

30
14:16:40 --> 14:16:43
Training a linear regression model
means adjusting these parameters to

31
14:16:43 --> 14:16:46
fit the regression line to the samples.

32
14:16:46 --> 14:16:49
The regression line can be
determined using what's referred to

33
14:16:49 --> 14:16:51
as the least squares method.

34
14:16:52 --> 14:16:56
This plot illustrates how
the least squares method works.

35
14:16:56 --> 14:16:57
The yellow dots are the data samples.

36
14:16:59 --> 14:17:01
The red line is the regression line,

37
14:17:01 --> 14:17:04
the straight line that
goes through the samples.

38
14:17:04 --> 14:17:08
This line represents the model's
prediction of the output given the input.

39
14:17:09 --> 14:17:14
Each green line indicates the distance
of each sample from the regression line.

40
14:17:14 --> 14:17:17
So the green line represents
the error between the prediction,

41
14:17:17 --> 14:17:21
which is the value of the red regression
line and the actual value of the sample.

42
14:17:22 --> 14:17:27
The square of this distance is
referred to as the residual

43
14:17:27 --> 14:17:29
associated with that sample.

44
14:17:29 --> 14:17:31
The least squares method
finds the regression line

45
14:17:31 --> 14:17:36
that makes the sum of
the residuals as small as possible.

46
14:17:36 --> 14:17:39
In other words, we want to find
the line that minimizes the sum

47
14:17:39 --> 14:17:41
of the squared errors of prediction.

48
14:17:43 --> 14:17:47
The goal of linear regression then is
to find the best fitting straight line

49
14:17:47 --> 14:17:49
through the samples using
the least squares method.

50
14:17:50 --> 14:17:54
Once the regression model is built,
we can use it to make predictions.

51
14:17:54 --> 14:17:58
For example,
given a measurement of 1.5 centimeters for

52
14:17:58 --> 14:18:02
petal width, the model will predict
a value of 4.5 centimeters for

53
14:18:02 --> 14:18:06
petal length base on the regression
line that it has constructed.

54
14:18:07 --> 14:18:11
In linear regression, if there is only
one input variable then the task is

55
14:18:11 --> 14:18:14
referred to as simple linear regression.

56
14:18:15 --> 14:18:18
In cases with more than
one input variables,

57
14:18:18 --> 14:18:21
then it is referred to as
multiple linear regression.

58
14:18:21 --> 14:18:25
To summarize, linear regression
captures the linear relationship

59
14:18:25 --> 14:18:29
between a numerical output and
the input variables.

60
14:18:29 --> 14:18:33
The least squares method can be used
to build a linear regression model

61
14:18:33 --> 14:18:35
by finding the best fitting
line through the samples.

1
04:33:19 --> 04:33:23
We studied the machine learning process,
applied techniques to explore and

2
04:33:23 --> 04:33:27
prepare data, discussed the different
categories of machine learning tasks,

3
04:33:27 --> 04:33:30
looked at metrics and
methods for evaluating a model,

4
04:33:30 --> 04:33:35
learned how to use scalable machine
learning algorithms for big data problems,

5
04:33:35 --> 04:33:38
and worked with two widely used tools to
construct the machine learning models.

6
04:33:39 --> 04:33:43
I hope that the lectures, along with
the hands-on activities, have given you

7
04:33:43 --> 04:33:46
a sound and practical introduction to
machine learning tools and techniques.

8
04:33:46 --> 04:33:51
I also hope that the course piqued
your interest in the exiting and

9
04:33:51 --> 04:33:53
rapidly developing field of
machine learning for big data.

10
04:33:55 --> 04:33:57
Keep in mind that the best
way to learn machine learning

11
04:33:57 --> 04:33:59
is to do machine learning.

12
04:33:59 --> 04:34:04
So I encourage you to go out and find
a problem or data set that interest you.

13
04:34:04 --> 04:34:08
Apply the techniques you've learned in
this course to it and start analyzing.

14
04:34:08 --> 04:34:12
Thank you for your time and effort on
this course, happy machine learning.

1
09:07:32 --> 09:07:35
If you recall, we have previously
discussed that the main categories of

2
09:07:35 --> 09:07:40
machine learning tasks are classification,
regression,

3
09:07:40 --> 09:07:43
cluster analysis, and
association analysis.

4
09:07:43 --> 09:07:46
We have discussed
classification in detail.

5
09:07:46 --> 09:07:49
Now let's look at the other categories,
starting with regression.

6
09:07:50 --> 09:07:55
After this video you will be able
to define what regression is,

7
09:07:55 --> 09:07:59
explain the difference between
regression and classification, and

8
09:07:59 --> 09:08:01
name some applications of regression.

9
09:08:03 --> 09:08:07
Before we talk about regression,
let's review classification.

10
09:08:07 --> 09:08:11
In a classification problem the input
data is presented to the machine learning

11
09:08:11 --> 09:08:17
model, and the task is to predict the
target corresponding to the input data.

12
09:08:17 --> 09:08:20
The target is a categorical variable.

13
09:08:20 --> 09:08:25
So the classification task is to
predict the category or label of

14
09:08:25 --> 09:08:28
the target, given the input data.

15
09:08:28 --> 09:08:32
The classification example shown
here is one we have seen before.

16
09:08:32 --> 09:08:37
The input variables are measurements
such as temperature, relative humidity,

17
09:08:37 --> 09:08:41
atmospheric pressure,
wind speed, wind direction, etc.

18
09:08:41 --> 09:08:42
The task for

19
09:08:42 --> 09:08:47
the model is to predict the weather
category associated with the input data.

20
09:08:47 --> 09:08:48
The possible values for

21
09:08:48 --> 09:08:53
the weather category is sunny,
windy, rainy, or cloudy.

22
09:08:53 --> 09:08:57
Since we're predicting the category,
this is a classification task.

23
09:08:57 --> 09:09:02
With that context in mind,
let's now discuss regression.

24
09:09:02 --> 09:09:06
When the model has to predict
a numeric value instead of a category,

25
09:09:06 --> 09:09:09
then the task becomes
a regression problem.

26
09:09:09 --> 09:09:13
An example of regression is to
predict the price of a stock.

27
09:09:13 --> 09:09:16
The stock price is a numeric value,
not a category.

28
09:09:16 --> 09:09:20
So this is a regression task
instead of a classification task.

29
09:09:20 --> 09:09:24
Note that if you were to predict not
the actual price of the stock, but whether

30
09:09:24 --> 09:09:29
the stock price will go up or go down,
then that would be a classification task.

31
09:09:29 --> 09:09:33
That is the main difference between
classification and regression.

32
09:09:33 --> 09:09:36
In classification,
you're predicting a category, and

33
09:09:36 --> 09:09:38
in regression you're
predicting a numeric value.

34
09:09:39 --> 09:09:42
Here are some examples where
regression can be used.

35
09:09:42 --> 09:09:47
Forecast the high temperature for the next
day, estimate the average housing price

36
09:09:47 --> 09:09:52
for a particular region, determine the
demand for a new product, a new book for

37
09:09:52 --> 09:09:57
example, based on similar existing
products, predict the power usage for

38
09:09:57 --> 09:09:59
a particular power grid.

39
09:10:00 --> 09:10:02
This is what the data
set might look like for

40
09:10:02 --> 09:10:06
the regression task of predicting
tomorrow's high temperature.

41
09:10:06 --> 09:10:08
The input variables could
be the high temperature for

42
09:10:08 --> 09:10:11
today, the low temperature for
today, and the month.

43
09:10:11 --> 09:10:15
And the target is the high temperature for
tomorrow.

44
09:10:15 --> 09:10:19
The model has to predict this
target value for each sample.

45
09:10:19 --> 09:10:23
Recall that in a supervised
task the target is provided.

46
09:10:23 --> 09:10:28
Well, for an unsupervised task the target
is not available or not known.

47
09:10:28 --> 09:10:31
Since the target label is provided for
each sample here,

48
09:10:31 --> 09:10:35
the regression task is a supervised one,
similar to classification.

49
09:10:36 --> 09:10:41
As with classification building a
regression model also involve two phases.

50
09:10:41 --> 09:10:44
A training phase in which
the model is built, and

51
09:10:44 --> 09:10:47
a testing phase in which
the model is applied to new data.

52
09:10:48 --> 09:10:52
The model is built using training data and
evaluated on test data.

53
09:10:53 --> 09:10:58
Similar to classification, the goal in
building a regression model is also to

54
09:10:58 --> 09:11:02
have a model perform well on training
data, as well as generalize to new data.

55
09:11:03 --> 09:11:07
The use of three different datasets
that we have previously discussed

56
09:11:07 --> 09:11:09
also apply to regression.

57
09:11:09 --> 09:11:12
Recall that the three
datasets are used as follows.

58
09:11:12 --> 09:11:17
The training dataset is used to train the
model, that is to adjust the parameters of

59
09:11:17 --> 09:11:19
the model to learn the input
to output mapping.

60
09:11:19 --> 09:11:24
The validation dataset is used to
determine when training should stop

61
09:11:24 --> 09:11:26
in order to avoid over fitting.

62
09:11:26 --> 09:11:32
And the test dataset is used to evaluate
the performance of the model on new data.

63
09:11:32 --> 09:11:36
In summary, in regression the model
needs to predict the numeric value

64
09:11:36 --> 09:11:38
corresponding to the input data.

65
09:11:38 --> 09:11:44
Since a target is provided for each
sample, regression is a supervised task.

66
09:11:44 --> 09:11:47
The target is always a numerical
variable in regression.

67
09:11:47 --> 09:11:49
In the next lecture,

68
09:11:49 --> 09:11:52
we will discuss a specific algorithm
to build a regression model.

1
18:19:24 --> 18:19:25
Hello.

2
18:19:25 --> 18:19:28
Welcome to the Graph Analytics module
in the Big Data specialization.

3
18:19:30 --> 18:19:34
I'm Amarnath Gupta, a research scientist
at the San Diego Supercomputer Center.

4
18:19:35 --> 18:19:37
What do I do research on?

5
18:19:38 --> 18:19:43
Well, a number of different areas,
all generally related to data engineering.

6
18:19:44 --> 18:19:48
But the area I'm recently very
excited about has to do with graphs.

7
18:19:50 --> 18:19:53
Now, graphs or
networks has many people call them,

8
18:19:54 --> 18:20:00
are about studying relationships and
relationship patterns on objects.

9
18:20:02 --> 18:20:08
I became interested in graphs
when I was a graduate student and

10
18:20:08 --> 18:20:14
the professor in our artificial
intelligence class showed us how a part of

11
18:20:14 --> 18:20:21
human knowledge can be represented as
a kind of graphs called semantic networks.

12
18:20:23 --> 18:20:28
She showed us how even
very simple things like

13
18:20:28 --> 18:20:32
relationships somewhere in a family can
be represented and viewed as graphs.

14
18:20:33 --> 18:20:36
Represent your knowledge huh?

15
18:20:36 --> 18:20:38
Now, that got me really excited.

16
18:20:39 --> 18:20:45
Someday, I thought this will be a really
interesting topic to research on.

17
18:20:46 --> 18:20:52
What I didn't realize then is that
in just a few years graphs and

18
18:20:52 --> 18:20:58
the need to model and analyze them would
become so dominant in both academia and

19
18:20:58 --> 18:21:03
industry that graphs will
be found everywhere today.

20
18:21:04 --> 18:21:09
Now we look at Facebook,
LinkedIn, Twitter, and many,

21
18:21:09 --> 18:21:14
many more companies that
are thriving in the market with data

22
18:21:14 --> 18:21:19
that are represented, modeled,
and processed as graphs.

23
18:21:21 --> 18:21:24
Now, even the entire World Wide Web,
if you think about it,

24
18:21:24 --> 18:21:26
is a giant graph that people analyze.

25
18:21:27 --> 18:21:29
And that brings us to this course.

26
18:21:29 --> 18:21:34
Now, in this course, I'll introduce you
to the wonderful world of Graph Analytics

27
18:21:34 --> 18:21:37
specifically, I'd like
to show how different

28
18:21:37 --> 18:21:42
kinds of real world data science problems
can be viewed and modeled as graphs.

29
18:21:43 --> 18:21:47
And how the process of solving
them can apply analytical

30
18:21:47 --> 18:21:52
techniques that used graph based methods,
that is Algorithms.

31
18:21:55 --> 18:21:56
This course would have four models.

32
18:21:58 --> 18:22:03
In model one, we'll introduce graphs and
different applications that use graphs.

33
18:22:06 --> 18:22:11
In model two,
we'll cover a number of common techniques,

34
18:22:11 --> 18:22:16
mathematical and algorithm techniques,
that are used in Graph Analytics.

35
18:22:17 --> 18:22:20
In module three,
we'll look at a graph database.

36
18:22:20 --> 18:22:23
And through some sort
of a hands on guidance,

37
18:22:23 --> 18:22:27
we'll show you how to store and
query graph data with the database.

38
18:22:28 --> 18:22:33
In module four, we'll cover some
strategies of handling very large

39
18:22:33 --> 18:22:38
graphs and discuss how existing
tools that are currently used by

40
18:22:38 --> 18:22:44
the community are actually prints.

41
18:22:44 --> 18:22:46
Thank you for joining this course and

42
18:22:46 --> 18:22:50
I sincerely hope that you'll find
it both exciting and useful.

43
18:22:52 --> 18:22:52
Happy learning.

1
12:42:15 --> 12:42:19
As we saw in these four examples
what graphs are used for

2
12:42:19 --> 12:42:22
are kind of different but they all show

3
12:42:22 --> 12:42:26
different viewpoints from which you
can use graphs for your analysis.

4
12:42:26 --> 12:42:31
Since this course focuses
on graph analytics,

5
12:42:31 --> 12:42:34
I'd like to briefly recap
what the term means.

6
12:42:35 --> 12:42:40
Analytics is the ability to
discover meaningful patterns and

7
12:42:40 --> 12:42:44
interesting insights into data using
mathematical properties of data.

8
12:42:46 --> 12:42:50
It covers the process of computing
with mathematical properties, and

9
12:42:50 --> 12:42:53
accessing the data itself efficiently.

10
12:42:53 --> 12:42:55
Further, it involves
the ability to represent and

11
12:42:55 --> 12:42:59
work with domain knowledge as we
saw in use case two with biology.

12
12:42:59 --> 12:43:04
Finally, analytics often involves
statistical modeling techniques for

13
12:43:04 --> 12:43:07
drawing inferences and
making predictions on data.

14
12:43:08 --> 12:43:13
With analytics, we should be able
to achieve the goals shown here.

15
12:43:14 --> 12:43:15
Take a minute to read.

16
12:43:25 --> 12:43:30
Therefore, graph analytics is
a special case of analytics where

17
12:43:30 --> 12:43:33
the underlying data can be
modeled as a set of graphs.

1
01:25:48 --> 01:25:53
So in this video,
we'll going to talk about graph analytics

2
01:25:53 --> 01:25:57
within the context of this
big data specialization.

3
01:26:00 --> 01:26:04
So in the previous courses you know about
the three important V's of big data.

4
01:26:05 --> 01:26:09
So the three well-known V's are volume,
velocity, and variety.

5
01:26:10 --> 01:26:15
We will also talk about a lesser-known V,
which is called valence.

6
01:26:17 --> 01:26:21
Okay, what we want to talk about is,

7
01:26:21 --> 01:26:27
what impact these things
have on graph data.

8
01:26:27 --> 01:26:32
So for volume, let's take a dataset like

9
01:26:32 --> 01:26:37
the load network of the United States.

10
01:26:37 --> 01:26:39
Well, that's a pretty large graph.

11
01:26:39 --> 01:26:43
So when we say volume,
I mean that the size of the graph

12
01:26:43 --> 01:26:47
is much larger than what you might have

13
01:26:48 --> 01:26:53
in the memory of a reasonable computer or
real computing infrastructure.

14
01:26:55 --> 01:27:00
Now, we will see what impact the size

15
01:27:00 --> 01:27:06
of the graph has on analytic operations.

16
01:27:06 --> 01:27:09
What we mean by velocity
when it comes to graphs?

17
01:27:09 --> 01:27:10
Well, think of Facebook again.

18
01:27:11 --> 01:27:13
So these little graphs are updates.

19
01:27:14 --> 01:27:22
So you write a post, then like somebody
else's post, and make a comment.

20
01:27:22 --> 01:27:23
That's a bunch of updates.

21
01:27:23 --> 01:27:25
That comes and adds to your graph.

22
01:27:27 --> 01:27:31
Well, then ten minutes later,
you do something similar, and

23
01:27:31 --> 01:27:33
that also comes and adds to the graph.

24
01:27:33 --> 01:27:36
Then your friend does the same thing,
it adds to your graph.

25
01:27:36 --> 01:27:41
So as time goes by,
you are sending more edges to your graph.

26
01:27:43 --> 01:27:47
And the speed at which
you are doing this for

27
01:27:47 --> 01:27:50
at least like Facebook can be really,
really high.

28
01:27:50 --> 01:27:53
So the rate of update in
Facebook is really high.

29
01:27:54 --> 01:28:01
This is what is called
streaming edges into graphs.

30
01:28:01 --> 01:28:07
And there can be multiple streams for
various reasons.

31
01:28:07 --> 01:28:08
What do we mean by variety?

32
01:28:08 --> 01:28:16
For graphs, it means that the graph is
collecting data from various places.

33
01:28:16 --> 01:28:21
And all these different places are giving
different kinds of information to

34
01:28:21 --> 01:28:21
the graph.

35
01:28:21 --> 01:28:24
So in the end,
the graph has more non-uniform and

36
01:28:24 --> 01:28:29
complex information potentially
coming from multiple sources.

37
01:28:29 --> 01:28:32
That's what we mean by variety
when we refer to graphs.

38
01:28:33 --> 01:28:36
That picture there, by the way, is
different kinds of protein interactions.

39
01:28:40 --> 01:28:43
The next one,
the less-known one is valence.

40
01:28:45 --> 01:28:50
Now, if you remember your chemistry,
this comes from valence electrons,

41
01:28:50 --> 01:28:54
which are electrons in an atom
which are used for bonding.

42
01:28:54 --> 01:28:57
The other electrons
are called core electrons.

43
01:28:57 --> 01:29:02
So the idea is if we increase
the valence of the graphs,

44
01:29:02 --> 01:29:06
you increase the connectiveness
of the graph.

45
01:29:06 --> 01:29:07
How, we will see.

46
01:29:10 --> 01:29:15
Now, graph size clearly impacts analytics.

47
01:29:15 --> 01:29:23
Why, a, it takes more space, but more
importantly, it increases the algorithmic

48
01:29:23 --> 01:29:28
complexity of any operation that
you want it to on the graph.

49
01:29:28 --> 01:29:33
Now, we'll see an example of that,
but what happens as

50
01:29:33 --> 01:29:38
a result is that the data-to-analysis
time becomes high.

51
01:29:38 --> 01:29:42
So I put in some data, and
I wanted to do this analysis.

52
01:29:42 --> 01:29:47
But there is so much data, that my
analysis takes way longer than it should.

53
01:29:48 --> 01:29:53
Let's give a simple example,
an example we have seen before.

54
01:29:53 --> 01:29:58
Remember, we had this little graph
from our biological example where we

55
01:29:58 --> 01:30:04
were asking, find a simple path between
Alzheimer's Disease and Colorectal Cancer.

56
01:30:04 --> 01:30:06
And in this case, the result is obvious.

57
01:30:10 --> 01:30:13
Now, let's pause and ask.

58
01:30:13 --> 01:30:17
There are two nodes that I mentioned,
in this case,

59
01:30:17 --> 01:30:21
my Colorectal Cancer and
Alzheimer's Disease nodes.

60
01:30:22 --> 01:30:27
And we are asking,
is there a simple path connecting them?

61
01:30:30 --> 01:30:32
This is called a decision problem.

62
01:30:32 --> 01:30:39
I give you a data, and I'm asking does
such a simple path exist or not exist?

63
01:30:39 --> 01:30:43
But this is actually a very
hard decision problem.

64
01:30:43 --> 01:30:47
And the computer scientists will tell you
that this is a very complicated problem

65
01:30:47 --> 01:30:50
because it has a very high complexity.

66
01:30:50 --> 01:30:52
Let's ask another question.

67
01:30:52 --> 01:30:55
Well, how many simple paths,
now I want to count.

68
01:30:55 --> 01:30:57
How many simple paths exist
between these two nodes?

69
01:30:58 --> 01:31:01
Indeed, it is another
hard computing problem.

70
01:31:03 --> 01:31:07
And if you really want to know,
the size of the result,

71
01:31:07 --> 01:31:12
in the worst case is exponential
in the number of nodes.

72
01:31:12 --> 01:31:15
So if we increase the number of nodes and
edges, if we increase the size of

73
01:31:15 --> 01:31:23
the graph such a seemingly simple question
can take a very, very, very long time.

74
01:31:25 --> 01:31:28
So that it's almost practically
impossible to compute it for

75
01:31:28 --> 01:31:32
a really large graph if we have
no other information supporting.

76
01:31:32 --> 01:31:35
That's the worst case.

77
01:31:35 --> 01:31:40
But when we say algorithmic complexity
increases, that's what we mean.

78
01:31:43 --> 01:31:48
Let's talk about velocity, and
I said our favorite example is Facebook.

79
01:31:48 --> 01:31:53
So we are adding a bunch of updates, which
means we are adding a bunch of edges.

80
01:31:53 --> 01:31:59
We are streaming the edges into the data,
and we want to compute a metric.

81
01:31:59 --> 01:32:03
We want to see what is
the shortest distance

82
01:32:03 --> 01:32:08
between person a and
person b or item a and item b.

83
01:32:08 --> 01:32:13
Or I want to know that
Facebook has communities.

84
01:32:13 --> 01:32:14
Twitter has communities like we saw.

85
01:32:16 --> 01:32:20
And how many people out there,
in these communities, and

86
01:32:20 --> 01:32:23
how many such communities are there,
like a Facebook group?

87
01:32:25 --> 01:32:30
Now, if you want to compute this metric,
and you get this

88
01:32:30 --> 01:32:34
edges very fast, it is very difficult
to know when you have the answer.

89
01:32:35 --> 01:32:43
Because you are going to get an increasing
number of edges in the system,

90
01:32:44 --> 01:32:49
and you keep computing this metric that
you want to find the answer for, and

91
01:32:49 --> 01:32:55
it will turn out that your continuous
stream does not fit in memory.

92
01:32:55 --> 01:32:59
Because your memory is limited
compared to the amount of

93
01:33:01 --> 01:33:05
edges, or edge updates you
are streaming into the system.

94
01:33:07 --> 01:33:11
So that's what's happened when you
have high velocity information.

95
01:33:11 --> 01:33:16
Very soon, your memory runs out,
and you want to compute

96
01:33:16 --> 01:33:21
your answer right now from
the data that you have.

97
01:33:24 --> 01:33:27
Okay, let's look at variety,
also known as heterogeneity.

98
01:33:29 --> 01:33:32
There are two aspects of heterogeneity.

99
01:33:32 --> 01:33:38
One, we have already mentioned, graph data
is often created through integration,

100
01:33:38 --> 01:33:41
like we saw in the case of the biology.

101
01:33:44 --> 01:33:50
And therefore, the variety comes because
the nature of data is very different.

102
01:33:52 --> 01:33:55
Also, they may not be all
the same kind of data.

103
01:33:56 --> 01:34:01
For example, the data may come
from a relational database,

104
01:34:02 --> 01:34:04
it may come from an XML database.

105
01:34:04 --> 01:34:06
It may come from another graph.

106
01:34:06 --> 01:34:08
It may come from a document.

107
01:34:09 --> 01:34:14
It may even come from complex
things like social networks,

108
01:34:14 --> 01:34:19
like citation networks between papers or
patents, between interaction networks,

109
01:34:21 --> 01:34:24
between web entities,
which are connected through links.

110
01:34:25 --> 01:34:31
And from human knowledge that has been
represented as graphs through ontologists.

111
01:34:31 --> 01:34:37
So all of these graphs, the nodes and
the edges do not mean the same thing.

112
01:34:37 --> 01:34:42
And somehow in there,
you need to capture what it means to have

113
01:34:42 --> 01:34:46
an edge because that will determine
what you can do with the edge.

114
01:34:46 --> 01:34:51
A simple example, in an ontology,

115
01:34:51 --> 01:34:58
is something that says a is a b,
and b is a c, so a is a c.

116
01:34:59 --> 01:35:06
The a is a c is an inference that you do,
given the other two relationships.

117
01:35:06 --> 01:35:09
What would be an example?

118
01:35:09 --> 01:35:15
My pet is a dog, and the dog is a mammal,
therefore, my pet is a mammal.

119
01:35:16 --> 01:35:20
You want to do inferences for
some edges likes is a.

120
01:35:21 --> 01:35:24
Now, you need to know this.

121
01:35:24 --> 01:35:28
You do not do this with the biology
example where you are looking at genes and

122
01:35:28 --> 01:35:33
proteins because that operation does
not make sense when you have genes and

123
01:35:33 --> 01:35:34
proteins.

124
01:35:34 --> 01:35:38
So therefore, every graph may
have a different semantics.

125
01:35:38 --> 01:35:41
And what happens with variety is
the number of sub-semantics and

126
01:35:41 --> 01:35:44
the number of valid
operations that you can do.

127
01:35:44 --> 01:35:46
That changes, and
that becomes more complex.

128
01:35:48 --> 01:35:52
Now, valence I said,
is about connectedness.

129
01:35:52 --> 01:35:55
It is also about
interdependency among data.

130
01:35:55 --> 01:36:00
So if I have a higher valence which means,
I have more data elements that

131
01:36:00 --> 01:36:06
are more strongly related, and
these relationships can be exploited.

132
01:36:08 --> 01:36:13
In most cases,
the part where valence becomes important,

133
01:36:13 --> 01:36:19
is that it increases over time, which
means, parts of the graph becomes denser,

134
01:36:19 --> 01:36:24
and the average distance
between node pairs decreases.

135
01:36:24 --> 01:36:27
Let me show you, here is my Gmail.

136
01:36:29 --> 01:36:37
And I have plotted my Gmail graphs
from 2006 to about two months back.

137
01:36:38 --> 01:36:42
When I first started using it,
I had these users,

138
01:36:42 --> 01:36:47
a very few users, and
they are not really related.

139
01:36:48 --> 01:36:50
Now with time, more and

140
01:36:50 --> 01:36:54
more people started communicating
with me through Gmail.

141
01:36:55 --> 01:36:59
And more and more of these people were
also talking amongst themselves and

142
01:36:59 --> 01:37:01
copying me and responding to me.

143
01:37:02 --> 01:37:08
By the end, you would see that
you can find dense groups

144
01:37:08 --> 01:37:12
within my Gmail because
the information and

145
01:37:12 --> 01:37:19
the connectedness between people have
evolved and become more dense over time.

146
01:37:19 --> 01:37:24
This is the phenomenon of valence, and
this is very important to study because

147
01:37:24 --> 01:37:31
you want to study things like, what parts
of the graph have become more dense?

148
01:37:31 --> 01:37:32
And why have they become more dense?

149
01:37:32 --> 01:37:34
Maybe something was going on.

150
01:37:34 --> 01:37:37
Maybe there was an event that
brought these people together,

151
01:37:37 --> 01:37:40
and you want to analyze that and
find that out from your graph analytics.

152
01:37:42 --> 01:37:47
That's why you want to understand
the effect of valence.

153
01:37:47 --> 01:37:52
You also want to understand what do I do
if the graph becomes very, very dense

154
01:37:52 --> 01:37:57
in a place, so that finding a path through
that dense space becomes very hard.

155
01:37:59 --> 01:38:05
You will see in a later video that when
this happens, the computer system,

156
01:38:05 --> 01:38:11
that is trying to process these graphs in
a parallel and distributed way has to do

157
01:38:11 --> 01:38:17
something special to handle these
increasing density in parts of the graph.

1
03:04:06 --> 03:04:11
So before we go into graph analytics,
the first question we want to ask is,

2
03:04:11 --> 03:04:12
what is a graph?

3
03:04:12 --> 03:04:15
Let's see.

4
03:04:15 --> 03:04:20
This is a plot of the sales of
some items against time and

5
03:04:20 --> 03:04:25
it gives you a nice visual representation
of the data, but is it a graph?

6
03:04:26 --> 03:04:28
While you think about it,
let's try another one.

7
03:04:30 --> 03:04:34
This is another common visual
representation of data.

8
03:04:34 --> 03:04:36
If you go to Google and type pie graph,

9
03:04:36 --> 03:04:39
you will see many results
that look like this.

10
03:04:39 --> 03:04:40
But is this a graph?

11
03:04:41 --> 03:04:43
In our research, this is not a graph.

12
03:04:45 --> 03:04:46
We call it a chart.

13
03:04:48 --> 03:04:51
Yes, you guessed it right this time.

14
03:04:51 --> 03:04:53
This too is a chart, and not a graph.

15
03:04:54 --> 03:04:57
So why do people call them graphs, then?

16
03:04:58 --> 03:05:01
In a sense, they're abbreviating.

17
03:05:01 --> 03:05:04
A chart depicts what's called
a graph of a function.

18
03:05:05 --> 03:05:06
Let me explain.

19
03:05:07 --> 03:05:08
Look at the two column table on the right.

20
03:05:10 --> 03:05:13
The first column has information
about product category

21
03:05:13 --> 03:05:15
with values like furniture,
office supplies, and technology.

22
03:05:17 --> 03:05:22
The second column represents another set
of values called total containing 1724,

23
03:05:22 --> 03:05:26
4610, and 2065.

24
03:05:26 --> 03:05:30
Now we can define our mapping option.

25
03:05:30 --> 03:05:34
Which means a correspondence
from Product Category to Total.

26
03:05:35 --> 03:05:39
So we map Furniture to the value 1,724,

27
03:05:39 --> 03:05:42
Office Supplies to the value 4,610,
and so forth.

28
03:05:42 --> 03:05:44
If we visually portray this,

29
03:05:44 --> 03:05:47
we can represent it like a bar chart or
a pie chart.

30
03:05:47 --> 03:05:52
This is why both the previous diagram and
this diagram are charts and not graphs.

31
03:05:55 --> 03:05:59
Graph analytics has its basis in a brand
of mathematics called graph theory.

32
03:05:59 --> 03:06:00
What's more interesting,

33
03:06:00 --> 03:06:05
however, is that graph theory was
born out of a very practical problem.

34
03:06:07 --> 03:06:11
The problem started in a very old city
in Prussia which is now in Russia

35
03:06:12 --> 03:06:13
called Konigsberg.

36
03:06:15 --> 03:06:20
Even if we look at in in Google Maps
it would sort of look like this.

37
03:06:20 --> 03:06:25
One of the interesting features of
Konigsberg is that it has two islands and

38
03:06:25 --> 03:06:28
these islands are connected
by seven bridges.

39
03:06:28 --> 03:06:34
Back in 1736, the city wanted to create
a walkway, and the criteria was,

40
03:06:34 --> 03:06:40
this walkway would traverse all
seven bridges such that somebody

41
03:06:40 --> 03:06:46
wanting to go from one part of the city
to another can cross a bridge only once.

42
03:06:46 --> 03:06:51
Now this is sort of an urban
planning problem right?

43
03:06:51 --> 03:06:54
Well in fact, it required
a mathematician to solve the problem.

44
03:06:55 --> 03:06:59
The mathematician named Euler shown
on the left looked at this and

45
03:06:59 --> 03:07:03
figured out that you really
cannot create such a walk.

46
03:07:03 --> 03:07:04
Why?

47
03:07:04 --> 03:07:08
He said, it cannot be done, because
there are an odd number of bridges, and

48
03:07:08 --> 03:07:09
proved it mathematically.

49
03:07:09 --> 03:07:13
From this problem, the whole field
called graph theory emerged.

50
03:07:14 --> 03:07:19
On the right, you can see Edsger Dijkstra,
a well-known computer scientist who has

51
03:07:19 --> 03:07:24
developed graph algorithms, one of which
we will study later in the course.

52
03:07:24 --> 03:07:29
His work has had far further impact on
both the theoretical computer science and

53
03:07:29 --> 03:07:30
practical applications.

54
03:07:31 --> 03:07:34
What's the difference between
the mathematics view and

55
03:07:34 --> 03:07:35
the computer science view?

56
03:07:36 --> 03:07:38
Let's try to define
the mathematical view of graphs.

57
03:07:39 --> 03:07:41
We start with a set of vertices.

58
03:07:41 --> 03:07:46
Here we have a set of six nodes or
vertices.

59
03:07:46 --> 03:07:49
Now, I'll add another set.

60
03:07:49 --> 03:07:50
I'll call this the set of edges.

61
03:07:52 --> 03:07:54
In our diagram there are only four edges,
but

62
03:07:54 --> 03:07:56
there's something special
about these edges.

63
03:07:57 --> 03:08:02
Each edge is just not
an ordinary atomic object.

64
03:08:02 --> 03:08:08
An edge like e1, actually, is one term
from v and then a second term from v.

65
03:08:09 --> 03:08:13
So this edge, e1, goes from v1 to v5.

66
03:08:14 --> 03:08:17
Pictorially, we can draw and
arrow from v1 to v5.

67
03:08:18 --> 03:08:22
Now if I had said v5 to v1,
the arrow would be reversed.

68
03:08:23 --> 03:08:24
So what do we have so far?

69
03:08:24 --> 03:08:28
We have a set of vertices and
a set of edges.

70
03:08:28 --> 03:08:30
That is the mathematical
definition of a graph.

71
03:08:32 --> 03:08:35
What about the computer
scientist's definition?

72
03:08:36 --> 03:08:40
Of course a computer scientist needs to
adhere to the mathematical definition, but

73
03:08:40 --> 03:08:44
they want to represent and
manipulate the same information.

74
03:08:44 --> 03:08:46
So they need a data structure.

75
03:08:46 --> 03:08:49
In other words,
something they can operate on.

76
03:08:49 --> 03:08:50
So what kind of operations would they do?

77
03:08:51 --> 03:08:52
Well, with a graph,

78
03:08:52 --> 03:08:57
they can say add edge, or add a new
vertex, find the nearest neighbors of

79
03:08:57 --> 03:09:01
the vertex where the term neighbor refers
to the nodes connected to the vertex.

80
03:09:03 --> 03:09:07
We said a computer scientist needs to
represent graphs using data structures.

81
03:09:07 --> 03:09:09
Here is one that you should recognize.

82
03:09:09 --> 03:09:12
It's a matrix,
called the adjacency matrix of a graph.

83
03:09:13 --> 03:09:16
Both the rows and
columns of this matrix represent notes.

84
03:09:18 --> 03:09:23
If I go from v1 one along the row,
I see that there's one at v3,

85
03:09:23 --> 03:09:27
which means there is
an edge from v1 to v3.

86
03:09:27 --> 03:09:32
Similarly there is another from v1 to v5.

87
03:09:32 --> 03:09:34
Let's look at some operation.

88
03:09:34 --> 03:09:38
Let's say I first want to
add an edge from v3 to v2.

89
03:09:38 --> 03:09:40
What should I do?

90
03:09:40 --> 03:09:43
I'll start from the row v3,

91
03:09:43 --> 03:09:49
go up to the column v2,
and add a 1 in that set.

92
03:09:49 --> 03:09:53
So I've added an edge,
which is an operation on the matrix.

93
03:09:54 --> 03:09:55
Another operation.

94
03:09:57 --> 03:09:58
I want to get the neighbors a v3.

95
03:09:59 --> 03:10:01
This will be a little more complicated.

96
03:10:03 --> 03:10:06
I look at the row v3 and
paint that row and

97
03:10:06 --> 03:10:08
I'll also look at the column v3 and
paint that column.

98
03:10:10 --> 03:10:16
If we go down the row of v3,
we'll get v2, so v2 is neighbor.

99
03:10:16 --> 03:10:21
And if we go down the column of v3,
we'll get v1 and v6.

100
03:10:22 --> 03:10:25
So v1 and v6 have alternators.

101
03:10:25 --> 03:10:26
What's the difference?

102
03:10:29 --> 03:10:35
Since the matrix has the From along
the rows and the Tos along the columns,

103
03:10:35 --> 03:10:38
following the row v3 gives us
the edges outgoing from v3.

104
03:10:39 --> 03:10:43
And following the column v3 gives
us the edges coming into v3.

105
03:10:46 --> 03:10:49
Is this the only
representation of the graph?

106
03:10:49 --> 03:10:50
No it's not.

107
03:10:50 --> 03:10:51
Here is another one,

108
03:10:52 --> 03:10:56
in this representation there
are two kinds of data objects.

109
03:10:56 --> 03:11:00
No data, which are the blue rectangles and
edge data which are the triangles.

110
03:11:02 --> 03:11:06
To get a sense of this representation,
look at the note v1 and

111
03:11:06 --> 03:11:08
follow the top yellow line.

112
03:11:08 --> 03:11:10
It'll reach v3.

113
03:11:10 --> 03:11:15
So this link structure directly captures
the graph diagram we created before.

114
03:11:15 --> 03:11:20
Now, start from v1 again and
follow the blue link and

115
03:11:20 --> 03:11:23
it will reach e1 and then v3.

116
03:11:23 --> 03:11:27
So that tells you that e1 is
an edge object between v1 and v3.

117
03:11:29 --> 03:11:35
Next, let's stand on the edge triangle e1,
and follow the red

118
03:11:35 --> 03:11:40
dashed line to get to e2, which is
the next edge from the same starting node.

119
03:11:41 --> 03:11:43
Is this possibly too complex?

120
03:11:43 --> 03:11:45
Yes it is.

121
03:11:45 --> 03:11:50
However, as we'll see down the road, many
graph database systems are using this kind

122
03:11:50 --> 03:11:55
of data structure internally, so that the
database operations become more efficient.

1
06:16:00 --> 06:16:03
In the first example
of graph analytics for

2
06:16:03 --> 06:16:06
big data, we'll consider
the social media called Twitter.

3
06:16:07 --> 06:16:09
And graphs representing tweets.

4
06:16:11 --> 06:16:14
What types of objects and
relationships does a tweet have?

5
06:16:14 --> 06:16:19
Well, in fact, tweets consist
of many of the same elements and

6
06:16:19 --> 06:16:20
relationships as the Facebook posts.

7
06:16:22 --> 06:16:26
Tweets have users, they contain text,

8
06:16:26 --> 06:16:31
tweets can point to one another or
to URL's, they have hashtags, they can

9
06:16:31 --> 06:16:36
include reference to various other types
of media, but what about relationships?

10
06:16:38 --> 06:16:42
Well much like Facebook, relationships and

11
06:16:42 --> 06:16:46
tweets are more reflective of
what the users do with tweets.

12
06:16:46 --> 06:16:50
For example, people create tweets,
they respond to tweets,

13
06:16:50 --> 06:16:52
they mention other users, and so forth.

14
06:16:53 --> 06:16:55
Let's look at these
relationships in detail.

15
06:16:57 --> 06:17:02
In this graph, all of the light
blue notes are tweets, and

16
06:17:02 --> 06:17:05
all of the purple notes are users.

17
06:17:05 --> 06:17:07
When a specific user
creates a specific tweet,

18
06:17:07 --> 06:17:10
an entity is created in
the graph as a result.

19
06:17:11 --> 06:17:13
We'll come back to this graph
in the next few slides.

20
06:17:14 --> 06:17:17
So what we want to do with it?

21
06:17:17 --> 06:17:19
Well to be very specific,

22
06:17:19 --> 06:17:23
this graph was created from tweets
of a bunch of online gamers.

23
06:17:24 --> 06:17:28
In this case, we were monitoring
the tweets of all people

24
06:17:28 --> 06:17:30
who played particular online video game.

25
06:17:31 --> 06:17:34
Typically, gamers are very
excited about their video games.

26
06:17:34 --> 06:17:37
They're exited about their characters
of the video games and they want to

27
06:17:37 --> 06:17:41
discuss the video games they're excited
about, when new versions are released.

28
06:17:43 --> 06:17:46
So what are the data
science questions here?

29
06:17:46 --> 06:17:50
People who look at these things, such as
behavioral psychologists, want to know.

30
06:17:50 --> 06:17:56
If you are a war game user or
if you play any other violent game, where

31
06:17:56 --> 06:18:01
there is a lot of fighting online, do
these people also show violent behavior?

32
06:18:01 --> 06:18:03
Maybe they do, maybe they don't.

33
06:18:04 --> 06:18:08
They also want to know whether as a result
of looking at somebody's tweets and

34
06:18:08 --> 06:18:13
following them over time they can tell if
the person is addicted to the game or not.

35
06:18:13 --> 06:18:15
So why use graphs for that?

36
06:18:16 --> 06:18:19
One of the things you could
do is what we showed here,

37
06:18:20 --> 06:18:25
from the graphs you can extract
certain elements like conversations.

38
06:18:27 --> 06:18:32
If you look at it, you will see all of
them have somebody posting something.

39
06:18:32 --> 06:18:37
Somebody responding to the post, and the
first person responding to the response.

40
06:18:37 --> 06:18:40
They are retweeting, and
they are responding, and so forth.

41
06:18:40 --> 06:18:44
You would see a conversation chain
going on which can be short or long,

42
06:18:44 --> 06:18:47
like you see, and
other people would join in.

43
06:18:47 --> 06:18:50
We don't know if it's violent or not, but

44
06:18:50 --> 06:18:52
at least there is a lively
conversation going on about something.

45
06:18:54 --> 06:18:59
Using graphs, we could find some
meaningful parts of a graph

46
06:18:59 --> 06:19:03
that we can further analyze using
techniques like text analytics.

47
06:19:05 --> 06:19:07
Another kind of thing people
would like to identify,

48
06:19:07 --> 06:19:11
is which players are interacting
with which other players?

49
06:19:11 --> 06:19:12
Who are these people?

50
06:19:12 --> 06:19:14
Do they form a close community?

51
06:19:14 --> 06:19:16
Can anybody join in?

52
06:19:16 --> 06:19:19
Are there groups, what are the groups?

53
06:19:19 --> 06:19:21
If you find a group,
who are the most influential users?

54
06:19:23 --> 06:19:27
Who are the people that everybody
refers to, listens to and so on?

55
06:19:27 --> 06:19:31
Graph analytics is used to answer all
these questions about a conversation

56
06:19:31 --> 06:19:35
that's going on live in
social media stream.

57
06:19:35 --> 06:19:37
Let's look at our next example.

1
12:35:38 --> 12:35:40
Our second use case is about biology.

2
12:35:41 --> 12:35:43
Interactions arise naturally in biology.

3
12:35:45 --> 12:35:50
Genes produce proteins, proteins regulate
the functions of other proteins,

4
12:35:51 --> 12:35:53
cells transmit signals to other cells.

5
12:35:55 --> 12:35:58
These functional genes lead to
pathological conditions that

6
12:35:58 --> 12:36:01
present themselves as
observable phenotypes.

7
12:36:01 --> 12:36:05
Some of these interactions that are
observed and measured by experiment lists.

8
12:36:07 --> 12:36:10
Another class of graph relations
represent human knowledge.

9
12:36:10 --> 12:36:15
For example, an edge may denote
an anatomical association, such as

10
12:36:15 --> 12:36:19
the nucleuses inside a cell, part of
the cerebral cortex is part of the brain.

11
12:36:19 --> 12:36:24
Similarly, one can encode different

12
12:36:24 --> 12:36:28
classifications of entities like
both humans and dogs and mammals.

13
12:36:30 --> 12:36:35
Researchers also find these
relationships from literature

14
12:36:35 --> 12:36:39
or by computational techniques
like bioinformatics algorithms.

15
12:36:40 --> 12:36:45
There are many bioinformatics algorithms
that find statistical correlations between

16
12:36:45 --> 12:36:45
genes and proteins.

17
12:36:46 --> 12:36:52
Many biological networks are created by
creating edges between entities because

18
12:36:52 --> 12:36:56
they are strongly correlated based on
measurements like gene expression level.

19
12:36:58 --> 12:37:02
Yet, a third category of
computed relationships

20
12:37:02 --> 12:37:06
associates terms by mining
scientific literature.

21
12:37:06 --> 12:37:10
If two entities are co-mentioned
in scientific articles very often,

22
12:37:10 --> 12:37:14
then there is a likelihood that
these two entities are related.

23
12:37:14 --> 12:37:18
In this case, the edge between them does
not depict a specific relationship, but

24
12:37:18 --> 12:37:20
the fact they are associated.

25
12:37:22 --> 12:37:28
All these interactions can be assembled
into networks of graphs where the nodes

26
12:37:28 --> 12:37:33
are biological entities and
edges represent different categories of

27
12:37:33 --> 12:37:39
molecular interactions,
associations between diseases and.

28
12:37:39 --> 12:37:42
Now there are two issues I
would like you to understand.

29
12:37:42 --> 12:37:48
First, graphs like the one
we saw are assembled.

30
12:37:48 --> 12:37:54
That is, integrated from many data sources
produced by many independent science

31
12:37:54 --> 12:38:00
groups who have different research goals,
use different scientific techniques,

32
12:38:00 --> 12:38:05
but the underlying biological entities
are common across many groups.

33
12:38:06 --> 12:38:10
It is the commonality that helps us
stitch together these logic graphs.

34
12:38:10 --> 12:38:16
Second, as more results get linked and
integrated, the size of the networks grow,

35
12:38:16 --> 12:38:20
leading to big data problems and
the need for big data technology.

36
12:38:21 --> 12:38:24
Now let's ask the same question.

37
12:38:24 --> 12:38:24
Why graphs?

38
12:38:26 --> 12:38:31
In bio medicine people are always
trying to discover new science, for

39
12:38:31 --> 12:38:34
instance, they want to discover
unknown relationships.

40
12:38:34 --> 12:38:40
For example, they can take two diseases,
very different diseases, colorectal

41
12:38:40 --> 12:38:46
cancer and Alzheimers disease, and
they might want to ask are they related?

42
12:38:46 --> 12:38:48
If so, can you connect the dots and

43
12:38:48 --> 12:38:52
find out what the intervening
network between them is?

44
12:38:52 --> 12:38:56
It turns out, as you can see,
there are several genes that directly or

45
12:38:56 --> 12:38:58
indirectly connect these two diseases.

46
12:38:59 --> 12:39:03
Thus, we can use path finding techniques
like this to discover previously

47
12:39:03 --> 12:39:05
unknown connections in a network.

48
12:39:07 --> 12:39:08
In addition,

49
12:39:08 --> 12:39:13
researchers sometimes think to explore the
networks to help their discovery process.

50
12:39:13 --> 12:39:17
I have collaborated with a group that
is using graph exploration techniques

51
12:39:17 --> 12:39:21
to figure out how phenotypes of
undiagnosed diseases all fit together.

1
01:14:59 --> 01:15:03
Our third use case is about
the human information network.

2
01:15:03 --> 01:15:05
That means a personal information network.

3
01:15:06 --> 01:15:09
The graphic you see there
is my LinkedIn network.

4
01:15:11 --> 01:15:13
Everybody who is on my LinkedIn is a node.

5
01:15:14 --> 01:15:17
And if they know each other,
there's an edge between them.

6
01:15:17 --> 01:15:20
You can possibly see some groups or
clusters there.

7
01:15:20 --> 01:15:24
One analytical question
we'll explore in module two

8
01:15:24 --> 01:15:26
is how to discover these
groups automatically.

9
01:15:28 --> 01:15:32
Now of course with LinkedIn we only
have professional information.

10
01:15:32 --> 01:15:35
But it is prudent to ask whether

11
01:15:35 --> 01:15:40
one can integrate the professional network
with other forms of personal information.

12
01:15:40 --> 01:15:45
This would be other social media data like
my friendship network from Facebook or

13
01:15:45 --> 01:15:51
Google, or it could be information
like my Outlook email network.

14
01:15:51 --> 01:15:56
One can add even more information and
put that interpersonal relationships.

15
01:15:56 --> 01:16:01
For example, Professor Norman who is
on my contact list is my director.

16
01:16:01 --> 01:16:03
And if we add my calendar to this,

17
01:16:03 --> 01:16:09
one can find events that I'll be attending
and people that I will be meeting.

18
01:16:09 --> 01:16:12
Taking it even further, for

19
01:16:12 --> 01:16:17
special applications you might
actually include financial and

20
01:16:17 --> 01:16:22
business transactions, or
performance in activities,

21
01:16:22 --> 01:16:27
or fitness information, or
the location from my GPS.

22
01:16:29 --> 01:16:34
Now, if you add all of these questions, we
should ask what we do with these things?

23
01:16:35 --> 01:16:35
Let's see.

24
01:16:37 --> 01:16:40
One use of this kind of
information is matching.

25
01:16:42 --> 01:16:46
For example, matching people with jobs
is not just a matter of screening

26
01:16:46 --> 01:16:50
series based on job descriptions or
evaluating their job experience.

27
01:16:51 --> 01:16:55
To recruit for high level positions,
like the board of directors of a company,

28
01:16:57 --> 01:17:01
you need to evaluate the network of the
candidate to determine their relationships

29
01:17:01 --> 01:17:03
with important organizations,
groups, and individuals.

30
01:17:05 --> 01:17:07
For some tasks like choosing a surgeon,

31
01:17:07 --> 01:17:10
you might also want inspect
their social media ratings.

32
01:17:12 --> 01:17:14
In other classic applications,
we might want to look for

33
01:17:14 --> 01:17:17
people who can influence a human network.

34
01:17:17 --> 01:17:20
Suppose, we are in an election
campaign team, and

35
01:17:20 --> 01:17:25
need our message to get to as many
people as possible in a city.

36
01:17:25 --> 01:17:30
We can not go door to door ourselves,
we need to go to some people, and

37
01:17:30 --> 01:17:33
these people will reach
others on our behalf.

38
01:17:34 --> 01:17:39
Graph analytic techniques may help
identify the fewest number of people

39
01:17:39 --> 01:17:43
who will have maximal reach into most
of the potential voters in the city.

40
01:17:45 --> 01:17:48
A third category of application
is threat detection.

41
01:17:50 --> 01:17:54
This network is created
by groups who study news.

42
01:17:54 --> 01:17:57
For example,
they collect information about all

43
01:17:57 --> 01:18:01
the militant groups in different countries
and all the reported acts of terrorism.

44
01:18:03 --> 01:18:05
This creates a network as you can see.

45
01:18:06 --> 01:18:08
We show only a part of the network.

46
01:18:08 --> 01:18:12
In this graph view,
the green notes are groups and

47
01:18:12 --> 01:18:15
the pink notes are deemed individuals.

48
01:18:16 --> 01:18:18
It should be clear that some
of these individuals and

49
01:18:18 --> 01:18:22
groups are more closely
associated than others.

50
01:18:23 --> 01:18:27
Discovering and tracking these groups,
their current activities,

51
01:18:27 --> 01:18:31
together with an analysis of details of
the events they're associated with might

52
01:18:31 --> 01:18:34
help analysts to get deeper
insight into the networks and

53
01:18:34 --> 01:18:37
perhaps anticipate their
future activities.

1
02:33:36 --> 02:33:40
Our fourth and
last use case has to do with smart cities.

2
02:33:41 --> 02:33:45
Now, a city is a geographically
bounded space, and

3
02:33:45 --> 02:33:50
contains many different networks
operating within the same spatial domain.

4
02:33:50 --> 02:33:52
What kind of networks?

5
02:33:52 --> 02:33:52
Let's see.

6
02:33:53 --> 02:33:55
It has transportation networks.

7
02:33:57 --> 02:34:02
Water and sewage networks,
power transmission networks,

8
02:34:02 --> 02:34:03
your IP broadband networks.

9
02:34:05 --> 02:34:08
Some of these networks have
multiple subtypes, for

10
02:34:08 --> 02:34:13
example transportation networks include
the bus networks, the subway network,

11
02:34:13 --> 02:34:16
the surface street network, and
the railway network, and so on.

12
02:34:17 --> 02:34:22
These networks form a physical
infrastructure and therefore

13
02:34:22 --> 02:34:26
can be represented as graphs where
each node has a geographic coordinate.

14
02:34:27 --> 02:34:32
But some of these networks can also be
thought of as a commodity flow network.

15
02:34:33 --> 02:34:36
People flow through
transportation networks.

16
02:34:36 --> 02:34:39
Sewage material flows through
a sewage network and so on.

17
02:34:41 --> 02:34:42
For many of these networks,

18
02:34:42 --> 02:34:47
a city planner would like to make
sure that they cover the entire city,

19
02:34:47 --> 02:34:51
that commute time is optimized,
traffic congestions are well planned.

20
02:34:51 --> 02:34:56
To accomplish this, they would need to
create what's called a network model.

21
02:34:56 --> 02:35:01
For example, urban planners develop
city traffic model servicing.

22
02:35:01 --> 02:35:04
A traffic model will use both
the geographic layout and

23
02:35:04 --> 02:35:07
connectivity of the network along
with the flow parameters like

24
02:35:07 --> 02:35:09
the number of commuters getting
on board at any station.

25
02:35:11 --> 02:35:15
Well, if we're planning
to create a smart hub,

26
02:35:15 --> 02:35:18
we need to make sure that all the right
things happen at the same place.

27
02:35:19 --> 02:35:23
People who come out of a metro
station find nearby businesses.

28
02:35:23 --> 02:35:25
They find nearby facilities.

29
02:35:25 --> 02:35:28
Those facilities should
have broadband network for

30
02:35:28 --> 02:35:30
people who are going to go
on their mobile phones.

31
02:35:30 --> 02:35:34
The same places need to have
a water supply network, and

32
02:35:34 --> 02:35:37
you have to plan it in such
a way that all these networks

33
02:35:37 --> 02:35:40
who exist within a certain
distance of each other.

34
02:35:40 --> 02:35:42
And all the facilities can
be planned accordingly.

35
02:35:44 --> 02:35:46
Beyond normal operations,

36
02:35:46 --> 02:35:50
we also need to model what would
happen if the network gets disrupted.

37
02:35:50 --> 02:35:52
What are the kinds of congestion or
traffic that might disturb the network?

38
02:35:53 --> 02:35:59
Therefore these graphs are no longer just
structures, but they represent things like

39
02:35:59 --> 02:36:03
congestions, things like people's behavior
and materials behavior over the network.

40
02:36:04 --> 02:36:07
One should also compute
energy use patterns for

41
02:36:07 --> 02:36:11
the busy parts of the network in order to
figure out how the network structure or

42
02:36:11 --> 02:36:15
the network flow can be altered to
enable energy optimal operations.

43
02:36:16 --> 02:36:19
As we saw in these four examples,

44
02:36:19 --> 02:36:22
what graphs are used for
are kind of different but

45
02:36:22 --> 02:36:27
they all show different view points from
which you can use graphs for analysis.

46
02:36:28 --> 02:36:32
So this course focuses on Graph Analytics.

47
02:36:32 --> 02:36:35
I would like to briefly
recap what the term means.

48
02:36:36 --> 02:36:41
Analytics is the ability to
discover meaningful patterns and

49
02:36:41 --> 02:36:47
interesting insights into data using
mathematical properties of data.

50
02:36:47 --> 02:36:51
It covers a process of computing
with mathematical properties and

51
02:36:51 --> 02:36:54
accessing the data itself efficiently.

52
02:36:54 --> 02:36:57
Further, it involves
the ability to represent and

53
02:36:57 --> 02:37:01
work with domain knowledge as we
saw in use case two with biology.

54
02:37:01 --> 02:37:05
Finally, analytics often involves
statistical modeling techniques for

55
02:37:05 --> 02:37:08
drawing inferences and
making predictions on data.

56
02:37:09 --> 02:37:14
With analytics, we should be able
to achieve the goals shown here.

57
02:37:30 --> 02:37:33
Therefore, Graph Analytics
is a special piece of

58
02:37:33 --> 02:37:37
analytics where the underlying data
can be modeled as a set of graphs.

1
05:11:13 --> 05:11:16
In the previous video,
we learned what a graph is.

2
05:11:17 --> 05:11:20
We gained insight into both
the mathematician's view and

3
05:11:20 --> 05:11:21
the computer scientist's view.

4
05:11:22 --> 05:11:26
Now we want to look at the big
picture of graph analytics.

5
05:11:27 --> 05:11:30
Why are we doing what we are doing, and

6
05:11:30 --> 05:11:33
why do I need a graph
representation on graph analytics?

7
05:11:34 --> 05:11:39
Let's start with a graph network which you
may already be familiar with, Facebook.

8
05:11:40 --> 05:11:42
Here is my Facebook page.

9
05:11:42 --> 05:11:45
Let's look at it and
consider some of the elements it contains.

10
05:11:48 --> 05:11:50
There is a primary user, and that's me.

11
05:11:52 --> 05:11:52
There are my friends.

12
05:11:54 --> 05:11:59
And typically there are some posts,
some of which may have media in them,

13
05:11:59 --> 05:12:00
such as video.

14
05:12:00 --> 05:12:02
What else does it have?

15
05:12:02 --> 05:12:05
Well, let's consider what's inside a post.

16
05:12:05 --> 05:12:08
The post contains text.

17
05:12:08 --> 05:12:10
It has tags.

18
05:12:10 --> 05:12:14
I tag some people,
some people comment on my post.

19
05:12:15 --> 05:12:17
If somebody's commenting,

20
05:12:17 --> 05:12:22
then that means there must be a commenter
who is also a user in the Facebook.

21
05:12:23 --> 05:12:27
Other people like my post or

22
05:12:27 --> 05:12:32
they like a comment and
they also respond to some of the comments.

23
05:12:33 --> 05:12:36
Many posts have locations
associated with them as well.

24
05:12:38 --> 05:12:41
When we consider all of these together,
do you see the graph?

25
05:12:41 --> 05:12:42
Well if not, we'll show you.

26
05:12:44 --> 05:12:48
Here, we see very much the same types of
information but now they're in a graph.

27
05:12:49 --> 05:12:52
First, notice me on the far left.

28
05:12:52 --> 05:12:56
If you look carefully at this graph you'll
also notice there are relations labeled

29
05:12:56 --> 05:13:01
Friend-of and Created_post and parts
of the post and everything that you saw

30
05:13:01 --> 05:13:07
before that are now organized in terms of
objects and relationship speaking objects.

31
05:13:08 --> 05:13:13
The objects here on the post, comments on
the post, replies to the comments and so

32
05:13:13 --> 05:13:17
forth, and the relationships also
include tagged in or refers to.

33
05:13:17 --> 05:13:20
So this is sort of the big picture, right?

34
05:13:22 --> 05:13:27
Next we will look at 4 specific
Use Cases in four different disciplines.

35
05:13:28 --> 05:13:30
The first one will be on Social Media.

36
05:13:32 --> 05:13:36
The second one will be on biology,
where we'll look at genes and diseases.

37
05:13:37 --> 05:13:42
The third one will be Human Information
Network, which means personal information.

38
05:13:43 --> 05:13:45
And the fourth one will
be on smart cities.

39
05:13:46 --> 05:13:48
Let's go through these
examples one by one.

1
10:25:01 --> 10:25:03
We will go over the logic of Dijkstra's
algorithm without writing code.

2
10:25:05 --> 10:25:06
If you are an advanced student and

3
10:25:06 --> 10:25:09
know the algorithm,
you can skip to the next lecture.

4
10:25:11 --> 10:25:14
The basic plan is to start from node I and

5
10:25:14 --> 10:25:17
progressively traverse
a sequence of notes.

6
10:25:18 --> 10:25:22
When the method attempts to choose
the next node to traverse to,

7
10:25:22 --> 10:25:27
it chooses a node for which the total rate
of the path to that node is the lowest.

8
10:25:29 --> 10:25:32
In the beginning,
the algorithm is on the start node I.

9
10:25:33 --> 10:25:36
The distance from I to I is 0.

10
10:25:36 --> 10:25:38
And the distance to all
other nodes is infinity,

11
10:25:38 --> 10:25:40
because the system doesn't know them yet.

12
10:25:42 --> 10:25:45
A second table, called a priority queue,

13
10:25:45 --> 10:25:51
is currently exactly the same as
the distance row of the first array.

14
10:25:52 --> 10:25:55
The system starts with processing node I,
the source node.

15
10:25:56 --> 10:26:01
That means it finds the nodes
that can reach from I, F and J.

16
10:26:02 --> 10:26:07
Note that the respective total weights,
that is 5 for F,

17
10:26:07 --> 10:26:11
and 15 for J,
to get to these nodes in the distance row.

18
10:26:11 --> 10:26:13
Then it marks I as done.

19
10:26:14 --> 10:26:18
We have made the node I agree,
because the node is processed.

20
10:26:20 --> 10:26:26
Next it looks at row d to find
the least distance, which is 5.

21
10:26:28 --> 10:26:30
The corresponding vertex is F.

22
10:26:31 --> 10:26:36
So next, the method traverses to F.

23
10:26:36 --> 10:26:41
Now the algorithm on node F and
has determined that out of its possible

24
10:26:41 --> 10:26:46
destinations, E, G, and J,
J is the least expensive.

25
10:26:46 --> 10:26:52
The total path, that is the weight
of the path to J, is 10.

26
10:26:52 --> 10:26:56
5 from I to F, plus 5 from F to J.

27
10:26:56 --> 10:27:00
This diagram shows that the priority
is now shorter because it has

28
10:27:00 --> 10:27:03
popped out the already processed load, I.

29
10:27:03 --> 10:27:08
At step three, we are processing node
J but face the following situation.

30
10:27:09 --> 10:27:12
We can go back to F from J, but

31
10:27:12 --> 10:27:17
that will cost 10 plus 15, that is 25.

32
10:27:17 --> 10:27:22
25 is worse than the cost of the current
path to F, which is 5 directly from I.

33
10:27:23 --> 10:27:28
Thus we do not go from F to J, and
do not update the distance shown.

34
10:27:28 --> 10:27:34
The other option is to go from J to G,
which incurs a cost of 10 plus 5, 15.

35
10:27:34 --> 10:27:39
Hm, this does not improve the current
cost to reach G through F,

36
10:27:39 --> 10:27:42
which is now at 15 already.

37
10:27:42 --> 10:27:46
Therefore, we do not
update the distance for G.

38
10:27:46 --> 10:27:50
So at this point,
we see that while J is processed,

39
10:27:50 --> 10:27:54
it had no impact on
the traversal process.

40
10:27:54 --> 10:27:58
We consider the distance row again, and
find that the next node to expand is G,

41
10:27:58 --> 10:28:00
which is reached through F.

42
10:28:01 --> 10:28:04
Continuing as before, G's processed.

43
10:28:04 --> 10:28:09
It opens up the possibility of
diverging to C, at a cost of 35.

44
10:28:09 --> 10:28:12
Or to D, at the cost of 25.

45
10:28:12 --> 10:28:17
But wait, we have an issue,
there are two competing nodes,

46
10:28:17 --> 10:28:23
E coming form F or D coming from G,
that are both expansion candidates.

47
10:28:23 --> 10:28:24
At this point,

48
10:28:24 --> 10:28:29
the algorithm can make a random choice
because there is no other information.

49
10:28:29 --> 10:28:32
Let's say we make an arbitrary choice,
we expand to E next.

50
10:28:33 --> 10:28:37
In an optional video, we will see

51
10:28:37 --> 10:28:42
how we can use the additional information
to make a more informed decision.

52
10:28:42 --> 10:28:43
After expanding to E,

53
10:28:43 --> 10:28:47
we can find that we have already
reached the node B, so we are done.

54
10:28:49 --> 10:28:54
The other choice, that is to go to D from
G, costs less than the path through B, but

55
10:28:54 --> 10:28:56
it doesn't matter now,
because the destination is reached.

56
10:28:57 --> 10:29:03
Just for the sake of completeness, if we
did let the algorithm continue to operate,

57
10:29:03 --> 10:29:06
it will terminate when all
reachable nodes are reached.

58
10:29:07 --> 10:29:11
We say all reachable nodes,
become some nodes like H,

59
10:29:12 --> 10:29:15
are not reachable because
it has no incoming edge.

60
10:29:16 --> 10:29:20
Such a node, as we said before,
is called the root node of the graph.

61
10:29:20 --> 10:29:23
In general,
a graph can have more than one root node.

62
10:29:25 --> 10:29:29
Now that we have reached our destination,
we need to construct the shortest path.

63
10:29:29 --> 10:29:35
We start by taking the destination B and
find its predecessor, E.

64
10:29:37 --> 10:29:43
Then we find the node E, and
check its predecessor, which is F.

65
10:29:43 --> 10:29:46
Finally, we find the predecessor
of F to obtain I,

66
10:29:46 --> 10:29:48
which is a source node for the task.

67
10:29:49 --> 10:29:52
So these nodes can then be
stretched together in reverse,

68
10:29:52 --> 10:29:57
thus building I to F, to E, to B,
which is highlighted in the film.

69
10:29:58 --> 10:30:03
So, how well does this algorithm work for
big graphs?

70
10:30:03 --> 10:30:06
Actually, not very well.

71
10:30:06 --> 10:30:08
We often measure
the performance of an algorithm

72
10:30:08 --> 10:30:10
in terms of the size of the data.

1
20:55:10 --> 20:55:15
So, we saw that the Dijkstra Algorithm
has a very high worst case complexity.

2
20:55:16 --> 20:55:19
Despite the high complexity of
the algorithm, there are several

3
20:55:19 --> 20:55:23
practical improvements that will
enhance the performance of the method.

4
20:55:23 --> 20:55:27
One of them is
Bi-directional Dijkstra Algorithm.

5
20:55:27 --> 20:55:31
The idea is very simple,
we will go forward from the source now and

6
20:55:31 --> 20:55:36
backward from the target node and stop
when the two expanding frontiers meet.

7
20:55:36 --> 20:55:39
We will briefly illustrate
the process without going deep into

8
20:55:39 --> 20:55:41
the details of every step.

9
20:55:41 --> 20:55:44
So the technique starts just
like the regular method.

10
20:55:44 --> 20:55:48
The control moves from
I to F at a cost of 5.

11
20:55:48 --> 20:55:50
But then, it switches and

12
20:55:50 --> 20:55:55
starts from the target, and
moves backward along the edges.

13
20:55:55 --> 20:55:57
So from A, it comes to D or C.

14
20:55:58 --> 20:56:03
We'll chose D,
because an AD is a least weight part.

15
20:56:03 --> 20:56:09
Now the forward step is performed again,
and we traverse from F to G.

16
20:56:09 --> 20:56:13
We are skipping the expansion
to J because as we saw before,

17
20:56:13 --> 20:56:15
it does not contribute to the path.

18
20:56:15 --> 20:56:18
So the total rate of the IFG path is 15.

19
20:56:18 --> 20:56:23
The backward process
then reaches G through D.

20
20:56:23 --> 20:56:26
The cost of the IFG is 15.

21
20:56:26 --> 20:56:28
And the cost of ADG is also 15.

22
20:56:28 --> 20:56:32
We stop because a common node is reached.

23
20:56:32 --> 20:56:35
We need to ensure that the weight
of the forward path and

24
20:56:35 --> 20:56:40
that of the reverse path are added, and
the total combined rate is minimal.

25
20:56:40 --> 20:56:44
At this point,
we can concatenate the partial paths

26
20:56:44 --> 20:56:48
to construct the full shortest path,
which is IFGDA.

27
20:56:48 --> 20:56:53
Now one point to remember is that
the length of the smallest weight path

28
20:56:53 --> 20:56:56
can be longer than the shortest hop path.

29
20:56:56 --> 20:57:02
Here the FCA path has 2 hops but
a weight of 20.

30
20:57:02 --> 20:57:07
But the weight of the 3 hop path,
FGDA, is 15.

31
20:57:07 --> 20:57:09
So just remember that.

1
17:52:19 --> 17:52:22
The final type of analytics
we'll discuss in this module

2
17:52:22 --> 17:52:24
is called Centrality Analysis.

3
17:52:25 --> 17:52:31
We call something central if it's
in the middle of a larger entity.

4
17:52:31 --> 17:52:35
We also call something central
if it's important and vital.

5
17:52:36 --> 17:52:42
For graphs, we'll identify important
notes by looking at how central they are.

6
17:52:42 --> 17:52:43
In the graph.

7
17:52:46 --> 17:52:50
Look at the the six node graph,
you don't have to know a lot

8
17:52:50 --> 17:52:55
to figure out that the orange
node is pretty important, Why?

9
17:52:55 --> 17:52:58
Well, one reason could be it's
the most vulnerable node.

10
17:52:58 --> 17:53:00
If you remove it the graph
will fall apart.

11
17:53:00 --> 17:53:03
because this is clearly one way to
look at the centrality of the node.

12
17:53:04 --> 17:53:05
But it's not the only way.

13
17:53:07 --> 17:53:09
Another way of looking at the orange node,

14
17:53:09 --> 17:53:12
is that it can reach all other
nodes quicker, than any other node.

15
17:53:14 --> 17:53:17
So this is the idea behind influences.

16
17:53:17 --> 17:53:21
People in a social network who
are connected enough to reach out and

17
17:53:21 --> 17:53:25
possibly influence a lot more
people than others will be able to.

18
17:53:27 --> 17:53:29
If the little graph represents
a transportation network.

19
17:53:30 --> 17:53:34
And you were asked to build a restaurant
somewhere around this network.

20
17:53:34 --> 17:53:37
You'll possibly choose an area
near the central node.

21
17:53:37 --> 17:53:42
Because more traffic will flow
through this node than other nodes.

22
17:53:42 --> 17:53:44
And some of them will get
out at the station and

23
17:53:44 --> 17:53:46
bring business to your restaurant.

24
17:53:47 --> 17:53:49
We can give many more examples.

25
17:53:49 --> 17:53:53
In biological networks,
house-keeping genes are genes needed for

26
17:53:53 --> 17:53:57
the maintenance of basic
cellular function, and

27
17:53:57 --> 17:54:01
are expressed in all cells of
an organism under normal and

28
17:54:01 --> 17:54:05
abnormal conditions, these genes
are central because they're vital and

29
17:54:05 --> 17:54:08
they're connected to many other
nodes in the biological network.

30
17:54:09 --> 17:54:13
So much so that they need to
be taken out of the network so

31
17:54:13 --> 17:54:15
that the rest of
the network can be studied.

32
17:54:15 --> 17:54:20
Out of all these examples,
we'll focus on a kind of problem

33
17:54:20 --> 17:54:23
researchers have called
the key player problem.

34
17:54:24 --> 17:54:26
The problem comes in two flavors.

35
17:54:27 --> 17:54:30
Now take a little time to
read the two examples.

36
17:54:42 --> 17:54:44
The first is sort of a negative piece.

37
17:54:46 --> 17:54:50
We have a network and we are trying
to find a small subset of people

38
17:54:50 --> 17:54:55
who's removal will maximally
disrupt the network.

39
17:54:56 --> 17:54:59
The key operative word here is maximally.

40
17:54:59 --> 17:55:05
So if there is a node which removal
breaks the network into two parts,

41
17:55:05 --> 17:55:09
it is not what we want if
there's another two nodes

42
17:55:09 --> 17:55:12
whose removal will break
the network into ten parts.

43
17:55:12 --> 17:55:17
The second case, however,
is a lot more conventional.

44
17:55:17 --> 17:55:22
The goal is to find a small set of nodes
with maximal combined reachability

45
17:55:22 --> 17:55:23
to other nodes.

46
17:55:23 --> 17:55:28
That means, taken together, these nodes
should reach almost all other nodes.

47
17:55:30 --> 17:55:33
You should know that
people use two terms to

48
17:55:33 --> 17:55:35
characterize a general concept
of network centrality.

49
17:55:37 --> 17:55:39
The first one, centrality,
is about a node.

50
17:55:41 --> 17:55:45
It measures how central the node
is with respect to the network.

51
17:55:46 --> 17:55:50
So the orange node in the left
graph has high centrality, and

52
17:55:50 --> 17:55:51
the blue nodes have low centrality.

53
17:55:53 --> 17:55:58
The second term, centralization,
is about the network.

54
17:55:58 --> 17:56:00
Now look at the graph on the right,

55
17:56:00 --> 17:56:05
where the dark orange node is still very
central for the light orange node, and

56
17:56:05 --> 17:56:09
between the other nodes, and
therefore has reasonable centrality.

57
17:56:11 --> 17:56:14
As more nodes start
having higher centrality,

58
17:56:14 --> 17:56:17
the centralization of the network drops

59
17:56:17 --> 17:56:21
because there is less variation in
the centrality values of the network.

60
17:56:22 --> 17:56:28
So for each type of centrality that we
discuss next we can compute the network

61
17:56:28 --> 17:56:33
centralization by considering the sum
of the difference between the maximum

62
17:56:33 --> 17:56:39
centrality and the centrality of the node
divided by the maximum centrality.

63
17:56:39 --> 17:56:42
There are thirty different
measures of centrality.

64
17:56:43 --> 17:56:46
We will consider only a few of them and

65
17:56:46 --> 17:56:49
explore the conceptual
principles supported here.

66
17:56:49 --> 17:56:53
The first and the most intuitive
measure is degree centrality.

67
17:56:53 --> 17:56:58
Quick measures, the degree of a node
divided by the possible edges it

68
17:56:58 --> 17:57:03
could have if it connected to each of
the other N- 1 nodes in the graph.

69
17:57:04 --> 17:57:04
Now, we have seen this before.

70
17:57:06 --> 17:57:09
This measure gives us a sense
of the hubness of the node.

71
17:57:09 --> 17:57:14
The higher the number is,
the more hub-like the node is.

72
17:57:14 --> 17:57:17
Thinking of our second key player problem.

73
17:57:17 --> 17:57:21
One way to approach this is to
find a hub-like measure for

74
17:57:21 --> 17:57:24
a group with multiple nodes.

75
17:57:24 --> 17:57:27
Instead of measuring the degree
centrality of individual nodes,

76
17:57:29 --> 17:57:34
the measure simply counts the number of
edges coming into the group as a whole

77
17:57:34 --> 17:57:36
compared to the number of outsiders.

78
17:57:37 --> 17:57:44
In our example two red nodes here
together connect to all blue nodes.

79
17:57:45 --> 17:57:50
Although there is just one
node neighboring both of them.

80
17:57:50 --> 17:57:53
Closeness centrality takes a different
approach to the centrality problem.

81
17:57:55 --> 17:58:00
It acts the shortest distance
of a node to all other nodes and

82
17:58:00 --> 17:58:02
divides it by a minus one.

83
17:58:02 --> 17:58:07
So in our graph, a node like I,
which is on the periphery of the graph,

84
17:58:07 --> 17:58:11
will be quite far from
all nodes in general, and

85
17:58:11 --> 17:58:15
therefore will have a very high
closeness centrality value.

86
17:58:15 --> 17:58:22
On the other hand, nodes like F, C and
H are much closer to all other nodes.

87
17:58:23 --> 17:58:26
Know that we define this measure
in terms of shortest paths.

88
17:58:28 --> 17:58:34
So if a moving object, like information,
flows to the shortest path

89
17:58:34 --> 17:58:39
in the network, F is more likely to
receive them earlier than other nodes.

90
17:58:39 --> 17:58:42
And therefore can process into
other nodes more quickly.

91
17:58:44 --> 17:58:45
Therefore.

92
17:58:45 --> 17:58:50
If we look to inject a new piece of
information into the network with the idea

93
17:58:50 --> 17:58:55
that it should read every other node
quickly, I should possibly inject it at F.

94
17:58:55 --> 17:59:02
Recall however, that node information
flow is two shortest routes.

95
17:59:02 --> 17:59:08
An example is gossip, that tends to
travel through centrality nodes.

96
17:59:08 --> 17:59:12
Closeness centrality does not work well
for these types of information nodes.

97
17:59:13 --> 17:59:17
Another very popular centrality measure
is called betweenness centrality.

98
17:59:18 --> 17:59:25
For any node it measures the fraction of
shortest paths flowing through that node.

99
17:59:25 --> 17:59:27
Compared to the number of
shortest paths in the graph.

100
17:59:29 --> 17:59:35
Since B is at one end,
its between a centrality is 0.

101
17:59:35 --> 17:59:36
But let's look at A.

102
17:59:36 --> 17:59:39
A is in the path from B to E.

103
17:59:40 --> 17:59:40
So is D.

104
17:59:42 --> 17:59:46
Therefore, A's score for
the B to E path is 0.5.

105
17:59:46 --> 17:59:54
Similarly, its score for the C to E path
is also 0.5, making the total score 1.

106
17:59:54 --> 17:59:59
E is in the path from A to D,
for there is one more

107
17:59:59 --> 18:00:05
path from A to do through C,
so E's score is 0.5.

108
18:00:05 --> 18:00:08
Now can you verify why C's score is 3.5?

109
18:00:08 --> 18:00:12
Betweenness centrality
is typically used for

110
18:00:12 --> 18:00:16
problem where a commodity is
flowing through the network.

111
18:00:16 --> 18:00:18
As in the case of closeness centrality.

112
18:00:20 --> 18:00:24
Any quantity that does not flow in
shortest path channels like infection or

113
18:00:24 --> 18:00:28
rumor on the internet doesn't work
well with betweeness centrality.

1
11:52:48 --> 11:52:51
This lesson on graph analytics,
is about identifying and

2
11:52:51 --> 11:52:54
tracking groups of interacting
entities in a network.

3
11:52:55 --> 11:52:57
We call these groups communities.

4
11:52:59 --> 11:53:03
Let's try to provide a more concrete
definition of communities in a network.

5
11:53:04 --> 11:53:06
We multiply the definition
by this diagram,

6
11:53:06 --> 11:53:09
showing a research study
on the Santa Fe Institute.

7
11:53:09 --> 11:53:14
A theoretical research institute
located in Santa Fe, New Mexico, US.

8
11:53:14 --> 11:53:18
They performed multidisciplinary
studies on fundamental principles

9
11:53:18 --> 11:53:21
of complex adaptive systems.

10
11:53:21 --> 11:53:24
The nodes in the graph are researchers.

11
11:53:24 --> 11:53:26
And an edge exists
between two researchers,

12
11:53:26 --> 11:53:27
if they collaborate with each other.

13
11:53:29 --> 11:53:34
As you can see, there are distinct
groups among researchers.

14
11:53:34 --> 11:53:36
A mathematically college researcher,

15
11:53:36 --> 11:53:40
does not collaborate with researchers
who work on structure of the RNA.

16
11:53:41 --> 11:53:45
So the graph,
is essentially a set of separate groups,

17
11:53:45 --> 11:53:49
thinly connected through a handful
of cross-disciplinary researchers,

18
11:53:49 --> 11:53:51
who collaborate across groups.

19
11:53:52 --> 11:53:55
These groups are then the communities
in this collaboration graph.

20
11:53:56 --> 11:53:57
What does this tell us?

21
11:53:58 --> 11:54:03
It tells us that communities are highly
interacting clusters in a graph.

22
11:54:03 --> 11:54:07
That is, they form pockets of
denser subgraphs that are more

23
11:54:07 --> 11:54:11
connected to each other,
than to members of the other clusters.

24
11:54:12 --> 11:54:17
Communities of humans, or otherwise,
are interesting things to study,

25
11:54:17 --> 11:54:20
because it gives us an insight
into the interaction patterns.

26
11:54:20 --> 11:54:22
And how they change with time.

27
11:54:23 --> 11:54:26
Here, are some analytics
questions about communities.

28
11:54:27 --> 11:54:29
We have divided them
into three categories.

29
11:54:31 --> 11:54:33
Analytics questions that do not
depend on time, are called static.

30
11:54:35 --> 11:54:39
Here, we ask questions about
the composition of the community,

31
11:54:39 --> 11:54:41
how tight-knit members are connected,
and so forth.

32
11:54:43 --> 11:54:49
In the second category, we involve the
formation, and evolution of the community.

33
11:54:50 --> 11:54:53
Communities can form temporal,
for example,

34
11:54:53 --> 11:54:56
around an event like a school shooting.

35
11:54:56 --> 11:54:59
Or some communities,
despite the comings and

36
11:54:59 --> 11:55:03
goings of members,
sustain themselves well.

37
11:55:03 --> 11:55:09
A Facebook group, a political party,
fans of a music band,

38
11:55:09 --> 11:55:12
are likely to continue over time,
and hence are non-transient.

39
11:55:14 --> 11:55:17
One can also be interested in
the history of formation of a community,

40
11:55:17 --> 11:55:18
like a criminal network.

41
11:55:19 --> 11:55:21
The third category is about predictions.

42
11:55:22 --> 11:55:26
Analysts would like to predict
how a community would grow.

43
11:55:26 --> 11:55:28
Whether it's composition
of members might change.

44
11:55:28 --> 11:55:33
Or whether there are emerging
power shifts within the community.

45
11:55:34 --> 11:55:38
Now before we ask these questions,
however,

46
11:55:38 --> 11:55:41
we need to first identify
communities in a large network.

47
11:55:42 --> 11:55:47
To find communities, we need to
formalize the idea that there are more

48
11:55:47 --> 11:55:52
connections within the community, and
fewer connections between two communities.

49
11:55:53 --> 11:55:58
One way to achieve this, is to think
of dividing the degree of a node,

50
11:55:58 --> 11:56:02
into an internal and
an external component.

51
11:56:02 --> 11:56:07
The internal component is the count
of edges within community.

52
11:56:07 --> 11:56:10
And the external degree, is the count
of edges outside the community.

53
11:56:12 --> 11:56:13
An example,

54
11:56:13 --> 11:56:17
will be to consider that my community
is where I live, that is San Diego.

55
11:56:17 --> 11:56:22
And then count the number of my friends
within San Diego versus outside San Diego.

56
11:56:23 --> 11:56:28
In the figure, the highlighted node
has four internal connections and

57
11:56:28 --> 11:56:29
one external connection.

58
11:56:31 --> 11:56:35
The next step, would be to think
of the internal degree, and

59
11:56:35 --> 11:56:37
the external degree of an entire cluster.

60
11:56:38 --> 11:56:43
We can sum up the internal degrees
of all nodes in a cluster.

61
11:56:43 --> 11:56:46
And call it the internal
degree of the whole cluster.

62
11:56:46 --> 11:56:49
And similarly,
sum the external degrees of the nodes,

63
11:56:49 --> 11:56:51
to compute the external
degree of the cluster.

64
11:56:52 --> 11:56:55
Now, we can define intra-cluster

65
11:56:55 --> 11:56:59
density to be the ratio of the number
of internal edges of the cluster,

66
11:56:59 --> 11:57:03
divided by the number of possible
connections inside the box.

67
11:57:04 --> 11:57:07
The denominator is N cues 2.

68
11:57:07 --> 11:57:11
Which is the number of pairwise
combination of nodes within the cluster.

69
11:57:11 --> 11:57:12
We call this delta int.

70
11:57:13 --> 11:57:17
Similarly, inter-cluster density,
delta x is the number

71
11:57:17 --> 11:57:22
of inter-cluster edges divided by
the possible pairings between nC,

72
11:57:23 --> 11:57:27
a node in the cluster, to n- nC,
the number of nodes outside the cluster.

73
11:57:28 --> 11:57:34
There are two kinds of methods used for
finding communities in the network.

74
11:57:34 --> 11:57:39
One of them focuses on local properties,
that is,

75
11:57:39 --> 11:57:43
properties for which one only
looks at a node and its neighbor.

76
11:57:44 --> 11:57:48
For the most ideal community
in a network is a subgraph,

77
11:57:48 --> 11:57:51
where every node is connected to
every other node in the subgraph.

78
11:57:52 --> 11:57:53
Such a structure is called a clique.

79
11:57:55 --> 11:57:58
To find a perfect community
structure as a clique,

80
11:57:58 --> 11:58:03
one can try to find the largest
clique within a graph, return cell.

81
11:58:04 --> 11:58:06
That is a computationally
challenging problem.

82
11:58:08 --> 11:58:12
It's much simpler to find cliques
if we know the value of k.

83
11:58:13 --> 11:58:17
That means the number of
members in the clique.

84
11:58:17 --> 11:58:20
We're going to show a simple
version of this in model three.

85
11:58:21 --> 11:58:25
The more general problem has been
solved by complex algorithms,

86
11:58:25 --> 11:58:26
that are beyond the scope of this course.

87
11:58:28 --> 11:58:34
In the real world, perfect cliques larger
than three or four are harder to find.

88
11:58:34 --> 11:58:36
So we need to relax the definition of it.

89
11:58:37 --> 11:58:40
Now, there are two types of relaxations,

90
11:58:40 --> 11:58:44
those based on distance,
and those based on density.

91
11:58:45 --> 11:58:51
Two distance based definitions
are n-clique and n-plan.

92
11:58:51 --> 11:58:55
We'll illustrate this over
a friendship graph shown here.

93
11:58:55 --> 11:58:59
n-clique, is a subgraph,

94
11:58:59 --> 11:59:03
such that the distance between each
node pair in that subgraph is n or less.

95
11:59:03 --> 11:59:09
By this definition, Holly, Paul,
and Gary form a two clique.

96
11:59:11 --> 11:59:14
That's a little awkward, isn't it?

97
11:59:14 --> 11:59:15
Yeah.

98
11:59:15 --> 11:59:17
They are within two
distance of each other.

99
11:59:17 --> 11:59:20
But the two clique does not
include intermediate nodes that

100
11:59:20 --> 11:59:21
connect the member nodes.

101
11:59:22 --> 11:59:26
So, Mike, is not in the two clique.

102
11:59:27 --> 11:59:31
This situation is corrected in n-clan.

103
11:59:31 --> 11:59:36
Where, to belong to the n-clan,
the shortest part between any members,

104
11:59:36 --> 11:59:38
without involving outsiders, is n or less.

105
11:59:38 --> 11:59:43
Now, Holly, Mike,

106
11:59:43 --> 11:59:48
Bill, Don, Harry, and
Gary form a two-clan.

107
11:59:48 --> 11:59:52
Clearly, this group is more cohesive
than the two-clique we saw before.

108
11:59:54 --> 11:59:58
n-clique and n-clan
are distance based measures for

109
11:59:58 --> 12:00:00
finding cohesive groups of communities.

110
12:00:00 --> 12:00:06
K-core is a density based method for
finding communities.

111
12:00:06 --> 12:00:09
Let's look at the dark, orange subgraph.

112
12:00:09 --> 12:00:14
Every node is connected to at least
three other nodes within the subgraph.

113
12:00:14 --> 12:00:15
They form a 3-core.

114
12:00:15 --> 12:00:20
Let's include the medium light
orange nodes in the subgraph now.

115
12:00:21 --> 12:00:26
Each node is connected to at least two
other members within the subgraph,

116
12:00:26 --> 12:00:27
they form a 2-core.

117
12:00:29 --> 12:00:36
Relaxing further, we can add the light
orange nodes and the graph as a 1-core.

1
23:53:23 --> 23:53:26
We have already defined
the degree of the node

2
23:53:26 --> 23:53:28
as the number of edges connected to it.

3
23:53:28 --> 23:53:31
Thus specifying if a node is
more connected than another.

4
23:53:33 --> 23:53:36
Looking closer we can separate out

5
23:53:36 --> 23:53:41
that two components denotes
degrees into in degree and out.

6
23:53:41 --> 23:53:46
Which are the counts of the incident and
the outgoing edges of a node respectively.

7
23:53:47 --> 23:53:50
In the example graph G has an indegree and

8
23:53:50 --> 23:53:56
outdegree of three making
the total degree equal to six.

9
23:53:56 --> 23:54:01
We first construct this degree table for
each node.

10
23:54:01 --> 23:54:05
It's a simple procedure where we count
the number of nodes with degree.

11
23:54:05 --> 23:54:06
0, 1, 2, and so forth.

12
23:54:06 --> 23:54:14
The degree versus count table is
a degree histogram of the graph.

13
23:54:14 --> 23:54:19
We can compare two graphs by computing
the vector distance between them.

14
23:54:19 --> 23:54:22
One simplistic measure is
just the Euclidean distance.

15
23:54:22 --> 23:54:27
For our case, the degree
histogram based on comparisons of

16
23:54:27 --> 23:54:31
the histograms find the graphs
to be very similar.

17
23:54:31 --> 23:54:35
The more sophisticated
methods are available but

18
23:54:35 --> 23:54:37
are outside the scope of this course.

19
23:54:49 --> 23:54:53
We can also compute histograms
of just the in degree or

20
23:54:53 --> 23:54:55
just the out degree of the graph.

21
23:54:56 --> 23:55:02
But perhaps more interesting is the joint
two dimensional histogram of the graph,

22
23:55:02 --> 23:55:06
the colorful histogram of the graph
can be interpreted here as follows.

23
23:55:06 --> 23:55:12
The graph has a maximum in degree of
three and a maximum out degree of three.

24
23:55:12 --> 23:55:15
This creates a two-dimensional
histogram with four times four

25
23:55:15 --> 23:55:17
equal to 16 different joined values.

26
23:55:19 --> 23:55:22
The actual value for any combination
is computed from the graph and

27
23:55:22 --> 23:55:23
color coded in the ratings.

28
23:55:24 --> 23:55:28
For example, there is no node with
in degree 0 and out degree 0.

29
23:55:28 --> 23:55:32
So the lower-left square of the graph
has value zero and color-coded blue.

30
23:55:34 --> 23:55:38
On the other hand, there are two nodes
with in-degree three and out-degree three.

31
23:55:38 --> 23:55:43
Thus, the top-right corner has the value
twp, which is 20% of the nodes.

32
23:55:44 --> 23:55:48
Color coded as light green.

33
23:55:48 --> 23:55:50
The 2D histogram provides
an interesting insight.

34
23:55:52 --> 23:55:55
The nodes with more incident
edges than outgoing edges

35
23:55:57 --> 23:56:00
represent entities that take
in more than they put out.

36
23:56:01 --> 23:56:07
In a social networking setting,
they represent members who are listeners.

37
23:56:08 --> 23:56:11
They receive a lot of posts,
but send much fewer posts.

38
23:56:13 --> 23:56:16
On the opposite side of the spectrum,

39
23:56:16 --> 23:56:22
there are talkers whose out-degrees
exceed their in-degrees.

40
23:56:23 --> 23:56:29
The entities that are in between,
we have both large values of in-degree and

41
23:56:29 --> 23:56:30
out-degree.

42
23:56:30 --> 23:56:32
These are communicators.

43
23:56:32 --> 23:56:36
In this graph, there seems to
be more talkers than listeners.

44
23:56:36 --> 23:56:37
Not surprising though.

45
23:56:37 --> 23:56:41
My social media friends
show similar statistics.

1
23:50:04 --> 23:50:07
In this video we will explore

2
23:50:07 --> 23:50:12
another fundamental property
of graphs called Connectivity.

3
23:50:12 --> 23:50:16
We list two important kinds
of graph analytic questions

4
23:50:16 --> 23:50:17
that are based on connectivity.

5
23:50:18 --> 23:50:22
In the first case we are asking
about the robustness of the network.

6
23:50:23 --> 23:50:26
Suppose we have a computer
network with many servers and

7
23:50:26 --> 23:50:28
a hacker is trying to
bring the system down,

8
23:50:29 --> 23:50:34
you set a small set of servers that the
hacker can target to disrupt the network.

9
23:50:35 --> 23:50:39
What we mean by disrupt is to disconnect
a part of the network from the rest.

10
23:50:41 --> 23:50:45
A similar problem may occur in
the power distribution network.

11
23:50:45 --> 23:50:50
In that case, an attacker may be able to
attack one or two central components so

12
23:50:50 --> 23:50:52
that large portions of
the network loses power.

13
23:50:54 --> 23:50:58
I have a geneticist colleague at
Tech Graduate Institute who studied

14
23:50:58 --> 23:51:00
the robustness of biological networks.

15
23:51:01 --> 23:51:06
He told me that many biological
networks have built in redundancy, so

16
23:51:06 --> 23:51:13
that even if you disrupt one important
gene, there are other genes in the network

17
23:51:13 --> 23:51:16
which will support the biological
function, possibly through other routes.

18
23:51:17 --> 23:51:21
Therefore a network is robust,
if removing one or

19
23:51:21 --> 23:51:25
more edges or
nodes still keeps it connected.

20
23:51:27 --> 23:51:31
The second category is about network
comparison in terms of their

21
23:51:31 --> 23:51:32
overall connectivity.

22
23:51:34 --> 23:51:37
The two graphs shown here are very
different in their structure.

23
23:51:38 --> 23:51:41
What are some parameters by
which we can compare them?

24
23:51:41 --> 23:51:47
That to talk about connectivity, we must
first define the concept of connectivity.

25
23:51:47 --> 23:51:48
Very simple.

26
23:51:49 --> 23:51:54
A graph is connected if we can
reach any node from any other node.

27
23:51:56 --> 23:52:01
Let's look at the crypto graph in
the picture, clearly we cannot

28
23:52:01 --> 23:52:06
reach from all nodes to all nodes here
because this graph is not connected.

29
23:52:06 --> 23:52:12
However, we can identify four parts of
the graph that are themselves connected.

30
23:52:13 --> 23:52:18
These islands of connected
parts are called components or

31
23:52:18 --> 23:52:19
connected components of a graph.

32
23:52:21 --> 23:52:25
For directed graphs, we need to be
a little more specific about connectivity.

33
23:52:26 --> 23:52:31
We say that the directed graph is strongly
connected, if we follow the direction of

34
23:52:31 --> 23:52:35
the edges and still reach every
node from every other node.

35
23:52:37 --> 23:52:42
A weaker form of connectivity is if we do
not care about the direction of the arrows

36
23:52:42 --> 23:52:45
and can reach every node
from every other node.

37
23:52:47 --> 23:52:51
Another way of saying it is that
the undirected version of the graph

38
23:52:51 --> 23:52:55
is connected,
this is called Weak Connected.

39
23:52:55 --> 23:52:59
Look at the graph that we have seen so
many times in this course.

40
23:52:59 --> 23:53:01
Is it strongly connected or
weakly connected?

41
23:53:09 --> 23:53:14
This leads to some big graph challenge.

42
23:53:14 --> 23:53:15
Given an arbitrarily large graph,

43
23:53:16 --> 23:53:20
can I find the connected components
of the graph efficiently?

44
23:53:21 --> 23:53:24
Second given an arbitrarily large graph,

45
23:53:24 --> 23:53:26
can we find its sub graphs
that are strongly connected?

46
23:53:28 --> 23:53:30
We'll touch upon these
questions in Module Four.

1
23:43:34 --> 23:43:36
So there are two ways to break a graph.

2
23:43:38 --> 23:43:42
Identifying the smallest node set which
if removed will disconnect the graph.

3
23:43:44 --> 23:43:50
Here, if we remove F, D and

4
23:43:50 --> 23:43:55
H or H, F, G,
we have disconnected the graph.

5
23:43:56 --> 23:44:01
So the separating set is either H,
F and G, or H, F and D.

6
23:44:02 --> 23:44:04
So the connectivity is three.

7
23:44:04 --> 23:44:10
Similarly, removing C-F, D-G, D-F and

8
23:44:10 --> 23:44:17
H-E edges will also disconnect the graph.

9
23:44:17 --> 23:44:19
Therefore, the edge connectivity is four.

10
23:44:20 --> 23:44:23
So this is how network
robustness is defined.

11
23:44:24 --> 23:44:27
Now, let's ask, is this network robust?

12
23:44:28 --> 23:44:32
Suppose the attacker removed node F.

13
23:44:32 --> 23:44:33
Why remove F?

14
23:44:34 --> 23:44:38
Because F is the most connected node.

15
23:44:38 --> 23:44:40
That is, F has the highest degree.

16
23:44:41 --> 23:44:45
If we remove F, five paths are disrupted.

17
23:44:45 --> 23:44:49
For example,
there is no way to go from C to G, or

18
23:44:49 --> 23:44:53
from I to E,
because these paths went through F.

19
23:44:54 --> 23:44:55
Let's try this exercise.

20
23:44:56 --> 23:44:59
Suppose the attacker
has already removed F.

21
23:44:59 --> 23:45:03
And would like to cause more
damage to the rest of the network.

22
23:45:03 --> 23:45:05
Which node should be targeted next?

23
23:45:06 --> 23:45:11
The answer is C,
because C is the next most connected node.

24
23:45:12 --> 23:45:16
Higher degree nodes make
the network more vulnerable.

1
23:28:50 --> 23:28:55
So far, the centrality of a node
is defined using the degree and

2
23:28:55 --> 23:28:58
the shortest distance to other nodes.

3
23:28:58 --> 23:28:59
Now, we introduce a different idea.

4
23:29:01 --> 23:29:04
We would like to say that
if you are important and

5
23:29:04 --> 23:29:07
I'm connected to you,
then I must be somewhat important too.

6
23:29:08 --> 23:29:11
In other words, my centrality

7
23:29:11 --> 23:29:16
is proportional to the combined
centrality values of my neighbors.

8
23:29:16 --> 23:29:20
Now, if we write that down mathematically
it would look like the top formula.

9
23:29:20 --> 23:29:24
The centrality of (Vi) is
the sum of its neighbors.

10
23:29:24 --> 23:29:27
Now, we can write that as an equation

11
23:29:27 --> 23:29:29
where Lambda is
a proportionality constant.

12
23:29:30 --> 23:29:35
The resulting equation looks exactly
like the Eigenvector equation we have

13
23:29:35 --> 23:29:36
seen before.

14
23:29:36 --> 23:29:39
Now again, you really do not
have to understand how it works.

15
23:29:39 --> 23:29:43
It's fine to know that if
we solve that equation,

16
23:29:43 --> 23:29:45
we will get the Eigen values lambda.

17
23:29:46 --> 23:29:49
Now, let's take the largest lambda and

18
23:29:49 --> 23:29:54
find the corresponding Eigenvector, which
will give you the centrality of each note.

19
23:29:55 --> 23:29:58
Notice the difference between
the degree centrality and

20
23:29:58 --> 23:30:00
the Eigenvector centrality
in the same graph.

21
23:30:02 --> 23:30:05
The yellow node in the middle
has a low degree centrality

22
23:30:05 --> 23:30:07
compared to its neighbors.

23
23:30:07 --> 23:30:13
However, with the Eigenvector centrality,
the node becomes comparably more important

24
23:30:13 --> 23:30:17
because the neighbor centrality status
boost some of the centrality of this node.

25
23:30:19 --> 23:30:22
In contrast,
consider the second highlighted note.

26
23:30:22 --> 23:30:27
It had the same on normalize degree
centrality as the previous node.

27
23:30:28 --> 23:30:32
But because the neighbors
are low centrality nodes,

28
23:30:32 --> 23:30:35
the eigenvector centrality goes down.

29
23:30:35 --> 23:30:41
So, this is the intended consequence
of the eigenvector centrality measure.

30
23:30:41 --> 23:30:47
Speaking of consequences,
Eigenvector centrality essentially says

31
23:30:47 --> 23:30:49
if you know the right people,
your importance will go up.

32
23:30:51 --> 23:30:52
Well, that's kind of risky proposition.

33
23:30:53 --> 23:30:59
Here is me in a social network,
and let's say I'm connected to

34
23:30:59 --> 23:31:03
this somewhat dubious character, and
I think it doesn't really matter.

35
23:31:04 --> 23:31:09
What I don't know is that my connection
has it's own set of connections, and

36
23:31:09 --> 23:31:10
look at who they are.

37
23:31:11 --> 23:31:15
So, on the one hand, these shady
characters that are now connected to

38
23:31:15 --> 23:31:20
indirectly does raise my
eigenvector centrality, but

39
23:31:20 --> 23:31:24
it also has quite a damaging
effect on my reputation.

40
23:31:24 --> 23:31:27
My EV centrality almost makes
me look like a suspect.

41
23:31:28 --> 23:31:28
Now, think about that.

42
23:31:30 --> 23:31:34
Now Brin and Page, the founders of Google

43
23:31:34 --> 23:31:37
had an interesting way to think
about the eigenvector centrality.

44
23:31:38 --> 23:31:40
They thought about a server.

45
23:31:40 --> 23:31:43
Well no, not that kind of server.

46
23:31:43 --> 23:31:44
This kind of server.

47
23:31:44 --> 23:31:46
The kind that surfs the web.

48
23:31:46 --> 23:31:49
But, this is a special web
surfer called a random surfer.

49
23:31:51 --> 23:31:56
And here is what he does, he picks
a web page and looks at the links.

50
23:31:58 --> 23:32:03
Then he chooses a random link, goes to
that page and does the same thing again.

51
23:32:05 --> 23:32:09
Except sometimes when you kind of get
bored and goes to totally new page.

52
23:32:11 --> 23:32:13
How often does he do this random jump?

53
23:32:14 --> 23:32:19
Let's say, there's always sort
of a 15% chance that he will.

54
23:32:19 --> 23:32:22
Or more generally,
with the probability of 1 minus alpha.

55
23:32:24 --> 23:32:27
So, Page and Brin's idea was to figure out

56
23:32:27 --> 23:32:31
that this surfer will visit a page with
a high chance, if the page is central.

57
23:32:33 --> 23:32:34
They came up with a measure for

58
23:32:34 --> 23:32:39
this stationary probability of a page
being visited by the random surfer.

59
23:32:40 --> 23:32:42
They did not call it centrality,
they called it pagerank.

60
23:32:44 --> 23:32:47
Let's see a YouTube video to
understand how pagerank behaves.

61
23:34:05 --> 23:34:09
Okay, now we are talking about
the World Wide Web which is a huge graph.

62
23:34:09 --> 23:34:12
How do we of such a graph?

63
23:34:13 --> 23:34:18
The answer is iteratively using
a method called power iteration.

64
23:34:19 --> 23:34:23
This method can be used because we
are looking for the largest eigenvalue.

65
23:34:23 --> 23:34:24
Let me show you.

66
23:34:25 --> 23:34:26
Let's take a small graph.

67
23:34:27 --> 23:34:31
Let's initialize the still unknown
page rank as zero for all nodes.

68
23:34:33 --> 23:34:38
Now, page rank A is a 0.15 chance,
that I was at A already.

69
23:34:38 --> 23:34:43
And 0.85 chance that I come to A from B,
or I come to A from C.

70
23:34:45 --> 23:34:48
However, at this point,
the page rank of B and C are at zero.

71
23:34:49 --> 23:34:53
So, in the first iteration,
page rank of A is 0.15.

72
23:34:54 --> 23:35:00
Now for B, I can only come to B from A,
but we cannot claim all of pagerank of A,

73
23:35:00 --> 23:35:04
because there is always a half chance
that the surfer will come to B

74
23:35:04 --> 23:35:08
from A because he could
also get to here from C.

75
23:35:08 --> 23:35:14
This, plus the 0.15 chance that
the surfer is already at B,

76
23:35:14 --> 23:35:16
makes Bs pagerank 0.21.

77
23:35:16 --> 23:35:20
Now after doing a few rounds of this
computation, between 50 and 100 iteration,

78
23:35:20 --> 23:35:20
let's say.

79
23:35:21 --> 23:35:22
The values will converge.

80
23:35:23 --> 23:35:27
Now, this computation has been
demonstrated to perform well

81
23:35:27 --> 23:35:28
in the MapReduce framework.

82
23:35:30 --> 23:35:34
In module four, we'll talk about
another way of computing this metric.

1
23:04:27 --> 23:04:30
Welcome back to the second
module of the course.

2
23:04:32 --> 23:04:35
In this module, we will cover
a number of basic principles and

3
23:04:35 --> 23:04:36
techniques of graph analytics.

4
23:04:38 --> 23:04:42
As we mentioned in the last module,
the goal of graph analytics

5
23:04:42 --> 23:04:47
is to utilize the mathematical
properties of data and provide

6
23:04:47 --> 23:04:51
efficient algorithmic solutions for large
and complex graph structure problems.

7
23:04:52 --> 23:04:57
As I said, in this module, we'll learn a
number of basic graph analytic techniques.

8
23:04:59 --> 23:05:02
After this module,
you'll be able to identify the right

9
23:05:02 --> 23:05:06
class of techniques to apply for
a graph analytics problem.

10
23:05:06 --> 23:05:10
To be more specific,
in this module we will consider

11
23:05:10 --> 23:05:14
the mathematical and
algorithmic aspects, and not so

12
23:05:14 --> 23:05:17
much the computing frameworks
to implement these methods.

13
23:05:18 --> 23:05:23
In Modules 3 and 4, we will look at
two different kinds of computing

14
23:05:23 --> 23:05:27
platforms that are used for implementing
the techniques discussed in this module.

15
23:05:28 --> 23:05:30
Here is the lesson plan for the module.

16
23:05:31 --> 23:05:34
First, we'll discuss a few basic terms and

17
23:05:34 --> 23:05:37
their definition which we'll use for
the rest of the course.

18
23:05:40 --> 23:05:42
Of course, they are not the only terms and
concepts we will learn.

19
23:05:44 --> 23:05:47
As we go through each technique,
we will add more terms and

20
23:05:47 --> 23:05:49
definitions in our vocabulary.

21
23:05:49 --> 23:05:53
Now following these definitions we
will consider four categories of

22
23:05:53 --> 23:05:56
graph analytic procedures.

23
23:05:56 --> 23:06:01
The first, called Path Analytics,
is centered around the Analytic Techniques

24
23:06:01 --> 23:06:04
where the primary objective involves
traversing to the nodes and

25
23:06:04 --> 23:06:05
edges of the ground.

26
23:06:06 --> 23:06:09
The second analytic technique inquires and

27
23:06:09 --> 23:06:12
explores the connectivity
pattern of the gaps.

28
23:06:12 --> 23:06:16
Where the term connectivity pattern
refers to the structure and

29
23:06:16 --> 23:06:18
organizations of the edges of the graph.

30
23:06:20 --> 23:06:23
The third analytics category
involves the discovery and

31
23:06:23 --> 23:06:27
behavior of communities which are closely
interacting entities in a network.

32
23:06:28 --> 23:06:34
The fourth category, termed Centrality
Analytics, detects and characterizes

33
23:06:34 --> 23:06:38
significant nodes of a network with
respect to a specific analysis problem.

34
23:06:39 --> 23:06:43
Of course there are many more types of
graph analytic techniques that we'll cover

35
23:06:43 --> 23:06:44
in the course.

36
23:06:44 --> 23:06:48
We'll provide some additional reading
material for those who are interested.

37
23:06:48 --> 23:06:51
But we start by recapitulating
our definition of graphs

38
23:06:51 --> 23:06:56
as a collection of vertices and edges,
which represent ordered pairs of nodes.

39
23:06:57 --> 23:07:00
While this mathematical
definition is indeed correct,

40
23:07:00 --> 23:07:05
in practice it needs to be extended to
give you other information elements.

41
23:07:07 --> 23:07:08
Let us consider a single Tweet.

42
23:07:10 --> 23:07:15
As we have mentioned previously,
a Tweet is a complex information output

43
23:07:15 --> 23:07:17
because it is a graph in itself
with several nodes and edges.

44
23:07:19 --> 23:07:21
But over and about the structure, a Tweet

45
23:07:21 --> 23:07:25
actually contains much more information
and code inside the nodes and edges.

46
23:07:28 --> 23:07:31
First, there several kinds
of nodes in a Tweet.

47
23:07:32 --> 23:07:36
For example it has a Tweet node,
a User node, a Media node,

48
23:07:36 --> 23:07:39
a URL node, a hashtag node and so forth.

49
23:07:40 --> 23:07:46
This assignment of kinds or labels to
nodes is often called node typing.

50
23:07:47 --> 23:07:51
Every graph application will
have its own set of types.

51
23:07:51 --> 23:07:56
And it'll assign one or more types to
a node but it is not mandatory for

52
23:07:56 --> 23:07:57
an application to use node types.

53
23:08:00 --> 23:08:05
Mathematically we can extend our original
definition with two more elements.

54
23:08:05 --> 23:08:10
The set of node types and the mapping
function that assigns types to nodes.

55
23:08:10 --> 23:08:13
That means it associates
a type to every node.

56
23:08:14 --> 23:08:20
However, not all nodes need to have
a type but in many applications they do.

57
23:08:23 --> 23:08:27
In addition to types,
a node also has attributes and values.

58
23:08:28 --> 23:08:29
In our Tweet example,

59
23:08:31 --> 23:08:35
text is the name of an attribute that
refers to the textual body of the Tweet

60
23:08:35 --> 23:08:40
whose value is a character string
written by the author of the Tweet.

61
23:08:41 --> 23:08:44
For a specific kind of data like a Tweet,

62
23:08:44 --> 23:08:47
one has a fixed set of attributes
as decided by Twitter.

63
23:08:48 --> 23:08:51
This collection of attributes
is called a node schema.

64
23:08:53 --> 23:08:54
For a general graph,

65
23:08:54 --> 23:08:57
a node schema may have an arbitrary
number of attribute value pairs.

66
23:08:58 --> 23:09:03
We will revisit this in module 3
when we discuss graphing the models.

67
23:09:04 --> 23:09:09
Similarly, at edge of a graph, we have
an edge type, also called an edge label.

68
23:09:10 --> 23:09:11
Also just like a node,

69
23:09:11 --> 23:09:16
an edge may have an edge schema
consisting of attribute value pierce.

70
23:09:16 --> 23:09:21
Here, Interaction Type is
an attribute in our biological

71
23:09:21 --> 23:09:25
network that describes the modality of
interaction between a pair of genes.

72
23:09:27 --> 23:09:30
For the specific edge we have highlighted,

73
23:09:30 --> 23:09:33
the genes interact through
biochemical activity.

74
23:09:33 --> 23:09:36
Because they are party to
some biochemical process.

75
23:09:38 --> 23:09:42
Clearly, there are different kinds
of interaction between these

76
23:09:42 --> 23:09:42
genes or proteins.

77
23:09:43 --> 23:09:48
That means an attribute called
interaction type can have a set of

78
23:09:48 --> 23:09:52
possible values like physical,
genetic and so on.

79
23:09:54 --> 23:09:59
This set of possible values is
called the domain of the attribute.

80
23:10:01 --> 23:10:04
Putting these elements back
into our mathematical model,

81
23:10:04 --> 23:10:09
we get a more concrete specification of
what a real live graph would contain.

82
23:10:10 --> 23:10:14
We have already discussed edge types
as well as node and edge properties.

83
23:10:16 --> 23:10:18
Take a minute to look through this again.

84
23:10:19 --> 23:10:25
Whenever you consider an application that
needs graph analytics, the first task

85
23:10:25 --> 23:10:29
should be to determine the informational
model of the graph your application needs.

86
23:10:30 --> 23:10:34
It's always a good exercise to
document the information model,

87
23:10:34 --> 23:10:37
in terms of the elements
described on the slide.

88
23:10:39 --> 23:10:41
Let's see a little more on
the topic of edge properties.

89
23:10:43 --> 23:10:47
Many application encode different
kids of numeric knowledge

90
23:10:47 --> 23:10:50
into edges of a graph in
the form of edge points.

91
23:10:51 --> 23:10:55
If we do not put weights
in an adjacency metrics,

92
23:10:55 --> 23:11:00
an edge is just represented by
placing a one in the appropriate cell.

93
23:11:00 --> 23:11:02
However, if we do use a weight,

94
23:11:02 --> 23:11:07
the weight value can be placed in
the adjacency matrix to facilitate

95
23:11:07 --> 23:11:09
down stream computation as
we'll show in the next lesson.

96
23:11:11 --> 23:11:12
What do the weights mean?

97
23:11:13 --> 23:11:15
That depends on the application.

98
23:11:15 --> 23:11:16
Let's see some examples.

99
23:11:18 --> 23:11:24
The most obvious example is a road map
where the nodes are road intersections and

100
23:11:24 --> 23:11:26
the edges represent
stretches of the street or

101
23:11:26 --> 23:11:28
highway between these intersections.

102
23:11:29 --> 23:11:33
The edge weight can represent the distance
of a particular segment of the road.

103
23:11:36 --> 23:11:40
In a personal communication network,
for example an email network,

104
23:11:40 --> 23:11:46
we can count the average number of emails
per week sent from John to Jill and

105
23:11:46 --> 23:11:49
use it as a proxy for
the strength of their connection.

106
23:11:49 --> 23:11:54
So more emails means
a stronger connection.

107
23:11:54 --> 23:11:58
In a biological network, one often has
to assess whether an interaction that

108
23:11:58 --> 23:12:02
can occur is actually likely to occur
given the concentration of the reactants,

109
23:12:02 --> 23:12:07
the chemical environment at the site
of the reaction and so forth.

110
23:12:07 --> 23:12:11
This is represented as a weight that
designates the likelihood of interaction.

111
23:12:12 --> 23:12:15
Finally, consider a knowledge network

112
23:12:15 --> 23:12:19
where nodes represent entities
like people, places and events.

113
23:12:19 --> 23:12:24
And edges represent relationships
like a person visited a place or

114
23:12:24 --> 23:12:28
movie actor Tom is dating
movie actress Kim.

115
23:12:28 --> 23:12:31
Now this kind of information may
be important for some news media.

116
23:12:31 --> 23:12:36
However, if the information does
not come from an authentic source,

117
23:12:36 --> 23:12:41
itself, it is more prudent to
put a certainty value on it.

118
23:12:41 --> 23:12:44
This certainty value may be
treated as a weight on the edge.

119
23:12:45 --> 23:12:48
Moving on from the information
model of the graph,

120
23:12:48 --> 23:12:52
the structure of the graph often
contains valuable insights to a graph.

121
23:12:53 --> 23:12:56
Many of the graph analytic techniques
we will discuss in this section

122
23:12:57 --> 23:13:00
will consider these structural
properties of graphs.

123
23:13:00 --> 23:13:07
One such structure is a loop,
which is an edge from a node to itself.

124
23:13:07 --> 23:13:08
In the example here,

125
23:13:08 --> 23:13:12
you can see that a protein can interact
with another protein of the same kind.

126
23:13:13 --> 23:13:15
Many other examples abound.

127
23:13:15 --> 23:13:17
People send emails to themselves.

128
23:13:17 --> 23:13:20
A road segment circles back
to the same intersection.

129
23:13:21 --> 23:13:24
A website has a link to its own URL.

130
23:13:24 --> 23:13:29
The existence of loops and the nodes that
have such loops can be very informative

131
23:13:29 --> 23:13:33
in some applications and
can be problematic for other applications.

132
23:13:35 --> 23:13:38
Another structure property of a graph
is the occurrence of multiple

133
23:13:38 --> 23:13:41
edges between the same node pair.

134
23:13:41 --> 23:13:45
The graphs with this feature
are called multi-graphs.

135
23:13:45 --> 23:13:49
In this example, the two map kinase
genes have five edges between them.

136
23:13:51 --> 23:13:52
Why multiple edges?

137
23:13:52 --> 23:13:59
It's because each edge has
a different information content.

138
23:13:59 --> 23:13:59
In this case,

139
23:13:59 --> 23:14:04
these two genes can have five different
types of interactions between them.

140
23:14:04 --> 23:14:08
Where each interaction has a different
value for the attribute interaction type.

141
23:14:09 --> 23:14:12
We see this all the time
in human networks too.

142
23:14:12 --> 23:14:17
A person can be my spouse,
a co-performer in music and

143
23:14:17 --> 23:14:19
my financial adviser,
all at the same time.

144
23:14:20 --> 23:14:24
Many analytics algorithms
are not natively designed for

145
23:14:24 --> 23:14:29
multigraphs, and often need a little
customization to handle them.

146
23:14:29 --> 23:14:33
We will mention some of these
customizations as we go forward and walk

147
23:14:33 --> 23:14:37
through the different kinds of analytics
applications preformed on graphs.

1
22:19:01 --> 22:19:03
So we talked about local properties.

2
22:19:04 --> 22:19:05
In this lecture,

3
22:19:05 --> 22:19:08
we'll cover a global property based
method of [INAUDIBLE] finding.

4
22:19:10 --> 22:19:14
The specific property we focus on,
is called modularity.

5
22:19:15 --> 22:19:17
It tries to estimate
the quality of clusters of

6
22:19:17 --> 22:19:18
communities in the [INAUDIBLE].

7
22:19:20 --> 22:19:21
The intuition is as follows.

8
22:19:22 --> 22:19:25
If we consider the edges in a group and

9
22:19:25 --> 22:19:29
try to see whether it's different
from what you'd see if the edges

10
22:19:29 --> 22:19:33
were assigned randomly with
some probability distribution.

11
22:19:33 --> 22:19:37
If there is a community, there will be
more edges than would happen at random.

12
22:19:38 --> 22:19:43
If there is no community in some part
of the graph, the number of edges in

13
22:19:43 --> 22:19:48
that part will either be close to
the random case or even lower than that.

14
22:19:48 --> 22:19:54
The modularity measure thus estimates
the quality of the clusters in

15
22:19:54 --> 22:20:01
the graph by evaluating this difference of
the actual minus the random edge fraction.

16
22:20:02 --> 22:20:04
So this is the mathematical
formulation of what I just described.

17
22:20:06 --> 22:20:10
The adjacency matrix, A,
gives us the actual edges.

18
22:20:11 --> 22:20:16
The Pij provides a probability
of a random edge.

19
22:20:16 --> 22:20:20
The m in the denominator gives
us the fractional edges.

20
22:20:22 --> 22:20:26
And the delta function's
task is to evaluate if i and

21
22:20:26 --> 22:20:29
j should be in the same cluster.

22
22:20:29 --> 22:20:34
If they are,
the contribution will be added to Q,

23
22:20:34 --> 22:20:39
the quality metric,
which is multi-layered.

24
22:20:40 --> 22:20:44
Well, we have not defined what
the probability model looks like.

25
22:20:44 --> 22:20:47
There are many ways to figure
out what Pij should look like.

26
22:20:49 --> 22:20:54
One simple model says, that the chance
that there is an edge between nodes i and

27
22:20:54 --> 22:21:01
j, is proportional to the degree of
node i times the degree of node j.

28
22:21:01 --> 22:21:03
That means if nodes i and

29
22:21:03 --> 22:21:07
j are already well connected, there is
a high chance that they share an edge.

30
22:21:08 --> 22:21:11
So if you're a mathematical person,
you might be thinking, okay,

31
22:21:12 --> 22:21:17
let's find clusters in the graph so
that Q is maximum and then we're done.

32
22:21:18 --> 22:21:24
Well sadly, maximizing Q is very hard.

33
22:21:24 --> 22:21:27
So we need to find
an approximate solution.

34
22:21:29 --> 22:21:32
So we will illustrate a very

35
22:21:32 --> 22:21:36
popular method of finding this
modularity based community detection.

36
22:21:38 --> 22:21:41
Well, the best way to describe
this is through a YouTube video.

37
22:21:42 --> 22:21:43
That's the URL.

38
22:21:44 --> 22:21:48
Now, the next slide and
the following slide,

39
22:21:48 --> 22:21:54
describes it, but
I think it's better to explain the method

40
22:21:54 --> 22:21:58
through screenshots off
the video as it happens.

41
22:22:00 --> 22:22:05
We will show you some
snapshots of this video.

42
22:22:05 --> 22:22:08
There are 309 nodes in this graph.

43
22:22:09 --> 22:22:12
Initially, they all have different colors.

44
22:22:12 --> 22:22:14
That is,
they belong to different communities.

45
22:22:16 --> 22:22:23
This screenshot shows the graph at
iteration 144 of the algorithm.

46
22:22:23 --> 22:22:27
The number of communities now is 286.

47
22:22:27 --> 22:22:34
The chart on the right side plots time on
the x-axis and modularity on the y-axis.

48
22:22:34 --> 22:22:36
As you see,
the modularity's on the rights.

49
22:22:37 --> 22:22:40
Roughly, at each iteration,

50
22:22:41 --> 22:22:46
the system is trying to change the color
of a node to that of its neighbors.

51
22:22:46 --> 22:22:51
But it actually changes the color
only if the modularity value of

52
22:22:51 --> 22:22:55
the whole graph changes as
a result of that color change.

53
22:22:55 --> 22:23:01
After a few more iterations,
the number of communities has become 241.

54
22:23:01 --> 22:23:06
The three arrows show some parts of the
graph where the nodes have changed colors.

55
22:23:06 --> 22:23:10
Modularity is on the rise.

56
22:23:10 --> 22:23:16
After 1,437 iterations, the modularity
of the graph is still going up.

57
22:23:16 --> 22:23:18
Now, there are 113 communities.

58
22:23:20 --> 22:23:24
The errors show some new areas where
the neighboring nodes have the same color.

59
22:23:25 --> 22:23:30
At 1,842 iterations,
the modulary gains slows down.

60
22:23:30 --> 22:23:35
Meanwhile, the number of
communities have reduced to 75.

61
22:23:35 --> 22:23:41
At 4,179 iterations, the modularity
growth has started becoming flat.

62
22:23:42 --> 22:23:48
But in the meantime, the number of colors,
that is communities, has reduced to 48.

63
22:23:48 --> 22:23:53
At around 5,196 iterations, the algorithm
decides that there is not enough

64
22:23:53 --> 22:23:56
reduction in the number of communities,

65
22:23:56 --> 22:24:00
which is reduced to only 45 in
the last 1,000 iterations or so.

66
22:24:01 --> 22:24:06
Now it collapses each community that
is clustered to a single node, and

67
22:24:06 --> 22:24:09
creates a cluster to cluster edges.

68
22:24:10 --> 22:24:15
The orange box,
an arrow shows this contraction.

69
22:24:15 --> 22:24:19
Now compared to the previous slide,
this collapsing or

70
22:24:19 --> 22:24:23
contraction of the plastic creates
a skeleton of the original draft.

71
22:24:24 --> 22:24:27
Now the algorithm starts again
with this reduced graph.

72
22:24:29 --> 22:24:32
You will find many graph
analysis software,

73
22:24:32 --> 22:24:36
where you can run the Louvain
method of community detection.

74
22:24:38 --> 22:24:40
[INAUDIBLE].

75
22:24:40 --> 22:24:44
I took my Linked-in network,
which has me at the center, and

76
22:24:44 --> 22:24:46
all my connections as nodes.

77
22:24:48 --> 22:24:51
If two of my contacts are also
connected in Linked-in,

78
22:24:51 --> 22:24:53
there is an edge between them.

79
22:24:54 --> 22:24:58
This kind of mean-centric network
is often called an ego network.

80
22:24:59 --> 22:25:05
When I [INAUDIBLE] the community direction
algorithm here, I found six communities,

81
22:25:05 --> 22:25:09
with one set of parameters and
seven communities with another.

82
22:25:10 --> 22:25:13
I could clearly see my
connections in San Diego.

83
22:25:13 --> 22:25:17
My connections in my professional network,
my friends in India, and so forth.

84
22:25:18 --> 22:25:23
There's always one false community,
which stands for

85
22:25:23 --> 22:25:26
others, nodes that do not clearly
belong to any specific group.

86
22:25:28 --> 22:25:33
Perhaps more interesting and
important than the static

87
22:25:33 --> 22:25:38
analysis of communities is to track
communities over a length of time.

88
22:25:38 --> 22:25:40
And determine how they evolve and why.

89
22:25:42 --> 22:25:46
There are six large
categories of evolution steps

90
22:25:46 --> 22:25:47
that can happen within a community.

91
22:25:48 --> 22:25:51
A community like a new
Facebook group can be born.

92
22:25:52 --> 22:25:56
A community, like a group of
people who gathered for an event,

93
22:25:56 --> 22:26:00
would dissolve because the event and
the mutual interest around it has ended.

94
22:26:02 --> 22:26:05
A community can grow because the members
rally around a common cause.

95
22:26:05 --> 22:26:11
Typically, new cross-community
edges start getting

96
22:26:11 --> 22:26:15
formed before the communities
actually merge.

97
22:26:15 --> 22:26:18
The communities can
shrink like my book club.

98
22:26:18 --> 22:26:20
Where do you see this in real life?

99
22:26:21 --> 22:26:22
Well, how about company mergers?

100
22:26:24 --> 22:26:27
Surely enough, the [INAUDIBLE] results
will happen when a community splits.

101
22:26:28 --> 22:26:30
Going along with the previous example,

102
22:26:30 --> 22:26:35
a closely working group in a company may
at some point create their own product and

103
22:26:35 --> 22:26:40
form a new company with very
little ties to the old company.

104
22:26:40 --> 22:26:45
One standard symptom of a group splitting
is that the nodes in the subgroup

105
22:26:45 --> 22:26:49
show an increase in the number of
edges just amongst themselves.

1
20:45:51 --> 20:45:54
So far we have seen two versions
of the Dijkstra Algorithm.

2
20:45:55 --> 20:45:59
Both these versions assume that the edge
weights provided by the network

3
20:45:59 --> 20:46:00
must be used as is.

4
20:46:01 --> 20:46:04
Now that can lead to some
interesting problems.

5
20:46:04 --> 20:46:06
We saw one such problem before.

6
20:46:06 --> 20:46:09
Remember we were trying to decide
whether we should go from G to D or

7
20:46:09 --> 20:46:14
from F to E because both options
had the same total weight?

8
20:46:14 --> 20:46:16
Now had we chosen to go from G to D,

9
20:46:16 --> 20:46:20
it would take us a few extra steps
to arrive at correct solution.

10
20:46:21 --> 20:46:25
One way of handling this problem
is to use additional knowledge.

11
20:46:25 --> 20:46:29
So intuitively, we want to say that
we know that we want to go to B.

12
20:46:29 --> 20:46:32
So traversing through
D is not a good idea.

13
20:46:32 --> 20:46:35
Because it will take us away from B.

14
20:46:35 --> 20:46:40
In other words we use the knowledge
of the destination of B's location

15
20:46:40 --> 20:46:42
to steer the direction of search.

16
20:46:43 --> 20:46:47
This variant is called
Goal-Directed Dijkstra Algorithm because

17
20:46:47 --> 20:46:52
it is using the information about the
target known at any point in the search.

18
20:46:52 --> 20:46:56
The trick to use this information is to
change the edge weights as we diverse.

19
20:46:58 --> 20:46:59
How do we change the weight?

20
20:46:59 --> 20:47:02
We use a formula, where the new weight

21
20:47:02 --> 20:47:06
is the original weight together with
a function called the potential function.

22
20:47:06 --> 20:47:08
Now we'll show this in our example.

23
20:47:08 --> 20:47:09
Since our graph is a proxy for

24
20:47:09 --> 20:47:14
a road network, we can assume that we
know the coordinates of every node.

25
20:47:14 --> 20:47:17
Therefore, we can compute
the distance between any two nodes.

26
20:47:17 --> 20:47:20
In practice, we will choose a few nodes so

27
20:47:20 --> 20:47:25
that we can compute the distance of every
other node from these chosen nodes.

28
20:47:25 --> 20:47:27
These chosen nodes are called landmarks.

29
20:47:27 --> 20:47:31
Let's assume B, which is our target,
is a landmark node, and

30
20:47:31 --> 20:47:34
let's rewind to the state where
we are trying to choose between

31
20:47:34 --> 20:47:37
the GD expansion or the FE expansion.

32
20:47:37 --> 20:47:43
So we calculate the distance of F,
G and E from B.

33
20:47:43 --> 20:47:46
BF is 20, BG is 80, and BE is 15.

34
20:47:46 --> 20:47:49
Now we'll apply the formula like this.

35
20:47:49 --> 20:47:53
For the FG case,
we subtract the BF distance

36
20:47:53 --> 20:47:57
from the weight and
add the BD distance to the weight.

37
20:47:57 --> 20:48:02
This gives us 70 because G is far from B.

38
20:48:02 --> 20:48:06
Similarly for the FE case,
we subtract the BF distance and

39
20:48:06 --> 20:48:10
add the BE distance to the weight and
it gives us 15.

40
20:48:10 --> 20:48:13
Now, with these modified weights
we choose the FE expansion.

41
20:48:13 --> 20:48:19
In practice this significantly improves
the actual performance of the algorithm.

42
20:48:19 --> 20:48:23
So, this technique is used by many online
mapping services when they give you

43
20:48:23 --> 20:48:23
directions.

1
17:34:13 --> 17:34:17
So now let's bring back the constraints
that we have ignored so far.

2
17:34:18 --> 17:34:23
There are two constraints here,
parts of the graph to include and

3
17:34:23 --> 17:34:25
parts of the graph to exclude.

4
17:34:25 --> 17:34:29
In this example we have to go to B, but

5
17:34:29 --> 17:34:34
we should go to J first, and
then travel from B to J.

6
17:34:35 --> 17:34:38
Also, we cannot use any
of the paths through E.

7
17:34:39 --> 17:34:41
This really means two things.

8
17:34:42 --> 17:34:48
First, we split the problem into two
independent shortest path problems

9
17:34:48 --> 17:34:51
that we can solve in parallel if needed.

10
17:34:51 --> 17:34:56
Second, when we go from J
to B We need to extract

11
17:34:57 --> 17:35:01
the useful subgraph that
we need to consider.

12
17:35:01 --> 17:35:04
For a large network and
a complex exclusion condition,

13
17:35:06 --> 17:35:09
we will essentially operate
over a smaller graph,

14
17:35:09 --> 17:35:11
thereby reducing the effective
size of the problem.

15
17:35:13 --> 17:35:17
As we'll see in module three,
this kind of subgraph extraction operation

16
17:35:17 --> 17:35:22
can be done effectively and
efficiently with a graph database system.

17
17:35:22 --> 17:35:25
This concludes our short
tour of path analytics.

1
11:09:38 --> 11:09:43
So, in this optional module, we'll look
at the two key player problems again.

2
11:09:45 --> 11:09:49
The goal of the first problem is
to identify a small set of nodes,

3
11:09:49 --> 11:09:53
whose removal will create
maximum disruption.

4
11:09:54 --> 11:09:59
Now, in this case, a traditional
centrality algorithm may not work,

5
11:09:59 --> 11:10:02
because the optimization goal
is to break up the network.

6
11:10:04 --> 11:10:06
So we need a quantitive
measure of the breakage.

7
11:10:08 --> 11:10:10
If dij is the distance between nodes i and

8
11:10:10 --> 11:10:15
j, then 1 over dij is
the closeness of these two nodes.

9
11:10:17 --> 11:10:21
If we add the closeness of all nodes and
normalize it by the number of node pairs,

10
11:10:21 --> 11:10:25
we'll get a measure of
cohesiveness as a fraction.

11
11:10:26 --> 11:10:32
So, 1 minus this value is
a measure of fragmentation.

12
11:10:33 --> 11:10:36
Our goal is to maximize
this fragmentation metric.

13
11:10:38 --> 11:10:44
In the model terrorist network shown here,
removing the red nodes A,

14
11:10:44 --> 11:10:49
B, and C will break up the network
into seven components,

15
11:10:49 --> 11:10:52
with F reaching a value of 0.59.

16
11:10:52 --> 11:10:58
The second key player problem, is trying
to find a group of S influencers,

17
11:10:58 --> 11:11:02
which can reach a maximum
number of nodes within K steps.

18
11:11:03 --> 11:11:07
The number of unique nodes reachable from
a starting node is called the reach,

19
11:11:07 --> 11:11:08
of the starting node.

20
11:11:10 --> 11:11:15
For this we need to adapt the concept
of reach to limit it to k steps.

21
11:11:16 --> 11:11:21
We also need to adapt it to measure
the distance of an arbitrary node

22
11:11:21 --> 11:11:23
from a group of nodes
between our influences.

23
11:11:25 --> 11:11:31
The distance from one node to a group
of nodes can be defined as a maximum,

24
11:11:31 --> 11:11:36
or average, or minimum distance,
of the node from the members of the group.

25
11:11:37 --> 11:11:39
Often, the minimum
distance is a good choice.

26
11:11:41 --> 11:11:43
So the distance, we could reach,

27
11:11:43 --> 11:11:48
can then be through of, as the proportion
of all nodes reached by the group,

28
11:11:48 --> 11:11:50
where the nodes are weighted
by the distance from the set.

29
11:11:52 --> 11:11:55
And only nodes at distance
1 are given full rate.

30
11:11:56 --> 11:12:00
Hence, a distance we could reach
that use a maximum value of 1,

31
11:12:00 --> 11:12:05
where every outside node is adjacent to at
least one member of the set of influences.

32
11:12:06 --> 11:12:11
In the network shown,
just three nodes, A, C, and D,

33
11:12:11 --> 11:12:16
are sufficient to reach every other
member within just four steps.

34
11:12:18 --> 11:12:22
Now this concludes this module, where we
looked at several analytic techniques and

35
11:12:22 --> 11:12:26
measures to extract different
kinds of insights from a network.

1
22:22:08 --> 22:22:12
We saw how a community
in a graph can evolve.

2
22:22:13 --> 22:22:15
To track the nature of evolution,

3
22:22:15 --> 22:22:19
we need to measure how
the community changes over time.

4
22:22:20 --> 22:22:22
So here are three cases.

5
22:22:22 --> 22:22:29
One, two and three of a community
changing between two observation points.

6
22:22:29 --> 22:22:35
The goal is to figure out whether these
are normal fluctuations in the network or

7
22:22:35 --> 22:22:36
are more drastic changes occurring.

8
22:22:38 --> 22:22:38
Look at them for a second.

9
22:22:40 --> 22:22:45
Just visually, the first case
seems to show just minor changes.

10
22:22:45 --> 22:22:51
Whereas case two shows a merger,
and case three shows a split.

11
22:22:54 --> 22:22:58
Now, to come up with a quantitative
measure of change over time, we need to

12
22:22:58 --> 22:23:03
take two observations from two consecutive
time points and fuse the graph.

13
22:23:04 --> 22:23:07
If you do it for case one,
you'll find one new node,

14
22:23:09 --> 22:23:11
one living node, and
the rest will come on over time.

15
22:23:13 --> 22:23:18
For case two, you will see
that two previous communities,

16
22:23:18 --> 22:23:23
colored differently, are internally
connected the same way as before.

17
22:23:23 --> 22:23:27
But some members of the two communities
have created new crosslinks.

18
22:23:28 --> 22:23:31
Now can you tell me what
you observe in case three?

19
22:23:31 --> 22:23:35
Well I see one join node, color purple.

20
22:23:36 --> 22:23:40
Apart from it, there are just two
edges connecting the two groups.

21
22:23:41 --> 22:23:46
Now with these observations,
we can now compute the autocorrelation

22
22:23:46 --> 22:23:50
between the graphs across time t and
t plus 1.

23
22:23:50 --> 22:23:55
This is just a measure of
the number of common nodes

24
22:23:55 --> 22:23:58
divided by the number of
nodes in the combined graph.

25
22:23:59 --> 22:24:03
If the community does not change at all,
this number is 1.

26
22:24:03 --> 22:24:06
If a community has only a few connection,
the number is lower.

27
22:24:08 --> 22:24:13
After computing autocorrelation
over every pair of time steps,

28
22:24:13 --> 22:24:15
we can then compute stationarity,

29
22:24:16 --> 22:24:21
which measures the overall change in
the autocorrelation over a period of time.

30
22:24:22 --> 22:24:25
So if we measure over 100 time steps,

31
22:24:25 --> 22:24:31
we will add the 99 correlation values
from the steps and then divide it by 99.

32
22:24:31 --> 22:24:36
This will tell us what fraction
of members remain unchanged

33
22:24:36 --> 22:24:38
on an average over these 100 time steps.

34
22:24:39 --> 22:24:44
Therefore, the 1 minus zeta tells
us the average ratio of members

35
22:24:44 --> 22:24:45
that are changed in a time step.

36
22:24:48 --> 22:24:50
Let's take three cases.

37
22:24:51 --> 22:24:55
In the first plot,
the size of the graph is small and

38
22:24:55 --> 22:24:56
nothing much is happening here.

39
22:24:56 --> 22:25:01
A note occasionally joins or leaves,
keeping the stationarity pretty flat.

40
22:25:03 --> 22:25:09
In the second case, the graph is small,
but there are a lot of changes, especially

41
22:25:09 --> 22:25:14
at time step seven a whole bunch of purple
nodes have joined and then they went away.

42
22:25:15 --> 22:25:18
The size of the graph
clearly reflects this

43
22:25:18 --> 22:25:22
with a purple spike that you see
on the size versus the time graph.

44
22:25:25 --> 22:25:28
This spike on the time series
by the way is called a burst.

45
22:25:28 --> 22:25:34
The third plot shows a large graph
with many nodes joining and leaving.

46
22:25:35 --> 22:25:40
The stationarity of this graph will be
quite low given the abrupt changes we

47
22:25:40 --> 22:25:42
observe over time.

1
20:47:47 --> 20:47:51
In this video,
we'll discuss what paths are and

2
20:47:51 --> 20:47:54
how to find your way as it travels
along the nodes and edges of the graph.

3
20:47:56 --> 20:47:57
Let's start with an example.

4
20:47:58 --> 20:47:59
For this example,

5
20:47:59 --> 20:48:03
we'll consider an edge weighted
graph with no loops as shown here.

6
20:48:03 --> 20:48:07
Term edge weighted means
the edges have weights.

7
20:48:08 --> 20:48:12
The weight of the edge I to J is 15.

8
20:48:12 --> 20:48:16
We can think of this graph as a small road
network, where the nodes are cities, and

9
20:48:16 --> 20:48:19
the edge weights are highway
distances between them.

10
20:48:20 --> 20:48:24
To walk, or traverse, through this
graph we'll define what a walk is.

11
20:48:25 --> 20:48:29
A walk is an arbitrary
sequence of nodes and

12
20:48:29 --> 20:48:32
edges that starts from some node and
ends on some node.

13
20:48:32 --> 20:48:38
Here we can go from H to F

14
20:48:38 --> 20:48:44
to G to C to

15
20:48:44 --> 20:48:51
F to E to B.

16
20:48:51 --> 20:48:55
Notice that in this walk,
we went through the node F twice.

17
20:48:57 --> 20:49:02
In many applications, we do not want
to consider arbitrary walks but

18
20:49:02 --> 20:49:05
consider a walk where
we do not repeat nodes

19
20:49:05 --> 20:49:07
unless we need to come back
to the starting point.

20
20:49:08 --> 20:49:11
Such a constrained walk is called a path.

21
20:49:11 --> 20:49:14
The green arrows indicate
a path from J to B.

22
20:49:16 --> 20:49:20
We said on the last slide that a path
can start from a node and end on it.

23
20:49:20 --> 20:49:25
Such a path is called a cycle when
the path has a three or more nodes.

24
20:49:25 --> 20:49:29
The J, G, C,

25
20:49:29 --> 20:49:34
F, J, is a four node cycle,
sometimes called a four cycle.

26
20:49:35 --> 20:49:38
C, F, E is a three cycle.

27
20:49:39 --> 20:49:45
However, although there is an edge from
F to J and another from J back to F,

28
20:49:45 --> 20:49:49
this two node path is not
a cycle by our definition.

29
20:49:50 --> 20:49:54
A network with no cycles
is called acyclic.

30
20:49:56 --> 20:49:59
A graph that is both directed and

31
20:49:59 --> 20:50:04
acyclic is called a directed
acyclic graph, in short, a deck.

32
20:50:05 --> 20:50:12
A trail is a concept similar to a path,
it is a walk with no repeating edges.

33
20:50:13 --> 20:50:19
In the graph shown in the walk,
H, F, G, C, F, E,

34
20:50:19 --> 20:50:24
C, F, J, is not a trail.

35
20:50:24 --> 20:50:27
Because C, F Is traversed twice.

36
20:50:29 --> 20:50:32
Remember the seven bridges of
the Konigsberg problem that we described

37
20:50:32 --> 20:50:33
in module one?

38
20:50:33 --> 20:50:34
In that problem,

39
20:50:34 --> 20:50:38
we had the constraint that one
cannot cross the same bridge twice.

40
20:50:40 --> 20:50:43
Often we see such constraints
in path planning problems.

41
20:50:44 --> 20:50:48
A common notion in directive
graphs is called reachability.

42
20:50:49 --> 20:50:54
If there is a path from node U to node V,
V is reachable from U.

43
20:50:55 --> 20:50:56
Reachability is not symmetric.

44
20:50:57 --> 20:51:01
Even if V is reachable from U,
U may or may not be reachable from V.

45
20:51:02 --> 20:51:07
As we can see in this graph,
I is not reachable from A.

46
20:51:09 --> 20:51:11
Look at A more carefully.

47
20:51:11 --> 20:51:13
There is no outgoing edge from A at all.

48
20:51:14 --> 20:51:17
So no node in this graph
is reachable from A.

49
20:51:18 --> 20:51:21
A is a terminal, a leaf node of the graph.

50
20:51:23 --> 20:51:25
Where do we see this in real life?

51
20:51:25 --> 20:51:29
Think of road network with one way streets
and road blocks through the construction.

52
20:51:30 --> 20:51:32
This can make some areas
unreachable by cars.

53
20:51:34 --> 20:51:37
In biology,
there are gene regulation networks,

54
20:51:37 --> 20:51:39
in which the nodes represent genes and

55
20:51:39 --> 20:51:44
edges represent the fact that gene
A regulates gene B and not visa verse.

56
20:51:45 --> 20:51:46
This makes it a directed graph.

57
20:51:48 --> 20:51:52
We can easily think of a case where
A regulates B, and B regulates C, but

58
20:51:52 --> 20:51:55
C does not regulate B or A.

59
20:51:55 --> 20:51:58
That's making A unreachable from C.

60
20:51:58 --> 20:52:01
The final concept you will
explore in this graph,

61
20:52:01 --> 20:52:05
in this lecture,
is that of a graph diameter.

62
20:52:05 --> 20:52:10
The diameter of a graph measures the
maximum number of steps, also called hops,

63
20:52:10 --> 20:52:15
want us to traverse to go to
the most distant node in the graph.

64
20:52:16 --> 20:52:19
That means,
if you go from an arbitrary node

65
20:52:19 --> 20:52:25
to any other arbitrary node in the graph,
following only the shortest paths roots,

66
20:52:25 --> 20:52:27
what is the maximum number
of steps you have to take?

67
20:52:29 --> 20:52:30
Let's see how this is computed.

68
20:52:31 --> 20:52:36
For this task, we'll create a matrix
called the shortest hop distance matrix.

69
20:52:37 --> 20:52:43
Just like the adjacency matrix, the rows
and columns here represent graph nodes.

70
20:52:44 --> 20:52:47
And each cell contains the distance
from the I eighth node,

71
20:52:47 --> 20:52:49
to the G eighth node
via the shortest path.

72
20:52:51 --> 20:52:55
The distance from any
node to itself is zero.

73
20:52:55 --> 20:52:58
So all diagonal elements
of the matrix are zero.

74
20:53:00 --> 20:53:04
If a node I cannot reach
node J through any path,

75
20:53:04 --> 20:53:07
the distance is marked as infinity.

76
20:53:07 --> 20:53:11
Thus, the distance from
B to C is infinity.

77
20:53:11 --> 20:53:16
You can go from E to F in two steps,
E to C to F.

78
20:53:16 --> 20:53:22
Therefore, the wait at the C cell is two.

79
20:53:22 --> 20:53:27
If you fill this matrix, you will
notice that the largest reachable value

80
20:53:27 --> 20:53:30
is 4 in doing the infinite distance.

81
20:53:31 --> 20:53:36
Thus, the diameter of this graph is
4 where It represents the part from I

82
20:53:36 --> 20:53:41
to A, through J, G and D.

83
20:53:41 --> 20:53:45
There are a few noteworthy
items in this Distance Matrix.

84
20:53:45 --> 20:53:51
The row of A has a value of 0 with itself
and infinity for every other node.

85
20:53:51 --> 20:53:54
This happens because A is a leaf node.

86
20:53:55 --> 20:53:59
Similarly, except for
the 0 distance with itself,

87
20:53:59 --> 20:54:03
the H column has infinity for
all of the nodes.

88
20:54:03 --> 20:54:07
This happens because H
has no incoming edges and

89
20:54:07 --> 20:54:11
therefore no other node can reach H.

1
17:41:59 --> 17:42:03
So we have seen how to compute
degree histograms of a graph.

2
17:42:04 --> 17:42:08
While degree histograms are useful
to characterize a graph,

3
17:42:08 --> 17:42:10
it is usually a means to an end.

4
17:42:11 --> 17:42:15
It's a known practice in statistics to
compute a mathematical expression for

5
17:42:15 --> 17:42:19
a statistical distribution
Using histograms.

6
17:42:20 --> 17:42:25
For graphs, we often look for a function
to describe the degree distribution.

7
17:42:26 --> 17:42:30
But this is expressed as
a distribution of the probability

8
17:42:30 --> 17:42:35
that a random vertex will
have exactly k neighbors.

9
17:42:35 --> 17:42:37
Now, this problem has been
investigated by many.

10
17:42:38 --> 17:42:42
One popular, well known model,
is a Power Law.

11
17:42:43 --> 17:42:49
A graph follows a Power Law if,
the best probability is given by k,

12
17:42:49 --> 17:42:52
erased to a negative
exponent called alpha.

13
17:42:53 --> 17:42:58
The value of alpha, for many practical
networks, is between two and three.

14
17:43:00 --> 17:43:01
More recently,

15
17:43:01 --> 17:43:04
people have suggested other distributions
that look like the power law graph.

16
17:43:05 --> 17:43:09
One of them is a log-normal graph.

17
17:43:09 --> 17:43:13
Here, the logarithm of k has
a Gaussian distribution.

18
17:43:13 --> 17:43:17
So this log-normal distribution
seems to have the better fit to

19
17:43:17 --> 17:43:20
natural graphs that are observed.

20
17:43:22 --> 17:43:24
So, why are power law graphs important?

21
17:43:25 --> 17:43:28
Interestingly, many very,

22
17:43:28 --> 17:43:31
very different real life graphs in
the world seem to follow the power law.

23
17:43:33 --> 17:43:38
If a graph does follow the power law, it
would have one large connected component

24
17:43:38 --> 17:43:46
with a very high proportion of notes
connected to it In a power law graph,

25
17:43:47 --> 17:43:51
most nodes have a low degree and
some nodes will be disconnected.

26
17:43:51 --> 17:43:56
The low degree nodes belong to
a very dense sub graphs and

27
17:43:56 --> 17:43:58
those sub graphs are connected
to each other through hubs.

28
17:44:00 --> 17:44:03
In the center of the graph
has a high density,

29
17:44:03 --> 17:44:06
power law graphs can be
difficult to compute with.

30
17:44:06 --> 17:44:10
For example, the shortest path
algorithm operating in the dense part

31
17:44:10 --> 17:44:15
will possibly be very inefficient, not
because of the size of the network, but

32
17:44:15 --> 17:44:19
because there are too many paths to
explore inside the denser pieces.

33
17:44:24 --> 17:44:28
The more interesting reason why
people study power-law graphs

34
17:44:28 --> 17:44:32
is because power-law graphs
are supposed to be robust.

35
17:44:33 --> 17:44:39
So in nature, all biological networks
show power-law graphs because

36
17:44:39 --> 17:44:44
It gives you a high rate of redundancy
against failure and attacks.

1
11:26:41 --> 11:26:46
The most primitive path analytics
question one can ask is to find the best

2
11:26:46 --> 11:26:48
path from node one to node two.

3
11:26:49 --> 11:26:52
What does best mean?

4
11:26:52 --> 11:26:54
Well, that depends on the actual
needs of the application.

5
11:26:54 --> 11:26:57
But in general, to specify the best path,

6
11:26:57 --> 11:27:02
we need to define when one
path is better than another.

7
11:27:02 --> 11:27:09
This is usually expressed in
terms of an optimization problem.

8
11:27:09 --> 11:27:13
Where we need to minimize and maximize
our subfunction subject to constraints.

9
11:27:13 --> 11:27:15
What kind of constraints?

10
11:27:15 --> 11:27:20
Two common criteria for graphs are,
inclusion and exclusion conditions.

11
11:27:21 --> 11:27:27
Inclusion criteria may specify which
nodes we have to include in the path.

12
11:27:27 --> 11:27:31
And exclusion criteria
specifies which nodes and

13
11:27:31 --> 11:27:33
edges should be excluded from the path.

14
11:27:33 --> 11:27:38
In addition,
one specify a preference criteria

15
11:27:38 --> 11:27:42
that act as a softer or
less strict constraint.

16
11:27:42 --> 11:27:45
For example, we would like to
minimize highways on my trip.

17
11:27:45 --> 11:27:46
Or avoid condition.

18
11:27:47 --> 11:27:52
These are soft because although
the users would like to have them

19
11:27:52 --> 11:27:56
enforced completely, it is all right
if they are not fully enforced.

20
11:27:56 --> 11:28:01
A good practical use case occurs when I'm
trying to drive to work in the morning.

21
11:28:01 --> 11:28:06
Ideally, I would like to take a path
having the shortest distance from my home.

22
11:28:06 --> 11:28:11
For example, node I, to my workplace,
which is node B in the graph.

23
11:28:12 --> 11:28:16
But I have to drop off my son at school.

24
11:28:16 --> 11:28:20
So my path must include his school,
the J here.

25
11:28:22 --> 11:28:26
However, I would like to avoid roads
around the new construction that's

26
11:28:26 --> 11:28:30
happening about five miles from my
workplace, like the node E in the graph.

27
11:28:32 --> 11:28:36
Because there is usually a huge traffic
delay around that construction site.

28
11:28:37 --> 11:28:40
I could also add
a preference criteria like

29
11:28:40 --> 11:28:42
I don't prefer to drive on the highway.

30
11:28:42 --> 11:28:44
But for this discussion,
we'll skip the preference idea.

31
11:28:44 --> 11:28:47
Too complicated?

32
11:28:47 --> 11:28:51
Okay, let's start with a simpler problem.

33
11:28:51 --> 11:28:56
To start with, let's drop the constraints
and look at the problem with just

34
11:28:56 --> 11:29:01
the optimization part of the problem,
having a single variable.

35
11:29:03 --> 11:29:08
In our case, that variable is the sum
of edge weights from the source,

36
11:29:08 --> 11:29:12
that is the starting node I,
to the target, which is B.

37
11:29:14 --> 11:29:19
This problem is handled by all mapping and
road direction software.

38
11:29:19 --> 11:29:22
Here is a Google map screenshot,
in which I am trying to go from my home

39
11:29:22 --> 11:29:26
in North San Diego,
to a collision center in a nearby city.

40
11:29:28 --> 11:29:31
Google Maps shows three different routes,
and

41
11:29:31 --> 11:29:34
highlights one as a preferred solution.

42
11:29:35 --> 11:29:41
You should readily see that the real
shortest path of 26.6 miles

43
11:29:41 --> 11:29:45
will take the longest time at the time of
the day when I was looking at the map.

44
11:29:46 --> 11:29:51
So this means the weights here are not
raw distances but estimated travel time.

45
11:29:51 --> 11:29:54
You should also notice the blue, red, and

46
11:29:54 --> 11:29:58
orange segments in the preferred
path are presented by Google.

47
11:29:58 --> 11:30:03
The orange and red street segments
clearly represent congestion areas and

48
11:30:03 --> 11:30:07
therefore have higher weight
than the blue segments.

49
11:30:08 --> 11:30:13
Therefore, the weights of the street
segments are not really static but

50
11:30:13 --> 11:30:16
change with many other factors,
like weather or the time of the day.

51
11:30:17 --> 11:30:22
This is why the least weight path problem
is an important problem to solve for

52
11:30:22 --> 11:30:23
the benefit of the commuter.

53
11:30:24 --> 11:30:27
A widely applied algorithm
that is applied for

54
11:30:27 --> 11:30:30
shortest path problems is
called Dijkstra's algorithm.

55
11:30:32 --> 11:30:37
Originally, Dijkstra considered a variant
of the problem where the task is

56
11:30:37 --> 11:30:41
to find the shortest path from a single
source node to all other nodes.

57
11:30:43 --> 11:30:45
We'll go through the algorithm here.

58
11:30:45 --> 11:30:49
However, there are many good online
resources including tutorials and

59
11:30:49 --> 11:30:51
YouTube videos describing the algorithm.

60
11:30:52 --> 11:30:56
For our discussion, we'll confine
ourselves to the case where both

61
11:30:56 --> 11:30:59
the source and
the target nodes are known in advance.

1
22:57:40 --> 22:57:46
As you probably already know, Cypher is
the scripting language used in Neo4j and

2
22:57:46 --> 22:57:50
it's what we have been using
already in the previous lectures.

3
22:57:50 --> 22:57:55
In this lecture, we're going to go through
a series of basic queries using Cypher

4
22:57:55 --> 22:57:58
with the focus on the data sets
that we've already been using.

5
22:57:58 --> 22:58:02
Here's a listing of the basic queries
we will go through step by step.

6
22:58:11 --> 22:58:12
So let's get started.

7
22:58:16 --> 22:58:22
To keep things simple we will be using a
text file containing the basic queries and

8
22:58:22 --> 22:58:27
we'll make this file available for
download as a reading in the module.

9
22:58:27 --> 22:58:31
I will briefly review the queries and

10
22:58:31 --> 22:58:36
then we'll toggle to the browser to view
the results of the queries in Neo4j.

11
22:58:37 --> 22:58:43
Neo4j has a very nice feature which
allows us to retain individual queries

12
22:58:43 --> 22:58:49
in multiple panels as we're going through
the process of exploring each query.

13
22:58:49 --> 22:58:53
So for the first few queries we'll be
using the simple road network data set

14
22:58:53 --> 22:58:56
that we used in previous demonstrations.

15
22:58:56 --> 22:58:58
And here we've already loaded
the data set into Neo4j.

16
22:58:59 --> 22:59:01
So let's look at our first query.

17
22:59:01 --> 22:59:03
Our first query is a very a simple one,

18
22:59:03 --> 22:59:07
in which we're counting the number
of nodes in the network.

19
22:59:07 --> 22:59:11
So the first line of code simply matches
all of the nodes with the label MyNode and

20
22:59:11 --> 22:59:14
it returns a count of those nodes.

21
22:59:14 --> 22:59:16
So let's look at the results.

22
22:59:16 --> 22:59:20
The results are very simple,
the value of eleven.

23
22:59:20 --> 22:59:23
And we can visually confirm this
by inspecting the graph itself and

24
22:59:23 --> 22:59:26
see it has 11 nodes and
we can see that up here.

25
22:59:29 --> 22:59:34
The next query is almost as simple,
we want to count the number of edges.

26
22:59:34 --> 22:59:39
Now, first thing to keep in mind
is in order to count edges,

27
22:59:39 --> 22:59:43
we also need to declare nodes that
are associated with those edges.

28
22:59:43 --> 22:59:47
So this first line of codes
includes a declaration

29
22:59:47 --> 22:59:49
of the nodes associated with the edges.

30
22:59:49 --> 22:59:52
Here we're identifying
with thevariable r and

31
22:59:52 --> 22:59:54
then we're returning
a count of those edges.

32
22:59:55 --> 23:00:00
So let's take a look in Neo4j and the
results of this query are a value of 14.

33
23:00:00 --> 23:00:04
And once again we can confirm this
visually by looking at our network and

34
23:00:04 --> 23:00:06
counting the number of edges.

35
23:00:09 --> 23:00:15
The next query involves finding all
of the leaf nodes in the network.

36
23:00:15 --> 23:00:17
Now as you may remember
from a previous lecture,

37
23:00:17 --> 23:00:22
leaf nodes are defined as those
nodes which have no outgoing edges.

38
23:00:24 --> 23:00:28
Notice that we're returning the node
represented by the variable M,

39
23:00:28 --> 23:00:30
which is the target node in this query.

40
23:00:30 --> 23:00:35
So here we're matching all nodes
associated with edges having the label two

41
23:00:36 --> 23:00:39
and we want to place
a constraint on those nodes,

42
23:00:39 --> 23:00:42
such that they have no outgoing edges.

43
23:00:42 --> 23:00:43
And then we return all those nodes.

44
23:00:44 --> 23:00:47
So let's take a look at the results.

45
23:00:47 --> 23:00:50
We're going to see a single node returned,
the node with the label p.

46
23:00:52 --> 23:00:57
And if we inspect our graph, we can
see over here on the right the node p,

47
23:00:57 --> 23:01:00
and sure enough it has no outgoing edges,
only one incoming edge.

48
23:01:04 --> 23:01:06
The next query is also similar,

49
23:01:06 --> 23:01:09
we're looking for
root nodes instead of leaf notes.

50
23:01:09 --> 23:01:10
And as you may also remember,

51
23:01:10 --> 23:01:15
root nodes are defined as a node
which has no incoming edges.

52
23:01:16 --> 23:01:20
You may notice that this segment of
the first line of code is sort of a mirror

53
23:01:20 --> 23:01:26
image of the same segment in the first
line of code from the previous query.

54
23:01:26 --> 23:01:30
We also want to place the constraint
on the nodes that we want to return

55
23:01:30 --> 23:01:34
by specifying that they can
have no incoming edges.

56
23:01:34 --> 23:01:38
And then, we return all of those nodes
When we look at our results in Neo4j.

57
23:01:38 --> 23:01:40
Once again,

58
23:01:40 --> 23:01:44
we see we only get a single node
return it's the node with the label H.

59
23:01:45 --> 23:01:49
And if we inspect our graph,
we see over here the node H and

60
23:01:49 --> 23:01:51
sure enough it has no incoming edges.

61
23:01:51 --> 23:01:55
Only a single outgoing edge which
means it's a root node of the network.

62
23:01:58 --> 23:02:03
The next query can be described
as a pattern matching query.

63
23:02:03 --> 23:02:06
We're looking for a pattern that
we're describing as triangles.

64
23:02:06 --> 23:02:09
This is a pattern that's
a bit more complex than

65
23:02:09 --> 23:02:13
the patterns we've been looking for
in our previous queries.

66
23:02:13 --> 23:02:18
A triangle can also be described as a
three cycle, consisting of three nodes and

67
23:02:18 --> 23:02:23
three edges where the beginning and
end node are the same.

68
23:02:23 --> 23:02:27
So here we're matching a node A which
goes through an edge to node B

69
23:02:27 --> 23:02:31
which goes through a second
edge to a second node C and

70
23:02:31 --> 23:02:34
through a third edge back
to the original node A.

71
23:02:34 --> 23:02:37
And then we return all of those notes.

72
23:02:37 --> 23:02:39
Let's look at our results.

73
23:02:39 --> 23:02:43
And here we see we have five
distinct nodes returned.

74
23:02:43 --> 23:02:46
And we have two triangles or
two three cycles.

75
23:02:46 --> 23:02:50
From D to E to G and from D to C to B.

76
23:02:53 --> 23:02:57
Finally the last query we're going
to execute with this data set,

77
23:02:57 --> 23:03:01
is going to explore the neighborhood
of a particular node.

78
23:03:01 --> 23:03:03
In this case, the node with the label d.

79
23:03:03 --> 23:03:07
And we're going to be looking for
what we're calling second neighbors of D.

80
23:03:07 --> 23:03:11
This means nodes that
are two nodes away from d.

81
23:03:11 --> 23:03:16
So the first line of code matches
all nodes that are two nodes away

82
23:03:16 --> 23:03:18
from a specific node.

83
23:03:18 --> 23:03:23
And then the second line of codes
specifies the actual node that we

84
23:03:23 --> 23:03:27
we want to consider by constraining
its name to have the label d,

85
23:03:27 --> 23:03:30
and then we return those nodes.

86
23:03:30 --> 23:03:33
And here we're using the command distinct

87
23:03:33 --> 23:03:36
because we want to make sure that we
don't return any duplicate nodes.

88
23:03:36 --> 23:03:38
All of our nodes must be unique.

89
23:03:39 --> 23:03:40
So let's look at the results.

90
23:03:42 --> 23:03:47
And here we have a network that
consists of nine nodes and 11 edges and

91
23:03:47 --> 23:03:53
we can see that each node is
two nodes away from the node D.

92
23:03:53 --> 23:03:57
Some nodes appear to be only one
node away from the node D but

93
23:03:57 --> 23:04:01
we can get to those nodes indirectly
through another node, which means that

94
23:04:01 --> 23:04:04
they're not only a first neighbor but
they're also a second neighbor.

95
23:04:06 --> 23:04:08
So we encourage you to play
around with these queries,

96
23:04:08 --> 23:04:12
and make minor changes with them,
and see what the results might be.

1
22:01:53 --> 22:01:58
For our next three queries,
we're going to switch to the terrorist

2
22:01:58 --> 22:02:00
data set that we had used in
our previous demonstration.

3
22:02:02 --> 22:02:07
Our first query involves finding
the types of a particular node.

4
22:02:07 --> 22:02:11
So, as you may recall in the terrorist
data set there were node types

5
22:02:11 --> 22:02:14
corresponding to a country.

6
22:02:14 --> 22:02:17
So, in this case we
want to match all nodes,

7
22:02:17 --> 22:02:20
where the name is equal to Afghanistan.

8
22:02:20 --> 22:02:25
And we will return the labels for
that particular match.

9
22:02:25 --> 22:02:27
And the results for
this query are relatively simple.

10
22:02:27 --> 22:02:33
The label for the node named Afghanistan
is country, as you might expect.

11
22:02:36 --> 22:02:40
Next, we're going to do something
similar but with an edge.

12
22:02:40 --> 22:02:43
In this case, we want to find the label

13
22:02:43 --> 22:02:48
of a edge associated with
a node named Afghanistan.

14
22:02:48 --> 22:02:53
And also, if you can recall from that
particular network, the label for

15
22:02:53 --> 22:03:00
an edge associated with a country named
Afghanistan would be an IS_FROM label.

16
22:03:03 --> 22:03:08
And the final query, we would like to
demonstrate with this terrorist data set,

17
22:03:08 --> 22:03:11
involves finding all of
the properties of a node.

18
22:03:12 --> 22:03:16
Now, based on how we defined
our data import script,

19
22:03:17 --> 22:03:21
we defined all of our
nodes to be of type Actor.

20
22:03:21 --> 22:03:25
So in this query, we're going to
search for all nodes of type Actor and

21
22:03:25 --> 22:03:30
then, we're going to return all of the
properties associated with those nodes.

22
22:03:30 --> 22:03:31
But, since there are thousands of nodes,

23
22:03:31 --> 22:03:34
we're going to limit
the results to the first 20.

24
22:03:34 --> 22:03:37
So, let's go ahead and submit our query.

25
22:03:38 --> 22:03:40
And here are the results.

26
22:03:40 --> 22:03:46
Each node represents a terrorist and
if we look at the rows data

27
22:03:46 --> 22:03:52
behind the scenes we can see the various
properties associated with each node.

28
22:03:52 --> 22:03:53
Each node has a name.

29
22:03:53 --> 22:03:57
Each node has aliases,
one or more aliases.

30
22:03:57 --> 22:03:59
And they have a type property.

31
22:03:59 --> 22:04:03
Now, depending on how we defined our
import script in the first place,

32
22:04:03 --> 22:04:06
these nodes could have
different types of properties.

33
22:04:06 --> 22:04:09
In this case, we only defined them
to have three different properties.

34
22:04:14 --> 22:04:18
For our next two queries,
we're going to use a different data set,

35
22:04:18 --> 22:04:23
it's a biological data set consisting
of genetics data representing

36
22:04:23 --> 22:04:25
interactions between genes.

37
22:04:25 --> 22:04:28
We will be providing the data for
you to download.

38
22:04:28 --> 22:04:31
In fact, there are two separate data sets.

39
22:04:31 --> 22:04:36
One is a complete data set
consisting of 250,000 rows,

40
22:04:36 --> 22:04:40
and the other data set consists
of the first 50,000 rows,

41
22:04:40 --> 22:04:43
and that's what we'll be using for
this demonstration.

42
22:04:43 --> 22:04:47
So, let's take a look at
a small sample of the data.

43
22:04:47 --> 22:04:52
Here you see a graph network
in which each node is a gene.

44
22:04:52 --> 22:04:55
And each edge represents
the association between genes.

45
22:04:57 --> 22:05:02
To load this data set, this is the smaller

46
22:05:02 --> 22:05:07
data set, required 1446 seconds,

47
22:05:07 --> 22:05:11
and it consists of 9656 labels and

48
22:05:11 --> 22:05:15
46621 relationships.

49
22:05:15 --> 22:05:20
So, the first query involves finding
loops in the data which represent genes

50
22:05:20 --> 22:05:22
that have associations
with there own types.

51
22:05:24 --> 22:05:27
So, it's a very simple query
in which the source node and

52
22:05:27 --> 22:05:31
the target node are the same and we'll
be returning those along the edges and

53
22:05:31 --> 22:05:33
we'll limit our results to the first ten.

54
22:05:33 --> 22:05:35
So, here are the results of our query.

55
22:05:35 --> 22:05:37
We can see a few different loops.

56
22:05:38 --> 22:05:44
If we look at the rows, our query returned
not only the node but the edge type.

57
22:05:45 --> 22:05:49
So, on the left we see the particular
gene and on the right,

58
22:05:49 --> 22:05:51
we see the type of association.

59
22:05:51 --> 22:05:53
And there are a range of different types.

60
22:05:57 --> 22:06:00
So, this data set also
contains multigraphs.

61
22:06:00 --> 22:06:04
If you recall from a previous lecture,
the definition of a multigraph

62
22:06:04 --> 22:06:09
is any two nodes which have two or
more edges between them.

63
22:06:09 --> 22:06:14
So in this case, we'll be matching
two separate node edge relationships.

64
22:06:14 --> 22:06:19
And we'll apply a constraint in which
the edges must be different for

65
22:06:19 --> 22:06:20
the same pairs of nodes.

66
22:06:20 --> 22:06:24
And then we will return those nodes and
those edges.

67
22:06:24 --> 22:06:26
We'll limit our results to the first ten.

68
22:06:26 --> 22:06:28
And here's the results.

69
22:06:28 --> 22:06:33
Here we see a set of four genes, and
there are two pairs that have three

70
22:06:33 --> 22:06:38
edges between them and another pair
that has four edges between them.

71
22:06:42 --> 22:06:46
Our final query addresses something that
is not necessarily been fully covered

72
22:06:46 --> 22:06:49
in previous lectures, but
it is useful enough to address here and

73
22:06:49 --> 22:06:52
you'll get an understanding of it by
going through this query example.

74
22:06:54 --> 22:06:58
We're going to essentially
extract a subset of nodes and

75
22:06:58 --> 22:07:02
edges from the graph that
we've been working with.

76
22:07:02 --> 22:07:04
And this is called an induced subgraph.

77
22:07:05 --> 22:07:10
In which, if we provide a set of nodes,
we want to return

78
22:07:10 --> 22:07:14
the network that consists only of those
nodes and their associated edges.

79
22:07:14 --> 22:07:20
So, my first line of code is very
familiar, we're matching nodes and

80
22:07:20 --> 22:07:22
edges, it's a very basic node and
edge match.

81
22:07:24 --> 22:07:28
The second line of code is where
the constraints are explicitly stated.

82
22:07:28 --> 22:07:33
A resulting network must consist
of edges in which the source node

83
22:07:33 --> 22:07:38
must be constrained to this
subset of node labels.

84
22:07:38 --> 22:07:43
And the target node must also be
constrained to be a part of the subset of

85
22:07:43 --> 22:07:45
nodes with just these labels.

86
22:07:45 --> 22:07:47
And then we return the nodes and
the edges.

87
22:07:48 --> 22:07:51
So, let's see the resulting graph.

88
22:07:51 --> 22:07:52
And here it is.

89
22:07:52 --> 22:07:53
So, as we would expect,

90
22:07:53 --> 22:07:57
we're seeing the five nodes that were
defined in our query constraints.

91
22:07:57 --> 22:08:01
And the edges corresponding to them,
retain the network structure.

92
22:08:01 --> 22:08:05
So, this concludes our review of
some of the basic queries you can do

93
22:08:05 --> 22:08:08
with Neo4j using the cipher
scripting language.

1
20:10:01 --> 20:10:04
Next, we will talk about
connectivity analytics with Cypher.

2
20:10:04 --> 20:10:06
If you remember in module two,

3
20:10:06 --> 20:10:11
we talked about connectivity analytics
in terms of network robustness.

4
20:10:11 --> 20:10:15
In other words, a measure of how resistant
a graph network is to being disconnected.

5
20:10:16 --> 20:10:19
Specifically, we used
two kinds of methods.

6
20:10:19 --> 20:10:24
One computed the eigenvalues, and the
second computed the degree distribution.

7
20:10:24 --> 20:10:29
For these examples, we're going to use
the second one, degree distributions.

8
20:10:29 --> 20:10:32
And we will use the same graph
network we've used previously.

9
20:10:32 --> 20:10:35
A simple graph representing
a road network.

10
20:10:36 --> 20:10:40
And here's a listing of the query examples
we're going to be applying to our network.

11
20:10:57 --> 20:11:03
My first query example finds all
of the outdegrees of all nodes.

12
20:11:03 --> 20:11:07
Now, if you'll notice, this query consists
of two parts, because there's a specific

13
20:11:07 --> 20:11:12
type of node, a leaf node, which does not
conform to this particular constraint.

14
20:11:12 --> 20:11:16
So our first match statement finds
all nodes with outgoing edges,

15
20:11:16 --> 20:11:19
as you can see here,
this is a directed edge.

16
20:11:19 --> 20:11:21
And then,
we returns the names of the nodes and

17
20:11:21 --> 20:11:24
the count as the variable outdegree.

18
20:11:24 --> 20:11:27
And for convenience,
we order by outdegree.

19
20:11:27 --> 20:11:32
And we need to combine that with
a specific query dealing with leaf nodes.

20
20:11:32 --> 20:11:36
We're familiar with how to
do that from past examples.

21
20:11:36 --> 20:11:41
And so, we'll match all leaf nodes and
return the name and

22
20:11:41 --> 20:11:43
the value zero for its outdegree.

23
20:11:43 --> 20:11:48
So when we submit this query we
get this listing right here.

24
20:11:48 --> 20:11:52
The node P has 0 for its outdegree and
all of the other nodes are as we might

25
20:11:52 --> 20:11:56
expect and they're ordered
by their value of outdegree.

26
20:12:00 --> 20:12:02
Our next query finds
the indegree of all nodes,

27
20:12:02 --> 20:12:05
which is very similar to
our previous example.

28
20:12:05 --> 20:12:07
But, in this case, as you might expect,

29
20:12:07 --> 20:12:12
we're going to take into account
root nodes instead of leaf nodes.

30
20:12:12 --> 20:12:15
And so, our match involves incoming edges.

31
20:12:15 --> 20:12:19
Indegree is a measure of all nodes
connected to a specific node with

32
20:12:19 --> 20:12:21
incoming edges.

33
20:12:21 --> 20:12:23
And we return similar results and

34
20:12:23 --> 20:12:29
we union that with the specific query
commands to find all of the root nodes,

35
20:12:29 --> 20:12:33
and then we return those names and
0 as the value of indegree.

36
20:12:33 --> 20:12:37
So when we submit this query,
here's our results as we might expect.

37
20:12:37 --> 20:12:40
In this case, H is our only root node.

38
20:12:40 --> 20:12:43
So it has a value of 0 for indegree, and

39
20:12:43 --> 20:12:46
all the other nodes
are as we might expect.

40
20:12:49 --> 20:12:53
And our third query example
finds the degree of all nodes,

41
20:12:53 --> 20:12:56
which is a combination of outdegree and
indegree.

42
20:12:56 --> 20:13:02
So in this case we're not including any
specific direction in our match statement.

43
20:13:02 --> 20:13:06
And we're returning the name and
the count for all of our edges.

44
20:13:06 --> 20:13:09
But we're using the distinct statement,

45
20:13:09 --> 20:13:13
otherwise we would be
counting some nodes twice.

46
20:13:13 --> 20:13:16
And then, for convenience,
we order this by the value of degree.

47
20:13:16 --> 20:13:21
And when we submit this query,
we get the results as shown here.

48
20:13:21 --> 20:13:26
We have 1 column with the name and
the other column with the degree and

49
20:13:26 --> 20:13:31
the values are as we would expect, we have
a leaf node P with the degree of 1 and

50
20:13:31 --> 20:13:33
a root node H with a degree of 1.

51
20:13:37 --> 20:13:43
Our next query example generates a degree
histogram of the graph since we're able to

52
20:13:43 --> 20:13:49
calculate the degree of each node, we can
sort those into actual values of degree.

53
20:13:49 --> 20:13:53
So if we look at the distribution
of degree among our nodes,

54
20:13:53 --> 20:13:58
we see there's 2 nodes with the degree 1,
there's 3 nodes with degree 2,

55
20:13:58 --> 20:14:04
there's 4 nodes with degree 3, and
there's 2 nodes with degree 4.

56
20:14:04 --> 20:14:07
So we're going to group those
in the form of a histogram.

57
20:14:07 --> 20:14:11
So when we submit this query,
we get this table.

58
20:14:11 --> 20:14:16
The first column list the degree
value in ascending order and

59
20:14:16 --> 20:14:22
the second column list the counts of
the nodes that have that degree value.

60
20:14:22 --> 20:14:27
So for those of you who are familiar with
SQL you might recognize this as similar

61
20:14:27 --> 20:14:30
to the group by command,
it performs a similar function

62
20:14:34 --> 20:14:38
Our next query example saves the degree
of the node as a new node property.

63
20:14:38 --> 20:14:44
This provides an added convenience so that
we don't have to calculate the degree of

64
20:14:44 --> 20:14:47
a node every time we're
performing some sort of analysis.

65
20:14:47 --> 20:14:50
So we match all nodes with edges, and

66
20:14:50 --> 20:14:54
there's no direction in this
particular edge definition.

67
20:14:54 --> 20:14:59
And then, we return distinct
counts of each node's degree, and

68
20:14:59 --> 20:15:05
then we create a new property, called deg,
and assign the value of degree to it.

69
20:15:05 --> 20:15:09
Then, we return the names and
the degree values, and so

70
20:15:09 --> 20:15:13
when we submit this query,
we see this distribution right here,

71
20:15:13 --> 20:15:17
with the names in the left column, and
the values of degree in the right column.

72
20:15:19 --> 20:15:23
And we can verify that if we issue a
command to return all of the properties of

73
20:15:23 --> 20:15:25
the specific node.

74
20:15:25 --> 20:15:28
So in this case I issued a command
to match the node named D and

75
20:15:28 --> 20:15:30
return all of its properties.

76
20:15:30 --> 20:15:34
And sure enough we see that it has
a property name and a property degree.

77
20:15:37 --> 20:15:39
Before we go to the last two examples,

78
20:15:39 --> 20:15:43
there's a philosophical issue that we
need to remember with all databases.

79
20:15:43 --> 20:15:47
Every database will allow you some
analytical computation and the remainder

80
20:15:47 --> 20:15:51
of the analytical computations must
be done outside of the database.

81
20:15:51 --> 20:15:54
However, it is always a judicious
idea to get the database to

82
20:15:54 --> 20:15:58
achieve an intermediate result formatted
in a way that you would need for

83
20:15:58 --> 20:15:59
the next computation.

84
20:15:59 --> 20:16:04
And then, you use that intermediate result
as the input to the next computation.

85
20:16:04 --> 20:16:07
We've seen that a number of
computations in graph analytics

86
20:16:07 --> 20:16:10
start with the adjacency matrix.

87
20:16:10 --> 20:16:14
So we should be able to force Cypher
to produce an adjacency matrix.

88
20:16:14 --> 20:16:15
And this is what we're doing here.

89
20:16:16 --> 20:16:22
So think of a Matrix as a three column
table, in which, here's one column,

90
20:16:22 --> 20:16:25
here's another column, and the third
column will be the values that we

91
20:16:25 --> 20:16:30
are calculating when we determine whether
two nodes have an edge between them.

92
20:16:30 --> 20:16:33
And we're introducing a new
construct in Cypher called case.

93
20:16:33 --> 20:16:38
This allows us to evaluate conditions and
return one result, or

94
20:16:38 --> 20:16:41
a different result
depending on the condition.

95
20:16:41 --> 20:16:46
Here, we're specifying that when
there is an edge between nodes n and

96
20:16:46 --> 20:16:50
m, then we return a value of 1,
otherwise return a value of 0.

97
20:16:50 --> 20:16:53
And we'll output those results as a value.

98
20:16:53 --> 20:16:57
And so, when we submit this query,
we get our three column table in which

99
20:16:57 --> 20:17:01
the first column is
the name of our first node.

100
20:17:01 --> 20:17:05
The second column is the name of our
second node and the value is either a 1 or

101
20:17:05 --> 20:17:09
a 1 depending on whether the nodes
have an edge between them.

102
20:17:09 --> 20:17:12
So in this case we see node A and
C have an edge,

103
20:17:12 --> 20:17:16
A and L have an edge and
so on as we would expect.

104
20:17:20 --> 20:17:25
So if we can calculate the adjacency
matrix then we can calculate any matrix.

105
20:17:25 --> 20:17:29
You might remember from our
module two lecture where we

106
20:17:29 --> 20:17:32
learned about this complex structure
called the Normalized Laplacian Matrix.

107
20:17:33 --> 20:17:35
So let's go ahead and calculate that.

108
20:17:35 --> 20:17:41
We'll perform something very similar
to what we did in the previous example.

109
20:17:41 --> 20:17:44
We'll match all nodes for
the first column, and all nodes for

110
20:17:44 --> 20:17:46
the second column.

111
20:17:46 --> 20:17:51
We'll return the names of those nodes and
then we'll use the case structure again

112
20:17:51 --> 20:17:55
to compare the names of each node and
determine whether we have the same node.

113
20:17:56 --> 20:18:01
If we do have the same node then
that is a diagonal of the matrix and

114
20:18:01 --> 20:18:01
should get a value of 1.

115
20:18:03 --> 20:18:06
If they are different nodes and
contain an edge between them,

116
20:18:06 --> 20:18:09
then we calculate the normalized
Laplacian with this equation here.

117
20:18:09 --> 20:18:14
And you'll also want to notice that here
we're using the actual degree property

118
20:18:14 --> 20:18:18
that we assigned to the nodes
in a previous example.

119
20:18:18 --> 20:18:22
This is an example of how that
can become a convenient option.

120
20:18:22 --> 20:18:25
So when the calculation is performed,
the value would be returned.

121
20:18:25 --> 20:18:30
If there's no edge between the 2 nodes,
then the value of 0 will be returned.

122
20:18:30 --> 20:18:32
And these values will end
up in the value column.

123
20:18:32 --> 20:18:36
So when we submit this query,
here's the table that get returned.

124
20:18:36 --> 20:18:39
This is the first column
with the source node.

125
20:18:39 --> 20:18:43
The second column with the target node and
the values.

126
20:18:43 --> 20:18:47
So in the first row, the first node
is P and the second node is P, so

127
20:18:47 --> 20:18:50
it's identical, which means it's
on the diagonal of the matrix.

128
20:18:50 --> 20:18:53
Likewise, for A in this row down here.

129
20:18:53 --> 20:18:58
And then the first value of the Laplacian
is calculated between nodes A and

130
20:18:58 --> 20:18:59
C, and so on.

131
20:19:01 --> 20:19:06
So that concludes our examples of how
to perform connectivity analytics

132
20:19:06 --> 20:19:07
in Neo4j with Cypher.

1
16:29:07 --> 16:29:11
Hello everyone and welcome to a series of
lessons on graph analytics with Neo4j.

2
16:29:12 --> 16:29:17
In this first tutorial we are going to go
briefly through the process of downloading

3
16:29:17 --> 16:29:22
and installing Neo4j, and
then we'll go ahead and run Neo4j and

4
16:29:23 --> 16:29:27
provide a brief review of
the Neo4j graphical interface.

5
16:29:27 --> 16:29:28
So, let's get started.

6
16:29:32 --> 16:29:37
To download Neo4j,
you'll want to browse to neo4j.com, and

7
16:29:37 --> 16:29:43
once you're at that URL,
you should see a webpage similar to this.

8
16:29:43 --> 16:29:47
Neo4j typically will detect which
operating system you're running, but

9
16:29:47 --> 16:29:50
if you want to download
other versions of Neo4j,

10
16:29:50 --> 16:29:53
click this link in the upper
right-hand corner to download Neo4j.

11
16:29:53 --> 16:29:58
We're going to use the community edition,
so we'll click this link right here.

12
16:29:58 --> 16:29:59
But if you'd like to
explore other releases,

13
16:29:59 --> 16:30:01
you can click the link down here.

14
16:30:02 --> 16:30:05
When we click
the Download Community Edition link,

15
16:30:05 --> 16:30:09
Neo4j should automatically
begin the download.

16
16:30:09 --> 16:30:14
There are versions of Neo4j for
Windows, Mac, and Linux.

17
16:30:14 --> 16:30:17
Neo4j is developed with the Java
programming language, so

18
16:30:17 --> 16:30:22
you'll want to be sure you have the
minimum system requirements to run Neo4j.

19
16:30:22 --> 16:30:25
We'll provide a link to those system
requirements in the resources of

20
16:30:25 --> 16:30:25
this video.

21
16:30:26 --> 16:30:31
This example is with
the Mac OS X operating system.

22
16:30:31 --> 16:30:34
And we've downloaded the .dmg file and
we can click to open that.

23
16:30:38 --> 16:30:42
And as is the case with the Mac
applications to install is very

24
16:30:42 --> 16:30:43
straightforward.

25
16:30:43 --> 16:30:47
We'll just click and drag this icon
into the Applications folder and

26
16:30:47 --> 16:30:50
that should automatically install.

27
16:30:50 --> 16:30:51
Once the application is installed,

28
16:30:51 --> 16:30:55
we should see it listed in
the Applications folder.

29
16:30:55 --> 16:30:57
So let's go ahead and run Neo4j.

30
16:31:01 --> 16:31:05
To run Neo4j, you'll just double-click
on the application's icon.

31
16:31:05 --> 16:31:09
Neo4j is browser-based, so
it launches a web server, and

32
16:31:09 --> 16:31:14
it launches a URL specific to Neo4j.

33
16:31:14 --> 16:31:17
So to activate that server
we'll click the Start button.

34
16:31:17 --> 16:31:21
If it's your first time using Neo4j
then you probably won't have any graph

35
16:31:21 --> 16:31:26
databases available to open,
so Neo4j makes it easy for

36
16:31:26 --> 16:31:30
you by providing a default
graph database of movie data.

37
16:31:31 --> 16:31:36
To run Neo4j, you click this link and
that should launch your default browser.

38
16:31:40 --> 16:31:44
Installation on Windows should be
just as straightforward as OS X.

39
16:31:44 --> 16:31:48
You'll click the same
Download Neo4j button and

40
16:31:48 --> 16:31:52
when the download page opens,
you'll click Download Community Edition.

41
16:31:52 --> 16:31:58
Neo4j should recognize you're running
Windows and should download an executable.

42
16:31:58 --> 16:32:03
Once that downloads we can run it and
we'll step through the same steps

43
16:32:03 --> 16:32:06
which are typical for
a Windows operating system.

44
16:32:06 --> 16:32:09
Now I already have Neo4j
installed in my Windows system so

45
16:32:09 --> 16:32:12
I will be updating my
existing installation.

46
16:32:16 --> 16:32:19
And Neo4j downloads a Java jar file.

47
16:32:20 --> 16:32:24
Once the application's installed
it will open the dialog box and

48
16:32:24 --> 16:32:26
we can start up our web server.

49
16:32:26 --> 16:32:30
And the same URL is available and
when we click that, our default

50
16:32:30 --> 16:32:34
browser should be launched and we should
see the same interface we see In OS X.

51
16:32:35 --> 16:32:39
You should be able to subsequently
run neo4j by accessing the executable

52
16:32:39 --> 16:32:41
in the Windows Start menu.

53
16:32:41 --> 16:32:44
So let's take a look at
the neo4j interface.

54
16:32:47 --> 16:32:52
As I mentioned previously,
neo4j is a browser-based application.

55
16:32:52 --> 16:32:56
So once the application launches it
should open your default browser and

56
16:32:56 --> 16:32:59
in my case, I'm using the Chrome browser.

57
16:32:59 --> 16:33:03
And you should see a URL like
this in the address bar.

58
16:33:03 --> 16:33:07
Most browsers provide a full-screen
viewing mode, so I'm going to go ahead and

59
16:33:07 --> 16:33:11
activate that, in this case,
what Chrome calls presentation mode.

60
16:33:11 --> 16:33:15
The interface for
neo4j consists of three main areas.

61
16:33:15 --> 16:33:19
There's the command line along the top
where you can issue commands and you can

62
16:33:19 --> 16:33:24
paste content into the command line and
execute multiple lines at one time.

63
16:33:25 --> 16:33:30
You can specify these commands
to be one of your favorites.

64
16:33:30 --> 16:33:32
You can clear the command box.

65
16:33:32 --> 16:33:35
And you can run, or execute,
the commands with the Play button.

66
16:33:35 --> 16:33:40
On the left-hand side is an expandable and
collapsible panel that provides

67
16:33:40 --> 16:33:43
supplementary information about the
database you're currently working with.

68
16:33:44 --> 16:33:47
There's an option to list your favorites,
and

69
16:33:47 --> 16:33:52
Neo4j provides an initial
default listing of favorites.

70
16:33:52 --> 16:33:54
And there's supplementary information and
links to references and

71
16:33:54 --> 16:33:57
examples and tutorials and so on.

72
16:33:57 --> 16:34:02
You can also modify the configuration
of your Neo4j installation for

73
16:34:02 --> 16:34:04
your particular user.

74
16:34:04 --> 16:34:07
And there's additional information
about the application.

75
16:34:07 --> 16:34:11
The third area is the main panel in
the middle where the results of all of

76
16:34:11 --> 16:34:14
the commands that you
execute are displayed.

77
16:34:14 --> 16:34:18
In fact, this panel itself has been
generated by issuing a command,

78
16:34:18 --> 16:34:21
which is displayed in
the very top of the panel.

79
16:34:21 --> 16:34:25
The command is ;play start.

80
16:34:25 --> 16:34:30
In fact, if I type that command into my
command line I should be able to generate

81
16:34:30 --> 16:34:33
a duplicate of that panel,
and sure enough, there it is.

82
16:34:33 --> 16:34:39
The upper right-hand corner of these
panels I can pin the panel to my webpage,

83
16:34:39 --> 16:34:41
and I can do this with as
many panels as I want and

84
16:34:41 --> 16:34:44
the window will simply allow
me to scroll up and down.

85
16:34:44 --> 16:34:47
I can view this particular
panel full screen.

86
16:34:47 --> 16:34:50
Or it can cancel or close it,
which is what I'm going to do.

87
16:34:50 --> 16:34:56
Now you can see neo4j makes it very
easy to learn all about graph analytics.

88
16:34:56 --> 16:35:00
So let's go ahead and get started
with learning how to create and

89
16:35:00 --> 16:35:02
modify our first graph network.

1
09:04:11 --> 09:04:16
Next we're going to get started with Neo4j
by creating our first graph network.

2
09:04:17 --> 09:04:20
To do this,
first we will review a graphical or

3
09:04:20 --> 09:04:24
a diagrammatic representation
of a graph network.

4
09:04:24 --> 09:04:28
Then, we'll introduce you to an equivalent
text representation of that network.

5
09:04:28 --> 09:04:33
Then, we will build on those text
representations in the form of pseudocode,

6
09:04:33 --> 09:04:37
with the ultimate goal being
to develop an actual script

7
09:04:37 --> 09:04:39
to create our network in Neo4j.

8
09:04:40 --> 09:04:44
Then, we'll go ahead and run the script to
create the network, and we'll explore it,

9
09:04:44 --> 09:04:46
and confirm it structure and content.

10
09:04:50 --> 09:04:56
You're looking at a simple graph network,
consisting of five nodes and five edges.

11
09:04:56 --> 09:05:00
Each node represents a person,
an individual.

12
09:05:00 --> 09:05:03
And, the edges represent
relationships between those people.

13
09:05:03 --> 09:05:07
Each node has a number associate with it,
N1 through N5.

14
09:05:07 --> 09:05:13
And, each edge has a corresponding number
associated with it, E1 through E5.

15
09:05:13 --> 09:05:18
Edges are relationships such
as Harry is known by Tom,

16
09:05:18 --> 09:05:20
or Julian is coworker of Harry.

17
09:05:20 --> 09:05:25
We could have more or less edges, but we
wanted to keep things relatively simple,

18
09:05:25 --> 09:05:29
while still maintaining
a reasonable degree of complexity.

19
09:05:29 --> 09:05:32
What we want is a script that we can

20
09:05:32 --> 09:05:37
process with Neo4j in order to
create an actual graph network.

21
09:05:37 --> 09:05:40
So, let's look at a text
representation of this network.

22
09:05:44 --> 09:05:49
So, here we list our five nodes and
five edges, and we're going to begin

23
09:05:49 --> 09:05:55
the process of extending the text
descriptions of our graph network.

24
09:05:55 --> 09:05:59
I'm going to scroll down briefly,
and just show you the end result, so

25
09:05:59 --> 09:06:03
you have a better idea of what
the final goal is going to be.

26
09:06:03 --> 09:06:07
This is the actual code that we're going
to submit to Neo4j in order to create

27
09:06:07 --> 09:06:08
our network.

28
09:06:08 --> 09:06:11
But, we're going to back
up a little bit and

29
09:06:11 --> 09:06:14
look at some simplified
versions of this syntax, so

30
09:06:14 --> 09:06:19
we can better understand how simple the
graph network relationships really are.

31
09:06:22 --> 09:06:27
Here, we list the five nodes and
five edges, as you saw just a moment ago.

32
09:06:27 --> 09:06:32
And down below,
we have a simple notation structure,

33
09:06:32 --> 09:06:35
which attempts to describe
the five edge relationships.

34
09:06:35 --> 09:06:39
The first line represents the edge E1.

35
09:06:39 --> 09:06:43
We can see that the nodes N1 and
N2 are included,

36
09:06:43 --> 09:06:49
because Harry is known by Tom, and
Tom is node N1 and Harry is node N2.

37
09:06:49 --> 09:06:53
The same goes for the second line,
the relationship between Julian and Harry.

38
09:06:53 --> 09:06:56
Julian is co-worker of Harry, and so on.

39
09:06:57 --> 09:07:02
So, let's take this a step further
by defining our graph network,

40
09:07:02 --> 09:07:06
so that each node is
a particular type of node.

41
09:07:06 --> 09:07:12
In this case, we're going to define a node
type as what we're calling a ToyNode.

42
09:07:12 --> 09:07:17
As we introduce each node and
its relationship with other nodes,

43
09:07:17 --> 09:07:20
we'll define the node
to be of type ToyNode.

44
09:07:20 --> 09:07:25
So, on this first line,
N1 goes through e1 to N2.

45
09:07:25 --> 09:07:27
And, both of those are introduced for
the first time, so

46
09:07:27 --> 09:07:30
we'll define them as type ToyNode.

47
09:07:30 --> 09:07:34
But, on the next line, since we
already introduced N2 as type ToyNode,

48
09:07:34 --> 09:07:35
we don't need to repeat that statement.

49
09:07:37 --> 09:07:42
And, so we continue in the same manner
with the remaining edge relationships.

50
09:07:42 --> 09:07:47
Taking this even further, we'll apply
a similar kind of constraint to our edges.

51
09:07:47 --> 09:07:53
In this case, we'll define our network
such that each edge is a particular type,

52
09:07:53 --> 09:07:56
which we're calling ToyRelation.

53
09:07:56 --> 09:07:59
Next, we're going to add
properties to our nodes and edges.

54
09:07:59 --> 09:08:03
Our nodes can have properties
such as name or job,

55
09:08:03 --> 09:08:08
so in this case, our first node,
N1, will have the name Tom.

56
09:08:08 --> 09:08:13
And, the appropriate syntax for this
includes curly braces surrounding the key

57
09:08:13 --> 09:08:18
value pairs, a colon separating
the key value pairs, and

58
09:08:18 --> 09:08:22
the values defined within single quotes.

59
09:08:22 --> 09:08:26
Likewise, each edge may have
a specific type of relationship,

60
09:08:26 --> 09:08:29
including co-worker, wife, and friend.

61
09:08:31 --> 09:08:35
So, finally this brings us to the actual
code we're going to use to create our

62
09:08:35 --> 09:08:37
graph network.

63
09:08:37 --> 09:08:40
So, let's go ahead and copy this,
and paste it into Neo4j, so

64
09:08:40 --> 09:08:41
we can take a look at our network.

65
09:08:44 --> 09:08:47
Here we are, running Neo4j in our browser.

66
09:08:47 --> 09:08:52
So, I'm going to paste the code that
I just copied from my text file

67
09:08:52 --> 09:08:55
into the command line
in the Neo4j interface.

68
09:08:56 --> 09:08:59
And, I'll click the Play button
to execute those commands, and

69
09:08:59 --> 09:09:04
we'll see the results returned
in this newly displayed panel.

70
09:09:04 --> 09:09:09
We can see that we have 5 labels added,
5 nodes were created, 13 properties were

71
09:09:09 --> 09:09:15
set, 5 relationships were created, and the
entire process required 31 milliseconds.

72
09:09:18 --> 09:09:23
However, we still can't actually view our
graph unless we issue yet another command.

73
09:09:23 --> 09:09:28
So, let's shuffle to our text file,
and take a look at that command.

74
09:09:28 --> 09:09:32
What this command does,
is it tries to identify a match

75
09:09:32 --> 09:09:38
in which a particular node has
a relationship with any other node.

76
09:09:38 --> 09:09:42
And, then we'll return those nodes and
relationships.

77
09:09:42 --> 09:09:45
So, we'll go ahead and copy this, and

78
09:09:45 --> 09:09:49
we'll paste it in to our command line,
and we'll execute.

79
09:09:49 --> 09:09:50
And, there's our graph.

80
09:09:50 --> 09:09:56
When we mouse over the nodes, we can see
information displayed on the bottom.

81
09:09:56 --> 09:10:00
And, when we select those nodes, that
information is displayed permanently along

82
09:10:00 --> 09:10:04
the bottom,
likewise with edge relationships.

83
09:10:04 --> 09:10:10
So, we would expect to see things such
as Michelle is the wife of Harry,

84
09:10:10 --> 09:10:15
Julian is a co-worker of Harry, and so on.

85
09:10:15 --> 09:10:18
So, it looks like our network
has been created successfully.

86
09:10:18 --> 09:10:21
We can also display
information in a tabular format

87
09:10:21 --> 09:10:24
by clicking the Rows icon on the left.

88
09:10:24 --> 09:10:28
And, we'll see that this information
is not constrained by the directed

89
09:10:28 --> 09:10:29
nature of our graph.

90
09:10:29 --> 09:10:32
For example, we see that Harry
has a relationship with Tom,

91
09:10:32 --> 09:10:34
in which Harry knows Tom.

92
09:10:34 --> 09:10:38
But likewise, down here,
Tom has a relationship with Harry,

93
09:10:38 --> 09:10:39
in which Tom knows Harry.

94
09:10:41 --> 09:10:44
Next, we're going to learn
how to add to this graph and

95
09:10:44 --> 09:10:47
modify some of the properties
of the nodes and edges.

1
18:14:57 --> 18:15:01
In this segment, we're going to
learn how to import data into Neo4j.

2
18:15:02 --> 18:15:07
We will begin by using a fairly simple
spreadsheet consisting of only a few

3
18:15:07 --> 18:15:11
rows and three columns in a format
that's fairly typical for

4
18:15:11 --> 18:15:13
importing into a graph database.

5
18:15:13 --> 18:15:19
We will review the Neo4j CYPHER
script used to perform this import.

6
18:15:19 --> 18:15:22
And then we'll run the script and
validate the resulting graph.

7
18:15:22 --> 18:15:24
Then we'll demonstrate
a similar process but

8
18:15:24 --> 18:15:28
with a more challenging dataset,
consisting of terrorist network data.

9
18:15:29 --> 18:15:31
We will review this dataset and

10
18:15:31 --> 18:15:34
the script commands necessary for
performing the import.

11
18:15:34 --> 18:15:37
And we'll run the script and
explore the resulting graph.

12
18:15:38 --> 18:15:42
And finally, we will review a third
dataset that you will use yourself to

13
18:15:42 --> 18:15:45
perform similar data inport operations.

14
18:15:50 --> 18:15:53
First, let's take a look
at our sample dataset.

15
18:15:53 --> 18:15:57
This spreadsheet consists of just
a few rows and three columns.

16
18:15:57 --> 18:15:59
Each column has a heading.

17
18:15:59 --> 18:16:04
The first column heading is Source,
the second column heading is Target, and

18
18:16:04 --> 18:16:06
the third column heading is distance.

19
18:16:06 --> 18:16:10
You can imagine that this might represent
data from a simple road network in which

20
18:16:10 --> 18:16:13
the Source and
Target values represent towns, and

21
18:16:13 --> 18:16:17
the distance values represent the actual
distance in miles between the towns.

22
18:16:21 --> 18:16:24
So let's take a look at
the Neo4j CYPHER script and

23
18:16:24 --> 18:16:28
see what we will need to do to
import our spreadsheet data.

24
18:16:28 --> 18:16:32
The first line of code
performs the actual import.

25
18:16:32 --> 18:16:36
The other three lines of code provide
constraints to the formatting

26
18:16:36 --> 18:16:37
of that data.

27
18:16:37 --> 18:16:41
Since our data file is
a comma separated values or

28
18:16:41 --> 18:16:45
CSV file,
we will need to specify that in our code.

29
18:16:45 --> 18:16:47
Our file also contains headers.

30
18:16:47 --> 18:16:49
And if you're using
a Windows operating system,

31
18:16:49 --> 18:16:51
your file path will look
something like this.

32
18:16:51 --> 18:16:56
Since we're running Neo4j in a browser,
the file path needs

33
18:16:56 --> 18:17:01
to conform to the HTTP address
conventions of the word file,

34
18:17:01 --> 18:17:06
followed by a colon,
followed by three forward slashes and

35
18:17:06 --> 18:17:12
then the hard disk letter where the file
is located, plus the path to that file.

36
18:17:12 --> 18:17:16
If you're using Mac OSX,
your command will be similar, but

37
18:17:16 --> 18:17:20
the file path will probably
look something more like this.

38
18:17:20 --> 18:17:24
The next three lines specify which
nodes will be the source nodes and

39
18:17:24 --> 18:17:26
which nodes will be the target nodes.

40
18:17:26 --> 18:17:32
And the properties will attach to them as
well as defining the relationships and

41
18:17:32 --> 18:17:34
the properties we will
attach to the relationships.

42
18:17:34 --> 18:17:39
As we read each line of data n,
we're going to use keyword line,

43
18:17:39 --> 18:17:43
to specify the individual line
we're currently working on.

44
18:17:43 --> 18:17:47
We use that here at the end
of our load command and so

45
18:17:47 --> 18:17:50
we'll have to continue to use it
in the subsequent merge commands.

46
18:17:52 --> 18:17:54
Our source node variable
is going to be n and

47
18:17:54 --> 18:17:59
we'll use the MyNode type,
which we've made up ourselves.

48
18:17:59 --> 18:18:03
We're going to add a Name property
to each of our source nodes.

49
18:18:03 --> 18:18:08
And we're going to attach the value in
the Source column to that particular node.

50
18:18:08 --> 18:18:13
Likewise for our target nodes,
we'll use a variable m.

51
18:18:13 --> 18:18:15
We'll define them as the same type,
MyNode.

52
18:18:17 --> 18:18:21
We will give them a Name and we will
extract that name from the Target column

53
18:18:21 --> 18:18:24
on the particular line we're working with.

54
18:18:24 --> 18:18:27
Finally, we need to define
our edge relationships.

55
18:18:27 --> 18:18:30
We're going to give each
edge a label of the word TO,

56
18:18:30 --> 18:18:36
and we're going to add a property called
dist, which represents the distance.

57
18:18:36 --> 18:18:39
And we'll attach the values
in the distance column

58
18:18:39 --> 18:18:41
from the particular line
we're currently working on.

59
18:18:41 --> 18:18:48
So let's go ahead and copy this code and
paste it into Neo4j, and see what happens.

60
18:18:48 --> 18:18:50
Now, I'm working on a Mac OSX machine, so

61
18:18:50 --> 18:18:55
I'm going to copy this code down here and
perform the import operation.

62
18:18:58 --> 18:19:03
So I've pasted the line
of code into Neo4j and

63
18:19:03 --> 18:19:06
I'll click Execute.

64
18:19:06 --> 18:19:08
And it takes a few moments to run.

65
18:19:08 --> 18:19:12
There's not too much data so it shouldn't
take more than just a few seconds.

66
18:19:12 --> 18:19:16
And now we get the results in which
11 labels, 11 nodes have been added,

67
18:19:16 --> 18:19:21
25 properties have been set and
14 relationships have been created.

68
18:19:21 --> 18:19:23
So let's take a look
at this graph network.

69
18:19:23 --> 18:19:27
We can see that the nodes are listed
all as MyNode and there's our graph.

70
18:19:28 --> 18:19:32
The edge relationships should all
have different distance values and

71
18:19:32 --> 18:19:34
each node should be named
a different letter.

72
18:19:34 --> 18:19:37
Okay, so
let's try a more difficult dataset.

73
18:19:40 --> 18:19:44
Here is the spreadsheet
containing terrorist data.

74
18:19:44 --> 18:19:49
The spreadsheet consists of seven
columns with headings such as Country,

75
18:19:49 --> 18:19:53
ActorName, ActorType, AffiliationTo,
AffiliationStartDate,

76
18:19:53 --> 18:20:00
AffiliationEndDate, and any aliases
associated with that particular terrorist.

77
18:20:00 --> 18:20:05
This dataset consists of
over 100,000 rows of data.

78
18:20:05 --> 18:20:10
Since that could take a very long
time to load into Neo4j, we will be

79
18:20:10 --> 18:20:15
working with a subset of this dataset
consisting of the first 1,000 rows.

80
18:20:15 --> 18:20:19
That much data will still
include three countries,

81
18:20:19 --> 18:20:22
which should be plenty of data for
our purposes.

82
18:20:25 --> 18:20:30
So here is the script we're going to use
to import the subset of terrorist data,

83
18:20:30 --> 18:20:35
which shares similarities with
the script we used previously.

84
18:20:35 --> 18:20:38
But since there are more
columns in our dataset,

85
18:20:38 --> 18:20:42
we're going to include some additional
properties into our graph network.

86
18:20:42 --> 18:20:46
The first line of code is very similar

87
18:20:46 --> 18:20:49
to the load command we've
used in previous datasets.

88
18:20:49 --> 18:20:56
The second line of code will use
a variable c and a label Country for

89
18:20:56 --> 18:21:00
the particular nodes representing
the individual countries in the dataset.

90
18:21:01 --> 18:21:05
In this particular case, we're going to
use the keyword row instead of line to

91
18:21:05 --> 18:21:07
read our data n, but
it really doesn't matter.

92
18:21:07 --> 18:21:12
We could use either word as long we
are consistent from command to command.

93
18:21:12 --> 18:21:17
So we're using the term row and
associating the value in the Country

94
18:21:17 --> 18:21:21
column with the node that we're
working on in that particular row.

95
18:21:21 --> 18:21:26
We will do something similar with
nodes that are intended to represent

96
18:21:26 --> 18:21:29
the actors or
the actual individual terrorists.

97
18:21:29 --> 18:21:35
We're going to use a variable a and
we will associate a property

98
18:21:35 --> 18:21:41
called Name, and associate
the ActorName with that property.

99
18:21:41 --> 18:21:45
We'll also associate
a property named Aliases, and

100
18:21:45 --> 18:21:49
associate the value in the Aliases
column with that property.

101
18:21:50 --> 18:21:55
And finally,
we will define a property called Type and

102
18:21:55 --> 18:21:59
associate the values in the ActorType
column with that property.

103
18:22:00 --> 18:22:04
We're going to create nodes
representing organizations as well and

104
18:22:04 --> 18:22:09
we will use the variable o, and
the label Organization for those nodes.

105
18:22:09 --> 18:22:14
We will attach a single property
to these nodes called Name and

106
18:22:14 --> 18:22:19
we'll assign the values in
the AffiliationTo column to that property.

107
18:22:19 --> 18:22:23
Then we're going to define
relationships between the Actors and

108
18:22:23 --> 18:22:26
the Organizations they're affiliated with.

109
18:22:28 --> 18:22:31
The relationship label is
going to be AFFILIATED_TO and

110
18:22:31 --> 18:22:35
we'll define a property called Start.

111
18:22:35 --> 18:22:39
And we'll assign the values in the
AffiliationStartDate with that property.

112
18:22:39 --> 18:22:44
Likewise, we will define
a property called End and

113
18:22:44 --> 18:22:48
assign the values in
the AffiliationEndDate with that property.

114
18:22:48 --> 18:22:53
And finally, we're going to create
relationships between the countries and

115
18:22:53 --> 18:22:55
the actors.

116
18:22:55 --> 18:23:00
In this case, we will define relationships
with the label IS_FROM that will

117
18:23:00 --> 18:23:05
describe the fact that a particular
actor is from a particular country.

118
18:23:05 --> 18:23:09
So if all this makes sense,
let's go ahead and copy this script and

119
18:23:09 --> 18:23:12
paste it into Neo4j, and see the results.

120
18:23:15 --> 18:23:18
So here we are in Neo4j, and
I'm going to go ahead and

121
18:23:18 --> 18:23:23
paste that script into the command line,
and we will execute it.

122
18:23:25 --> 18:23:33
We loaded 1,000 rows of data, which
consists of 658 labels and 658 nodes.

123
18:23:33 --> 18:23:38
3,464 properties and
1,403 relationships and

124
18:23:38 --> 18:23:41
it took about two and a half seconds.

125
18:23:41 --> 18:23:46
So let's look at a small
subset of this network.

126
18:23:51 --> 18:23:57
Here, we see the equivalent of
the first 25 rows of the dataset.

127
18:23:57 --> 18:24:01
There's only enough data
such that 1 Country,

128
18:24:01 --> 18:24:05
8 Actors and 15 Organizations are visible.

129
18:24:05 --> 18:24:08
Let's go ahead and
change this command to 250.

130
18:24:08 --> 18:24:12
By clicking on the line of
code in the top of our panel,

131
18:24:12 --> 18:24:16
it automatically gets pasted
into the command line above.

132
18:24:16 --> 18:24:22
So all we need to do is add a zero on the
end of our command and execute that again.

133
18:24:22 --> 18:24:26
Now we have a much larger,
more complex graph,

134
18:24:26 --> 18:24:30
but we still only have one country.

135
18:24:30 --> 18:24:32
In order to see more than one country,

136
18:24:32 --> 18:24:39
we'll need to render the entire 1,000
rows of our terrorist data subset.

137
18:24:39 --> 18:24:42
But in so doing, we will have a difficult
time viewing the entire graph.

138
18:24:43 --> 18:24:47
The community edition of Neo4j
is limited in its ability to

139
18:24:47 --> 18:24:49
navigate a graph network.

140
18:24:49 --> 18:24:52
But I'm going to show you
a little trick by editing

141
18:24:52 --> 18:24:56
the HTML behind the scenes to
scale the view of our graph.

142
18:24:57 --> 18:25:00
So let's go ahead and
render all 1,000 rows.

143
18:25:06 --> 18:25:11
So most recent versions of the major
browsers provide the ability to go behind

144
18:25:11 --> 18:25:14
the scenes and edit the HTML.

145
18:25:14 --> 18:25:19
So the trick is to find a place on
the viewing panel that does not have

146
18:25:19 --> 18:25:20
any objects.

147
18:25:20 --> 18:25:23
So that when you right-click and
inspect the element,

148
18:25:23 --> 18:25:26
you'll be inspecting the viewing area.

149
18:25:26 --> 18:25:29
Neo4j uses SVG graphics, or

150
18:25:29 --> 18:25:34
Scalable Vector Graphics to
render its graph networks.

151
18:25:34 --> 18:25:38
And SVG uses a g element,
which can be seen here on the right.

152
18:25:38 --> 18:25:44
So in order to change the scale of our
view, we simply need to double-click and

153
18:25:44 --> 18:25:49
add scale, open parentheses, and
the scale factor that we'd like.

154
18:25:49 --> 18:25:56
We hit Return and
the graph network now is zoomed out.

155
18:25:56 --> 18:26:02
I'm going to try to position it so
we can see at least two countries,

156
18:26:02 --> 18:26:05
and I'll close my HTML panel.

157
18:26:06 --> 18:26:08
And so now we can view two countries.

158
18:26:08 --> 18:26:14
We can see Albania in the upper region,
and Afghanistan in the lower right.

159
18:26:14 --> 18:26:17
And we can see that there are actors and

160
18:26:17 --> 18:26:22
organizations that have
relationships with both countries.

161
18:26:22 --> 18:26:27
Now there are add ons to Neo4j that
make navigating a graph network a little

162
18:26:27 --> 18:26:28
easier, but

163
18:26:28 --> 18:26:33
this trick is convenient for those of you
who have not added any Neo4j extensions.

164
18:26:36 --> 18:26:41
The last thing that we're going to do
is take a look at the sample dataset of

165
18:26:41 --> 18:26:46
gene-disease associations, and give you
an idea what's going to be expected of

166
18:26:46 --> 18:26:49
you in the accompanying assignment for
this module.

167
18:26:49 --> 18:26:54
This data consists of
information associating

168
18:26:54 --> 18:26:57
different genes with different diseases.

169
18:26:57 --> 18:27:01
The spreadsheet consists of
columns with headings for geneId,

170
18:27:01 --> 18:27:06
geneSymbol, geneName, diseaseId,

171
18:27:06 --> 18:27:11
the diseaseName, the score that
represents the extent to which that gene

172
18:27:11 --> 18:27:15
is associated with that
particular disease.

173
18:27:15 --> 18:27:21
The NumberofPubmed articles
containing that information.

174
18:27:21 --> 18:27:22
The associationTypes,

175
18:27:22 --> 18:27:27
there are up to three different types of
associations between a gene and a disease.

176
18:27:27 --> 18:27:30
And then the sources of data and

177
18:27:30 --> 18:27:35
information that confirm this
gene disease relationship.

178
18:27:35 --> 18:27:40
Now this dataset contains
over 400,000 rows of data.

179
18:27:40 --> 18:27:44
So if you have difficulty
importing this entire dataset,

180
18:27:44 --> 18:27:49
then you'll be better off extracting
the first few thousand rows.

181
18:27:49 --> 18:27:53
So you're goal will be to
define the load statement,

182
18:27:53 --> 18:27:58
which includes a CSV with headers,
that will allow you to import enough

183
18:27:58 --> 18:28:03
data into Neo4j to give you an idea
that you've done it successfully.

1
12:43:00 --> 12:43:04
We've already learned a little
about the Neo4j interface.

2
12:43:04 --> 12:43:07
And we've learned how to create
a relatively simple graph with it.

3
12:43:08 --> 12:43:09
In this lecture,

4
12:43:09 --> 12:43:13
we're going to learn how to add
another node and an edge to the graph.

5
12:43:15 --> 12:43:18
We'll also go through
the process of adding a node and

6
12:43:18 --> 12:43:20
edge incorrectly to the graph.

7
12:43:21 --> 12:43:23
And we'll learn how to
correct that mistake.

8
12:43:23 --> 12:43:27
And finally we'll learn how to modify
an existing node's information.

9
12:43:33 --> 12:43:38
Okay, so here's our network that we
created in the previous lecture.

10
12:43:38 --> 12:43:40
The first modification
that we want to make,

11
12:43:40 --> 12:43:42
is adding a single node to the network.

12
12:43:44 --> 12:43:48
So let's say that Julian has a fiance and

13
12:43:48 --> 12:43:52
her name is Joyce and
she works as a store clerk.

14
12:43:52 --> 12:43:56
So let's look at the code that we're
going to use to make that modification.

15
12:43:57 --> 12:44:00
So this process involves two steps or
two separate commands.

16
12:44:00 --> 12:44:05
First command requires you to find the
node that you want to add the new node to.

17
12:44:05 --> 12:44:11
So we use the match command and
we specify the ToyNode named Julian.

18
12:44:11 --> 12:44:16
Once that command is issued then
we'll use the merge command and

19
12:44:16 --> 12:44:21
define the relation between Julian and
the new node.

20
12:44:21 --> 12:44:24
And it's going to be fiancee.

21
12:44:24 --> 12:44:27
And then the new node
will also be a ToyNode.

22
12:44:27 --> 12:44:31
And the name is Joyce, and
her job is store clerk.

23
12:44:31 --> 12:44:35
So let's go ahead and
copy both these lines of code.

24
12:44:35 --> 12:44:40
And we'll paste them into our command
line, and we'll run these commands.

25
12:44:40 --> 12:44:43
And the results that
are returned look good.

26
12:44:43 --> 12:44:49
Neo4j says that it has added 1 label,
created 1 node, set 3 properties,

27
12:44:49 --> 12:44:55
created 1 relationship, and
it's required 55 milliseconds to execute.

28
12:44:55 --> 12:44:57
So let's look at that network.

29
12:44:57 --> 12:44:59
Maybe the easiest way to
view an existing network,

30
12:44:59 --> 12:45:03
if it's the only one you're working on and
you know the constraints involved,

31
12:45:03 --> 12:45:09
just ToyNodes, is to expand the panel on
the left and just click the ToyNode node.

32
12:45:09 --> 12:45:12
And you'll easily see the new network.

33
12:45:12 --> 12:45:15
And we can confirm that Joyce
has been successfully added, and

34
12:45:15 --> 12:45:20
when we select that,
we can see that her job is store clerk.

35
12:45:20 --> 12:45:25
The command that was issued automatically
by clicking the ToyNode button in

36
12:45:25 --> 12:45:30
the panel Is a little different than what
we've been using to view our networks.

37
12:45:30 --> 12:45:33
But they both work essentially
the same with this particular network.

38
12:45:34 --> 12:45:37
So next let's see what happens if
we do something incorrectly, and

39
12:45:37 --> 12:45:39
how to correct the mistake.

40
12:45:44 --> 12:45:45
So to do this,

41
12:45:45 --> 12:45:49
we'll need to get back to our original
network without the added node.

42
12:45:49 --> 12:45:53
And maybe the easiest way to do
this is to delete everything and

43
12:45:53 --> 12:45:55
recreate our original network.

44
12:45:56 --> 12:45:58
So first I'm going to copy and

45
12:45:58 --> 12:46:02
paste command to delete all
of the nodes and edges.

46
12:46:02 --> 12:46:07
You'll find this command in the getting
started video supplementary resources.

47
12:46:08 --> 12:46:12
It involves the match command and
I'm matching all nodes and

48
12:46:12 --> 12:46:16
all relationships and I'm deleting
those nodes and relationships.

49
12:46:16 --> 12:46:21
So let's issue that command,
which should say it's deleted 6 nodes and

50
12:46:21 --> 12:46:23
6 relationships, and sure enough.

51
12:46:23 --> 12:46:29
Neo4J's command line has a nice feature of
maintaining a history of the commands and

52
12:46:29 --> 12:46:33
so in OSX we can use the command+up
arrow to cycle through the commands and

53
12:46:33 --> 12:46:38
find the original create command
that created our network.

54
12:46:38 --> 12:46:40
On Windows, it's a CTRL+up arrow command.

55
12:46:40 --> 12:46:45
So here I've found that command and
I'm going to re-execute it.

56
12:46:45 --> 12:46:48
And we have our original
network back again.

57
12:46:48 --> 12:46:49
Let's view that network.

58
12:46:51 --> 12:46:52
And there it is.

59
12:46:54 --> 12:46:59
So let's say for whatever reason I am
not quite fully understanding how to add

60
12:46:59 --> 12:47:04
an additional node to an existing network,
and I want to use the create command.

61
12:47:06 --> 12:47:08
For example, I might be thinking that

62
12:47:08 --> 12:47:12
this statement right here will
accomplish the same kind of thing.

63
12:47:12 --> 12:47:17
By using the create command and
by specifying a ToyNode named Julian.

64
12:47:19 --> 12:47:22
Well, if we go ahead and
give that a try let's see what happens.

65
12:47:22 --> 12:47:28
So I'll copy this command and I will paste
it into my command line and execute it.

66
12:47:30 --> 12:47:35
And it says it's added 2 labels and
created 2 nodes and set 4 properties.

67
12:47:36 --> 12:47:41
And if we look at this, We

68
12:47:41 --> 12:47:48
see we've actually got another node named
Julian who has a relationship with Joyce.

69
12:47:48 --> 12:47:53
So that much is correct, but it's not
the same Julian from the original network.

70
12:47:53 --> 12:47:55
So how do we undo this?

71
12:47:56 --> 12:48:02
Well, we'll need to specify the Julian
that has a relationship with Joyce and

72
12:48:02 --> 12:48:04
delete both the Julian and
the Joyce notes.

73
12:48:07 --> 12:48:11
So here we'll use the match
command once again.

74
12:48:11 --> 12:48:17
And we'll identify the node, the ToyNode,
with the name Joyce and any relationship

75
12:48:17 --> 12:48:23
she has with any other node should be
deleted in addition to that other node.

76
12:48:23 --> 12:48:25
Let's go ahead and copy this.

77
12:48:25 --> 12:48:31
And we'll paste it into our
command line and execute.

78
12:48:31 --> 12:48:33
And it says it deleted 2 nodes and
deleted 1 relationship.

79
12:48:34 --> 12:48:36
Let's view our network again.

80
12:48:38 --> 12:48:40
And it's back to normal.

81
12:48:40 --> 12:48:45
So that's one example of how you can
intuitively figure out how to correct

82
12:48:45 --> 12:48:46
certain mistakes.

83
12:48:52 --> 12:48:57
Next we're going to modify
information of an existing node.

84
12:48:57 --> 12:49:03
So if you remember when we first created
our network, Harry didn't have a job.

85
12:49:03 --> 12:49:07
So let's go ahead and add a job to Harry.

86
12:49:07 --> 12:49:07
First of all,

87
12:49:07 --> 12:49:12
we'll need to actually select
the node by using the match command.

88
12:49:13 --> 12:49:15
So we'll issue that here and

89
12:49:15 --> 12:49:19
we'll specify that that node
name must be equal to Harry.

90
12:49:21 --> 12:49:24
And then we're going to
use the set command and

91
12:49:24 --> 12:49:29
specify that job will be equal to drummer.

92
12:49:29 --> 12:49:30
So let's go ahead and copy that.

93
12:49:32 --> 12:49:34
And we'll paste it and execute it.

94
12:49:36 --> 12:49:41
And the results that are returned
says that one property has been set.

95
12:49:41 --> 12:49:44
But let's say that Harry does
more than just play drums.

96
12:49:44 --> 12:49:47
Let's say that he can
also play lead guitar.

97
12:49:47 --> 12:49:51
So in this case we will be
adding an additional property

98
12:49:51 --> 12:49:56
to a property that already exists and
we'll see how Neo4J handles that.

99
12:49:56 --> 12:50:01
It's a relatively simple modification
we make, in which we're setting

100
12:50:01 --> 12:50:05
the job key equal to the existing job key

101
12:50:05 --> 12:50:11
+ an additional value to that key,
in this case, lead guitarist.

102
12:50:11 --> 12:50:13
So let's copy that statement.

103
12:50:15 --> 12:50:18
And we'll paste it and execute it.

104
12:50:18 --> 12:50:23
And we get a similar result returned and
then let's look at our network.

105
12:50:26 --> 12:50:32
And when we select Harry, sure enough
now we see that he has two jobs.

106
12:50:32 --> 12:50:33
They're separated by a comma.

107
12:50:33 --> 12:50:36
One is drummer and one is lead guitarist.

108
12:50:36 --> 12:50:39
There's much more you can do with Neo4j,
but

109
12:50:39 --> 12:50:43
we'll want to move on and
learn some more advanced capabilities

110
12:50:43 --> 12:50:47
that work us closer towards
managing our big data challenges.

1
01:33:48 --> 01:33:52
Next, we will talk about
Path Analytics using Cypher,

2
01:33:52 --> 01:33:55
the query language for Neo4j.

3
01:33:55 --> 01:33:59
Here's the listing of the various
queries we will be demonstrating.

4
01:34:12 --> 01:34:16
There are some things that Cypher
is capable of doing very well.

5
01:34:16 --> 01:34:19
And there are other things that
require a little bit of creativity

6
01:34:19 --> 01:34:22
in order to get the results
that you're looking for.

7
01:34:22 --> 01:34:24
And we'll show you some examples of that.

8
01:34:24 --> 01:34:29
It's also important to keep in mind
that because we're working with paths,

9
01:34:29 --> 01:34:33
which are an official
structure in graph networks,

10
01:34:33 --> 01:34:36
each one of these examples
includes a new variable.

11
01:34:36 --> 01:34:40
Which in this case,
we're using the letter p to represent for

12
01:34:40 --> 01:34:44
the actual path objects that
we're going to be returning.

13
01:34:44 --> 01:34:49
You may also see the complete word
path instead of just the single letter

14
01:34:49 --> 01:34:51
p to represent these objects.

15
01:34:51 --> 01:34:55
We're going to continue to use the data
set of a simple road network that we've

16
01:34:55 --> 01:35:00
already been using in previous
demonstrations that contains 11 nodes and

17
01:35:00 --> 01:35:02
14 edges.

18
01:35:04 --> 01:35:07
So the first query we're
going to demonstrate

19
01:35:07 --> 01:35:10
is finding a path between specific nodes.

20
01:35:10 --> 01:35:14
So this would be very much like trying
to find a root between two different

21
01:35:14 --> 01:35:16
locations in our road network.

22
01:35:16 --> 01:35:21
In this case, we're going to find
a path between the node named H and

23
01:35:21 --> 01:35:22
the node named P.

24
01:35:23 --> 01:35:27
To do this,
we'll use the match command and

25
01:35:27 --> 01:35:33
we'll say match p which is a variable
we're using to represent our path,

26
01:35:33 --> 01:35:36
= node a, going through an edge to node c.

27
01:35:36 --> 01:35:41
There's something slightly different about
this edge, and that is that we're using

28
01:35:41 --> 01:35:46
a star to represent an arbitrary number
of edges in sequence between a and c, and

29
01:35:46 --> 01:35:51
we'll be returning all of those edges
that are necessary to complete the path.

30
01:35:51 --> 01:35:55
And in this case we only want
to return a single path.

31
01:35:55 --> 01:35:59
So when we submit this query,
we see this path.

32
01:35:59 --> 01:36:03
It consists of eight nodes and
seven edges.

33
01:36:03 --> 01:36:05
And it begins with H and ends with P.

34
01:36:07 --> 01:36:11
Now, another common function we
will use frequently with paths

35
01:36:11 --> 01:36:14
is finding the length
between two specific nodes.

36
01:36:15 --> 01:36:21
So we'll issue the same two lines of code
and then we'll use this new command,

37
01:36:21 --> 01:36:23
length, to return an actual value.

38
01:36:23 --> 01:36:27
We want to be returning an actual path.

39
01:36:27 --> 01:36:28
And we just want a single value.

40
01:36:28 --> 01:36:31
And when we submit this query,
we get the result seven.

41
01:36:31 --> 01:36:35
And we can see that by visually
inspecting the graph or our seven edges.

42
01:36:35 --> 01:36:38
But because most networks
are much more complex than this,

43
01:36:38 --> 01:36:42
we would need to understand the necessary
query to return the length.

44
01:36:44 --> 01:36:46
And ideally,
in the case of our road network,

45
01:36:46 --> 01:36:50
we would want to find the shortest
path between those two nodes.

46
01:36:50 --> 01:36:52
So in this case we're introducing yet

47
01:36:52 --> 01:36:56
another new command specific
to paths called shortestPath.

48
01:36:56 --> 01:36:59
We will use the same variable key, and

49
01:36:59 --> 01:37:04
the same descriptions in our syntax,
in connecting node a with node c.

50
01:37:04 --> 01:37:09
And in this case, were going to look for
the shortest path between node a and

51
01:37:09 --> 01:37:15
node p, and we're going to return that
path as well as the length of that path.

52
01:37:15 --> 01:37:16
And we're just going to
return a single path.

53
01:37:17 --> 01:37:22
And when we submit this query,
we get a path that's five nodes and

54
01:37:22 --> 01:37:27
four edges long and if we look at
the text results that are returned,

55
01:37:27 --> 01:37:32
we'll see a length displayed
in the length column.

56
01:37:32 --> 01:37:35
And that value is 4, and we can see
that by visually inspecting our graph.

57
01:37:38 --> 01:37:43
The next query we are going to demonstrate
is intended to illustrate that

58
01:37:43 --> 01:37:46
there may be more than one shortest path.

59
01:37:46 --> 01:37:50
And so, we may want to know all of the
possible shortest paths in order to make

60
01:37:50 --> 01:37:54
a choice between which one we prefer.

61
01:37:54 --> 01:38:00
So we'll be using a command that is built
into Neo4j called, allShortestPaths.

62
01:38:00 --> 01:38:04
We'll be issuing a similar query
to what we issued previously,

63
01:38:04 --> 01:38:10
we're going to try to find all of the
shortest paths between node a and node p.

64
01:38:10 --> 01:38:16
And instead of the letters a and c, we're
using the terms source and destination.

65
01:38:16 --> 01:38:20
But the results that we're going to return
will actually be in the form of an array.

66
01:38:20 --> 01:38:25
We're using a new term, extract,
which is based on the following.

67
01:38:25 --> 01:38:30
Assuming we have matched our path p,
we want to identify

68
01:38:30 --> 01:38:34
all of the nodes in p and
extract their names.

69
01:38:34 --> 01:38:39
And we'll return these names as a listing,
which we'll call the variable paths.

70
01:38:39 --> 01:38:45
If there's more than one shortest path,
we'll get multiple listings of node names.

71
01:38:45 --> 01:38:50
So when we submit this query, the results
are listed in the rows display and

72
01:38:50 --> 01:38:54
we see there are actually
two shortest paths.

73
01:38:54 --> 01:38:57
They each have five nodes and four edges.

74
01:38:59 --> 01:39:04
Now, we may want to issue a query
that finds the shortest path but

75
01:39:04 --> 01:39:08
with particular constraints or
conditions that we place on them.

76
01:39:09 --> 01:39:14
So in this case we still want
to find the shortest path, but

77
01:39:14 --> 01:39:19
in this case we may want to constrain
the path length to be greater

78
01:39:19 --> 01:39:21
than a particular value, in this case 5.

79
01:39:21 --> 01:39:25
And then, we want to return essentially
the same results that we returned in

80
01:39:25 --> 01:39:26
the previous query.

81
01:39:26 --> 01:39:31
But we'll also want to return the length
of the resulting path just so

82
01:39:31 --> 01:39:33
we have that information conveniently.

83
01:39:33 --> 01:39:35
So when we issue this command

84
01:39:36 --> 01:39:41
we get a path with length six
between node A and node P.

85
01:39:41 --> 01:39:45
So it's clearly longer than the shortest
path that we had found earlier.

86
01:39:48 --> 01:39:52
Now that we are somewhat familiar
with the two shortest path commands,

87
01:39:52 --> 01:39:55
the shortest path, or a single path and

88
01:39:55 --> 01:40:00
the all shortest paths command or multiple
shortest paths, we're going to use

89
01:40:00 --> 01:40:06
that in a little bit of a creative way
to return the diameter of the graph.

90
01:40:06 --> 01:40:08
And if you remember from
a previous lecture,

91
01:40:08 --> 01:40:11
the definition of
the diameter of the graph

92
01:40:11 --> 01:40:17
is actually the longest continuous
path between two nodes in the graph.

93
01:40:17 --> 01:40:23
So by using the shortest path command, but
returning all possible shortest paths,

94
01:40:23 --> 01:40:28
we're actually going to get the longest
path included in those results returned.

95
01:40:28 --> 01:40:30
Now, if we look carefully at this script,

96
01:40:30 --> 01:40:33
it is a little different
than our previous scripts.

97
01:40:33 --> 01:40:38
In this case our match command is
matching all nodes of type MyNode.

98
01:40:38 --> 01:40:41
We'll assign those to the variable end.

99
01:40:41 --> 01:40:47
We're also matching the all nodes of type
MyNode and assigning that to variable m.

100
01:40:47 --> 01:40:48
So these matches are the same.

101
01:40:48 --> 01:40:53
But we want to place a constraint
such that the nodes in n are not

102
01:40:53 --> 01:40:55
the same as the nodes in m, and

103
01:40:55 --> 01:41:01
then we want to find all of the shortest
paths between unique nodes in n and m.

104
01:41:01 --> 01:41:07
And return the names of those nodes as
well as the length of that resulting path.

105
01:41:07 --> 01:41:10
And the trick is to use
the command order by.

106
01:41:10 --> 01:41:15
And so for those of you who are familiar
already with SQL query language,

107
01:41:15 --> 01:41:17
you'll recognize order by.

108
01:41:17 --> 01:41:20
You'll also recognize the descend command.

109
01:41:20 --> 01:41:26
So if we order the resulting paths
by their length in descending order,

110
01:41:26 --> 01:41:31
and only return 1, that path should
actually be the longest path.

111
01:41:31 --> 01:41:33
And that's equal to
the diameter of the graph.

112
01:41:34 --> 01:41:38
So when we submit this query,
here's the results that we get.

113
01:41:38 --> 01:41:43
We get a path between node e and
node l with length severn.

114
01:41:43 --> 01:41:48
Or maybe it occurs to you that maybe this
is not the only diameter of the graph,

115
01:41:48 --> 01:41:51
the only path with length of seven.

116
01:41:52 --> 01:41:59
So we can modify our query just a little
bit and change the limit from one to five.

117
01:41:59 --> 01:42:02
And we'll see the results.

118
01:42:02 --> 01:42:04
And sure enough,
we actually get five paths.

119
01:42:05 --> 01:42:08
And 3 of those have length 7.

120
01:42:08 --> 01:42:12
So there are actually three
distinct paths which qualify as

121
01:42:12 --> 01:42:15
a diameter of this particular graph.

1
03:16:09 --> 03:16:13
So up until now we've been
calculating path length

2
03:16:13 --> 03:16:17
based on the number of hops between
our beginning node and our end node.

3
03:16:17 --> 03:16:22
This is roughly equivalent to counting
the number of towns between one town and

4
03:16:22 --> 03:16:23
another town.

5
03:16:23 --> 03:16:26
But it doesn't really get at
the value that is usually of

6
03:16:26 --> 03:16:28
greatest importance to us.

7
03:16:28 --> 03:16:33
And that is the actual distance between
one location and another location.

8
03:16:33 --> 03:16:38
Which is found in the values that we've
assigned to the edges between the nodes.

9
03:16:38 --> 03:16:42
So in this next example, we're going
to perform that kind of a calculation.

10
03:16:42 --> 03:16:46
So the first two lines of code
we're already fairly familiar with.

11
03:16:46 --> 03:16:50
We're matching a path between node a and
node c,

12
03:16:50 --> 03:16:54
where the first node is H,
and the second node is P.

13
03:16:54 --> 03:16:57
And this third line of code
is also fairly familiar.

14
03:16:57 --> 03:17:02
We're extracting the names of the nodes
and the path that's been returned, and

15
03:17:02 --> 03:17:07
we're returning a listing of those
names as well as a length of the path.

16
03:17:08 --> 03:17:11
All of that is being returned
as the variable pathLength.

17
03:17:11 --> 03:17:15
We've added a third element
to our return statement, and

18
03:17:15 --> 03:17:17
that is using the reduce statement.

19
03:17:17 --> 03:17:22
So what we're doing here is,
the purpose of the reduce statement,

20
03:17:22 --> 03:17:26
takes a set of values and
reduces them down to a single value.

21
03:17:27 --> 03:17:32
So in this line of code we begin by
setting a variable s equal to 0.

22
03:17:32 --> 03:17:37
And then we define a variable E, which
represents the set of relationships in

23
03:17:37 --> 03:17:40
a path that's returned, or
in other words, the edges.

24
03:17:40 --> 03:17:43
And we pass that into this variable s,
and add to it,

25
03:17:43 --> 03:17:47
the value of the distance that
we've assigned to that edge.

26
03:17:49 --> 03:17:54
So we're performing
an aggregate calculation.

27
03:17:54 --> 03:17:59
And returning the final results
to a variable called pathDist.

28
03:17:59 --> 03:18:01
And we're limiting that
results to a single value.

29
03:18:01 --> 03:18:06
And so when we do this, we should
get a value that is more indicative

30
03:18:06 --> 03:18:12
of the actual distance between
our source and our destination.

31
03:18:12 --> 03:18:13
And so here's the results.

32
03:18:13 --> 03:18:16
The path itself, as we know,
begins in H and ends in P.

33
03:18:16 --> 03:18:22
And it has a pathLength of 7,
but it has a pathDist of 39.

34
03:18:22 --> 03:18:26
So we could interpret this to mean
that even though there are 6 towns

35
03:18:26 --> 03:18:30
between the source town and
the destination town, or

36
03:18:30 --> 03:18:35
a pathLength of 7, the actual distance
in miles would be a value of 39.

37
03:18:37 --> 03:18:39
So with that we can apply
Dijkstra's algorithm.

38
03:18:40 --> 03:18:47
So here I'm going to match the node with
the name A, and the node with the name P.

39
03:18:47 --> 03:18:53
And we're going to find the shortest
path in terms of hops from A to P.

40
03:18:53 --> 03:18:57
And we'll set that equal
to the variable path.

41
03:18:57 --> 03:19:02
Then we'll perform a reduce command,
and set the variable dist = 0.

42
03:19:02 --> 03:19:04
And we'll go through, and

43
03:19:04 --> 03:19:09
sum all of the distances of each
of the edges in our shortest path.

44
03:19:09 --> 03:19:12
And return that value as a distance.

45
03:19:12 --> 03:19:16
And we'll also return the path variable.

46
03:19:16 --> 03:19:21
So remember, this is not the path in
our network with the least weights.

47
03:19:21 --> 03:19:28
It is the weight of the shortest
path based on numbers of hops.

48
03:19:28 --> 03:19:32
Now that's an inherent feature of
the shortest path command in Cipher.

49
03:19:32 --> 03:19:34
So here is the path that's returned.

50
03:19:34 --> 03:19:38
It's a five node path with four edges and

51
03:19:38 --> 03:19:43
the total sum of the weights of
those edges sums to a value of 22.

52
03:19:45 --> 03:19:49
So in our previous query,
we specified that we wanted a match for

53
03:19:49 --> 03:19:53
the source node and the destination node.

54
03:19:53 --> 03:19:58
But if I don't specify my destination
node, I can apply Dijkstra's single

55
03:19:58 --> 03:20:03
source shortest path algorithm
from node A to any other node.

56
03:20:04 --> 03:20:09
So when we apply this query,
the results displayed consist of

57
03:20:09 --> 03:20:14
the actual original path from
A to P with a distance of 22.

58
03:20:14 --> 03:20:19
And we'll see a display of all of
the intermediate paths generated in

59
03:20:19 --> 03:20:24
the process, all the way down to
a single edge path between A and C.

60
03:20:25 --> 03:20:31
So just to reiterate, what we've
calculated is the shortest hop path

61
03:20:31 --> 03:20:37
with the weights added, the sum of
the weights of the edges in that path.

62
03:20:37 --> 03:20:40
This is not the least weight
path of the entire network.

63
03:20:43 --> 03:20:46
Okay, so let's switch gears for a moment.

64
03:20:46 --> 03:20:51
As we learned in one of our previous
lectures, we can extract a subset

65
03:20:51 --> 03:20:56
of nodes and edges from a particular
graph for various reasons.

66
03:20:56 --> 03:21:00
Let's say for example that we want
to avoid a particular town or

67
03:21:00 --> 03:21:02
a particular area where
there might be congestion.

68
03:21:02 --> 03:21:06
And this would be represented
by one of our nodes.

69
03:21:06 --> 03:21:09
So we're going to perform a similar
match as we've done in the past,

70
03:21:09 --> 03:21:15
where we're going to match any node n with
any node m, with a two edge between them.

71
03:21:15 --> 03:21:20
But we want to apply an additional
constrained in which none of the n nodes

72
03:21:20 --> 03:21:22
are going to include the node D.

73
03:21:22 --> 03:21:27
And none of the m nodes are going
to include the node D as well.

74
03:21:27 --> 03:21:30
And then we'll return the resulting graph.

75
03:21:31 --> 03:21:35
So when we do that, we get a graph but
looks very much as we might expect.

76
03:21:35 --> 03:21:39
Very similar to our previous graph,
but it is missing node D, so

77
03:21:39 --> 03:21:44
it only has ten nodes and
it's now been reduced to ten edges.

78
03:21:47 --> 03:21:51
So now let's say we want to calculate
the shortest path over there graph that we

79
03:21:51 --> 03:21:54
just returned in the previous query.

80
03:21:54 --> 03:22:00
So in this case, we're going to match the
shortest path between node A and node P.

81
03:22:00 --> 03:22:04
But in the second line, we want to
issue sort of a negative statement

82
03:22:04 --> 03:22:09
in which the resulting list of
node names that we extract using

83
03:22:09 --> 03:22:13
the extract statement
cannot contain the node D.

84
03:22:15 --> 03:22:18
And then we'll return that path and
the length of that path.

85
03:22:18 --> 03:22:23
So when we issue this command,
here's our resulting path.

86
03:22:23 --> 03:22:26
It's a five node path of length four.

87
03:22:26 --> 03:22:30
As we recall from one of our earlier
queries where we were trying to calculate

88
03:22:30 --> 03:22:34
all of the shortest paths,
we returned two paths.

89
03:22:34 --> 03:22:38
One path that contained D, and this is
the second path, which did not contain D.

90
03:22:41 --> 03:22:44
So we can make this a little complicated.

91
03:22:44 --> 03:22:49
Instead of avoiding a single node in
our resulting path, we're looking for

92
03:22:49 --> 03:22:55
a graph that doesn't contain the immediate
neighborhood of a specific node.

93
03:22:55 --> 03:22:59
This means all of the nearest, or
the first neighbors of a specific node.

94
03:22:59 --> 03:23:04
So in this case we're going
to match the same node D, and

95
03:23:04 --> 03:23:08
all edges between D, and any other node.

96
03:23:08 --> 03:23:12
And then we are going to issue
a collect command to collect all of

97
03:23:12 --> 03:23:14
the distinct neighbors of D.

98
03:23:14 --> 03:23:19
And we'll apply a constraint to that,
in which the returned

99
03:23:19 --> 03:23:24
list of neighbors cannot contain
the node with the name D.

100
03:23:24 --> 03:23:28
Likewise, the neighbors list for

101
03:23:28 --> 03:23:32
the target nodes,
can also not contain the node D.

102
03:23:32 --> 03:23:36
And when we submit this command,
we see a network that looks like this.

103
03:23:36 --> 03:23:39
Five nodes and four edges.

104
03:23:39 --> 03:23:41
And that seems to makes sense.

105
03:23:41 --> 03:23:46
Node D isn't in the network,
nor are its first neighbors.

106
03:23:46 --> 03:23:48
But if you recall the original network,

107
03:23:48 --> 03:23:54
there may be a peculiar result that you
might find a little bit disconcerting.

108
03:23:54 --> 03:23:57
So let's look at our original network.

109
03:23:57 --> 03:24:00
Now here's the node D, and
here are all it's nearest neighbors.

110
03:24:00 --> 03:24:03
So these are the forbidden
neighbors that we want to

111
03:24:03 --> 03:24:05
remove from our resulting graph.

112
03:24:06 --> 03:24:08
And we seemed to have
done that successfully.

113
03:24:08 --> 03:24:12
These are the five nodes that
are retained in the resulting graph, but

114
03:24:12 --> 03:24:13
there's a node out here.

115
03:24:13 --> 03:24:20
The node p, which seems to be neglected or
not handled in the results.

116
03:24:20 --> 03:24:22
It's not a first neighbor of D.

117
03:24:23 --> 03:24:27
So it, in some ways arguably should
be returned in our results, but

118
03:24:27 --> 03:24:31
it's not part of the connected
graph that we saw returned.

119
03:24:31 --> 03:24:37
This is one area in which Cipher does
not handle these situations by default.

120
03:24:37 --> 03:24:42
So we'll need to supplement our
query with an additional query.

121
03:24:42 --> 03:24:47
In this case, the node P was a leaf node,
so we want to make sure that

122
03:24:47 --> 03:24:53
not only matching the nodes that
conform to these constraints above.

123
03:24:54 --> 03:24:59
But we also want to include the node or
any nodes which are leaf nodes,

124
03:24:59 --> 03:25:05
which may also be arguably part of the
results that you expect to be returned.

125
03:25:05 --> 03:25:08
Now in our network,
we do have one root node,

126
03:25:08 --> 03:25:11
but it doesn't impact the results
in this particular query.

127
03:25:11 --> 03:25:14
In the interests of being complete, for

128
03:25:14 --> 03:25:19
any network, most networks being much more
complex than the one we're working with.

129
03:25:19 --> 03:25:24
We'd want to take into account not only
those leaf nodes that might be left out,

130
03:25:24 --> 03:25:26
but also any root notes
that might be left out.

131
03:25:29 --> 03:25:34
And finally, our last query
example extends the previous query

132
03:25:34 --> 03:25:38
to find the graph which doesn't
contain a selective neighborhood,

133
03:25:38 --> 03:25:43
in this case,
the two neighborhood of a particular node.

134
03:25:43 --> 03:25:44
In this example we're
going to use the node F.

135
03:25:44 --> 03:25:51
And we want to eliminate all of
the second neighbors of that node.

136
03:25:51 --> 03:25:55
So initially we match all of those
nodes that are second neighbors of F,

137
03:25:55 --> 03:25:57
including F itself.

138
03:25:57 --> 03:26:01
And we'll place those in
a variable called MyList.

139
03:26:01 --> 03:26:04
Then we go back through the network and
match all of the nodes and

140
03:26:04 --> 03:26:09
edges, where the source nodes are not
part of the nodes in the MyList and

141
03:26:09 --> 03:26:12
the target nodes are not
contained in MyList.

142
03:26:12 --> 03:26:14
And then we return those nodes and edges.

143
03:26:16 --> 03:26:17
And here's the resulting graph.

144
03:26:20 --> 03:26:25
It does not contain F or
its first or second neighbors.

145
03:26:25 --> 03:26:32
If we scroll down to look at the original
graph, here's node F, nodes H,

146
03:26:32 --> 03:26:38
J, C, A and L are all the first and
second neighbors of F.

147
03:26:38 --> 03:26:43
So those should be eliminated from the
graph that gets returned from our results.

148
03:26:43 --> 03:26:44
And sure enough,

149
03:26:44 --> 03:26:50
this subset of nodes represents the
results that were returned from our query.

150
03:26:50 --> 03:26:55
It consists of nodes B, D, E, G, and P.

151
03:26:55 --> 03:26:58
And so this concludes our review
of some of the more advanced

152
03:26:58 --> 03:27:00
path analytics queries.

153
03:27:00 --> 03:27:02
We were using a simple network, but

154
03:27:02 --> 03:27:06
we're providing additional data
sets that are much larger and

155
03:27:06 --> 03:27:10
present a more realistic challenge
in applying pathanalythics queries.

1
06:43:11 --> 06:43:16
Hello everyone and welcome to this week's
module on graph analytics with Neo4j

2
06:43:16 --> 06:43:18
using the Cypher query language.

3
06:43:18 --> 06:43:21
I'm Jeff Sale and I'll be your
instructor for this series of lessons.

4
06:43:21 --> 06:43:25
I've been an instructional designer at
the San Diego Supercomputer Center for

5
06:43:25 --> 06:43:30
more than ten years, but I've also had
a passion for scientific visualization and

6
06:43:30 --> 06:43:33
visual analytics in one form or
another for over two decades.

7
06:43:33 --> 06:43:37
And I'm very excited about this
opportunity to introduce you to this

8
06:43:37 --> 06:43:40
free and very powerful graph
analytics tool called Neo4j.

9
06:43:41 --> 06:43:44
First we realized that many of you
may not have the systems capable of

10
06:43:44 --> 06:43:48
pushing the boundaries of
Neo4j's performance limits.

11
06:43:48 --> 06:43:51
Plus the fact that many of you are fitting
this course into your already busy

12
06:43:51 --> 06:43:56
schedules means we'll be working with data
sets, which will load into Neo4j and can

13
06:43:56 --> 06:44:02
be analyze in a reasonable length of time,
on the order of minutes not hours or days.

14
06:44:02 --> 06:44:06
However you can be sure that Neo4j
is capable of processing and

15
06:44:06 --> 06:44:10
analyzing extremely complex graph networks
consisting of millions of nodes and

16
06:44:10 --> 06:44:11
relationships.

17
06:44:12 --> 06:44:16
This module consists of a series of
hands-on demonstrations with Neo4j,

18
06:44:16 --> 06:44:19
which begin with examples of
some basic cypher queries

19
06:44:19 --> 06:44:23
that soon progress to some of
the more advanced cypher queries.

20
06:44:23 --> 06:44:27
We'll begin by using a relatively simple
graph representing a road network, but

21
06:44:27 --> 06:44:30
we'll also use much larger and
more complex data sets,

22
06:44:30 --> 06:44:34
including sociological data on
global terrorist groups and

23
06:44:34 --> 06:44:38
genetics data on associations and
interactions between genes.

24
06:44:38 --> 06:44:41
These data sets are in fact sub
sets of much larger data sets and

25
06:44:41 --> 06:44:46
we're making both of the sub sets and
complete data sets available for download.

26
06:44:46 --> 06:44:49
Once you become comfortable working
with the smaller data sets,

27
06:44:49 --> 06:44:51
we encourage you to explore
the larger sets on your own.

28
06:44:52 --> 06:44:56
Finally you'll notice that each
video is accompanied by a text file

29
06:44:56 --> 06:44:59
containing all of the code used
in the video demonstrations.

30
06:44:59 --> 06:45:03
These files include dozens of sample
scripts, written in cypher, designed to

31
06:45:03 --> 06:45:08
make it easy for you to learn, not only
basic cypher cards, but also queries which

32
06:45:08 --> 06:45:12
focus on more advanced methods such
pathenoids and connectivity analytics.

33
06:45:14 --> 06:45:17
When you're finished with this module
you'll be able to write cypher scripts to

34
06:45:17 --> 06:45:21
import and
analyze your own data using Neo4j.

35
06:45:21 --> 06:45:24
So thank you for
enrolling in this course and let's get

36
06:45:24 --> 06:45:28
started doing some real graph analytics
with Neo4j and the Cypher query language.

1
13:28:39 --> 13:28:42
A parallel computation is at
the heart of big data computing.

2
13:28:42 --> 13:28:47
However, to specify what becomes parallel,
we usually think of a conceptual model

3
13:28:47 --> 13:28:50
of parallel computation often called,
a programming model.

4
13:28:52 --> 13:28:55
A parallel programming model is
a way to specify abstractly,

5
13:28:55 --> 13:28:57
how a parallel program will run.

6
13:28:58 --> 13:29:00
Naturally for a program to be parallel,

7
13:29:00 --> 13:29:05
there must be a number of
concurrently operating processes.

8
13:29:05 --> 13:29:07
But how do these processes communicate and
exchange data?

9
13:29:08 --> 13:29:11
How do they decide,
when to communicate with each other?

10
13:29:12 --> 13:29:14
Further, what exactly is done in parallel?

11
13:29:15 --> 13:29:17
To think of the first question.

12
13:29:17 --> 13:29:21
Two processes communicate
data by sharing memory.

13
13:29:23 --> 13:29:27
Indeed, there are architectures in which
all memory in multiple machines can be

14
13:29:27 --> 13:29:30
made to virtually look like,
one large addressable memory space.

15
13:29:31 --> 13:29:33
However, two processes,

16
13:29:33 --> 13:29:37
we also communicate by passing
messages to one another.

17
13:29:37 --> 13:29:40
Either, directly from one
process to another, or

18
13:29:40 --> 13:29:43
through a common message carrying pipe,
often called a message bus.

19
13:29:45 --> 13:29:48
The second question can
also have multiple answers.

20
13:29:49 --> 13:29:54
Two of the most common ways of achieving
parallelism are pass parallelism and

21
13:29:54 --> 13:29:55
data parallels.

22
13:29:55 --> 13:30:00
In task parallelism, a large task can
be decomposed into multiple sub-tasks,

23
13:30:00 --> 13:30:02
each of which can be run concurrently.

24
13:30:03 --> 13:30:08
In data parallelism, the data can be
partitioned into many smaller fragments

25
13:30:08 --> 13:30:11
and operation can run on each partition,
independent of the others.

26
13:30:13 --> 13:30:17
Typically, these partial operations
have then synchronized and

27
13:30:17 --> 13:30:20
partially process data combined
to produce a full answer.

28
13:30:21 --> 13:30:23
Many parallel data management systems,

29
13:30:23 --> 13:30:25
operate a partition due
to parallel manner.

30
13:30:27 --> 13:30:30
We need to remember that task
parallelism is somewhat independent of

31
13:30:30 --> 13:30:31
data parallelism.

32
13:30:31 --> 13:30:35
And it is possible to have both problems
of parallelism in a programming model.

33
13:30:36 --> 13:30:39
It is important to emphasize
the issue of a programming model,

34
13:30:39 --> 13:30:43
should be not be confused with
the issue of a programming language.

35
13:30:44 --> 13:30:47
A programming language is independent
of the programming model.

36
13:30:47 --> 13:30:49
And therefore,
a programming model can be implemented

37
13:30:49 --> 13:30:51
in several different languages.

38
13:30:52 --> 13:30:56
As I mentioned, the programming model
we are going to consider is BSP.

39
13:30:57 --> 13:31:01
BSP wasn't initially created for
graph computation.

40
13:31:01 --> 13:31:03
It was thought of as
a parallel computing model,

41
13:31:03 --> 13:31:07
that will bridge the gap between software
models of parallel computation, and

42
13:31:07 --> 13:31:10
hardware capabilities for
supporting parallelism.

43
13:31:10 --> 13:31:13
The basic idea of BSP is as follows.

44
13:31:14 --> 13:31:16
There are number of processors.

45
13:31:16 --> 13:31:20
Each processor can perform local
computation, using its own local memory.

46
13:31:22 --> 13:31:28
There's a router, which can serve to pass
a message from any processor to any other.

47
13:31:29 --> 13:31:32
When one pair of nodes
are exchanging messages,

48
13:31:32 --> 13:31:34
another third node can
still perform computation.

49
13:31:37 --> 13:31:40
There's a facility that can
synchronize the state of

50
13:31:40 --> 13:31:41
all auto-substative processes.

51
13:31:43 --> 13:31:45
This synchronize may either
happen periodically,

52
13:31:45 --> 13:31:51
at intervals of L time units or
there may be another way of specifying,

53
13:31:51 --> 13:31:53
when this synchronization
is going to happen.

54
13:31:53 --> 13:31:58
But when it does, all processors affected
by it, will come to a consistent state.

55
13:31:59 --> 13:32:04
When synchronization is performed,
a fresh round of computation can start.

56
13:32:04 --> 13:32:05
We call this.

57
13:32:05 --> 13:32:07
Synchronization point.

58
13:32:07 --> 13:32:08
Barrier synchronization.

59
13:32:08 --> 13:32:12
Because all executed processes
must reach this barrier point,

60
13:32:12 --> 13:32:15
before the next step of
processing can continue.

61
13:32:15 --> 13:32:19
A BSP program is broken up into
a sequence stop supersteps.

62
13:32:21 --> 13:32:25
In each superstep, each processor
will get the data, if needed.

63
13:32:25 --> 13:32:28
Performance computation if needed and

64
13:32:28 --> 13:32:30
then, exchange data
with the right partner.

65
13:32:31 --> 13:32:35
Once all the nodes are done,
the system helps to synchronize.

66
13:32:35 --> 13:32:36
Then, the next round starts.

67
13:32:37 --> 13:32:42
Each processor can determine,
if it needs to compute or exchange data.

68
13:32:42 --> 13:32:44
If not, it will make itself inactive.

69
13:32:45 --> 13:32:49
If required later, a processor can
be woken up to be active again.

70
13:32:51 --> 13:32:54
When all processors are inactive,
the computation stops.

71
13:32:56 --> 13:32:59
In applying BSP model to graphs,
we make a few assumptions.

72
13:33:00 --> 13:33:03
We assume that a processor
is synonymous with a vertex.

73
13:33:05 --> 13:33:09
So for
a processor can only send messages to or

74
13:33:09 --> 13:33:11
receive from, its neighboring processes.

75
13:33:13 --> 13:33:18
We also assume, a vertex has an ID and
possibly a complex value.

76
13:33:18 --> 13:33:21
And an edge,
we also have an idea and fact.

77
13:33:22 --> 13:33:25
Each vertex knows,
which edges it's connected to.

78
13:33:27 --> 13:33:31
We cannot think of a computation
as a vertex centered task.

79
13:33:31 --> 13:33:34
We shall [INAUDIBLE] what this means.

80
13:33:36 --> 13:33:38
In a now famous paper from Google.

81
13:33:38 --> 13:33:42
This programming model was called,
think like a vertex.

82
13:33:43 --> 13:33:46
Well to think like a vertex,
we need to know what a vertex can do.

83
13:33:47 --> 13:33:49
Here's a list of actions,
a vertex can take.

84
13:33:52 --> 13:33:53
The first one is easy.

85
13:33:53 --> 13:33:55
A vertex can find its own identifier.

86
13:33:56 --> 13:34:01
The second operation is to get or
set the value of the node.

87
13:34:01 --> 13:34:05
This operation may be a little involved,
if the value is a complex data object.

88
13:34:05 --> 13:34:08
For our purposes,
we'll assume the value is a scalar.

89
13:34:09 --> 13:34:11
Next, a node can ask for

90
13:34:11 --> 13:34:15
its own edges, the result of
the operation is a set of edge objects.

91
13:34:16 --> 13:34:18
A node may also count its edges.

92
13:34:19 --> 13:34:22
Since we are referring to
outgoing edges throughout,

93
13:34:22 --> 13:34:24
this is the out degree of the vertex.

94
13:34:25 --> 13:34:26
Recognize that,

95
13:34:26 --> 13:34:31
this means a vertex does not natively
have access to its incident notes.

96
13:34:33 --> 13:34:36
However, it does have control
of the outgoing edges.

97
13:34:36 --> 13:34:38
So it can get inside the edge values.

98
13:34:40 --> 13:34:43
There may be two different
ways of specifying an edge.

99
13:34:43 --> 13:34:46
The edge we have an ID
that the node can get.

100
13:34:46 --> 13:34:50
Passively, more commonly, an edge is
identified the vertex of targets.

101
13:34:51 --> 13:34:55
So in our diagram V1 lasts for
the edge, targeting V4.

102
13:34:57 --> 13:35:03
So in the situation like v3 and v5, we're
there are multiple edges between v3 and

103
13:35:03 --> 13:35:09
v5, the source v3 can ask for
the values of all edges going to v5.

104
13:35:11 --> 13:35:14
The operate operation can add or
remove an edge of a vertex.

105
13:35:16 --> 13:35:22
Finally, since the vertices are processes,
they can start or stop computing.

106
13:35:22 --> 13:35:25
Typically a node wakes up, if it
receives a message from another node.

107
13:35:25 --> 13:35:30
Now in comparison to a vertex,
an edge can do far less.

108
13:35:31 --> 13:35:36
It can get its own ID if the system
allows edge ID's, it can set and

109
13:35:36 --> 13:35:41
retrieve its own values, and it can get
the ID of the node its pointing to.

110
13:35:41 --> 13:35:41
That's it.

111
13:35:43 --> 13:35:46
Now, we still have not defined,
how to think like a vertex.

112
13:35:46 --> 13:35:48
That's what we'll do next,

113
13:35:48 --> 13:35:52
using an example that we have
seen several times before.

114
13:35:52 --> 13:35:56
It's Dijkstra's single source
shortest path, SSSP, algorithm.

115
13:35:58 --> 13:36:00
We have seen this algorithm before.

116
13:36:00 --> 13:36:04
But now, we'll show how to compute
it in a parallel setting using BSP.

117
13:36:06 --> 13:36:09
Here is the edge value,
that's the weight of the edge.

118
13:36:10 --> 13:36:12
This is known before the algorithm starts.

119
13:36:14 --> 13:36:18
Each vertex,
runs the exact same routine concurrently.

120
13:36:19 --> 13:36:20
Each vertex asks.

121
13:36:20 --> 13:36:23
1, is it super step zero?

122
13:36:23 --> 13:36:26
2, if yes,

123
13:36:26 --> 13:36:31
then if this is a source vertex,
it sets the value to zero.

124
13:36:32 --> 13:36:35
Else, it sets the value to infinity,
which is a large number.

125
13:36:37 --> 13:36:38
The source vertex,

126
13:36:38 --> 13:36:42
propagates its edge value to the nodes
at the other end of the edges.

127
13:36:42 --> 13:36:43
Just the source vertex.

128
13:36:44 --> 13:36:45
All other vertices are quiet.

129
13:36:47 --> 13:36:50
The propagation process works like this.

130
13:36:50 --> 13:36:54
A vertex gets its own value,
which for the source vertex is zero.

131
13:36:54 --> 13:37:00
It gets its edges, for each edge,
it gets the value of the edge,

132
13:37:00 --> 13:37:06
adds it to its own value and sends
the result to the end point of the edge.

133
13:37:07 --> 13:37:10
The blue numbers indicate
that the messages are sent.

134
13:37:10 --> 13:37:12
All vertices go to halt.

135
13:37:12 --> 13:37:16
That is, they now have hit
a synchronization barrier.

136
13:37:17 --> 13:37:20
Notice, that the receiving nodes
do not look at the messages yet.

137
13:37:21 --> 13:37:25
It's the system's job to ensure that
the messages are available to these nodes

138
13:37:25 --> 13:37:26
at the next superstep.

139
13:37:28 --> 13:37:33
All nodes who have received messages
wake up and read the messages.

140
13:37:33 --> 13:37:37
If a node receives multiple messages,
it picks the minimum.

141
13:37:37 --> 13:37:43
In our case, the two active nodes
have received only one value each.

142
13:37:43 --> 13:37:47
We have for the sake of convenience,
colored the processed edges in yellow.

143
13:37:47 --> 13:37:50
This is just for
visualization purposes in this example.

144
13:37:51 --> 13:37:56
Now, it compares this band for
a minimum value, to its own value.

145
13:37:56 --> 13:38:03
And if its own value is greater, it sets
its own value with the minimum value.

146
13:38:03 --> 13:38:08
In our case, both notes set their value
to that, of the incoming message.

147
13:38:09 --> 13:38:12
The same propagation routine works again.

148
13:38:12 --> 13:38:15
So, each note completes the new distance.

149
13:38:16 --> 13:38:20
And sends the message along an edge
to the other endpoint, then halts.

150
13:38:21 --> 13:38:25
The same step is repeated
in the next superstep.

151
13:38:26 --> 13:38:30
At this point,
the nodes have updated their values.

152
13:38:30 --> 13:38:32
The node with the value 6,

153
13:38:32 --> 13:38:37
has received our message,
just along one of the three edges on it.

154
13:38:38 --> 13:38:38
Continuing.

155
13:38:40 --> 13:38:41
At the end of superstep 2,

156
13:38:41 --> 13:38:46
all nodes are ready to receive messages
from all their incident edges.

157
13:38:48 --> 13:38:52
The node with a value 6, received a value
which is lower than its current value.

158
13:38:53 --> 13:38:57
Now the active nodes,
have no more messages to send.

159
13:38:57 --> 13:38:59
So each vertex, votes to halt.

160
13:39:00 --> 13:39:04
The vertex ID and the value
are read out from each vertex, and

161
13:39:04 --> 13:39:06
then the process starts.

162
13:39:06 --> 13:39:10
If these nodes are in different
machines of a cluster,

163
13:39:10 --> 13:39:14
the system will rely on
the underlying platform like YARN.

164
13:39:14 --> 13:39:16
Or sparks underlying
infrastructure to ensure,

165
13:39:16 --> 13:39:20
that the edges going across machines can
send and receive messages effectively.

166
13:39:21 --> 13:39:25
This should give you a sense of the speed
of this process for a large scale network.

1
03:08:04 --> 03:08:09
As we said earlier,
while the Giraph paradigm implements BSB,

2
03:08:09 --> 03:08:11
it must also be pragmatic.

3
03:08:12 --> 03:08:16
One such point of pragmatism is
the computation of aggregate values.

4
03:08:17 --> 03:08:20
In the think like a vertex paradigm,

5
03:08:20 --> 03:08:24
operations local to a vertex can
be performed in parallel, and

6
03:08:24 --> 03:08:28
each vertex only has to work
with its immediate neighborhood.

7
03:08:28 --> 03:08:32
This is very useful, but
it isn't sufficient at times.

8
03:08:32 --> 03:08:36
For example, we need to know and use
the total number of edges in the graph.

9
03:08:37 --> 03:08:41
This will be computed by adding
edges connected to each vertex, but

10
03:08:41 --> 03:08:45
once aggregated,
it does not belong to any specific vertex.

11
03:08:46 --> 03:08:49
So whom does this vertex
send the aggregate to?

12
03:08:50 --> 03:08:54
Also suppose, a vertex creates some
edges as part of its compute step.

13
03:08:55 --> 03:08:58
When does this information get sent out?

14
03:08:59 --> 03:09:01
Let's answer the first question first.

15
03:09:02 --> 03:09:07
The class in charge of these aggregates
is called the DefaultMasterCompute class.

16
03:09:08 --> 03:09:11
This class specializes MasterCompute.

17
03:09:13 --> 03:09:16
How does the DefaultMasterCompute
relate to the basic vertex computation?

18
03:09:18 --> 03:09:23
We have seen that all vertex programs are
created by first defining a vertex class.

19
03:09:24 --> 03:09:29
This class has a compute function that
performs like the think like a vertex

20
03:09:29 --> 03:09:29
logic.

21
03:09:31 --> 03:09:36
Now let's add to it a basic vertex
function and an aggregate function.

22
03:09:36 --> 03:09:37
Now this function, for

23
03:09:37 --> 03:09:42
our example of all the total number
of aggregate edges looks like this.

24
03:09:44 --> 03:09:46
The name of the function is aggregate.

25
03:09:47 --> 03:09:51
As you can see in yellow, it just gets
the number of edges off this vertex.

26
03:09:53 --> 03:09:59
What's a little strange is that the first
argument of this aggregation has

27
03:09:59 --> 03:10:03
an id of something whose
name is total number edges.

28
03:10:05 --> 03:10:07
What really happens is as follows.

29
03:10:07 --> 03:10:12
The defaltMasterCompute class, which
is in charge of this global aggregate

30
03:10:12 --> 03:10:15
operations, is specialized
by your aggregate class.

31
03:10:16 --> 03:10:18
This class has an ID, and

32
03:10:18 --> 03:10:24
the aggregator gets registered with
Giraph's defaultMasterCompute class.

33
03:10:25 --> 03:10:32
The ID we saw before refers to your
registered aggregator's classes ID.

34
03:10:32 --> 03:10:36
The MasterCompute class performs
the centralized computation between

35
03:10:36 --> 03:10:37
supersteps.

36
03:10:37 --> 03:10:41
This class is initiated
on the master node and

37
03:10:41 --> 03:10:45
will run every superstep
before the workers do.

38
03:10:45 --> 03:10:49
Communication with the workers
should be performed via aggregators.

39
03:10:51 --> 03:10:54
The values of the aggregators
are broadcast to the workers before

40
03:10:54 --> 03:10:56
the vertex compute is called.

41
03:10:56 --> 03:11:00
And collected by the master before
the master compute is called.

42
03:11:00 --> 03:11:05
This means the aggregator values
used by the workers are consistent

43
03:11:05 --> 03:11:10
with the aggregator values from
the master from the same superstep.

44
03:11:10 --> 03:11:12
And the aggregator used by
the master are consistent

45
03:11:12 --> 03:11:16
with the aggregator values from
the workers from the previous superstep.

46
03:11:17 --> 03:11:20
Now let's go back to the big picture for
a second.

47
03:11:20 --> 03:11:21
Giraph is a big data software.

48
03:11:23 --> 03:11:28
Why not implement the Giraph system
with a set of MapReduce jobs?

49
03:11:28 --> 03:11:29
Too much disk requirement.

50
03:11:29 --> 03:11:32
No in-memory caching.

51
03:11:32 --> 03:11:34
Every superstep becomes a job.

52
03:11:34 --> 03:11:38
So all intermediate steps
are written to files, and

53
03:11:38 --> 03:11:42
that is not a very scalable solution for
iterative operations.

54
03:11:42 --> 03:11:46
However, it will be incorrect
to think that Giraph works

55
03:11:46 --> 03:11:48
only in an in memory process.

56
03:11:49 --> 03:11:52
It can always have less
memory than it needs.

57
03:11:52 --> 03:11:56
There are two broad categories of
what's called out-of-core computation.

58
03:11:58 --> 03:12:01
The first situation occurs when the graph
is really large compared to the capacity

59
03:12:01 --> 03:12:02
of the cluster it's running on.

60
03:12:03 --> 03:12:08
Each worker stores the vertices assigned
to it inside a set of partitions.

61
03:12:09 --> 03:12:13
Inside a partition are a subset of
vertices together with their data.

62
03:12:14 --> 03:12:17
During a superstep, the worker node

63
03:12:17 --> 03:12:22
processes multiple partitions
concurrently, one per thread.

64
03:12:22 --> 03:12:27
If a graph is large, then not all
partitions are stored in memory.

65
03:12:27 --> 03:12:32
Typically only N partitions
are kept in memory at all times and

66
03:12:32 --> 03:12:35
the rest of the partitions
are swapped into disk.

67
03:12:36 --> 03:12:41
The second situation occurs when
the number of messages becomes high.

68
03:12:42 --> 03:12:45
Normally, vertices are processed
in the order of their IDs.

69
03:12:46 --> 03:12:49
A large number of messages
is handled by creating

70
03:12:49 --> 03:12:53
temporary message stores which
are sorted by their destination IDs.

71
03:12:55 --> 03:12:58
All messages going to the same
vertex are placed together.

72
03:12:58 --> 03:13:03
To do this, messages are sorted
in memory periodically.

73
03:13:03 --> 03:13:06
The message store files
are accessed based on

74
03:13:06 --> 03:13:08
which vertices are being
processed at this moment.

75
03:13:09 --> 03:13:13
This whole system is managed by
Giraph's out of core message processor.

1
06:21:17 --> 06:21:21
In this lesson, we'll talk about
two dominant systems developed for

2
06:21:21 --> 06:21:23
large scale graph processing.

3
06:21:24 --> 06:21:27
The first one, called Giraph,
is from Apache and

4
06:21:27 --> 06:21:30
implements a BSP model on Hadoop.

5
06:21:31 --> 06:21:36
The second system is called Graphx,
is developed on the Spark platform,

6
06:21:36 --> 06:21:41
which as you know, emphasizes on
interactive in memory computations.

7
06:21:41 --> 06:21:44
While BSP is a popular
graph processing model,

8
06:21:44 --> 06:21:49
the actual implementation of
BSP in an infrastructure needs

9
06:21:49 --> 06:21:52
additional programmability beyond
what we have discussed so far.

10
06:21:53 --> 06:21:58
In Giraph, several additional capabilities
are added to make it more practical.

11
06:21:59 --> 06:22:04
A thorough coverage of the Giraph platform
is beyond the scoop of these lectures.

12
06:22:04 --> 06:22:07
However, we'll touch upon
a few of these capabilities.

13
06:22:08 --> 06:22:14
We'll first consider graph IO,
that is how graphs can come into a system

14
06:22:14 --> 06:22:19
represented inside the system, and
when completed are written out.

15
06:22:19 --> 06:22:24
Next, we'll describe how Giraph
interacts with external data sources.

16
06:22:24 --> 06:22:27
Some of these data sources
use a different data model,

17
06:22:27 --> 06:22:29
other sources include databases.

18
06:22:31 --> 06:22:33
Once a graph is imported,

19
06:22:33 --> 06:22:35
it is important to make sure that
the system runs efficiently.

20
06:22:37 --> 06:22:42
We will look at a method that uses a
special kind of global aggregate operation

21
06:22:42 --> 06:22:45
which saves time by reducing
the amount of messaging

22
06:22:45 --> 06:22:47
to compute aggregate functions
like sum and products.

23
06:22:50 --> 06:22:54
Finally, we'll recognize that
even if Giraph is designed for

24
06:22:54 --> 06:22:57
performing iterative,
in memory computation,

25
06:22:57 --> 06:23:01
there are times where it is absolutely
necessary to store data on disk.

26
06:23:02 --> 06:23:06
We'll briefly touch upon Giraph's
ability to handle out of core graphs and

27
06:23:06 --> 06:23:07
out of core messages.

28
06:23:08 --> 06:23:11
A graph can be written in many ways.

29
06:23:11 --> 06:23:17
For Neo4J, we saw how graphs can be
important to the database from a CSV file.

30
06:23:17 --> 06:23:23
In Giraph, two of the most common input
formats are Adjacency List and Edge List.

31
06:23:25 --> 06:23:29
For an Adjacency List,
each line has the node ID, a node value

32
06:23:29 --> 06:23:33
which is a single number here, and
a list of destination, weight pairs.

33
06:23:35 --> 06:23:39
Thus, in line one,
A has a value of 10 and 2 neighbors B and

34
06:23:39 --> 06:23:42
F with edge weights 2 and 5, respectively.

35
06:23:44 --> 06:23:47
Since G has no outgoing edge,
the adjacency list is empty.

36
06:23:49 --> 06:23:52
The current way of representing
graphs is in terms of triplets.

37
06:23:52 --> 06:23:57
Containing the source and destination
nodes followed by an [INAUDIBLE].

38
06:23:57 --> 06:23:59
Notice the way we have shown it here.

39
06:24:00 --> 06:24:02
And the node values is not represented.

40
06:24:03 --> 06:24:06
Let us simplify the Adjacency List
representation of it.

41
06:24:07 --> 06:24:11
We remove the colons, commas, braces, and

42
06:24:11 --> 06:24:15
parenthesis, and
get a space separated set of lines.

43
06:24:15 --> 06:24:16
One line for each vertex.

44
06:24:17 --> 06:24:22
We further replace the node IDs A,
B, C, etc., with 1, 2, 3, etc.,

45
06:24:22 --> 06:24:25
so that these IDs are integers.

46
06:24:26 --> 06:24:30
So what do we need to specify
to parse this for Giraph?

47
06:24:30 --> 06:24:37
One, the graph is a text subject and
not, let's say, a database subject.

48
06:24:37 --> 06:24:42
Two, it is a vertex based representation,
each line is a vertex.

49
06:24:43 --> 06:24:44
This splitter here is a space.

50
06:24:46 --> 06:24:48
The idea of the node is a first value for
each line.

51
06:24:50 --> 06:24:52
The value is a second token.

52
06:24:53 --> 06:24:57
The next pair of items you find an edge
with the target and the weight,

53
06:24:57 --> 06:24:58
respectively.

54
06:24:58 --> 06:25:02
And lastly, there is a list of these
pairs until the end of the line.

55
06:25:03 --> 06:25:08
Therefore, each line would typically
lead to the creation of both notes and

56
06:25:08 --> 06:25:09
a set of edges.

57
06:25:10 --> 06:25:15
This shows a typical reader formula
decency matrix written in Java.

58
06:25:15 --> 06:25:18
Again, you don't have to know Java
to get the elements of this program.

59
06:25:20 --> 06:25:23
Our reader is clearly customized for
your specific input.

60
06:25:24 --> 06:25:29
Very often the starting point is
a basic reader provided by Giraph.

61
06:25:31 --> 06:25:35
Like the reader that knows how to read
vertices from each line of a text swipe.

62
06:25:36 --> 06:25:41
To customize it, you extend it and
create your own version.

63
06:25:42 --> 06:25:44
Now, you need to define
how to get the ID and

64
06:25:44 --> 06:25:48
value of the vertex by writing
separate message for them.

65
06:25:48 --> 06:25:53
Notice that the ID comes from
the zeroth item of each line after

66
06:25:53 --> 06:25:59
the split by white space, and
the value comes from the next open,

67
06:25:59 --> 06:26:02
the second term, marked by 1 for
the 0 base of the light.

68
06:26:03 --> 06:26:05
The next code element is this block here.

69
06:26:07 --> 06:26:11
This specifies how to create edges
by iterating through every line.

70
06:26:12 --> 06:26:16
To keep the short, we'll remove
the part that gets the edges here.

71
06:26:16 --> 06:26:20
As Giraph as mature it has
included many specialized

72
06:26:20 --> 06:26:23
to interoperate with compatible resources.

73
06:26:24 --> 06:26:27
This diagram is from Giraph where
the show some of these sources.

74
06:26:28 --> 06:26:31
We can group them into
three different categories.

75
06:26:33 --> 06:26:37
Group one interoperates with Hive and
HBase.

76
06:26:37 --> 06:26:41
You possibly remember these
systems from a prior course.

77
06:26:41 --> 06:26:44
These systems are designed to give
a higher level of data access on

78
06:26:44 --> 06:26:45
interface on top of MapReduce.

79
06:26:47 --> 06:26:51
Group two accesses relational
systems like MySQL and Cassandra.

80
06:26:52 --> 06:26:57
But these systems have accessed indirectly
through a software module called Gora.

81
06:26:58 --> 06:27:03
Gora uses a JSON schema to map
the relation schema of the SQL database

82
06:27:03 --> 06:27:05
to a structure that Giraph can read.

83
06:27:05 --> 06:27:09
Group three accesses graph
databases like Neo4J and DEX,

84
06:27:09 --> 06:27:12
which is now called Sparksee.

85
06:27:13 --> 06:27:16
These systems are all taxes indirectly.

86
06:27:16 --> 06:27:18
Using the [INAUDIBLE]
service of Tinkerpop.

87
06:27:18 --> 06:27:21
Which is a graph API layer that can use

88
06:27:21 --> 06:27:24
many different Giraph stores including
[INAUDIBLE] graph and Titan.

89
06:27:26 --> 06:27:28
Consider a relational
table stored in Hive.

90
06:27:29 --> 06:27:32
The table shown here is extracted
from the bio grid data source that

91
06:27:32 --> 06:27:33
we mentioned in module two.

92
06:27:34 --> 06:27:38
Each row of the table represents
a molecule interaction.

93
06:27:38 --> 06:27:42
We can create a network from here just
by considering the first two columns.

94
06:27:43 --> 06:27:47
The first column represents the source
node of an edge colored red.

95
06:27:47 --> 06:27:50
And the second column represents
the target node of the edge colored blue.

96
06:27:51 --> 06:27:54
The label on the edge comes from
the fifth column of the table

97
06:27:54 --> 06:27:56
which is a black bold font.

98
06:27:58 --> 06:28:00
Let's assume that these predict items,

99
06:28:00 --> 06:28:04
these are items that we want to
pick up from the Hive table.

100
06:28:05 --> 06:28:08
The simplest way to get a record from hive

101
06:28:08 --> 06:28:13
to Giraph is to extend the class
called SimpleHiveRowToEdge.

102
06:28:14 --> 06:28:19
For this class, we need to specify
the source node, the target node, and

103
06:28:19 --> 06:28:22
the edge value using three
methods as shown here.

104
06:28:24 --> 06:28:27
My extension is called MyHiveRowToEdge.

105
06:28:28 --> 06:28:32
It shows the implementation of these
methods where we just pick up the first,

106
06:28:33 --> 06:28:37
second, and fifth columns,
as we described before.

107
06:28:39 --> 06:28:43
Now, as mentioned before,
Giraph interacts with Neo4J through

108
06:28:43 --> 06:28:47
the Gremlin API provided by Tinkerpop.

109
06:28:47 --> 06:28:51
One can think of Gremlin as
a traversal API, which means,

110
06:28:51 --> 06:28:56
it allows one to start from some node and
walk the graph step by step.

111
06:28:56 --> 06:29:00
To show this,
consider disease gene graph on the right.

112
06:29:00 --> 06:29:02
Let's call this graph G.

113
06:29:03 --> 06:29:07
So g.V represents all the vertices of G.

114
06:29:07 --> 06:29:12
Therefore g.V has name
MC4R selects the node that

115
06:29:12 --> 06:29:17
has a property called
name whose value is MC4R.

116
06:29:18 --> 06:29:23
Let's add to this path the condition .out,

117
06:29:23 --> 06:29:26
which chooses the out
edges of the MC4R node and

118
06:29:26 --> 06:29:31
then traverses the associatedWith edge
to the orange node called obesity.

119
06:29:32 --> 06:29:34
For this call, returns the vertex only.

120
06:29:35 --> 06:29:42
Now, adding the path to values
means gives us obesity.

121
06:29:42 --> 06:29:45
We can also expand differently
from the obesity node.

122
06:29:46 --> 06:29:50
When we say inV() we refer
to all nodes that have

123
06:29:50 --> 06:29:53
incoming edges to the current node.

124
06:29:53 --> 06:29:55
In this case, there is only one.

125
06:29:55 --> 06:29:56
The LEPR node.

126
06:29:57 --> 06:30:01
To this, we add the traversal out beam and

127
06:30:01 --> 06:30:05
thus we get back the out going edge from
the LEPR node highlighted in together.

128
06:30:06 --> 06:30:09
We can also look at the Giraph Gremlin
near project connection

129
06:30:09 --> 06:30:10
from Tinkerpop's viewpoint.

130
06:30:12 --> 06:30:15
Tinkerpop is trying to create
a standard language for graph reversal,

131
06:30:15 --> 06:30:20
just like Neo4J is trying to create Open
Cypher as a standard query language for

132
06:30:20 --> 06:30:21
graph databases.

133
06:30:22 --> 06:30:27
In trying to create the standard,
Tinkerpop recognizes that the actual

134
06:30:27 --> 06:30:31
storage management for graph databases
should be provided by another vendor.

135
06:30:32 --> 06:30:34
The vendor needs to implement
the Gremlin API for access.

136
06:30:36 --> 06:30:41
Similarly for graphic processing,
including expensive analytic operations

137
06:30:41 --> 06:30:44
should be performed by what
they call a graph computer.

138
06:30:45 --> 06:30:48
This is the role played by
Giraph as well as Spark.

139
06:30:48 --> 06:30:50
Both of which interface with Tinkerpop.

1
12:52:08 --> 12:52:11
Next, we'll count the number of
vertices and edges, define a min and

2
12:52:11 --> 12:52:17
max function for Spark's reduce method,
computer the min and max degrees,

3
12:52:17 --> 12:52:21
and compute the histogram data
of the degree of connectedness.

4
12:52:23 --> 12:52:26
In this hands on,
we will cover the degree distribution of

5
12:52:26 --> 12:52:29
the metros graph from
the first hands on exercise.

6
12:52:29 --> 12:52:34
This will help you practice finding
the degrees of connectedness in a graph.

7
12:52:34 --> 12:52:35
These numbers will be used for

8
12:52:35 --> 12:52:38
plotting the visualizations in
the next hands on exercises.

9
12:52:41 --> 12:52:43
The degree of a vertex is
the number of edges or

10
12:52:43 --> 12:52:46
connections the vertex has to
other vertices in the graph.

11
12:52:47 --> 12:52:51
In directed graphs,
each vertex has an in degree,

12
12:52:51 --> 12:52:54
the number of edges
directed to the vertex.

13
12:52:54 --> 12:52:59
In and out degree, the number of
edges directed away from the vertex.

14
12:52:59 --> 12:53:02
The metros graph is an example
of a directed graph.

15
12:53:04 --> 12:53:08
Each metropolis vertex has one
outgoing edge to a country vertex.

16
12:53:09 --> 12:53:15
Each country vertex has one or more
incoming edges from metropolis vertices.

17
12:53:15 --> 12:53:16
This will be a quiz question.

18
12:53:23 --> 12:53:27
Starting again where we left off from
the previous hands-on exercise, first,

19
12:53:27 --> 12:53:30
ensure your Cloudera VM is started, and

20
12:53:30 --> 12:53:34
that you downloaded the dataset
examples of analytics.

21
12:53:34 --> 12:53:36
The link is in the content for this week.

22
12:53:37 --> 12:53:41
Use the numEdges attribute to print
the number of edges in metrosGraph.

23
12:53:41 --> 12:53:44
As you can see the result is 65,

24
12:53:44 --> 12:53:49
which matches the number of
lines in metro_country.csv.

25
12:53:49 --> 12:53:53
Now use the numVertices
attribute to print the number

26
12:53:53 --> 12:53:55
of vertices in metrosGraph.

27
12:53:55 --> 12:54:00
As you can see the result is 93,
which matches the number of

28
12:54:00 --> 12:54:06
lines in metro.csv65 plus the number
of lines in country.csv, 28.

29
12:54:13 --> 12:54:18
Define the max and the min reduce
operation to compute the highest and

30
12:54:18 --> 12:54:19
lowest degree vertex.

31
12:54:22 --> 12:54:27
Let us find the vertex with
the most outgoing edges or

32
12:54:27 --> 12:54:32
the vertex with the largest
out degree by passing the max

33
12:54:32 --> 12:54:39
function to a reduced operation on
the out degrees of metrosGraph.

34
12:54:40 --> 12:54:45
The result in this case is vertex
ID five with one outgoing edge.

35
12:54:45 --> 12:54:49
The result could have been any metropolis
because every metropolis in this graph

36
12:54:49 --> 12:54:52
has one outgoing edge to its country.

37
12:54:52 --> 12:54:55
Let us find the vertex with
the most incoming edges, or

38
12:54:55 --> 12:54:58
the vertex with the largest inDegree.

39
12:54:58 --> 12:55:01
This is done the same way
as the previous example,

40
12:55:01 --> 12:55:06
except you'll run the reduce operation
on the inDegrees of metrosGraph.

41
12:55:06 --> 12:55:11
The result is VertexId 108
with 14 incoming edges.

42
12:55:12 --> 12:55:18
Apply a filter to the metrosGraph
vertices to find out which vertex is 108.

43
12:55:18 --> 12:55:21
The answer is the United States.

44
12:55:21 --> 12:55:27
This means that the United States has
14 metropolises in the metros.csvfile.

45
12:55:27 --> 12:55:32
We can also compute how many vertices have
one out going edge by applying a filter of

46
12:55:32 --> 12:55:35
one to the outgoing degrees and
counting the results.

47
12:55:36 --> 12:55:43
The result is 65 because there are 65
metropolises with one outgoing degree.

48
12:55:43 --> 12:55:45
None of the countries have
any outgoing degrees.

49
12:55:46 --> 12:55:49
Let us ignore whether or
not the edge is in or out, and

50
12:55:49 --> 12:55:52
just find which vertex has the most edges.

51
12:55:52 --> 12:55:56
Again, we will run the reduce
operation with the max function.

52
12:55:56 --> 12:55:59
But this time we will run it on
metrosGraph's degrees attribute.

53
12:56:00 --> 12:56:03
The result is 108 again
with 14 connections.

54
12:56:03 --> 12:56:08
This means that the United States is
the most connected vertex in metrosGraph.

55
12:56:15 --> 12:56:18
Finally, let us calculate
the histogram data of the degrees for

56
12:56:18 --> 12:56:20
the countries in metrosGraph.

57
12:56:21 --> 12:56:25
First, create a map that
only includes countries.

58
12:56:25 --> 12:56:29
So create a filter to only include
the vertices with the vertex ID that is

59
12:56:29 --> 12:56:31
greater than or equal to 100.

60
12:56:31 --> 12:56:34
Then you will group the map
by the size of the degree and

61
12:56:34 --> 12:56:37
sort the map from lowest
to highest degree.

62
12:56:39 --> 12:56:43
The output shows six pairs in an array.

63
12:56:43 --> 12:56:46
The first number is the number of edges,
and

64
12:56:46 --> 12:56:50
the second number is the number of
vertices that have that number of edges.

65
12:56:50 --> 12:56:55
In other words, the result of
the query shows that there are 18

66
12:56:55 --> 12:57:00
countries with 1 metropolis,
4 countries with 2 metropolises,

67
12:57:00 --> 12:57:05
2 countries with 3 metropolises,
2 countries with 5 metropolises,

68
12:57:05 --> 12:57:11
1 country with 9 metropolises, and
1 country with 14 metropolises.

1
01:49:18 --> 01:49:24
Next, we'll import the GraphX libraries,
import the Vertices, import the Edges,

2
01:49:24 --> 01:49:30
create a Graph, and use Spark's filter
method to return Vertices in the graph.

3
01:49:30 --> 01:49:33
Hi, this is Cristine Kirkpatrick,
division director for

4
01:49:33 --> 01:49:35
Information Technology Systems and

5
01:49:35 --> 01:49:39
Services at
the San Diego Super Computer Center.

6
01:49:39 --> 01:49:41
I'll guide you through
the hands-on exercises.

7
01:49:41 --> 01:49:44
I recommend you follow
along with the video once.

8
01:49:44 --> 01:49:46
Then, either using the video or

9
01:49:46 --> 01:49:50
the hands-on reading, go through the
hands-on exercise on your own computer.

10
01:49:51 --> 01:49:55
This hands-on exercise will show you
how to build a graph using GraphX,

11
01:49:55 --> 01:49:58
Spark's API for graphs, and
graph-parallel computation.

12
01:49:59 --> 01:50:02
We'll start with importing the data and
then build a simple graph.

13
01:50:03 --> 01:50:07
Note that the first four hands on
assignments are meant to be completed

14
01:50:07 --> 01:50:08
sequentially.

15
01:50:08 --> 01:50:12
That is, you will need to have your
Cloudera VM running continuously.

16
01:50:12 --> 01:50:13
If you must shut down your VM and

17
01:50:13 --> 01:50:18
restart, you will need to run the commands
from this first hands on assignment again.

18
01:50:19 --> 01:50:23
Otherwise you may receive error messages
because the data has not been imported.

19
01:50:23 --> 01:50:25
The data references metro areas.

20
01:50:26 --> 01:50:31
In an effort to disambiguate or
distinguish between metropolitan areas and

21
01:50:31 --> 01:50:33
an English word also used
to refer to a train or

22
01:50:33 --> 01:50:37
light rail, when we mean metro
area we will say metropolis.

23
01:50:38 --> 01:50:41
First, ensure your
cloudera vm is started and

24
01:50:41 --> 01:50:44
that you've downloaded the data set,
examples of analytics.

25
01:50:44 --> 01:50:46
The link is in the content for this week.

26
01:50:47 --> 01:50:50
The download process might name
the zip file something very long.

27
01:50:51 --> 01:50:57
Copy the examples of analytics.zip
file to the cloudera's home folder.

28
01:50:57 --> 01:50:59
If the zip file is on the desktop,

29
01:50:59 --> 01:51:02
simply drag it to the Cloudera's
home folder on the desktop.

30
01:51:02 --> 01:51:05
If the file is saved elsewhere,
then you will need to navigate

31
01:51:05 --> 01:51:09
to the folder where it is saved before
dragging it to the Cloudera's home folder.

32
01:51:10 --> 01:51:15
Open the terminal window by clicking the
terminal icon at the top of the screen.

33
01:51:15 --> 01:51:19
Use the unzip command to extract the zip
file to the cloudera home directory.

34
01:51:19 --> 01:51:22
You can copy and
paste the name of the zip file.

35
01:51:22 --> 01:51:25
Let it extract with the default name,
ExamplesOfAnalytics.

36
01:51:26 --> 01:51:29
Go into the ExamplesOfAnalytics
directory and

37
01:51:29 --> 01:51:33
list the contents of
the EOA data directory.

38
01:51:33 --> 01:51:36
You should see five .csv files and
one .txt file.

39
01:51:37 --> 01:51:43
In order to access the CSV and TXT files,
we have to first copy the files to HDFS.

40
01:51:44 --> 01:51:50
Rally HDFS foot command to copy
the EOA data directory to HDFS

41
01:51:50 --> 01:51:53
then less the contents of
the EOA data directory on HDFS.

42
01:51:55 --> 01:52:00
Start the spark shell and include the
graph stream and breeze fizz libraries.

43
01:52:00 --> 01:52:03
Don't worry about copying
the full command from the video.

44
01:52:03 --> 01:52:06
A text version will be included within
the course reading that contains the full

45
01:52:06 --> 01:52:08
command for you to copy and paste.

46
01:52:11 --> 01:52:15
Import the log for j classes and
suppress the notice in info messages.

47
01:52:21 --> 01:52:27
Import sparks graphx and
rdd classes along with scala source class.

48
01:52:27 --> 01:52:32
The data set we are going to use in
this hands on is from three files.

49
01:52:32 --> 01:52:36
The metro.csv file and the country.csv
file contain the vertices for

50
01:52:36 --> 01:52:40
the graph and
metro underscore country.csv.

51
01:52:40 --> 01:52:44
contains the edges that make up the
relationships between the metro areas and

52
01:52:44 --> 01:52:45
the country they belong too.

53
01:52:47 --> 01:52:51
Before importing any of the CSV files, we
are going to list the first five lines of

54
01:52:51 --> 01:52:54
each file to verify that
they are indeed CSV files.

55
01:52:55 --> 01:53:02
Notice that the metric.csv contains an ID,
the metropolis name and the population.

56
01:53:02 --> 01:53:06
The country.csv file contains
only an ID and the country name.

57
01:53:06 --> 01:53:10
The metro_country.csv file
contains the metro ID and

58
01:53:10 --> 01:53:14
the ID of the country that
the metropolis belongs to.

59
01:53:20 --> 01:53:25
Create a class called PlaceNode to store
the information about the vertices.

60
01:53:25 --> 01:53:28
Then extend the PlaceNode class
with case classes specifically for

61
01:53:28 --> 01:53:30
the metro's and the countries vertices.

62
01:53:31 --> 01:53:32
Create attributes for

63
01:53:32 --> 01:53:37
the metro class to store the name and
the population from the CSV file.

64
01:53:37 --> 01:53:40
The country class only needs
an attribute to store the name.

65
01:53:41 --> 01:53:47
To import the metro.csv file, create
a spark resilient distributive data set or

66
01:53:47 --> 01:53:53
rdd named metros made up of
a vertex id in the metro class.

67
01:53:53 --> 01:53:56
An rdd represents an immutable
partition collection

68
01:53:56 --> 01:53:58
of elements that can be
operated on in parallel.

69
01:53:59 --> 01:54:02
When the contents of
the .csv files printed,

70
01:54:02 --> 01:54:05
the line with the column names
started with the symbol #.

71
01:54:05 --> 01:54:09
So create a filter to ignore any
line that starts with a # so

72
01:54:09 --> 01:54:12
that the column names
don't import into the RDD.

73
01:54:13 --> 01:54:17
The .csv files being imported
contain values separated by commas.

74
01:54:17 --> 01:54:21
So you will need to split each line
in the metro.csv file into rows,

75
01:54:21 --> 01:54:24
by using a comma as a delimiter.

76
01:54:24 --> 01:54:26
Map the first row to the vertex ID.

77
01:54:26 --> 01:54:29
The second row is the metro
name attribute, and

78
01:54:29 --> 01:54:31
the third row is the population attribute.

79
01:54:32 --> 01:54:37
You're going to import the country's .csv
feed file the same way as the metro.csv

80
01:54:37 --> 01:54:38
feed file.

81
01:54:38 --> 01:54:43
However this time since the ids in both
the metro.csv file and the country.csv

82
01:54:43 --> 01:54:48
file start with one you will add 100
to the vertex id of the countries.

83
01:54:48 --> 01:54:52
So that the vertex ids are unique
between both data sets.

84
01:54:52 --> 01:54:55
If the ids are not unique
that will prevent us from

85
01:54:55 --> 01:54:57
creating an accurate graph.

86
01:54:57 --> 01:54:58
This will be on the quiz.

87
01:55:04 --> 01:55:08
Import the edges into
an RDD named mclinks.

88
01:55:08 --> 01:55:11
This is done the same way as
the previous two examples.

89
01:55:11 --> 01:55:14
Remember to add 100 to
the countries vertex ID.

90
01:55:20 --> 01:55:25
Concatenate the metros and countries
vertices into a single variable and

91
01:55:25 --> 01:55:29
use GraphX's graph function to
create a graph of the metros and

92
01:55:29 --> 01:55:32
countries vertices with
the MC links edges.

93
01:55:33 --> 01:55:35
Let us take a look at
what is in the graph.

94
01:55:35 --> 01:55:38
Use the vertices and edges attributes
to print five vertices and

95
01:55:38 --> 01:55:40
five edges from the metros graph.

96
01:55:46 --> 01:55:49
Query the graph to find
how the metropolises and

97
01:55:49 --> 01:55:51
the countries are related.

98
01:55:51 --> 01:55:53
Let us find which country Tokyo is in.

99
01:55:53 --> 01:55:56
Tokyo has a vertex ID of 1.

100
01:55:56 --> 01:56:00
Use RDD's filter method to filter
all of the edges in metro's graph

101
01:56:00 --> 01:56:05
that have a source for text ID of one and
create a map of destination vertex ID's.

102
01:56:06 --> 01:56:09
The result is 101 which is Japan.

103
01:56:09 --> 01:56:14
You can verify this by looking at
the metro.csv and country.csv files.

104
01:56:14 --> 01:56:19
Remember, we added 100 to
the IDs in country.csv.

105
01:56:19 --> 01:56:20
Now, let us do the opposite and

106
01:56:20 --> 01:56:25
find all of the metropolises that are in
China which has a vertex ID of 103.

107
01:56:25 --> 01:56:29
This time we're going to filter
all of the edges in metro's graph

108
01:56:29 --> 01:56:35
that have a vertex ID of 103 and
create a map of all of the source IDs.

109
01:56:35 --> 01:56:39
The result is 3,4, 7, 24, and 34.

110
01:56:39 --> 01:56:46
You can look in metrostats.csv and verify
that those metropolises are in China.

1
03:46:04 --> 03:46:09
Next, we'll create a new dataset,
join two datasets with JoinVertices,

2
03:46:09 --> 03:46:14
join two datasets with outerJoinVertices,
and create a new return type for

3
03:46:14 --> 03:46:15
the joined vertices.

4
03:46:15 --> 03:46:17
In this hands-on exercise,

5
03:46:17 --> 03:46:21
we will create a new graph made
up of five airline flights.

6
03:46:21 --> 03:46:23
The vertices will represent the airports,

7
03:46:23 --> 03:46:27
and the edges will represent
the departures and the arrivals.

8
03:46:27 --> 03:46:31
Once the graph has been created
we will add a second dataset

9
03:46:31 --> 03:46:33
with additional airport information and

10
03:46:33 --> 03:46:39
practice using GraphX's join methods to
create new graphs by joining the datasets.

11
03:46:39 --> 03:46:42
We will start a new spark shell session,
so

12
03:46:42 --> 03:46:44
we will be opening a new terminal window.

13
03:46:45 --> 03:46:49
Open the new terminal window by
clicking the terminal icon at the top

14
03:46:49 --> 03:46:49
of the screen.

15
03:46:51 --> 03:46:52
Start the spark shell.

16
03:47:02 --> 03:47:06
Import the log for j classes and
suppress the notice and info messages.

17
03:47:08 --> 03:47:11
Import sparks graphics and rdd classes.

18
03:47:11 --> 03:47:15
Now we will create the vertices
ourselves from a list.

19
03:47:15 --> 03:47:19
The list will contain the vertex ID and
the name of the airport.

20
03:47:19 --> 03:47:21
Now we will create the edges.

21
03:47:21 --> 03:47:24
The edges will represent a flight
departing from one airport and

22
03:47:24 --> 03:47:26
arriving at another.

23
03:47:26 --> 03:47:29
We will set the edge property
to a fake flight number.

24
03:47:29 --> 03:47:32
Create the flights graph
by joining the vertices and

25
03:47:32 --> 03:47:34
the edges using graph
flexes graph functions.

26
03:47:35 --> 03:47:38
Let us explore the graph so far.

27
03:47:38 --> 03:47:42
Look through each triplet in flights graph
and print the contents of the graph.

28
03:47:42 --> 03:47:45
We will specify which airport
the flight departs from,

29
03:47:45 --> 03:47:48
which airport the flight arrives at,
and the flight number.

30
03:47:48 --> 03:47:51
You'll just put all of
the vertices in the graph.

31
03:47:51 --> 03:47:55
Now we are willing to create an additional
dataset to store additional information

32
03:47:55 --> 03:47:57
about each airport.

33
03:47:57 --> 03:48:02
First, we will create a case class called
airport information with the city and

34
03:48:02 --> 03:48:04
airport code properties.

35
03:48:04 --> 03:48:08
Then we are going to create the airport
information vertices from a list

36
03:48:08 --> 03:48:09
as we did earlier.

37
03:48:09 --> 03:48:11
We will make sure the vertex ID for

38
03:48:11 --> 03:48:17
the airport information matches the vertex
ID of the airport defined earlier.

39
03:48:17 --> 03:48:21
Note, there does not have to be a one
to one relationship between the airport

40
03:48:21 --> 03:48:24
vertices and
the airport information vertices.

41
03:48:24 --> 03:48:29
Los Angeles International Airport is
present in the airport vertices, but

42
03:48:29 --> 03:48:32
it is missing from the airport
information vertices.

43
03:48:32 --> 03:48:37
The airport information vertices contains
information about airports in London and

44
03:48:37 --> 03:48:38
Hong Kong.

45
03:48:38 --> 03:48:41
But we don't have any flights departing or
arriving from those airports.

46
03:48:47 --> 03:48:50
Let us complete our first join.

47
03:48:50 --> 03:48:55
A mapping function has to be defined in
order to join vertices with graphics join

48
03:48:55 --> 03:48:56
vertices method.

49
03:48:56 --> 03:49:00
We are going to define a map function that
will join the name of the airport from

50
03:49:00 --> 03:49:05
the airport vertices with the city name
from the airport information vertices.

51
03:49:05 --> 03:49:09
In order to create a new graph with
the vertices that has the airport name

52
03:49:09 --> 03:49:11
colon city name.

53
03:49:11 --> 03:49:12
After the map function in defined,

54
03:49:12 --> 03:49:16
then we will use the joined vertices
method to create the new graph.

55
03:49:16 --> 03:49:21
Vertices without a matching value in
the RDD, retain their original value.

56
03:49:21 --> 03:49:24
Now, print out the vertices
of the new graph.

57
03:49:24 --> 03:49:27
Notice all of the vertices contain
the airport and city names,

58
03:49:27 --> 03:49:32
except Los Angeles International Airport,
which retained its original value.

59
03:49:37 --> 03:49:41
Now we will join the vertices using
GraphX's outerJoinVertices method.

60
03:49:42 --> 03:49:46
OuterJoinVertices is
similar to JoinVertices,

61
03:49:46 --> 03:49:50
except that the user-defined map
function is applied to all vertices and

62
03:49:50 --> 03:49:51
it can change the vertex property type.

63
03:49:53 --> 03:49:57
Here, we are using the OuterJoinVertices
method to create a new graph where

64
03:49:57 --> 03:50:01
the vertices contain a property
with the name of the airport and

65
03:50:01 --> 03:50:04
a property makeup of
the airport information class.

66
03:50:04 --> 03:50:07
Print the vertices once
the new graph is made.

67
03:50:08 --> 03:50:12
Notice that the type of the second
vertex property is sum.

68
03:50:12 --> 03:50:15
This is because
Los Angeles International Airport

69
03:50:15 --> 03:50:18
does not have a corresponding
vertex in airport information.

70
03:50:18 --> 03:50:22
Therefore GraphX assigned all of
the properties the sum type so

71
03:50:22 --> 03:50:25
that all of the vertex
property types are the same.

72
03:50:26 --> 03:50:28
We can use the get or

73
03:50:28 --> 03:50:32
else method to assign a default
value if one does not exist.

74
03:50:32 --> 03:50:35
Now, let us rerun
the outer join vertices but

75
03:50:35 --> 03:50:39
this time use the get, get or
else method to create an airport

76
03:50:39 --> 03:50:45
information class with NA as the values
for the city and the airport code.

77
03:50:45 --> 03:50:48
Print the vertices once
the new graph is made.

78
03:50:49 --> 03:50:54
Now, the type of the second vertex
property is airport information in

79
03:50:54 --> 03:50:58
Los Angeles International Airport,
airport information class,

80
03:50:58 --> 03:51:01
has the properties NA and NA.

81
03:51:06 --> 03:51:11
Finally, in this last exercise, we will
create a new case class called Airport

82
03:51:11 --> 03:51:16
that contains property for the name,
city, and code of the airport.

83
03:51:16 --> 03:51:20
First, we will define the new Airport
case class with the name, city, and

84
03:51:20 --> 03:51:22
code properties.

85
03:51:22 --> 03:51:25
Then use the outer join vertices
method to join the data sets, and

86
03:51:25 --> 03:51:26
create the new graph.

87
03:51:27 --> 03:51:32
Create a mapping so if the airport has an
airport information vertex, the city and

88
03:51:32 --> 03:51:36
code are taken from the instance
of the airport information class.

89
03:51:36 --> 03:51:38
Otherwise, only include the airport name.

90
03:51:39 --> 03:51:43
Finally, print the vertices
of the new graph.

91
03:51:43 --> 03:51:48
The new graph has only one vertex property
which is an instance of the airport class.

92
03:51:48 --> 03:51:52
GraphX has lots of methods for
joining graph datasets.

93
03:51:52 --> 03:51:56
This hands on example only scratched
the surface of GraphX's power.

1
07:38:01 --> 07:38:05
Next, we'll create a new graph by
adding the continents dataset,

2
07:38:05 --> 07:38:07
import the GraphStream library,

3
07:38:07 --> 07:38:11
import the countriesGraph into
a GraphStream SingleGraph,

4
07:38:11 --> 07:38:14
visualize the countriesGraph,
and visualize the Facebook graph.

5
07:38:15 --> 07:38:19
In this hands-on exercise we
will use the GraphStream library

6
07:38:19 --> 07:38:21
to visualize countries graph.

7
07:38:21 --> 07:38:25
The metros graph that we have been
working with in the previous exercises.

8
07:38:25 --> 07:38:27
This time with the continent vertices and
edges added.

9
07:38:30 --> 07:38:33
Starting again where we left off
from the previous hands-on exercise,

10
07:38:33 --> 07:38:36
ensure your [INAUDIBLE] has started and
that you've imported the data.

11
07:38:43 --> 07:38:46
First, in order to make
the graph more interesting,

12
07:38:46 --> 07:38:50
we are going to import additional
vertices from the continent.csv, and

13
07:38:50 --> 07:38:54
additional edges from
country_continent.csv to create

14
07:38:54 --> 07:38:58
a relationship between the country and
the continent it belongs to.

15
07:38:59 --> 07:39:02
We will show the steps for
importing the datasets, but

16
07:39:02 --> 07:39:04
we do not cover the steps in detail.

17
07:39:04 --> 07:39:07
If you aren't sure how to
import a dataset, go back and

18
07:39:07 --> 07:39:09
view the video called
hands-on building a graph.

19
07:39:11 --> 07:39:16
Now, we are going to concatenate the
metros, countries, and continents vertices

20
07:39:16 --> 07:39:20
into a single variable and concatenate
the metros to countries edges, and

21
07:39:20 --> 07:39:25
countries to continent edges
into another single variable.

22
07:39:25 --> 07:39:28
Finally, you will create a new
graph called countriesGraph.

23
07:39:36 --> 07:39:39
Now, import the GraphStream library.

24
07:39:39 --> 07:39:43
We will not go over in detail how
to use the GraphStream library.

25
07:39:43 --> 07:39:45
You can review its documentation and

26
07:39:45 --> 07:39:48
other online resources on your
own if you are interested.

27
07:39:54 --> 07:39:58
First, create a new instance of
GraphStreams SingleGraph class,

28
07:39:58 --> 07:40:00
using the countriesGraph.

29
07:40:08 --> 07:40:12
Next, we will set some attributes for
the graph we are going to visualize,

30
07:40:12 --> 07:40:14
including setting a style for the graph.

31
07:40:14 --> 07:40:17
GraphStream uses cascading style sheets or

32
07:40:17 --> 07:40:22
CSS just like ordinary web pages,
to control the appearance of the graph.

33
07:40:22 --> 07:40:27
The zip file that you downloaded from
Coursera contain the CSS file that we will

34
07:40:27 --> 07:40:28
use in this hands-on exercise.

35
07:40:30 --> 07:40:34
Now, load the countries graph
vertices into the visualization

36
07:40:34 --> 07:40:37
using GraphStreams add node method.

37
07:40:37 --> 07:40:41
Notice that we are setting
the style UI.class of each vertex

38
07:40:41 --> 07:40:46
depending on if the vertex is an instance
of the metro, country or continent class.

39
07:40:48 --> 07:40:53
Add the edges of country's graph to
the visualization using GraphStreams

40
07:40:53 --> 07:40:54
add edge method.

41
07:40:55 --> 07:40:59
Finally, call the display
method to visualize the graph.

42
07:40:59 --> 07:41:01
The graph will look similar to this.

43
07:41:02 --> 07:41:06
The small blue dots are the metropolises,
the medium sized red dots

44
07:41:06 --> 07:41:09
are the countries, and
the large green dots are the continents.

45
07:41:10 --> 07:41:13
By looking at the clusters
you can easily identify which

46
07:41:13 --> 07:41:16
continent in the visualization
is Antarctica.

47
07:41:16 --> 07:41:19
It is the green dot that
has no connections or

48
07:41:19 --> 07:41:22
it is the least connected
cluster in the network.

49
07:41:22 --> 07:41:24
I hope your paying attention,
because this will be on the quiz.

50
07:41:26 --> 07:41:30
The large cluster of dots at the bottom is
Asia because it has the most countries and

51
07:41:30 --> 07:41:32
metropolises in the graph.

52
07:41:32 --> 07:41:36
The Asia cluster is the most
connected cluster in the network.

53
07:41:36 --> 07:41:38
Can you identify which clusters
are the other continents

54
07:41:38 --> 07:41:40
by looking at the visualization?

55
07:41:46 --> 07:41:50
Let's look at the visualization
of a much larger dataset,

56
07:41:50 --> 07:41:53
the Facebook dataset with 90,000 vertices.

57
07:41:53 --> 07:41:57
You've seen these techniques
before with the metro dataset, but

58
07:41:57 --> 07:42:00
that graph had only around 90 vertices.

59
07:42:00 --> 07:42:04
Because we want you to learn the concepts,
not be bogged down with repetitive tasks,

60
07:42:04 --> 07:42:09
we've supplied this Cloudera file in order
to create the visualization of the graph.

61
07:42:09 --> 07:42:12
All you need to do is open
a new terminal window.

62
07:42:13 --> 07:42:16
Go into the examples of
analytic directory and

63
07:42:16 --> 07:42:19
run the spark shell with
the Facebook.Cloudera file.

64
07:42:19 --> 07:42:22
Don't worry about copying
the full command from the video.

65
07:42:22 --> 07:42:25
A text version will be included
within the course reading

66
07:42:25 --> 07:42:28
that contains the full command for
you to copy and paste.

67
07:42:33 --> 07:42:35
What do you see?

68
07:42:35 --> 07:42:37
My first reaction is that
it looks like broccoli.

69
07:42:37 --> 07:42:41
A green vegetable reviled by many
American children, unjustly.

70
07:42:41 --> 07:42:44
Social networks are made up of
several clusters of communities or

71
07:42:44 --> 07:42:47
pockets of people who interact densely

72
07:42:47 --> 07:42:51
that are brought together by people who
are members of multiple communities.

73
07:42:51 --> 07:42:54
This interlinking of clusters
gives the social network

74
07:42:54 --> 07:42:57
a broccoli like shape when visualized.

75
07:42:57 --> 07:43:00
I hope you heard the last thing I said
because that will be on the quiz.

1
15:21:00 --> 15:21:04
Next we'll import the BreezeViz library,
define a function to calculate the degree

2
15:21:04 --> 15:21:09
histogram, calculate the probability
distribution for the degree histogram, and

3
15:21:09 --> 15:21:11
graph the results.

4
15:21:11 --> 15:21:14
In this hands on exercise,
we will plot the degree histogram created

5
15:21:14 --> 15:21:18
in the previous hands on exercise
using the BreezeViz library.

6
15:21:24 --> 15:21:28
Starting again where we left off
in the previous hands-on exercise,

7
15:21:28 --> 15:21:30
ensure your is started.

8
15:21:30 --> 15:21:33
First we will import
the BreezeViz library.

9
15:21:40 --> 15:21:44
Next, we will define a function to
calculate the degree histogram of Metro's

10
15:21:44 --> 15:21:47
graph, so we can plot it with BreezeViz.

11
15:21:47 --> 15:21:51
The definition of the degree histogram
function is nearly identical

12
15:21:51 --> 15:21:54
to the code from the previous
exercise that helped us create.

13
15:21:54 --> 15:21:56
The histogram data array.

14
15:22:02 --> 15:22:06
Now we will calculate the probability
distribution of the vertex degrees over

15
15:22:06 --> 15:22:11
the whole graph by normalizing the vertex
degree by the total number of vertices so

16
15:22:11 --> 15:22:14
that the degree probabilities add up to 1.

17
15:22:14 --> 15:22:20
The output of the first command is 28,
the number of countries in country.csv.

18
15:22:20 --> 15:22:25
The output of the second command
is an array of numbered pairs.

19
15:22:25 --> 15:22:27
The first number is the vertex degree,

20
15:22:27 --> 15:22:31
and the second number is
the probability distribution.

21
15:22:31 --> 15:22:34
Notice that the sum of all
the probability distributions equals one.

22
15:22:40 --> 15:22:44
Now we will plot two graphs to
visualize the degree histogram.

23
15:22:44 --> 15:22:48
The first graph will be a line
graph of the degree histogram and

24
15:22:48 --> 15:22:51
the second will be
the degree histogram itself.

25
15:22:51 --> 15:22:56
We will not go over in detail how to use
the BreezeViz library to create graphs.

26
15:22:56 --> 15:23:00
The project is well documented and there
are many resources online if you want to

27
15:23:00 --> 15:23:03
explore the BreezeViz library on your own.

28
15:23:04 --> 15:23:08
For the first graph we will define
the X axis as the vertex degree and

29
15:23:08 --> 15:23:11
the Y axis as the degree probability.

30
15:23:11 --> 15:23:14
For the second graph we will
just pass the metro graph

31
15:23:14 --> 15:23:17
degree attribute to
BreezeViz's histogram function

32
15:23:37 --> 15:23:41
You should see two graphical
representations of the data.

33
15:23:41 --> 15:23:43
The bottom one is the histogram.

34
15:23:43 --> 15:23:46
The top shows the degrees
distribution plotting the histogram.

35
15:23:46 --> 15:23:51
As we saw in the previous exercise's
array most countries have one metropolis.

1
06:44:51 --> 06:44:54
Welcome back to the fourth and
final module of the course.

2
06:44:55 --> 06:45:00
In this module, we'll cover the underlying
principles of large scale graph processing

3
06:45:00 --> 06:45:02
and the software infrastructure
that supports it.

4
06:45:04 --> 06:45:07
So, in this module,
you'll learn a programming model for

5
06:45:07 --> 06:45:10
graph computation and
systems that implement this model.

6
06:45:12 --> 06:45:16
After this model, you'll be able to
formulate graph analytics computations

7
06:45:16 --> 06:45:17
in terms of the programming model.

8
06:45:18 --> 06:45:21
This module will consist of three lessons.

9
06:45:22 --> 06:45:26
The first lesson, will revisit
the concept of the programming model and

10
06:45:26 --> 06:45:30
introduce a programming model
called Bulk Synchronous Parallel or

11
06:45:30 --> 06:45:35
BSP that is designed specifically for
graph oriented computation.

12
06:45:36 --> 06:45:41
I will discuss two well known versions of
the central idea, Pregel from Google, and

13
06:45:41 --> 06:45:43
Graphlab from Carnegie Mellon University.

14
06:45:44 --> 06:45:48
The second lesson,
we'll discuss two software systems,

15
06:45:48 --> 06:45:52
Giraph that operates on Hadoop,
and GraphX that operates on Spark.

16
06:45:53 --> 06:45:56
We'll compare the basic architecture
of these two systems and

17
06:45:56 --> 06:45:58
point out their differences.

18
06:45:59 --> 06:46:04
The final lesson will show example
computations, using graphics platform.

19
06:46:04 --> 06:46:06
Since graphics come with Spark,

20
06:46:06 --> 06:46:10
we've created these examples on the
virtual machine, which you have access to.

21
06:46:10 --> 06:46:12
The code and
some data sets are available for

22
06:46:12 --> 06:46:13
those who wish to play with the system.

1
13:31:04 --> 13:31:06
Now we are going to
introduce you to Graphx,

2
13:31:08 --> 13:31:10
which we'll primarily cover
as a hands-on presentation.

3
13:31:12 --> 13:31:16
Graphx was developed by the Amp
lab at UC Berkeley, and

4
13:31:16 --> 13:31:18
has now become an Apache product.

5
13:31:19 --> 13:31:23
In this brief introduction, we are using
a few slides from Amp lab presentations.

6
13:31:26 --> 13:31:30
Like [INAUDIBLE] graphic
uses a property graph model.

7
13:31:31 --> 13:31:35
That means both nodes and
edges can have attributes and values.

8
13:31:37 --> 13:31:41
In Graphx, the node properties
are stored in a Vertex Table.

9
13:31:41 --> 13:31:43
And edge properties
are stored in an Edge Table.

10
13:31:45 --> 13:31:49
The connectivity information, that is
which edge connects to which nodes,

11
13:31:49 --> 13:31:53
is stored separately from the nodes and
edge properties.

12
13:31:53 --> 13:31:55
Since Graphx is based on spark,

13
13:31:55 --> 13:32:01
whose central information representation
is based on resilient data sets or RDDs.

14
13:32:02 --> 13:32:06
You may recall that RDDs are typically
in memory information objects.

15
13:32:07 --> 13:32:11
These objects can be used to perform
an action, which returns a value.

16
13:32:12 --> 13:32:16
Or they can perform a transformation,
which can produce another RDD.

17
13:32:17 --> 13:32:21
Graphx is built on special RDDs for
vertices and edges.

18
13:32:22 --> 13:32:26
Note that VertexIDs are defined
to be unique by design.

19
13:32:26 --> 13:32:32
VertexRDD A represents a set of vertices
all of which have an attribute called A.

20
13:32:34 --> 13:32:39
The Edge class is an object with a source
vertex, a destination vertex, and

21
13:32:39 --> 13:32:39
edge attribute.

22
13:32:41 --> 13:32:46
The EdgeRDD extends this
basic edge by storing

23
13:32:46 --> 13:32:51
edges in a columnar format on
each partition of performance.

24
13:32:51 --> 13:32:56
Very often, it's easier to operate
on a data structure that has both

25
13:32:56 --> 13:32:58
node properties and edge properties.

26
13:32:59 --> 13:33:03
If you are familiar with this schema, you
will readily see that this is a three-way

27
13:33:03 --> 13:33:07
join operation between the node set,
the edge set and the second node set.

28
13:33:09 --> 13:33:12
Finally, GraphX implements
it's own version of BSP.

29
13:33:13 --> 13:33:16
This implementation,
not surprisingly, is called Pregel.

30
13:33:17 --> 13:33:23
It allows a user to write a vertex
program, a send message routine,

31
13:33:23 --> 13:33:25
and a message combiner routine,
just like BSP.

32
13:33:27 --> 13:33:30
However, this implementation also performs

33
13:33:30 --> 13:33:32
vertex partitioning like
we saw in graph lab.

34
13:33:34 --> 13:33:36
Also, similarly to graph lab,

35
13:33:36 --> 13:33:40
it enables the message sending computation
to reach the attributes of both vertices.

36
13:33:41 --> 13:33:44
The code limit shown here
is written in Scala.

37
13:33:46 --> 13:33:50
The hands on code after this you'll
see are all certain in scatter.

38
13:33:50 --> 13:33:52
Will have a notice here in no scatter.

39
13:33:52 --> 13:33:57
We would like to show you
here an example which uses

40
13:33:57 --> 13:34:02
the Pregel runtime object in GraphX
to implement the simple BSP desk.

41
13:34:03 --> 13:34:08
We would like to show here how Pregel
runtime objects in Graphx works.

42
13:34:09 --> 13:34:12
We point out all the functions
that are defined by users

43
13:34:12 --> 13:34:14
in running the Pregel operation.

44
13:34:15 --> 13:34:20
The vprogf function,
which takes a vertex and a message and

45
13:34:20 --> 13:34:24
returns a new attribute value for
that vertex.

46
13:34:24 --> 13:34:29
The sendMsgr function, which computes
the new message along each edge.

47
13:34:30 --> 13:34:35
The Option[M] construct says that it is
optional for a vertex to send the message.

48
13:34:35 --> 13:34:38
That is,
the system should not show an error

49
13:34:38 --> 13:34:41
if your message is not sent for
the vertex.

50
13:34:41 --> 13:34:44
The combinef function is
a message combiner that we

51
13:34:44 --> 13:34:46
mention in passive for graph.

52
13:34:46 --> 13:34:50
Finally, the user can also specify
the number of reiterations that

53
13:34:50 --> 13:34:53
the GraphX BSP process will run for.

1
03:05:58 --> 03:05:58
In 2010, Google

2
03:06:00 --> 03:06:03
published a paper outlining a system
that they had been working on.

3
03:06:06 --> 03:06:10
This publication about a system called
Pregel, has been one of the most

4
03:06:10 --> 03:06:15
influential publications on
large scale graph computing.

5
03:06:15 --> 03:06:19
The Pregel system essentially implemented
the BSB model that we covered in

6
03:06:19 --> 03:06:19
the last lecture.

7
03:06:21 --> 03:06:24
To show how the Pregel
system is programmed,

8
03:06:24 --> 03:06:29
we present the published code of Google's
most famous algorithm, the PageRank.

9
03:06:31 --> 03:06:35
Recall that PageRank's task is
to compute note centrality.

10
03:06:36 --> 03:06:40
The basic philosophy of PageRank is
that a note that is connected to an more

11
03:06:40 --> 03:06:42
important note gains more importance.

12
03:06:43 --> 03:06:48
In the filler on the right, B is a very
important node with a high page rank

13
03:06:48 --> 03:06:52
because a lot of other nodes directly or
indirectly point to it.

14
03:06:52 --> 03:06:57
So C being his direct neighbor
that receives an edge from B,

15
03:06:57 --> 03:06:59
also has a high page rank.

16
03:06:59 --> 03:07:02
All the C itself,
there's not have too many incident edges.

17
03:07:03 --> 03:07:07
On the left side we have
Google's published C++ code

18
03:07:07 --> 03:07:10
that implements PageRank method
on the Pregel infrastructure.

19
03:07:11 --> 03:07:14
You don't have to know C++ to understand
the basic essence of the code.

20
03:07:15 --> 03:07:19
We will explain the basic elements of
the Vertex program in the next few slides.

21
03:07:21 --> 03:07:24
Remember from the last lecture
that the VSP technique considers

22
03:07:24 --> 03:07:25
a vertex to be a process.

23
03:07:27 --> 03:07:31
Every vertex implements
a method called compute.

24
03:07:33 --> 03:07:37
This method, implements the logic of what
a vertex should do during the super steps.

25
03:07:39 --> 03:07:44
The program starts by creating a special
kind of vertex called the PageRank vertex

26
03:07:44 --> 03:07:46
for which this compute
method is specified.

27
03:07:48 --> 03:07:52
You will notice that the compute
method starts by saying what happens

28
03:07:52 --> 03:07:54
when the super step is one or more.

29
03:07:56 --> 03:07:57
But what happens in super step zero?

30
03:07:58 --> 03:08:02
Usually, superstep 0 is used for
initialization.

31
03:08:02 --> 03:08:07
Every vertex, initialization
PageRank value to a same number

32
03:08:07 --> 03:08:11
which is one divided by the total
number of vertices in the graph.

33
03:08:13 --> 03:08:18
Computationally, a PageRank of a vertex is
a number calculated by adding two terms.

34
03:08:20 --> 03:08:24
The first term, depends on the total
number of vertices in the graph, and

35
03:08:24 --> 03:08:26
is therefore the same every time.

36
03:08:26 --> 03:08:31
And the second term depends on the page
rank of the neighbors of the vertex.

37
03:08:33 --> 03:08:35
How does the vertex
compute the second term?

38
03:08:36 --> 03:08:39
It gets the PageRank values of
the neighbors in its messages,

39
03:08:39 --> 03:08:40
and adds them up.

40
03:08:41 --> 03:08:45
As we saw for SSSP,
after a vertex computes its value,

41
03:08:45 --> 03:08:51
it goes to the propagate step and
sends a message to its outgoing edges.

42
03:08:51 --> 03:08:54
For PageRank, the message it sends out

43
03:08:54 --> 03:08:57
is just computed value divided
by the number of outgoing edges.

44
03:08:58 --> 03:09:03
When this is done, the node halts for
the next superstep, and waits for

45
03:09:03 --> 03:09:05
some other node to wake it up.

46
03:09:06 --> 03:09:11
At this point, we have seen two
examples of graph analytic operations

47
03:09:11 --> 03:09:15
executed on the BSB programming model.

48
03:09:15 --> 03:09:19
Now, we'll look at the same problem
from a slightly different viewpoint.

49
03:09:20 --> 03:09:25
GraphLab, originally a project from
Carnegie Mellon University, now turned

50
03:09:25 --> 03:09:29
into a company called Dato, took a similar
yet different approach to the problem.

51
03:09:30 --> 03:09:34
In GraphLab, any kind of data can be
associated with a vertex or an edge.

52
03:09:35 --> 03:09:38
This information is stored in
what is called a data graph.

53
03:09:39 --> 03:09:42
Let's look at the same
page like I already did.

54
03:09:42 --> 03:09:45
The syntax is a bit different but
the logical blocks are identical.

55
03:09:47 --> 03:09:49
These are the same blocks
that we highlighted before.

56
03:09:50 --> 03:09:55
GraphLab breaks up these blocks into
three different user specified functions

57
03:09:56 --> 03:10:00
called Gather, Apply,
Scatter, or GAS, for short.

58
03:10:02 --> 03:10:04
Okay, so what's different?

59
03:10:05 --> 03:10:08
Let's mention a few important differences.

60
03:10:08 --> 03:10:10
First, let's consider gather.

61
03:10:11 --> 03:10:16
Rather than adopting a message passing or
data flow model like Pregel,

62
03:10:16 --> 03:10:21
GraphLab allows the user defined update
function complete freedom to read and

63
03:10:21 --> 03:10:25
modify any of the data on
adjacent vertices edges.

64
03:10:26 --> 03:10:32
In GraphLab, a receiving vertex has
access to the data on adjacent vertices

65
03:10:32 --> 03:10:37
even if the adjacent vertices did
not schedule the current update.

66
03:10:38 --> 03:10:43
In contrast, for Pregel the control
is with descending nodes.

67
03:10:43 --> 03:10:46
An update can happen only when
a node sends out messages.

68
03:10:47 --> 03:10:51
For some graph analytics operations
like a dynamic version of PageRank,

69
03:10:51 --> 03:10:52
this is very important.

70
03:10:53 --> 03:10:59
Further, if vertex can update
its value asynchronously.

71
03:10:59 --> 03:11:03
That is, as soon as it receives
an update without having to wait for

72
03:11:03 --> 03:11:05
all nodes like we do in VSP.

73
03:11:06 --> 03:11:11
This often helps some intuitive algorithms
like page rank converge faster.

74
03:11:12 --> 03:11:15
When the graph is large and
must be split across machines,

75
03:11:16 --> 03:11:20
BSP cuts the graph along edges,
as we see on this slide.

76
03:11:21 --> 03:11:25
So every cut edge results in
a machine to machine communication.

77
03:11:26 --> 03:11:29
If we increase the number
of across machine edges,

78
03:11:29 --> 03:11:32
we increase communication cost.

79
03:11:32 --> 03:11:34
This has an interesting
practical consequence.

80
03:11:36 --> 03:11:40
As we have mentioned,
many graph applications have communities.

81
03:11:40 --> 03:11:42
And central nodes that have high degree.

82
03:11:43 --> 03:11:46
Many young people today have
over 500 Facebook friends.

83
03:11:47 --> 03:11:50
So when an analytical
operation like Page Rank,

84
03:11:50 --> 03:11:53
that goes through multiple
iterations until convergence, for

85
03:11:53 --> 03:11:57
these graphs, the communication
cost can become very high.

86
03:11:58 --> 03:12:02
In the cluster graph shown here, every
color represents a different machine.

87
03:12:03 --> 03:12:05
Let's look at that red vertex.

88
03:12:05 --> 03:12:08
What would happen if we
split it like this instead?

89
03:12:10 --> 03:12:13
The red node gets split.

90
03:12:13 --> 03:12:18
And different vertices of different
machines work with their own copy

91
03:12:18 --> 03:12:18
of the red vertex.

92
03:12:20 --> 03:12:24
In the diagram,
the primary red vertex is marked zero and

93
03:12:24 --> 03:12:26
copies are marked one through five.

94
03:12:27 --> 03:12:30
Now the gather phase happens for
each copy.

95
03:12:33 --> 03:12:36
Followed by a second operation from
the copies to the primary red vertex.

96
03:12:38 --> 03:12:42
And this is followed by new user
defined operation called merge

97
03:12:42 --> 03:12:45
that combines the partial results from
the copies to the primary vertex.

98
03:12:45 --> 03:12:50
So for PageRank, the merge operation
boils down to computing the total

99
03:12:50 --> 03:12:54
summation of the partial summation
of the edges computed at the copies.

100
03:12:54 --> 03:12:59
Thus in this lesson,
we have seen two of the most influential

101
03:12:59 --> 03:13:03
paradigms of large scale graph
computation when the number of nodes and

102
03:13:03 --> 03:13:06
edges run into tens of millions and more.

