
1
00:00:00.870 --> 00:00:04.540
In this short video we’ll
talk about how Meltwater

2
00:00:04.540 --> 00:00:09.240
helped Danone using sentiment analysis.

3
00:00:09.240 --> 00:00:12.600
Meltwater is a company
that helps other companies

4
00:00:12.600 --> 00:00:17.100
analyze what people are saying about
them and manage their online reputation.

5
00:00:18.700 --> 00:00:22.980
One of the case studies on their
website is about Danone baby nutrition.

6
00:00:24.180 --> 00:00:29.358
Meltwater helped Danone to monitor
the opinions through social media for

7
00:00:29.358 --> 00:00:31.873
one of their marketing campaigns.

8
00:00:31.873 --> 00:00:35.204
They were able to measure what
was impactful and what was not,

9
00:00:35.204 --> 00:00:36.690
through such monitoring.

10
00:00:37.890 --> 00:00:42.610
Meltwater also helped Danone manage
a potential reputation issue.

11
00:00:42.610 --> 00:00:45.380
When a crisis occurred
related to horse DNA

12
00:00:45.380 --> 00:00:48.320
being in some meat products across Europe.

13
00:00:48.320 --> 00:00:52.830
While Danone was confident that they
didn't have an issue with their products,

14
00:00:52.830 --> 00:00:57.730
having the information a couple of
hours before news hit the UK press.

15
00:00:57.730 --> 00:00:59.690
Allowed them to check and

16
00:00:59.690 --> 00:01:04.580
reassure their customers that their
products were safe to consume.

17
00:01:04.580 --> 00:01:09.040
You can imagine millions of mothers
having been reassured and happy for

18
00:01:09.040 --> 00:01:11.180
Danone's efforts on this.

19
00:01:11.180 --> 00:01:16.375
This is an excellent story about how
big data helped manage public opinion.

20
00:01:16.375 --> 00:01:21.136
And I'm sure Meltwater was able to help
them to measure the opinion impact through

21
00:01:21.136 --> 00:01:22.448
social media as well.

1
00:00:01.240 --> 00:00:04.508
Big data is now being
generated all around us.

2
00:00:04.508 --> 00:00:05.750
So what?

3
00:00:05.750 --> 00:00:07.450
It's the applications.

4
00:00:07.450 --> 00:00:12.170
It is the way in which big data can
serve human needs that makes it valued.

5
00:00:13.430 --> 00:00:17.971
Let's look at a few examples of the
applications big data is allowing us to

6
00:00:17.971 --> 00:00:19.226
imagine and build.

7
00:00:37.796 --> 00:00:43.420
Big data allows us to build better models,
which produce higher precision results.

8
00:00:44.470 --> 00:00:49.021
We are witnessing hugely innovative
approaches in how companies

9
00:00:49.021 --> 00:00:51.758
market themselves and sell products.

10
00:00:51.758 --> 00:00:53.958
How human resources are managed.

11
00:00:53.958 --> 00:00:56.178
How disasters are responded to.

12
00:00:56.178 --> 00:01:00.389
And many other applications that
evidenced based data is being

13
00:01:00.389 --> 00:01:02.460
used to influence decisions.

14
00:01:04.660 --> 00:01:06.670
What exactly does that mean?

15
00:01:06.670 --> 00:01:08.150
Here is one example.

16
00:01:08.150 --> 00:01:10.640
Many of you might have experienced it,
I do.

17
00:01:12.540 --> 00:01:16.140
Data, Amazon keeps some
things I've been looking at

18
00:01:16.140 --> 00:01:19.610
allows them to personalize
what they show me.

19
00:01:19.610 --> 00:01:24.300
Which hopefully helps narrow down
the huge raft of options I might get

20
00:01:24.300 --> 00:01:27.380
than just searching on dinner plates.

21
00:01:27.380 --> 00:01:32.307
Now, businesses can leverage technology
to make better informed decisions

22
00:01:32.307 --> 00:01:37.246
that are actually based on signals
generated by actual consumers, like me.

23
00:01:39.454 --> 00:01:43.980
Big data enables you to hear
the voice of each consumer as

24
00:01:43.980 --> 00:01:46.590
opposed to consumers at large.

25
00:01:47.920 --> 00:01:51.500
Now, many companies,
including Walmart and Target,

26
00:01:51.500 --> 00:01:57.160
use this information to personalize their
communications with their costumers, which

27
00:01:57.160 --> 00:02:01.240
in turns leads to better met consumer
expectations and happier customers.

28
00:02:03.550 --> 00:02:09.628
Which basically is to say, big data
has enabled personalized marketing.

29
00:02:09.628 --> 00:02:14.250
Consumers are copiously generating
publicly accessible data through

30
00:02:14.250 --> 00:02:16.560
social media sites,
like Twitter or Facebook.

31
00:02:17.690 --> 00:02:22.130
Through such data, the companies
are able to see their purchase history,

32
00:02:22.130 --> 00:02:26.970
what they searched for, what they watched,
where they have been, and

33
00:02:26.970 --> 00:02:29.650
what they're interested in
through their likes and shares.

34
00:02:30.920 --> 00:02:35.717
Let's look at some examples of how
companies are putting this information to

35
00:02:35.717 --> 00:02:39.868
build better marketing campaigns and
reach the right customers.

36
00:02:42.718 --> 00:02:46.985
One area we are all familiar with
are the recommendation engines.

37
00:02:46.985 --> 00:02:52.190
These engines leverage user patterns and
product features

38
00:02:52.190 --> 00:02:58.028
to predict best match product for
enriching the user experience.

39
00:02:58.028 --> 00:03:00.303
If you ever shopped on Amazon,

40
00:03:00.303 --> 00:03:04.596
you know you get recommendations
based on your purchase.

41
00:03:04.596 --> 00:03:07.861
Similarly, Netflix would
recommend you to watch

42
00:03:07.861 --> 00:03:10.590
new shows based on your viewing history.

43
00:03:12.610 --> 00:03:18.550
Another technique that companies use is
sentiment analysis, or in simple terms,

44
00:03:18.550 --> 00:03:22.550
analysis of the feelings around events and
products.

45
00:03:23.880 --> 00:03:27.900
Remember the blue plates I
purchased on Amazon.com?

46
00:03:27.900 --> 00:03:31.680
I not only can read the reviews
before purchasing them,

47
00:03:31.680 --> 00:03:35.460
I can also write a product
review once I receive my plates.

48
00:03:37.280 --> 00:03:40.500
This way, other customers can be informed.

49
00:03:41.880 --> 00:03:47.110
But more importantly, Amazon can keep
a watch on the product reviews and

50
00:03:47.110 --> 00:03:49.220
trends for a particular product.

51
00:03:49.220 --> 00:03:51.951
In this case, blue plates.

52
00:03:51.951 --> 00:03:58.300
For example, they can judge if a product
review is positive or negative.

53
00:03:59.940 --> 00:04:03.140
In this case,
while the first review is negative,

54
00:04:05.140 --> 00:04:07.530
the next two reviews are positive.

55
00:04:09.180 --> 00:04:13.130
Since these reviews are written in
English using a technique called natural

56
00:04:13.130 --> 00:04:16.780
language processing, and
other analytical methods,

57
00:04:16.780 --> 00:04:22.120
Amazon can analyze the general opinion of
a person or public about such a product.

58
00:04:24.130 --> 00:04:29.128
This is why sentiment analysis often
gets referred to as opinion mining.

59
00:04:31.298 --> 00:04:35.729
News channels are filled with
Twitter feed analysis every time

60
00:04:35.729 --> 00:04:39.420
an event of importance occurs,
such as elections.

61
00:04:40.810 --> 00:04:46.162
Brands utilize sentiment analysis
to understand how customers

62
00:04:46.162 --> 00:04:51.630
relate to their product,
positively, negatively, neutral.

63
00:04:51.630 --> 00:04:54.448
This depends heavily on use of
natural language processing.

64
00:04:57.183 --> 00:04:59.289
Mobile devices are ubiquitous and

65
00:04:59.289 --> 00:05:02.780
people almost always carry
their cellphones with them.

66
00:05:03.810 --> 00:05:07.200
Mobile advertising is a huge market for
businesses.

67
00:05:08.760 --> 00:05:13.581
Platforms utilize the sensors
in mobile devices,

68
00:05:13.581 --> 00:05:18.847
such as GPS, and
provide real time location based ads,

69
00:05:18.847 --> 00:05:23.456
offer discounts,
based on this deluge of data.

70
00:05:23.456 --> 00:05:28.063
This time, let's imagine that
I bought a new house and

71
00:05:28.063 --> 00:05:32.178
I happen to be in a few
miles range of a Home Depot.

72
00:05:32.178 --> 00:05:35.444
Sending me mobile coupons about paint,
shelves, and

73
00:05:35.444 --> 00:05:39.290
other new home related purchases
would remind me of Home Depot.

74
00:05:40.550 --> 00:05:43.590
There's a big chance I
would stop by Home Depot.

75
00:05:43.590 --> 00:05:44.790
Bingo!

76
00:05:44.790 --> 00:05:49.470
Now I would like to take a moment to
analyze what kinds of big data are needed

77
00:05:49.470 --> 00:05:50.270
to make this happen.

78
00:05:51.495 --> 00:05:55.990
There's definitely the integration
of my consumer information and

79
00:05:55.990 --> 00:06:00.710
the online and offline databases
that include my recent purchases.

80
00:06:00.710 --> 00:06:05.569
But more importantly,
the geolocation data that falls under

81
00:06:05.569 --> 00:06:09.128
a larger type of big data,
spacial big data.

82
00:06:09.128 --> 00:06:11.780
We will talk about spacial
data later in this class.

83
00:06:13.190 --> 00:06:17.770
Let's now talk about how the global
consumer behavior can be used for

84
00:06:17.770 --> 00:06:18.360
product growth.

85
00:06:20.170 --> 00:06:23.760
We are now moving from
personalize marketing

86
00:06:23.760 --> 00:06:26.230
to the consumer behavior as a whole.

87
00:06:27.920 --> 00:06:32.758
Every business wants to understand
their consumer’s collective

88
00:06:32.758 --> 00:06:37.258
behavior in order to capture
the ever-changing landscape.

89
00:06:37.258 --> 00:06:42.809
Several big data products enable this
by developing models to capture user

90
00:06:42.809 --> 00:06:48.730
behavior and allow businesses to target
the right audience for their product.

91
00:06:50.340 --> 00:06:53.250
Or, develop new products for
uncharted territories.

92
00:06:55.570 --> 00:06:57.550
Let's look at this example.

93
00:06:57.550 --> 00:07:00.797
After an analysis of their sales for
weekdays,

94
00:07:00.797 --> 00:07:06.319
an airline company might notice that their
morning flights are always sold out,

95
00:07:06.319 --> 00:07:09.908
while their afternoon
flights run below capacity.

96
00:07:09.908 --> 00:07:15.545
This company might decide to add more
morning flights based on such analysis.

97
00:07:16.900 --> 00:07:21.550
Notice that they are not using
individual consumer choices, but

98
00:07:21.550 --> 00:07:26.760
using all the flights purchased without
consideration to who purchased them.

99
00:07:28.200 --> 00:07:29.150
They might, however,

100
00:07:29.150 --> 00:07:34.400
decide to pay closer attention to
the demographic of these consumers

101
00:07:34.400 --> 00:07:38.850
using big data to also add similar
flights in other geographical regions.

102
00:07:40.350 --> 00:07:45.240
With rapid advances in genome
sequencing technology,

103
00:07:45.240 --> 00:07:51.580
the life sciences industry is experiencing
an enormous draw in biomedical big data.

104
00:07:53.120 --> 00:07:59.168
This biomedical data is being used
by many applications in research and

105
00:07:59.168 --> 00:08:01.398
personalized medicine.

106
00:08:01.398 --> 00:08:06.400
Did you know genomics data is one of
the largest growing big data types?

107
00:08:06.400 --> 00:08:13.940
Between 100 million and 2 billion human
genomes could be sequenced by year 2025.

108
00:08:13.940 --> 00:08:14.520
Impressive.

109
00:08:16.700 --> 00:08:19.080
This [INAUDIBLE] sequence data demands for

110
00:08:19.080 --> 00:08:24.580
between 2 exabytes and
40 exabytes in data storage.

111
00:08:24.580 --> 00:08:30.190
In comparison, all of YouTube only
requires 1 to 2 exabytes a year.

112
00:08:32.530 --> 00:08:35.886
An exabyte is 10 to the power 18 bites.

113
00:08:35.886 --> 00:08:41.818
That is, 18 zeros after 40.

114
00:08:41.818 --> 00:08:48.270
Of course, analysis of such massive
volumes of sequence data is expensive.

115
00:08:48.270 --> 00:08:50.680
It could take up to 10,000
trillion CPU hours.

116
00:08:54.580 --> 00:08:59.040
One of the biomedical applications
that this much data is enabling

117
00:08:59.040 --> 00:09:00.450
is personalized medicine.

118
00:09:02.060 --> 00:09:06.879
Before personalized medicine,
most patients without a specific type and

119
00:09:06.879 --> 00:09:09.862
stage of cancer received
the same treatment,

120
00:09:09.862 --> 00:09:12.858
which worked better for
some than the others.

121
00:09:14.968 --> 00:09:20.086
Research in this area is enabling
development of methods to analyze

122
00:09:20.086 --> 00:09:25.472
large scale data to develop solutions
that tailor to each individual,

123
00:09:25.472 --> 00:09:28.900
and hence hypothesize
to be more effective.

124
00:09:30.450 --> 00:09:36.260
A person with cancer may now still receive
a treatment plan that is standard,

125
00:09:36.260 --> 00:09:38.620
such as surgery to remove a tumor.

126
00:09:39.870 --> 00:09:43.750
However, the doctor may
also be able to recommend

127
00:09:43.750 --> 00:09:45.460
some type of personalized
cancer treatment.

128
00:09:47.150 --> 00:09:51.900
A big challenge in biomedical big data
applications, like many other fields,

129
00:09:51.900 --> 00:09:56.860
is how we can integrate many types of data
sources to gain further insight problem.

130
00:09:58.300 --> 00:10:01.648
In one of our future lectures,
my colleagues here at

131
00:10:01.648 --> 00:10:06.670
the Supercomputer Center,
will explain how he and his colleague have

132
00:10:06.670 --> 00:10:11.940
used big data from a variety of sources
for personalized patient interventions.

133
00:10:13.840 --> 00:10:19.010
Another application of big data comes
from interconnected mesh of large

134
00:10:19.010 --> 00:10:24.490
number of sensors implanted
across smart cities.

135
00:10:24.490 --> 00:10:28.790
Analysis of data generated
from sensors in real time

136
00:10:28.790 --> 00:10:33.140
allows cities to deliver better
service quality to inhabitants.

137
00:10:33.140 --> 00:10:38.340
And reduce unwanted affect such
as pollution, traffic congestion,

138
00:10:38.340 --> 00:10:41.470
higher than optimal cost on
delivering urban services.

139
00:10:42.910 --> 00:10:44.750
Let's take our city, San Diego.

140
00:10:46.370 --> 00:10:51.794
San Diego generates a huge volumes
of data from many sources.

141
00:10:51.794 --> 00:10:57.676
Traffic sensors, satellites,
camera networks, and more.

142
00:10:57.676 --> 00:10:59.694
What if we could integrate and

143
00:10:59.694 --> 00:11:04.140
synthesize these data streams to
do even more for our community?

144
00:11:05.260 --> 00:11:07.000
Using such big data,

145
00:11:07.000 --> 00:11:12.170
we can work toward making San Diego
the prototype digital city.

146
00:11:12.170 --> 00:11:15.020
Not only for life-threatening hazards, but

147
00:11:15.020 --> 00:11:20.280
making our daily lives better, such as
managing traffic flow more efficiently or

148
00:11:20.280 --> 00:11:24.870
maximizing energy savings,
even as we'll see next, wildfires.

149
00:11:26.450 --> 00:11:31.390
If you want to read more,
here's a link to the AT Kearney report,

150
00:11:31.390 --> 00:11:34.120
where they talk about other
areas using big data.

151
00:11:35.580 --> 00:11:39.880
As a summary,
big data has a huge potential

152
00:11:39.880 --> 00:11:44.140
to enable models with higher
precision in many application areas.

153
00:11:45.230 --> 00:11:50.175
And these highly precise models
are influencing and transforming business.

1
00:00:01.140 --> 00:00:04.170
Big Data Generated By People,
how is it being used?

2
00:00:05.740 --> 00:00:08.370
We listed a number of challenges for

3
00:00:08.370 --> 00:00:11.400
using unstructured data
generated by human activities.

4
00:00:13.080 --> 00:00:17.730
Now let's look at some of the emerging
technologies to tackle these challenges.

5
00:00:17.730 --> 00:00:22.930
And see some examples that turn
unstructured data into valuable insights.

6
00:00:41.968 --> 00:00:46.780
Although unstructured data specially
the kind generated by people has

7
00:00:46.780 --> 00:00:49.320
a number of challenges.

8
00:00:49.320 --> 00:00:53.300
The good news is that the business
culture of today is shifting

9
00:00:53.300 --> 00:00:57.070
to tackle these challenges and
take full advantage of such data.

10
00:00:58.150 --> 00:01:02.120
As it is often said,
a challenge is a perfect opportunity.

11
00:01:03.130 --> 00:01:06.150
This is certainly the case for
big data and

12
00:01:06.150 --> 00:01:09.680
these challenges have created
a tech industry of it's own.

13
00:01:10.860 --> 00:01:15.730
This industry is mostly centered or
as we would say, layered or

14
00:01:15.730 --> 00:01:21.493
stacked, around a few fundamental
open source big data frameworks.

15
00:01:21.493 --> 00:01:25.320
Need big data tools
are designed from scratch

16
00:01:25.320 --> 00:01:28.990
to manage unstructured information and
analyze it.

17
00:01:28.990 --> 00:01:32.860
A majority of these tools
are based on an open source

18
00:01:32.860 --> 00:01:34.620
big data framework called Hadoop.

19
00:01:35.840 --> 00:01:40.010
Hadoop is designed to support
the processing of large data sets

20
00:01:40.010 --> 00:01:42.960
in a distributed computing environment.

21
00:01:42.960 --> 00:01:47.510
This definition would already give you a
hint that it tackles the first challenge.

22
00:01:47.510 --> 00:01:51.140
Namely, the volume of
unstructured information.

23
00:01:52.150 --> 00:01:55.740
Hadoop can handle big batches
of distributed information but

24
00:01:55.740 --> 00:01:58.250
most often there's a need for

25
00:01:58.250 --> 00:02:03.650
a real time processing of people generated
data like Twitter or Facebook updates.

26
00:02:05.220 --> 00:02:09.950
Financial compliance monitoring is another
area of our central time processing is

27
00:02:09.950 --> 00:02:13.380
needed, in particular
to reduce market data.

28
00:02:14.750 --> 00:02:20.780
Social media and market data are two
types of what we call high velocity data.

29
00:02:21.840 --> 00:02:25.720
Storm and
Spark are two other open source frameworks

30
00:02:25.720 --> 00:02:29.480
that handle such real time
data generated at a fast rate.

31
00:02:30.490 --> 00:02:31.490
Both Storm and

32
00:02:31.490 --> 00:02:36.230
Spark can integrate data with any
database or data storage technology.

33
00:02:37.560 --> 00:02:41.810
As we have emphasized
before unstructured data

34
00:02:41.810 --> 00:02:45.740
does not have a relational data model so
it doesn't generally

35
00:02:45.740 --> 00:02:50.150
fit into the traditional data warehouse
model based on relational databases.

36
00:02:51.350 --> 00:02:55.870
Data warehouses are central repositories
of integrated data from one or

37
00:02:55.870 --> 00:02:56.490
more sources.

38
00:02:58.300 --> 00:03:04.260
The data that gets stored in warehouses,
gets extracted from multiple sources.

39
00:03:05.460 --> 00:03:09.850
It gets transformed into
a common structured form and

40
00:03:09.850 --> 00:03:13.120
it can slow that into
the central database for

41
00:03:13.120 --> 00:03:17.400
use by workers creating analytical
reports throughout an enterprise.

42
00:03:18.650 --> 00:03:25.613
This Exact Transform Load
process is commonly called ETL.

43
00:03:25.613 --> 00:03:29.480
This approach was fairly standard in
enterprise data systems until recently.

44
00:03:30.560 --> 00:03:34.530
As you probably noticed,
it is fairly static and

45
00:03:34.530 --> 00:03:37.350
does not fit well with today's
dynamic big data world.

46
00:03:38.550 --> 00:03:43.073
So how do today's businesses
get around this problem?

47
00:03:43.073 --> 00:03:47.834
Many businesses today are using a hybrid
approach in which their smaller

48
00:03:47.834 --> 00:03:51.894
structured data remains in
their relational databases, and

49
00:03:51.894 --> 00:03:56.750
large unstructured datasets get stored
in NoSQL databases in the cloud.

50
00:03:58.200 --> 00:04:03.840
NoSQL Data technologies are based
on non-relational concepts and

51
00:04:03.840 --> 00:04:08.310
provide data storage options
typically on computing clouds

52
00:04:08.310 --> 00:04:12.180
beyond the traditional relational
databases centered rate houses.

53
00:04:14.170 --> 00:04:19.640
The main advantage of using
NoSQL solutions is their ability

54
00:04:19.640 --> 00:04:24.430
to organize the data for
scalable access to fit the problem and

55
00:04:24.430 --> 00:04:27.330
objectives pertaining to
how the data will be used.

56
00:04:29.130 --> 00:04:34.990
For example, if the data will be used
in an analysis to find connections

57
00:04:34.990 --> 00:04:40.130
between data sets, then the best
solution is a graph database.

58
00:04:41.960 --> 00:04:44.800
Neo4j is an example of a graph database.

59
00:04:45.910 --> 00:04:49.630
Graph networks is a topic that
the graph analytics course

60
00:04:49.630 --> 00:04:53.140
later in this specialization,
we'll explain in depth.

61
00:04:53.140 --> 00:04:59.350
If the data will be best accessed using
key value pairs like a search engine

62
00:04:59.350 --> 00:05:05.490
scenario, the best solution is probably
a dedicated key value paired database.

63
00:05:08.710 --> 00:05:11.660
Cassandra is an example
of a key value database.

64
00:05:13.060 --> 00:05:14.010
These, and

65
00:05:14.010 --> 00:05:18.710
many other types of NoSQL systems will
be explained further in course two.

66
00:05:20.190 --> 00:05:24.160
So we are now confident that there
are emerging technologies for

67
00:05:24.160 --> 00:05:28.410
individual challenges to manage
people generated unstructured data.

68
00:05:29.610 --> 00:05:33.800
But how does one take advantage
of these to generate value?

69
00:05:35.710 --> 00:05:43.150
As we saw big data must pass through a
series of steps before it generates value.

70
00:05:43.150 --> 00:05:47.520
Namely data access, storage,
cleaning, and analysis.

71
00:05:49.220 --> 00:05:55.210
One approach to solve this problem is
to run each stage as a different layer.

72
00:05:56.400 --> 00:06:00.210
And use tools available to
fit the problem at hand, and

73
00:06:00.210 --> 00:06:03.730
scale analytical solutions to big data.

74
00:06:03.730 --> 00:06:08.110
In coming lectures, we will see
important tools that you can use

75
00:06:08.110 --> 00:06:11.840
to solve your big data problems in
addition to the ones you have seen today.

76
00:06:13.070 --> 00:06:17.960
Now let's take a step back and remind
ourselves what some of the value was.

77
00:06:19.220 --> 00:06:23.920
Remember how companies can listen to the
real voice of customers using big data?

78
00:06:25.540 --> 00:06:29.150
It is this type of generated
data that enabled it.

79
00:06:30.190 --> 00:06:35.150
Sentiment analysis analyzes social
media and other data to find

80
00:06:35.150 --> 00:06:40.750
whether people associate positively or
negatively with you business.

81
00:06:40.750 --> 00:06:45.210
Organizations are utilizing
processing of personal data to

82
00:06:45.210 --> 00:06:47.980
understand the true
preferences of their customers.

83
00:06:49.130 --> 00:06:54.110
Now let's take a fun quiz to guess how
much Twitter data companies analyze

84
00:06:54.110 --> 00:06:57.510
every day to measure sentiment
around their product.

85
00:06:59.080 --> 00:07:01.900
The answer is 12 terabytes a day.

86
00:07:03.040 --> 00:07:08.160
For comparison,
you would need to listen continuously for

87
00:07:08.160 --> 00:07:11.620
two years to finish listening
to 1 terabyte of music.

88
00:07:13.000 --> 00:07:15.332
Another example application area for

89
00:07:15.332 --> 00:07:18.960
people generated data is customer
behavior modeling and prediction.

90
00:07:20.100 --> 00:07:24.730
Amazon, Netflix and
a lot of other organizations,

91
00:07:24.730 --> 00:07:28.400
use analytics to analyze
preferences of their customers.

92
00:07:29.710 --> 00:07:35.260
Based on consumer behavior, organizations
suggest better products to customers,

93
00:07:36.350 --> 00:07:40.430
and in turn have happier customers and
higher profits.

94
00:07:41.610 --> 00:07:47.250
Another application area where the value
comes in the form of societal impact and

95
00:07:47.250 --> 00:07:50.540
social welfare, is disaster management.

96
00:07:51.720 --> 00:07:54.550
As you have seen in my wildfire example,

97
00:07:54.550 --> 00:07:58.030
there are many types of big data that
can help with disaster response.

98
00:07:59.450 --> 00:08:03.420
Data in the form of pictures and
tweets, helps facilitate

99
00:08:03.420 --> 00:08:08.380
a collective response to disaster
situations, such as evacuations through

100
00:08:08.380 --> 00:08:12.310
the safest route based on community
feedback through social media.

101
00:08:13.340 --> 00:08:16.820
There are also networks that
turn crowd sourcing and

102
00:08:16.820 --> 00:08:20.650
big data analytics into collective
disaster response tools.

103
00:08:21.920 --> 00:08:24.990
The International Network
of Crisis Mappers,

104
00:08:24.990 --> 00:08:29.950
also called Crisis Mappers Net,
is the largest of such networks and

105
00:08:29.950 --> 00:08:34.360
includes an active international
community of volunteers.

106
00:08:34.360 --> 00:08:40.190
Crisis Mappers use big data in the form
of aerial and satellite imagery,

107
00:08:40.190 --> 00:08:45.440
participatory maps and
live Twitter updates to analyze

108
00:08:45.440 --> 00:08:51.611
the data using geospatial platforms,
advanced visualization,

109
00:08:51.611 --> 00:08:56.300
live simulation and computational and
statistical models.

110
00:08:57.780 --> 00:09:03.220
Once analyzed the results get reported to
rapid response and humanitarian agencies

111
00:09:04.220 --> 00:09:10.020
in the form of mobile and
web applications.

112
00:09:10.020 --> 00:09:16.510
In 2015, right after the Nepal earthquake
Crises Mappers crowd source the analysis

113
00:09:16.510 --> 00:09:21.990
of tweets and mainstream media to
rapidly access disaster damage and

114
00:09:21.990 --> 00:09:27.210
needs and to identify where
humanitarian help is needed.

115
00:09:27.210 --> 00:09:32.663
This example is amazing and shows how
big data can have huge impacts for

116
00:09:32.663 --> 00:09:35.263
social welfare in times of need.

117
00:09:35.263 --> 00:09:38.400
You can learn more about this
story at the following link.

118
00:09:40.790 --> 00:09:45.679
As a summary, although there are
challenges in working with unstructured

119
00:09:45.679 --> 00:09:50.270
people generated data at a scale and
speed that applications demand.

120
00:09:50.270 --> 00:09:54.124
There are also emerging technologies and
solutions that are being

121
00:09:54.124 --> 00:09:58.890
used by many applications to generate
value from the rich source of information.

1
00:00:00.920 --> 00:00:05.485
Big Data Generated By People,
The Unstructured Challenge.

2
00:00:21.404 --> 00:00:26.471
People are generating massive amounts of
data every day through their activities on

3
00:00:26.471 --> 00:00:31.405
various social media networking sites
like Facebook, Twitter, and LinkedIn.

4
00:00:31.405 --> 00:00:36.655
Or online photo sharing sites like
Instagram, Flickr, or Picasa.

5
00:00:38.540 --> 00:00:40.620
And video sharing websites like YouTube.

6
00:00:42.020 --> 00:00:47.100
In addition an enormous amount of
information gets generated via

7
00:00:47.100 --> 00:00:52.250
blogging and commenting,
internet searches, more via text messages,

8
00:00:53.320 --> 00:00:56.460
email, and through personal documents.

9
00:00:57.920 --> 00:01:02.711
Most of this data is text-heavy and
unstructured,

10
00:01:02.711 --> 00:01:08.780
that is non-conforming to
a well-defined data model.

11
00:01:08.780 --> 00:01:12.690
We can also consider this
data to be content with

12
00:01:12.690 --> 00:01:15.630
occasionally some
description attached to it.

13
00:01:15.630 --> 00:01:20.360
This much activity Leads
to a huge growth in data.

14
00:01:21.480 --> 00:01:25.980
Did you know that in a single day,
Facebook users produce

15
00:01:25.980 --> 00:01:30.550
more data than combined US
academic research libraries?

16
00:01:32.100 --> 00:01:35.130
Let's look at some similar
daily data volume numbers

17
00:01:36.130 --> 00:01:38.460
from some of the biggest online platforms.

18
00:01:39.720 --> 00:01:43.790
It is amazing that some of these
numbers are in the petabyte range for

19
00:01:43.790 --> 00:01:44.700
daily activity.

20
00:01:45.840 --> 00:01:48.930
A petabyte is a thousand terabytes.

21
00:01:50.470 --> 00:01:55.360
The sheer size of mostly unstructured
data generated by humans

22
00:01:55.360 --> 00:01:56.950
brings a lot of challenges.

23
00:01:58.730 --> 00:02:05.250
Unstructured data refers to data that does
not conform to a predefined data model.

24
00:02:07.200 --> 00:02:10.380
So no relation model and no SQL.

25
00:02:11.980 --> 00:02:16.190
It is mostly anything that we
don't store in a traditional

26
00:02:16.190 --> 00:02:17.640
Relational database management system.

27
00:02:19.170 --> 00:02:22.480
Consider a sales receipt that
you get from a grocery store.

28
00:02:23.580 --> 00:02:26.790
It has a section for a date, a section for

29
00:02:26.790 --> 00:02:30.690
store name, and
a section for total amount.

30
00:02:32.200 --> 00:02:34.310
This is an example of structure.

31
00:02:35.340 --> 00:02:40.550
Humans generate a lot of
unstructured data in form of text.

32
00:02:40.550 --> 00:02:42.960
There's no given format to that.

33
00:02:42.960 --> 00:02:46.180
Look at all the documents that you
have written with your hand so far.

34
00:02:47.220 --> 00:02:52.550
Collectively, it is a bank of unstructured
data you have personally generated.

35
00:02:53.580 --> 00:02:58.160
In fact, 80 to 90% of all data in

36
00:02:58.160 --> 00:03:02.980
the world is unstructured and
this number is rapidly growing.

37
00:03:04.490 --> 00:03:09.531
Examples of unstructured data generated
by people includes texts, images,

38
00:03:09.531 --> 00:03:16.190
videos, audio,
internet searches, and emails.

39
00:03:16.190 --> 00:03:21.580
In addition to it's rapid growth
major challenges of unstructured data

40
00:03:21.580 --> 00:03:26.768
include multiple data formats,
like webpages, images, PDFs,

41
00:03:26.768 --> 00:03:34.170
power point, XML, and other formats that
were mainly built for human consumption.

42
00:03:34.170 --> 00:03:41.030
Think of it, although I can sort my
email with date, sender and subject.

43
00:03:41.030 --> 00:03:44.690
It would be really difficult
to write a program,

44
00:03:44.690 --> 00:03:49.430
to categorize all my email messages
based on their content and

45
00:03:49.430 --> 00:03:55.460
organize them for me accordingly another
challenge of human generated data

46
00:03:55.460 --> 00:04:01.560
is the volume and fast generation of data,
which is what we call velocity.

47
00:04:01.560 --> 00:04:07.504
Just take a moment to study this info
graphic, and observe what happens in one

48
00:04:07.504 --> 00:04:12.728
minute on the internet, and
consider how much to contribute to it.

49
00:04:15.088 --> 00:04:22.586
Moreover, confirmation of unstructured
data is often time consuming and costly.

50
00:04:22.586 --> 00:04:28.130
The costs and
time of the process of acquiring, storing,

51
00:04:28.130 --> 00:04:34.110
cleaning, retrieving, and processing
unstructured data can add up to quite and

52
00:04:34.110 --> 00:04:37.220
investment before we can start
reaping value from this process.

53
00:04:39.230 --> 00:04:41.790
It can be pretty hard
to find the tools and

54
00:04:41.790 --> 00:04:45.810
people to implement such a process and
reap value in the end.

55
00:04:47.030 --> 00:04:50.570
As a summary,
although there is an enormous amount of

56
00:04:50.570 --> 00:04:54.780
data generated by people,
most of this data is unstructured.

57
00:04:55.890 --> 00:04:59.230
The challenges of working with
unstructured data should not

58
00:04:59.230 --> 00:05:01.170
be taken lightly.

59
00:05:01.170 --> 00:05:07.310
Next, we'll look at how businesses are
tackling these challenges to gain insight.

60
00:05:07.310 --> 00:05:10.770
And thus, value out of working
with people generated data.

1
00:00:01.120 --> 00:00:05.970
As we have already seen, there are many
different exciting applications

2
00:00:05.970 --> 00:00:08.060
that are being enabled
by the Big Data era.

3
00:00:09.160 --> 00:00:13.500
As part of my core research here at
the San Diego Supercomputer Center,

4
00:00:13.500 --> 00:00:15.420
I work on building methodologies and

5
00:00:15.420 --> 00:00:20.150
tools to make Big Data useful to dynamic
data driven scientific applications.

6
00:00:21.170 --> 00:00:25.600
My colleagues and I work on many grand
challenge data science applications,

7
00:00:25.600 --> 00:00:30.903
in all areas of science and engineering,
including genomics, geoinformatics,

8
00:00:30.903 --> 00:00:36.050
metro science, energy management,
biomedicine, and personalized health.

9
00:00:37.660 --> 00:00:42.492
What is common to all these applications
is their unique way of bringing

10
00:00:42.492 --> 00:00:46.046
together new modes of data and
computing research.

11
00:00:46.046 --> 00:00:52.195
Let me tell you the one I'm passionate

12
00:00:52.195 --> 00:00:57.192
about, Wildfire Analytics,

13
00:00:57.192 --> 00:01:03.342
which breaks up into two components,

14
00:01:03.342 --> 00:01:07.779
prediction and response.

15
00:01:18.713 --> 00:01:21.629
Why is this so important?

16
00:01:21.629 --> 00:01:25.608
On May 2014, in San Diego County where

17
00:01:25.608 --> 00:01:30.804
the instructors of this
specialization live and work,

18
00:01:30.804 --> 00:01:36.330
there were 14 fires burning,
as many as nine at one time,

19
00:01:36.330 --> 00:01:42.964
which burned a total of 26,000 acres,
11,000 hectares,

20
00:01:42.964 --> 00:01:48.520
an area just less than the size
of the City of San Francisco.

21
00:01:50.710 --> 00:01:56.087
Six people were injured and
one person died.

22
00:01:56.087 --> 00:02:00.093
And these wildfires
resulted in a total cost of

23
00:02:00.093 --> 00:02:04.620
over $60 million US in damage and
firefighting.

24
00:02:05.870 --> 00:02:10.925
These wildfires can become so severe
that we actually call them firestorms.

25
00:02:12.150 --> 00:02:15.170
Although we cannot
control such fire storms,

26
00:02:15.170 --> 00:02:19.070
something we can do is to get ahead
of them by predicting their behavior.

27
00:02:20.790 --> 00:02:25.670
This is why disaster management
of ongoing wildfires relies

28
00:02:25.670 --> 00:02:29.470
heavily on understanding their
direction and rate of spread.

29
00:02:30.560 --> 00:02:33.000
As these fires are a part of our lives,

30
00:02:33.000 --> 00:02:38.410
we wanted to see if we can use Big Data to
monitor, predict and manage a firestorm.

31
00:02:40.400 --> 00:02:42.337
Why can Big Data help?

32
00:02:42.337 --> 00:02:46.685
As we will see in this video,
indeed, wildfire prevention and

33
00:02:46.685 --> 00:02:50.879
response can benefit from many
streams in our data torrent.

34
00:02:50.879 --> 00:02:56.087
Some streams are generated by
people through devices they carry.

35
00:02:56.087 --> 00:03:01.713
A lot come from sensors and satellites,
things that measure environmental factors.

36
00:03:01.713 --> 00:03:05.855
And some come from organizational data,
including area maps,

37
00:03:05.855 --> 00:03:10.839
better service updates and field content
databases, which archive how much

38
00:03:10.839 --> 00:03:15.991
registers vegetation and other types of
fuel are in the way of a potential fire.

39
00:03:17.840 --> 00:03:20.337
What makes this a Big Data problem?

40
00:03:20.337 --> 00:03:25.233
Because novel approaches and
responses can be taken if

41
00:03:25.233 --> 00:03:29.712
we can integrate this many
diverse data streams.

42
00:03:29.712 --> 00:03:34.504
Many such data sources have already
existed for quite some time.

43
00:03:34.504 --> 00:03:39.835
But what is lacking in disaster
management today is a dynamic system

44
00:03:39.835 --> 00:03:44.979
integration of real time sensor networks,
satellite imagery,

45
00:03:44.979 --> 00:03:50.405
near real time data management tools,
wildfire simulation tools,

46
00:03:50.405 --> 00:03:54.334
connectivity to emergency command centers,
and

47
00:03:54.334 --> 00:03:58.470
all these before, during,
and after a firestorm.

48
00:03:59.800 --> 00:04:03.450
As you will see,
the integration of diverse streams and

49
00:04:03.450 --> 00:04:07.760
novel ways is really what's driving
our ability to see new things and

50
00:04:07.760 --> 00:04:11.270
develop predictive analytics,
which may help improve our world.

51
00:04:12.770 --> 00:04:14.140
What are these diverse sources?

52
00:04:15.940 --> 00:04:20.870
One of the most important data sources
is sensor data streaming in from weather

53
00:04:20.870 --> 00:04:26.540
stations and satellites,
such sensed data include temperature,

54
00:04:26.540 --> 00:04:27.960
humidity, air pressure.

55
00:04:29.280 --> 00:04:33.346
We can also include image
data streaming from

56
00:04:33.346 --> 00:04:38.379
mountaintop cameras and
satellites in this category.

57
00:04:38.379 --> 00:04:41.967
Another important data source
comes from institutions

58
00:04:41.967 --> 00:04:44.867
such as
the San Diego Supercomputer Center,

59
00:04:44.867 --> 00:04:48.250
which generate data related
to wildfire modeling.

60
00:04:48.250 --> 00:04:53.585
These include past and current fire
perimeter maps put together by

61
00:04:53.585 --> 00:04:58.824
the authorities and fuel maps that
tell us about the vegetation,

62
00:04:58.824 --> 00:05:02.171
and other types of fuel in a fire's path.

63
00:05:02.171 --> 00:05:08.171
These types of data sources are often
static or updated at a slow rate,

64
00:05:08.171 --> 00:05:14.083
but they provide valuable data
that is well-curated and verified.

65
00:05:15.560 --> 00:05:20.450
A huge part of data on fires is
actually generated by the public

66
00:05:20.450 --> 00:05:24.820
on social media sites such as Twitter,
which support photo sharing resources.

67
00:05:26.650 --> 00:05:32.050
These are the hardest data sources to
streamline during an existing fire, but

68
00:05:32.050 --> 00:05:36.340
they can be very valuable once
integrated with other data sources.

69
00:05:38.820 --> 00:05:44.380
Imagine synthesizing all the pictures
on Twitter about an ongoing fire or

70
00:05:44.380 --> 00:05:47.850
checking the public sentiment
around the boundaries of a fire.

71
00:05:49.960 --> 00:05:53.710
Once you have access to such
information at your fingertips,

72
00:05:53.710 --> 00:05:56.610
there are many things you
can do with such data.

73
00:05:56.610 --> 00:06:01.300
You can simply monitor it, or
maybe you can visualize it.

74
00:06:03.310 --> 00:06:08.370
But it's not until you bring all these
different types of data sources together

75
00:06:08.370 --> 00:06:12.910
and integrate them with real time analysis
and predictive modeling that you can

76
00:06:12.910 --> 00:06:17.570
really make contributions in predicting
and responding to wildfire emergencies.

77
00:06:19.000 --> 00:06:22.221
So now,
I will like you to take a moment and

78
00:06:22.221 --> 00:06:27.296
imagine how Big Data might help
with firefighting in the future.

79
00:06:27.296 --> 00:06:32.280
All these streams of data will come
together in 3D displays that can show

80
00:06:32.280 --> 00:06:37.182
all the related information along
with weather and fire predictions,

81
00:06:37.182 --> 00:06:40.462
just like the way tornadoes
are managed today.
