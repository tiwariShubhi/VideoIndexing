
1
00:00:01.150 --> 00:00:05.590
In this video, we'll talk about a new
that is usually not covered much.

2
00:00:06.820 --> 00:00:07.510
It's called valence.

3
00:00:15.960 --> 00:00:19.440
Simply put Valence
refers to Connectedness.

4
00:00:20.440 --> 00:00:23.660
The more connected data is,
the higher it's valences.

5
00:00:24.830 --> 00:00:27.000
The term valence comes from chemistry.

6
00:00:28.030 --> 00:00:32.800
In chemistry, we talk about core electrons
and valence electrons of an atom.

7
00:00:33.910 --> 00:00:38.810
Valence electrons are in the outer most
shell, have the highest energy level and

8
00:00:38.810 --> 00:00:41.960
are responsible for
bonding with other atoms.

9
00:00:41.960 --> 00:00:46.560
That higher valence results in greater
boding, that is greater connectedness.

10
00:00:47.740 --> 00:00:51.670
This idea is carried over into our
definition of the term valence

11
00:00:51.670 --> 00:00:53.180
in the context of big data.

12
00:00:55.010 --> 00:00:58.750
Data items are often directly
connected to one another.

13
00:00:58.750 --> 00:01:01.490
A city is connected to
the country it belongs to.

14
00:01:02.530 --> 00:01:06.030
Two Facebook users are connected
because they are friends.

15
00:01:06.030 --> 00:01:08.340
An employee is connected
to his work place.

16
00:01:09.640 --> 00:01:11.470
Data could also be indirectly connected.

17
00:01:12.710 --> 00:01:16.150
Two scientists are connected,
because they are both physicists.

18
00:01:17.530 --> 00:01:23.520
For a data collection valence measures
the ratio of actually connected data items

19
00:01:23.520 --> 00:01:27.390
to the possible number of connections
that could occur within the collection.

20
00:01:28.460 --> 00:01:31.100
The most important aspect of valence

21
00:01:31.100 --> 00:01:33.870
is that the data connectivity
increases over time.

22
00:01:34.910 --> 00:01:39.620
The series of network graphs comes from
a social experiment where scientists

23
00:01:39.620 --> 00:01:44.230
attending a conference were asked to meet
other scientists they did not know before.

24
00:01:44.230 --> 00:01:45.830
After several rounds of meetings,

25
00:01:45.830 --> 00:01:49.840
they found new connections
shown by their red edges.

26
00:01:49.840 --> 00:01:55.500
Increase in valence can lead to emergent
group behavior in people networks,

27
00:01:55.500 --> 00:02:00.310
like creation of new groups and coalitions
that have shared values and goals.

28
00:02:02.760 --> 00:02:04.770
A high valence data set is denser.

29
00:02:06.060 --> 00:02:09.670
This makes many regular,
analytic critiques very inefficient.

30
00:02:10.930 --> 00:02:14.350
More complex analytical methods
must be adopted to account for

31
00:02:14.350 --> 00:02:15.490
the increasing density.

32
00:02:16.780 --> 00:02:20.760
More interesting challenges arise due
to the dynamic behavior of the data.

33
00:02:22.030 --> 00:02:24.170
Now there is a need to model and

34
00:02:24.170 --> 00:02:28.480
predict how valence of a connected data
set may change with time and volume.

35
00:02:29.890 --> 00:02:34.660
The dynamic behavior also leads to
the problem of event detection,

36
00:02:34.660 --> 00:02:38.510
such as bursts in the local
cohesion in parts of the data.

37
00:02:38.510 --> 00:02:40.760
And emergent behavior
in the whole data set,

38
00:02:40.760 --> 00:02:43.570
such as increased
polarization in a community.

1
00:00:01.450 --> 00:00:04.620
Now we'll talk about a form of
scalability called variety.

2
00:00:06.040 --> 00:00:10.500
In this case, scale does not
refer to the largeness of data.

3
00:00:10.500 --> 00:00:12.398
It refers to increased diversity.

4
00:00:21.985 --> 00:00:24.450
Here is an important mantra
you need to think about.

5
00:00:25.660 --> 00:00:30.530
When we, as data scientists, think of
data variety, we think of the additional

6
00:00:30.530 --> 00:00:35.450
complexity that results from more kinds
of data that we need to store, process,

7
00:00:35.450 --> 00:00:37.010
and combine.

8
00:00:37.010 --> 00:00:39.970
Now, many years ago when I
started studying data management,

9
00:00:40.980 --> 00:00:43.030
we always thought of data as tables.

10
00:00:44.310 --> 00:00:50.060
These tables could be in spreadsheets or
databases or just files, but somehow

11
00:00:50.060 --> 00:00:53.290
they will be modeled and manipulated
as rows and columns of of tables.

12
00:00:54.610 --> 00:01:00.620
Now, tables are still really important and
dominant, however today a much wider

13
00:01:00.620 --> 00:01:04.800
variety of data are collected, stored, and
analyzed to solve real world problems.

14
00:01:05.990 --> 00:01:10.610
Image data, text data, network data,
geographic maps, computer

15
00:01:10.610 --> 00:01:15.390
generated simulations are only a few of
the types of data we encounter everyday.

16
00:01:16.460 --> 00:01:21.010
The heterogeneity of data can be
characterized along several dimensions.

17
00:01:21.010 --> 00:01:22.990
We mentioned four such axes here.

18
00:01:24.910 --> 00:01:27.630
Structural variety refers
to the difference in

19
00:01:27.630 --> 00:01:29.870
the representation of the data.

20
00:01:29.870 --> 00:01:33.600
For example, an EKG signal is very
different from a newspaper article.

21
00:01:34.690 --> 00:01:39.630
A satellite image of wildfires from
NASA is very different from tweets

22
00:01:39.630 --> 00:01:42.110
sent out by people who
are seeing the fire spread.

23
00:01:43.230 --> 00:01:48.100
Media variety refers to the medium
in which the data gets delivered.

24
00:01:48.100 --> 00:01:52.150
The audio of a speech versus
the transcript of the speech

25
00:01:52.150 --> 00:01:55.300
may represent the same information
in two different media.

26
00:01:56.460 --> 00:01:59.872
Data objects like news video
may have multiple media.

27
00:01:59.872 --> 00:02:03.436
An image sequence, an audio,
and closed captioned text,

28
00:02:03.436 --> 00:02:05.730
all time synchronized to each other.

29
00:02:06.850 --> 00:02:11.160
Semantic variety is best
described two examples.

30
00:02:11.160 --> 00:02:15.480
We often use different units for
quantities we measure.

31
00:02:15.480 --> 00:02:19.870
Sometimes we also use qualitative
versus quantitative measures.

32
00:02:19.870 --> 00:02:23.230
For example, age can be a number or

33
00:02:23.230 --> 00:02:26.440
we represent it by terms like infant,
juvenile, or adult.

34
00:02:28.140 --> 00:02:31.030
Another kind of semantic
variety comes from different

35
00:02:31.030 --> 00:02:33.450
assumptions of conditions on the data.

36
00:02:33.450 --> 00:02:39.510
For example, if we conduct two income
surveys on two different groups of people,

37
00:02:39.510 --> 00:02:40.970
we may not be able to compare or

38
00:02:40.970 --> 00:02:44.020
combine them without knowing more
about the populations themselves.

39
00:02:45.300 --> 00:02:47.620
The variation and
availability takes many forms.

40
00:02:48.730 --> 00:02:52.510
For one, data can be available real time,

41
00:02:52.510 --> 00:02:56.110
like sensor data, or it can be stored,
like patient records.

42
00:02:57.350 --> 00:03:00.430
Similarly data can be
accessible continuously, for

43
00:03:00.430 --> 00:03:01.630
example from a traffic cam.

44
00:03:02.640 --> 00:03:04.490
Versus intermittently, for

45
00:03:04.490 --> 00:03:07.750
example, only when the satellite
is over the region of interest.

46
00:03:08.780 --> 00:03:12.650
This makes a difference between what
operations one can do with data,

47
00:03:12.650 --> 00:03:15.020
especially if the volume
of the data is large.

48
00:03:16.260 --> 00:03:19.700
We'll cover this in more
detail in course two

49
00:03:19.700 --> 00:03:24.049
when we explore the different genres
of data and how we model them.

50
00:03:25.620 --> 00:03:28.470
We should not think that
a single data object, or

51
00:03:28.470 --> 00:03:32.880
a collection of similar data objects,
will be all uniform in themselves.

52
00:03:32.880 --> 00:03:35.040
Emails, for example, is a hybrid entity.

53
00:03:36.480 --> 00:03:40.110
Some of this information can be a table,
like shown here.

54
00:03:41.310 --> 00:03:43.940
Now, the body of the email
usually has text in it.

55
00:03:45.040 --> 00:03:48.360
However, some of the text may
have ornaments around them.

56
00:03:48.360 --> 00:03:49.370
For example,

57
00:03:49.370 --> 00:03:54.320
the part highlighted in yellow represents
something called a markup on text.

58
00:03:55.420 --> 00:03:58.600
We'll get to markups later in the course.

59
00:03:58.600 --> 00:04:00.500
Emails contain attachments.

60
00:04:00.500 --> 00:04:02.920
These are files, or embedded images,

61
00:04:02.920 --> 00:04:06.140
or other multimedia objects
that the mailer allows.

62
00:04:06.140 --> 00:04:10.150
This screenshot from my Outlook
shows the image of a scanned image

63
00:04:10.150 --> 00:04:11.470
of a handwritten note.

64
00:04:12.540 --> 00:04:16.230
When you take a collection of
all emails from your mailbox, or

65
00:04:16.230 --> 00:04:20.360
that from an organization,
you will see that senders and

66
00:04:20.360 --> 00:04:22.490
receivers form a communication network.

67
00:04:24.130 --> 00:04:28.917
In 2001, there was a famous scandal around
a company called Enron that engaged in

68
00:04:28.917 --> 00:04:32.390
fraudulent financial reporting practices.

69
00:04:32.390 --> 00:04:37.450
Their email network, partly shown here,
has been studied by data scientist to find

70
00:04:37.450 --> 00:04:42.330
usual and unusual patterns of connections
among the people in the organization.

71
00:04:43.810 --> 00:04:47.080
An email collection can also
have it's own semantics.

72
00:04:47.080 --> 00:04:51.340
For example, an email cannot refer to,
that means cannot copy or

73
00:04:51.340 --> 00:04:52.640
forward, a previous email.

74
00:04:53.920 --> 00:04:58.170
Finally, an email server
is a real-time data source.

75
00:04:58.170 --> 00:05:00.160
But an email repository is not.

76
00:05:01.190 --> 00:05:05.610
Does email, and email collections,
demonstrate significant

77
00:05:05.610 --> 00:05:10.270
internal variation in structure,
media, semantics, and availability?

1
00:00:00.090 --> 00:00:03.478
Characteristics of Big Data- Velocity.

2
00:00:19.075 --> 00:00:23.612
Velocity refers to the increasing
speed at which big data is created and

3
00:00:23.612 --> 00:00:28.160
the increasing speed at which the data
needs to be stored and analyzed.

4
00:00:29.250 --> 00:00:34.310
Processing of data in real-time to match
its production rate as it gets generated

5
00:00:34.310 --> 00:00:37.810
is a particular goal
of big data analytics.

6
00:00:37.810 --> 00:00:41.090
For example,
this type of capability allows for

7
00:00:41.090 --> 00:00:45.110
personalization of advertisement
on the web pages you visit

8
00:00:45.110 --> 00:00:49.610
based on your recent search,
viewing, and purchase history.

9
00:00:49.610 --> 00:00:55.350
If a business cannot take advantage
of the data as it gets generated, or

10
00:00:55.350 --> 00:00:59.540
at the speed analysis of it is needed,
they often miss opportunities.

11
00:01:00.620 --> 00:01:05.540
In order to build a case for the
importance of this dimension of big data,

12
00:01:05.540 --> 00:01:07.290
let's imagine we are taking a road trip.

13
00:01:08.850 --> 00:01:12.610
You're looking for
some better information to start packing.

14
00:01:12.610 --> 00:01:14.730
In this case, the newer the information,

15
00:01:14.730 --> 00:01:19.140
the higher its relevance
in deciding what to pack.

16
00:01:19.140 --> 00:01:21.420
Would you use last month's
weather information or

17
00:01:21.420 --> 00:01:24.310
data from last year at this time?

18
00:01:24.310 --> 00:01:29.550
Or, would you use the weather
information from this week, yesterday or

19
00:01:29.550 --> 00:01:30.630
better, today?

20
00:01:31.820 --> 00:01:35.750
It makes sense to obtain the latest
information about weather and

21
00:01:35.750 --> 00:01:38.830
process it in a way that
makes your decisions easier.

22
00:01:38.830 --> 00:01:44.600
If the information is old,
it doesn't matter how accurate it is.

23
00:01:45.950 --> 00:01:49.190
Being able to catch up with
the velocity of big data and

24
00:01:49.190 --> 00:01:54.640
analyzing it as it gets generated can
even impact the quality of human life.

25
00:01:54.640 --> 00:02:00.700
Sensors and smart devices monitoring
the human body can detect abnormalities

26
00:02:00.700 --> 00:02:06.510
in real time and trigger immediate action,
potentially saving lives.

27
00:02:06.510 --> 00:02:10.950
This type of processing is what
we call real time processing.

28
00:02:10.950 --> 00:02:15.210
Real-time processing is quite
different from its remote relative,

29
00:02:15.210 --> 00:02:15.800
batch processing.

30
00:02:18.270 --> 00:02:22.320
Batch processing was the norm
until a couple of years ago.

31
00:02:22.320 --> 00:02:26.160
Large amounts of data would be
fed into large machines and

32
00:02:26.160 --> 00:02:27.870
processed for days at a time.

33
00:02:29.230 --> 00:02:33.870
While this type of processing is still
very common today, decisions based on

34
00:02:33.870 --> 00:02:39.490
information that is even few days old
can be catastrophic to some businesses.

35
00:02:41.620 --> 00:02:45.320
Organizations which make
decisions on latest data

36
00:02:45.320 --> 00:02:46.960
are more likely to hit the target.

37
00:02:48.860 --> 00:02:52.780
For this reason it's important
to match the speed of processing

38
00:02:52.780 --> 00:02:57.345
with the speed of information generation,
and get real time decision making power.

39
00:02:57.345 --> 00:03:01.497
In addition, today's sensor-powered

40
00:03:01.497 --> 00:03:06.843
socioeconomic climate
requires faster decisions.

41
00:03:06.843 --> 00:03:11.332
Hence, we can not wait for
all the data to be first produced,

42
00:03:11.332 --> 00:03:13.190
then fed into a machine.

43
00:03:14.480 --> 00:03:18.830
There are many applications where
new information is streaming and

44
00:03:18.830 --> 00:03:22.120
needs to be integrated
with existing data to

45
00:03:22.120 --> 00:03:26.825
produce decisions such as emergency
response planning in a tornado, or

46
00:03:26.825 --> 00:03:32.500
deciding trading strategies in real time,
or getting estimates in advertising.

47
00:03:34.520 --> 00:03:40.340
We have to digest chunks of data as they
are produced and give meaningful results.

48
00:03:42.900 --> 00:03:44.385
As more data comes in,

49
00:03:44.385 --> 00:03:48.930
your results will need to adapt to
reflect this change in the input.

50
00:03:50.060 --> 00:03:56.160
Decisions based on processing of already
acquired data such as batch processing,

51
00:03:56.160 --> 00:03:58.660
may give an incomplete picture.

52
00:03:58.660 --> 00:04:04.550
And hence, the applications need real
time status of the context at hand.

53
00:04:04.550 --> 00:04:05.910
That is, streaming analysis.

54
00:04:07.280 --> 00:04:12.420
Fortunately, with the event
of cheap sensors technology,

55
00:04:12.420 --> 00:04:17.520
mobile phones, and social media,
we can obtain the latest information

56
00:04:17.520 --> 00:04:21.940
at a much rapid rate and
in real time in comparison with the past.

57
00:04:22.990 --> 00:04:26.650
So how do you make sure we match
the velocity of the expectations

58
00:04:26.650 --> 00:04:28.910
to gain insights from big data?

59
00:04:28.910 --> 00:04:31.900
With the velocity of the big data.

60
00:04:31.900 --> 00:04:34.050
Rate of generation, retrieval,

61
00:04:34.050 --> 00:04:37.650
or processing of data is
application specific.

62
00:04:38.760 --> 00:04:43.660
The need for real time data-driven
actions within a business case is

63
00:04:43.660 --> 00:04:48.620
what in the end dictates the velocity
of analytics over big data.

64
00:04:49.920 --> 00:04:53.620
Sometimes precision of a minute is needed.

65
00:04:53.620 --> 00:04:55.750
Sometimes half a day.

66
00:04:55.750 --> 00:05:00.710
Let's look at these four paths and
discuss when to pick the right one for

67
00:05:00.710 --> 00:05:02.270
your analysis.

68
00:05:02.270 --> 00:05:06.937
The dollar signs next to the numbers
in this example indicate how costly

69
00:05:06.937 --> 00:05:09.000
the operation is.

70
00:05:09.000 --> 00:05:11.160
The more dollars, the higher the cost.

71
00:05:12.600 --> 00:05:16.750
When the timeliness of processed
information plays no role in decision

72
00:05:16.750 --> 00:05:21.950
making, the speed at which data
is generated becomes irrelevant.

73
00:05:21.950 --> 00:05:27.220
In other words, you can wait for
as long as it takes to process data.

74
00:05:27.220 --> 00:05:29.730
Days, months, weeks.

75
00:05:29.730 --> 00:05:33.120
And once processing is over,
you will look at the results and

76
00:05:33.120 --> 00:05:34.550
probably share them with someone.

77
00:05:35.680 --> 00:05:40.640
When timeliness is not an issue,
you can choose any of the four paths.

78
00:05:41.800 --> 00:05:44.150
You will likely pick the cheapest one.

79
00:05:45.290 --> 00:05:48.720
When timeliness of end result is an issue

80
00:05:48.720 --> 00:05:52.810
deciding which of the four paths
to choose is not so simple.

81
00:05:52.810 --> 00:05:56.620
You will have to make a decision
based on cost of hardware,

82
00:05:56.620 --> 00:06:01.110
time sensitivity of information,
future scenarios.

83
00:06:01.110 --> 00:06:06.480
In other words,
this becomes a business driven question.

84
00:06:06.480 --> 00:06:12.930
For example, if speed is really important
at all costs, you will pick path four.

85
00:06:12.930 --> 00:06:19.130
As a summary, we need to pay attention
to the velocity of big data.

86
00:06:19.130 --> 00:06:23.460
Streaming data gives information
on what's going on right now.

87
00:06:23.460 --> 00:06:28.670
Streaming data has velocity, meaning
it gets generated at various rates.

88
00:06:28.670 --> 00:06:33.370
And analysis of such data in
real time gives agility and

89
00:06:33.370 --> 00:06:37.620
adaptability to maximize
benefits you want to extract.

1
00:00:01.640 --> 00:00:04.377
Characteristics of Big Data, Veracity.

2
00:00:20.366 --> 00:00:23.980
Veracity of Big Data refers
to the quality of the data.

3
00:00:25.010 --> 00:00:29.760
It sometimes gets referred
to as validity or

4
00:00:29.760 --> 00:00:33.250
volatility referring to
the lifetime of the data.

5
00:00:34.840 --> 00:00:38.360
Veracity is very important for
making big data operational.

6
00:00:39.810 --> 00:00:43.900
Because big data can be noisy and
uncertain.

7
00:00:43.900 --> 00:00:50.885
It can be full of biases,
abnormalities and it can be imprecise.

8
00:00:50.885 --> 00:00:54.741
Data is of no value if it's not accurate,

9
00:00:54.741 --> 00:00:59.900
the results of big data analysis are only
as good as the data being analyzed.

10
00:01:01.480 --> 00:01:06.840
This is often described in analytics
as junk in equals junk out.

11
00:01:08.040 --> 00:01:12.760
So we can say although big data
provides many opportunities to make

12
00:01:12.760 --> 00:01:18.100
data enabled decisions,
the evidence provided by data

13
00:01:18.100 --> 00:01:22.710
is only valuable if the data
is of a satisfactory quality.

14
00:01:23.880 --> 00:01:26.870
There are many different
ways to define data quality.

15
00:01:27.890 --> 00:01:30.200
In the context of big data,

16
00:01:30.200 --> 00:01:34.040
quality can be defined as a function
of a couple of different variables.

17
00:01:35.230 --> 00:01:41.310
Accuracy of the data, the trustworthiness
or reliability of the data source.

18
00:01:41.310 --> 00:01:44.940
And how the data was generated
are all important factors

19
00:01:44.940 --> 00:01:46.720
that affect the quality of data.

20
00:01:48.100 --> 00:01:53.910
Additionally how meaningful the data
is with respect to the program that

21
00:01:53.910 --> 00:02:00.360
analyzes it, is an important factor, and
makes context a part of the quality.

22
00:02:02.040 --> 00:02:07.850
In this chart from 2015,
we see the volumes of data increasing,

23
00:02:07.850 --> 00:02:12.380
starting with small amounts
of enterprise data to larger,

24
00:02:12.380 --> 00:02:17.480
people generated voice over IP and
social media data and

25
00:02:17.480 --> 00:02:21.010
even larger machine generated sensor data.

26
00:02:21.010 --> 00:02:26.040
We also see that the uncertainty of
the data increases as we go from

27
00:02:26.040 --> 00:02:28.840
enterprise data to sensor data.

28
00:02:28.840 --> 00:02:31.320
This is as we would expect it to be.

29
00:02:31.320 --> 00:02:35.980
Traditional enterprise
data in warehouses have

30
00:02:35.980 --> 00:02:41.080
standardized quality solutions
like master processes for extract,

31
00:02:41.080 --> 00:02:46.310
transform and load of the data which
we referred to as before as ETL.

32
00:02:46.310 --> 00:02:51.080
As enterprises started incorporating less
structured and unstructured people and

33
00:02:51.080 --> 00:02:54.870
machine data into their
big data solutions,

34
00:02:54.870 --> 00:02:58.670
the data become messier and
more uncertain.

35
00:02:58.670 --> 00:03:00.210
There are many reasons for this.

36
00:03:01.220 --> 00:03:07.620
First, unstructured data on the internet
is imprecise and uncertain.

37
00:03:07.620 --> 00:03:13.730
In addition, high velocity big data
leaves very little or no time for

38
00:03:13.730 --> 00:03:19.810
ETL, and in turn hindering the quality
assurance processes of the data.

39
00:03:19.810 --> 00:03:24.090
Let's look at these product reviews for
a banana slicer on amazon.com.

40
00:03:24.090 --> 00:03:28.960
One of the five star reviews say that

41
00:03:28.960 --> 00:03:34.190
it saved her marriage and compared it
to the greatest inventions in history.

42
00:03:34.190 --> 00:03:38.900
Another five star reviewer said
that his parole officer recommended

43
00:03:38.900 --> 00:03:42.470
the slicer as he is not
allowed to be around knives.

44
00:03:42.470 --> 00:03:44.400
These are obviously fake reviewers.

45
00:03:45.410 --> 00:03:49.070
Now think of an automated product
assessment going through such

46
00:03:49.070 --> 00:03:53.930
splendid reviews and estimating lots
of sales for the banana slicer and

47
00:03:53.930 --> 00:03:58.390
in turn suggesting stocking more
of the slicer in the inventory.

48
00:03:58.390 --> 00:03:59.670
Amazon will have problems.

49
00:04:00.760 --> 00:04:06.420
For a more serious case let's look at
the Google flu trends case from 2013.

50
00:04:06.420 --> 00:04:11.788
For January 2013,
the Google Friends actually

51
00:04:11.788 --> 00:04:17.280
estimated almost twice as many
flu cases as was reported by CDC,

52
00:04:17.280 --> 00:04:20.870
the Centers for
Disease Control and Prevention.

53
00:04:22.390 --> 00:04:26.960
The primary reason behind this was that
Google Flu Trends used a big data on

54
00:04:26.960 --> 00:04:32.070
the internet and did not account properly
for uncertainties about the data.

55
00:04:33.210 --> 00:04:37.780
Maybe the news and social media
attention paid to the particularly

56
00:04:37.780 --> 00:04:42.450
serious level of flu that
year effected the estimate.

57
00:04:42.450 --> 00:04:46.840
And resulted in what we
call an over estimation.

58
00:04:46.840 --> 00:04:49.050
This is a perfect example for

59
00:04:49.050 --> 00:04:55.470
how inaccurate the results can be if
only big data is used in the analysis.

60
00:04:55.470 --> 00:04:59.820
Imagine the economical impact of
making health care preparations for

61
00:04:59.820 --> 00:05:01.920
twice the amount of flu cases.

62
00:05:01.920 --> 00:05:03.610
That would be huge.

63
00:05:03.610 --> 00:05:07.850
The Google flu trends example
also brings up the need for

64
00:05:07.850 --> 00:05:13.350
being able to identify where exactly
the big data they used comes from.

65
00:05:13.350 --> 00:05:16.300
What transformation did
big data go through up

66
00:05:16.300 --> 00:05:19.420
until the moment it was used for
a estimate?

67
00:05:19.420 --> 00:05:23.460
This is what we refer
to as data providence.

68
00:05:23.460 --> 00:05:27.630
Just like we refer to
an artifacts provenance.

69
00:05:27.630 --> 00:05:33.180
As a summary, the growing
torrents of big data pushes for

70
00:05:33.180 --> 00:05:36.590
fast solutions to utilize
it in analytical solutions.

71
00:05:37.630 --> 00:05:42.000
This creates challenges on
keeping track of data quality.

72
00:05:42.000 --> 00:05:46.510
What has been collected,
where it came from, and

73
00:05:46.510 --> 00:05:48.940
how it was analyzed prior to its use.

74
00:05:50.150 --> 00:05:52.910
This is akin to an art artifact

75
00:05:52.910 --> 00:05:55.490
having providence of everything
it has gone through.

76
00:05:56.540 --> 00:06:01.560
But even more complicated to achieve
with large volumes of data coming

77
00:06:01.560 --> 00:06:04.130
in varieties and velocities.

1
00:00:00.830 --> 00:00:03.232
Characteristics of Big Data- Volume.

2
00:00:13.965 --> 00:00:19.190
Volume is the big data dimension that
relates to the sheer size of big data.

3
00:00:20.410 --> 00:00:24.580
This volume can come from
large datasets being shared or

4
00:00:24.580 --> 00:00:30.240
many small data pieces and
events being collected over time.

5
00:00:31.460 --> 00:00:35.335
Every minute 204 million emails are sent,

6
00:00:35.335 --> 00:00:41.308
200,000 photos are uploaded, and 1.8
million likes are generated on Facebook.

7
00:00:41.308 --> 00:00:48.710
On YouTube, 1.3 million videos are viewed
and 72 hours of video are uploaded.

8
00:00:51.160 --> 00:00:54.120
But how much data are we talking about?

9
00:00:54.120 --> 00:00:59.170
The size and the scale of storage for
big data can be massive.

10
00:00:59.170 --> 00:01:03.310
You heard me say words that
start with peta, exa and

11
00:01:03.310 --> 00:01:09.220
yotta, to define size, but
what does all that really mean?

12
00:01:09.220 --> 00:01:15.210
For comparison, 100 megabytes will
hold a couple of encyclopedias.

13
00:01:16.460 --> 00:01:20.340
A DVD is around 5 GBs, and

14
00:01:20.340 --> 00:01:25.837
1 TB would hold around 300
hours of good quality video.

15
00:01:25.837 --> 00:01:32.530
A data-oriented business currently
collects data in the order of terabytes,

16
00:01:32.530 --> 00:01:35.880
but petabytes are becoming more
common to our daily lives.

17
00:01:36.930 --> 00:01:42.290
CERN's large hadron collider
generates 15 petabytes a year.

18
00:01:42.290 --> 00:01:47.340
According to predictions by an IDC report
sponsored by a big data company called

19
00:01:47.340 --> 00:01:54.150
EMC, digital data, will grow by
a factor of 44 until the year 2020.

20
00:01:54.150 --> 00:01:58.190
This is a growth from 0.8 zetabytes,

21
00:01:59.750 --> 00:02:04.280
In 2009 to 35.2 zettabytes in 2020.

22
00:02:04.280 --> 00:02:11.090
A zettabyte is 1 trillion gigabytes,
that's 10 to the power of 21.

23
00:02:11.090 --> 00:02:14.140
The effects of it will be huge!

24
00:02:15.160 --> 00:02:20.420
Think of all the time, cost,
energy that will be used to store and

25
00:02:20.420 --> 00:02:23.860
make sense of such an amount of data.

26
00:02:23.860 --> 00:02:26.430
The next era will be yottabytes.

27
00:02:26.430 --> 00:02:31.050
Ten to the power of 24 and
brontobytes, ten to the power of 27.

28
00:02:31.050 --> 00:02:36.410
Which is really hard to imagine for
most of us at this time.

29
00:02:36.410 --> 00:02:41.740
This is also what we call data
at an astronomical scale.

30
00:02:41.740 --> 00:02:44.280
The choice of putting the Milky Way Galaxy

31
00:02:45.540 --> 00:02:49.560
in the middle of the circle
is not just for aesthetics.

32
00:02:50.660 --> 00:02:54.520
This is what we would see if
we were to scale up 10 to

33
00:02:54.520 --> 00:02:56.780
the 21 times into the universe.

34
00:02:56.780 --> 00:02:57.510
Cool, isn't it?

35
00:02:58.750 --> 00:03:01.760
Please refer to the reading
in this module called,

36
00:03:01.760 --> 00:03:07.220
what does astronomical scale mean,
for a nice video on the powers of ten.

37
00:03:07.220 --> 00:03:14.400
All of these point to an exponential
growth in data volume and storage.

38
00:03:14.400 --> 00:03:17.200
What is the relevance of
this much data in our world?

39
00:03:18.260 --> 00:03:20.210
Remember the planes collecting big data?

40
00:03:21.550 --> 00:03:25.170
Our hope, as passengers,
is data means better flight safety.

41
00:03:26.540 --> 00:03:32.070
The idea is to understand that businesses
and organizations are collecting and

42
00:03:32.070 --> 00:03:36.520
leveraging large volumes of data
to improve their end products,

43
00:03:36.520 --> 00:03:40.366
whether it is safety, reliability,
healthcare, or governance.

44
00:03:40.366 --> 00:03:45.300
In general, in business the goal

45
00:03:45.300 --> 00:03:50.660
is to turn this much data into
some form of business advantage.

46
00:03:50.660 --> 00:03:55.140
The question is how do we
utilize larger volumes of data

47
00:03:55.140 --> 00:03:57.920
to improve our end product's quality?

48
00:03:57.920 --> 00:04:00.790
Despite a number of
challenges related to it.

49
00:04:01.840 --> 00:04:06.650
There are a number of challenges related
to the massive volumes of big data.

50
00:04:08.210 --> 00:04:11.970
The most obvious one is of course storage.

51
00:04:11.970 --> 00:04:14.800
As the size of the data increases so

52
00:04:14.800 --> 00:04:18.740
does the amount of storage space
required to store that data efficiently.

53
00:04:19.810 --> 00:04:24.340
However, we also need to be able
to retrieve that large amount of

54
00:04:24.340 --> 00:04:26.030
data fast enough, and

55
00:04:26.030 --> 00:04:32.140
move it to processing units in a timely
fashion to get results when we need them.

56
00:04:32.140 --> 00:04:35.900
This brings additional challenges
such as networking, bandwidth,

57
00:04:35.900 --> 00:04:37.205
cost of storing data.

58
00:04:37.205 --> 00:04:41.670
In-house versus cloud storage and
things like that.

59
00:04:41.670 --> 00:04:46.530
Additional challenges arise during
processing of such large data.

60
00:04:46.530 --> 00:04:50.450
Most existing analytical methods
won't scale to such sums of

61
00:04:50.450 --> 00:04:53.500
data in terms of memory,
processing, or IO needs.

62
00:04:55.150 --> 00:04:56.980
This means their performance will drop.

63
00:04:58.060 --> 00:05:02.220
You might be able to get good performance
for data from hundreds of customers.

64
00:05:02.220 --> 00:05:09.314
But how about scaling your solution
to 1,000 or 10,000 customers?

65
00:05:09.314 --> 00:05:15.270
As the volume increases performance and
cost start becoming a challenge.

66
00:05:16.880 --> 00:05:21.530
Businesses need a holistic
strategy to handle processing of

67
00:05:21.530 --> 00:05:26.090
large scale data to their benefit
in the most cost effective manner.

68
00:05:26.090 --> 00:05:29.690
Evaluating the options across
the dimensions mentioned here,

69
00:05:29.690 --> 00:05:33.580
is the first step when it comes to
continuously increasing data size.

70
00:05:33.580 --> 00:05:38.290
We will revisit this topic
later on in this course.

71
00:05:38.290 --> 00:05:44.330
As a summary volume is the dimension
of big data related to its size and

72
00:05:44.330 --> 00:05:45.540
its exponential growth.

73
00:05:46.770 --> 00:05:52.126
The challenges with working with volumes
of big data include cost, scalability,

74
00:05:52.126 --> 00:05:56.725
and performance related to their storage,
access, and processing.
