
1
00:00:01.860 --> 00:00:06.740
In this video we will provide a quick
summary of the main points from our

2
00:00:06.740 --> 00:00:08.960
first course on introduction to big data.

3
00:00:10.310 --> 00:00:12.930
If you have just completed
our first course and

4
00:00:12.930 --> 00:00:16.810
do not need a refresher,
you may now skip to the next lecture.

5
00:00:18.480 --> 00:00:23.000
After this video,
you will be able to recall

6
00:00:23.000 --> 00:00:27.480
what started the big data era and
the three main big data sources.

7
00:00:29.310 --> 00:00:32.830
Summarize the volume,
variety, velocity and

8
00:00:32.830 --> 00:00:35.350
veracity issues related to each source.

9
00:00:36.790 --> 00:00:42.140
Explain the five step data science
process to gain value from big data.

10
00:00:43.510 --> 00:00:46.680
Remember the main elements
of the Hadoop Stack.

11
00:00:49.670 --> 00:00:54.950
We began our first course with
an explanation of how a lead torrent of

12
00:00:54.950 --> 00:01:00.690
big data combined with cloud computing
capabilities to process data anytime and

13
00:01:00.690 --> 00:01:05.450
anywhere has been at the core of
the launch of the Big Data Era.

14
00:01:08.470 --> 00:01:13.240
This big torrent of big data is often
boil down to a few varieties of data

15
00:01:13.240 --> 00:01:18.282
generated by machines, people and

16
00:01:18.282 --> 00:01:23.810
organizations with machine generated data.

17
00:01:23.810 --> 00:01:28.650
We refer to the data generated from real
time sensors and industrial machinery or

18
00:01:28.650 --> 00:01:30.020
vehicles.

19
00:01:30.020 --> 00:01:33.350
Web logs that track user behavior online.

20
00:01:33.350 --> 00:01:35.470
environmental sensors,

21
00:01:35.470 --> 00:01:38.680
personal health trackers among
many other sense data sources.

22
00:01:40.220 --> 00:01:46.050
With human generated data, we really refer
to the vast amount of social media data,

23
00:01:46.050 --> 00:01:49.760
status updates, tweets, photos and videos.

24
00:01:51.260 --> 00:01:56.600
With organization generated data,
we refer to more traditional types of data

25
00:01:56.600 --> 00:01:59.770
including transaction
information data bases and

26
00:01:59.770 --> 00:02:02.740
structure data often
stored in data warehouses.

27
00:02:03.960 --> 00:02:10.180
Note that big data can be structured,
semi-structured, and unstructured.

28
00:02:10.180 --> 00:02:15.030
Which is a topic we will talk about
more and in depth later in this course.

29
00:02:18.775 --> 00:02:23.975
Whatever your big data application is and
the types of big data you're using,

30
00:02:23.975 --> 00:02:29.905
the real value will come from integrating
different types of data sources and

31
00:02:29.905 --> 00:02:31.755
analyzing them at scale.

32
00:02:33.225 --> 00:02:36.355
Overall, by modeling, managing and

33
00:02:36.355 --> 00:02:40.700
integrating diverse streams
to improve our business and

34
00:02:40.700 --> 00:02:44.880
add value to our big data even
before we start analyzing it.

35
00:02:45.910 --> 00:02:47.591
As a part of modeling and

36
00:02:47.591 --> 00:02:52.804
managing big data is focusing on
the dimensions of scale availability and

37
00:02:52.804 --> 00:02:58.959
considering the challenges associated with
this dimensions to pick the right tools.

38
00:03:02.138 --> 00:03:07.460
Volume, variety and
velocity are the main dimensions which

39
00:03:07.460 --> 00:03:12.480
we characterized big data and
describe its challenges.

40
00:03:13.710 --> 00:03:18.550
We have huge amounts of data
in different formats and

41
00:03:18.550 --> 00:03:21.940
varying quality which
must be processed quickly

42
00:03:24.750 --> 00:03:30.810
veracity refers to the biases,
noise, and abnormality in data,

43
00:03:30.810 --> 00:03:36.190
or the unmeasurable certainty is in the
truthfulness and trustworthiness of data,

44
00:03:37.270 --> 00:03:41.470
and valence refers to
the connectedness of big data.

45
00:03:41.470 --> 00:03:43.790
Such as in the form of graph networks.

46
00:03:46.340 --> 00:03:52.500
Each V presents a challenging
dimension of big data mainly of size,

47
00:03:52.500 --> 00:03:57.160
complexity, speed, quality,
and consecutiveness.

48
00:03:57.160 --> 00:04:00.910
Although we can list some
other v' based on the context.

49
00:04:00.910 --> 00:04:05.390
We prefer to list these five as
fundamental dimensions which

50
00:04:05.390 --> 00:04:08.350
this big data specialization
helps you work on.

51
00:04:09.570 --> 00:04:15.130
Moreover, we must be sure to
never forget the sixth V: Value,

52
00:04:15.130 --> 00:04:18.220
at the heart of the big
data challenge is turning

53
00:04:18.220 --> 00:04:21.590
all of the other dimensions into
truly useful business value.

54
00:04:22.760 --> 00:04:26.490
How will Big Data benefit you and
your organization?

55
00:04:26.490 --> 00:04:30.960
The idea behind processing all
this Big Data in the first place

56
00:04:30.960 --> 00:04:33.100
is to bring value to the problem at hand.

57
00:04:34.360 --> 00:04:38.310
We need to take steps into
Big Data engineering and

58
00:04:38.310 --> 00:04:42.030
scalable data science to
generate value out of Big Data.

59
00:04:43.910 --> 00:04:44.900
We have all heard it.

60
00:04:44.900 --> 00:04:50.370
Data signs turns big data into insides,
or even actions.

61
00:04:51.490 --> 00:04:52.940
But what does that really mean?

62
00:04:54.340 --> 00:04:59.180
Data signs can be taught of as
the basis for empirical research.

63
00:04:59.180 --> 00:05:02.940
Like data is used to induce
information on the observations.

64
00:05:04.120 --> 00:05:06.830
These observations are mainly data.

65
00:05:06.830 --> 00:05:12.280
In our case, big data related to
a business or scientific use case.

66
00:05:14.550 --> 00:05:20.100
Inside is a term we use to refer to
the data products of data science.

67
00:05:20.100 --> 00:05:23.450
It is extracted from
a diverse amount of data

68
00:05:23.450 --> 00:05:27.620
through a combination of exploratory
data analysis and modeling.

69
00:05:28.830 --> 00:05:33.560
The questions are sometimes less
specific and it can require looking

70
00:05:33.560 --> 00:05:38.140
carefully at the data for patterns in
it to come up with a specific question.

71
00:05:40.400 --> 00:05:45.470
Another important point to recognize
is that data science is not static

72
00:05:45.470 --> 00:05:47.450
one time analysis.

73
00:05:47.450 --> 00:05:52.990
It involves a process where models
where you generate give us insights

74
00:05:52.990 --> 00:05:57.155
are constantly improve to a further and
prequel evidence and iterations.

1
00:00:01.390 --> 00:00:03.650
There are many ways to
look at this process.

2
00:00:04.790 --> 00:00:11.120
One way of looking at it
as two distinct activities.

3
00:00:11.120 --> 00:00:17.000
Mainly, big data engineering and
big data analytics,

4
00:00:17.000 --> 00:00:21.940
or computational big data
science as I like to call it,

5
00:00:21.940 --> 00:00:24.890
since more than simple
analytics are being performed.

6
00:00:26.840 --> 00:00:34.053
A more detailed way of looking at
the process reveals five listing steps or

7
00:00:34.053 --> 00:00:38.125
activities of the data science process,

8
00:00:38.125 --> 00:00:43.850
namely acquire, prepare,
analyze, report, and act.

9
00:00:43.850 --> 00:00:48.900
We can simply say that data science
happens at the boundary of all the steps.

10
00:00:48.900 --> 00:00:52.950
Ideally, this process should
support experimental work,

11
00:00:52.950 --> 00:00:58.930
which is constantly iterated and
leads to more scientific exploration,

12
00:00:58.930 --> 00:01:04.130
as well as producing actionable
results during these explorations

13
00:01:04.130 --> 00:01:08.495
using dynamic scalability on big data and
cloud platforms.

14
00:01:11.095 --> 00:01:15.842
This five step process can be used in
alternative ways in real life big data

15
00:01:15.842 --> 00:01:20.685
applications if we add the dependencies
of different tools to each other.

16
00:01:22.325 --> 00:01:25.915
The influence of big data pushes for

17
00:01:25.915 --> 00:01:30.335
alternative scalability approaches
at each step of the process.

18
00:01:31.600 --> 00:01:35.740
Acquire includes anything
that helps us retrieve data,

19
00:01:35.740 --> 00:01:39.740
including finding, accessing,
acquiring, and moving data.

20
00:01:41.500 --> 00:01:49.170
It includes identification of and
authenticated access to all related data,

21
00:01:49.170 --> 00:01:53.490
as well as transportation of data
from sources to destinations.

22
00:01:54.890 --> 00:02:00.760
It includes ways to subset and
match the data to regions or

23
00:02:00.760 --> 00:02:05.350
times of interest, which we sometimes
refer to as geospatial querying.

24
00:02:07.130 --> 00:02:11.234
We divide the prepare data
step into two sub-steps,

25
00:02:11.234 --> 00:02:14.070
based on the nature of the activity.

26
00:02:15.630 --> 00:02:21.060
The first step in data preparation
involves exploring the data

27
00:02:21.060 --> 00:02:25.690
to understand its nature,
what it means, its quality, and format.

28
00:02:27.160 --> 00:02:30.550
It often takes a preliminary
analysis of data, or

29
00:02:30.550 --> 00:02:32.670
samples of data, to understand it.

30
00:02:33.700 --> 00:02:36.770
This is why this primary
step is called prepare.

31
00:02:39.030 --> 00:02:42.990
Once we know more about the data
through exploratory analysis,

32
00:02:42.990 --> 00:02:46.420
the next step is pre-processing
of data for analysis.

33
00:02:47.540 --> 00:02:52.690
It includes cleaning data,
subsetting, or filtering data, and

34
00:02:52.690 --> 00:02:58.600
creating data, which programs can read and
understand by modelling raw data

35
00:02:58.600 --> 00:03:04.710
into a more defined data model, or
packaging it using a specific data format.

36
00:03:05.850 --> 00:03:11.270
We will learn more about data models and
data formats later in this course.

37
00:03:12.940 --> 00:03:15.780
If there are multiple data sets involved,

38
00:03:15.780 --> 00:03:20.220
this step also includes integration
of different data sources or

39
00:03:20.220 --> 00:03:23.745
streams, which is a topic we will
explore in our course three.

40
00:03:26.980 --> 00:03:32.050
The prepared data then would be
passed on to the analysis step,

41
00:03:32.050 --> 00:03:35.670
which involves selection of
analytical techniques to use,

42
00:03:35.670 --> 00:03:38.840
building a model of the data,
and analyzing results.

43
00:03:39.920 --> 00:03:43.520
This step can take a couple
of iterations on its own or

44
00:03:43.520 --> 00:03:47.119
might require a data scientist
to go back to steps 1 and

45
00:03:47.119 --> 00:03:50.410
2 to get more data or
package data in a different way.

46
00:03:51.700 --> 00:03:53.520
So, exploration never ends.

47
00:03:56.050 --> 00:04:02.413
Step 4 for communicating results includes
evaluation of analytical results,

48
00:04:02.413 --> 00:04:07.464
presenting them in a visual way,
creating reports that include

49
00:04:07.464 --> 00:04:12.250
an assessment of results with
respect to success criteria.

50
00:04:12.250 --> 00:04:19.293
Activities in this step can often be
referred to with terms like interpret,

51
00:04:19.293 --> 00:04:23.720
summarize, visualize, and post-process.

52
00:04:23.720 --> 00:04:28.880
The last step brings us back to the very
first reason we do data science,

53
00:04:28.880 --> 00:04:29.550
for a purpose.

54
00:04:31.090 --> 00:04:36.140
Reporting insights from analysis and
determining actions from insights based

55
00:04:36.140 --> 00:04:41.267
on the purpose you initially defined
is what we refer to as the act step.

56
00:04:43.210 --> 00:04:48.140
We have now seen all of the steps
in a typical data science process.

57
00:04:48.140 --> 00:04:52.750
Please note that this is an iterative
process and findings from

58
00:04:52.750 --> 00:04:57.310
one step may require previous steps
to be repeated, but need information,

59
00:04:57.310 --> 00:05:01.250
leading for further exploration and
application of these steps.

60
00:05:02.650 --> 00:05:07.230
Scalability of this process to big
data analysis requires the use of

61
00:05:07.230 --> 00:05:10.099
big data platforms like Hadoop.

1
00:00:00.450 --> 00:00:04.280
The Hadoop ecosystem frameworks and
applications

2
00:00:04.280 --> 00:00:08.870
provide such functionality through
several overarching themes and goals.

3
00:00:11.000 --> 00:00:15.830
First, they provide scalability
to store large volumes of data

4
00:00:15.830 --> 00:00:17.150
on commodity hardware.

5
00:00:18.760 --> 00:00:21.570
As the number of systems increase, so

6
00:00:21.570 --> 00:00:25.620
does the chance for
crashes and hardware failures.

7
00:00:25.620 --> 00:00:30.390
They handle fault tolerance to
gracefully recover from these problems.

8
00:00:32.650 --> 00:00:37.595
In addition, they are designed to handle
big data capacity and compressing text

9
00:00:37.595 --> 00:00:43.185
files, graphs of social networks,
streaming sensor data and raster images.

10
00:00:43.185 --> 00:00:45.635
We can add more data
types to this variety.

11
00:00:46.695 --> 00:00:48.355
For any given data type,

12
00:00:49.515 --> 00:00:53.635
you can find several projects in
the ecosystem that support it.

13
00:00:55.250 --> 00:00:58.890
Finally, they facilitate
a shared environment,

14
00:00:58.890 --> 00:01:02.050
allow multiple jobs to
execute simultaneously.

15
00:01:04.350 --> 00:01:08.930
Additionally, the Hadoop ecosystem
includes a wide range of open source

16
00:01:08.930 --> 00:01:14.170
projects backed by a large and
active community.

17
00:01:14.170 --> 00:01:18.480
These projects are free to use and
easy to find support for.

18
00:01:20.060 --> 00:01:25.690
Today, there are over 100
Big Data open source projects,

19
00:01:25.690 --> 00:01:31.250
and this continues to grow, many rely
on Hadoop, but some are independent.

20
00:01:33.960 --> 00:01:39.230
Here is, one way of looking at a subset
of tools in the Hadoop Ecosystem.

21
00:01:40.360 --> 00:01:45.240
This layer diagram is organized
vertically based on the interface.

22
00:01:46.390 --> 00:01:52.720
Lower level interfaces to storage and
scheduling on the bottom and

23
00:01:52.720 --> 00:01:56.520
high level languages and
interactivity at the top.

24
00:01:59.080 --> 00:02:05.112
The Hadoop distributed file system,
or HDFS, is the foundation for

25
00:02:05.112 --> 00:02:11.777
many big data frameworks since it
provides scalable and reliable storage.

26
00:02:11.777 --> 00:02:16.915
As the size of your data increases,
you can add commodity

27
00:02:16.915 --> 00:02:21.314
hardware to HDFS to
increase storage capacity.

28
00:02:21.314 --> 00:02:27.107
So it enables what we call
scaling out of your resources.

29
00:02:29.267 --> 00:02:32.960
Hadoop YARN provide
flexible scheduling and

30
00:02:32.960 --> 00:02:36.660
resource management over the HTFS storage.

31
00:02:37.660 --> 00:02:43.955
Yarn is use at Yahoo to schedule
jobs across 40,000 servers.

32
00:02:45.105 --> 00:02:50.045
MapReduce is a programming model
that simplifies parallel computing.

33
00:02:50.045 --> 00:02:54.725
Instead of dealing with the complexities
of synchronization and scheduling you only

34
00:02:54.725 --> 00:03:00.325
need to give MapReduce two
functions map and reduce.

35
00:03:00.325 --> 00:03:02.643
This programming model is so

36
00:03:02.643 --> 00:03:07.962
powerful that Google previously
used it for indexing websites.

37
00:03:07.962 --> 00:03:12.820
MapReduce, only assumes
a limited model to express data.

38
00:03:12.820 --> 00:03:16.429
Hive, and
Pig are two additional programming models,

39
00:03:16.429 --> 00:03:20.421
on top of MapReduce,
to augment data modeling of MapReduce,

40
00:03:20.421 --> 00:03:24.744
with relational algebra and
data flow modeling, respectively.

41
00:03:26.869 --> 00:03:31.682
Hive was created at Facebook to
issue SQL-like queries using

42
00:03:31.682 --> 00:03:34.280
MapReduce on their data in HDFS.

43
00:03:35.410 --> 00:03:40.773
Pig was created at Yahoo to model
dataflow based programs using MapReduce.

44
00:03:40.773 --> 00:03:45.345
Thanks to YARNs ability to
manage resources, not just for

45
00:03:45.345 --> 00:03:48.715
MapReduce but other programming models.

46
00:03:48.715 --> 00:03:52.955
Giraph was built for
processing large scale graphs efficiently.

47
00:03:54.335 --> 00:04:00.676
For example, Facebook uses Giraph to
analyze the social graphs of its users.

48
00:04:00.676 --> 00:04:05.181
Similarly, Storm, Spark and
Flink were built for

49
00:04:05.181 --> 00:04:09.595
real time and
In-memory processing of big data.

50
00:04:09.595 --> 00:04:14.013
On top of the YARN resource scheduler and
HDFS.

51
00:04:14.013 --> 00:04:20.555
In-memory processing is a powerful
way of running big data applications,

52
00:04:20.555 --> 00:04:26.598
even faster, achieving 100x better
performance for some tasks.

53
00:04:26.598 --> 00:04:30.845
Sometimes your data processing or
tasks are not easily or

54
00:04:30.845 --> 00:04:36.332
efficiently represented using the file and
directory model of storage,

55
00:04:36.332 --> 00:04:42.010
examples of this include collections
of key values or large sparse tables.

56
00:04:43.282 --> 00:04:48.859
NoSQL projects such as
Cassandra MongoDB and

57
00:04:48.859 --> 00:04:52.681
HBase handle all these cases.

58
00:04:52.681 --> 00:04:57.396
Cassandra was created at Facebook and
Facebook also use HBase for

59
00:04:57.396 --> 00:04:59.290
its messaging platform.

60
00:05:01.934 --> 00:05:07.562
Finally, running all this tools requires
a centralized management system for

61
00:05:07.562 --> 00:05:12.520
synchronization, configuration And
to ensure high availability.

62
00:05:13.660 --> 00:05:18.828
Zookeeper, created by Yahoo to
wrangle services named after animals,

63
00:05:18.828 --> 00:05:20.614
performs these duties.

64
00:05:23.214 --> 00:05:27.023
Just looking at the small number
of Hadoop stack components,

65
00:05:27.023 --> 00:05:31.700
we can already see that most of them
are dedicated to data modeling.

66
00:05:31.700 --> 00:05:36.010
Management, and
efficient processing of the data.

67
00:05:36.010 --> 00:05:40.610
In the rest of this course,
we will give you fundamental knowledge and

68
00:05:40.610 --> 00:05:46.580
some practical skills on how to start
modeling and managing your data, and

69
00:05:46.580 --> 00:05:52.310
picking the right tools for this activity
from a plethora of big data tools.

1
00:00:01.170 --> 00:00:04.780
Welcome to course two of
the big data specialization.

2
00:00:04.780 --> 00:00:06.390
I'm Amarnath Gupta.

3
00:00:06.390 --> 00:00:07.270
>> And I'm Ilkay Altintas.

4
00:00:07.270 --> 00:00:10.830
We are really excited to work
with you in this course,

5
00:00:10.830 --> 00:00:14.820
to develop your understanding and skills
in big data modeling and management.

6
00:00:16.100 --> 00:00:20.470
You might have just finished our first
course, and see the potential and

7
00:00:20.470 --> 00:00:22.570
challenges of big data.

8
00:00:22.570 --> 00:00:25.630
If you haven't it's not required but for

9
00:00:25.630 --> 00:00:29.510
those with less background in
the area you might find it valuable.

10
00:00:30.890 --> 00:00:34.070
>> Let's explain what we mean by
big data modeling and management.

11
00:00:35.400 --> 00:00:39.470
Suppose you have an application
where the data is big in a sense and

12
00:00:39.470 --> 00:00:45.470
it has a large volume, or has high speed,
or comes with a lot of variations.

13
00:00:45.470 --> 00:00:49.220
Even before you think of
how to handle the bigness,

14
00:00:49.220 --> 00:00:51.450
you need to have a sense of
what the data looks like.

15
00:00:52.550 --> 00:00:57.860
The goal of data modeling is to
formally explore the nature of data, so

16
00:00:57.860 --> 00:01:00.870
that you can figure out what
kind of storage you need, and

17
00:01:00.870 --> 00:01:02.470
what kind of processing you can do on it.

18
00:01:04.010 --> 00:01:07.010
The goal of data management
is to figure out

19
00:01:07.010 --> 00:01:10.840
what kind of infrastructure support
you would need for the data.

20
00:01:10.840 --> 00:01:15.130
For example, does your environment need
to keep multiple replicas of the data?

21
00:01:15.130 --> 00:01:18.454
Do you need to do statistical
computation with the data?

22
00:01:18.454 --> 00:01:23.120
Once these operational requirements,
you'll

23
00:01:23.120 --> 00:01:27.110
be able to choose the right system that
will let you perform these operations.

24
00:01:28.990 --> 00:01:33.200
>> We will also introduce
management of big data as it is

25
00:01:33.200 --> 00:01:38.530
streaming from data sources and talk
about storage architectures for big data.

26
00:01:38.530 --> 00:01:42.790
For example,
how can high velocity data get ingested,

27
00:01:42.790 --> 00:01:48.360
managed, stored in order to enable
real time analytical capabilities.

28
00:01:48.360 --> 00:01:53.560
Or what is the difference between
data at rest and data in motion?

29
00:01:53.560 --> 00:01:56.230
And how can a data system enable both?

30
00:01:57.570 --> 00:02:00.680
>> Once you've understood the basic
concepts of data modeling,

31
00:02:00.680 --> 00:02:04.300
data management, and streaming data,
we will introduce you

32
00:02:04.300 --> 00:02:09.120
to the characteristics of large volume
data and how to think about that.

33
00:02:09.120 --> 00:02:13.080
Thus, we will transition from
classical database management systems,

34
00:02:13.080 --> 00:02:18.730
that is DBMSs,
to big data management systems, or BDMSs.

35
00:02:18.730 --> 00:02:23.000
We'll present brief overviews of
several big data management systems

36
00:02:23.000 --> 00:02:24.430
available in the marketplace today.

37
00:02:25.630 --> 00:02:31.310
We are so excited to show you examples
of archived and streaming big data sets.

38
00:02:31.310 --> 00:02:36.500
Our goal is to provide you with simple
hands on exercises that require

39
00:02:36.500 --> 00:02:41.290
no programming, but show you what
big data looks like, and why various

40
00:02:41.290 --> 00:02:47.030
big data management systems are suitable
for specific kinds of big data.

41
00:02:47.030 --> 00:02:50.510
In the end you will be
able to design a simple

42
00:02:50.510 --> 00:02:53.110
big data information system
using this knowledge.

43
00:02:54.110 --> 00:02:56.280
We wish you a fun time learning and

44
00:02:56.280 --> 00:03:00.540
hope to hear from you in the discussion
forums and learner stories.

45
00:03:00.540 --> 00:03:02.529
>> Happy learning and think big data.

1
00:00:01.000 --> 00:00:06.820
This is the second course in the 2016
version of our big data specialization.

2
00:00:06.820 --> 00:00:10.820
>> After listening to learners like you,
we have changed some of the content and

3
00:00:10.820 --> 00:00:12.149
ordering in the specialization.

4
00:00:13.210 --> 00:00:17.200
This course takes you on the first
step of any big data project,

5
00:00:17.200 --> 00:00:19.470
its modeling and management.

6
00:00:19.470 --> 00:00:23.221
>> We hope that this background
on what big data looks like and

7
00:00:23.221 --> 00:00:27.662
how it is modeled and managed in
a large scale system will make you feel

8
00:00:27.662 --> 00:00:32.487
more prepared to take steps towards
retrieving and processing big data and

9
00:00:32.487 --> 00:00:37.338
perform big data analytics which
are topics coming up in the next courses.
