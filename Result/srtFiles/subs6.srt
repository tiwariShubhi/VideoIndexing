
1
00:00:00.005 --> 00:00:06.278
Step 2-B: Pre-processing Data

2
00:00:20.283 --> 00:00:24.904
The raw data that you get directly from
your sources are never in the format that

3
00:00:24.904 --> 00:00:26.875
you need to perform analysis on.

4
00:00:27.995 --> 00:00:31.885
There are two main goals in
the data pre-processing step.

5
00:00:31.885 --> 00:00:36.455
The first is to clean the data to
address data quality issues, and

6
00:00:36.455 --> 00:00:41.320
the second is to transform the raw
data to make it suitable for analysis.

7
00:00:43.850 --> 00:00:47.050
A very important part of data preparation

8
00:00:47.050 --> 00:00:49.606
is to address quality
of issues in your data.

9
00:00:49.606 --> 00:00:54.000
Real-world data is messy.

10
00:00:54.000 --> 00:01:00.160
There are many examples of quality issues
with data from real applications including

11
00:01:00.160 --> 00:01:05.990
inconsistent data like a customer with two
different addresses, duplicate customer

12
00:01:05.990 --> 00:01:11.460
records, for example, customers address
recorded at two different sales locations.

13
00:01:12.490 --> 00:01:14.600
And the two recordings don't agree.

14
00:01:16.190 --> 00:01:19.160
Missing customer agent demographics or
studies.

15
00:01:21.010 --> 00:01:25.750
Missing values like missing a customer
age in the demographic studies.

16
00:01:27.000 --> 00:01:32.160
invalid data like an invalid zip code for
example, a six digit code.

17
00:01:33.350 --> 00:01:39.060
And outliers like a sense of failure
causing values to be much higher or

18
00:01:39.060 --> 00:01:41.480
lower than expected for a period of time.

19
00:01:43.340 --> 00:01:45.860
Since we get the data downstream

20
00:01:45.860 --> 00:01:48.960
we usually have little control
over how the data is collected.

21
00:01:50.390 --> 00:01:55.120
Preventing data quality problems as
the data is being collected is not

22
00:01:55.120 --> 00:01:56.070
often an option.

23
00:01:57.740 --> 00:01:59.950
So we have the data that we get and

24
00:01:59.950 --> 00:02:03.770
we have to address quality issues
by detecting and correcting them.

25
00:02:05.710 --> 00:02:09.170
Here are some approaches we can take
to address this quality issues.

26
00:02:11.780 --> 00:02:14.630
We can remove data records
with missing values.

27
00:02:16.330 --> 00:02:19.290
We can merge duplicate records.

28
00:02:19.290 --> 00:02:23.430
This will require a way to determine
how to resolve conflicting values.

29
00:02:24.780 --> 00:02:29.320
Perhaps it makes sense to retain the newer
value whenever there's a conflict.

30
00:02:31.030 --> 00:02:34.500
For invalid values, the best estimate for

31
00:02:34.500 --> 00:02:38.330
a reasonable value can be
used as a replacement.

32
00:02:38.330 --> 00:02:42.800
For example, for
a missing age value for an employee,

33
00:02:42.800 --> 00:02:47.320
a reasonable value can be estimated based
on the employee's length of employment.

34
00:02:49.820 --> 00:02:54.340
Outliers can also be removed if
they are not important to the task.

35
00:02:56.590 --> 00:03:00.370
In order to address data
quality issues effectively,

36
00:03:00.370 --> 00:03:04.590
knowledge about the application,
such as how the data was collected,

37
00:03:04.590 --> 00:03:09.880
the user population, and the intended
uses of the application is important.

38
00:03:11.680 --> 00:03:15.280
This domain knowledge is
essential to making informed

39
00:03:15.280 --> 00:03:19.070
decisions on how to handle incomplete or
incorrect data.

40
00:03:22.740 --> 00:03:25.480
The second part of preparing data

41
00:03:25.480 --> 00:03:29.299
is to manipulate the clean data into
the format needed for analysis.

42
00:03:30.500 --> 00:03:32.910
The step is known by many names.

43
00:03:34.940 --> 00:03:39.894
Data manipulation,
data preprocessing, data wrangling,

44
00:03:39.894 --> 00:03:43.419
and even data munging, some operations for

45
00:03:43.419 --> 00:03:49.420
this type of operation I mean data
munging, wrangling, preprocessing,

46
00:03:49.420 --> 00:03:54.373
include, scaling, transformation,
feature selection,

47
00:03:54.373 --> 00:03:58.778
dimensionality reduction,
and data manipulation.

48
00:04:01.628 --> 00:04:08.840
Scaling involves changing the range of
values to be between a specified range.

49
00:04:08.840 --> 00:04:10.970
Such as from zero to one.

50
00:04:12.220 --> 00:04:16.460
This is done to avoid having certain
features that large values from

51
00:04:16.460 --> 00:04:18.570
dominating the results.

52
00:04:18.570 --> 00:04:22.990
For example,
in analyzing data with height and weight.

53
00:04:22.990 --> 00:04:27.350
To magnitude of weight values is much
greater than of the height values.

54
00:04:29.220 --> 00:04:32.880
So scaling all values
to be between zero and

55
00:04:32.880 --> 00:04:37.670
one will equalize contributions from
both height and weight features.

56
00:04:40.900 --> 00:04:46.000
Various transformations can be performed
on the data to reduce noise and

57
00:04:46.000 --> 00:04:46.720
variability.

58
00:04:48.380 --> 00:04:50.970
One such transformation is aggregation.

59
00:04:52.430 --> 00:04:57.260
Aggregate data generally results
in data with less variability,

60
00:04:57.260 --> 00:04:58.890
which may help with your analysis.

61
00:05:00.190 --> 00:05:05.720
For example, daily sales figures
may have many serious changes.

62
00:05:06.730 --> 00:05:11.880
Aggregating values to weekly or monthly
sales figures will result in similar data.

63
00:05:14.720 --> 00:05:19.340
Other filtering techniques can also be
used to remove variability in the data.

64
00:05:19.340 --> 00:05:23.530
Of course, this comes at
the cost of less detailed data.

65
00:05:23.530 --> 00:05:27.760
So these factors must be weighed for
the specific application.

66
00:05:30.570 --> 00:05:36.400
Future selection can involve removing
redundant or irrelevant features,

67
00:05:36.400 --> 00:05:40.300
combining features, and
creating new features.

68
00:05:41.460 --> 00:05:44.070
During the exploring data step,

69
00:05:44.070 --> 00:05:47.860
you might have discovered that
two features are correlated.

70
00:05:49.030 --> 00:05:52.350
In that case one of these
features can be removed

71
00:05:52.350 --> 00:05:55.090
without negatively affecting
the analysis results.

72
00:05:56.170 --> 00:05:59.870
For example,
the purchase price of a product and

73
00:05:59.870 --> 00:06:03.360
the amount of sales tax paid,
are likely to be correlated.

74
00:06:04.800 --> 00:06:08.550
Eliminating the sales tax amount,
then will be beneficial.

75
00:06:10.860 --> 00:06:12.810
Removing redundant or

76
00:06:12.810 --> 00:06:17.010
irrelevant features will make
the subsequent analysis much simpler.

77
00:06:19.320 --> 00:06:25.220
In other cases, you may want to combine
features or create new ones.

78
00:06:25.220 --> 00:06:29.540
For example,
adding the applicant's education level

79
00:06:29.540 --> 00:06:32.820
as a feature to a loan approval
application would make sense.

80
00:06:34.940 --> 00:06:39.790
There are also algorithms to automatically
determine the most relevant features,

81
00:06:39.790 --> 00:06:42.130
based on various mathematical properties.

82
00:06:45.440 --> 00:06:50.200
Dimensionality reduction is useful when
the data set has a large number of

83
00:06:50.200 --> 00:06:50.750
dimensions.

84
00:06:52.020 --> 00:06:55.830
It involves finding a smaller
subset of dimensions that

85
00:06:55.830 --> 00:06:58.350
captures most of
the variation in the data.

86
00:07:00.030 --> 00:07:03.600
This reduces the dimensions of the data

87
00:07:03.600 --> 00:07:08.400
while eliminating irrelevant features and
makes analysis simpler.

88
00:07:10.130 --> 00:07:12.089
A technique commonly used for

89
00:07:12.089 --> 00:07:16.878
dimensional reduction is called
principle component analysis or PCA.

90
00:07:20.528 --> 00:07:26.120
Raw data often has to be manipulated to
be in the correct format for analysis.

91
00:07:27.140 --> 00:07:32.890
For example, from samples recording
daily changes in stock prices,

92
00:07:32.890 --> 00:07:35.520
we may want the capture price changes for

93
00:07:35.520 --> 00:07:38.990
a particular market segments
like real estate or health care.

94
00:07:40.230 --> 00:07:45.150
This would require determining which
stocks belong to which market segment.

95
00:07:45.150 --> 00:07:49.540
Grouping them together, and
perhaps computing the mean, range,

96
00:07:49.540 --> 00:07:51.530
standard deviation for each group.

97
00:07:54.120 --> 00:07:54.760
In summary,

98
00:07:55.910 --> 00:08:00.470
data preparation is a very important
part of the data science process.

99
00:08:00.470 --> 00:08:05.530
In fact, this is where you will spend most
of your time on any data science effort.

100
00:08:06.710 --> 00:08:11.680
It can be a tedious process,
but it is a crucial step.

101
00:08:11.680 --> 00:08:15.200
Always remember, garbage in, garbage out.

102
00:08:15.200 --> 00:08:16.987
If you don't spend the time and

103
00:08:16.987 --> 00:08:21.250
effort to create good data for the
analysis, you will not get good results

104
00:08:21.250 --> 00:08:25.399
no matter how sophisticated
the analysis technique you're using is.

1
00:00:00.005 --> 00:00:03.004
Step 3: Analyzing Data.

2
00:00:12.608 --> 00:00:15.650
Now that you have your
data nicely prepared,

3
00:00:15.650 --> 00:00:18.140
the next step is to analyze the data.

4
00:00:19.350 --> 00:00:23.620
Data analysis involves building
a model from your data,

5
00:00:23.620 --> 00:00:25.350
which is called input data.

6
00:00:26.620 --> 00:00:32.370
The input data is used by the analysis
technique to build a model.

7
00:00:34.010 --> 00:00:38.340
What your model generates
is the output data.

8
00:00:38.340 --> 00:00:41.140
There are different types of problems, and

9
00:00:41.140 --> 00:00:43.890
so there are different types
of analysis techniques.

10
00:00:45.370 --> 00:00:50.424
The main categories of analysis techniques
are classification, regression,

11
00:00:50.424 --> 00:00:56.330
clustering, association analysis,
and graph analysis.

12
00:00:56.330 --> 00:00:58.520
We will describe each one.

13
00:00:58.520 --> 00:01:03.650
In classification, the goal is to
predict the category of the input data.

14
00:01:04.740 --> 00:01:10.102
An example of this is predicting
the weather as being sunny,

15
00:01:10.102 --> 00:01:13.850
rainy, windy, or cloudy in this case.

16
00:01:13.850 --> 00:01:19.820
Another example is to classify
a tumor as either benign or malignant.

17
00:01:21.170 --> 00:01:27.270
In this case, the classification is
referred to as binary classification,

18
00:01:27.270 --> 00:01:29.830
since there are only two categories.

19
00:01:29.830 --> 00:01:32.660
But you can have many categories as well,

20
00:01:32.660 --> 00:01:37.530
as the weather prediction problem
shown here having four categories.

21
00:01:37.530 --> 00:01:41.780
Another example is to identify
handwritten digits as

22
00:01:41.780 --> 00:01:45.860
being in one of the ten
categories from zero to nine.

23
00:01:48.160 --> 00:01:53.310
When your model has to predict
a numeric value instead of a category,

24
00:01:53.310 --> 00:01:56.080
then the task becomes
a regression problem.

25
00:01:57.380 --> 00:02:01.210
An example of regression is to
predict the price of a stock.

26
00:02:02.410 --> 00:02:06.250
The stock price is a numeric value,
not a category.

27
00:02:06.250 --> 00:02:09.560
So this is a regression task
instead of a classification task.

28
00:02:11.300 --> 00:02:15.020
Other examples of regression
are estimating the weekly sales of a new

29
00:02:15.020 --> 00:02:19.740
product and
predicting the score on a test.

30
00:02:19.740 --> 00:02:25.910
In clustering, the goal is to
organize similar items into groups.

31
00:02:25.910 --> 00:02:31.390
An example is grouping a company's
customer base into distinct segments for

32
00:02:31.390 --> 00:02:36.360
more effective targeted marketing
like seniors, adults and

33
00:02:36.360 --> 00:02:38.420
teenagers, as we see here.

34
00:02:38.420 --> 00:02:43.050
Another such example is identifying
areas of similar topography,

35
00:02:43.050 --> 00:02:47.420
like mountains, deserts,
plains for land use application.

36
00:02:47.420 --> 00:02:52.300
Yet another example is determining
different groups of weather patterns,

37
00:02:52.300 --> 00:02:55.150
like rainy, cold or snowy.

38
00:02:55.150 --> 00:02:58.740
The goal in association analysis
is to come up with a set

39
00:02:58.740 --> 00:03:03.330
of rules to capture associations
within items or events.

40
00:03:03.330 --> 00:03:07.935
The rules are used to determine when
items or events occur together.

41
00:03:07.935 --> 00:03:12.300
A common application of association
analysis is known as market

42
00:03:12.300 --> 00:03:17.710
basket analysis, which is used to
understand customer purchasing behavior.

43
00:03:17.710 --> 00:03:22.660
For example, association analysis
can reveal that banking customers

44
00:03:22.660 --> 00:03:27.040
who have certificate of deposit accounts,
surety CDs, also

45
00:03:27.040 --> 00:03:31.790
tend to be interested in other investment
vehicles, such as money market accounts.

46
00:03:31.790 --> 00:03:35.000
This information can be used for
cross-selling.

47
00:03:35.000 --> 00:03:39.140
If you advertise money market
accounts to your customers with CDs,

48
00:03:39.140 --> 00:03:42.100
they're likely to open such an account.

49
00:03:42.100 --> 00:03:47.150
According to data mining folklore,
a supermarket chain used association

50
00:03:47.150 --> 00:03:51.990
analysis to discover a connection between
two seemingly unrelated products.

51
00:03:51.990 --> 00:03:56.940
They discovered that many customers who go
to the supermarket late on Sunday night

52
00:03:56.940 --> 00:04:02.330
to buy diapers also tend to buy beer,
who are likely to be fathers.

53
00:04:02.330 --> 00:04:05.590
This information was then
used to place beer and

54
00:04:05.590 --> 00:04:10.720
diapers close together and
they saw a jump in sales of both items.

55
00:04:10.720 --> 00:04:13.760
This is the famous diaper beer connection.

56
00:04:13.760 --> 00:04:18.090
When your data can be transformed into
a graph representation with nodes and

57
00:04:18.090 --> 00:04:22.330
links, then you want to use graph
analytics to analyze your data.

58
00:04:22.330 --> 00:04:26.220
This kind of data comes about when
you have a lot of entities and

59
00:04:26.220 --> 00:04:30.150
connections between those entities,
like social networks.

60
00:04:30.150 --> 00:04:35.111
Some examples where graph analytics can
be useful are exploring the spread of

61
00:04:35.111 --> 00:04:39.780
a disease or epidemic by analyzing
hospitals' and doctors' records.

62
00:04:39.780 --> 00:04:44.849
Identification of security threats
by monitoring social media,

63
00:04:44.849 --> 00:04:46.483
email and text data.

64
00:04:46.483 --> 00:04:51.015
And optimization of mobile
communications network traffic.

65
00:04:51.015 --> 00:04:56.110
And optimization of mobile
telecommunications network traffic,

66
00:04:56.110 --> 00:04:59.690
to ensure call quality and
reduce dropped calls.

67
00:04:59.690 --> 00:05:03.850
Modeling starts with selecting,
one of the techniques we listed

68
00:05:03.850 --> 00:05:08.680
as the appropriate analysis technique,
depending on the type of problem you have.

69
00:05:08.680 --> 00:05:12.690
Then you construct the model
using the data you've prepared.

70
00:05:12.690 --> 00:05:17.210
To validate the model,
you apply it to new data samples.

71
00:05:17.210 --> 00:05:19.960
This is to evaluate how well the model

72
00:05:19.960 --> 00:05:22.540
does on data that was
used to construct it.

73
00:05:22.540 --> 00:05:26.870
The common practice is to divide
the prepared data into a set of data for

74
00:05:26.870 --> 00:05:30.170
constructing the model and
reserving some of the data for

75
00:05:30.170 --> 00:05:33.330
evaluating the model after
it has been constructed.

76
00:05:33.330 --> 00:05:37.740
You can also use new data prepared the
same way as with the data that was used to

77
00:05:37.740 --> 00:05:39.060
construct model.

78
00:05:39.060 --> 00:05:43.950
Evaluating the model depends on the type
of analysis techniques you used.

79
00:05:43.950 --> 00:05:47.595
Let's briefly look at how
to evaluate each technique.

80
00:05:47.595 --> 00:05:52.595
For classification and regression,
you will have the correct output for

81
00:05:52.595 --> 00:05:54.655
each sample in your input data.

82
00:05:54.655 --> 00:05:56.945
Comparing the correct output and

83
00:05:56.945 --> 00:06:01.880
the output predicted by the model,
provides a way to evaluate the model.

84
00:06:01.880 --> 00:06:05.150
For clustering,
the groups resulting from clustering

85
00:06:05.150 --> 00:06:08.850
should be examined to see if they
make sense for your application.

86
00:06:08.850 --> 00:06:13.250
For example, do the customer
segments reflect your customer base?

87
00:06:13.250 --> 00:06:17.310
Are they helpful for
use in your targeted marketing campaigns?

88
00:06:17.310 --> 00:06:19.170
For association analysis and

89
00:06:19.170 --> 00:06:24.940
graph analysis, some investigation will be
needed to see if the results are correct.

90
00:06:24.940 --> 00:06:27.920
For example, network traffic delays

91
00:06:27.920 --> 00:06:33.220
need to be investigated to see what your
model predicts is actually happening.

92
00:06:33.220 --> 00:06:36.830
And whether the sources of the delays
are where they are predicted

93
00:06:36.830 --> 00:06:38.050
to be in the real system.

94
00:06:39.160 --> 00:06:44.450
After you have evaluated your model to get
a sense of its performance on your data,

95
00:06:44.450 --> 00:06:47.510
you will be able to
determine the next steps.

96
00:06:47.510 --> 00:06:51.000
Some questions to consider are,
should the analysis be

97
00:06:51.000 --> 00:06:55.340
performed with more data in order
to get a better model performance?

98
00:06:55.340 --> 00:06:57.760
Would using different data types help?

99
00:06:57.760 --> 00:06:59.990
For example, in your clustering results,

100
00:06:59.990 --> 00:07:04.070
is it difficult to distinguish
customers from distinct regions?

101
00:07:04.070 --> 00:07:07.050
Would adding zip code
to your input data help

102
00:07:07.050 --> 00:07:10.150
to generate finer grained
customer segments?

103
00:07:10.150 --> 00:07:14.060
Do the analysis results suggest
a more detailed look at

104
00:07:14.060 --> 00:07:15.900
some aspect of the problem?

105
00:07:15.900 --> 00:07:19.690
For example, predicting sunny
weather gives very good results,

106
00:07:19.690 --> 00:07:22.390
but rainy weather
predictions are just so-so.

107
00:07:22.390 --> 00:07:27.840
This means that you should take a closer
look at your examples for rainy weather.

108
00:07:27.840 --> 00:07:31.520
Perhaps you just need more
samples of rainy weather, or

109
00:07:31.520 --> 00:07:34.845
perhaps there are some
anomalies in those samples.

110
00:07:34.845 --> 00:07:40.380
Or maybe there are some missing data
that needs to be included in order

111
00:07:40.380 --> 00:07:42.440
to completely capture rainy weather.

112
00:07:42.440 --> 00:07:47.037
The ideal situation would be that your
model platforms very well with respect to

113
00:07:47.037 --> 00:07:49.620
the success criteria that were determined

114
00:07:49.620 --> 00:07:53.380
when you defined the problem at
the beginning of the project.

115
00:07:53.380 --> 00:07:56.920
In that case, you're ready to
move on to communicating and

116
00:07:56.920 --> 00:08:00.870
acting on the results that you
obtained from your analysis.

117
00:08:00.870 --> 00:08:05.670
As a summary, data analysis involves
selecting the appropriate technique for

118
00:08:05.670 --> 00:08:09.940
your problem, building the model,
then evaluating the results.

119
00:08:11.150 --> 00:08:13.516
As there are different types of problems,

120
00:08:13.516 --> 00:08:16.678
there are also different
types of analysis techniques.

1
00:00:02.925 --> 00:00:05.646
Step four, reporting insights.

2
00:00:15.628 --> 00:00:20.498
The fourth step in our data science
process is reporting the insights gained

3
00:00:20.498 --> 00:00:21.800
from our analysis.

4
00:00:23.660 --> 00:00:27.840
This is a very important step
to communicate your insights and

5
00:00:27.840 --> 00:00:30.320
make a case for
what actions should follow.

6
00:00:32.040 --> 00:00:37.380
It can change shape based on your
audience and should not be taken lightly.

7
00:00:38.510 --> 00:00:39.760
So how do you get started?

8
00:00:43.150 --> 00:00:48.040
The first thing to do is to look
at your analysis results and

9
00:00:48.040 --> 00:00:53.720
decide what to present or report as the
biggest value or biggest set of values.

10
00:00:54.990 --> 00:00:59.530
In deciding what to present you
should ask yourself these questions.

11
00:01:01.260 --> 00:01:02.740
What is the punchline?

12
00:01:02.740 --> 00:01:05.330
In other words, what are the main results?

13
00:01:07.930 --> 00:01:12.590
What added value do
these results provide or

14
00:01:12.590 --> 00:01:14.870
how can the model add to the application?

15
00:01:17.120 --> 00:01:21.840
How do the results compare to
the success criteria determined at

16
00:01:21.840 --> 00:01:23.260
the beginning of the project?

17
00:01:26.030 --> 00:01:30.220
Answers to these questions are the items
you need to include in your report or

18
00:01:30.220 --> 00:01:30.990
presentation.

19
00:01:32.120 --> 00:01:36.336
So make them the main topics and
gather facts to back them up.

20
00:01:39.198 --> 00:01:44.330
Keep in mind that not all of
your results may be rosy.

21
00:01:44.330 --> 00:01:49.650
Your analysis may show results that are
counter to what you were hoping to find,

22
00:01:49.650 --> 00:01:53.410
or results that are inconclusive or
puzzling.

23
00:01:54.600 --> 00:01:56.530
You need to show these results as well.

24
00:01:58.620 --> 00:02:02.950
Domain experts may find some of
these results to be puzzling, and

25
00:02:02.950 --> 00:02:06.932
inconclusive findings may
lead to additional analysis.

26
00:02:08.020 --> 00:02:11.920
Remember the point of
reporting your findings

27
00:02:11.920 --> 00:02:14.410
is to determine what
the next step should be.

28
00:02:17.070 --> 00:02:21.538
All findings must be presented so
that informed decisions can be made.

29
00:02:24.818 --> 00:02:29.180
Visualization is an important
tool in presenting your results.

30
00:02:30.220 --> 00:02:35.400
The techniques that we discuss and
explore in data can be used here as well.

31
00:02:35.400 --> 00:02:36.760
What were they?

32
00:02:36.760 --> 00:02:41.690
Scatter plots, line graphs,
heat maps, and other types of graphs

33
00:02:43.040 --> 00:02:46.390
are effective ways to present
your results visually.

34
00:02:48.330 --> 00:02:52.150
This time you're not
plotting the input data, but

35
00:02:52.150 --> 00:02:55.250
you're plotting the output
data with similar tools.

36
00:02:57.120 --> 00:03:01.238
You should also have tables with
details from your analysis as backups,

37
00:03:01.238 --> 00:03:04.418
if someone wants to take
a deeper dive into the results.

38
00:03:07.398 --> 00:03:10.080
There are many visualization
tools that are available.

39
00:03:11.660 --> 00:03:14.750
Some of the most popular open
source ones are listed here.

40
00:03:16.205 --> 00:03:20.370
R is a software package for
general data analysis.

41
00:03:21.990 --> 00:03:24.830
It has powerful visualization
capabilities as well.

42
00:03:26.300 --> 00:03:30.800
Python is a general purpose
programming language

43
00:03:30.800 --> 00:03:35.090
that also has a number of packages to
support data analysis and graphics.

44
00:03:36.510 --> 00:03:39.690
D3 is a JavaScript library for

45
00:03:39.690 --> 00:03:44.820
producing interactive web based
visualizations and data driven documents.

46
00:03:46.500 --> 00:03:51.325
Leaflet is a lightweight mobile
friendly JavaScript library

47
00:03:51.325 --> 00:03:53.650
to create interactive maps.

48
00:03:55.360 --> 00:04:00.656
Tableau Public Allows you
to create visualizations,

49
00:04:00.656 --> 00:04:07.916
in your public profile, and share them,
or put them, on a site, or blog.

50
00:04:07.916 --> 00:04:12.925
Google Charts provides
cross-browser compatibility,

51
00:04:12.925 --> 00:04:17.940
and closed platform portability
to iPhones and Android.

52
00:04:19.430 --> 00:04:27.865
Timeline is a JavaScript library
that allows you to create timelines.

53
00:04:27.865 --> 00:04:33.787
In summary, you want to report your
findings by presenting your results and

54
00:04:33.787 --> 00:04:37.780
value add with graphs
using visualization tools.

1
00:00:02.010 --> 00:00:05.922
Step 5, Act, turning insights into action.

2
00:00:15.758 --> 00:00:20.048
Now that you have evaluated
the results from your analysis and

3
00:00:20.048 --> 00:00:25.329
generated reports on the potential value
of the results, the next step is to

4
00:00:25.329 --> 00:00:31.075
determine what action or actions should
be taken, based on the insights gained?

5
00:00:31.075 --> 00:00:34.290
Remember why we started
bringing together the data and

6
00:00:34.290 --> 00:00:36.570
analyzing it in the first place?

7
00:00:36.570 --> 00:00:41.100
To find actionable insights
within all these data sets,

8
00:00:41.100 --> 00:00:44.970
to answer questions, or for
improving business processes.

9
00:00:46.120 --> 00:00:47.470
For example,

10
00:00:47.470 --> 00:00:51.440
is there something in your process that
should change to remove bottle necks?

11
00:00:52.580 --> 00:00:56.640
Is there data that should be added to your
application to make it more accurate?

12
00:00:57.810 --> 00:01:02.440
Should you segment your population
into more well defined groups for

13
00:01:02.440 --> 00:01:04.240
more effective targeted marketing?

14
00:01:05.510 --> 00:01:09.508
This is the first step in
turning insights into action.

15
00:01:09.508 --> 00:01:12.733
Now that you've determined
what action to take,

16
00:01:12.733 --> 00:01:17.450
the next step is figuring out
how to implement the action.

17
00:01:17.450 --> 00:01:21.610
What is necessary to add this action
into your process or application?

18
00:01:22.820 --> 00:01:24.080
How should it be automated?

19
00:01:25.440 --> 00:01:31.110
The stakeholders need to be identified and
become involved in this change.

20
00:01:31.110 --> 00:01:36.090
Just as with any process improvement
changes, we need to monitor and

21
00:01:36.090 --> 00:01:40.020
measure the impact of the action
on the process or application.

22
00:01:41.300 --> 00:01:45.600
Assessing the impact
leads to an evaluation.

23
00:01:45.600 --> 00:01:49.710
Evaluating results from the implemented
action will determine your next steps.

24
00:01:50.740 --> 00:01:55.640
Is there additional analysis that need
to be performed in order to yield

25
00:01:55.640 --> 00:01:56.730
even better results?

26
00:01:58.100 --> 00:01:59.740
What data should be revisited?

27
00:02:00.950 --> 00:02:04.980
Are there additional opportunities
that should be explored?

28
00:02:04.980 --> 00:02:09.685
For example, let's not forget
what big data enables us to do.

29
00:02:09.685 --> 00:02:16.553
Real-time actions based on high
velocity streaming information.

30
00:02:16.553 --> 00:02:21.067
We need to define what part of our
business needs real-time action to be

31
00:02:21.067 --> 00:02:25.670
able to influence the operations or
the interaction with the customer.

32
00:02:27.450 --> 00:02:32.430
Once we define these real time actions,
we need to make sure that

33
00:02:32.430 --> 00:02:37.080
there are automated systems, or
processes to perform such actions, and

34
00:02:37.080 --> 00:02:41.320
provide failure recovery
in case of problems.

35
00:02:41.320 --> 00:02:43.547
As a summary, big data and

36
00:02:43.547 --> 00:02:49.320
data science are only useful if
the insights can be turned into action,

37
00:02:49.320 --> 00:02:54.198
and if the actions are carefully
defined and evaluated.

1
00:00:01.360 --> 00:00:03.545
Steps in the Data Science Process.

2
00:00:11.405 --> 00:00:16.739
We have already seen a simple linear
form of data science process,

3
00:00:16.739 --> 00:00:21.900
including five distinct activities
that depend on each other.

4
00:00:23.140 --> 00:00:28.095
Let's summarize each activity further
before we go into the details of each.

5
00:00:28.095 --> 00:00:34.617
Acquire includes anything that makes
us retrieve data including; finding,

6
00:00:34.617 --> 00:00:39.210
accessing, acquiring, and moving data.

7
00:00:39.210 --> 00:00:45.640
It includes identification of and
authenticated access to all related data.

8
00:00:45.640 --> 00:00:50.000
And transportation of data from
sources to distributed files systems.

9
00:00:51.720 --> 00:00:57.340
It includes way to subset and match
the data to regions or times of interest.

10
00:00:57.340 --> 00:01:00.830
As we sometimes refer to
it as geo-spacial query.

11
00:01:02.150 --> 00:01:08.010
The next activity is prepare data,
we divide the pre-data activity.

12
00:01:08.010 --> 00:01:11.574
Into two steps based on
the nature of the activity.

13
00:01:11.574 --> 00:01:16.990
Namely, explore data and pre-process data.

14
00:01:16.990 --> 00:01:21.890
The first step in data preparation
involves literally looking

15
00:01:21.890 --> 00:01:26.750
at the data to understand its nature,
what it means, its quality and format.

16
00:01:28.130 --> 00:01:32.320
It often takes a preliminary
analysis of data, or

17
00:01:32.320 --> 00:01:34.230
samples of data, to understand it.

18
00:01:35.440 --> 00:01:38.230
This is why this step is called explore.

19
00:01:39.350 --> 00:01:43.100
Once we know more about the data
through exploratory analysis,

20
00:01:43.100 --> 00:01:46.625
the next step is pre-processing
of data for analysis.

21
00:01:46.625 --> 00:01:52.180
Pre-processing includes cleaning data,
sub-setting or

22
00:01:52.180 --> 00:01:57.660
filtering data, creating data,
which programs can read and

23
00:01:57.660 --> 00:02:03.660
understand, such as modeling raw
data into a more defined data model,

24
00:02:03.660 --> 00:02:07.260
or packaging it using
a specific data format.

25
00:02:08.810 --> 00:02:11.630
If there are multiple data sets involved,

26
00:02:11.630 --> 00:02:17.300
this step also includes integration
of multiple data sources, or streams.

27
00:02:17.300 --> 00:02:22.070
The prepared data then would be
passed onto the analysis step,

28
00:02:22.070 --> 00:02:24.590
which involves selection of
analytical techniques to use,

29
00:02:25.800 --> 00:02:29.190
building a model of the data,
and analyzing results.

30
00:02:30.330 --> 00:02:34.120
This step can take a couple
of iterations on its own or

31
00:02:34.120 --> 00:02:37.180
might require data scientists
to go back to steps one and

32
00:02:37.180 --> 00:02:42.350
two to get more data or
package data in a different way.

33
00:02:42.350 --> 00:02:48.070
Step four for communicating results
includes evaluation of analytical results.

34
00:02:48.070 --> 00:02:52.400
Presenting them in a visual way,
creating reports that include

35
00:02:52.400 --> 00:02:56.080
an assessment of results with
respect to success criteria.

36
00:02:57.100 --> 00:03:01.690
Activities in this step can often be
referred to with terms like interpret,

37
00:03:01.690 --> 00:03:04.780
summarize, visualize, or post process.

38
00:03:05.900 --> 00:03:10.390
The last step brings us back to the very
first reason we do data science,

39
00:03:11.680 --> 00:03:12.240
the purpose.

40
00:03:13.340 --> 00:03:18.470
Reporting insights from analysis and
determining actions from insights based

41
00:03:18.470 --> 00:03:23.300
on the purpose you initially defined
is what we refer to as the act step.

42
00:03:24.700 --> 00:03:29.010
We have now seen all the steps in
a typical data science process.

43
00:03:30.220 --> 00:03:35.320
Please note that this is an iterative
process and findings from one step

44
00:03:35.320 --> 00:03:38.970
may require the previous step to
be repeated with new information.
