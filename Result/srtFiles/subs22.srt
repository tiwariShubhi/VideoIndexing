
1
00:00:15.850 --> 00:00:20.205
Open XC is an open source hardware-software platform.

2
00:00:20.205 --> 00:00:25.230
We wanted to see what correlations we could actually create within this model dashboards.

3
00:00:25.230 --> 00:00:30.915
It plugs into your car, and it gives you all the data that you can possibly want.

4
00:00:30.915 --> 00:00:33.510
What we're interested in is looking at how you

5
00:00:33.510 --> 00:00:36.070
can make the car much more modular and customizable,

6
00:00:36.070 --> 00:00:38.365
getting a lot of this data and afford cars

7
00:00:38.365 --> 00:00:40.750
out and letting developers do whatever they want with it.

8
00:00:40.750 --> 00:00:44.170
You could kind of like update technology as technology progresses.

9
00:00:44.170 --> 00:00:45.930
So what we're really trying to do is like catch up with

10
00:00:45.930 --> 00:00:50.060
the consumer electronics design cycle not the automotive design cycle types.

11
00:00:50.060 --> 00:00:52.265
The clock hasn't started yet.

12
00:00:52.265 --> 00:00:56.120
One of the tests that we did was a gas car versus an electric car.

13
00:00:56.120 --> 00:00:58.230
So we're trying to see which is faster,

14
00:00:58.230 --> 00:00:59.965
which is expecting the most energy,

15
00:00:59.965 --> 00:01:02.070
which is the most cost efficient.

16
00:01:02.070 --> 00:01:05.240
And we wanted to

17
00:01:05.240 --> 00:01:09.025
measure doesn't electric car driver drive different than a gas car driver.

18
00:01:09.025 --> 00:01:11.345
So actually by using the Open XC data,

19
00:01:11.345 --> 00:01:13.120
and by splunking that creating links in

20
00:01:13.120 --> 00:01:17.190
dashboards will give it to match ups and correlations.

21
00:01:23.110 --> 00:01:29.965
It all comes down to how much are they mashing the gas.

22
00:01:29.965 --> 00:01:35.410
We've been internally calling that dashboard the "Lead Foot Dashboard".

23
00:01:45.700 --> 00:01:53.060
Who's the most efficient driver?

24
00:01:53.060 --> 00:01:56.740
There's one of our drivers actually going low and

25
00:01:56.740 --> 00:02:00.085
slow and actually using into the brakes not really gnashing on the gas,

26
00:02:00.085 --> 00:02:02.635
and driving more like a normal person would.

27
00:02:02.635 --> 00:02:07.265
When you drive an electric car, it's a lot quicker off the line than a gas powered car.

28
00:02:07.265 --> 00:02:10.215
So, we find people kind of smashing

29
00:02:10.215 --> 00:02:13.975
on the gas pedal but sort of easing up on the electric pedal.

30
00:02:13.975 --> 00:02:15.595
So I've got some data for you here.

31
00:02:15.595 --> 00:02:17.950
We see. What's this I'm doing?

32
00:02:17.950 --> 00:02:19.995
It says you're doing.

33
00:02:19.995 --> 00:02:20.645
There we go.

34
00:02:20.645 --> 00:02:24.570
Battery car is high. There you go.

35
00:02:24.570 --> 00:02:27.230
Hurry up.

36
00:02:27.230 --> 00:02:29.735
There we go. We don't even need because Open XC, We just cut it.

37
00:02:29.735 --> 00:02:31.670
These are basically all of

38
00:02:31.670 --> 00:02:35.160
the sensor readings that are being broadcast from that Firefly box.

39
00:02:35.160 --> 00:02:38.455
It's plugged into the diagnostics port of the car.

40
00:02:38.455 --> 00:02:41.350
It's about how much power can we kind of give

41
00:02:41.350 --> 00:02:45.190
away in order to incentivize a community of makers.

42
00:02:45.190 --> 00:02:47.645
We want to be able to actually harness that and create

43
00:02:47.645 --> 00:02:50.605
an environment where they can much more quickly,

44
00:02:50.605 --> 00:02:51.990
experiment with our ideas,

45
00:02:51.990 --> 00:02:54.725
enrich the platform in general,

46
00:02:54.725 --> 00:02:59.130
make our vehicles in that sense more valuable.

1
00:00:02.180 --> 00:00:05.380
In this hands on activity,
we will be performing queries in Splunk.

2
00:00:06.620 --> 00:00:09.980
First, we will open a browser and
login to Splunk.

3
00:00:09.980 --> 00:00:13.590
Next, we will import a CSV file and
search its contents.

4
00:00:13.590 --> 00:00:18.030
We will see how to filter fields for
specific values and

5
00:00:18.030 --> 00:00:20.532
also perform statistical
calculations on the data.

6
00:00:20.532 --> 00:00:23.678
Let's begin.

7
00:00:23.678 --> 00:00:28.236
First, open a web browser and
navigate to the Splunk web page.

8
00:00:28.236 --> 00:00:33.103
We'll enter localhost:8000.

9
00:00:36.793 --> 00:00:41.870
Next we'll log in to Splunk using
admin and the default password.

10
00:00:51.160 --> 00:00:55.231
Next, we'll import a CSV file into Splunk.

11
00:00:55.231 --> 00:00:56.952
We'll click on Add Data.

12
00:01:00.159 --> 00:01:05.704
Upload We'll click on Select File.

13
00:01:08.366 --> 00:01:11.838
And we'll choose the census.csv
file that we downloaded.

14
00:01:16.364 --> 00:01:17.583
Click Next.

15
00:01:22.148 --> 00:01:25.010
On the left,
it should say source type csv.

16
00:01:25.010 --> 00:01:30.066
If it does not, click on the button and

17
00:01:30.066 --> 00:01:35.128
go down to Structured and select csv.

18
00:01:35.128 --> 00:01:37.290
In this table,
we see a preview of the data.

19
00:01:39.050 --> 00:01:42.100
You should see the column names
of the CSV file at the top.

20
00:01:44.260 --> 00:01:45.360
Click on Next.

21
00:01:48.372 --> 00:01:49.888
Review.

22
00:01:51.026 --> 00:01:52.227
And Submit.

23
00:01:54.602 --> 00:01:58.797
Now that the file has been imported
successfully, click on Start Searching.

24
00:02:04.511 --> 00:02:09.539
In the search box,
it fills in the default query,

25
00:02:09.539 --> 00:02:16.260
source="cencus.csv" the host name and
sourcetype="csv"

26
00:02:18.161 --> 00:02:22.067
We could change these fields
to search other files or

27
00:02:22.067 --> 00:02:25.894
other data types if we
imported those into Splunk.

28
00:02:25.894 --> 00:02:31.135
Now let's search census.csv for
particular values in the fields.

29
00:02:31.135 --> 00:02:34.478
Let's search for all the data
where the state is California.

30
00:02:37.251 --> 00:02:40.908
We'll enter STNAME="California".

31
00:02:46.621 --> 00:02:50.521
You'll see the results down here.

32
00:02:50.521 --> 00:02:53.604
You could search for
other states by using or.

33
00:02:53.604 --> 00:02:57.581
For example, we can add OR
STNAME="Alaska".

34
00:02:57.581 --> 00:03:01.840
This will search for
state names equal to California or Alaska.

35
00:03:05.450 --> 00:03:07.270
We can add conditions
to our query as well.

36
00:03:07.270 --> 00:03:14.930
Let's search for state equals California
whose population was over one million.

37
00:03:14.930 --> 00:03:21.467
We'll do this by saying
STNAME = "California"

38
00:03:21.467 --> 00:03:26.925
CENSUS2010POP > 1000000.

39
00:03:32.477 --> 00:03:35.900
Can limit the results to one or
more columns.

40
00:03:35.900 --> 00:03:42.446
You do this by adding pipe table
CTYNAME to the end of our query.

41
00:03:47.573 --> 00:03:52.336
In Spunk queries, the pipe command is used
to send the outputs from the first part of

42
00:03:52.336 --> 00:03:53.778
the query into the next.

43
00:03:56.139 --> 00:03:58.441
You can also show more than
one column from the output.

44
00:03:58.441 --> 00:04:02.982
If we add a comma CENSUS2010POP
to the end of this,

45
00:04:02.982 --> 00:04:07.145
we'll see both the city name and
the population.

46
00:04:07.145 --> 00:04:10.944
You can also see a visualization of this
data by clicking on the Visualization tab.

47
00:04:13.883 --> 00:04:18.344
At the bottom, on the x-axis,
we see the county names, and

48
00:04:18.344 --> 00:04:21.420
the y values are the population numbers.

49
00:04:24.300 --> 00:04:27.994
Now let's perform some
statistics on this data.

50
00:04:27.994 --> 00:04:31.630
We'll begin by counting the number of
records where the state is California.

51
00:04:33.570 --> 00:04:39.951
You can do this by saying
STNAME="California" pipe stats count.

52
00:04:44.282 --> 00:04:49.468
Now switch back to the statistics tab,
see the result here

53
00:04:52.360 --> 00:04:55.775
Now let's see the total population for
California.

54
00:04:55.775 --> 00:05:01.241
You can replace count
with sum(CENUS2010POP)

55
00:05:05.290 --> 00:05:07.923
You can also calculate
the average population.

56
00:05:07.923 --> 00:05:09.781
We'll replace sum with mean.

1
00:00:01.970 --> 00:00:06.280
Hello, My name is Mitch Fleichmann,
senior instructor here at Splunk.

2
00:00:06.280 --> 00:00:08.670
Today we are going to
install Splunk on Linux.

3
00:00:10.230 --> 00:00:13.760
I'm using one of
the Splunk Education Linux servers.

4
00:00:13.760 --> 00:00:14.564
Let's examine the environment.

5
00:00:17.146 --> 00:00:21.040
First of all,
notice I'm logged in as the user splunker.

6
00:00:21.040 --> 00:00:24.600
As a best practice,
do not install Splunk as the root user.

7
00:00:27.730 --> 00:00:33.498
I'm currently in the /opt directory,
where I've already downloaded

8
00:00:33.498 --> 00:00:38.426
the tarball from splunk.com/download for
this platform.

9
00:00:40.954 --> 00:00:45.755
As a final check, let's check the system
to make sure we have the correct operating

10
00:00:45.755 --> 00:00:46.932
system and kernel.

11
00:00:49.680 --> 00:00:56.072
This is indeed a Linux machine, let's
get confirmation that it is also 64 bit.

12
00:01:02.580 --> 00:01:04.090
So we are good to go.

13
00:01:04.090 --> 00:01:07.519
The next step is to unzip and
untar the installer and

14
00:01:07.519 --> 00:01:10.729
that we can do with the gunzip and
tar commands.

15
00:01:18.205 --> 00:01:23.650
When everything untucks, you'll notice
a new sub directory created named splunk.

16
00:01:24.960 --> 00:01:28.670
And we can navigate to the splunk/bin
directory to start up Splunk.

17
00:01:31.450 --> 00:01:36.000
Couple of ways to start
Splunk with no switches.

18
00:01:37.330 --> 00:01:41.890
On the first startup, you'll be prompted
to read and agree to the software license.

19
00:01:43.610 --> 00:01:47.141
Or as a shortcut, you can start
up Splunk and accept the license.

20
00:01:55.707 --> 00:02:00.315
And notice a couple of port numbers
being grabbed, port 8000 for

21
00:02:00.315 --> 00:02:03.902
splunk web and port 8089 for
splunk management.

22
00:02:08.003 --> 00:02:09.528
The splunk daemon has started.

23
00:02:12.675 --> 00:02:14.603
Splunk is generating its own keys.

24
00:02:17.707 --> 00:02:21.520
And we can see the splunk web interface,
the URL.

25
00:02:26.050 --> 00:02:26.950
As a best practice,

26
00:02:26.950 --> 00:02:31.050
you may also want to consider automating
splunk to start when the machine boots.

27
00:02:32.060 --> 00:02:37.635
That you can do as the root user.

28
00:02:37.635 --> 00:02:41.841
By issuing the splunk enable command,

29
00:02:41.841 --> 00:02:46.686
enable boot-start as the -user splunker,

30
00:02:46.686 --> 00:02:51.793
the same user that we
just consult splunk with.

31
00:03:00.498 --> 00:03:03.960
So lets log in to splunk web and
see how the system looks.

32
00:03:03.960 --> 00:03:05.410
And for that we'll go back to the browser.

33
00:03:07.980 --> 00:03:09.230
Go to the appropriate URL.

34
00:03:13.340 --> 00:03:15.360
And we can see upon first login,

35
00:03:15.360 --> 00:03:20.730
you are prompted to login with
the credentials admin, password changeme.

36
00:03:28.180 --> 00:03:29.780
Since this is the first login,

37
00:03:29.780 --> 00:03:33.170
you also coach to change your
password to something more secure.

38
00:03:33.170 --> 00:03:36.218
And it's highly recommended to
follow this best practice as well.

39
00:03:46.435 --> 00:03:48.027
And also on first connection,

40
00:03:48.027 --> 00:03:51.715
you'll also see a splash screen
showing you what's new in version 6.

41
00:03:53.160 --> 00:03:54.950
In this case, Powerful Analytics.

42
00:03:56.190 --> 00:03:58.790
Some changes to the UI to
make it more intuitive.

43
00:04:01.400 --> 00:04:04.919
Simplified component management for
cluster management,

44
00:04:04.919 --> 00:04:06.647
folder management and so on.

45
00:04:08.737 --> 00:04:10.651
Also a richer developer experience.

46
00:04:14.147 --> 00:04:17.560
Close down this window and
you can explore the navigation options.

47
00:04:18.940 --> 00:04:20.410
And notice in the left side,

48
00:04:20.410 --> 00:04:23.470
you see a panel showing you
the apps to navigate to and manage.

49
00:04:24.960 --> 00:04:29.060
And on the right side, you see some
panel showing you data in the system and

50
00:04:29.060 --> 00:04:30.380
various links for help.

51
00:04:32.730 --> 00:04:34.936
Let's go to the search and
reporting app by clicking here.

52
00:04:39.527 --> 00:04:43.879
And you can see the search far up top,

53
00:04:43.879 --> 00:04:49.788
some tips on how to search and
then data to search.

54
00:04:49.788 --> 00:04:53.720
Since this is a fresh install,
there is no data to search, so

55
00:04:53.720 --> 00:04:57.049
the next step is to index data and
begin searching.

56
00:04:57.049 --> 00:04:57.890
Good luck.

1
00:00:00.025 --> 00:00:04.660
[SOUND] Hello, this is Chris Busheers,

2
00:00:04.660 --> 00:00:08.881
part of the Splunk education team.

3
00:00:08.881 --> 00:00:13.530
In this video, I'll show you how
install Splunk onto a Window's server.

4
00:00:13.530 --> 00:00:16.869
First we need to get the software
from the splunk.com download page.

5
00:00:18.010 --> 00:00:22.710
We will need to select that
the platform is 32 or 64-bit.

6
00:00:22.710 --> 00:00:25.605
If you aren't sure if your system is 32 or

7
00:00:25.605 --> 00:00:28.200
64-bit, you can check
your system properties.

8
00:00:29.320 --> 00:00:34.750
As you can see, this server is 64-bit,
so we can install that version.

9
00:00:34.750 --> 00:00:39.720
If we saw a system type of 32-bit,
we would download the 32-bit version.

10
00:00:40.720 --> 00:00:43.020
We run the installer by
double clicking on it.

11
00:00:44.220 --> 00:00:48.920
There is button to view the license
agreement, and a check box to accept it.

12
00:00:50.310 --> 00:00:53.800
At this point we can either install
Splunk with the defaults, or

13
00:00:53.800 --> 00:00:55.160
customize our installation.

14
00:00:56.290 --> 00:01:00.379
Let's click on the customize options
to see what settings can be selected.

15
00:01:01.660 --> 00:01:05.860
The first option is to change
the installation location of Splunk.

16
00:01:05.860 --> 00:01:09.940
We are fine with this location,
so we click Next.

17
00:01:09.940 --> 00:01:15.100
Now we must choose what account type
to install Splunk as, Local System or

18
00:01:15.100 --> 00:01:15.830
Domain Account.

19
00:01:17.070 --> 00:01:21.420
A Local System account will allow
Splunk to access all data on, or

20
00:01:21.420 --> 00:01:23.550
forwarded, to this machine.

21
00:01:23.550 --> 00:01:26.140
A Domain Account will allow
you to collect logs and

22
00:01:26.140 --> 00:01:31.070
metrics from remote machines as
well as local and forwarded data.

23
00:01:31.070 --> 00:01:35.010
You are required to provide a Domain
Account with the proper domain rights

24
00:01:35.010 --> 00:01:36.730
to use this type.

25
00:01:36.730 --> 00:01:40.530
Local System works well for
us, so we click Next.

26
00:01:40.530 --> 00:01:45.012
We can select to have a shortcut to Splunk
added, and click Install to continue.

27
00:01:45.012 --> 00:01:48.319
[MUSIC]

28
00:01:48.319 --> 00:01:52.750
Once installed, we can select to have
Splunk launch, and click Finish.

29
00:01:52.750 --> 00:01:55.720
This Splunk web interface
opens in our default browser.

30
00:01:56.750 --> 00:02:01.130
We enter the default user name of admin,
and a password of change made.

31
00:02:01.130 --> 00:02:04.400
A dialog box appears asking
us to change our password.

32
00:02:04.400 --> 00:02:07.310
It is always best practice to do this.

33
00:02:07.310 --> 00:02:10.820
Once logged in, we are taken to
the Splunk launcher homepage.

34
00:02:10.820 --> 00:02:13.780
And that's all it takes to get
Splunk installed on Windows.

35
00:02:13.780 --> 00:02:15.895
Now dig in, and start exploring.

36
00:02:15.895 --> 00:02:21.000
[SOUND]

1
00:00:03.530 --> 00:00:04.180
Our third and

2
00:00:04.180 --> 00:00:09.080
final case is applicable to most companies
that create customer-focused products.

3
00:00:11.030 --> 00:00:15.270
They want to understand how their
customers are responding to the products,

4
00:00:15.270 --> 00:00:17.800
how the product marketing
efforts are performing,

5
00:00:17.800 --> 00:00:21.470
what kind of problems customers
are encountering, and what new features or

6
00:00:21.470 --> 00:00:24.330
feature improvements the customers
are seeking, and so forth.

7
00:00:25.380 --> 00:00:28.010
But how does the company
get this information?

8
00:00:28.010 --> 00:00:31.060
What kind of data sources
would carry this information?

9
00:00:31.060 --> 00:00:33.710
The figure show some of these sources.

10
00:00:33.710 --> 00:00:40.210
They are in focused user surveys,
emails sent by the customers, in blogs and

11
00:00:40.210 --> 00:00:46.350
product review forums, specialized
groups on social media and user forums.

12
00:00:46.350 --> 00:00:52.990
In short, they are on the Internet or
in material received through the Internet.

13
00:00:52.990 --> 00:00:54.250
Now, how many sources are there?

14
00:00:55.830 --> 00:00:57.200
Two.

15
00:00:57.200 --> 00:00:58.940
The number would vary.

16
00:00:58.940 --> 00:01:01.060
A new sites, a new postings, and

17
00:01:01.060 --> 00:01:02.950
new discussion threads
would come up all the time.

18
00:01:04.000 --> 00:01:07.870
In all of these,
the goal is to identify information that

19
00:01:07.870 --> 00:01:12.130
truly relates to the companies product,
its features and its utility.

20
00:01:14.470 --> 00:01:17.530
To cast this as a type
of big data problem,

21
00:01:17.530 --> 00:01:20.871
we look at a task that computer
scientists called Data Fusion.

22
00:01:22.690 --> 00:01:27.840
Consider a set of data sources, S,
as we mentioned on the last slide and

23
00:01:27.840 --> 00:01:29.320
a set of data items, D.

24
00:01:30.630 --> 00:01:35.420
A data item represents a particular
aspect of a real world entity

25
00:01:35.420 --> 00:01:37.290
which in our case is
a product of the company.

26
00:01:39.180 --> 00:01:44.860
For each data item, a source can, but
not necessarily will, provide a value.

27
00:01:44.860 --> 00:01:46.020
For example,

28
00:01:46.020 --> 00:01:51.450
the usability of an ergonomically
split keyboard can have a value good.

29
00:01:52.590 --> 00:01:57.500
The value can be atomic,
like good, or a set, or a list or

30
00:01:57.500 --> 00:01:58.910
sometimes embedded in the string.

31
00:02:00.370 --> 00:02:04.270
For example, the cursor sometimes
freezes when using the touchpad,

32
00:02:05.410 --> 00:02:09.300
is a string which has
a value about the touchpad.

33
00:02:11.400 --> 00:02:16.130
The goal of Data Fusion is to find
the values of Data Items from a source.

34
00:02:18.060 --> 00:02:23.160
In many cases, the system would find
a unique true value of an item.

35
00:02:23.160 --> 00:02:27.580
For example, the launch data of a product
in Europe should be the same true value

36
00:02:27.580 --> 00:02:29.530
regardless of the data
source one looks at.

37
00:02:30.700 --> 00:02:34.520
In other cases, we could find
a value distribution of an item.

38
00:02:34.520 --> 00:02:37.960
For example, the usability of our
keyboard may have a value distribution.

39
00:02:39.070 --> 00:02:43.960
That's with Data Fusion, we should be
able to collect the values of real world

40
00:02:43.960 --> 00:02:46.790
items from a subset of data sources.

41
00:02:46.790 --> 00:02:49.850
It is a subset because
not all Data Sources

42
00:02:49.850 --> 00:02:51.940
will have relevant information
about the Data Item.

43
00:02:53.440 --> 00:02:56.212
There are some other versions
of what a Data Fusion is but for

44
00:02:56.212 --> 00:02:58.992
our purposes we'll stick with
this general description.

45
00:03:01.190 --> 00:03:05.856
Now one obvious problem with the Internet
is that there are too many data

46
00:03:05.856 --> 00:03:09.530
sources at any time,
these lead to many difficulties.

47
00:03:10.910 --> 00:03:14.570
First, it is to be understood
that with too many data sources

48
00:03:14.570 --> 00:03:17.310
there will be many values for
the same item.

49
00:03:18.530 --> 00:03:21.190
Often these will differ and
sometimes they will conflict.

50
00:03:22.580 --> 00:03:25.740
A standard technique in this case
is to use a voting mechanism.

51
00:03:27.200 --> 00:03:31.820
However, even a voting
mechanism can be complex

52
00:03:31.820 --> 00:03:33.350
due to problems with the data source.

53
00:03:35.040 --> 00:03:38.800
One of the problems is to estimate
the trustworthiness of the source.

54
00:03:40.130 --> 00:03:42.080
For each data source,

55
00:03:42.080 --> 00:03:48.260
we need to evaluate whether it's reporting
some basic or known facts correctly.

56
00:03:48.260 --> 00:03:51.510
If a source mentions details
about a rainbow colored iPhone,

57
00:03:51.510 --> 00:03:55.010
which does not exist,
it's trustworthiness reduces

58
00:03:55.010 --> 00:03:57.960
because of the falsity of
the provided value of this data item.

59
00:03:59.270 --> 00:04:03.140
Accordingly, a higher vote count can be
assigned to a more trustworthy source.

60
00:04:04.590 --> 00:04:07.640
And then, this can be used in voting.

61
00:04:09.560 --> 00:04:11.260
The second aspect is Copy Detection.

62
00:04:12.700 --> 00:04:16.670
Detecting weather once was has copied
information from another can be very

63
00:04:16.670 --> 00:04:19.490
important for
detail fusion task in customer analytics.

64
00:04:20.660 --> 00:04:22.680
If a source has copied information,

65
00:04:23.750 --> 00:04:28.900
it's such that discounted vote count
can be assigned to a copy value and

66
00:04:28.900 --> 00:04:34.190
voting that means the copy in
source will have less weight.

67
00:04:35.210 --> 00:04:40.160
Now this is especially relevant when we
compute value distributions, because if we

68
00:04:40.160 --> 00:04:45.680
treat copies as genuine information, we
will statistically bias the distribution.

69
00:04:45.680 --> 00:04:50.279
Now here is active research on how to
detect copies, how to determine bias and

70
00:04:50.279 --> 00:04:54.685
then arrive at a statistically sound
estimation of value distribution.

71
00:04:54.685 --> 00:04:59.628
But to our knowledge, these methods are
yet to be applied to existing software for

72
00:04:59.628 --> 00:05:01.070
big data integration.

73
00:05:04.473 --> 00:05:06.240
It should be very clear by now but

74
00:05:06.240 --> 00:05:10.240
there are two kinds of big data
situations when it comes to information.

75
00:05:11.400 --> 00:05:16.180
The first two uses cases that we
saw requires an integration system

76
00:05:16.180 --> 00:05:20.380
to consider all sources because
the application demand so.

77
00:05:21.620 --> 00:05:27.660
In contrast, problems where data comes
from too many redundant, potentially

78
00:05:27.660 --> 00:05:32.380
unreliable sources like the Internet, the
best results can be obtained if we have

79
00:05:32.380 --> 00:05:36.490
a way of evaluating the worthiness of
sources before information integration.

80
00:05:37.660 --> 00:05:40.790
But this problem is
called Source Selection.

81
00:05:40.790 --> 00:05:44.570
The picture on the right shows the result
of a cost benefit analysis for

82
00:05:44.570 --> 00:05:46.080
data fusion.

83
00:05:46.080 --> 00:05:49.239
The x-axis indicates the number
of sources used, and

84
00:05:49.239 --> 00:05:53.400
the y-axis measures the proportion
of true results that were returned.

85
00:05:55.090 --> 00:05:59.480
We can clearly see that the plot peaks
around six-to-eight sources, and

86
00:05:59.480 --> 00:06:01.990
that the efficiency falls
as more sources are added.

87
00:06:03.790 --> 00:06:08.930
In a cost benefit analysis,
the cost must include both the human and

88
00:06:08.930 --> 00:06:10.580
the computational costs,

89
00:06:10.580 --> 00:06:14.270
while the benefit is a function of
the accuracy of the fusion result.

90
00:06:14.270 --> 00:06:19.210
The technique for
solving this problem comes from economics.

91
00:06:20.520 --> 00:06:23.870
Assuming that cost and
benefits are measure in the same unit, for

92
00:06:23.870 --> 00:06:24.660
example, dollars.

93
00:06:25.780 --> 00:06:28.580
They proposed to continue
selecting sources

94
00:06:28.580 --> 00:06:32.740
until the marginal benefit is
less than the marginal cost.

95
00:06:34.150 --> 00:06:38.210
Now recent techniques were performing
this computation at quite scalable.

96
00:06:38.210 --> 00:06:41.910
In one setting,
selecting the most beneficial sources

97
00:06:41.910 --> 00:06:45.520
from a total of one million
sources took less than one hour.

98
00:06:47.900 --> 00:06:51.980
This completes our coverage of
the big data integration problems.
