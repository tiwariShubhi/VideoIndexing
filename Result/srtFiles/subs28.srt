
1
00:00:00.008 --> 00:00:05.620
Now that Maya has explained
the basics of machine learning,

2
00:00:05.620 --> 00:00:10.660
let's remember how big data
influences analytical applications.

3
00:00:10.660 --> 00:00:15.020
And how we can take advantage of
the existing big data tools and

4
00:00:15.020 --> 00:00:17.490
techniques in machine learning.

5
00:00:17.490 --> 00:00:20.960
How can machine learning
algorithms be scaled up

6
00:00:20.960 --> 00:00:24.010
to process large volumes of data?

7
00:00:24.010 --> 00:00:25.120
Let's talk about that now.

8
00:00:27.100 --> 00:00:30.395
After this video, you will be able to

9
00:00:30.395 --> 00:00:34.665
explain how machine learning
techniques can scale up to big data.

10
00:00:35.685 --> 00:00:40.855
And discuss the role of distributed
computing platforms like Hadoop and

11
00:00:40.855 --> 00:00:43.485
Spark in applying machine
learning to big data.

12
00:00:44.855 --> 00:00:48.605
With the massive amounts of data
that need to be processed for

13
00:00:48.605 --> 00:00:52.775
applications, such as drug
effectiveness analysis,

14
00:00:52.775 --> 00:00:57.200
climate monitoring, and
website recommendations to name a few.

15
00:00:58.420 --> 00:01:03.570
We need to be able to add scalability
to machine learning techniques.

16
00:01:03.570 --> 00:01:05.600
How do we apply machine learning at scale?

17
00:01:06.790 --> 00:01:11.770
One way, is to scale up by
adding more memory, processors,

18
00:01:11.770 --> 00:01:17.400
and storage to our system so
that it can store and process more data.

19
00:01:17.400 --> 00:01:19.110
This is not the big data approach.

20
00:01:20.270 --> 00:01:25.182
Specialized hardware such as
graphical processing units, or

21
00:01:25.182 --> 00:01:29.536
GPUs for short,
can also be added to speed up the miracle

22
00:01:29.536 --> 00:01:33.730
operations common in machine
learning algorithms.

23
00:01:33.730 --> 00:01:37.300
Although this is a good approach,
this is also not the big data approach.

24
00:01:38.650 --> 00:01:44.510
As we learned in our introductory course,
one problem with this approach

25
00:01:44.510 --> 00:01:48.720
is that larger specialized
hardware can be very costly.

26
00:01:50.180 --> 00:01:55.590
Another problem is that we
eventually will hit a limit.

27
00:01:55.590 --> 00:01:58.830
There's only so
much hardware you can add to a machine.

28
00:01:59.990 --> 00:02:03.100
An alternative approach is to scale out.

29
00:02:04.860 --> 00:02:10.260
This means using many local commodity
distribution systems together.

30
00:02:11.310 --> 00:02:16.160
Data is distributed over these
systems to gain processing speed up.

31
00:02:18.030 --> 00:02:25.870
As shown in this illustration the idea is
to divide the data into smaller subsets.

32
00:02:25.870 --> 00:02:31.900
The same processing is applied to
each subset, or map, and the results

33
00:02:31.900 --> 00:02:36.660
are merged at the end to come up with the
overall results for the original dataset.

34
00:02:37.770 --> 00:02:43.700
Let's consider an example,
where we want to apply the same operation

35
00:02:43.700 --> 00:02:48.420
to all the samples in
a dataset of N samples.

36
00:02:48.420 --> 00:02:51.640
In this case, N is four.

37
00:02:51.640 --> 00:02:56.150
If it takes T time units to perform this

38
00:02:56.150 --> 00:03:01.460
operation on each sample,
then with sequential processing

39
00:03:01.460 --> 00:03:06.350
the time to apply that operation
to all samples, is N times T.

40
00:03:07.470 --> 00:03:10.582
If we have a cluster of four processors,

41
00:03:10.582 --> 00:03:14.890
we can distribute the data
across the four processors.

42
00:03:16.770 --> 00:03:23.730
Each process performs the operation on
the dataset subset of N over four samples.

43
00:03:25.230 --> 00:03:30.990
Processing of the four subsets
of the data is done in parallel.

44
00:03:30.990 --> 00:03:34.390
That is, the subsets
are processed at the same time.

45
00:03:36.010 --> 00:03:42.870
The processing time for the distributed
approach is approximately N over 4 times T

46
00:03:44.010 --> 00:03:50.700
plus any overhead required to merge
the subset results and maybe shuffle them.

47
00:03:52.100 --> 00:03:57.050
This is a speedup of nearly four
times over the sequential approach.

48
00:03:59.040 --> 00:04:03.021
On a distributed computing
platform such as Spark or Hadoop,

49
00:04:03.021 --> 00:04:08.610
scalable machine learning algorithms
use the same scale out approach.

50
00:04:08.610 --> 00:04:12.620
Data is distributed across
different processors,

51
00:04:12.620 --> 00:04:17.690
which operate on the data subsets
in parallel using map, reduce, and

52
00:04:17.690 --> 00:04:20.230
other distributed
parallel transformations.

53
00:04:21.480 --> 00:04:22.420
This allows for

54
00:04:22.420 --> 00:04:26.050
machine learning techniques to be
applied to large volumes of data.

55
00:04:27.230 --> 00:04:33.300
In this course, we will use Spark and
its scalable machine learning library,

56
00:04:33.300 --> 00:04:38.830
MLF, to show you how machine
learning can be applied to big data.

57
00:04:38.830 --> 00:04:39.640
And don't forget,

58
00:04:39.640 --> 00:04:44.890
this is the processing of the machine
learning on where the data resides.

59
00:04:44.890 --> 00:04:47.480
And we call this, the Big Data Approach.

60
00:04:48.770 --> 00:04:54.020
However, you can also imagine
a scenario where you also

61
00:04:54.020 --> 00:04:59.070
update the machine learning
algorithms to scale up.

62
00:04:59.070 --> 00:05:03.389
So you can paralyze the machine
learning algorithms themselves,

63
00:05:03.389 --> 00:05:07.644
and also use processing of big
data together with this approach.

1
00:00:00.910 --> 00:00:05.618
In this video, we will provide you with
a quick summary of the main points

2
00:00:05.618 --> 00:00:09.644
from our first three courses to
recall what you have learned.

3
00:00:09.644 --> 00:00:13.520
If you have just completed our third
course and do not need a refresher,

4
00:00:13.520 --> 00:00:15.470
you might skip to the next lecture.

5
00:00:18.140 --> 00:00:23.325
We started our first course explaining
how a new torrent of big data

6
00:00:23.325 --> 00:00:28.974
combined with cloud computing
capabilities to process data anytime and

7
00:00:28.974 --> 00:00:33.803
anywhere has been at the core of
the launch of the big data era.

8
00:00:33.803 --> 00:00:37.995
Such capabilities enable or
present opportunities for

9
00:00:37.995 --> 00:00:43.704
many dynamic data-driven applications,
including energy management,

10
00:00:43.704 --> 00:00:48.620
smart cities, precision medicine,
and smart manufacturing.

11
00:00:49.620 --> 00:00:54.163
These applications are increasingly
more data-driven, dynamic and

12
00:00:54.163 --> 00:00:58.470
heterogeneous in terms of
their technology needs.

13
00:00:58.470 --> 00:01:01.490
They're also more process-driven and

14
00:01:01.490 --> 00:01:06.130
need to be tackled using
a collaborative approach by a team that

15
00:01:06.130 --> 00:01:10.400
puts value on accountability and
reproducibility of the results.

16
00:01:11.840 --> 00:01:17.250
Overall, by modeling,
managing, integrating diverse

17
00:01:17.250 --> 00:01:22.260
data streams we add value
to our big data and

18
00:01:22.260 --> 00:01:26.030
improve our business even more
before we start analyzing it.

19
00:01:27.640 --> 00:01:29.080
A part of modeling and

20
00:01:29.080 --> 00:01:34.740
managing big data is focusing on
the dimensions of the scalability and

21
00:01:34.740 --> 00:01:39.080
considering the challenges associated with
these dimensions to pick the right tools.

22
00:01:40.990 --> 00:01:45.859
We also talked about characteristics
of big data, referring to some Vs

23
00:01:45.859 --> 00:01:51.360
like volume, variety, velocity,
veracity and valence.

24
00:01:52.500 --> 00:01:58.727
Each week presents a challenging
dimension of big data,

25
00:01:58.727 --> 00:02:06.010
namely size, complexity, speed,
quality and connectedness.

26
00:02:06.010 --> 00:02:08.530
We also added a sixth V,

27
00:02:08.530 --> 00:02:12.920
value, referring to the real reason
we are interested in big data.

28
00:02:14.550 --> 00:02:18.660
To turn it into an advantage in the
context of a problem using data science

29
00:02:18.660 --> 00:02:21.890
techniques, big data needs to be analyzed.

30
00:02:23.470 --> 00:02:30.530
We explained a five steps process for data
science that includes data acquisition,

31
00:02:30.530 --> 00:02:34.500
modeling, management,
integration, and analysis.

32
00:02:35.620 --> 00:02:38.370
The influence of big data pushes for

33
00:02:38.370 --> 00:02:42.710
alternative scalability approaches
at each step of the process.

34
00:02:44.820 --> 00:02:50.310
If we just focus on the scalability
challenges related to the three Vs,

35
00:02:50.310 --> 00:02:53.720
we can say big data has varying volume and

36
00:02:53.720 --> 00:02:59.020
velocity, requiring dynamic and
scalable batch and stream processing.

37
00:03:00.110 --> 00:03:05.715
Big data has variety, requiring management
of data in many different data systems,

38
00:03:05.715 --> 00:03:07.790
and integration of it at scale.

39
00:03:09.630 --> 00:03:12.470
In our introduction to
the big data course,

40
00:03:12.470 --> 00:03:16.590
we talked about the version of a layer
diagram for the tools in the Hadoop

41
00:03:16.590 --> 00:03:20.950
ecosystem, organized vertically
based on the interface.

42
00:03:22.520 --> 00:03:27.210
Low level interfaces for storage and
scheduling on the bottom

43
00:03:28.890 --> 00:03:32.610
and high level languages and
interactivity at the top.

44
00:03:33.820 --> 00:03:38.691
Most of the tools in the Hadoop ecosystem
were initially built to compliment

45
00:03:38.691 --> 00:03:43.880
the capabilities of Hadoop for distributed
file system management using HDFS.

46
00:03:45.060 --> 00:03:48.490
Data processing using
the MapReduce engine, and

47
00:03:48.490 --> 00:03:52.810
resource scheduling, and
negotiation using the YARN engine.

48
00:03:54.320 --> 00:03:57.580
Over time,
a number of new projects were built,

49
00:03:57.580 --> 00:04:03.010
either to add to these complementary
tools or to handle additional types of

50
00:04:03.010 --> 00:04:08.640
big data management and processing not
available in Hadoop, just like Spark.

51
00:04:10.260 --> 00:04:15.430
Arguably, the most important
change to Hadoop over time

52
00:04:15.430 --> 00:04:19.960
was the separation of YARN from
the MapReduce programming model

53
00:04:19.960 --> 00:04:22.590
to solely handle resource
management concerns.

54
00:04:24.060 --> 00:04:29.850
This allowed for Hadoop to be extensible
to different programming models and enable

55
00:04:29.850 --> 00:04:34.640
the development of a number of processing
engines for batch and stream processing.

56
00:04:36.140 --> 00:04:40.500
Another way to look at the vast number of
tools that have been added to the Hadoop

57
00:04:40.500 --> 00:04:44.420
ecosystem is from the point of
view of their functionality

58
00:04:44.420 --> 00:04:45.990
in the big data processing pipeline.

59
00:04:47.090 --> 00:04:52.680
Simply put, these are associated
with three distinct layers for

60
00:04:52.680 --> 00:04:57.660
data management and storage,
for data processing and

61
00:04:57.660 --> 00:05:00.780
for resource coordination and
workflow management.

62
00:05:02.360 --> 00:05:08.122
In our second course, we talked in detail
about the bottom layer in this diagram,

63
00:05:08.122 --> 00:05:10.810
namely data management and storage.

64
00:05:12.920 --> 00:05:18.890
While this layer includes Hadoop's HDFS,
there are a number of other systems

65
00:05:18.890 --> 00:05:25.680
that rely on HDFS as a file system or
implement their own no-SQL storage option.

66
00:05:25.680 --> 00:05:30.255
As big data can have a variety of
structured, semi-structured, and

67
00:05:30.255 --> 00:05:35.730
unstructured formats and
gets analyzed through a variety of tools,

68
00:05:35.730 --> 00:05:38.880
many tools were introduced to
fit this variety of needs.

69
00:05:40.210 --> 00:05:43.040
We call these big data management systems.

70
00:05:44.510 --> 00:05:48.690
We reviewed Redis and Aerospike as

71
00:05:48.690 --> 00:05:53.180
key value stores where each data item
is identified with a unique key.

72
00:05:55.670 --> 00:06:00.200
We also got some practical
experience with Lucene and

73
00:06:00.200 --> 00:06:04.842
Gephi as vector and
graph-stores respectively.

74
00:06:06.310 --> 00:06:10.660
We also talked about Vertica as
a column-store database where

75
00:06:10.660 --> 00:06:14.570
information is stored in
columns rather than rows.

76
00:06:16.660 --> 00:06:20.998
Cassandra and
HBase are also in this category.

77
00:06:20.998 --> 00:06:27.749
Finally, we introduced Solr and
Asterisk DB for managing unstructured and

78
00:06:27.749 --> 00:06:32.822
semi-structured text and
MongoDB as a document store.

79
00:06:35.591 --> 00:06:40.444
The processing layer is where
all these different types

80
00:06:40.444 --> 00:06:45.091
of data get retrieved,
integrated, and analyzed,

81
00:06:45.091 --> 00:06:49.343
which was the primary
focus of our third course.

82
00:06:49.343 --> 00:06:52.451
In the integration and processing layer,

83
00:06:52.451 --> 00:06:57.283
we roughly refer to the tools that
are built on top of HTFS and YARN,

84
00:06:57.283 --> 00:07:01.965
although some of them were with
other storage and file systems.

85
00:07:03.675 --> 00:07:10.064
YARN is a significant enable of many of
these tools making a number of batch and

86
00:07:10.064 --> 00:07:16.277
stream processing engines like Storm,
Spark, Flink and Beam possible.

87
00:07:16.277 --> 00:07:20.373
This layer also includes tools
like Hive and Spark SQL for

88
00:07:20.373 --> 00:07:24.894
bringing a query interface on top
of the storage layer, Pig for

89
00:07:24.894 --> 00:07:30.014
scripting simple big data pipelines
using the MapReduce framework and

90
00:07:30.014 --> 00:07:33.512
a number of specialized
analytical libraries,

91
00:07:33.512 --> 00:07:36.775
formation learning, and graph analytics.

92
00:07:36.775 --> 00:07:43.538
Giraph and GraphX of Spark are examples
of such libraries for graph processing.

93
00:07:43.538 --> 00:07:46.448
Mahout on top of the Hadoop stack and

94
00:07:46.448 --> 00:07:50.820
MLlib of Spark Are two options for
machine learning.

95
00:07:51.970 --> 00:07:55.665
Although we had a basic overview
of graph processing and

96
00:07:55.665 --> 00:07:59.991
machine learning for big data
analytics earlier in our second and

97
00:07:59.991 --> 00:08:03.860
third courses,
we haven't gone into the details there.

98
00:08:03.860 --> 00:08:09.576
In this course, we will use Spark's MLlib
as one of our two main tools,

99
00:08:09.576 --> 00:08:15.692
providing a deeper introduction to
the machine learning library of Spark.

100
00:08:15.692 --> 00:08:22.075
The third and top layer in our diagram is
the coordination and management layer.

101
00:08:22.075 --> 00:08:25.888
This is where integrations,
scheduling, coordination, and

102
00:08:25.888 --> 00:08:30.840
monitoring of applications across many
tools in the bottom two layers take place.

103
00:08:31.890 --> 00:08:36.940
This layer is also where the results of
the big data analysis get communicated

104
00:08:36.940 --> 00:08:42.030
to other programs, websites, visualization
tools and business intelligence tools.

105
00:08:43.740 --> 00:08:48.905
Workflow management systems help to
develop automated solutions that

106
00:08:48.905 --> 00:08:53.835
can manage and coordinate the process
of combining data management and

107
00:08:53.835 --> 00:09:00.165
analytical tasks in a big data pipeline as
a configurable, structured set of steps.

108
00:09:01.275 --> 00:09:04.275
Workflow driven thinking also matches

109
00:09:04.275 --> 00:09:08.810
this basic process of data science
that we overviewed before.

110
00:09:08.810 --> 00:09:12.510
Oozie is an example workflow
scheduler that can interact with

111
00:09:12.510 --> 00:09:15.620
many of the tools in the integration and
processing layer.

112
00:09:16.740 --> 00:09:21.364
Zookeeper is the resource
coordination tool which monitors and

113
00:09:21.364 --> 00:09:26.680
manages and coordinates all these
tools and named after animal.

114
00:09:28.510 --> 00:09:32.950
Now that we've reviewed all three
layers we are ready to come back to

115
00:09:32.950 --> 00:09:38.790
the integration and processing layer, but
now in the context of machine learning.

116
00:09:38.790 --> 00:09:42.060
In which we will use machine
learning techniques to

117
00:09:42.060 --> 00:09:46.840
apply to our five step data science
process and analyze big data.

118
00:09:48.570 --> 00:09:54.370
Just a simple Google search for
big data processing pipelines will bring

119
00:09:54.370 --> 00:09:58.950
a vast number of pipelines with
a large number of technologies

120
00:09:58.950 --> 00:10:03.520
that support scalable data cleaning,
preparation, and analysis.

121
00:10:05.080 --> 00:10:09.650
How do we make sense of all of it to
make sure we use the right tools for

122
00:10:09.650 --> 00:10:10.240
our application?

123
00:10:11.320 --> 00:10:14.580
How do we pick the right
pre-processing and

124
00:10:14.580 --> 00:10:17.780
machine learning techniques to
start doing predictive modeling?

125
00:10:19.450 --> 00:10:24.190
Over the next few weeks Dr.
Mei will walk you through some of the most

126
00:10:24.190 --> 00:10:29.515
fundamental machine learning techniques,
along with introductory hands

127
00:10:29.515 --> 00:10:35.102
on exercises we designed for you to ease
you into the world of machine learning.

128
00:10:35.102 --> 00:10:36.418
Let's get started.

1
00:00:01.340 --> 00:00:04.970
Let's go through an overview of the tools
we will be using in this course.

2
00:00:06.210 --> 00:00:10.890
After this video you will be
able to describe what KNIME is.

3
00:00:10.890 --> 00:00:16.450
Describe what Spark MLlib is, and contrast
KNIME and MLlib as machine learning tools.

4
00:00:18.240 --> 00:00:21.940
The machine learning tools that we will
be using in this course are KNIME and

5
00:00:21.940 --> 00:00:23.410
Spark MLlib.

6
00:00:23.410 --> 00:00:25.870
These are both open source tools.

7
00:00:25.870 --> 00:00:28.936
This lecture will introduce
these tools to you.

8
00:00:28.936 --> 00:00:31.780
You will need to use them for
the hands on activities in this course.

9
00:00:33.880 --> 00:00:39.390
KNIME Analytics is a platform for data
analytics, reporting, and visualization.

10
00:00:39.390 --> 00:00:44.650
The KNIME platform uses a graphical user
interface based approach with drag and

11
00:00:44.650 --> 00:00:48.890
drop features to facilitate
constructing an analytic solution.

12
00:00:48.890 --> 00:00:51.630
The basic components in KNIME
are referred to as nodes.

13
00:00:52.690 --> 00:00:55.770
Each node provides
a specific functionalities,

14
00:00:55.770 --> 00:00:57.880
such as reading in a file,

15
00:00:57.880 --> 00:01:01.900
creating a specific type of machine
running model and generating a plot.

16
00:01:03.080 --> 00:01:06.790
Nodes can be connected to create
machine running workflows or pipelines.

17
00:01:08.130 --> 00:01:11.510
KNIME stands for
Konstanz Information Miner.

18
00:01:11.510 --> 00:01:15.610
The Konstanz is for
the University of Konstanz in Germany.

19
00:01:15.610 --> 00:01:18.885
And note that the K in KNIME is
silent in the pronunciation.

20
00:01:21.187 --> 00:01:24.875
In KNIME you assemble the steps that
need to be performed in a machine

21
00:01:24.875 --> 00:01:28.190
learning process by connecting
nodes to create a workflow.

22
00:01:29.200 --> 00:01:33.070
To create a workflow the user
chooses the appropriate nodes

23
00:01:33.070 --> 00:01:37.750
from the node repository and
assembles them into a workflow.

24
00:01:37.750 --> 00:01:41.039
The workflow can then be
executed in the KNIME work bench.

25
00:01:43.200 --> 00:01:47.340
A node implements a specific
operation in a workflow.

26
00:01:47.340 --> 00:01:49.880
In this screenshot we see two nodes.

27
00:01:49.880 --> 00:01:53.380
The file reader node is used to
read data from a text file or

28
00:01:53.380 --> 00:01:55.820
a URL or a web address.

29
00:01:55.820 --> 00:01:59.370
The decision tree learner node
builds a decision tree model.

30
00:02:00.470 --> 00:02:06.226
Each node can have input and output ports
and can be connected to other nodes.

31
00:02:06.226 --> 00:02:09.535
When a node is executed,
it takes data from its input port,

32
00:02:09.535 --> 00:02:15.650
performs some operations on the data and
writes the results to the output port.

33
00:02:15.650 --> 00:02:18.950
Data is transferred between
nodes that are connected.

34
00:02:18.950 --> 00:02:23.130
A node can be configured by opening
up its configuration dialog.

35
00:02:23.130 --> 00:02:25.329
This is where the parameters for
the node can be set.

36
00:02:27.718 --> 00:02:31.878
The Node Repository is where you will find
all the nodes available in your KNIME

37
00:02:31.878 --> 00:02:33.730
installation.

38
00:02:33.730 --> 00:02:36.800
The nodes are organized by categories.

39
00:02:36.800 --> 00:02:41.260
KNIME provides an array of nodes to
perform operations for data access,

40
00:02:41.260 --> 00:02:45.610
data manipulation, analysis,
visualization, and reporting.

41
00:02:47.840 --> 00:02:52.691
As you can see, KNIME provides
a visual approach to machine learning.

42
00:02:52.691 --> 00:02:57.043
It's GUI-based, drag-and-drop approach
provides an easy way to create and

43
00:02:57.043 --> 00:02:59.160
execute a machine learning workflow.

44
00:03:00.320 --> 00:03:04.530
The open source version of KNIME however
is limited in how large of a dataset

45
00:03:04.530 --> 00:03:05.230
it can handle.

46
00:03:06.290 --> 00:03:10.100
There are commercial extensions to
KNIME to manage large datasets and

47
00:03:10.100 --> 00:03:14.390
offer other extra functionalities, but
these extensions are not open source.

48
00:03:16.150 --> 00:03:18.456
Now let's talk about Spark MLlib.

49
00:03:18.456 --> 00:03:21.407
You've worked with Spark before if
you took the third course in this

50
00:03:21.407 --> 00:03:22.710
specialization on big data.

51
00:03:23.980 --> 00:03:26.210
Spark is a distributed computing platform.

52
00:03:27.210 --> 00:03:30.900
MLlib is a scalable machine learning
library that runs on top of Spark.

53
00:03:32.070 --> 00:03:36.770
It provides distributed implementations of
commonly used machine learning algorithms

54
00:03:36.770 --> 00:03:37.469
and utilities.

55
00:03:38.470 --> 00:03:41.502
The ML and MLlib of course stands for
machine learning.

56
00:03:43.801 --> 00:03:49.120
To implement machine learning operations
in Spark MLlib, you need to write code.

57
00:03:49.120 --> 00:03:52.740
So MLlib is not a GUI-based approach.

58
00:03:52.740 --> 00:03:56.360
This segment of code reads and
parses data from a file,

59
00:03:56.360 --> 00:04:00.937
then builds a decision
tree classification model.

60
00:04:00.937 --> 00:04:05.480
MLlib, as with the base Spark core,
provides an application programming

61
00:04:05.480 --> 00:04:11.570
interface, or API, for
Java, Python, Scala and R.

62
00:04:11.570 --> 00:04:15.174
This means that you can write code in
these programming languages to execute

63
00:04:15.174 --> 00:04:17.098
the base operations provided in Spark.

64
00:04:19.082 --> 00:04:22.640
Spark MLlib runs on
a distributed platform.

65
00:04:22.640 --> 00:04:24.550
It provides machine
learning algorithms and

66
00:04:24.550 --> 00:04:28.560
techniques that are implemented
using distributed processing.

67
00:04:28.560 --> 00:04:32.470
So MLlib is used for processing and
analyzing large datasets.

68
00:04:33.500 --> 00:04:37.610
And as we have discussed, writing code is
required to implement operations in MLlib.

69
00:04:39.680 --> 00:04:44.576
In summary, KNIME is a GUI-based machine
learning tool, while Spark MLlib provides

70
00:04:44.576 --> 00:04:50.040
a programming-based scalable platform for
processing very large datasets.

71
00:04:50.040 --> 00:04:54.270
You will be using both Spark MLlib and
KNIME throughout this course.

72
00:04:54.270 --> 00:04:58.420
We have readings and hands-on exercises to
help you get familiar with these popular

73
00:04:58.420 --> 00:05:01.040
open source tools for machine learning.

74
00:05:01.040 --> 00:05:04.360
I think you will find it very informative
and fun to work with these tools.

1
00:00:01.335 --> 00:00:05.239
Welcome to course four of
the big data specialization.

2
00:00:05.239 --> 00:00:10.926
I'm Ilkay Altintas, for the new learners
I'm the Chief Data Science Officer

3
00:00:10.926 --> 00:00:17.059
at the San Diego Supercomputer Center at
the University of California, San Diego.

4
00:00:17.059 --> 00:00:21.317
I feel honored to teach you
the basics of big data modeling,

5
00:00:21.317 --> 00:00:25.170
management, and
analysis in this specialization.

6
00:00:25.170 --> 00:00:29.544
And to work with Dr.
Mai Nguyen on this class.

7
00:00:29.544 --> 00:00:33.270
>> And I'm Mai Nguyen, while most of
you might be familiar with Ilkay,

8
00:00:33.270 --> 00:00:34.915
I'm a new face.

9
00:00:34.915 --> 00:00:39.010
I'm excited to be here to teach you
what I love doing, machine learning.

10
00:00:39.010 --> 00:00:42.330
I received my PhD in computer science
with a focus on machine learning

11
00:00:42.330 --> 00:00:45.240
from the University of
California in San Diego.

12
00:00:45.240 --> 00:00:47.390
Since then,
I have worked as a data scientist,

13
00:00:47.390 --> 00:00:50.550
and instructor of machine
learning in various venues.

14
00:00:50.550 --> 00:00:53.580
I am the Lead for Data Analytics at SDFC.

15
00:00:53.580 --> 00:00:56.770
In this role, I work on data
science projects doing research

16
00:00:56.770 --> 00:00:59.480
on scalability on machine
learning methods to big data.

17
00:01:00.870 --> 00:01:05.970
>> We are really happy to have you in this
course to develop your understanding and

18
00:01:05.970 --> 00:01:08.060
skills in machine learning.

19
00:01:08.060 --> 00:01:11.060
>> And
give you an introductory level experience

20
00:01:11.060 --> 00:01:13.500
with application of machine
learning to big data.

21
00:01:14.650 --> 00:01:19.410
By now you might have just finished
our first three courses and

22
00:01:19.410 --> 00:01:26.250
learned the basics of big data modeling,
management, integration, and processing.

23
00:01:26.250 --> 00:01:28.990
If you haven't, it's not required.

24
00:01:28.990 --> 00:01:32.320
But for those with less background
in big data management and

25
00:01:32.320 --> 00:01:34.650
systems, you might find it valuable.

26
00:01:35.950 --> 00:01:38.590
>> We understand that you may
not even have heard anything on

27
00:01:38.590 --> 00:01:40.090
machine learning yet.

28
00:01:40.090 --> 00:01:44.230
That's why we will start by
discussing what machine learning is.

29
00:01:44.230 --> 00:01:48.330
Describing some sample applications and
presenting the typical process

30
00:01:48.330 --> 00:01:52.500
of a machine learning project to give
you a sense of what machine learning is.

31
00:01:52.500 --> 00:01:55.210
Then we will delve into some
commonly used machine learning

32
00:01:55.210 --> 00:01:57.310
techniques like classification and
clustering.

33
00:01:58.480 --> 00:02:03.630
>> We are also going to show you how
to explore your data, prepare it for

34
00:02:03.630 --> 00:02:08.900
analysis, and evaluate the results you
get with your machine learning model.

35
00:02:10.020 --> 00:02:14.440
These are all necessary steps for
a successful machine learning solution.

36
00:02:16.060 --> 00:02:18.980
>> As you know, for
many data science applications

37
00:02:18.980 --> 00:02:23.210
one has to use many different tools and
methods to analyze data.

38
00:02:23.210 --> 00:02:26.000
In fact, keeping up with the rapid
development of new tools is

39
00:02:26.000 --> 00:02:29.190
one of the challenges of
today's big data environment.

40
00:02:29.190 --> 00:02:32.870
In this course, we will introduce you to
two different types of machine learning

41
00:02:32.870 --> 00:02:35.470
tools namely Nime and Spark MNL.

42
00:02:36.520 --> 00:02:41.320
Nime is a graphical user interface based
tool that requires no programming.

43
00:02:41.320 --> 00:02:44.370
And as representative of
a set of tools used in visual

44
00:02:44.370 --> 00:02:46.810
workflow approach to machine learning.

45
00:02:46.810 --> 00:02:49.784
You will have hand on practice with
Nime as you go through the exercises in

46
00:02:49.784 --> 00:02:50.380
this course.

47
00:02:51.740 --> 00:02:56.260
We are also excited to show you
examples of data processing

48
00:02:56.260 --> 00:03:00.229
using Sparks Machine Learning library and
OwlWeb.

49
00:03:00.229 --> 00:03:02.770
Our goal here is to provide you

50
00:03:02.770 --> 00:03:07.840
with simple hands-on exercises that
require introductory level programming,

51
00:03:07.840 --> 00:03:11.930
to inspire you on how big data machine
learning tools can be operated.

52
00:03:13.230 --> 00:03:15.390
We wish you a fun time learning and

53
00:03:15.390 --> 00:03:20.420
hope to hear from you in the discussion
forums and learner stories as always.

54
00:03:21.530 --> 00:03:24.590
>> We have suggested time estimates
each week for the course.

55
00:03:24.590 --> 00:03:27.990
But feel free to take the course
at a faster or slower pace.

56
00:03:27.990 --> 00:03:30.920
And don't forget to connect to other
learners through the forums to

57
00:03:30.920 --> 00:03:33.000
enhance your learning experience.

58
00:03:33.000 --> 00:03:33.800
>> Happy learning.

59
00:03:33.800 --> 00:03:34.711
>> Happy learning.

1
00:00:00.790 --> 00:00:03.940
In the last lecture we
discussed data quality issues.

2
00:00:03.940 --> 00:00:08.340
We will now discuss some common techniques
for addressing those quality issues.

3
00:00:08.340 --> 00:00:13.600
After this video, you will be able
to define what imputation means,

4
00:00:13.600 --> 00:00:16.570
illustrate three ways to
handle missing values, and

5
00:00:16.570 --> 00:00:21.270
describe the role of domain knowledge
in addressing data quality issues.

6
00:00:21.270 --> 00:00:25.150
As we discussed in the last lecture,
real world data is messy.

7
00:00:25.150 --> 00:00:28.776
Some data quality issues that you can
find in your data are missing values,

8
00:00:28.776 --> 00:00:34.420
duplicate data, invalid data,
noise and outliers.

9
00:00:34.420 --> 00:00:37.840
You will need to clean your data if you
want to perform any meaningful analysis

10
00:00:37.840 --> 00:00:38.390
on that data.

11
00:00:39.960 --> 00:00:42.880
Recall that missing data occurs
when you don't have a value for

12
00:00:42.880 --> 00:00:45.380
certain variables in some samples.

13
00:00:45.380 --> 00:00:49.050
A simple way to handle missing data is
to simply drop any samples with missing

14
00:00:49.050 --> 00:00:50.220
values or NAs.

15
00:00:51.460 --> 00:00:54.320
All machine learning tools provide
a mechanism or command for

16
00:00:54.320 --> 00:00:57.200
filtering out rows with
any missing values.

17
00:00:57.200 --> 00:01:00.360
The advantage of this approach
is that it is very simple.

18
00:01:00.360 --> 00:01:04.770
The caveat is that you are removing
data when you filter out examples.

19
00:01:04.770 --> 00:01:08.510
If the number of samples dropped is large,
then you end up losing a lot of your data.

20
00:01:09.830 --> 00:01:12.970
An alternative to dropping
samples with missing data is to

21
00:01:12.970 --> 00:01:14.810
impute the missing values.

22
00:01:14.810 --> 00:01:18.720
Imputing means to replace the missing
values with some reasonable values.

23
00:01:19.900 --> 00:01:24.000
The advantage of this approach is that
you're making use of all your data.

24
00:01:24.000 --> 00:01:27.310
Oc course, imputing is more complicated
than simply dropping samples.

25
00:01:28.650 --> 00:01:31.000
There are several ways to
impute missing values.

26
00:01:31.000 --> 00:01:34.640
One strategy is to replace
the missing values with the mean or

27
00:01:34.640 --> 00:01:37.100
median value of the variable.

28
00:01:37.100 --> 00:01:42.490
For example, a missing value for years of
employment can be replaced by the mean or

29
00:01:42.490 --> 00:01:47.120
median value for years of employment for
all current employees.

30
00:01:47.120 --> 00:01:50.290
Another approach is to use
the most frequent value

31
00:01:50.290 --> 00:01:52.070
in place of the missing value.

32
00:01:52.070 --> 00:01:56.230
For example, the most frequently
recorded age of customers

33
00:01:56.230 --> 00:02:01.110
associated with the specific item can
be used if that value is missing.

34
00:02:01.110 --> 00:02:05.160
Alternatively, a sensible value can
be derived as a replacement for

35
00:02:05.160 --> 00:02:06.510
a missing value.

36
00:02:06.510 --> 00:02:09.980
For example, a missing value for
income can be set to zero for

37
00:02:09.980 --> 00:02:12.340
customers less then 18 years old, or

38
00:02:12.340 --> 00:02:17.120
it can be replaced with an average
value based on occupation and location.

39
00:02:17.120 --> 00:02:20.110
Note that this approach requires
knowledge about the application and

40
00:02:20.110 --> 00:02:24.130
the variable with missing values in
order to make reasonable choices

41
00:02:24.130 --> 00:02:27.140
about what valuables would be sensible
to replace the missing values.

42
00:02:28.660 --> 00:02:33.390
In the case of duplicate data one
approach is to delete the older record.

43
00:02:33.390 --> 00:02:36.500
Another approach is to
merge duplicate records.

44
00:02:36.500 --> 00:02:41.330
This often requires a way to determine
how to resolve conflicting values.

45
00:02:41.330 --> 00:02:45.720
For example, in the case of multiple
addresses for the same customer,

46
00:02:45.720 --> 00:02:50.360
some logic for determining similarities
between addresses might be necessary.

47
00:02:50.360 --> 00:02:53.130
For example,
St period is the same as Street.

48
00:02:54.740 --> 00:02:59.180
To address invalid data, consulting
another data source may be necessary.

49
00:02:59.180 --> 00:03:02.910
For example,
an invalid zip code can be corrected

50
00:03:02.910 --> 00:03:06.810
by looking up the correct zip
code based on city and state.

51
00:03:06.810 --> 00:03:11.200
A best estimate for a reasonable value
can also be used as a replacement.

52
00:03:11.200 --> 00:03:14.460
For example, for
a missing age value for an employee,

53
00:03:14.460 --> 00:03:18.980
a reasonable value can be estimated based
on the employee's length of employment.

54
00:03:20.850 --> 00:03:23.750
Noise that distorts the data
values can be addressed by

55
00:03:23.750 --> 00:03:26.010
filtering out the source of the noise.

56
00:03:26.010 --> 00:03:30.390
For example, filtering out the frequency
of a constant background noise

57
00:03:30.390 --> 00:03:33.680
will remove that noise
component from a recording.

58
00:03:33.680 --> 00:03:36.670
This filtering must be
done with care however,

59
00:03:36.670 --> 00:03:40.290
as it can also remove some components
of the true data in the process.

60
00:03:41.760 --> 00:03:45.005
Outliers can be detected through
the use of summary statistics and

61
00:03:45.005 --> 00:03:46.760
plots of the data.

62
00:03:46.760 --> 00:03:49.940
Outliers can significantly skew
the distribution of your data and

63
00:03:49.940 --> 00:03:52.430
thus the results of your analysis.

64
00:03:52.430 --> 00:03:55.240
In cases where outliers are not
the focus of your analysis,

65
00:03:55.240 --> 00:03:59.100
you will want to remove these
outlier samples from your data set.

66
00:03:59.100 --> 00:04:01.710
For example,
when a thermostat malfunctions and

67
00:04:01.710 --> 00:04:05.070
causes values to fluctuate wildly,
or to be much higher or

68
00:04:05.070 --> 00:04:08.760
lower than normal,
these samples should be filtered out.

69
00:04:08.760 --> 00:04:13.140
In some applications, however, outliers
are exactly what you're looking for.

70
00:04:13.140 --> 00:04:16.100
So when you detect outliers,
you don't want to throw them out.

71
00:04:16.100 --> 00:04:19.110
Instead, you want to
examine them more closely.

72
00:04:19.110 --> 00:04:23.610
A classic example of this is in fraud
detection, where outliers represent

73
00:04:23.610 --> 00:04:27.410
potential fraudulent use and
those samples should be analyzed closely.

74
00:04:28.700 --> 00:04:31.750
In order to address data
quality issues effectively

75
00:04:31.750 --> 00:04:34.640
knowledge about
the application is crucial.

76
00:04:34.640 --> 00:04:37.360
Things such as how the data was collected,

77
00:04:37.360 --> 00:04:42.980
the user population, the intended use
of the application etc, are important.

78
00:04:42.980 --> 00:04:47.060
This domain knowledge is essential
to making informed decisions on how

79
00:04:47.060 --> 00:04:50.936
to best impute missing values,
how to handle duplicate records and

80
00:04:50.936 --> 00:04:54.688
invalid data and what to do about
noise and outliers in your data.
