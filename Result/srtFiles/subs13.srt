
1
00:00:03.582 --> 00:00:06.800
The third component of
a data model is constraints.

2
00:00:08.080 --> 00:00:10.418
A constraint is a logical statement.

3
00:00:10.418 --> 00:00:15.770
That means one can compute and test
whether the statement is true or false.

4
00:00:16.980 --> 00:00:21.130
Constraints are part of the data model
because they can specify something about

5
00:00:21.130 --> 00:00:24.240
the semantics, that is,
the meaning of the data.

6
00:00:24.240 --> 00:00:27.705
For example,
the constraint that a week has seven and

7
00:00:27.705 --> 00:00:32.402
only seven days is something that a data
system would not know unless this

8
00:00:32.402 --> 00:00:36.025
knowledge is passed on to it
in the form of a constraint.

9
00:00:36.025 --> 00:00:39.531
Another constraint, shown here,

10
00:00:42.132 --> 00:00:46.690
Tells the system that the number of
titles for a movie is restricted to one.

11
00:00:49.740 --> 00:00:50.800
Different data models,

12
00:00:50.800 --> 00:00:54.260
as we'll see in the next module,
will have different kinds of constraints.

13
00:00:56.140 --> 00:00:58.760
There may be many different
kinds of constraints.

14
00:01:00.020 --> 00:01:04.039
A value constraint is a logical
statement about data values.

15
00:01:05.520 --> 00:01:09.600
On a previous slide we have said,
that the age, that is,

16
00:01:09.600 --> 00:01:13.540
the value of data elements representing
the age of an entity can not be negative.

17
00:01:16.310 --> 00:01:20.840
We also saw an example of
uniqueness constraint when we said

18
00:01:20.840 --> 00:01:23.880
every movie can have only one title.

19
00:01:25.200 --> 00:01:27.120
In the words of logic,

20
00:01:27.120 --> 00:01:32.460
there should exist no data object that's
a movie and has more than one title.

21
00:01:33.860 --> 00:01:37.719
It's easy to see that enforcing
these constraints requires us

22
00:01:37.719 --> 00:01:41.446
to count the number of titles and
then verify that it's one.

23
00:01:41.446 --> 00:01:46.490
Now, one can generalize this to count
the number of values associated with

24
00:01:46.490 --> 00:01:51.310
each object and check whether it lies
between an upper and lower bound.

25
00:01:52.380 --> 00:01:56.010
This is often called a cardinality
constraint of data property.

26
00:01:59.530 --> 00:02:04.388
In a medical example here,
the constraint has a lower limit of 0 and

27
00:02:04.388 --> 00:02:05.931
an upper limit of 3.

28
00:02:10.390 --> 00:02:15.253
A different kind of value constraint can
be enforced by restricting the type of

29
00:02:15.253 --> 00:02:17.030
the data allowed in a field.

30
00:02:18.620 --> 00:02:23.210
If we do not have such a constraint we
can put any type of data in the field.

31
00:02:23.210 --> 00:02:28.300
For example, you can have -99 as
the value of the last name of a person,

32
00:02:28.300 --> 00:02:30.680
of course that would be wrong.

33
00:02:30.680 --> 00:02:34.750
To ensure that this does not happen,
we can enforce the type of the last name

34
00:02:35.780 --> 00:02:40.300
to be a non-numeric alphabetic string.

35
00:02:40.300 --> 00:02:45.420
This example shows a logical
expression for this constraint.

36
00:02:45.420 --> 00:02:50.840
A type constraint is a special
kind of domain constraint.

37
00:02:50.840 --> 00:02:52.420
The domain of a data property or

38
00:02:52.420 --> 00:02:57.900
attribute is the possible set of values
that are allowed for that attribute.

39
00:02:57.900 --> 00:03:01.026
For example, the possible values for

40
00:03:01.026 --> 00:03:05.624
the day part of the date field
can be between 1 and 31.

41
00:03:05.624 --> 00:03:09.746
While a month may have
the value between 1 and 12.

42
00:03:09.746 --> 00:03:16.200
Or, alternately, a value from the set
January, February, ect till December.

43
00:03:17.870 --> 00:03:21.697
Now, one can devise a more complex
constraint, where the value of the date

44
00:03:21.697 --> 00:03:26.099
for April, June, and September and
November, are restricted between 1 and 30.

45
00:03:26.099 --> 00:03:30.609
And if you think about it,
all three constraints that we have

46
00:03:30.609 --> 00:03:35.420
described in the last slide
are value constraints.

47
00:03:35.420 --> 00:03:39.380
So they only state how to restrict
the values of some data property.

48
00:03:40.470 --> 00:03:45.392
In sharp contrast, structural properties
restrict the structure of the data.

49
00:03:45.392 --> 00:03:48.700
We'll choose a more complex example for
this.

50
00:03:50.320 --> 00:03:53.952
Suppose we are a matrix,
as shown in the example, and

51
00:03:53.952 --> 00:03:56.794
we've restricted to be a square matrix.

52
00:03:56.794 --> 00:04:00.919
So the number of columns is exactly
equal to the number of rows.

53
00:04:03.230 --> 00:04:06.570
We have not put any restriction
on the number of rows or columns.

54
00:04:06.570 --> 00:04:07.910
But just that they have to be the same.

55
00:04:09.280 --> 00:04:11.560
Now this constrains
the structure of the matrix and

56
00:04:11.560 --> 00:04:16.140
implies that the number of entries in
the structure will be a squared number.

57
00:04:17.480 --> 00:04:22.600
If we convert this matrix to a three
column table as shown, and impose

58
00:04:22.600 --> 00:04:28.000
the same squareness constraint, it will
translate to a more complex condition.

59
00:04:28.000 --> 00:04:31.540
That the number of data
rows will be the square of

60
00:04:31.540 --> 00:04:34.200
the number of unique values
in column one of the table.

61
00:04:35.900 --> 00:04:39.410
We'll encounter some more structural
constraints in the next module.

1
00:00:03.208 --> 00:00:07.770
The second component of a data model is
a set of operations that can be performed

2
00:00:07.770 --> 00:00:09.390
on the data.

3
00:00:09.390 --> 00:00:10.250
And in this module,

4
00:00:10.250 --> 00:00:15.650
we'll discuss the operations without
considering the bigness aspect.

5
00:00:15.650 --> 00:00:17.110
In course three,

6
00:00:17.110 --> 00:00:20.710
we'll come back to the issue of performing
these operations when the data is large.

7
00:00:22.515 --> 00:00:25.830
Now,operations specified
the methods to manipulate the data.

8
00:00:26.840 --> 00:00:31.220
SInce different data models are typically
associated with different structures,

9
00:00:31.220 --> 00:00:34.210
the operations on them will be different.

10
00:00:34.210 --> 00:00:39.410
But some types of operations are usually
performed across all data models.

11
00:00:39.410 --> 00:00:41.030
We'll describe a few of them here.

12
00:00:43.320 --> 00:00:50.818
One common operation extract a part of
a collection based on the condition.

13
00:00:50.818 --> 00:00:54.422
In the example here,
we have a set of records and

14
00:00:54.422 --> 00:00:59.110
we're looking for a sub set that
satisfies the condition that

15
00:00:59.110 --> 00:01:03.548
the fifth field has a value
greater than 100,000.

16
00:01:03.548 --> 00:01:07.603
The only one record
satisfies this requirement.

17
00:01:07.603 --> 00:01:11.580
Note that we called this operation
subsetting rather loosely.

18
00:01:12.940 --> 00:01:17.210
Depending on the context,
it's also called selection or filtering.

19
00:01:20.410 --> 00:01:26.300
The next common operation is retrieving
a part of a structure that is specified.

20
00:01:26.300 --> 00:01:29.440
In this case,
we specify that we are interested

21
00:01:29.440 --> 00:01:33.130
in just the first two fields
of a collection off records.

22
00:01:34.880 --> 00:01:40.340
But this produces a new collection of
records which has only these fields.

23
00:01:41.350 --> 00:01:44.460
This operation like before has many names.

24
00:01:44.460 --> 00:01:46.910
The most dominant name is projection.

25
00:01:48.340 --> 00:01:52.204
In the next module, we'll see several
versions of this operation for

26
00:01:52.204 --> 00:01:53.588
different data models.

27
00:01:56.108 --> 00:02:00.176
The next two operations are about
combining two collections

28
00:02:00.176 --> 00:02:01.460
into a larger one.

29
00:02:02.540 --> 00:02:06.450
The term combine may be
interpreted in various ways.

30
00:02:06.450 --> 00:02:08.830
The most straightforward
of them is said union.

31
00:02:09.910 --> 00:02:12.500
The assumption behind the union operation

32
00:02:12.500 --> 00:02:15.930
is that the two collections
involved have the same structure.

33
00:02:15.930 --> 00:02:22.050
In other words, if one collection has
four fields and another has 14 fields, or

34
00:02:22.050 --> 00:02:27.230
if one has four fields on people and
the dates of birth, and the other has four

35
00:02:27.230 --> 00:02:31.350
things about countries and their capitols,
they cannot be combined through union.

36
00:02:33.650 --> 00:02:37.340
In the example here,
their two collections have three and

37
00:02:37.340 --> 00:02:42.620
two records, respectively, with one
record that's common between them.

38
00:02:42.620 --> 00:02:43.120
The green one.

39
00:02:45.100 --> 00:02:48.170
The result collection has four record,

40
00:02:48.170 --> 00:02:53.300
because duplicates are disallowed
because it's a set operation.

41
00:02:53.300 --> 00:02:58.110
There is indeed another version of
union where duplicates are allowed and

42
00:02:58.110 --> 00:03:00.540
will produce five records instead of four.

43
00:03:03.040 --> 00:03:08.020
The second kind of combining,
called a Join, can be done when

44
00:03:08.020 --> 00:03:12.460
the two collections have different data
content but have some common elements.

45
00:03:14.430 --> 00:03:16.290
In the example shown,

46
00:03:16.290 --> 00:03:19.920
the first field is the common element
between the two collections on the left.

47
00:03:21.040 --> 00:03:24.630
In this kind of data combination
there are two stages.

48
00:03:24.630 --> 00:03:30.380
First, for each data item think
of a record of collection one,

49
00:03:30.380 --> 00:03:33.490
one finds a set of matching
data items in collection two.

50
00:03:35.120 --> 00:03:38.960
Thus, the first records of
the two collections match

51
00:03:38.960 --> 00:03:40.060
based on the first field.

52
00:03:41.490 --> 00:03:43.750
In the second phase of the operation,

53
00:03:43.750 --> 00:03:46.710
all fields of the matching
record pairs are put together.

54
00:03:47.780 --> 00:03:51.170
In the first record of the result
collection shown on the right,

55
00:03:51.170 --> 00:03:54.890
one gets the first four fields
on the first collection and

56
00:03:54.890 --> 00:03:57.230
the remaining two fields
from the second collection.

57
00:03:58.500 --> 00:04:04.380
Now in this one example, we found one pair
of matching records from the collections.

58
00:04:04.380 --> 00:04:07.790
In general, one would find more
than one matching record pairs.

59
00:04:09.260 --> 00:04:12.415
As you can see,
this operation is more complex and

60
00:04:12.415 --> 00:04:16.863
can be very expensive when the size
of the true collections are large.

1
00:00:00.800 --> 00:00:06.950
In the big data world, we often hear the
term structured data, that is, data having

2
00:00:06.950 --> 00:00:12.490
a structure which is quite different
from the so-called unstructured data.

3
00:00:12.490 --> 00:00:13.500
But what is a structure?

4
00:00:14.910 --> 00:00:16.040
Let's consider file 1.

5
00:00:17.310 --> 00:00:21.960
It's a typical CSV file that has
three lines with different content.

6
00:00:21.960 --> 00:00:26.260
But the file content is uniform
in the sense that each line,

7
00:00:26.260 --> 00:00:30.400
call it a record,
has exactly three fields,

8
00:00:30.400 --> 00:00:33.360
which we sometimes call data properties or
attributes.

9
00:00:34.720 --> 00:00:41.420
Further, the first two of these fields
are strings and the third one is a date.

10
00:00:41.420 --> 00:00:44.770
We can add more records, that's
lines with the same pattern of data,

11
00:00:44.770 --> 00:00:46.210
to the file in the same fashion.

12
00:00:47.240 --> 00:00:52.430
The content will grow, but the pattern of
data organization will remain identical.

13
00:00:53.440 --> 00:00:58.600
This repeatable pattern of data
organization makes the file structured.

14
00:00:58.600 --> 00:01:02.870
Now let's look at file 2,
which is four records of five fields each,

15
00:01:03.910 --> 00:01:07.680
except that the third record seems
to be missing the last entry.

16
00:01:08.710 --> 00:01:09.660
Is this file structured?

17
00:01:10.810 --> 00:01:15.383
We argue that it is, because the missing
value makes the third record incomplete,

18
00:01:15.383 --> 00:01:20.310
but it does not break the structure or
the pattern of the data organization.

19
00:01:20.310 --> 00:01:23.088
Let's looks at these
two files side by side.

20
00:01:23.088 --> 00:01:29.320
Clearly file 2 has more fields, and hence
is sort of wider than the first file.

21
00:01:29.320 --> 00:01:31.980
Would you say that they
have the same structure?

22
00:01:31.980 --> 00:01:33.990
Well, on the face of it they don't.

23
00:01:33.990 --> 00:01:35.740
But if you think more broadly,

24
00:01:35.740 --> 00:01:39.260
you would notice that they
are both collection of k fields.

25
00:01:40.330 --> 00:01:44.770
The size of the collection,
respectively three and four, differs.

26
00:01:44.770 --> 00:01:48.320
And k is 3 in the first case and
5 in the second.

27
00:01:48.320 --> 00:01:52.330
But we can think of 3 and 5 as parameters.

28
00:01:52.330 --> 00:01:57.280
In that case, we will say that these
files have been generated by a similar

29
00:01:57.280 --> 00:02:02.160
organizational structure, and
hence they have the same data model.

30
00:02:03.440 --> 00:02:06.190
Now in contrast, consider this file.

31
00:02:06.190 --> 00:02:10.480
Just looking at it, it's impossible to
figure out how the data is organized and

32
00:02:10.480 --> 00:02:12.420
how to identify subparts of the data.

33
00:02:13.440 --> 00:02:15.850
We would call this data unstructured.

34
00:02:16.860 --> 00:02:21.605
Often, compressed data like JPEG images,
MP3 audio files,

35
00:02:21.605 --> 00:02:26.450
MPEG3 video files, encrypted data,
are usually unstructured.

36
00:02:27.490 --> 00:02:32.098
In module two, we'll elaborate on data
models that are not fully structured or

37
00:02:32.098 --> 00:02:33.900
are structured differently.

1
00:00:00.890 --> 00:00:03.790
This is the first of two
hands-on exercises involving

2
00:00:03.790 --> 00:00:06.170
sensor data from a weather station.

3
00:00:06.170 --> 00:00:09.530
In this one, we will look at
static data in a text file.

4
00:00:09.530 --> 00:00:12.900
The next one we will look at live data
streaming from the weather station

5
00:00:12.900 --> 00:00:13.490
in real time.

6
00:00:13.490 --> 00:00:17.820
In this exercise, we will begin
by opening a terminal window and

7
00:00:17.820 --> 00:00:21.330
changing into the directory
containing the station measurements.

8
00:00:21.330 --> 00:00:24.560
We will look at these measurements in
a text file and then look at the key for

9
00:00:24.560 --> 00:00:27.540
these measurements so
we can understand what the values mean.

10
00:00:27.540 --> 00:00:29.310
Finally, we will plot the measurements.

11
00:00:30.340 --> 00:00:30.840
Let's begin.

12
00:00:31.870 --> 00:00:35.023
First, we will open a terminal window by
clicking on the Terminal icon on the top

13
00:00:35.023 --> 00:00:35.692
of the toolbar.

14
00:00:38.179 --> 00:00:44.814
Next, we'll cd into the directory

15
00:00:44.814 --> 00:00:50.350
containing the sensor data.

16
00:00:51.380 --> 00:00:58.358
We'll run cd
Downloads/big-data-two/sensor.

17
00:01:00.680 --> 00:01:07.750
We can write ls to see
the contents of this directory.

18
00:01:12.274 --> 00:01:18.554
The data from the weather station is
in a text file called wx-data.txt.

19
00:01:21.370 --> 00:01:26.550
We can run more wx-data.txt to
see the contents of this file.

20
00:01:34.970 --> 00:01:37.990
Each line of this file is
a separate set of measurements.

21
00:01:37.990 --> 00:01:42.390
There are two columns in this file,
the first column is a time stamp and

22
00:01:42.390 --> 00:01:45.160
it's separated by
a second column by a tab.

23
00:01:45.160 --> 00:01:51.710
The second column itself has separate
columns and these are separated by commas.

24
00:01:51.710 --> 00:01:56.790
The time stamp is the number
of seconds since 1970.

25
00:01:56.790 --> 00:01:59.450
You'll notice that it increases for
each time stamp.

26
00:02:01.830 --> 00:02:04.520
You'll notice that it increases for
each measurement.

27
00:02:04.520 --> 00:02:06.550
But sometimes measurements
come in at the same time.

28
00:02:06.550 --> 00:02:10.028
For example this one at 006.

29
00:02:10.028 --> 00:02:15.053
The measurements,
we see that the prefix is 0R1 for

30
00:02:15.053 --> 00:02:18.188
most of them but some have 0R2.

31
00:02:18.188 --> 00:02:21.060
If we look at the other measurements,

32
00:02:21.060 --> 00:02:26.646
we see that all the 0R1 measurements
start with Dn, Dm, Dx, and so on.

33
00:02:26.646 --> 00:02:31.670
Whereas R2 begins with Ta, Ua, and Pa.

34
00:02:34.310 --> 00:02:37.419
If we scroll down in the text
file by hitting the space bar,

35
00:02:37.419 --> 00:02:40.728
we'll see there are other
measurements besides R1 and R2.

36
00:02:43.860 --> 00:02:49.360
For example, there's R5 that has Th,
Vh, Vs and so on.

37
00:02:49.360 --> 00:02:53.830
And there's R0 which has
all the measurements.

38
00:02:53.830 --> 00:02:57.510
So Dn, Dm, Dx, Ta, Ua, Pa.

39
00:03:00.550 --> 00:03:01.930
And the remaining ones.

40
00:03:04.160 --> 00:03:07.434
Next we'll open another internal
window and look at the key to this

41
00:03:07.434 --> 00:03:10.598
measurements click on the tool
bar to open the terminal window.

42
00:03:10.598 --> 00:03:15.900
cd into downloads big data two sensor

43
00:03:23.120 --> 00:03:31.131
And the key to these measurements is
in a file called wxt520format.txt.

44
00:03:31.131 --> 00:03:36.290
We can run more wxt520format.txt
to see this file.

45
00:03:44.190 --> 00:03:47.116
This file says where each
of the prefix is mean, for

46
00:03:47.116 --> 00:03:49.307
example Sn is the wind speed minimum.

47
00:03:49.307 --> 00:03:56.430
Sm is the wind speed average.

48
00:03:56.430 --> 00:04:00.700
And Ta is the air temperature.

49
00:04:00.700 --> 00:04:04.530
So if we go back to our sensor file,
we see here Ta equals 13.9c.

50
00:04:04.530 --> 00:04:10.187
That means the air temperature at this

51
00:04:10.187 --> 00:04:15.690
time was 13.9 degrees celsius.

52
00:04:15.690 --> 00:04:21.022
We can also create a plot of
the data in this text file

53
00:04:21.022 --> 00:04:25.994
by running plot-data.py wx-data.txt Ta.

54
00:04:32.727 --> 00:04:37.529
This says to plot the data in
the wx-data file and the measure that we

55
00:04:37.529 --> 00:04:42.680
want to apply is Ta, which according
to our key, is the air temperature.

56
00:04:46.880 --> 00:04:51.520
When we run it, it displays a plot of the
air temperature found in the text file.

1
00:00:00.568 --> 00:00:02.240
In this hands-on exercise,

2
00:00:02.240 --> 00:00:05.870
we will be looking at an image file,
which uses an array data model.

3
00:00:07.050 --> 00:00:11.380
First, we will open a terminal window and
display an image file on the screen.

4
00:00:12.890 --> 00:00:16.630
Next, we will examine the structure
of the image, and finally,

5
00:00:16.630 --> 00:00:19.860
extract pixel values from
various locations in the image.

6
00:00:21.550 --> 00:00:22.660
Let's begin.

7
00:00:22.660 --> 00:00:25.954
First, we'll open a terminal window by
clicking on the terminal icon at the top

8
00:00:25.954 --> 00:00:26.650
of the toolbar.

9
00:00:33.101 --> 00:00:37.989
Next, we'll CDN2 the directory
containing the image,

10
00:00:37.989 --> 00:00:41.390
cdn2downloads/bigdata2/image.

11
00:00:48.410 --> 00:00:51.470
We can run ls to see the image
in different scripts.

12
00:00:54.050 --> 00:00:58.344
The file, Australia.jpg,
is an image that we want to view,

13
00:00:58.344 --> 00:01:00.090
we can use eog to view it.

14
00:01:00.090 --> 00:01:08.830
Run eog Australia.jpg Eog is
a common image viewer on Linux.

15
00:01:11.192 --> 00:01:15.870
Australia.jpg is a satellite image
of the Australian continent.

16
00:01:15.870 --> 00:01:17.560
Now let's look at the structure
of this image file.

17
00:01:21.320 --> 00:01:25.815
If we go back to our terminal window,
we can run the image

18
00:01:25.815 --> 00:01:30.611
viewer in the background by
hitting CTRL+Z, and then bg.

19
00:01:30.611 --> 00:01:35.319
We can view the dimensions or
structure of the array data model of this

20
00:01:35.319 --> 00:01:38.904
image by running
dimensions.py Australia.jpg.

21
00:01:43.740 --> 00:01:50.860
This says that the image has
5250 columns and 4320 rows.

22
00:01:50.860 --> 00:01:53.240
So it is a two-dimensional image.

23
00:01:53.240 --> 00:01:55.671
Additionally, each cell or

24
00:01:55.671 --> 00:02:01.470
pixel within this image is
composed of three 8-bit pixels.

25
00:02:01.470 --> 00:02:06.130
These pixels are composed of three
elements, red, green, and blue.

26
00:02:06.130 --> 00:02:09.410
We can extract or
view the individual pixel elements

27
00:02:09.410 --> 00:02:13.110
at a specific location in the image
by using the pixel.py script.

28
00:02:14.440 --> 00:02:21.240
We can run pixel.py Australia.jpg 0
0 to see the value at one location.

29
00:02:29.430 --> 00:02:33.430
The 0 0 location is
the corner of the image.

30
00:02:33.430 --> 00:02:39.281
If we go back to the image, the corners
are all the ocean, so they're dark blue.

31
00:02:39.281 --> 00:02:43.995
If we look at the value that was
extracted, we see that blue has a high

32
00:02:43.995 --> 00:02:48.490
value of 50, whereas red and
green are low with 11 and 10.

33
00:02:48.490 --> 00:02:51.991
If we view it at another corner,
by looking at 5000 0,

34
00:02:51.991 --> 00:02:53.610
we'll see the same value.

35
00:03:01.136 --> 00:03:05.679
If we go back to the image, the middle
of the image, which is the land,

36
00:03:05.679 --> 00:03:07.070
is orange or yellow.

37
00:03:07.070 --> 00:03:12.261
It's definitely not blue, so
let's take a look at a pixel value there.

38
00:03:12.261 --> 00:03:17.010
Okay, run pixel.py
Australia.jpg 2000 2000.

39
00:03:23.750 --> 00:03:31.070
This says that the red has a value of 118,
green is 89, and the blue is 57.

40
00:03:31.070 --> 00:03:34.130
So the red and green are higher than blue,
so it's not ocean.
