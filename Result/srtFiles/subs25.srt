
1
00:00:01.740 --> 00:00:04.460
Analytical operations
in big data pipelines.

2
00:00:05.940 --> 00:00:07.730
After this video,

3
00:00:07.730 --> 00:00:13.080
you will be able to list common analytical
operations within big data pipelines and

4
00:00:13.080 --> 00:00:16.650
describe sample applications for
these analytical operations.

5
00:00:18.550 --> 00:00:24.200
In this lesson, we will be
looking at analytical operations.

6
00:00:24.200 --> 00:00:28.810
These are operations used in analytics,
which is the process of

7
00:00:28.810 --> 00:00:33.370
transforming data into insights for
making more informed decisions.

8
00:00:34.690 --> 00:00:39.720
The purpose of analytical operations is to
analyze the data to discover meaningful

9
00:00:39.720 --> 00:00:44.089
trends and patterns, in order to gain
insights into the problem being studied.

10
00:00:45.138 --> 00:00:47.810
The knowledge gained from these insights

11
00:00:47.810 --> 00:00:51.890
ultimately lead to more informed
decisions driven by data.

12
00:00:54.150 --> 00:00:58.326
Here are some common analytical operations
that we will discuss in this lecture.

13
00:00:58.326 --> 00:01:02.704
Classification, clustering,

14
00:01:02.704 --> 00:01:06.380
path analysis and connectivity analysis.

15
00:01:07.630 --> 00:01:09.490
Let's start with classification.

16
00:01:10.730 --> 00:01:18.130
In classification, the goal is to predict
a categorical target from the input data.

17
00:01:18.130 --> 00:01:21.290
A categorical target is one
with discreet values or

18
00:01:21.290 --> 00:01:23.820
categories, instead of continuous values.

19
00:01:25.470 --> 00:01:28.460
For example, this diagram shows

20
00:01:28.460 --> 00:01:34.260
a classification task to determine the
risk associated with a loan application.

21
00:01:34.260 --> 00:01:37.801
The input consists of the loan amount,

22
00:01:37.801 --> 00:01:44.571
applicant information such as income,
age, debts, and a down payment.

23
00:01:46.202 --> 00:01:50.903
From this input data,
the task is to determine whether

24
00:01:50.903 --> 00:01:54.898
the loan application is low risk or
high risk.

25
00:01:57.216 --> 00:02:00.852
There are many classification techniques
or algorithms that can be used for

26
00:02:00.852 --> 00:02:02.480
this problem.

27
00:02:02.480 --> 00:02:07.160
We will discuss a specific one, namely,
decision tree in the next slide.

28
00:02:09.330 --> 00:02:13.590
The decision tree algorithm is
one technique for classification.

29
00:02:13.590 --> 00:02:15.000
With this technique,

30
00:02:15.000 --> 00:02:20.040
decisions to perform the classification
task are modeled as a tree structure.

31
00:02:22.070 --> 00:02:26.770
For the loan risk assessment problem,
a simple decision tree is shown here,

32
00:02:27.960 --> 00:02:32.470
where the loan application is
classified as being either low risk, or

33
00:02:32.470 --> 00:02:34.930
high risk, based on the loan amount.

34
00:02:34.930 --> 00:02:37.750
The applicant's income,
and the applicant's age.

35
00:02:40.090 --> 00:02:44.110
The decision tree algorithm is implemented
in many machine learning tools.

36
00:02:45.310 --> 00:02:51.718
This diagram shows how to specify
decision tree from input data, KNIME.

37
00:02:51.718 --> 00:02:55.650
A graphical user-interface-based
machine learning platform.

38
00:02:57.370 --> 00:03:02.400
Some examples of classification
are the prediction of whether cells from

39
00:03:02.400 --> 00:03:07.540
a tumor are benign or
malignant, categorization of

40
00:03:07.540 --> 00:03:12.630
handwritten digits as being zero,
one, two, etc, up to nine.

41
00:03:14.180 --> 00:03:18.760
And determining whether a credit card
transaction is legitimate or fraudulent,

42
00:03:20.940 --> 00:03:25.255
and classification of a loan application
as being low-risk, medium-risk or

43
00:03:25.255 --> 00:03:26.540
high-risk, as you've seen.

44
00:03:28.370 --> 00:03:31.970
Another common analytical
operation is cluster analysis.

45
00:03:33.180 --> 00:03:35.880
In cluster analysis, or clustering,

46
00:03:35.880 --> 00:03:40.320
the goal is to organize similar
items in to groups of association.

47
00:03:41.880 --> 00:03:47.045
This diagram shows an example of cluster
analysis in which customers are clustered

48
00:03:47.045 --> 00:03:51.510
into groups according to their
preferences of movie genre.

49
00:03:53.080 --> 00:03:56.450
So, customers who like Sci-Fi
movies are grouped together.

50
00:03:58.010 --> 00:04:01.814
Those who like drama movies
are grouped together,

51
00:04:01.814 --> 00:04:06.337
and customers who like horror
movies are grouped together.

52
00:04:06.337 --> 00:04:10.482
With this grouping, new movies,
as well as other products,

53
00:04:10.482 --> 00:04:15.184
such as books, can be offered to
the right type of costumers in order to

54
00:04:15.184 --> 00:04:18.073
generate interest and increase revenue.

55
00:04:20.775 --> 00:04:25.516
A simple and commonly used algorithm for
cluster analysis is k-means.

56
00:04:27.163 --> 00:04:31.670
With k-means,
samples are divided into k clusters.

57
00:04:31.670 --> 00:04:35.100
This clustering is done in order
to minimize the variance or

58
00:04:35.100 --> 00:04:38.620
similarity between samples
within the same cluster

59
00:04:38.620 --> 00:04:41.590
using some similarity
measures such as distance.

60
00:04:42.690 --> 00:04:47.210
In this example, k is equal to three, and

61
00:04:47.210 --> 00:04:53.170
k-means divides the original data shown
on the left into three clusters,

62
00:04:53.170 --> 00:04:57.620
shown as blue, green, and
red on the chart on the right.

63
00:04:59.700 --> 00:05:03.530
The k-means clustering algorithm is
implemented on many machine-learning

64
00:05:03.530 --> 00:05:04.180
platforms.

65
00:05:05.360 --> 00:05:08.550
The code here shows how to read in and

66
00:05:08.550 --> 00:05:13.190
parse input data, and
perform k-means clustering on the data.

67
00:05:13.190 --> 00:05:18.210
Other examples of cluster analysis
are grouping a company’s customer base

68
00:05:18.210 --> 00:05:24.560
into distinct segments for more effective
targeted marketing, finding articles or

69
00:05:24.560 --> 00:05:28.240
webpages with similar topics for
retrieving relevant information.

70
00:05:30.060 --> 00:05:35.844
Identification of areas in the city with
rates of particular types of crimes for

71
00:05:35.844 --> 00:05:40.077
effective management of law
enforcement resources, and

72
00:05:40.077 --> 00:05:46.055
determining different groups of weather
patterns such as rainy, cold or snowy.

73
00:05:48.190 --> 00:05:51.632
Classification and cluster analysis
are considered machine learning and

74
00:05:51.632 --> 00:05:53.620
analytical operations.

75
00:05:53.620 --> 00:05:56.790
There are also analytical
operations from graph analytics,

76
00:05:56.790 --> 00:06:01.790
which is the field of analytics where
the underlying data is structured as, or

77
00:06:01.790 --> 00:06:03.670
can be modeled as the set of graphs.

78
00:06:05.020 --> 00:06:09.190
One analytical operation using
graphs as path analysis,

79
00:06:09.190 --> 00:06:13.000
which analyzes sequences of nodes and
edges in a graph.

80
00:06:14.390 --> 00:06:17.500
A common application of path analysis

81
00:06:17.500 --> 00:06:21.980
is to find routes from one
location to another location.

82
00:06:21.980 --> 00:06:27.440
For example, you might want to find the
shortest path from your home to your work.

83
00:06:27.440 --> 00:06:32.100
This path may be different depending on
conditions such as the day of the week,

84
00:06:32.100 --> 00:06:36.450
time of day, traffic congestion,
weather and etc.

85
00:06:38.390 --> 00:06:43.530
This code shows some operations for
path analysis on neo4j,

86
00:06:43.530 --> 00:06:48.033
which is a graph database system
using a query language called Cypher.

87
00:06:49.210 --> 00:06:54.440
The first operation finds the shortest
path between specific nodes in a graph.

88
00:06:55.610 --> 00:06:59.460
The second operation finds all
the shortest paths in a graph.

89
00:07:00.890 --> 00:07:04.770
Connectivity analysis of graphs
has to do with finding and

90
00:07:04.770 --> 00:07:07.970
tracking groups to determine
interactions between entities.

91
00:07:09.110 --> 00:07:13.330
Entities in highly interacting
groups are more connected

92
00:07:13.330 --> 00:07:17.680
to each other than to entities
of other groups in a graph.

93
00:07:18.820 --> 00:07:21.860
These groups are called communities, and

94
00:07:21.860 --> 00:07:26.280
are interesting to analyze as they
give insights into the degree and

95
00:07:26.280 --> 00:07:31.410
patterns of the interaction between
entities, and also between communities.

96
00:07:32.880 --> 00:07:39.570
Some applications of connectivity analysis
are to extract conversation threads.

97
00:07:39.570 --> 00:07:42.030
For example,
by looking at tweets and retweets.

98
00:07:43.250 --> 00:07:47.720
To find interacting groups, for example,
to determine which users are interacting

99
00:07:47.720 --> 00:07:53.900
with each other users, to find
influencers, for example, to understand

100
00:07:53.900 --> 00:07:58.720
who are the main users leading to
the conversation about a particular topic.

101
00:07:58.720 --> 00:08:01.550
Or, who do people pay attention to?

102
00:08:01.550 --> 00:08:05.020
This information can be used to
identify the fewest number of

103
00:08:05.020 --> 00:08:06.990
people with the greatest influence.

104
00:08:06.990 --> 00:08:11.830
For example, for political campaigns,
or marketing on social media.

105
00:08:13.360 --> 00:08:15.680
This code shows some operations for

106
00:08:15.680 --> 00:08:20.210
connectivity analysis on neo4j using
the query language, Cypher, again.

107
00:08:21.980 --> 00:08:25.660
The first operation finds the degree
of all the nodes in a graph,

108
00:08:25.660 --> 00:08:30.450
and the second creates a histogram
of degrees for all nodes in a graph

109
00:08:30.450 --> 00:08:35.990
to determine how connected a node in
a graph is, we need to look at its degree.

110
00:08:35.990 --> 00:08:39.860
The degree of a node is the number
of edges connected to the node.

111
00:08:41.220 --> 00:08:45.250
A degree histogram shows the distribution
of node degrees in the graph and

112
00:08:45.250 --> 00:08:50.330
is useful in comparing graphs and
identifying types of users, for

113
00:08:50.330 --> 00:08:55.410
example, those who follow, versus those
who are followed in social networks.

114
00:08:56.940 --> 00:09:01.970
To summarize and add to these techniques,
the decision tree algorithm for

115
00:09:01.970 --> 00:09:06.470
classification and k-means algorithm for
cluster analysis that we covered in this

116
00:09:06.470 --> 00:09:09.040
lecture are techniques
from machine learning.

117
00:09:10.210 --> 00:09:15.650
Machine learning is a field of analytics
focused on the study and construction of

118
00:09:15.650 --> 00:09:20.819
computer systems that can learn from data
without being explicitly programmed.

119
00:09:22.140 --> 00:09:25.540
Our course on machine learning in
this specialization will cover these

120
00:09:25.540 --> 00:09:29.230
algorithms in more detail,
along with other algorithms used for

121
00:09:29.230 --> 00:09:31.660
classification and cluster analysis.

122
00:09:31.660 --> 00:09:34.690
As well as algorithms for
other machine learning tasks,

123
00:09:34.690 --> 00:09:39.360
such as regression,
association analysis, and tools for

124
00:09:39.360 --> 00:09:42.480
implementing and
executing machine learning algorithms.

125
00:09:44.380 --> 00:09:49.130
As a summary of the Graph Analytics,
the Path Analytics technique for finding

126
00:09:49.130 --> 00:09:53.390
the shortest path and the connectivity
analysis technique for analyzing communities

127
00:09:53.390 --> 00:09:58.260
that we discussed earlier,
are techniques used in graph analytics.

128
00:09:58.260 --> 00:10:02.970
As explained earlier,
graph analytics is the field of analytics,

129
00:10:02.970 --> 00:10:07.520
where the underlying data is structured or
can be modeled as a set of graphs.

130
00:10:08.760 --> 00:10:13.200
Our graph analytics course in the
specialization will cover these and other

131
00:10:13.200 --> 00:10:18.174
graph techniques, and we'll also cover
tools and platforms for graph analytics.

132
00:10:18.174 --> 00:10:25.610
In summary, analytic operations
are used to discover meaningful

133
00:10:25.610 --> 00:10:31.115
patterns in the data in order to provide
insights into the problem being studied.

134
00:10:31.115 --> 00:10:36.635
We looked at some of the examples of
analytical operations for classification,

135
00:10:36.635 --> 00:10:41.355
cluster analysis, path analysis and
connectivity analysis in this lecture.

1
00:00:01.226 --> 00:00:02.488
In this hands-on activity,

2
00:00:02.488 --> 00:00:05.520
we'll be performing word count on
the complete works of Shakespeare.

3
00:00:07.110 --> 00:00:11.350
First, we will copy the Shakespeare
text into the Hadoop file system.

4
00:00:11.350 --> 00:00:14.847
Next, we will create a new
Jupyter Notebook, and

5
00:00:14.847 --> 00:00:17.937
read the Shakespeare
text into a Spark RDD.

6
00:00:17.937 --> 00:00:21.177
We will then perform WordCount
using map and reduce,

7
00:00:21.177 --> 00:00:24.352
and write the results to HDFS and
view the contents.

8
00:00:27.553 --> 00:00:29.301
Let's begin.

9
00:00:29.301 --> 00:00:33.300
In the intro to big data course,
we copy the Shakespeare text into HDFS.

10
00:00:33.300 --> 00:00:36.630
Let's see if it's still there.

11
00:00:36.630 --> 00:00:38.150
If not, we can copy it now.

12
00:00:38.150 --> 00:00:42.180
Click on the terminal icon
at the top of the toolbar.

13
00:00:43.950 --> 00:00:49.830
Now we can run hadoop fs- ls to see what's
in our hadoop filesystem directory.

14
00:00:51.900 --> 00:00:54.380
There are no files in HTFS,
so let's copy it.

15
00:00:54.380 --> 00:01:01.070
If you already have words.txt in your HTFS
directory, you can skip this next step.

16
00:01:02.450 --> 00:01:06.837
Cd into downloads,

17
00:01:06.837 --> 00:01:15.359
big-data-3/spark-wordcount.

18
00:01:15.359 --> 00:01:17.450
We can do ls to see the file.

19
00:01:18.960 --> 00:01:21.390
Let's copy this file to HTFS.

20
00:01:21.390 --> 00:01:28.221
We run Hadoop,
fs copy from local, words.txt.

21
00:01:33.314 --> 00:01:37.800
We can write Hadoop fs -ls again
to verify that the file is there.

22
00:01:41.290 --> 00:01:42.600
Now let's do work count in spark.

23
00:01:44.220 --> 00:01:47.380
We will do this in an iPython
notebook using Jupyter server.

24
00:01:49.070 --> 00:01:51.587
Look on the web browser icon,
the top of the toolbar.

25
00:01:55.112 --> 00:01:59.706
And go to the Jupyter server URL,
which is local host port 8889.

26
00:02:02.360 --> 00:02:04.789
Next, let's create a new iPython notebook

27
00:02:10.685 --> 00:02:16.855
The first step is to read the words.txt
files in HTFS into a spark RDD.

28
00:02:16.855 --> 00:02:18.572
We'll call the RDD, lines.

29
00:02:22.140 --> 00:02:27.128
We can read it using the spark context SC,
in calling the text file method.

30
00:02:32.609 --> 00:02:37.041
The argument is the URL of
the word set TXT file and HDFS.

31
00:02:47.270 --> 00:02:51.998
Let's run this We can

32
00:02:51.998 --> 00:02:56.920
view the contents of this RDD
by calling lines.take(5).

33
00:03:01.553 --> 00:03:05.010
The argument 5 says how many
lines to show of the RDD.

34
00:03:06.930 --> 00:03:11.112
Next, we'll transform this RDD
of lines into an RDD of words.

35
00:03:11.112 --> 00:03:15.755
We'll say, words = lines.flatmap,

36
00:03:18.940 --> 00:03:20.852
lambda line:

37
00:03:24.123 --> 00:03:28.568
line.split Double quote
space double quote.

38
00:03:31.374 --> 00:03:38.852
This creates a new RDD called, words,
by running flatMap over the line RDD.

39
00:03:38.852 --> 00:03:41.092
The argument is this lambda expression.

40
00:03:43.729 --> 00:03:47.310
A lambda in Python is a simple way
to declare a one line expression.

41
00:03:48.860 --> 00:03:51.890
In this case,
there's one argument called line and

42
00:03:51.890 --> 00:03:55.160
we called it split method on this line and
we split on spaces.

43
00:03:57.080 --> 00:03:59.720
We can run this and
look at the contents of words.

44
00:04:09.720 --> 00:04:12.500
We can see that each element
now is an individual word.

45
00:04:14.769 --> 00:04:17.136
Next, we'll create tuples of these words.

46
00:04:18.907 --> 00:04:23.138
We'll put them in a new RDD called tuples.

47
00:04:23.138 --> 00:04:26.405
Enter, tuples

48
00:04:26.405 --> 00:04:31.433
= words.map:lambda

49
00:04:31.433 --> 00:04:36.465
word; (word, 1).

50
00:04:41.144 --> 00:04:45.080
This creates the tuples
by transforming words.

51
00:04:45.080 --> 00:04:47.370
This uses map and another lambda function.

52
00:04:47.370 --> 00:04:53.400
In this case, the lambda takes
one argument and returns a tuple.

53
00:04:53.400 --> 00:04:55.350
Where the first value of
the tuple is the word.

54
00:04:56.500 --> 00:04:58.320
The second value, is the number 1.

55
00:04:58.320 --> 00:05:04.460
Not that in this case, we use map,
whereas before, we used flat map.

56
00:05:06.490 --> 00:05:09.930
In this case, we want a tuple for
every word in the words.

57
00:05:09.930 --> 00:05:13.690
So we have a one to one mapping
between inputs and outputs.

58
00:05:13.690 --> 00:05:19.064
Previously, while we were splitting lines
into word, each line had multiple words.

59
00:05:21.677 --> 00:05:22.348
In general,

60
00:05:22.348 --> 00:05:26.950
you want to use map when you have a one to
one mapping between inputs and outputs.

61
00:05:26.950 --> 00:05:31.804
In flatMap you have a one to many or
none mapping between inputs and

62
00:05:31.804 --> 00:05:37.607
outputs Let's run this and look at tuples.

63
00:05:45.043 --> 00:05:50.538
We can see that each word now has
a tuple initialized with the count of 1.

64
00:05:50.538 --> 00:05:55.080
We can now count all the words by
combining or reducing these tuples.

65
00:05:55.080 --> 00:05:57.190
We'll put this in a new RDD called counts.

66
00:05:58.660 --> 00:06:03.039
So we'll say counts equals
tuples.reduce by key.

67
00:06:07.602 --> 00:06:11.955
Lambda a,b:.

68
00:06:11.955 --> 00:06:13.125
a + b.

69
00:06:16.313 --> 00:06:20.038
In this case, the lambda function
takes two arguments, a and be, and

70
00:06:20.038 --> 00:06:22.199
will return the result of adding a and b.

71
00:06:24.876 --> 00:06:26.071
To view the result,

72
00:06:33.657 --> 00:06:37.815
You can see now that the counts for
each words have been created.

73
00:06:37.815 --> 00:06:40.839
We can write this result back to HDFS.

74
00:06:40.839 --> 00:06:50.239
Let's say
counts.coalesce(1).saveAsTextFile,

75
00:06:50.239 --> 00:06:53.120
and then the URL.

76
00:07:04.966 --> 00:07:09.675
The coalesce means we only
want a single output file.

77
00:07:09.675 --> 00:07:12.172
Let's go back to our shell and
view the results.

78
00:07:15.582 --> 00:07:18.930
We'll run hadoop fs -ls
to see the directory.

79
00:07:21.809 --> 00:07:24.553
And run it again to look inside
the wordcount directory.

80
00:07:29.294 --> 00:07:32.602
And once more,
to look inside wordcount/outputDir.

81
00:07:38.808 --> 00:07:45.420
As you recall,
the output from hadoop jobs is part-0000.

82
00:07:45.420 --> 00:07:48.837
This is also true for spark jobs.

83
00:07:48.837 --> 00:07:51.484
Let's copy this file to
the local file system.

84
00:07:51.484 --> 00:07:55.899
We'll run hadoop fs CopyToLocal

85
00:07:55.899 --> 00:08:02.212
wordcount/outputDir/part-00000.

86
00:08:09.530 --> 00:08:11.066
You can view the results with more.

1
00:00:01.550 --> 00:00:02.530
In this hands on activity,

2
00:00:02.530 --> 00:00:05.190
we will be using Spark Streaming
to read weather data.

3
00:00:06.900 --> 00:00:09.470
First, we open
the Spark Streaming Jupyter Notebook.

4
00:00:10.610 --> 00:00:13.680
Next, we will look at sensor format and
measurement types.

5
00:00:14.695 --> 00:00:19.110
We'll then create a Spark DStream of
weather data, read the measurements, and

6
00:00:19.110 --> 00:00:20.630
create a sliding window of the data.

7
00:00:20.630 --> 00:00:26.180
We will define a function to display the
maximum and minimum values in the window.

8
00:00:26.180 --> 00:00:28.770
We start to stream processing
to give their results.

9
00:00:31.350 --> 00:00:35.610
Before we begin this activity, we need
to change the virtual box settings for

10
00:00:35.610 --> 00:00:37.100
our carder virtual machine.

11
00:00:39.110 --> 00:00:42.330
Start streaming needs more
than one thread of execution.

12
00:00:42.330 --> 00:00:45.620
So we need to change the settings to
add more than one virtual processor.

13
00:00:47.180 --> 00:00:52.930
First, shut down your cloudera virtual
machine and go to the virtual box manager.

14
00:00:54.960 --> 00:00:58.300
Select the cloudera virtual box and
click on settings.

15
00:01:00.720 --> 00:01:05.040
Next, click on system, click on Processor.

16
00:01:07.520 --> 00:01:14.444
And change the number of
CPU's to be two or more.

17
00:01:14.444 --> 00:01:19.910
When you're done, click okay,
and start the machine as usual.

18
00:01:23.740 --> 00:01:24.760
Let's begin.

19
00:01:24.760 --> 00:01:27.280
First, click on the browser icon
at the top of the tool bar.

20
00:01:29.370 --> 00:01:35.560
Navigate to the Jupyter Notebook server,
monitoring local host calling 8889.

21
00:01:35.560 --> 00:01:39.778
We'll then go in to downloads.

22
00:01:39.778 --> 00:01:43.337
Big data 3.

23
00:01:43.337 --> 00:01:46.186
Spark-streaming.

24
00:01:46.186 --> 00:01:48.616
Let's then open Spark-Streaming notebook.

25
00:01:51.346 --> 00:01:55.070
This first line, shows the example
data we get from the weather station.

26
00:01:56.680 --> 00:01:59.680
Each line has a time stamp and
a set of measurements.

27
00:02:01.860 --> 00:02:05.200
Each of these abbreviations is
a particular type of measurement,

28
00:02:05.200 --> 00:02:06.680
followed by the actual value.

29
00:02:09.140 --> 00:02:11.328
The next cell shows the key for
these measurements.

30
00:02:11.328 --> 00:02:16.190
For this hands-on, we are interested
in the average wind direction.

31
00:02:16.190 --> 00:02:18.059
Which is abbreviated as DM.

32
00:02:20.470 --> 00:02:24.178
This next cell, defines a function
that parses each line of text and

33
00:02:24.178 --> 00:02:26.080
pulls out the average wind speed.

34
00:02:27.610 --> 00:02:29.860
We define it here, so
we don't have to type it in later.

35
00:02:31.360 --> 00:02:32.627
Let's run this cell.

36
00:02:35.308 --> 00:02:38.790
Next, let's create
a streaming spark context.

37
00:02:38.790 --> 00:02:40.870
First, we'll need to import the module.

38
00:02:40.870 --> 00:02:46.590
We'll enter from pyspark.streaming
import StreamingContext.

39
00:02:46.590 --> 00:02:51.130
We can create a new streaming context.

40
00:02:51.130 --> 00:02:52.920
We'll put in in a variable called ssc.

41
00:02:54.400 --> 00:02:59.218
We'll enter ssc = StreamingContext(sc,1).

42
00:02:59.218 --> 00:03:02.710
The SC is a StreamingContext.

43
00:03:02.710 --> 00:03:07.070
The 1 specifies the batch interval,
1 second in this case.

44
00:03:07.070 --> 00:03:07.760
Let's run this.

45
00:03:10.040 --> 00:03:11.580
Next, we'll create a dstream.

46
00:03:13.100 --> 00:03:17.100
We'll import the streaming weather data,
over a TCP connection.

47
00:03:17.100 --> 00:03:19.099
We'll put this in a dstream called, Lines.

48
00:03:21.940 --> 00:03:26.249
Let's say lines = ssc.socketTextStream,

49
00:03:26.249 --> 00:03:31.661
we'll enter the host name in
port of the weather station,

50
00:03:31.661 --> 00:03:35.757
rtd.hpwren.ucsd.edu for 12028.

51
00:03:35.757 --> 00:03:36.784
Let's run this.

52
00:03:40.924 --> 00:03:46.260
Next, we'll create a new d-stream called
vals that would hold the measurements.

53
00:03:46.260 --> 00:03:52.390
We'll say vals = lines.flatMap parse.

54
00:03:52.390 --> 00:03:55.020
This calls the parse function,
we defined above for

55
00:03:55.020 --> 00:03:57.228
each of the lines coming
from the weather station.

56
00:03:57.228 --> 00:04:01.620
The resulting D-Stream will have just
the average wind direction values.

57
00:04:02.850 --> 00:04:03.570
We'll run this.

58
00:04:07.150 --> 00:04:11.340
Next, we'll create a window that
will aggregate the D-Stream values.

59
00:04:13.250 --> 00:04:17.470
We'll say, window = vals.window(10,5).

60
00:04:17.470 --> 00:04:22.850
The first argument specifies that the
length of the window should be 10 seconds.

61
00:04:22.850 --> 00:04:27.515
The second argument specifies that
the window should move every 5 seconds.

62
00:04:27.515 --> 00:04:29.900
Let's run this.

63
00:04:31.740 --> 00:04:34.480
Next, we'll define a function
that prints the minimum and

64
00:04:34.480 --> 00:04:36.240
maximum values that we see.

65
00:04:36.240 --> 00:04:37.710
We'll start by entering the definition.

66
00:04:39.440 --> 00:04:44.230
Def stats,
this will take an rdd as an argument.

67
00:04:47.330 --> 00:04:50.478
Next, let's print the entire
contents of the rdd.

68
00:04:50.478 --> 00:04:54.736
Print, parenthesis rdd.collect,

69
00:04:54.736 --> 00:04:59.932
this'll print the entire
content of the rdd.

70
00:04:59.932 --> 00:05:01.484
In a real big data application,

71
00:05:01.484 --> 00:05:04.020
this will be impractical due
to the size of the data.

72
00:05:05.020 --> 00:05:07.940
However, for this hands on,
the rdd is small, and so

73
00:05:07.940 --> 00:05:10.330
we can use this to see
the contents of the rdd.

74
00:05:12.720 --> 00:05:14.670
Next, we'll print the min and max.

75
00:05:15.940 --> 00:05:17.279
Before we do that however,

76
00:05:17.279 --> 00:05:21.303
we should check to make sure that
the size of the rdd is greater than zero.

77
00:05:21.303 --> 00:05:24.767
We'll check that rdd.count
is greater than 0.

78
00:05:29.285 --> 00:05:34.647
Finally, we'll print the MinID, MAX.

79
00:05:34.647 --> 00:05:39.826
We'll enter print (“max = {} min =

80
00:05:39.826 --> 00:05:44.853
{}”) Outside of the quote we'll do

81
00:05:44.853 --> 00:05:50.653
.format(rdd.max,rdd.min())).

82
00:05:50.653 --> 00:05:54.286
Let's run this, next,

83
00:05:54.286 --> 00:05:59.860
let's call this function stats.

84
00:05:59.860 --> 00:06:01.944
So all the rdds in our sliding window.

85
00:06:01.944 --> 00:06:08.366
I'll enter window.foreachRDD(stats).

86
00:06:08.366 --> 00:06:13.539
Run this.

87
00:06:13.539 --> 00:06:16.110
We're now ready to start
our streaming processing.

88
00:06:16.110 --> 00:06:19.880
We can do this by entering ssc.start.

89
00:06:19.880 --> 00:06:21.567
We'll run this to start the streaming.

90
00:06:29.214 --> 00:06:32.975
When we want to stop this streaming,
we'll run ssc.stop

91
00:06:38.028 --> 00:06:40.200
Please scroll up and
look at the beginning of the output.

92
00:06:43.130 --> 00:06:47.770
We'll see that it's printing the full
window and the min and max values.

93
00:06:50.010 --> 00:06:52.610
Notice that in the beginning,
the window is not yet filled.

94
00:06:52.610 --> 00:06:54.610
In this case, there's only three entries.

95
00:06:55.760 --> 00:06:59.029
We count to see that the window
is moving by five measurements.

96
00:07:00.220 --> 00:07:03.510
For example, the last five
measurements in the second window,

97
00:07:04.540 --> 00:07:06.870
are the first five measurements
in the third window.

1
00:00:00.770 --> 00:00:04.034
In this hands on activity we
will be using SparkSQL to

2
00:00:04.034 --> 00:00:06.119
query data from an SQL database.

3
00:00:07.604 --> 00:00:11.825
First we will open
the SparkSQL Jupyter Notebook.

4
00:00:11.825 --> 00:00:14.080
We will connect Spark to a Postgres table.

5
00:00:15.740 --> 00:00:20.214
And then view the Spark DataFrame
schema and count the rows.

6
00:00:20.214 --> 00:00:22.040
We will view the contents
of the data frame.

7
00:00:23.220 --> 00:00:25.110
See how to filter rows and columns.

8
00:00:26.130 --> 00:00:29.174
And finally perform aggregate
operation on a column.

9
00:00:33.636 --> 00:00:34.714
Let's begin.

10
00:00:34.714 --> 00:00:38.247
First, click on the browser icon,
the top of the toolbar.

11
00:00:38.247 --> 00:00:41.062
>> [SOUND]
>> Next,

12
00:00:41.062 --> 00:00:45.288
navigate to the Jupyter Notebook server.

13
00:00:45.288 --> 00:00:48.275
It's localhost:8889.

14
00:00:51.117 --> 00:00:54.358
Go to Downloads,

15
00:00:54.358 --> 00:00:58.900
Big Data 3, Spark SQL.

16
00:01:00.460 --> 00:01:02.040
To open the SparkSQL Notebook.

17
00:01:02.040 --> 00:01:07.710
The first three cells have
already been entered for you.

18
00:01:09.760 --> 00:01:16.915
First, we import the SQLContext, run this.

19
00:01:16.915 --> 00:01:24.878
Next, we create an SQLContext
from the SparkContext run this.

20
00:01:24.878 --> 00:01:29.676
And next, we'll create a Spark DataFrame
from a Postgres table.

21
00:01:32.980 --> 00:01:34.950
We used the read attribute format.

22
00:01:36.820 --> 00:01:40.020
The jdbc argument means that we're
using a Java database connection.

23
00:01:42.180 --> 00:01:44.000
The next line sets the URL option.

24
00:01:44.000 --> 00:01:49.081
It says we're using Postgres
database running on the local host.

25
00:01:49.081 --> 00:01:54.287
The database name is Cloudera and
the username is Cloudera.

26
00:01:54.287 --> 00:01:56.048
The second option, DB table,

27
00:01:56.048 --> 00:02:00.000
says we want our data frame
to be the game clicks table.

28
00:02:00.000 --> 00:02:01.130
And finally we call load.

29
00:02:03.290 --> 00:02:04.400
Let's execute this.

30
00:02:06.370 --> 00:02:11.153
You can see the schema of the data
frame by calling df.printschema.

31
00:02:14.602 --> 00:02:17.593
This shows the name of each
column along with the data type.

32
00:02:20.499 --> 00:02:24.253
We can count the rows in this
df frame by calling df.count.

33
00:02:31.512 --> 00:02:35.786
We can look at the first five
rows by calling df.show(5)

34
00:02:39.493 --> 00:02:41.362
This shows all the columns
in the data frame.

35
00:02:43.240 --> 00:02:46.260
We can select specific columns
by using the select method.

36
00:02:48.080 --> 00:02:52.275
Let's select just the User ID and
Team Level columns.

37
00:02:52.275 --> 00:02:57.860
I'll enter df.select("userid",teamlevel.

38
00:02:57.860 --> 00:02:58.641
Parenthesis.

39
00:03:01.524 --> 00:03:04.610
And finally we only want to
see the top five rows.

40
00:03:04.610 --> 00:03:06.312
So we'll do .show(5).

41
00:03:10.870 --> 00:03:15.368
We can also select rows
that have a specific value.

42
00:03:15.368 --> 00:03:19.067
Let's look for the rows where
the team level is greater than one.

43
00:03:19.067 --> 00:03:23.092
We'll enter df.filter.

44
00:03:23.092 --> 00:03:27.393
We'll specify that we want team level
greater than one by entering df,

45
00:03:27.393 --> 00:03:30.412
square bracket, team level,
greater than one.

46
00:03:34.336 --> 00:03:38.183
And again, we only want the user ID and
team level columns.

47
00:03:43.078 --> 00:03:45.368
And finally only the first five rows.

48
00:03:50.624 --> 00:03:55.366
We can use the group by method to
aggregate a particular column.

49
00:03:55.366 --> 00:03:59.542
For example, the ishit column
has a value of zero or one.

50
00:03:59.542 --> 00:04:04.611
And we can use group by to count how
many times each of these values occurs.

51
00:04:04.611 --> 00:04:12.068
Or a df.groupby ishit, and
we'll call count to count the values

52
00:04:21.520 --> 00:04:25.552
We can also perform aggregate statistical
operations on the data in a data frame.

53
00:04:27.000 --> 00:04:31.140
Let's compute the mean and
sum values for ishit.

54
00:04:31.140 --> 00:04:36.756
First we need to import
the statistical functions we'll

55
00:04:36.756 --> 00:04:41.910
run from.pyspark.sql.functions
import star.

56
00:04:41.910 --> 00:04:47.891
Next we'll run df.select
(mean) ishit,sum ishit

57
00:04:55.375 --> 00:04:58.580
We can also join two data
frames on a particular column.

58
00:04:59.980 --> 00:05:04.740
Let's join the existing data frame of the
game clicks table with the adclicks table.

59
00:05:05.850 --> 00:05:08.410
First, we need to create data frame for
the adclicks table.

60
00:05:09.810 --> 00:05:10.796
Let's go back up.

61
00:05:15.286 --> 00:05:16.991
Copy the content of this cell,

62
00:05:23.747 --> 00:05:26.795
Paste it.

63
00:05:26.795 --> 00:05:31.356
We put the adclicks table
the data frame called df2.

64
00:05:31.356 --> 00:05:34.462
And we'll change the db table
option to the adclicks.

65
00:05:38.376 --> 00:05:39.388
Run it.

66
00:05:41.687 --> 00:05:44.117
Let's print the schema of df2.

67
00:05:52.283 --> 00:05:56.064
You can see that it also has
a column called user id.

68
00:05:56.064 --> 00:06:00.024
So let's join the game clicks
data frame with the add

69
00:06:00.024 --> 00:06:02.643
clicks data frame on this column.

70
00:06:02.643 --> 00:06:06.229
We put the result in a new
data frame called merged.

71
00:06:06.229 --> 00:06:15.490
We'll say merge = df.join.df2 "userid".

72
00:06:15.490 --> 00:06:15.990
We'll run it.

73
00:06:17.820 --> 00:06:19.163
Let's look at the schema.

74
00:06:19.163 --> 00:06:21.193
We'll call merge.printschema.

75
00:06:25.763 --> 00:06:29.499
We can see that this merged data frame
has the column for both game clicks and

76
00:06:29.499 --> 00:06:30.105
adclicks.

77
00:06:31.780 --> 00:06:35.537
Finally we'll look at the top five
rows in this merged data frame.

78
00:06:35.537 --> 00:06:38.208
We'll run merge.show.

1
00:00:02.020 --> 00:00:05.610
We have now seen some
simple transformations and

2
00:00:05.610 --> 00:00:10.900
how Spark can create RDDs from
each other using transformations.

3
00:00:10.900 --> 00:00:15.390
We learned that transformations are
evaluated after an action is performed.

4
00:00:16.810 --> 00:00:21.800
So we can simply define actions as RDD
operations that trigger the evaluation of

5
00:00:21.800 --> 00:00:27.530
the transformation pipeline and return
the final result to the driver program or

6
00:00:27.530 --> 00:00:29.810
save the results to a persistent storage.

7
00:00:31.660 --> 00:00:36.580
We can also call them the last
step in a Spark pipeline.

8
00:00:36.580 --> 00:00:38.700
Let's now look at a few action operations.

9
00:00:40.870 --> 00:00:46.150
After this video, you will be able to
explain the steps of a Spark pipeline

10
00:00:46.150 --> 00:00:52.010
ending with a collect action and list
four common action operations in Spark.

11
00:00:54.750 --> 00:00:57.740
A very common action in Spark is collect.

12
00:00:59.090 --> 00:01:03.875
In this example, we can imagine that
initially we are reading from HDFS.

13
00:01:05.390 --> 00:01:10.020
The RDD partitions that go through
the transformation steps in our big data

14
00:01:10.020 --> 00:01:14.970
pipeline are defined as flatMap and
groupbyKey.

15
00:01:16.290 --> 00:01:21.625
When the final step is done,
the collect action is called and

16
00:01:21.625 --> 00:01:25.840
Spark sends all the tasks for
execution to the worker notes.

17
00:01:28.372 --> 00:01:32.940
Collect will send all the resulting
RDDs from the workers and

18
00:01:32.940 --> 00:01:37.140
copy them to the Java virtual
machine on the driver program.

19
00:01:37.140 --> 00:01:41.600
And then, this will be piped
also to our Python shell.

20
00:01:43.150 --> 00:01:48.076
While collect copies all the data,
another action, take,

21
00:01:48.076 --> 00:01:51.720
copies the first n results of the driver.

22
00:01:53.690 --> 00:01:57.440
If the results are too large
to fit in the driver memory,

23
00:01:57.440 --> 00:02:01.290
then there's an opportunity to write
them directly to HDFS instead.

24
00:02:03.230 --> 00:02:07.850
Among many other actions,
reduce is probably the most famous one.

25
00:02:08.980 --> 00:02:13.300
Reduce takes two elements and
returns a result, like sum.

26
00:02:13.300 --> 00:02:19.680
But in this case, we don't have a key,
we just have a large area of some values.

27
00:02:19.680 --> 00:02:22.150
And we are running this function over and

28
00:02:22.150 --> 00:02:26.580
over again to reduce everything
to one single value.

29
00:02:26.580 --> 00:02:29.210
For example,
to the global sum of everything.

30
00:02:30.905 --> 00:02:34.945
Another very useful action Is saveAsText,

31
00:02:34.945 --> 00:02:38.585
to save the results to local disk or
HDFS, and

32
00:02:38.585 --> 00:02:44.485
this is very useful if the output of
the power computation is pretty large.
