1 
00:00:00.870000 --> 00:00:04.540000 
In this short video we’ll
talk about how Meltwater 
2 
00:00:04,540 --> 00:00:09,240 
helped Danone using sentiment analysis. 
3 
00:00:09,240 --> 00:00:12,600 
Meltwater is a company
that helps other companies 
4 
00:00:12,600 --> 00:00:17,100 
analyze what people are saying about
them and manage their online reputation. 
5 
00:00:18,700 --> 00:00:22,980 
One of the case studies on their
website is about Danone baby nutrition. 
6 
00:00:24,180 --> 00:00:29,358 
Meltwater helped Danone to monitor
the opinions through social media for 
7 
00:00:29,358 --> 00:00:31,873 
one of their marketing campaigns. 
8 
00:00:31,873 --> 00:00:35,204 
They were able to measure what
was impactful and what was not, 
9 
00:00:35,204 --> 00:00:36,690 
through such monitoring. 
10 
00:00:37,890 --> 00:00:42,610 
Meltwater also helped Danone manage
a potential reputation issue. 
11 
00:00:42,610 --> 00:00:45,380 
When a crisis occurred
related to horse DNA 
12 
00:00:45,380 --> 00:00:48,320 
being in some meat products across Europe. 
13 
00:00:48,320 --> 00:00:52,830 
While Danone was confident that they
didn't have an issue with their products, 
14 
00:00:52,830 --> 00:00:57,730 
having the information a couple of
hours before news hit the UK press. 
15 
00:00:57,730 --> 00:00:59,690 
Allowed them to check and 
16 
00:00:59,690 --> 00:01:04,580 
reassure their customers that their
products were safe to consume. 
17 
00:01:04,580 --> 00:01:09,040 
You can imagine millions of mothers
having been reassured and happy for 
18 
00:01:09,040 --> 00:01:11,180 
Danone's efforts on this. 
19 
00:01:11,180 --> 00:01:16,375 
This is an excellent story about how
big data helped manage public opinion. 
20 
00:01:16,375 --> 00:01:21,136 
And I'm sure Meltwater was able to help
them to measure the opinion impact through 
21 
00:01:21,136 --> 00:01:22,448 
social media as well. 
1 
00:00:01,240 --> 00:00:04,508 
Big data is now being
generated all around us. 
2 
00:00:04,508 --> 00:00:05,750 
So what? 
3 
00:00:05,750 --> 00:00:07,450 
It's the applications. 
4 
00:00:07,450 --> 00:00:12,170 
It is the way in which big data can
serve human needs that makes it valued. 
5 
00:00:13,430 --> 00:00:17,971 
Let's look at a few examples of the
applications big data is allowing us to 
6 
00:00:17,971 --> 00:00:19,226 
imagine and build. 
7 
00:00:37,796 --> 00:00:43,420 
Big data allows us to build better models,
which produce higher precision results. 
8 
00:00:44,470 --> 00:00:49,021 
We are witnessing hugely innovative
approaches in how companies 
9 
00:00:49,021 --> 00:00:51,758 
market themselves and sell products. 
10 
00:00:51,758 --> 00:00:53,958 
How human resources are managed. 
11 
00:00:53,958 --> 00:00:56,178 
How disasters are responded to. 
12 
00:00:56,178 --> 00:01:00,389 
And many other applications that
evidenced based data is being 
13 
00:01:00,389 --> 00:01:02,460 
used to influence decisions. 
14 
00:01:04,660 --> 00:01:06,670 
What exactly does that mean? 
15 
00:01:06,670 --> 00:01:08,150 
Here is one example. 
16 
00:01:08,150 --> 00:01:10,640 
Many of you might have experienced it,
I do. 
17 
00:01:12,540 --> 00:01:16,140 
Data, Amazon keeps some
things I've been looking at 
18 
00:01:16,140 --> 00:01:19,610 
allows them to personalize
what they show me. 
19 
00:01:19,610 --> 00:01:24,300 
Which hopefully helps narrow down
the huge raft of options I might get 
20 
00:01:24,300 --> 00:01:27,380 
than just searching on dinner plates. 
21 
00:01:27,380 --> 00:01:32,307 
Now, businesses can leverage technology
to make better informed decisions 
22 
00:01:32,307 --> 00:01:37,246 
that are actually based on signals
generated by actual consumers, like me. 
23 
00:01:39,454 --> 00:01:43,980 
Big data enables you to hear
the voice of each consumer as 
24 
00:01:43,980 --> 00:01:46,590 
opposed to consumers at large. 
25 
00:01:47,920 --> 00:01:51,500 
Now, many companies,
including Walmart and Target, 
26 
00:01:51,500 --> 00:01:57,160 
use this information to personalize their
communications with their costumers, which 
27 
00:01:57,160 --> 00:02:01,240 
in turns leads to better met consumer
expectations and happier customers. 
28 
00:02:03,550 --> 00:02:09,628 
Which basically is to say, big data
has enabled personalized marketing. 
29 
00:02:09,628 --> 00:02:14,250 
Consumers are copiously generating
publicly accessible data through 
30 
00:02:14,250 --> 00:02:16,560 
social media sites,
like Twitter or Facebook. 
31 
00:02:17,690 --> 00:02:22,130 
Through such data, the companies
are able to see their purchase history, 
32 
00:02:22,130 --> 00:02:26,970 
what they searched for, what they watched,
where they have been, and 
33 
00:02:26,970 --> 00:02:29,650 
what they're interested in
through their likes and shares. 
34 
00:02:30,920 --> 00:02:35,717 
Let's look at some examples of how
companies are putting this information to 
35 
00:02:35,717 --> 00:02:39,868 
build better marketing campaigns and
reach the right customers. 
36 
00:02:42,718 --> 00:02:46,985 
One area we are all familiar with
are the recommendation engines. 
37 
00:02:46,985 --> 00:02:52,190 
These engines leverage user patterns and
product features 
38 
00:02:52,190 --> 00:02:58,028 
to predict best match product for
enriching the user experience. 
39 
00:02:58,028 --> 00:03:00,303 
If you ever shopped on Amazon, 
40 
00:03:00,303 --> 00:03:04,596 
you know you get recommendations
based on your purchase. 
41 
00:03:04,596 --> 00:03:07,861 
Similarly, Netflix would
recommend you to watch 
42 
00:03:07,861 --> 00:03:10,590 
new shows based on your viewing history. 
43 
00:03:12,610 --> 00:03:18,550 
Another technique that companies use is
sentiment analysis, or in simple terms, 
44 
00:03:18,550 --> 00:03:22,550 
analysis of the feelings around events and
products. 
45 
00:03:23,880 --> 00:03:27,900 
Remember the blue plates I
purchased on Amazon.com? 
46 
00:03:27,900 --> 00:03:31,680 
I not only can read the reviews
before purchasing them, 
47 
00:03:31,680 --> 00:03:35,460 
I can also write a product
review once I receive my plates. 
48 
00:03:37,280 --> 00:03:40,500 
This way, other customers can be informed. 
49 
00:03:41,880 --> 00:03:47,110 
But more importantly, Amazon can keep
a watch on the product reviews and 
50 
00:03:47,110 --> 00:03:49,220 
trends for a particular product. 
51 
00:03:49,220 --> 00:03:51,951 
In this case, blue plates. 
52 
00:03:51,951 --> 00:03:58,300 
For example, they can judge if a product
review is positive or negative. 
53 
00:03:59,940 --> 00:04:03,140 
In this case,
while the first review is negative, 
54 
00:04:05,140 --> 00:04:07,530 
the next two reviews are positive. 
55 
00:04:09,180 --> 00:04:13,130 
Since these reviews are written in
English using a technique called natural 
56 
00:04:13,130 --> 00:04:16,780 
language processing, and
other analytical methods, 
57 
00:04:16,780 --> 00:04:22,120 
Amazon can analyze the general opinion of
a person or public about such a product. 
58 
00:04:24,130 --> 00:04:29,128 
This is why sentiment analysis often
gets referred to as opinion mining. 
59 
00:04:31,298 --> 00:04:35,729 
News channels are filled with
Twitter feed analysis every time 
60 
00:04:35,729 --> 00:04:39,420 
an event of importance occurs,
such as elections. 
61 
00:04:40,810 --> 00:04:46,162 
Brands utilize sentiment analysis
to understand how customers 
62 
00:04:46,162 --> 00:04:51,630 
relate to their product,
positively, negatively, neutral. 
63 
00:04:51,630 --> 00:04:54,448 
This depends heavily on use of
natural language processing. 
64 
00:04:57,183 --> 00:04:59,289 
Mobile devices are ubiquitous and 
65 
00:04:59,289 --> 00:05:02,780 
people almost always carry
their cellphones with them. 
66 
00:05:03,810 --> 00:05:07,200 
Mobile advertising is a huge market for
businesses. 
67 
00:05:08,760 --> 00:05:13,581 
Platforms utilize the sensors
in mobile devices, 
68 
00:05:13,581 --> 00:05:18,847 
such as GPS, and
provide real time location based ads, 
69 
00:05:18,847 --> 00:05:23,456 
offer discounts,
based on this deluge of data. 
70 
00:05:23,456 --> 00:05:28,063 
This time, let's imagine that
I bought a new house and 
71 
00:05:28,063 --> 00:05:32,178 
I happen to be in a few
miles range of a Home Depot. 
72 
00:05:32,178 --> 00:05:35,444 
Sending me mobile coupons about paint,
shelves, and 
73 
00:05:35,444 --> 00:05:39,290 
other new home related purchases
would remind me of Home Depot. 
74 
00:05:40,550 --> 00:05:43,590 
There's a big chance I
would stop by Home Depot. 
75 
00:05:43,590 --> 00:05:44,790 
Bingo! 
76 
00:05:44,790 --> 00:05:49,470 
Now I would like to take a moment to
analyze what kinds of big data are needed 
77 
00:05:49,470 --> 00:05:50,270 
to make this happen. 
78 
00:05:51,495 --> 00:05:55,990 
There's definitely the integration
of my consumer information and 
79 
00:05:55,990 --> 00:06:00,710 
the online and offline databases
that include my recent purchases. 
80 
00:06:00,710 --> 00:06:05,569 
But more importantly,
the geolocation data that falls under 
81 
00:06:05,569 --> 00:06:09,128 
a larger type of big data,
spacial big data. 
82 
00:06:09,128 --> 00:06:11,780 
We will talk about spacial
data later in this class. 
83 
00:06:13,190 --> 00:06:17,770 
Let's now talk about how the global
consumer behavior can be used for 
84 
00:06:17,770 --> 00:06:18,360 
product growth. 
85 
00:06:20,170 --> 00:06:23,760 
We are now moving from
personalize marketing 
86 
00:06:23,760 --> 00:06:26,230 
to the consumer behavior as a whole. 
87 
00:06:27,920 --> 00:06:32,758 
Every business wants to understand
their consumer’s collective 
88 
00:06:32,758 --> 00:06:37,258 
behavior in order to capture
the ever-changing landscape. 
89 
00:06:37,258 --> 00:06:42,809 
Several big data products enable this
by developing models to capture user 
90 
00:06:42,809 --> 00:06:48,730 
behavior and allow businesses to target
the right audience for their product. 
91 
00:06:50,340 --> 00:06:53,250 
Or, develop new products for
uncharted territories. 
92 
00:06:55,570 --> 00:06:57,550 
Let's look at this example. 
93 
00:06:57,550 --> 00:07:00,797 
After an analysis of their sales for
weekdays, 
94 
00:07:00,797 --> 00:07:06,319 
an airline company might notice that their
morning flights are always sold out, 
95 
00:07:06,319 --> 00:07:09,908 
while their afternoon
flights run below capacity. 
96 
00:07:09,908 --> 00:07:15,545 
This company might decide to add more
morning flights based on such analysis. 
97 
00:07:16,900 --> 00:07:21,550 
Notice that they are not using
individual consumer choices, but 
98 
00:07:21,550 --> 00:07:26,760 
using all the flights purchased without
consideration to who purchased them. 
99 
00:07:28,200 --> 00:07:29,150 
They might, however, 
100 
00:07:29,150 --> 00:07:34,400 
decide to pay closer attention to
the demographic of these consumers 
101 
00:07:34,400 --> 00:07:38,850 
using big data to also add similar
flights in other geographical regions. 
102 
00:07:40,350 --> 00:07:45,240 
With rapid advances in genome
sequencing technology, 
103 
00:07:45,240 --> 00:07:51,580 
the life sciences industry is experiencing
an enormous draw in biomedical big data. 
104 
00:07:53,120 --> 00:07:59,168 
This biomedical data is being used
by many applications in research and 
105 
00:07:59,168 --> 00:08:01,398 
personalized medicine. 
106 
00:08:01,398 --> 00:08:06,400 
Did you know genomics data is one of
the largest growing big data types? 
107 
00:08:06,400 --> 00:08:13,940 
Between 100 million and 2 billion human
genomes could be sequenced by year 2025. 
108 
00:08:13,940 --> 00:08:14,520 
Impressive. 
109 
00:08:16,700 --> 00:08:19,080 
This [INAUDIBLE] sequence data demands for 
110 
00:08:19,080 --> 00:08:24,580 
between 2 exabytes and
40 exabytes in data storage. 
111 
00:08:24,580 --> 00:08:30,190 
In comparison, all of YouTube only
requires 1 to 2 exabytes a year. 
112 
00:08:32,530 --> 00:08:35,886 
An exabyte is 10 to the power 18 bites. 
113 
00:08:35,886 --> 00:08:41,818 
That is, 18 zeros after 40. 
114 
00:08:41,818 --> 00:08:48,270 
Of course, analysis of such massive
volumes of sequence data is expensive. 
115 
00:08:48,270 --> 00:08:50,680 
It could take up to 10,000
trillion CPU hours. 
116 
00:08:54,580 --> 00:08:59,040 
One of the biomedical applications
that this much data is enabling 
117 
00:08:59,040 --> 00:09:00,450 
is personalized medicine. 
118 
00:09:02,060 --> 00:09:06,879 
Before personalized medicine,
most patients without a specific type and 
119 
00:09:06,879 --> 00:09:09,862 
stage of cancer received
the same treatment, 
120 
00:09:09,862 --> 00:09:12,858 
which worked better for
some than the others. 
121 
00:09:14,968 --> 00:09:20,086 
Research in this area is enabling
development of methods to analyze 
122 
00:09:20,086 --> 00:09:25,472 
large scale data to develop solutions
that tailor to each individual, 
123 
00:09:25,472 --> 00:09:28,900 
and hence hypothesize
to be more effective. 
124 
00:09:30,450 --> 00:09:36,260 
A person with cancer may now still receive
a treatment plan that is standard, 
125 
00:09:36,260 --> 00:09:38,620 
such as surgery to remove a tumor. 
126 
00:09:39,870 --> 00:09:43,750 
However, the doctor may
also be able to recommend 
127 
00:09:43,750 --> 00:09:45,460 
some type of personalized
cancer treatment. 
128 
00:09:47,150 --> 00:09:51,900 
A big challenge in biomedical big data
applications, like many other fields, 
129 
00:09:51,900 --> 00:09:56,860 
is how we can integrate many types of data
sources to gain further insight problem. 
130 
00:09:58,300 --> 00:10:01,648 
In one of our future lectures,
my colleagues here at 
131 
00:10:01,648 --> 00:10:06,670 
the Supercomputer Center,
will explain how he and his colleague have 
132 
00:10:06,670 --> 00:10:11,940 
used big data from a variety of sources
for personalized patient interventions. 
133 
00:10:13,840 --> 00:10:19,010 
Another application of big data comes
from interconnected mesh of large 
134 
00:10:19,010 --> 00:10:24,490 
number of sensors implanted
across smart cities. 
135 
00:10:24,490 --> 00:10:28,790 
Analysis of data generated
from sensors in real time 
136 
00:10:28,790 --> 00:10:33,140 
allows cities to deliver better
service quality to inhabitants. 
137 
00:10:33,140 --> 00:10:38,340 
And reduce unwanted affect such
as pollution, traffic congestion, 
138 
00:10:38,340 --> 00:10:41,470 
higher than optimal cost on
delivering urban services. 
139 
00:10:42,910 --> 00:10:44,750 
Let's take our city, San Diego. 
140 
00:10:46,370 --> 00:10:51,794 
San Diego generates a huge volumes
of data from many sources. 
141 
00:10:51,794 --> 00:10:57,676 
Traffic sensors, satellites,
camera networks, and more. 
142 
00:10:57,676 --> 00:10:59,694 
What if we could integrate and 
143 
00:10:59,694 --> 00:11:04,140 
synthesize these data streams to
do even more for our community? 
144 
00:11:05,260 --> 00:11:07,000 
Using such big data, 
145 
00:11:07,000 --> 00:11:12,170 
we can work toward making San Diego
the prototype digital city. 
146 
00:11:12,170 --> 00:11:15,020 
Not only for life-threatening hazards, but 
147 
00:11:15,020 --> 00:11:20,280 
making our daily lives better, such as
managing traffic flow more efficiently or 
148 
00:11:20,280 --> 00:11:24,870 
maximizing energy savings,
even as we'll see next, wildfires. 
149 
00:11:26,450 --> 00:11:31,390 
If you want to read more,
here's a link to the AT Kearney report, 
150 
00:11:31,390 --> 00:11:34,120 
where they talk about other
areas using big data. 
151 
00:11:35,580 --> 00:11:39,880 
As a summary,
big data has a huge potential 
152 
00:11:39,880 --> 00:11:44,140 
to enable models with higher
precision in many application areas. 
153 
00:11:45,230 --> 00:11:50,175 
And these highly precise models
are influencing and transforming business. 
1 
00:00:01,140 --> 00:00:04,170 
Big Data Generated By People,
how is it being used? 
2 
00:00:05,740 --> 00:00:08,370 
We listed a number of challenges for 
3 
00:00:08,370 --> 00:00:11,400 
using unstructured data
generated by human activities. 
4 
00:00:13,080 --> 00:00:17,730 
Now let's look at some of the emerging
technologies to tackle these challenges. 
5 
00:00:17,730 --> 00:00:22,930 
And see some examples that turn
unstructured data into valuable insights. 
6 
00:00:41,968 --> 00:00:46,780 
Although unstructured data specially
the kind generated by people has 
7 
00:00:46,780 --> 00:00:49,320 
a number of challenges. 
8 
00:00:49,320 --> 00:00:53,300 
The good news is that the business
culture of today is shifting 
9 
00:00:53,300 --> 00:00:57,070 
to tackle these challenges and
take full advantage of such data. 
10 
00:00:58,150 --> 00:01:02,120 
As it is often said,
a challenge is a perfect opportunity. 
11 
00:01:03,130 --> 00:01:06,150 
This is certainly the case for
big data and 
12 
00:01:06,150 --> 00:01:09,680 
these challenges have created
a tech industry of it's own. 
13 
00:01:10,860 --> 00:01:15,730 
This industry is mostly centered or
as we would say, layered or 
14 
00:01:15,730 --> 00:01:21,493 
stacked, around a few fundamental
open source big data frameworks. 
15 
00:01:21,493 --> 00:01:25,320 
Need big data tools
are designed from scratch 
16 
00:01:25,320 --> 00:01:28,990 
to manage unstructured information and
analyze it. 
17 
00:01:28,990 --> 00:01:32,860 
A majority of these tools
are based on an open source 
18 
00:01:32,860 --> 00:01:34,620 
big data framework called Hadoop. 
19 
00:01:35,840 --> 00:01:40,010 
Hadoop is designed to support
the processing of large data sets 
20 
00:01:40,010 --> 00:01:42,960 
in a distributed computing environment. 
21 
00:01:42,960 --> 00:01:47,510 
This definition would already give you a
hint that it tackles the first challenge. 
22 
00:01:47,510 --> 00:01:51,140 
Namely, the volume of
unstructured information. 
23 
00:01:52,150 --> 00:01:55,740 
Hadoop can handle big batches
of distributed information but 
24 
00:01:55,740 --> 00:01:58,250 
most often there's a need for 
25 
00:01:58,250 --> 00:02:03,650 
a real time processing of people generated
data like Twitter or Facebook updates. 
26 
00:02:05,220 --> 00:02:09,950 
Financial compliance monitoring is another
area of our central time processing is 
27 
00:02:09,950 --> 00:02:13,380 
needed, in particular
to reduce market data. 
28 
00:02:14,750 --> 00:02:20,780 
Social media and market data are two
types of what we call high velocity data. 
29 
00:02:21,840 --> 00:02:25,720 
Storm and
Spark are two other open source frameworks 
30 
00:02:25,720 --> 00:02:29,480 
that handle such real time
data generated at a fast rate. 
31 
00:02:30,490 --> 00:02:31,490 
Both Storm and 
32 
00:02:31,490 --> 00:02:36,230 
Spark can integrate data with any
database or data storage technology. 
33 
00:02:37,560 --> 00:02:41,810 
As we have emphasized
before unstructured data 
34 
00:02:41,810 --> 00:02:45,740 
does not have a relational data model so
it doesn't generally 
35 
00:02:45,740 --> 00:02:50,150 
fit into the traditional data warehouse
model based on relational databases. 
36 
00:02:51,350 --> 00:02:55,870 
Data warehouses are central repositories
of integrated data from one or 
37 
00:02:55,870 --> 00:02:56,490 
more sources. 
38 
00:02:58,300 --> 00:03:04,260 
The data that gets stored in warehouses,
gets extracted from multiple sources. 
39 
00:03:05,460 --> 00:03:09,850 
It gets transformed into
a common structured form and 
40 
00:03:09,850 --> 00:03:13,120 
it can slow that into
the central database for 
41 
00:03:13,120 --> 00:03:17,400 
use by workers creating analytical
reports throughout an enterprise. 
42 
00:03:18,650 --> 00:03:25,613 
This Exact Transform Load
process is commonly called ETL. 
43 
00:03:25,613 --> 00:03:29,480 
This approach was fairly standard in
enterprise data systems until recently. 
44 
00:03:30,560 --> 00:03:34,530 
As you probably noticed,
it is fairly static and 
45 
00:03:34,530 --> 00:03:37,350 
does not fit well with today's
dynamic big data world. 
46 
00:03:38,550 --> 00:03:43,073 
So how do today's businesses
get around this problem? 
47 
00:03:43,073 --> 00:03:47,834 
Many businesses today are using a hybrid
approach in which their smaller 
48 
00:03:47,834 --> 00:03:51,894 
structured data remains in
their relational databases, and 
49 
00:03:51,894 --> 00:03:56,750 
large unstructured datasets get stored
in NoSQL databases in the cloud. 
50 
00:03:58,200 --> 00:04:03,840 
NoSQL Data technologies are based
on non-relational concepts and 
51 
00:04:03,840 --> 00:04:08,310 
provide data storage options
typically on computing clouds 
52 
00:04:08,310 --> 00:04:12,180 
beyond the traditional relational
databases centered rate houses. 
53 
00:04:14,170 --> 00:04:19,640 
The main advantage of using
NoSQL solutions is their ability 
54 
00:04:19,640 --> 00:04:24,430 
to organize the data for
scalable access to fit the problem and 
55 
00:04:24,430 --> 00:04:27,330 
objectives pertaining to
how the data will be used. 
56 
00:04:29,130 --> 00:04:34,990 
For example, if the data will be used
in an analysis to find connections 
57 
00:04:34,990 --> 00:04:40,130 
between data sets, then the best
solution is a graph database. 
58 
00:04:41,960 --> 00:04:44,800 
Neo4j is an example of a graph database. 
59 
00:04:45,910 --> 00:04:49,630 
Graph networks is a topic that
the graph analytics course 
60 
00:04:49,630 --> 00:04:53,140 
later in this specialization,
we'll explain in depth. 
61 
00:04:53,140 --> 00:04:59,350 
If the data will be best accessed using
key value pairs like a search engine 
62 
00:04:59,350 --> 00:05:05,490 
scenario, the best solution is probably
a dedicated key value paired database. 
63 
00:05:08,710 --> 00:05:11,660 
Cassandra is an example
of a key value database. 
64 
00:05:13,060 --> 00:05:14,010 
These, and 
65 
00:05:14,010 --> 00:05:18,710 
many other types of NoSQL systems will
be explained further in course two. 
66 
00:05:20,190 --> 00:05:24,160 
So we are now confident that there
are emerging technologies for 
67 
00:05:24,160 --> 00:05:28,410 
individual challenges to manage
people generated unstructured data. 
68 
00:05:29,610 --> 00:05:33,800 
But how does one take advantage
of these to generate value? 
69 
00:05:35,710 --> 00:05:43,150 
As we saw big data must pass through a
series of steps before it generates value. 
70 
00:05:43,150 --> 00:05:47,520 
Namely data access, storage,
cleaning, and analysis. 
71 
00:05:49,220 --> 00:05:55,210 
One approach to solve this problem is
to run each stage as a different layer. 
72 
00:05:56,400 --> 00:06:00,210 
And use tools available to
fit the problem at hand, and 
73 
00:06:00,210 --> 00:06:03,730 
scale analytical solutions to big data. 
74 
00:06:03,730 --> 00:06:08,110 
In coming lectures, we will see
important tools that you can use 
75 
00:06:08,110 --> 00:06:11,840 
to solve your big data problems in
addition to the ones you have seen today. 
76 
00:06:13,070 --> 00:06:17,960 
Now let's take a step back and remind
ourselves what some of the value was. 
77 
00:06:19,220 --> 00:06:23,920 
Remember how companies can listen to the
real voice of customers using big data? 
78 
00:06:25,540 --> 00:06:29,150 
It is this type of generated
data that enabled it. 
79 
00:06:30,190 --> 00:06:35,150 
Sentiment analysis analyzes social
media and other data to find 
80 
00:06:35,150 --> 00:06:40,750 
whether people associate positively or
negatively with you business. 
81 
00:06:40,750 --> 00:06:45,210 
Organizations are utilizing
processing of personal data to 
82 
00:06:45,210 --> 00:06:47,980 
understand the true
preferences of their customers. 
83 
00:06:49,130 --> 00:06:54,110 
Now let's take a fun quiz to guess how
much Twitter data companies analyze 
84 
00:06:54,110 --> 00:06:57,510 
every day to measure sentiment
around their product. 
85 
00:06:59,080 --> 00:07:01,900 
The answer is 12 terabytes a day. 
86 
00:07:03,040 --> 00:07:08,160 
For comparison,
you would need to listen continuously for 
87 
00:07:08,160 --> 00:07:11,620 
two years to finish listening
to 1 terabyte of music. 
88 
00:07:13,000 --> 00:07:15,332 
Another example application area for 
89 
00:07:15,332 --> 00:07:18,960 
people generated data is customer
behavior modeling and prediction. 
90 
00:07:20,100 --> 00:07:24,730 
Amazon, Netflix and
a lot of other organizations, 
91 
00:07:24,730 --> 00:07:28,400 
use analytics to analyze
preferences of their customers. 
92 
00:07:29,710 --> 00:07:35,260 
Based on consumer behavior, organizations
suggest better products to customers, 
93 
00:07:36,350 --> 00:07:40,430 
and in turn have happier customers and
higher profits. 
94 
00:07:41,610 --> 00:07:47,250 
Another application area where the value
comes in the form of societal impact and 
95 
00:07:47,250 --> 00:07:50,540 
social welfare, is disaster management. 
96 
00:07:51,720 --> 00:07:54,550 
As you have seen in my wildfire example, 
97 
00:07:54,550 --> 00:07:58,030 
there are many types of big data that
can help with disaster response. 
98 
00:07:59,450 --> 00:08:03,420 
Data in the form of pictures and
tweets, helps facilitate 
99 
00:08:03,420 --> 00:08:08,380 
a collective response to disaster
situations, such as evacuations through 
100 
00:08:08,380 --> 00:08:12,310 
the safest route based on community
feedback through social media. 
101 
00:08:13,340 --> 00:08:16,820 
There are also networks that
turn crowd sourcing and 
102 
00:08:16,820 --> 00:08:20,650 
big data analytics into collective
disaster response tools. 
103 
00:08:21,920 --> 00:08:24,990 
The International Network
of Crisis Mappers, 
104 
00:08:24,990 --> 00:08:29,950 
also called Crisis Mappers Net,
is the largest of such networks and 
105 
00:08:29,950 --> 00:08:34,360 
includes an active international
community of volunteers. 
106 
00:08:34,360 --> 00:08:40,190 
Crisis Mappers use big data in the form
of aerial and satellite imagery, 
107 
00:08:40,190 --> 00:08:45,440 
participatory maps and
live Twitter updates to analyze 
108 
00:08:45,440 --> 00:08:51,611 
the data using geospatial platforms,
advanced visualization, 
109 
00:08:51,611 --> 00:08:56,300 
live simulation and computational and
statistical models. 
110 
00:08:57,780 --> 00:09:03,220 
Once analyzed the results get reported to
rapid response and humanitarian agencies 
111 
00:09:04,220 --> 00:09:10,020 
in the form of mobile and
web applications. 
112 
00:09:10,020 --> 00:09:16,510 
In 2015, right after the Nepal earthquake
Crises Mappers crowd source the analysis 
113 
00:09:16,510 --> 00:09:21,990 
of tweets and mainstream media to
rapidly access disaster damage and 
114 
00:09:21,990 --> 00:09:27,210 
needs and to identify where
humanitarian help is needed. 
115 
00:09:27,210 --> 00:09:32,663 
This example is amazing and shows how
big data can have huge impacts for 
116 
00:09:32,663 --> 00:09:35,263 
social welfare in times of need. 
117 
00:09:35,263 --> 00:09:38,400 
You can learn more about this
story at the following link. 
118 
00:09:40,790 --> 00:09:45,679 
As a summary, although there are
challenges in working with unstructured 
119 
00:09:45,679 --> 00:09:50,270 
people generated data at a scale and
speed that applications demand. 
120 
00:09:50,270 --> 00:09:54,124 
There are also emerging technologies and
solutions that are being 
121 
00:09:54,124 --> 00:09:58,890 
used by many applications to generate
value from the rich source of information. 
1 
00:00:00,920 --> 00:00:05,485 
Big Data Generated By People,
The Unstructured Challenge. 
2 
00:00:21,404 --> 00:00:26,471 
People are generating massive amounts of
data every day through their activities on 
3 
00:00:26,471 --> 00:00:31,405 
various social media networking sites
like Facebook, Twitter, and LinkedIn. 
4 
00:00:31,405 --> 00:00:36,655 
Or online photo sharing sites like
Instagram, Flickr, or Picasa. 
5 
00:00:38,540 --> 00:00:40,620 
And video sharing websites like YouTube. 
6 
00:00:42,020 --> 00:00:47,100 
In addition an enormous amount of
information gets generated via 
7 
00:00:47,100 --> 00:00:52,250 
blogging and commenting,
internet searches, more via text messages, 
8 
00:00:53,320 --> 00:00:56,460 
email, and through personal documents. 
9 
00:00:57,920 --> 00:01:02,711 
Most of this data is text-heavy and
unstructured, 
10 
00:01:02,711 --> 00:01:08,780 
that is non-conforming to
a well-defined data model. 
11 
00:01:08,780 --> 00:01:12,690 
We can also consider this
data to be content with 
12 
00:01:12,690 --> 00:01:15,630 
occasionally some
description attached to it. 
13 
00:01:15,630 --> 00:01:20,360 
This much activity Leads
to a huge growth in data. 
14 
00:01:21,480 --> 00:01:25,980 
Did you know that in a single day,
Facebook users produce 
15 
00:01:25,980 --> 00:01:30,550 
more data than combined US
academic research libraries? 
16 
00:01:32,100 --> 00:01:35,130 
Let's look at some similar
daily data volume numbers 
17 
00:01:36,130 --> 00:01:38,460 
from some of the biggest online platforms. 
18 
00:01:39,720 --> 00:01:43,790 
It is amazing that some of these
numbers are in the petabyte range for 
19 
00:01:43,790 --> 00:01:44,700 
daily activity. 
20 
00:01:45,840 --> 00:01:48,930 
A petabyte is a thousand terabytes. 
21 
00:01:50,470 --> 00:01:55,360 
The sheer size of mostly unstructured
data generated by humans 
22 
00:01:55,360 --> 00:01:56,950 
brings a lot of challenges. 
23 
00:01:58,730 --> 00:02:05,250 
Unstructured data refers to data that does
not conform to a predefined data model. 
24 
00:02:07,200 --> 00:02:10,380 
So no relation model and no SQL. 
25 
00:02:11,980 --> 00:02:16,190 
It is mostly anything that we
don't store in a traditional 
26 
00:02:16,190 --> 00:02:17,640 
Relational database management system. 
27 
00:02:19,170 --> 00:02:22,480 
Consider a sales receipt that
you get from a grocery store. 
28 
00:02:23,580 --> 00:02:26,790 
It has a section for a date, a section for 
29 
00:02:26,790 --> 00:02:30,690 
store name, and
a section for total amount. 
30 
00:02:32,200 --> 00:02:34,310 
This is an example of structure. 
31 
00:02:35,340 --> 00:02:40,550 
Humans generate a lot of
unstructured data in form of text. 
32 
00:02:40,550 --> 00:02:42,960 
There's no given format to that. 
33 
00:02:42,960 --> 00:02:46,180 
Look at all the documents that you
have written with your hand so far. 
34 
00:02:47,220 --> 00:02:52,550 
Collectively, it is a bank of unstructured
data you have personally generated. 
35 
00:02:53,580 --> 00:02:58,160 
In fact, 80 to 90% of all data in 
36 
00:02:58,160 --> 00:03:02,980 
the world is unstructured and
this number is rapidly growing. 
37 
00:03:04,490 --> 00:03:09,531 
Examples of unstructured data generated
by people includes texts, images, 
38 
00:03:09,531 --> 00:03:16,190 
videos, audio,
internet searches, and emails. 
39 
00:03:16,190 --> 00:03:21,580 
In addition to it's rapid growth
major challenges of unstructured data 
40 
00:03:21,580 --> 00:03:26,768 
include multiple data formats,
like webpages, images, PDFs, 
41 
00:03:26,768 --> 00:03:34,170 
power point, XML, and other formats that
were mainly built for human consumption. 
42 
00:03:34,170 --> 00:03:41,030 
Think of it, although I can sort my
email with date, sender and subject. 
43 
00:03:41,030 --> 00:03:44,690 
It would be really difficult
to write a program, 
44 
00:03:44,690 --> 00:03:49,430 
to categorize all my email messages
based on their content and 
45 
00:03:49,430 --> 00:03:55,460 
organize them for me accordingly another
challenge of human generated data 
46 
00:03:55,460 --> 00:04:01,560 
is the volume and fast generation of data,
which is what we call velocity. 
47 
00:04:01,560 --> 00:04:07,504 
Just take a moment to study this info
graphic, and observe what happens in one 
48 
00:04:07,504 --> 00:04:12,728 
minute on the internet, and
consider how much to contribute to it. 
49 
00:04:15,088 --> 00:04:22,586 
Moreover, confirmation of unstructured
data is often time consuming and costly. 
50 
00:04:22,586 --> 00:04:28,130 
The costs and
time of the process of acquiring, storing, 
51 
00:04:28,130 --> 00:04:34,110 
cleaning, retrieving, and processing
unstructured data can add up to quite and 
52 
00:04:34,110 --> 00:04:37,220 
investment before we can start
reaping value from this process. 
53 
00:04:39,230 --> 00:04:41,790 
It can be pretty hard
to find the tools and 
54 
00:04:41,790 --> 00:04:45,810 
people to implement such a process and
reap value in the end. 
55 
00:04:47,030 --> 00:04:50,570 
As a summary,
although there is an enormous amount of 
56 
00:04:50,570 --> 00:04:54,780 
data generated by people,
most of this data is unstructured. 
57 
00:04:55,890 --> 00:04:59,230 
The challenges of working with
unstructured data should not 
58 
00:04:59,230 --> 00:05:01,170 
be taken lightly. 
59 
00:05:01,170 --> 00:05:07,310 
Next, we'll look at how businesses are
tackling these challenges to gain insight. 
60 
00:05:07,310 --> 00:05:10,770 
And thus, value out of working
with people generated data. 
1 
00:00:01,120 --> 00:00:05,970 
As we have already seen, there are many
different exciting applications 
2 
00:00:05,970 --> 00:00:08,060 
that are being enabled
by the Big Data era. 
3 
00:00:09,160 --> 00:00:13,500 
As part of my core research here at
the San Diego Supercomputer Center, 
4 
00:00:13,500 --> 00:00:15,420 
I work on building methodologies and 
5 
00:00:15,420 --> 00:00:20,150 
tools to make Big Data useful to dynamic
data driven scientific applications. 
6 
00:00:21,170 --> 00:00:25,600 
My colleagues and I work on many grand
challenge data science applications, 
7 
00:00:25,600 --> 00:00:30,903 
in all areas of science and engineering,
including genomics, geoinformatics, 
8 
00:00:30,903 --> 00:00:36,050 
metro science, energy management,
biomedicine, and personalized health. 
9 
00:00:37,660 --> 00:00:42,492 
What is common to all these applications
is their unique way of bringing 
10 
00:00:42,492 --> 00:00:46,046 
together new modes of data and
computing research. 
11 
00:00:46,046 --> 00:00:52,195 
Let me tell you the one I'm passionate 
12 
00:00:52,195 --> 00:00:57,192 
about, Wildfire Analytics, 
13 
00:00:57,192 --> 00:01:03,342 
which breaks up into two components, 
14 
00:01:03,342 --> 00:01:07,779 
prediction and response. 
15 
00:01:18,713 --> 00:01:21,629 
Why is this so important? 
16 
00:01:21,629 --> 00:01:25,608 
On May 2014, in San Diego County where 
17 
00:01:25,608 --> 00:01:30,804 
the instructors of this
specialization live and work, 
18 
00:01:30,804 --> 00:01:36,330 
there were 14 fires burning,
as many as nine at one time, 
19 
00:01:36,330 --> 00:01:42,964 
which burned a total of 26,000 acres,
11,000 hectares, 
20 
00:01:42,964 --> 00:01:48,520 
an area just less than the size
of the City of San Francisco. 
21 
00:01:50,710 --> 00:01:56,087 
Six people were injured and
one person died. 
22 
00:01:56,087 --> 00:02:00,093 
And these wildfires
resulted in a total cost of 
23 
00:02:00,093 --> 00:02:04,620 
over $60 million US in damage and
firefighting. 
24 
00:02:05,870 --> 00:02:10,925 
These wildfires can become so severe
that we actually call them firestorms. 
25 
00:02:12,150 --> 00:02:15,170 
Although we cannot
control such fire storms, 
26 
00:02:15,170 --> 00:02:19,070 
something we can do is to get ahead
of them by predicting their behavior. 
27 
00:02:20,790 --> 00:02:25,670 
This is why disaster management
of ongoing wildfires relies 
28 
00:02:25,670 --> 00:02:29,470 
heavily on understanding their
direction and rate of spread. 
29 
00:02:30,560 --> 00:02:33,000 
As these fires are a part of our lives, 
30 
00:02:33,000 --> 00:02:38,410 
we wanted to see if we can use Big Data to
monitor, predict and manage a firestorm. 
31 
00:02:40,400 --> 00:02:42,337 
Why can Big Data help? 
32 
00:02:42,337 --> 00:02:46,685 
As we will see in this video,
indeed, wildfire prevention and 
33 
00:02:46,685 --> 00:02:50,879 
response can benefit from many
streams in our data torrent. 
34 
00:02:50,879 --> 00:02:56,087 
Some streams are generated by
people through devices they carry. 
35 
00:02:56,087 --> 00:03:01,713 
A lot come from sensors and satellites,
things that measure environmental factors. 
36 
00:03:01,713 --> 00:03:05,855 
And some come from organizational data,
including area maps, 
37 
00:03:05,855 --> 00:03:10,839 
better service updates and field content
databases, which archive how much 
38 
00:03:10,839 --> 00:03:15,991 
registers vegetation and other types of
fuel are in the way of a potential fire. 
39 
00:03:17,840 --> 00:03:20,337 
What makes this a Big Data problem? 
40 
00:03:20,337 --> 00:03:25,233 
Because novel approaches and
responses can be taken if 
41 
00:03:25,233 --> 00:03:29,712 
we can integrate this many
diverse data streams. 
42 
00:03:29,712 --> 00:03:34,504 
Many such data sources have already
existed for quite some time. 
43 
00:03:34,504 --> 00:03:39,835 
But what is lacking in disaster
management today is a dynamic system 
44 
00:03:39,835 --> 00:03:44,979 
integration of real time sensor networks,
satellite imagery, 
45 
00:03:44,979 --> 00:03:50,405 
near real time data management tools,
wildfire simulation tools, 
46 
00:03:50,405 --> 00:03:54,334 
connectivity to emergency command centers,
and 
47 
00:03:54,334 --> 00:03:58,470 
all these before, during,
and after a firestorm. 
48 
00:03:59,800 --> 00:04:03,450 
As you will see,
the integration of diverse streams and 
49 
00:04:03,450 --> 00:04:07,760 
novel ways is really what's driving
our ability to see new things and 
50 
00:04:07,760 --> 00:04:11,270 
develop predictive analytics,
which may help improve our world. 
51 
00:04:12,770 --> 00:04:14,140 
What are these diverse sources? 
52 
00:04:15,940 --> 00:04:20,870 
One of the most important data sources
is sensor data streaming in from weather 
53 
00:04:20,870 --> 00:04:26,540 
stations and satellites,
such sensed data include temperature, 
54 
00:04:26,540 --> 00:04:27,960 
humidity, air pressure. 
55 
00:04:29,280 --> 00:04:33,346 
We can also include image
data streaming from 
56 
00:04:33,346 --> 00:04:38,379 
mountaintop cameras and
satellites in this category. 
57 
00:04:38,379 --> 00:04:41,967 
Another important data source
comes from institutions 
58 
00:04:41,967 --> 00:04:44,867 
such as
the San Diego Supercomputer Center, 
59 
00:04:44,867 --> 00:04:48,250 
which generate data related
to wildfire modeling. 
60 
00:04:48,250 --> 00:04:53,585 
These include past and current fire
perimeter maps put together by 
61 
00:04:53,585 --> 00:04:58,824 
the authorities and fuel maps that
tell us about the vegetation, 
62 
00:04:58,824 --> 00:05:02,171 
and other types of fuel in a fire's path. 
63 
00:05:02,171 --> 00:05:08,171 
These types of data sources are often
static or updated at a slow rate, 
64 
00:05:08,171 --> 00:05:14,083 
but they provide valuable data
that is well-curated and verified. 
65 
00:05:15,560 --> 00:05:20,450 
A huge part of data on fires is
actually generated by the public 
66 
00:05:20,450 --> 00:05:24,820 
on social media sites such as Twitter,
which support photo sharing resources. 
67 
00:05:26,650 --> 00:05:32,050 
These are the hardest data sources to
streamline during an existing fire, but 
68 
00:05:32,050 --> 00:05:36,340 
they can be very valuable once
integrated with other data sources. 
69 
00:05:38,820 --> 00:05:44,380 
Imagine synthesizing all the pictures
on Twitter about an ongoing fire or 
70 
00:05:44,380 --> 00:05:47,850 
checking the public sentiment
around the boundaries of a fire. 
71 
00:05:49,960 --> 00:05:53,710 
Once you have access to such
information at your fingertips, 
72 
00:05:53,710 --> 00:05:56,610 
there are many things you
can do with such data. 
73 
00:05:56,610 --> 00:06:01,300 
You can simply monitor it, or
maybe you can visualize it. 
74 
00:06:03,310 --> 00:06:08,370 
But it's not until you bring all these
different types of data sources together 
75 
00:06:08,370 --> 00:06:12,910 
and integrate them with real time analysis
and predictive modeling that you can 
76 
00:06:12,910 --> 00:06:17,570 
really make contributions in predicting
and responding to wildfire emergencies. 
77 
00:06:19,000 --> 00:06:22,221 
So now,
I will like you to take a moment and 
78 
00:06:22,221 --> 00:06:27,296 
imagine how Big Data might help
with firefighting in the future. 
79 
00:06:27,296 --> 00:06:32,280 
All these streams of data will come
together in 3D displays that can show 
80 
00:06:32,280 --> 00:06:37,182 
all the related information along
with weather and fire predictions, 
81 
00:06:37,182 --> 00:06:40,462 
just like the way tornadoes
are managed today. 
1 
00:00:01,270 --> 00:00:06,840 
Let's look at a 2nd example where big data
can have a big impact on saving lives. 
2 
00:00:06,840 --> 00:00:10,428 
I mean,
literally saving lives one life at a time. 
3 
00:00:10,428 --> 00:00:14,508 
I collaborated with a number of
world-class researchers in San Diego, and 
4 
00:00:14,508 --> 00:00:18,911 
an industrial group who are dedicated to
improving human health through research 
5 
00:00:18,911 --> 00:00:20,970 
and practice of precision medicine. 
6 
00:00:22,400 --> 00:00:24,250 
What is precision medicine? 
7 
00:00:24,250 --> 00:00:28,950 
It is an emerging area of medicine
targeted toward an individual person. 
8 
00:00:28,950 --> 00:00:33,910 
Analysing her genetics, her environment,
her daily activities so 
9 
00:00:33,910 --> 00:00:37,880 
that one can detect or
predict a health problem early, 
10 
00:00:37,880 --> 00:00:42,570 
help prevent disease and
in case of illness provide the right drug 
11 
00:00:42,570 --> 00:00:46,690 
at the right dose that is
suitable just for her. 
12 
00:00:46,690 --> 00:00:51,587 
Very recently the White House and
the National Institute of Health here 
13 
00:00:51,587 --> 00:00:56,401 
in the U.S. have declared it to be
a top priority area for research and 
14 
00:00:56,401 --> 00:00:58,779 
development for the next decade. 
15 
00:00:58,779 --> 00:01:01,689 
The expected learning
outcome of this video is for 
16 
00:01:01,689 --> 00:01:04,668 
you to give example of sensor,
organizational and 
17 
00:01:04,668 --> 00:01:07,870 
people-generated data used
in precision medicine. 
18 
00:01:09,050 --> 00:01:13,650 
And, explain why the integration of
different kinds of data is critical 
19 
00:01:13,650 --> 00:01:14,940 
in advancing healthcare. 
20 
00:01:16,150 --> 00:01:20,000 
For any technology to
succeed in real life we need 
21 
00:01:20,000 --> 00:01:24,940 
not only a certain level of maturity of
the technology itself, but a number of 
22 
00:01:24,940 --> 00:01:30,050 
enabling factors including social
economic environment, market demands, 
23 
00:01:30,050 --> 00:01:34,860 
consumer readiness, cost effectiveness,
all of which must work together. 
24 
00:01:36,430 --> 00:01:39,790 
Why is big data for
precision medicine important now? 
25 
00:01:39,790 --> 00:01:40,290 
Let's see. 
26 
00:01:41,480 --> 00:01:44,900 
An important aspect of precision
medicine is to utilize 
27 
00:01:44,900 --> 00:01:49,570 
an individual's genetic profile for
his or her own diagnoses and treatment. 
28 
00:01:50,910 --> 00:01:52,630 
Analyzing the human genome, 
29 
00:01:52,630 --> 00:01:56,670 
which holds the key to human health
is rapidly becoming more affordable. 
30 
00:01:57,950 --> 00:02:01,110 
Today's cost to sequence a genome is less 
31 
00:02:01,110 --> 00:02:03,760 
than 10% of what it
cost just back in 2008. 
32 
00:02:03,760 --> 00:02:09,070 
But, human genomic data is big. 
33 
00:02:09,070 --> 00:02:10,390 
How big? 
34 
00:02:10,390 --> 00:02:14,660 
In a perfect world, just the three
billion letters of your genome 
35 
00:02:14,660 --> 00:02:17,220 
would require about 700
megabytes to store. 
36 
00:02:18,300 --> 00:02:23,280 
In the real world, meaning the kind of
data generated from genome sequencing 
37 
00:02:23,280 --> 00:02:27,540 
machines, we need 200GB to store a genome. 
38 
00:02:28,620 --> 00:02:32,700 
And it takes now about
a day to sequence a genome. 
39 
00:02:32,700 --> 00:02:37,520 
We are finally beginning to create more
electronic records that can be stored and 
40 
00:02:37,520 --> 00:02:39,130 
manipulated in digital media. 
41 
00:02:40,270 --> 00:02:45,180 
Most doctors offices and hospitals now
use electronic health record systems 
42 
00:02:45,180 --> 00:02:49,130 
which contain all details of
a patient's visit and lab test. 
43 
00:02:50,410 --> 00:02:51,290 
How big is this data? 
44 
00:02:52,710 --> 00:02:57,710 
As a quick example The Samaritan Medical
Center Watertown New York at 294 that 
45 
00:02:57,710 --> 00:03:03,690 
Community Hospital reported
120 terabytes as of 2013. 
46 
00:03:03,690 --> 00:03:07,490 
The data value more than double
in just the last two years. 
47 
00:03:08,860 --> 00:03:13,860 
So clearly just in a past two years
dramatic changes have prepared the health 
48 
00:03:13,860 --> 00:03:18,920 
care industry to produce and analyze
larger mounts of complex patient data. 
49 
00:03:20,170 --> 00:03:25,570 
To summarize what we have seen so far
the key components of these changes are: 
50 
00:03:25,570 --> 00:03:31,460 
Reduced cost of data generation and
analysis, increased availability of cheap 
51 
00:03:31,460 --> 00:03:37,430 
large data storage, and they increased
digitization of previously paper records. 
52 
00:03:38,430 --> 00:03:43,560 
But we need one more capability to advance
toward the promised land of individualized 
53 
00:03:43,560 --> 00:03:44,410 
health care practices. 
54 
00:03:45,950 --> 00:03:50,420 
We need to combine various types of
data produce by different groups in 
55 
00:03:50,420 --> 00:03:51,130 
a meaningful way. 
56 
00:03:52,580 --> 00:03:56,150 
Let's look at this issue from
the same point of view as Ilka did. 
57 
00:03:56,150 --> 00:03:59,760 
With her discussion of how big data
can help with wildfire analytics. 
58 
00:04:01,040 --> 00:04:04,980 
The key is the integration of
multiple types of data sources. 
59 
00:04:04,980 --> 00:04:09,130 
Data from sensors,
organizations and people. 
60 
00:04:09,130 --> 00:04:12,200 
In the next few slides,
we look at each of these, and 
61 
00:04:12,200 --> 00:04:17,820 
then I'll share a story about some of the
new and really exciting ways people data 
62 
00:04:17,820 --> 00:04:22,660 
especially has the potential to
change healthcare big data landscape. 
63 
00:04:24,240 --> 00:04:25,320 
Let's start with sensor data. 
64 
00:04:26,600 --> 00:04:31,000 
Sure, digital hospital equipment
have been producing sensor data for 
65 
00:04:31,000 --> 00:04:35,720 
years, but it was unlikely that
the data was ever stored or 
66 
00:04:35,720 --> 00:04:39,790 
shared, let alone
analyzed retrospectively. 
67 
00:04:39,790 --> 00:04:41,025 
These were intended for 
68 
00:04:41,025 --> 00:04:45,880 
real-time use, to inform healthcare
professionals, and then got discarded. 
69 
00:04:47,110 --> 00:04:50,320 
Now we have many more sensors and
deployment. 
70 
00:04:50,320 --> 00:04:52,850 
And many more places
that are capturing and 
71 
00:04:52,850 --> 00:04:55,970 
explicitly gathering information
to be stored and analyzed. 
72 
00:04:57,190 --> 00:04:59,970 
Let's just take a new kind of data 
73 
00:04:59,970 --> 00:05:02,720 
that's increasingly becoming
common in our daily lives. 
74 
00:05:04,960 --> 00:05:06,600 
Fitness devices are everywhere now 
75 
00:05:07,770 --> 00:05:11,320 
their sales have skyrocketed
in the last few years. 
76 
00:05:11,320 --> 00:05:16,360 
They are in wristbands, watches, shoes and
vests, directly communicating with your 
77 
00:05:16,360 --> 00:05:21,160 
personal mobile device, tracking several
activity variables like blood pressure, 
78 
00:05:21,160 --> 00:05:26,080 
different types of activities,
blood glucose levels, etc at every moment. 
79 
00:05:26,080 --> 00:05:28,850 
Their goal is to improve wellness. 
80 
00:05:28,850 --> 00:05:32,080 
By having you monitor
your daily status and 
81 
00:05:32,080 --> 00:05:35,410 
hopefully improve your
lifestyle to stay healthy. 
82 
00:05:35,410 --> 00:05:40,080 
But the data they generate can be
very useful medical information 
83 
00:05:40,080 --> 00:05:44,100 
because this data is about what
happens in your normal life and 
84 
00:05:44,100 --> 00:05:45,700 
not just when you go to the doctor. 
85 
00:05:47,450 --> 00:05:49,660 
How much data do they generate? 
86 
00:05:49,660 --> 00:05:53,890 
The device called FitBit can
produce several gigabytes a day. 
87 
00:05:53,890 --> 00:05:58,900 
Could this data be used to save healthcare
costs, effect a healthier lifestyle? 
88 
00:05:58,900 --> 00:05:59,660 
That's a question mark. 
89 
00:06:01,090 --> 00:06:05,660 
It's safe to guess that this data
alone wouldn't drive the dream of 
90 
00:06:05,660 --> 00:06:07,350 
precision medicine. 
91 
00:06:07,350 --> 00:06:11,290 
But what if we consider integrating
it with other sources of data 
92 
00:06:11,290 --> 00:06:14,060 
like electronic health records or
a genomic profile? 
93 
00:06:15,150 --> 00:06:17,310 
This remains an open question. 
94 
00:06:18,320 --> 00:06:22,460 
This is an open arena for research that
my colleagues at scripts are doing. 
95 
00:06:22,460 --> 00:06:27,090 
It's also a potentially significant area
for product and business development. 
96 
00:06:27,090 --> 00:06:30,480 
Let's look at some examples of health
related data being generated by 
97 
00:06:30,480 --> 00:06:31,120 
organizations. 
98 
00:06:32,630 --> 00:06:35,380 
Many public databases
including those curated and 
99 
00:06:35,380 --> 00:06:39,690 
managed by NCBI, the National Center for
Biotechnology Information, 
100 
00:06:39,690 --> 00:06:43,180 
had been created to capture the basic
scientific data and knowledge for 
101 
00:06:43,180 --> 00:06:48,090 
humans and other model organisms at
the different building blocks of life. 
102 
00:06:48,090 --> 00:06:53,220 
These databases carry both experimental
and computed data that are necessary to 
103 
00:06:53,220 --> 00:06:56,193 
observations for
unconquered diseases like cancer. 
104 
00:06:56,193 --> 00:07:00,730 
In addition,
many have created knoweledge-bases 
105 
00:07:00,730 --> 00:07:04,500 
like the Geneontology and
The Unified Medical Language System 
106 
00:07:04,500 --> 00:07:08,050 
to assemble human knowledge in
a machine processable form. 
107 
00:07:08,050 --> 00:07:11,660 
These are just a few examples of
organizational data sources and 
108 
00:07:11,660 --> 00:07:15,040 
governmental data gathered by health
care systems around the world 
109 
00:07:15,040 --> 00:07:18,050 
could also be used as a massive
source of information. 
110 
00:07:19,380 --> 00:07:22,210 
But really some of
the most interesting and 
111 
00:07:22,210 --> 00:07:27,040 
novel opportunities seem likely to come
from the area of people generated data. 
112 
00:07:28,410 --> 00:07:32,450 
Mobile healths apps is an area
that is growing significantly. 
113 
00:07:32,450 --> 00:07:35,720 
There are apps now to monitor heart rates,
blood pressure, and 
114 
00:07:35,720 --> 00:07:37,310 
test oxygen saturation levels. 
115 
00:07:38,860 --> 00:07:42,630 
Apps, we might say,
record data from sensors but 
116 
00:07:42,630 --> 00:07:45,040 
are also obviously generated from people. 
117 
00:07:46,230 --> 00:07:50,160 
But there is more people generated
data that's interesting beyond censure 
118 
00:07:51,530 --> 00:07:52,120 
measurements. 
119 
00:07:52,120 --> 00:07:55,280 
In 2015 the Webby People's Voice Award 
120 
00:07:55,280 --> 00:07:58,310 
went to an app which supports
meditation and mindfulness. 
121 
00:07:59,410 --> 00:08:02,400 
Rather than an electronic sensing device, 
122 
00:08:02,400 --> 00:08:07,259 
a human would indicate how many
minutes per day they spent meditating. 
123 
00:08:07,259 --> 00:08:11,078 
If they interact with the app
which reminds them to be mindful, 
124 
00:08:11,078 --> 00:08:15,630 
then we have human generated behavior
that we couldn't get from a sensor. 
125 
00:08:16,840 --> 00:08:22,510 
There are well over 100,000 health apps
today in either iTunes or Google Play. 
126 
00:08:22,510 --> 00:08:25,710 
And by some estimates
the Mobile Health App market 
127 
00:08:25,710 --> 00:08:29,064 
may be worth 27 billion dollars by 2017. 
128 
00:08:30,070 --> 00:08:35,500 
So really we are just seen the beginning
of what data might be generated here 
129 
00:08:35,500 --> 00:08:41,490 
from what's being called human sensors,
but to really understand where the power 
130 
00:08:41,490 --> 00:08:45,980 
of people generated data might take us
in the era of big data for healthcare. 
131 
00:08:45,980 --> 00:08:48,360 
Let's imagine how things stand now. 
132 
00:08:48,360 --> 00:08:51,660 
In general,
a patient goes to see their doctor and 
133 
00:08:51,660 --> 00:08:55,960 
maybe their doctor asks if they have had
any side effects from their medications. 
134 
00:08:57,060 --> 00:08:58,430 
The accuracy and 
135 
00:08:58,430 --> 00:09:02,470 
hence the quality of data patients provide
in this kind of setting is very low. 
136 
00:09:03,620 --> 00:09:05,170 
Not that it's really the patients fault. 
137 
00:09:06,240 --> 00:09:09,930 
It might have been days or
weeks ago that they experienced something. 
138 
00:09:09,930 --> 00:09:13,760 
They may be unsure whether something
they experienced was actually a reaction 
139 
00:09:13,760 --> 00:09:15,380 
to then report it. 
140 
00:09:15,380 --> 00:09:18,590 
And there might be details about exactly
when they took a medication that 
141 
00:09:18,590 --> 00:09:20,790 
are meaningful, but
they've forgotten it after the fact. 
142 
00:09:22,520 --> 00:09:28,120 
Today, people are self reporting reactions
and experiences they are having. 
143 
00:09:28,120 --> 00:09:32,540 
We're on Twitter, on blog sites,
online support groups, 
144 
00:09:32,540 --> 00:09:37,030 
online data sharing services: these
are sources of data that we've 
145 
00:09:37,030 --> 00:09:41,660 
never had before that can be used to
understand in a far more detailed and 
146 
00:09:41,660 --> 00:09:46,360 
personal rate the impact of drug
integrations are responses to certain 
147 
00:09:47,640 --> 00:09:50,430 
If applications were designed
to integrate doctor and 
148 
00:09:50,430 --> 00:09:55,040 
hospital records with information
on when drugs were taken and 
149 
00:09:55,040 --> 00:10:00,000 
then to further mine social media or
collect self reports from patients. 
150 
00:10:00,000 --> 00:10:03,230 
Who knows what kinds of questions
we will be able to answer? 
151 
00:10:03,230 --> 00:10:05,008 
Or new questions we may be able to ask? 
1 
00:00:01,690 --> 00:00:03,830 
Where does big data come from? 
2 
00:00:03,830 --> 00:00:07,720 
The first thing I would like to
say before talking about big data 
3 
00:00:07,720 --> 00:00:09,010 
is that it is not new. 
4 
00:00:10,040 --> 00:00:14,600 
Most of the big data sources existed
before, but the scale we use and 
5 
00:00:14,600 --> 00:00:17,020 
apply them today has changed. 
6 
00:00:17,020 --> 00:00:21,590 
Just look at this image of open
link data on the Internet. 
7 
00:00:21,590 --> 00:00:23,740 
I thought this image was so cool. 
8 
00:00:23,740 --> 00:00:27,350 
It shows not only there are so
many sources of data, but 
9 
00:00:27,350 --> 00:00:28,490 
they're also connected. 
10 
00:00:29,690 --> 00:00:32,147 
If you want to check it out yourself, 
11 
00:00:32,147 --> 00:00:35,408 
we'll give you the link
at the end of this video. 
12 
00:00:35,408 --> 00:00:40,859 
Big data is often boiled down to a few
varieties of data generated by machines, 
13 
00:00:40,859 --> 00:00:44,010 
people, and organizations. 
14 
00:00:44,010 --> 00:00:49,057 
With machine generated data we refer to
data generated from real time sensors in 
15 
00:00:49,057 --> 00:00:54,029 
industrial machinery or vehicles that
logs that track user behavior online, 
16 
00:00:54,029 --> 00:00:55,687 
environmental sensors or 
17 
00:00:55,687 --> 00:00:59,930 
personal health trackers, and
many other sense data resources. 
18 
00:01:01,080 --> 00:01:05,560 
The Large Hadron Collider
generates 40 terabytes of data 
19 
00:01:05,560 --> 00:01:08,560 
every second during experiments. 
20 
00:01:08,560 --> 00:01:14,078 
But human generated data, we refer to
the vast amount of social media data, 
21 
00:01:14,078 --> 00:01:18,420 
status updates, tweets,
photos, and medias. 
22 
00:01:18,420 --> 00:01:23,943 
With organizational generated data we
refer to more traditional types of data, 
23 
00:01:23,943 --> 00:01:27,598 
including transaction
information in databases and 
24 
00:01:27,598 --> 00:01:31,026 
structured data open
stored in data warehouses. 
25 
00:01:31,026 --> 00:01:35,437 
Note that big data can be either
structured, semi-structured, or 
26 
00:01:35,437 --> 00:01:40,390 
unstructured, which is a topic we will
talk about more later in this course. 
27 
00:01:41,530 --> 00:01:47,180 
In most business use cases, any single
source of data on its own is not useful. 
28 
00:01:48,490 --> 00:01:53,880 
Real value often comes from combining
these streams of big data sources 
29 
00:01:53,880 --> 00:01:58,210 
with each other and
analyzing them to generate new insights, 
30 
00:01:58,210 --> 00:02:00,430 
which then goes back into
being big data themselves. 
31 
00:02:01,890 --> 00:02:03,870 
Once you have such insights, 
32 
00:02:03,870 --> 00:02:08,320 
it then enables what we call data
enabled decisions and actions. 
33 
00:02:09,670 --> 00:02:13,858 
Let's now look into these different
types of big data in more detail. 
1 
00:00:01,081 --> 00:00:04,312 
Why is Big Data generated
by machines useful? 
2 
00:00:28,325 --> 00:00:31,997 
Let's go back for
a second to our first example for 
3 
00:00:31,997 --> 00:00:34,715 
machine generated big data planes. 
4 
00:00:35,790 --> 00:00:38,940 
What is producing all
that data on the plane? 
5 
00:00:40,430 --> 00:00:43,180 
If you look at some of
the sensors that contribute 
6 
00:00:43,180 --> 00:00:46,600 
to the half terabyte of
data generated on a plane, 
7 
00:00:46,600 --> 00:00:52,040 
we will find that some of it comes from
accelerometers that measure turbulence. 
8 
00:00:53,350 --> 00:00:58,393 
There are also sensors built into
the engines for temperature, pressure, 
9 
00:00:58,393 --> 00:01:02,666 
many other measurable factors
to detect engine malfunctions. 
10 
00:01:02,666 --> 00:01:07,487 
Constant real-time analysis of all
the data collected provides help 
11 
00:01:07,487 --> 00:01:11,496 
monitoring and
problem detection at 40,000 feet. 
12 
00:01:11,496 --> 00:01:15,622 
That's approximately 12,000
meters above ground. 
13 
00:01:17,800 --> 00:01:22,210 
We call this type of
analytical processing in-situ. 
14 
00:01:22,210 --> 00:01:27,940 
Previously, in traditional relational
database management systems, 
15 
00:01:27,940 --> 00:01:32,160 
data was often moved to
computational space for processing. 
16 
00:01:32,160 --> 00:01:37,000 
In Big Data space In-Situ
means bringing the computation 
17 
00:01:37,000 --> 00:01:40,865 
to where data is located or,
in this case, generated. 
18 
00:01:43,200 --> 00:01:47,936 
A key feature of these types
of real-time notifications is 
19 
00:01:47,936 --> 00:01:51,090 
that they enable real-time actions. 
20 
00:01:51,090 --> 00:01:55,756 
However, using such a capability
would require you to approach your 
21 
00:01:55,756 --> 00:01:58,580 
application and your work differently. 
22 
00:02:00,000 --> 00:02:05,090 
If you are using an activity tracker, you
should probably come up with a strategy 
23 
00:02:05,090 --> 00:02:09,540 
for how you will incorporate the usage of
these useful gadgets into your lifestyle. 
24 
00:02:10,780 --> 00:02:15,520 
Just like that, if you're planning to
incorporate Big Data driven insights into 
25 
00:02:15,520 --> 00:02:21,610 
your organization, you need to define
a new strategy, and a new way of working. 
26 
00:02:23,570 --> 00:02:29,184 
Most Big Data centric businesses have
updated their culture to be more real-time 
27 
00:02:29,184 --> 00:02:34,160 
action oriented, refining real-time
processes to handle anything from customer 
28 
00:02:34,160 --> 00:02:38,080 
relations and fraud detection,
to system monitoring and control. 
29 
00:02:39,200 --> 00:02:44,150 
In addition, such volumes of real-time
data and analytical operations that need 
30 
00:02:44,150 --> 00:02:49,880 
to take place requires an increased
use of scalable computing systems, 
31 
00:02:49,880 --> 00:02:53,870 
which need to be a part of the planning
for an organizational Big Data strategy. 
32 
00:02:55,700 --> 00:02:59,580 
They see affects of such
changes also in SCADA system. 
33 
00:03:00,920 --> 00:03:04,630 
SCADA stands for Supervisory Control and
Data Acquisition. 
34 
00:03:06,580 --> 00:03:11,710 
SCADA is a type of industrial control
system for remote monitoring and 
35 
00:03:11,710 --> 00:03:17,190 
control of industrial processes
that exists in the physical world, 
36 
00:03:17,190 --> 00:03:21,020 
potentially including multiple sites,
many types of sensors. 
37 
00:03:23,080 --> 00:03:28,410 
In addition to monitoring and control,
SCADA system can be used to define 
38 
00:03:28,410 --> 00:03:33,390 
actions for reduced waste and improved
efficiency in industrial processes, 
39 
00:03:33,390 --> 00:03:39,100 
including those of manufacturing and
power generation, public or 
40 
00:03:39,100 --> 00:03:43,820 
private infrastructure processes,
including water treatment, oil, and 
41 
00:03:43,820 --> 00:03:49,040 
gas pipelines, and
electrical power transmission, and 
42 
00:03:49,040 --> 00:03:54,300 
facility processes including buildings,
airports, ships, and space stations. 
43 
00:03:55,550 --> 00:04:00,240 
They can even be used in smart
building applications to monitor and 
44 
00:04:00,240 --> 00:04:05,910 
control heating, ventilation,
air conditioning systems like HVAC, 
45 
00:04:05,910 --> 00:04:08,030 
access, and energy consumption. 
46 
00:04:09,330 --> 00:04:14,040 
Again, the management of these processes
once the trends, patterns, and 
47 
00:04:14,040 --> 00:04:20,460 
anomalies are identified in real-time
needs to be decided in the Big Data case. 
48 
00:04:20,460 --> 00:04:26,960 
As a summary, as the largest and fastest
type of Big Data, machine generated 
49 
00:04:26,960 --> 00:04:31,690 
data can uniquely enable real-time
actions in many systems and processes. 
50 
00:04:32,740 --> 00:04:40,198 
However, a culture shift is needed for
its computing and real-time action. 
1 
00:00:00,910 --> 00:00:02,630 
Big data generated by machines. 
2 
00:00:03,680 --> 00:00:05,508 
It's everywhere and there's a lot. 
3 
00:00:16,258 --> 00:00:19,120 
Do big planes require big data? 
4 
00:00:19,120 --> 00:00:20,650 
Absolutely! 
5 
00:00:20,650 --> 00:00:24,770 
Did you know that a Boeing 787 produces 
6 
00:00:24,770 --> 00:00:27,450 
half a terabyte of data
every time it flies? 
7 
00:00:28,640 --> 00:00:33,680 
Really, almost every part of
the plane updates both the flight and 
8 
00:00:33,680 --> 00:00:35,990 
the ground team about
its status constantly. 
9 
00:00:37,105 --> 00:00:38,530 
Where's all this data coming from? 
10 
00:00:39,560 --> 00:00:43,360 
This is an example of machine-generated
data coming from sensors. 
11 
00:00:44,730 --> 00:00:47,730 
If you look at all sources of big data, 
12 
00:00:47,730 --> 00:00:51,470 
machine data is the largest
source of big data. 
13 
00:00:51,470 --> 00:00:54,850 
Additionally, it is very complex. 
14 
00:00:54,850 --> 00:01:00,960 
In general, we call machines that provide
some type of sensing capability smart. 
15 
00:01:00,960 --> 00:01:04,760 
Have you ever wondered why you
call your cell phone a smartphone? 
16 
00:01:05,890 --> 00:01:08,790 
Because it gives you a way
to track many things, 
17 
00:01:08,790 --> 00:01:12,740 
including your geolocation, and
connect you to other things. 
18 
00:01:13,930 --> 00:01:16,716 
So what makes a smart device smart, smart? 
19 
00:01:16,716 --> 00:01:22,110 
Generally speaking, There are three
main properties of smart devices 
20 
00:01:22,110 --> 00:01:27,200 
based on what they do with sensors and
things they encapsulate. 
21 
00:01:27,200 --> 00:01:29,480 
They can connect to other devices or 
22 
00:01:29,480 --> 00:01:35,490 
networks, they can execute services and
collect data autonomously, 
23 
00:01:35,490 --> 00:01:40,230 
that means on their own, they have
some knowledge of the environment. 
24 
00:01:41,550 --> 00:01:46,940 
The widespread availability of the smart
devices and their interconnectivity 
25 
00:01:46,940 --> 00:01:53,080 
led to a new term being coined,
The Internet of Things. 
26 
00:01:53,080 --> 00:01:58,450 
Think of a world of smart devices at home,
in your car, in the office, 
27 
00:01:58,450 --> 00:02:04,110 
city, remote rural areas,
the sky, even the ocean, 
28 
00:02:04,110 --> 00:02:07,340 
all connected and all generating data. 
29 
00:02:08,360 --> 00:02:12,590 
Let's look at an example of a device
that has some of these things in it. 
30 
00:02:14,350 --> 00:02:19,930 
An activity tracker is a device or
application for monitoring and 
31 
00:02:19,930 --> 00:02:25,790 
tracking fitness-related metrics
such as distance walked or run, 
32 
00:02:25,790 --> 00:02:32,670 
calorie consumption, and in some cases,
heartbeat and quality of sleep. 
33 
00:02:32,670 --> 00:02:37,140 
What if everyone in New York City
wore an activity tracker? 
34 
00:02:37,140 --> 00:02:40,350 
What if everyone wore several? 
35 
00:02:40,350 --> 00:02:45,300 
I personally have three activity
trackers that I use on a daily basis. 
36 
00:02:45,300 --> 00:02:50,200 
One to track my sleep, another one
to track my physical activity, and 
37 
00:02:50,200 --> 00:02:54,250 
a third, my smartphone,
that goes everywhere with me. 
38 
00:02:54,250 --> 00:03:00,030 
So it is not that unusual to imagine that
this will be the case for many people. 
39 
00:03:00,030 --> 00:03:05,320 
As you have already heard from in
a previous lecture on personalized data, 
40 
00:03:05,320 --> 00:03:08,760 
such activity trackers
have enabled a new way 
41 
00:03:08,760 --> 00:03:12,210 
of doing patient intervention
via personalized medicine. 
42 
00:03:13,670 --> 00:03:18,440 
Similarly, the sensors in planes have
generated a new way of looking at 
43 
00:03:18,440 --> 00:03:21,370 
fleet management and flight safety. 
44 
00:03:21,370 --> 00:03:26,055 
As a summary,
machines collect data 24/7 via 
45 
00:03:26,055 --> 00:03:31,980 
their built-in sensors,
both at personal and industrial scales. 
46 
00:03:31,980 --> 00:03:35,308 
And thus, they are the largest
of all the big data sources. 
1 
00:00:01,300 --> 00:00:02,963 
Organization-Generated Data. 
2 
00:00:02,963 --> 00:00:09,580 
Benefits come from combining
with other data types. 
3 
00:00:09,580 --> 00:00:12,380 
How are some organizations
benefiting from big data? 
4 
00:00:29,278 --> 00:00:31,844 
Let's look at real world examples to
see the advantages these organizations 
5 
00:00:31,844 --> 00:00:32,800 
are getting out of big data. 
6 
00:00:33,960 --> 00:00:36,120 
One of these companies is UPS. 
7 
00:00:37,720 --> 00:00:41,650 
UPS delivers 16 million shipments per day. 
8 
00:00:41,650 --> 00:00:46,360 
They get around 40 million
tracking requests. 
9 
00:00:46,360 --> 00:00:46,900 
That's huge. 
10 
00:00:48,270 --> 00:00:54,431 
An estimate of how much data UPS has
on its operations is 16 petabytes. 
11 
00:00:55,880 --> 00:01:00,580 
Can you guess how much money UPS can
save by reducing each driver's route 
12 
00:01:00,580 --> 00:01:01,620 
by just one mile? 
13 
00:01:03,410 --> 00:01:09,113 
If they can reduce distance traveled
by each truck by even one mile, 
14 
00:01:09,113 --> 00:01:13,830 
UPS can save a whopping $50 million U.S.
per year. 
15 
00:01:15,490 --> 00:01:19,010 
This is where big data steps in. 
16 
00:01:19,010 --> 00:01:22,610 
Utilizing complex optimization
over large datasets 
17 
00:01:22,610 --> 00:01:27,250 
can lead to route optimizations that were
previously not visible to the company. 
18 
00:01:29,110 --> 00:01:32,900 
Big data, together with smart processing, 
19 
00:01:32,900 --> 00:01:36,720 
enables UPS to manage thousands
of route optimizations. 
20 
00:01:37,900 --> 00:01:43,310 
Let's travel from package
delivery to the retail domain. 
21 
00:01:43,310 --> 00:01:46,870 
An organization from
the retail shopping domain 
22 
00:01:46,870 --> 00:01:49,760 
that heavily utilizes big data is Walmart. 
23 
00:01:51,590 --> 00:01:58,220 
Walmart is a big organization that gets
250 million customers in 10,000 stores. 
24 
00:02:00,820 --> 00:02:07,570 
Did you know they collect 2.5
petabytes of data per hour? 
25 
00:02:09,280 --> 00:02:14,699 
They collect data on Twitter tweets,
local events, local weather, 
26 
00:02:14,699 --> 00:02:19,640 
in-store purchases, online clicks and 
27 
00:02:19,640 --> 00:02:23,530 
many other sales, customer and
product related data. 
28 
00:02:25,070 --> 00:02:31,080 
They use this data to find patterns
such as which products are frequently 
29 
00:02:31,080 --> 00:02:36,878 
purchased together, and what is the best
new product to introduce in their stores, 
30 
00:02:36,878 --> 00:02:41,530 
to predict demand at
the particular location, 
31 
00:02:43,670 --> 00:02:47,898 
and to customize customer recommendations. 
32 
00:02:47,898 --> 00:02:50,430 
Overall, by leveraging big data and 
33 
00:02:50,430 --> 00:02:55,390 
analytics, Walmart has maintained
its position as a top retailer. 
34 
00:02:56,650 --> 00:03:02,130 
UPS and Walmart examples were just two out
of a number of companies using big data. 
35 
00:03:03,380 --> 00:03:07,760 
Big data is producing results for
companies in all sectors. 
36 
00:03:09,670 --> 00:03:14,370 
Studies forecast spending on
big data technologies to go up 
37 
00:03:14,370 --> 00:03:16,630 
drastically in the next five years. 
38 
00:03:18,320 --> 00:03:25,280 
A study by Bane and Company
suggests that early adopters of big 
39 
00:03:25,280 --> 00:03:30,650 
data analytics have gained a significant
lead over the rest of the corporate world. 
40 
00:03:32,620 --> 00:03:34,430 
For graphics referenced here, 
41 
00:03:35,450 --> 00:03:40,700 
we see that companies that use
analytics are twice as likely 
42 
00:03:40,700 --> 00:03:45,160 
to be in the top quartile of financial
performance within their industries. 
43 
00:03:46,330 --> 00:03:51,368 
Five times as likely to make decisions
much faster than market peers. 
44 
00:03:52,440 --> 00:03:56,180 
Three times as likely to
execute decisions as intended. 
45 
00:03:56,180 --> 00:04:02,880 
And twice as likely to use data very
frequently when making decisions. 
46 
00:04:04,040 --> 00:04:06,610 
This points to the growth and
demand of people and 
47 
00:04:06,610 --> 00:04:12,450 
technology centered around or
specializing in big data applications. 
48 
00:04:12,450 --> 00:04:17,870 
As a summary, organizations are gaining
significant benefit from integrating 
49 
00:04:17,870 --> 00:04:22,920 
big data practices into their culture and
breaking their silos. 
50 
00:04:22,920 --> 00:04:28,080 
Some major benefits to organizations are
operational efficiency, improved marketing 
51 
00:04:28,080 --> 00:04:32,630 
outcomes, higher profits, and
improved customer satisfaction. 
1 
00:00:00,870 --> 00:00:06,650 
Big data, generated by organizations,
structured but often siloed. 
2 
00:00:06,650 --> 00:00:11,930 
The last type of big data we will discuss
is big data generated by organizations. 
3 
00:00:13,260 --> 00:00:18,740 
This type of data is the closest to
what most businesses currently have. 
4 
00:00:18,740 --> 00:00:21,550 
But it's considered a bit out of fashion,
or 
5 
00:00:21,550 --> 00:00:25,180 
traditional, compared to
other types of big data. 
6 
00:00:25,180 --> 00:00:29,760 
However, it is at least as important
as other types of big data. 
7 
00:00:47,566 --> 00:00:51,350 
So how do organizations produce data? 
8 
00:00:51,350 --> 00:00:54,940 
The answer to how
an organization generates data 
9 
00:00:54,940 --> 00:00:57,260 
is very unique to the organization and
context. 
10 
00:00:58,590 --> 00:01:02,540 
Each organization has distinct
operation practices and 
11 
00:01:02,540 --> 00:01:07,860 
business models, which result in
a variety of data generation platforms. 
12 
00:01:07,860 --> 00:01:12,697 
For example, the type and source of data
that a bank gets is very different from 
13 
00:01:12,697 --> 00:01:15,650 
what a hardware equipment
manufacturer gets. 
14 
00:01:17,610 --> 00:01:23,808 
Some common types of organizational big
data come from commercial transactions, 
15 
00:01:23,808 --> 00:01:29,033 
credit cards, government institutions,
e-commerce, banking or 
16 
00:01:29,033 --> 00:01:35,070 
stock records, medical records, sensors,
transactions, clicks and so on. 
17 
00:01:36,350 --> 00:01:39,930 
Almost every event can
be potentially stored. 
18 
00:01:41,500 --> 00:01:45,320 
Organizations store this data for
current and 
19 
00:01:45,320 --> 00:01:49,040 
future use, as well as for
analysis of the past. 
20 
00:01:50,380 --> 00:01:54,970 
Let's say you're an organization
that collects sales transactions. 
21 
00:01:54,970 --> 00:01:59,500 
You can use this data for pattern
recognition to detect correlated products, 
22 
00:02:00,530 --> 00:02:05,990 
to estimate demand for
products likely to go up in sales, and 
23 
00:02:05,990 --> 00:02:07,780 
capture fraudulent activity. 
24 
00:02:08,930 --> 00:02:14,560 
Moreover, when you know your
sales record and can correlate it 
25 
00:02:14,560 --> 00:02:20,370 
with your marketing records, you can find
which campaigns really made an impact. 
26 
00:02:20,370 --> 00:02:24,340 
You are already becoming
a data savvy organization. 
27 
00:02:24,340 --> 00:02:29,180 
Now think of bringing your
sales data together with other 
28 
00:02:29,180 --> 00:02:34,680 
external open public data,
such as major world events in the news. 
29 
00:02:34,680 --> 00:02:38,420 
You can ask, was it savvy marketing or 
30 
00:02:38,420 --> 00:02:43,310 
a consequence of external events
that triggered your sales? 
31 
00:02:43,310 --> 00:02:45,037 
Using proper analytics, 
32 
00:02:45,037 --> 00:02:50,800 
you can now build inventories to match
your predicted growth and demand. 
33 
00:02:50,800 --> 00:02:57,060 
In addition, organizations build and
apply processes to record and 
34 
00:02:57,060 --> 00:03:02,310 
monitor business events of interest,
such as registering a customer, 
35 
00:03:02,310 --> 00:03:05,116 
manufacturing a product or
taking an order. 
36 
00:03:05,116 --> 00:03:10,930 
These processes collect
highly structured data 
37 
00:03:10,930 --> 00:03:14,460 
that include transactions,
reference tables, and 
38 
00:03:14,460 --> 00:03:19,810 
relationships, as well as
the metadata that sets its context. 
39 
00:03:21,130 --> 00:03:26,580 
Usually, structured data is stored in
relational database management systems. 
40 
00:03:28,030 --> 00:03:31,805 
However, we call any data
that is in the form of 
41 
00:03:31,805 --> 00:03:36,708 
a record located in a fixed field or
file structured data. 
42 
00:03:36,708 --> 00:03:40,300 
This definition also
includes spreadsheets. 
43 
00:03:41,750 --> 00:03:46,490 
As I have mentioned before,
traditionally this type of 
44 
00:03:46,490 --> 00:03:51,070 
highly structured data is the vast
majority of what IT managed and 
45 
00:03:51,070 --> 00:03:55,420 
processed in both operational and
business intelligence systems. 
46 
00:03:56,750 --> 00:04:00,410 
Let's look at the sales transaction
data in our previous example. 
47 
00:04:01,890 --> 00:04:05,575 
If you look at the data in
the relational table on the right. 
48 
00:04:05,575 --> 00:04:09,687 
As the name structured suggests, 
49 
00:04:09,687 --> 00:04:14,649 
the table is organized to store data using 
50 
00:04:14,649 --> 00:04:18,493 
a structure defined by a model. 
51 
00:04:18,493 --> 00:04:24,450 
Each column is tagged to tell us what
data that column intends to store. 
52 
00:04:24,450 --> 00:04:26,950 
This is what we call a data model. 
53 
00:04:28,310 --> 00:04:31,680 
A data model defines
each of these columns and 
54 
00:04:31,680 --> 00:04:37,290 
fields in the table, and
defines relationships between them. 
55 
00:04:37,290 --> 00:04:43,960 
If you look at the product ID column,
you will see that it includes only 
56 
00:04:43,960 --> 00:04:48,940 
identifiers that can potentially be linked
to another table defining these products. 
57 
00:04:50,350 --> 00:04:55,990 
The ability to define such relationships,
easily make structured data, 
58 
00:04:55,990 --> 00:05:01,220 
or in this case relational databases,
highly adopted by many organizations. 
59 
00:05:02,460 --> 00:05:06,330 
There are commonly used
languages like SQL, 
60 
00:05:06,330 --> 00:05:11,600 
the Structured Query Language, to extract
data of interest from such tables. 
61 
00:05:12,690 --> 00:05:15,800 
This is referred to as querying the data. 
62 
00:05:17,160 --> 00:05:23,060 
However, it could even be a challenge
to integrate such structured data. 
63 
00:05:23,060 --> 00:05:28,460 
This image shows us a continuum
of technologies to model, 
64 
00:05:28,460 --> 00:05:32,880 
collect and query unstructured
data coming from software and 
65 
00:05:32,880 --> 00:05:35,180 
hardware components
within an organization. 
66 
00:05:36,400 --> 00:05:41,150 
In the past, such challenges
led to information being stored 
67 
00:05:41,150 --> 00:05:45,050 
in what we call silos,
even within an organization. 
68 
00:05:46,400 --> 00:05:51,700 
Many organizations have traditionally
captured data at the department level, 
69 
00:05:51,700 --> 00:05:56,950 
without proper infrastructure and
policy to share and integrate this data. 
70 
00:05:56,950 --> 00:06:01,550 
This has hindered the growth of
scalable pattern recognition 
71 
00:06:01,550 --> 00:06:04,510 
to the benefits of
the entire organization. 
72 
00:06:04,510 --> 00:06:08,700 
Because no one system has access to
all data that the organization owns. 
73 
00:06:10,480 --> 00:06:14,080 
Each data set is compartmentalized. 
74 
00:06:14,080 --> 00:06:20,320 
If such silos are left untouched,
organizations risk having outdated, 
75 
00:06:20,320 --> 00:06:24,320 
unsynchronized, and
even invisible data sets. 
76 
00:06:25,570 --> 00:06:29,260 
Organizations are realizing
the detrimental outcomes of this rigid 
77 
00:06:29,260 --> 00:06:30,820 
structure. 
78 
00:06:30,820 --> 00:06:36,270 
And changing policies and infrastructure
to enable integrated processing of 
79 
00:06:36,270 --> 00:06:39,025 
all data to the entire
organization's benefit. 
80 
00:06:39,025 --> 00:06:44,090 
Cloud-based solutions
are seen as agile and 
81 
00:06:44,090 --> 00:06:47,550 
low capital intensive
solutions in this area. 
82 
00:06:47,550 --> 00:06:53,309 
As a summary, while highly structured
organizational data is very useful and 
83 
00:06:53,309 --> 00:06:57,671 
trustworthy, and
thus a valuable source of information, 
84 
00:06:57,671 --> 00:07:01,860 
organizations must pay special
attention to breaking up 
85 
00:07:01,860 --> 00:07:06,243 
the silos of information to
make full use of its potential. 
1 
00:00:00,660 --> 00:00:03,542 
The key, integrating diverse data. 
2 
00:00:13,178 --> 00:00:16,267 
Whatever your big data application is, and 
3 
00:00:16,267 --> 00:00:21,527 
the types of big data you are using
the real value will come from integrating 
4 
00:00:21,527 --> 00:00:26,056 
different types of data sources,
and analyzing them at scale. 
5 
00:00:26,056 --> 00:00:28,947 
So how do we start getting this value? 
6 
00:00:28,947 --> 00:00:30,706 
Sometimes, all it takes, 
7 
00:00:30,706 --> 00:00:35,620 
is looking at the data you already
collect in a different way. 
8 
00:00:35,620 --> 00:00:39,060 
And it can mean a big difference
in your return on investment. 
9 
00:00:40,590 --> 00:00:45,970 
This new story from June 2015
mentions that Carnival Cruises 
10 
00:00:45,970 --> 00:00:50,760 
is using structured and unstructured
data from a variety of sources. 
11 
00:00:52,420 --> 00:00:55,200 
Carnival turns it into profit 
12 
00:00:55,200 --> 00:00:58,640 
using price optimization
techniques on the integrated data. 
13 
00:00:59,640 --> 00:01:02,460 
For you to achieve such a success story, 
14 
00:01:02,460 --> 00:01:07,030 
you will need to include data
integration into your big data practice. 
15 
00:01:07,030 --> 00:01:11,950 
However, there are some unique challenges
when attempting to integrate these 
16 
00:01:11,950 --> 00:01:16,120 
diverse data sources and
scaling the solutions. 
17 
00:01:16,120 --> 00:01:20,220 
Course Two on on the specialization,
we'll teach you more about these issues. 
18 
00:01:21,260 --> 00:01:26,010 
But let's take a moment to define why
effect of data integration is useful? 
19 
00:01:27,060 --> 00:01:33,040 
Data integration means bringing
together data from diverse sources and 
20 
00:01:33,040 --> 00:01:37,170 
turning them into coherent and
more useful information. 
21 
00:01:38,440 --> 00:01:40,390 
We also call this knowledge. 
22 
00:01:41,740 --> 00:01:47,970 
The main objective here is taming or
more technically managing data and 
23 
00:01:47,970 --> 00:01:52,520 
turning it into something you can
make use of programmatically. 
24 
00:01:52,520 --> 00:01:57,460 
A data integration process
involves many parts. 
25 
00:01:57,460 --> 00:02:03,250 
It starts with discovering,
accessing, and monitoring data and 
26 
00:02:03,250 --> 00:02:09,340 
continues with modeling and transforming
data from a variety of sources. 
27 
00:02:09,340 --> 00:02:13,160 
But why do we need data
integration in the first place? 
28 
00:02:13,160 --> 00:02:16,900 
Let's start by focusing on
differences between big data sets 
29 
00:02:16,900 --> 00:02:18,140 
coming from different sources. 
30 
00:02:19,500 --> 00:02:24,640 
You might have flat file formatted data,
relational database data, 
31 
00:02:24,640 --> 00:02:31,470 
data encoded in XML or JSON,
both common for internet generated data. 
32 
00:02:31,470 --> 00:02:35,170 
These different formats and
models are useful 
33 
00:02:35,170 --> 00:02:38,690 
because they are designed to express
different data in unique ways. 
34 
00:02:39,880 --> 00:02:42,560 
In a way, different data formats and 
35 
00:02:42,560 --> 00:02:49,100 
models make big data more useful and
more challenging all at the same time. 
36 
00:02:49,100 --> 00:02:53,700 
When you integrate data in different
formats, you make the final 
37 
00:02:53,700 --> 00:02:58,030 
product richer in the number of
features you describe the data with. 
38 
00:02:58,030 --> 00:03:03,300 
For example,
by integrating environmental sensor and 
39 
00:03:03,300 --> 00:03:08,300 
camera data with geographical
information system data, such as in 
40 
00:03:08,300 --> 00:03:14,178 
my wildfire prediction application,
I can use the spacial data capabilities 
41 
00:03:14,178 --> 00:03:19,970 
with non-spacial data to more
accurately run fire simulations. 
42 
00:03:21,230 --> 00:03:25,920 
In the past, although we were able
to see the images of the fire 
43 
00:03:27,280 --> 00:03:31,630 
from mountain top cameras,
just like this image, we were still 
44 
00:03:31,630 --> 00:03:35,880 
not able to tell what the exact
location of the fire is automatically. 
45 
00:03:37,460 --> 00:03:41,000 
Now, when a fire is detected
from a mountain top camera, 
46 
00:03:42,180 --> 00:03:47,640 
viewsheds are used to estimate
the location of the fire. 
47 
00:03:49,490 --> 00:03:54,760 
This location information can
be fed into the fire simulator 
48 
00:03:54,760 --> 00:03:58,070 
as early as it's detected
to predict the size and 
49 
00:03:58,070 --> 00:04:02,090 
location of the fire in the next
hour more accurately and faster. 
50 
00:04:03,730 --> 00:04:08,920 
Similarly, I can use real time
data with eye curve data sets, and 
51 
00:04:08,920 --> 00:04:09,940 
use them all together. 
52 
00:04:10,980 --> 00:04:15,520 
Additionally, by bringing
the data together, and 
53 
00:04:15,520 --> 00:04:22,100 
providing programmable access to it, I'm
now making each data set more accessible. 
54 
00:04:23,200 --> 00:04:28,020 
Moreover, integration of
diverse datasets significantly 
55 
00:04:28,020 --> 00:04:32,220 
reduces the overall data complexity
in my data-driven product. 
56 
00:04:33,320 --> 00:04:39,610 
The data becomes more available for
use and unified as a system of its own. 
57 
00:04:40,860 --> 00:04:45,010 
One advantage of such an integration
is not often mentioned. 
58 
00:04:46,500 --> 00:04:49,830 
Such a streamlined and
integrated data system 
59 
00:04:49,830 --> 00:04:53,550 
can increase the collaboration between
different parts of your data systems. 
60 
00:04:54,930 --> 00:04:58,430 
Each part can now clearly see 
61 
00:04:58,430 --> 00:05:02,000 
how their data is integrated
into the overall system. 
62 
00:05:02,000 --> 00:05:07,860 
Including the user scenarios and the
security and privacy processes around it. 
63 
00:05:09,080 --> 00:05:16,320 
Overall by integrating diverse data
streams you add value to your big data and 
64 
00:05:16,320 --> 00:05:20,320 
improve your business even
before you start analyzing it. 
65 
00:05:21,640 --> 00:05:25,783 
Next, we will focus on the dimensions
of the scalability and 
66 
00:05:25,783 --> 00:05:29,937 
discuss how we can start tackling
some of these challenges. 
1 
00:00:01,310 --> 00:00:03,380 
What launched the big data era? 
2 
00:00:04,730 --> 00:00:09,092 
In this video, you will learn about
two new opportunities that have 
3 
00:00:09,092 --> 00:00:12,088 
contributed to the launch
of the big data era. 
4 
00:00:27,737 --> 00:00:32,070 
Opportunities are often
a signal of changing times. 
5 
00:00:34,110 --> 00:00:40,100 
In 2013, an influential report by
a company called McKinsey claimed that 
6 
00:00:40,100 --> 00:00:45,900 
the area of data science will be the
number one catalyst for economic growth. 
7 
00:00:47,380 --> 00:00:50,320 
McKinsey identified one of our new 
8 
00:00:50,320 --> 00:00:54,366 
opportunities that contributed to
the launch of the big data era. 
9 
00:00:54,366 --> 00:00:59,190 
A growing torrent of data. 
10 
00:00:59,190 --> 00:01:05,150 
This refers to the idea that data seems to
be coming continuously and at a fast rate. 
11 
00:01:06,340 --> 00:01:11,330 
Think about this,
today you can buy a hard drive to store 
12 
00:01:11,330 --> 00:01:15,624 
all the music in the world for only $600. 
13 
00:01:16,710 --> 00:01:24,600 
That's an amazing storage capability over
any previous forms of music storage. 
14 
00:01:24,600 --> 00:01:29,286 
In 2010 there were 5 billion
mobile phones in use. 
15 
00:01:29,286 --> 00:01:33,018 
You can be sure that
there are more today and 
16 
00:01:33,018 --> 00:01:37,476 
as I'm sure you will understand,
these phones and 
17 
00:01:37,476 --> 00:01:42,244 
the apps we install on them
are a big source of big data, 
18 
00:01:42,244 --> 00:01:47,342 
which all the time, every day,
contributes to our core. 
19 
00:01:47,342 --> 00:01:50,444 
And Facebook, which recently just set 
20 
00:01:50,444 --> 00:01:55,144 
a record of having one billion
people login in a single day, 
21 
00:01:55,144 --> 00:02:00,320 
has more that 30 billion pieces
of content shared every month. 
22 
00:02:01,430 --> 00:02:03,620 
Well, that number's from 2013. 
23 
00:02:03,620 --> 00:02:07,310 
So I'm sure that it's much
higher than that now. 
24 
00:02:08,720 --> 00:02:11,950 
Does it make you think how many
Facebook share's you made last month? 
25 
00:02:13,760 --> 00:02:17,105 
All this leads to projections
of serious growth. 
26 
00:02:17,105 --> 00:02:24,860 
40% in global data per year,
and 5% in global IT spending. 
27 
00:02:24,860 --> 00:02:29,080 
This much data has sure
pushed the data science field 
28 
00:02:29,080 --> 00:02:32,420 
to start remaining itself and
the business world of today. 
29 
00:02:33,450 --> 00:02:38,970 
But, there's something else contributing
to the catalyzing power of data science. 
30 
00:02:38,970 --> 00:02:41,820 
It is called cloud computing. 
31 
00:02:41,820 --> 00:02:44,600 
We call this on demand computing. 
32 
00:02:44,600 --> 00:02:48,480 
Cloud computing is one of
the ways in which computing 
33 
00:02:48,480 --> 00:02:53,220 
has now become something that
we ca do anytime, and anywhere. 
34 
00:02:53,220 --> 00:02:56,170 
You may be surprised to know
that some of your favorite 
35 
00:02:56,170 --> 00:02:59,900 
apps are from businesses
being run from coffee shops. 
36 
00:02:59,900 --> 00:03:04,420 
This new ability,
combined with our torrent of data, 
37 
00:03:04,420 --> 00:03:10,220 
gives us the opportunity to perform novel,
dynamic and scalable data analysis, 
38 
00:03:10,220 --> 00:03:13,824 
to tell us new things about our world and
ourself. 
39 
00:03:13,824 --> 00:03:20,037 
To summarize, a new torrent of big data
combined with computing capability 
40 
00:03:20,037 --> 00:03:25,968 
anytime, anywhere has been at the core
of the launch of the big data era. 
1 
00:00:01,080 --> 00:00:02,360 
Asking the Right Questions. 
2 
00:00:17,893 --> 00:00:23,810 
The first step in any process is to define
what it is you are trying to tackle. 
3 
00:00:25,030 --> 00:00:27,975 
What is the problem that
needs to be addressed, or 
4 
00:00:27,975 --> 00:00:30,240 
the opportunity that
needs to be ascertained. 
5 
00:00:31,280 --> 00:00:35,223 
Without this,
you won't have a clear goal in mind, or 
6 
00:00:35,223 --> 00:00:38,153 
know when you've solved your problem. 
7 
00:00:38,153 --> 00:00:42,833 
An example question is,
how can sales figures and 
8 
00:00:42,833 --> 00:00:48,072 
call center logs be combined
to evaluate a new product, 
9 
00:00:48,072 --> 00:00:53,534 
or in a manufacturing process,
how can data from multiple 
10 
00:00:53,534 --> 00:00:59,573 
sensors in an instrument be used
to detect instrument failure? 
11 
00:00:59,573 --> 00:01:02,745 
How can we understand our customers and 
12 
00:01:02,745 --> 00:01:07,273 
market better to achieve
effective target marketing? 
13 
00:01:07,273 --> 00:01:11,320 
Next you need to assess the situation
with respect to the problem or 
14 
00:01:11,320 --> 00:01:14,730 
the opportunity you have defined. 
15 
00:01:14,730 --> 00:01:20,011 
This is a step where you need to
exercise caution analyzing risks, 
16 
00:01:20,011 --> 00:01:23,609 
costs, benefits, contingencies, 
17 
00:01:23,609 --> 00:01:28,650 
regulations, resources and
requirements of the situation. 
18 
00:01:28,650 --> 00:01:30,910 
What are the requirements of the problem? 
19 
00:01:30,910 --> 00:01:33,850 
What are the assumptions and constraints? 
20 
00:01:33,850 --> 00:01:35,450 
What resources are available? 
21 
00:01:35,450 --> 00:01:39,614 
This is in terms of both personnel and
capital, 
22 
00:01:39,614 --> 00:01:43,685 
such as computer systems, instruments etc. 
23 
00:01:43,685 --> 00:01:47,425 
What are the main costs
associated with this project? 
24 
00:01:47,425 --> 00:01:49,335 
What are the potential benefits? 
25 
00:01:49,335 --> 00:01:52,625 
What risks are there in
pursuing the project? 
26 
00:01:52,625 --> 00:01:57,125 
What are the contingencies to
potential risks, and so on? 
27 
00:01:57,125 --> 00:02:02,570 
Answers to these questions will help you
get a better overview of the situation. 
28 
00:02:02,570 --> 00:02:05,700 
And better understanding of
what the project involves. 
29 
00:02:05,700 --> 00:02:10,172 
Then you need to define your goals and
objectives, 
30 
00:02:10,172 --> 00:02:13,813 
based on the answers to these questions. 
31 
00:02:13,813 --> 00:02:17,850 
Defining success criteria
is also very important. 
32 
00:02:17,850 --> 00:02:20,770 
What do you hope to achieve
by the end of this project? 
33 
00:02:20,770 --> 00:02:22,368 
Having clear goals and and 
34 
00:02:22,368 --> 00:02:28,280 
success criteria will help you to assess
the project throughout its life cycle. 
35 
00:02:28,280 --> 00:02:32,780 
Once you know the problem you want to
address and understand the constraints and 
36 
00:02:32,780 --> 00:02:37,990 
goals, then you can formulate
the plan to come up with the answer, 
37 
00:02:37,990 --> 00:02:41,330 
that is the solution to
your business problem. 
38 
00:02:41,330 --> 00:02:46,920 
As a summary, defining the questions
you're looking to find answers for 
39 
00:02:46,920 --> 00:02:51,250 
is a huge factor contributing to
the success of a data science project. 
40 
00:02:52,260 --> 00:02:56,830 
By following the explained set of steps,
you can formulate better questions 
41 
00:02:56,830 --> 00:03:01,500 
to solve using analytical skills and
link them to business value. 
1 
00:00:03,028 --> 00:00:04,788 
Building a Big Data Strategy. 
2 
00:00:23,698 --> 00:00:29,760 
Before we focus on big data strategy,
let's look at what strategy means. 
3 
00:00:30,790 --> 00:00:33,090 
Although it is associated
with a military term, 
4 
00:00:34,600 --> 00:00:39,490 
a dictionary search on strategy shows
the meaning as a plan of action or 
5 
00:00:39,490 --> 00:00:43,820 
policy designed to achieve a major or
overall aim. 
6 
00:00:46,200 --> 00:00:51,990 
This definition calls out the four major
parts that need to be in any strategy. 
7 
00:00:51,990 --> 00:00:58,110 
Namely, aim, policy, plan, and action. 
8 
00:00:59,280 --> 00:01:02,620 
Now, we are talking about
a big data strategy. 
9 
00:01:02,620 --> 00:01:05,770 
So what do these four terms mean for us? 
10 
00:01:05,770 --> 00:01:10,382 
When building our big data strategy,
we look at what we have, 
11 
00:01:10,382 --> 00:01:15,793 
what high level goals we want to achieve,
what we need to do to get there, 
12 
00:01:15,793 --> 00:01:20,878 
and what are the policies around
data from the beginning to the end. 
13 
00:01:24,568 --> 00:01:29,210 
A big data strategy starts
with big objectives. 
14 
00:01:29,210 --> 00:01:33,939 
Notice that I didn't say it
starts with collecting data 
15 
00:01:33,939 --> 00:01:39,171 
because in this activity we
are really trying to identify what 
16 
00:01:39,171 --> 00:01:44,118 
data is useful and
why by focusing on what data to collect. 
17 
00:01:44,118 --> 00:01:48,410 
Every organization or team is unique. 
18 
00:01:48,410 --> 00:01:51,120 
Different projects have
different objectives. 
19 
00:01:51,120 --> 00:01:55,960 
Hence, it's important to first
define what your team's goals are. 
20 
00:01:55,960 --> 00:02:00,570 
Have you ever had the scenario where you
see the temperature on the weather report 
21 
00:02:00,570 --> 00:02:03,350 
and someone else highlights
the humidity instead? 
22 
00:02:04,500 --> 00:02:07,820 
To find problems relevant to solve and 
23 
00:02:07,820 --> 00:02:13,290 
data related to it, it might be
useful to start with your objectives. 
24 
00:02:13,290 --> 00:02:15,500 
Once you define these objectives, or 
25 
00:02:15,500 --> 00:02:21,030 
more generally speaking, questions to turn
big data into advantage for your business, 
26 
00:02:21,030 --> 00:02:25,450 
you can look at what you have and
analyze the gaps and actions to get there. 
27 
00:02:26,940 --> 00:02:30,192 
It is important to focus
on both short term and 
28 
00:02:30,192 --> 00:02:33,108 
long term objectives in this activity. 
29 
00:02:33,108 --> 00:02:37,791 
These objectives should also be linked
to big data analytics with business 
30 
00:02:37,791 --> 00:02:38,720 
objectives. 
31 
00:02:38,720 --> 00:02:43,620 
To make the best use of big data,
each company needs to evaluate how 
32 
00:02:43,620 --> 00:02:48,220 
data science or big data analytics would
add value to their business objectives. 
33 
00:02:50,010 --> 00:02:53,730 
Once you have established that
analytics can help your business, 
34 
00:02:53,730 --> 00:02:57,550 
you need to create
a culture to embrace it. 
35 
00:02:57,550 --> 00:02:59,820 
The first and foremost ingredient for 
36 
00:02:59,820 --> 00:03:04,790 
a successful data science program
is organizational buy-in. 
37 
00:03:04,790 --> 00:03:07,810 
A big data strategy must
have commitment and 
38 
00:03:07,810 --> 00:03:11,010 
sponsorship from the company's leadership. 
39 
00:03:11,010 --> 00:03:16,930 
Goals for using big data analytics should
be developed with all stakeholders and 
40 
00:03:16,930 --> 00:03:20,640 
clearly communicated to
everyone in the organization. 
41 
00:03:20,640 --> 00:03:24,740 
So that its value is understood and
appreciated by all. 
42 
00:03:27,350 --> 00:03:30,790 
The next step is to build
your data science team. 
43 
00:03:32,160 --> 00:03:36,930 
A diverse team with data scientists,
information technologists, 
44 
00:03:36,930 --> 00:03:42,520 
application developers, and business
owners is necessary to be effective. 
45 
00:03:42,520 --> 00:03:46,830 
As well as the mentality that everyone
works together as partners with 
46 
00:03:46,830 --> 00:03:47,950 
common goals. 
47 
00:03:47,950 --> 00:03:48,970 
Remember, one for all. 
48 
00:03:50,690 --> 00:03:54,960 
No one is a customer or
service provider of another. 
49 
00:03:54,960 --> 00:03:59,360 
Rather, everyone works together and
delivers as a team. 
50 
00:04:02,470 --> 00:04:05,930 
Since big data is a team game,
and multi-disciplinary, 
51 
00:04:05,930 --> 00:04:10,180 
a big part of a big data
strategy is constant training 
52 
00:04:10,180 --> 00:04:13,400 
of team members on new big data tools and
analytics. 
53 
00:04:13,400 --> 00:04:17,050 
As well as business practices and
objectives. 
54 
00:04:17,050 --> 00:04:22,095 
This becomes even more critical if your
business depends on deep expertise 
55 
00:04:22,095 --> 00:04:27,381 
on one or more subject areas with subject
matter experts working on problems, 
56 
00:04:27,381 --> 00:04:28,838 
utilizing big data. 
57 
00:04:31,947 --> 00:04:37,138 
Such businesses might have subject matter
experts who can be trained to add big 
58 
00:04:37,138 --> 00:04:42,730 
data skills, and provide more value added
support than a newcomer would have. 
59 
00:04:42,730 --> 00:04:47,024 
Similarly, any project member would
be trained to understand what 
60 
00:04:47,024 --> 00:04:50,564 
the business objectives and
products are, and how he or 
61 
00:04:50,564 --> 00:04:55,478 
she can utilize big data to improve those
objectives using his or her skills. 
62 
00:04:58,218 --> 00:05:04,186 
Many organizations might benefit by having
a small data science team whose main job 
63 
00:05:04,186 --> 00:05:10,000 
is do data experiments and test new ideas
before they get deployed at full scale. 
64 
00:05:12,740 --> 00:05:15,200 
They might come up with
a new idea themselves 
65 
00:05:15,200 --> 00:05:16,990 
based on the analysis they perform. 
66 
00:05:18,190 --> 00:05:20,220 
They take more research level role. 
67 
00:05:21,240 --> 00:05:26,010 
However, their findings can drastically
shape your business strategy 
68 
00:05:26,010 --> 00:05:27,540 
almost on a daily basis. 
69 
00:05:29,060 --> 00:05:32,620 
The impact of such teams
becomes evident over time 
70 
00:05:32,620 --> 00:05:36,950 
as other parts of your organization starts
to see the results of their finding and 
71 
00:05:36,950 --> 00:05:38,790 
analysis affecting their strategies. 
72 
00:05:40,110 --> 00:05:45,550 
They become strategic partners of
all verticals in your business. 
73 
00:05:45,550 --> 00:05:47,820 
Once you see that something works, 
74 
00:05:47,820 --> 00:05:52,210 
you can start collecting more data to see
similar results at organizational scale. 
75 
00:05:55,270 --> 00:06:00,330 
Since data is key to any big
data initiative, it is essential 
76 
00:06:00,330 --> 00:06:04,870 
that data across the organization
is easily accessed and integrated. 
77 
00:06:06,280 --> 00:06:10,700 
Data silos as you know, are like
a death knell on effective analytics. 
78 
00:06:12,820 --> 00:06:16,470 
So barriers to data
access must be removed. 
79 
00:06:16,470 --> 00:06:21,430 
Opening up the silos must be encouraged
and supported from the organization's 
80 
00:06:21,430 --> 00:06:26,020 
leaders in order to promote a data
sharing mindset for the company. 
81 
00:06:28,360 --> 00:06:31,900 
Another aspect of defining
your big data strategy 
82 
00:06:31,900 --> 00:06:34,700 
is defining the policies around big data. 
83 
00:06:35,810 --> 00:06:40,190 
Although it has an amazing amount
of potential for your business, 
84 
00:06:40,190 --> 00:06:44,540 
using big data should also raise some
concerns in long term planning for data. 
85 
00:06:45,870 --> 00:06:49,270 
Although this is a very complex issue, 
86 
00:06:49,270 --> 00:06:53,230 
here are some questions you should
think of addressing around policy. 
87 
00:06:53,230 --> 00:06:55,350 
What are the privacy concerns? 
88 
00:06:55,350 --> 00:06:58,770 
Who should have access to,
or control data? 
89 
00:06:58,770 --> 00:07:04,190 
What is the lifetime of data,
which is sometimes defined as volatility, 
90 
00:07:04,190 --> 00:07:05,290 
anatomy of big data? 
91 
00:07:06,970 --> 00:07:09,890 
How does data get curated and cleaned up? 
92 
00:07:11,780 --> 00:07:14,320 
What ensures data quality
in the long term? 
93 
00:07:15,570 --> 00:07:19,530 
How do different parts of your
organization communicate or 
94 
00:07:19,530 --> 00:07:21,400 
interoperate using this data? 
95 
00:07:22,440 --> 00:07:26,070 
Are there any legal and
regulatory standards in place? 
96 
00:07:28,295 --> 00:07:32,270 
Cultivating an analytics
driven culture is crucial 
97 
00:07:32,270 --> 00:07:34,600 
to the success of a big data strategy. 
98 
00:07:36,090 --> 00:07:39,690 
The mindset that you want to
establish is that analytics 
99 
00:07:39,690 --> 00:07:44,200 
is an integral part of doing business,
not a separate afterthought. 
100 
00:07:45,550 --> 00:07:50,170 
Analytics activities must be tied
to your business objectives, and 
101 
00:07:50,170 --> 00:07:53,480 
you must be willing to use analytics
in driving business decisions. 
102 
00:07:54,790 --> 00:07:59,598 
Analytics and business together bring
about exciting opportunities and 
103 
00:07:59,598 --> 00:08:01,750 
growth to your big data strategy. 
104 
00:08:04,980 --> 00:08:08,570 
Finally, one size does not fit all. 
105 
00:08:08,570 --> 00:08:10,780 
Hence, big data technologies and 
106 
00:08:10,780 --> 00:08:15,490 
analytics is growing rapidly as your
business is an evolving entity. 
107 
00:08:16,740 --> 00:08:21,180 
You have to iterate your strategy to
take advantage of new advances and 
108 
00:08:21,180 --> 00:08:24,310 
also make your business more
dynamic in the face of change. 
109 
00:08:27,300 --> 00:08:31,380 
As a summary,
when building a big data strategy, 
110 
00:08:31,380 --> 00:08:35,130 
it is important to integrate big data
analytics with business objectives. 
111 
00:08:36,590 --> 00:08:40,450 
Communicate goals and
provide organizational buy-in for 
112 
00:08:40,450 --> 00:08:42,420 
analytics projects. 
113 
00:08:42,420 --> 00:08:46,990 
Build teams with diverse talents,
and establish a teamwork mindset. 
114 
00:08:48,000 --> 00:08:52,370 
Remove barriers to data access and
integration. 
115 
00:08:52,370 --> 00:08:57,160 
Finally, these activities need to be
iterated to respond to new business 
116 
00:08:57,160 --> 00:08:59,568 
goals and technological advances. 
1 
00:00:01,150 --> 00:00:05,590 
In this video, we'll talk about a new
that is usually not covered much. 
2 
00:00:06,820 --> 00:00:07,510 
It's called valence. 
3 
00:00:15,960 --> 00:00:19,440 
Simply put Valence
refers to Connectedness. 
4 
00:00:20,440 --> 00:00:23,660 
The more connected data is,
the higher it's valences. 
5 
00:00:24,830 --> 00:00:27,000 
The term valence comes from chemistry. 
6 
00:00:28,030 --> 00:00:32,800 
In chemistry, we talk about core electrons
and valence electrons of an atom. 
7 
00:00:33,910 --> 00:00:38,810 
Valence electrons are in the outer most
shell, have the highest energy level and 
8 
00:00:38,810 --> 00:00:41,960 
are responsible for
bonding with other atoms. 
9 
00:00:41,960 --> 00:00:46,560 
That higher valence results in greater
boding, that is greater connectedness. 
10 
00:00:47,740 --> 00:00:51,670 
This idea is carried over into our
definition of the term valence 
11 
00:00:51,670 --> 00:00:53,180 
in the context of big data. 
12 
00:00:55,010 --> 00:00:58,750 
Data items are often directly
connected to one another. 
13 
00:00:58,750 --> 00:01:01,490 
A city is connected to
the country it belongs to. 
14 
00:01:02,530 --> 00:01:06,030 
Two Facebook users are connected
because they are friends. 
15 
00:01:06,030 --> 00:01:08,340 
An employee is connected
to his work place. 
16 
00:01:09,640 --> 00:01:11,470 
Data could also be indirectly connected. 
17 
00:01:12,710 --> 00:01:16,150 
Two scientists are connected,
because they are both physicists. 
18 
00:01:17,530 --> 00:01:23,520 
For a data collection valence measures
the ratio of actually connected data items 
19 
00:01:23,520 --> 00:01:27,390 
to the possible number of connections
that could occur within the collection. 
20 
00:01:28,460 --> 00:01:31,100 
The most important aspect of valence 
21 
00:01:31,100 --> 00:01:33,870 
is that the data connectivity
increases over time. 
22 
00:01:34,910 --> 00:01:39,620 
The series of network graphs comes from
a social experiment where scientists 
23 
00:01:39,620 --> 00:01:44,230 
attending a conference were asked to meet
other scientists they did not know before. 
24 
00:01:44,230 --> 00:01:45,830 
After several rounds of meetings, 
25 
00:01:45,830 --> 00:01:49,840 
they found new connections
shown by their red edges. 
26 
00:01:49,840 --> 00:01:55,500 
Increase in valence can lead to emergent
group behavior in people networks, 
27 
00:01:55,500 --> 00:02:00,310 
like creation of new groups and coalitions
that have shared values and goals. 
28 
00:02:02,760 --> 00:02:04,770 
A high valence data set is denser. 
29 
00:02:06,060 --> 00:02:09,670 
This makes many regular,
analytic critiques very inefficient. 
30 
00:02:10,930 --> 00:02:14,350 
More complex analytical methods
must be adopted to account for 
31 
00:02:14,350 --> 00:02:15,490 
the increasing density. 
32 
00:02:16,780 --> 00:02:20,760 
More interesting challenges arise due
to the dynamic behavior of the data. 
33 
00:02:22,030 --> 00:02:24,170 
Now there is a need to model and 
34 
00:02:24,170 --> 00:02:28,480 
predict how valence of a connected data
set may change with time and volume. 
35 
00:02:29,890 --> 00:02:34,660 
The dynamic behavior also leads to
the problem of event detection, 
36 
00:02:34,660 --> 00:02:38,510 
such as bursts in the local
cohesion in parts of the data. 
37 
00:02:38,510 --> 00:02:40,760 
And emergent behavior
in the whole data set, 
38 
00:02:40,760 --> 00:02:43,570 
such as increased
polarization in a community. 
1 
00:00:01,450 --> 00:00:04,620 
Now we'll talk about a form of
scalability called variety. 
2 
00:00:06,040 --> 00:00:10,500 
In this case, scale does not
refer to the largeness of data. 
3 
00:00:10,500 --> 00:00:12,398 
It refers to increased diversity. 
4 
00:00:21,985 --> 00:00:24,450 
Here is an important mantra
you need to think about. 
5 
00:00:25,660 --> 00:00:30,530 
When we, as data scientists, think of
data variety, we think of the additional 
6 
00:00:30,530 --> 00:00:35,450 
complexity that results from more kinds
of data that we need to store, process, 
7 
00:00:35,450 --> 00:00:37,010 
and combine. 
8 
00:00:37,010 --> 00:00:39,970 
Now, many years ago when I
started studying data management, 
9 
00:00:40,980 --> 00:00:43,030 
we always thought of data as tables. 
10 
00:00:44,310 --> 00:00:50,060 
These tables could be in spreadsheets or
databases or just files, but somehow 
11 
00:00:50,060 --> 00:00:53,290 
they will be modeled and manipulated
as rows and columns of of tables. 
12 
00:00:54,610 --> 00:01:00,620 
Now, tables are still really important and
dominant, however today a much wider 
13 
00:01:00,620 --> 00:01:04,800 
variety of data are collected, stored, and
analyzed to solve real world problems. 
14 
00:01:05,990 --> 00:01:10,610 
Image data, text data, network data,
geographic maps, computer 
15 
00:01:10,610 --> 00:01:15,390 
generated simulations are only a few of
the types of data we encounter everyday. 
16 
00:01:16,460 --> 00:01:21,010 
The heterogeneity of data can be
characterized along several dimensions. 
17 
00:01:21,010 --> 00:01:22,990 
We mentioned four such axes here. 
18 
00:01:24,910 --> 00:01:27,630 
Structural variety refers
to the difference in 
19 
00:01:27,630 --> 00:01:29,870 
the representation of the data. 
20 
00:01:29,870 --> 00:01:33,600 
For example, an EKG signal is very
different from a newspaper article. 
21 
00:01:34,690 --> 00:01:39,630 
A satellite image of wildfires from
NASA is very different from tweets 
22 
00:01:39,630 --> 00:01:42,110 
sent out by people who
are seeing the fire spread. 
23 
00:01:43,230 --> 00:01:48,100 
Media variety refers to the medium
in which the data gets delivered. 
24 
00:01:48,100 --> 00:01:52,150 
The audio of a speech versus
the transcript of the speech 
25 
00:01:52,150 --> 00:01:55,300 
may represent the same information
in two different media. 
26 
00:01:56,460 --> 00:01:59,872 
Data objects like news video
may have multiple media. 
27 
00:01:59,872 --> 00:02:03,436 
An image sequence, an audio,
and closed captioned text, 
28 
00:02:03,436 --> 00:02:05,730 
all time synchronized to each other. 
29 
00:02:06,850 --> 00:02:11,160 
Semantic variety is best
described two examples. 
30 
00:02:11,160 --> 00:02:15,480 
We often use different units for
quantities we measure. 
31 
00:02:15,480 --> 00:02:19,870 
Sometimes we also use qualitative
versus quantitative measures. 
32 
00:02:19,870 --> 00:02:23,230 
For example, age can be a number or 
33 
00:02:23,230 --> 00:02:26,440 
we represent it by terms like infant,
juvenile, or adult. 
34 
00:02:28,140 --> 00:02:31,030 
Another kind of semantic
variety comes from different 
35 
00:02:31,030 --> 00:02:33,450 
assumptions of conditions on the data. 
36 
00:02:33,450 --> 00:02:39,510 
For example, if we conduct two income
surveys on two different groups of people, 
37 
00:02:39,510 --> 00:02:40,970 
we may not be able to compare or 
38 
00:02:40,970 --> 00:02:44,020 
combine them without knowing more
about the populations themselves. 
39 
00:02:45,300 --> 00:02:47,620 
The variation and
availability takes many forms. 
40 
00:02:48,730 --> 00:02:52,510 
For one, data can be available real time, 
41 
00:02:52,510 --> 00:02:56,110 
like sensor data, or it can be stored,
like patient records. 
42 
00:02:57,350 --> 00:03:00,430 
Similarly data can be
accessible continuously, for 
43 
00:03:00,430 --> 00:03:01,630 
example from a traffic cam. 
44 
00:03:02,640 --> 00:03:04,490 
Versus intermittently, for 
45 
00:03:04,490 --> 00:03:07,750 
example, only when the satellite
is over the region of interest. 
46 
00:03:08,780 --> 00:03:12,650 
This makes a difference between what
operations one can do with data, 
47 
00:03:12,650 --> 00:03:15,020 
especially if the volume
of the data is large. 
48 
00:03:16,260 --> 00:03:19,700 
We'll cover this in more
detail in course two 
49 
00:03:19,700 --> 00:03:24,049 
when we explore the different genres
of data and how we model them. 
50 
00:03:25,620 --> 00:03:28,470 
We should not think that
a single data object, or 
51 
00:03:28,470 --> 00:03:32,880 
a collection of similar data objects,
will be all uniform in themselves. 
52 
00:03:32,880 --> 00:03:35,040 
Emails, for example, is a hybrid entity. 
53 
00:03:36,480 --> 00:03:40,110 
Some of this information can be a table,
like shown here. 
54 
00:03:41,310 --> 00:03:43,940 
Now, the body of the email
usually has text in it. 
55 
00:03:45,040 --> 00:03:48,360 
However, some of the text may
have ornaments around them. 
56 
00:03:48,360 --> 00:03:49,370 
For example, 
57 
00:03:49,370 --> 00:03:54,320 
the part highlighted in yellow represents
something called a markup on text. 
58 
00:03:55,420 --> 00:03:58,600 
We'll get to markups later in the course. 
59 
00:03:58,600 --> 00:04:00,500 
Emails contain attachments. 
60 
00:04:00,500 --> 00:04:02,920 
These are files, or embedded images, 
61 
00:04:02,920 --> 00:04:06,140 
or other multimedia objects
that the mailer allows. 
62 
00:04:06,140 --> 00:04:10,150 
This screenshot from my Outlook
shows the image of a scanned image 
63 
00:04:10,150 --> 00:04:11,470 
of a handwritten note. 
64 
00:04:12,540 --> 00:04:16,230 
When you take a collection of
all emails from your mailbox, or 
65 
00:04:16,230 --> 00:04:20,360 
that from an organization,
you will see that senders and 
66 
00:04:20,360 --> 00:04:22,490 
receivers form a communication network. 
67 
00:04:24,130 --> 00:04:28,917 
In 2001, there was a famous scandal around
a company called Enron that engaged in 
68 
00:04:28,917 --> 00:04:32,390 
fraudulent financial reporting practices. 
69 
00:04:32,390 --> 00:04:37,450 
Their email network, partly shown here,
has been studied by data scientist to find 
70 
00:04:37,450 --> 00:04:42,330 
usual and unusual patterns of connections
among the people in the organization. 
71 
00:04:43,810 --> 00:04:47,080 
An email collection can also
have it's own semantics. 
72 
00:04:47,080 --> 00:04:51,340 
For example, an email cannot refer to,
that means cannot copy or 
73 
00:04:51,340 --> 00:04:52,640 
forward, a previous email. 
74 
00:04:53,920 --> 00:04:58,170 
Finally, an email server
is a real-time data source. 
75 
00:04:58,170 --> 00:05:00,160 
But an email repository is not. 
76 
00:05:01,190 --> 00:05:05,610 
Does email, and email collections,
demonstrate significant 
77 
00:05:05,610 --> 00:05:10,270 
internal variation in structure,
media, semantics, and availability? 
1 
00:00:00,090 --> 00:00:03,478 
Characteristics of Big Data- Velocity. 
2 
00:00:19,075 --> 00:00:23,612 
Velocity refers to the increasing
speed at which big data is created and 
3 
00:00:23,612 --> 00:00:28,160 
the increasing speed at which the data
needs to be stored and analyzed. 
4 
00:00:29,250 --> 00:00:34,310 
Processing of data in real-time to match
its production rate as it gets generated 
5 
00:00:34,310 --> 00:00:37,810 
is a particular goal
of big data analytics. 
6 
00:00:37,810 --> 00:00:41,090 
For example,
this type of capability allows for 
7 
00:00:41,090 --> 00:00:45,110 
personalization of advertisement
on the web pages you visit 
8 
00:00:45,110 --> 00:00:49,610 
based on your recent search,
viewing, and purchase history. 
9 
00:00:49,610 --> 00:00:55,350 
If a business cannot take advantage
of the data as it gets generated, or 
10 
00:00:55,350 --> 00:00:59,540 
at the speed analysis of it is needed,
they often miss opportunities. 
11 
00:01:00,620 --> 00:01:05,540 
In order to build a case for the
importance of this dimension of big data, 
12 
00:01:05,540 --> 00:01:07,290 
let's imagine we are taking a road trip. 
13 
00:01:08,850 --> 00:01:12,610 
You're looking for
some better information to start packing. 
14 
00:01:12,610 --> 00:01:14,730 
In this case, the newer the information, 
15 
00:01:14,730 --> 00:01:19,140 
the higher its relevance
in deciding what to pack. 
16 
00:01:19,140 --> 00:01:21,420 
Would you use last month's
weather information or 
17 
00:01:21,420 --> 00:01:24,310 
data from last year at this time? 
18 
00:01:24,310 --> 00:01:29,550 
Or, would you use the weather
information from this week, yesterday or 
19 
00:01:29,550 --> 00:01:30,630 
better, today? 
20 
00:01:31,820 --> 00:01:35,750 
It makes sense to obtain the latest
information about weather and 
21 
00:01:35,750 --> 00:01:38,830 
process it in a way that
makes your decisions easier. 
22 
00:01:38,830 --> 00:01:44,600 
If the information is old,
it doesn't matter how accurate it is. 
23 
00:01:45,950 --> 00:01:49,190 
Being able to catch up with
the velocity of big data and 
24 
00:01:49,190 --> 00:01:54,640 
analyzing it as it gets generated can
even impact the quality of human life. 
25 
00:01:54,640 --> 00:02:00,700 
Sensors and smart devices monitoring
the human body can detect abnormalities 
26 
00:02:00,700 --> 00:02:06,510 
in real time and trigger immediate action,
potentially saving lives. 
27 
00:02:06,510 --> 00:02:10,950 
This type of processing is what
we call real time processing. 
28 
00:02:10,950 --> 00:02:15,210 
Real-time processing is quite
different from its remote relative, 
29 
00:02:15,210 --> 00:02:15,800 
batch processing. 
30 
00:02:18,270 --> 00:02:22,320 
Batch processing was the norm
until a couple of years ago. 
31 
00:02:22,320 --> 00:02:26,160 
Large amounts of data would be
fed into large machines and 
32 
00:02:26,160 --> 00:02:27,870 
processed for days at a time. 
33 
00:02:29,230 --> 00:02:33,870 
While this type of processing is still
very common today, decisions based on 
34 
00:02:33,870 --> 00:02:39,490 
information that is even few days old
can be catastrophic to some businesses. 
35 
00:02:41,620 --> 00:02:45,320 
Organizations which make
decisions on latest data 
36 
00:02:45,320 --> 00:02:46,960 
are more likely to hit the target. 
37 
00:02:48,860 --> 00:02:52,780 
For this reason it's important
to match the speed of processing 
38 
00:02:52,780 --> 00:02:57,345 
with the speed of information generation,
and get real time decision making power. 
39 
00:02:57,345 --> 00:03:01,497 
In addition, today's sensor-powered 
40 
00:03:01,497 --> 00:03:06,843 
socioeconomic climate
requires faster decisions. 
41 
00:03:06,843 --> 00:03:11,332 
Hence, we can not wait for
all the data to be first produced, 
42 
00:03:11,332 --> 00:03:13,190 
then fed into a machine. 
43 
00:03:14,480 --> 00:03:18,830 
There are many applications where
new information is streaming and 
44 
00:03:18,830 --> 00:03:22,120 
needs to be integrated
with existing data to 
45 
00:03:22,120 --> 00:03:26,825 
produce decisions such as emergency
response planning in a tornado, or 
46 
00:03:26,825 --> 00:03:32,500 
deciding trading strategies in real time,
or getting estimates in advertising. 
47 
00:03:34,520 --> 00:03:40,340 
We have to digest chunks of data as they
are produced and give meaningful results. 
48 
00:03:42,900 --> 00:03:44,385 
As more data comes in, 
49 
00:03:44,385 --> 00:03:48,930 
your results will need to adapt to
reflect this change in the input. 
50 
00:03:50,060 --> 00:03:56,160 
Decisions based on processing of already
acquired data such as batch processing, 
51 
00:03:56,160 --> 00:03:58,660 
may give an incomplete picture. 
52 
00:03:58,660 --> 00:04:04,550 
And hence, the applications need real
time status of the context at hand. 
53 
00:04:04,550 --> 00:04:05,910 
That is, streaming analysis. 
54 
00:04:07,280 --> 00:04:12,420 
Fortunately, with the event
of cheap sensors technology, 
55 
00:04:12,420 --> 00:04:17,520 
mobile phones, and social media,
we can obtain the latest information 
56 
00:04:17,520 --> 00:04:21,940 
at a much rapid rate and
in real time in comparison with the past. 
57 
00:04:22,990 --> 00:04:26,650 
So how do you make sure we match
the velocity of the expectations 
58 
00:04:26,650 --> 00:04:28,910 
to gain insights from big data? 
59 
00:04:28,910 --> 00:04:31,900 
With the velocity of the big data. 
60 
00:04:31,900 --> 00:04:34,050 
Rate of generation, retrieval, 
61 
00:04:34,050 --> 00:04:37,650 
or processing of data is
application specific. 
62 
00:04:38,760 --> 00:04:43,660 
The need for real time data-driven
actions within a business case is 
63 
00:04:43,660 --> 00:04:48,620 
what in the end dictates the velocity
of analytics over big data. 
64 
00:04:49,920 --> 00:04:53,620 
Sometimes precision of a minute is needed. 
65 
00:04:53,620 --> 00:04:55,750 
Sometimes half a day. 
66 
00:04:55,750 --> 00:05:00,710 
Let's look at these four paths and
discuss when to pick the right one for 
67 
00:05:00,710 --> 00:05:02,270 
your analysis. 
68 
00:05:02,270 --> 00:05:06,937 
The dollar signs next to the numbers
in this example indicate how costly 
69 
00:05:06,937 --> 00:05:09,000 
the operation is. 
70 
00:05:09,000 --> 00:05:11,160 
The more dollars, the higher the cost. 
71 
00:05:12,600 --> 00:05:16,750 
When the timeliness of processed
information plays no role in decision 
72 
00:05:16,750 --> 00:05:21,950 
making, the speed at which data
is generated becomes irrelevant. 
73 
00:05:21,950 --> 00:05:27,220 
In other words, you can wait for
as long as it takes to process data. 
74 
00:05:27,220 --> 00:05:29,730 
Days, months, weeks. 
75 
00:05:29,730 --> 00:05:33,120 
And once processing is over,
you will look at the results and 
76 
00:05:33,120 --> 00:05:34,550 
probably share them with someone. 
77 
00:05:35,680 --> 00:05:40,640 
When timeliness is not an issue,
you can choose any of the four paths. 
78 
00:05:41,800 --> 00:05:44,150 
You will likely pick the cheapest one. 
79 
00:05:45,290 --> 00:05:48,720 
When timeliness of end result is an issue 
80 
00:05:48,720 --> 00:05:52,810 
deciding which of the four paths
to choose is not so simple. 
81 
00:05:52,810 --> 00:05:56,620 
You will have to make a decision
based on cost of hardware, 
82 
00:05:56,620 --> 00:06:01,110 
time sensitivity of information,
future scenarios. 
83 
00:06:01,110 --> 00:06:06,480 
In other words,
this becomes a business driven question. 
84 
00:06:06,480 --> 00:06:12,930 
For example, if speed is really important
at all costs, you will pick path four. 
85 
00:06:12,930 --> 00:06:19,130 
As a summary, we need to pay attention
to the velocity of big data. 
86 
00:06:19,130 --> 00:06:23,460 
Streaming data gives information
on what's going on right now. 
87 
00:06:23,460 --> 00:06:28,670 
Streaming data has velocity, meaning
it gets generated at various rates. 
88 
00:06:28,670 --> 00:06:33,370 
And analysis of such data in
real time gives agility and 
89 
00:06:33,370 --> 00:06:37,620 
adaptability to maximize
benefits you want to extract. 
1 
00:00:01,640 --> 00:00:04,377 
Characteristics of Big Data, Veracity. 
2 
00:00:20,366 --> 00:00:23,980 
Veracity of Big Data refers
to the quality of the data. 
3 
00:00:25,010 --> 00:00:29,760 
It sometimes gets referred
to as validity or 
4 
00:00:29,760 --> 00:00:33,250 
volatility referring to
the lifetime of the data. 
5 
00:00:34,840 --> 00:00:38,360 
Veracity is very important for
making big data operational. 
6 
00:00:39,810 --> 00:00:43,900 
Because big data can be noisy and
uncertain. 
7 
00:00:43,900 --> 00:00:50,885 
It can be full of biases,
abnormalities and it can be imprecise. 
8 
00:00:50,885 --> 00:00:54,741 
Data is of no value if it's not accurate, 
9 
00:00:54,741 --> 00:00:59,900 
the results of big data analysis are only
as good as the data being analyzed. 
10 
00:01:01,480 --> 00:01:06,840 
This is often described in analytics
as junk in equals junk out. 
11 
00:01:08,040 --> 00:01:12,760 
So we can say although big data
provides many opportunities to make 
12 
00:01:12,760 --> 00:01:18,100 
data enabled decisions,
the evidence provided by data 
13 
00:01:18,100 --> 00:01:22,710 
is only valuable if the data
is of a satisfactory quality. 
14 
00:01:23,880 --> 00:01:26,870 
There are many different
ways to define data quality. 
15 
00:01:27,890 --> 00:01:30,200 
In the context of big data, 
16 
00:01:30,200 --> 00:01:34,040 
quality can be defined as a function
of a couple of different variables. 
17 
00:01:35,230 --> 00:01:41,310 
Accuracy of the data, the trustworthiness
or reliability of the data source. 
18 
00:01:41,310 --> 00:01:44,940 
And how the data was generated
are all important factors 
19 
00:01:44,940 --> 00:01:46,720 
that affect the quality of data. 
20 
00:01:48,100 --> 00:01:53,910 
Additionally how meaningful the data
is with respect to the program that 
21 
00:01:53,910 --> 00:02:00,360 
analyzes it, is an important factor, and
makes context a part of the quality. 
22 
00:02:02,040 --> 00:02:07,850 
In this chart from 2015,
we see the volumes of data increasing, 
23 
00:02:07,850 --> 00:02:12,380 
starting with small amounts
of enterprise data to larger, 
24 
00:02:12,380 --> 00:02:17,480 
people generated voice over IP and
social media data and 
25 
00:02:17,480 --> 00:02:21,010 
even larger machine generated sensor data. 
26 
00:02:21,010 --> 00:02:26,040 
We also see that the uncertainty of
the data increases as we go from 
27 
00:02:26,040 --> 00:02:28,840 
enterprise data to sensor data. 
28 
00:02:28,840 --> 00:02:31,320 
This is as we would expect it to be. 
29 
00:02:31,320 --> 00:02:35,980 
Traditional enterprise
data in warehouses have 
30 
00:02:35,980 --> 00:02:41,080 
standardized quality solutions
like master processes for extract, 
31 
00:02:41,080 --> 00:02:46,310 
transform and load of the data which
we referred to as before as ETL. 
32 
00:02:46,310 --> 00:02:51,080 
As enterprises started incorporating less
structured and unstructured people and 
33 
00:02:51,080 --> 00:02:54,870 
machine data into their
big data solutions, 
34 
00:02:54,870 --> 00:02:58,670 
the data become messier and
more uncertain. 
35 
00:02:58,670 --> 00:03:00,210 
There are many reasons for this. 
36 
00:03:01,220 --> 00:03:07,620 
First, unstructured data on the internet
is imprecise and uncertain. 
37 
00:03:07,620 --> 00:03:13,730 
In addition, high velocity big data
leaves very little or no time for 
38 
00:03:13,730 --> 00:03:19,810 
ETL, and in turn hindering the quality
assurance processes of the data. 
39 
00:03:19,810 --> 00:03:24,090 
Let's look at these product reviews for
a banana slicer on amazon.com. 
40 
00:03:24,090 --> 00:03:28,960 
One of the five star reviews say that 
41 
00:03:28,960 --> 00:03:34,190 
it saved her marriage and compared it
to the greatest inventions in history. 
42 
00:03:34,190 --> 00:03:38,900 
Another five star reviewer said
that his parole officer recommended 
43 
00:03:38,900 --> 00:03:42,470 
the slicer as he is not
allowed to be around knives. 
44 
00:03:42,470 --> 00:03:44,400 
These are obviously fake reviewers. 
45 
00:03:45,410 --> 00:03:49,070 
Now think of an automated product
assessment going through such 
46 
00:03:49,070 --> 00:03:53,930 
splendid reviews and estimating lots
of sales for the banana slicer and 
47 
00:03:53,930 --> 00:03:58,390 
in turn suggesting stocking more
of the slicer in the inventory. 
48 
00:03:58,390 --> 00:03:59,670 
Amazon will have problems. 
49 
00:04:00,760 --> 00:04:06,420 
For a more serious case let's look at
the Google flu trends case from 2013. 
50 
00:04:06,420 --> 00:04:11,788 
For January 2013,
the Google Friends actually 
51 
00:04:11,788 --> 00:04:17,280 
estimated almost twice as many
flu cases as was reported by CDC, 
52 
00:04:17,280 --> 00:04:20,870 
the Centers for
Disease Control and Prevention. 
53 
00:04:22,390 --> 00:04:26,960 
The primary reason behind this was that
Google Flu Trends used a big data on 
54 
00:04:26,960 --> 00:04:32,070 
the internet and did not account properly
for uncertainties about the data. 
55 
00:04:33,210 --> 00:04:37,780 
Maybe the news and social media
attention paid to the particularly 
56 
00:04:37,780 --> 00:04:42,450 
serious level of flu that
year effected the estimate. 
57 
00:04:42,450 --> 00:04:46,840 
And resulted in what we
call an over estimation. 
58 
00:04:46,840 --> 00:04:49,050 
This is a perfect example for 
59 
00:04:49,050 --> 00:04:55,470 
how inaccurate the results can be if
only big data is used in the analysis. 
60 
00:04:55,470 --> 00:04:59,820 
Imagine the economical impact of
making health care preparations for 
61 
00:04:59,820 --> 00:05:01,920 
twice the amount of flu cases. 
62 
00:05:01,920 --> 00:05:03,610 
That would be huge. 
63 
00:05:03,610 --> 00:05:07,850 
The Google flu trends example
also brings up the need for 
64 
00:05:07,850 --> 00:05:13,350 
being able to identify where exactly
the big data they used comes from. 
65 
00:05:13,350 --> 00:05:16,300 
What transformation did
big data go through up 
66 
00:05:16,300 --> 00:05:19,420 
until the moment it was used for
a estimate? 
67 
00:05:19,420 --> 00:05:23,460 
This is what we refer
to as data providence. 
68 
00:05:23,460 --> 00:05:27,630 
Just like we refer to
an artifacts provenance. 
69 
00:05:27,630 --> 00:05:33,180 
As a summary, the growing
torrents of big data pushes for 
70 
00:05:33,180 --> 00:05:36,590 
fast solutions to utilize
it in analytical solutions. 
71 
00:05:37,630 --> 00:05:42,000 
This creates challenges on
keeping track of data quality. 
72 
00:05:42,000 --> 00:05:46,510 
What has been collected,
where it came from, and 
73 
00:05:46,510 --> 00:05:48,940 
how it was analyzed prior to its use. 
74 
00:05:50,150 --> 00:05:52,910 
This is akin to an art artifact 
75 
00:05:52,910 --> 00:05:55,490 
having providence of everything
it has gone through. 
76 
00:05:56,540 --> 00:06:01,560 
But even more complicated to achieve
with large volumes of data coming 
77 
00:06:01,560 --> 00:06:04,130 
in varieties and velocities. 
1 
00:00:00,830 --> 00:00:03,232 
Characteristics of Big Data- Volume. 
2 
00:00:13,965 --> 00:00:19,190 
Volume is the big data dimension that
relates to the sheer size of big data. 
3 
00:00:20,410 --> 00:00:24,580 
This volume can come from
large datasets being shared or 
4 
00:00:24,580 --> 00:00:30,240 
many small data pieces and
events being collected over time. 
5 
00:00:31,460 --> 00:00:35,335 
Every minute 204 million emails are sent, 
6 
00:00:35,335 --> 00:00:41,308 
200,000 photos are uploaded, and 1.8
million likes are generated on Facebook. 
7 
00:00:41,308 --> 00:00:48,710 
On YouTube, 1.3 million videos are viewed
and 72 hours of video are uploaded. 
8 
00:00:51,160 --> 00:00:54,120 
But how much data are we talking about? 
9 
00:00:54,120 --> 00:00:59,170 
The size and the scale of storage for
big data can be massive. 
10 
00:00:59,170 --> 00:01:03,310 
You heard me say words that
start with peta, exa and 
11 
00:01:03,310 --> 00:01:09,220 
yotta, to define size, but
what does all that really mean? 
12 
00:01:09,220 --> 00:01:15,210 
For comparison, 100 megabytes will
hold a couple of encyclopedias. 
13 
00:01:16,460 --> 00:01:20,340 
A DVD is around 5 GBs, and 
14 
00:01:20,340 --> 00:01:25,837 
1 TB would hold around 300
hours of good quality video. 
15 
00:01:25,837 --> 00:01:32,530 
A data-oriented business currently
collects data in the order of terabytes, 
16 
00:01:32,530 --> 00:01:35,880 
but petabytes are becoming more
common to our daily lives. 
17 
00:01:36,930 --> 00:01:42,290 
CERN's large hadron collider
generates 15 petabytes a year. 
18 
00:01:42,290 --> 00:01:47,340 
According to predictions by an IDC report
sponsored by a big data company called 
19 
00:01:47,340 --> 00:01:54,150 
EMC, digital data, will grow by
a factor of 44 until the year 2020. 
20 
00:01:54,150 --> 00:01:58,190 
This is a growth from 0.8 zetabytes, 
21 
00:01:59,750 --> 00:02:04,280 
In 2009 to 35.2 zettabytes in 2020. 
22 
00:02:04,280 --> 00:02:11,090 
A zettabyte is 1 trillion gigabytes,
that's 10 to the power of 21. 
23 
00:02:11,090 --> 00:02:14,140 
The effects of it will be huge! 
24 
00:02:15,160 --> 00:02:20,420 
Think of all the time, cost,
energy that will be used to store and 
25 
00:02:20,420 --> 00:02:23,860 
make sense of such an amount of data. 
26 
00:02:23,860 --> 00:02:26,430 
The next era will be yottabytes. 
27 
00:02:26,430 --> 00:02:31,050 
Ten to the power of 24 and
brontobytes, ten to the power of 27. 
28 
00:02:31,050 --> 00:02:36,410 
Which is really hard to imagine for
most of us at this time. 
29 
00:02:36,410 --> 00:02:41,740 
This is also what we call data
at an astronomical scale. 
30 
00:02:41,740 --> 00:02:44,280 
The choice of putting the Milky Way Galaxy 
31 
00:02:45,540 --> 00:02:49,560 
in the middle of the circle
is not just for aesthetics. 
32 
00:02:50,660 --> 00:02:54,520 
This is what we would see if
we were to scale up 10 to 
33 
00:02:54,520 --> 00:02:56,780 
the 21 times into the universe. 
34 
00:02:56,780 --> 00:02:57,510 
Cool, isn't it? 
35 
00:02:58,750 --> 00:03:01,760 
Please refer to the reading
in this module called, 
36 
00:03:01,760 --> 00:03:07,220 
what does astronomical scale mean,
for a nice video on the powers of ten. 
37 
00:03:07,220 --> 00:03:14,400 
All of these point to an exponential
growth in data volume and storage. 
38 
00:03:14,400 --> 00:03:17,200 
What is the relevance of
this much data in our world? 
39 
00:03:18,260 --> 00:03:20,210 
Remember the planes collecting big data? 
40 
00:03:21,550 --> 00:03:25,170 
Our hope, as passengers,
is data means better flight safety. 
41 
00:03:26,540 --> 00:03:32,070 
The idea is to understand that businesses
and organizations are collecting and 
42 
00:03:32,070 --> 00:03:36,520 
leveraging large volumes of data
to improve their end products, 
43 
00:03:36,520 --> 00:03:40,366 
whether it is safety, reliability,
healthcare, or governance. 
44 
00:03:40,366 --> 00:03:45,300 
In general, in business the goal 
45 
00:03:45,300 --> 00:03:50,660 
is to turn this much data into
some form of business advantage. 
46 
00:03:50,660 --> 00:03:55,140 
The question is how do we
utilize larger volumes of data 
47 
00:03:55,140 --> 00:03:57,920 
to improve our end product's quality? 
48 
00:03:57,920 --> 00:04:00,790 
Despite a number of
challenges related to it. 
49 
00:04:01,840 --> 00:04:06,650 
There are a number of challenges related
to the massive volumes of big data. 
50 
00:04:08,210 --> 00:04:11,970 
The most obvious one is of course storage. 
51 
00:04:11,970 --> 00:04:14,800 
As the size of the data increases so 
52 
00:04:14,800 --> 00:04:18,740 
does the amount of storage space
required to store that data efficiently. 
53 
00:04:19,810 --> 00:04:24,340 
However, we also need to be able
to retrieve that large amount of 
54 
00:04:24,340 --> 00:04:26,030 
data fast enough, and 
55 
00:04:26,030 --> 00:04:32,140 
move it to processing units in a timely
fashion to get results when we need them. 
56 
00:04:32,140 --> 00:04:35,900 
This brings additional challenges
such as networking, bandwidth, 
57 
00:04:35,900 --> 00:04:37,205 
cost of storing data. 
58 
00:04:37,205 --> 00:04:41,670 
In-house versus cloud storage and
things like that. 
59 
00:04:41,670 --> 00:04:46,530 
Additional challenges arise during
processing of such large data. 
60 
00:04:46,530 --> 00:04:50,450 
Most existing analytical methods
won't scale to such sums of 
61 
00:04:50,450 --> 00:04:53,500 
data in terms of memory,
processing, or IO needs. 
62 
00:04:55,150 --> 00:04:56,980 
This means their performance will drop. 
63 
00:04:58,060 --> 00:05:02,220 
You might be able to get good performance
for data from hundreds of customers. 
64 
00:05:02,220 --> 00:05:09,314 
But how about scaling your solution
to 1,000 or 10,000 customers? 
65 
00:05:09,314 --> 00:05:15,270 
As the volume increases performance and
cost start becoming a challenge. 
66 
00:05:16,880 --> 00:05:21,530 
Businesses need a holistic
strategy to handle processing of 
67 
00:05:21,530 --> 00:05:26,090 
large scale data to their benefit
in the most cost effective manner. 
68 
00:05:26,090 --> 00:05:29,690 
Evaluating the options across
the dimensions mentioned here, 
69 
00:05:29,690 --> 00:05:33,580 
is the first step when it comes to
continuously increasing data size. 
70 
00:05:33,580 --> 00:05:38,290 
We will revisit this topic
later on in this course. 
71 
00:05:38,290 --> 00:05:44,330 
As a summary volume is the dimension
of big data related to its size and 
72 
00:05:44,330 --> 00:05:45,540 
its exponential growth. 
73 
00:05:46,770 --> 00:05:52,126 
The challenges with working with volumes
of big data include cost, scalability, 
74 
00:05:52,126 --> 00:05:56,725 
and performance related to their storage,
access, and processing. 
1 
00:00:02,826 --> 00:00:04,162 
Data Science. 
2 
00:00:04,162 --> 00:00:06,298 
Getting value out of big data. 
3 
00:00:26,242 --> 00:00:31,860 
We have all heard data science turned
data into insights or even actions. 
4 
00:00:31,860 --> 00:00:34,250 
But what does that really mean? 
5 
00:00:34,250 --> 00:00:36,990 
Data science can be
thought of as a basis for 
6 
00:00:36,990 --> 00:00:41,440 
empirical research where data is used
to induce information for observations. 
7 
00:00:42,490 --> 00:00:46,310 
These observations are mainly data,
in our case, 
8 
00:00:46,310 --> 00:00:49,985 
big data, related to a business or
scientific case. 
9 
00:00:49,985 --> 00:00:54,430 
Insight is a term 
10 
00:00:54,430 --> 00:00:59,050 
we use to refer to the data
products of data science. 
11 
00:00:59,050 --> 00:01:03,523 
It is extracted from a diverse amount
of data through a combination of 
12 
00:01:03,523 --> 00:01:07,686 
exploratory data analysis and modeling. 
13 
00:01:07,686 --> 00:01:13,440 
The questions are sometimes more specific,
and sometimes it requires 
14 
00:01:13,440 --> 00:01:18,170 
looking at the data and patterns in it
to come up with the specific question. 
15 
00:01:20,420 --> 00:01:25,550 
Another important point to recognize
is that data science is not static. 
16 
00:01:25,550 --> 00:01:28,020 
It is not one time analysis. 
17 
00:01:28,020 --> 00:01:32,920 
It involves a process where models
generated to lead to insights 
18 
00:01:32,920 --> 00:01:37,630 
are constantly improved through further
empirical evidence, or simply, data. 
19 
00:01:39,640 --> 00:01:45,060 
For example, a book retailer like
Amazon.com can constantly improve 
20 
00:01:45,060 --> 00:01:49,560 
the model of a customer's book preferences
using the customer demographic, 
21 
00:01:50,580 --> 00:01:55,630 
his or her previous purchases and
the book reviews of the customer. 
22 
00:01:57,920 --> 00:02:02,350 
The book retailer can also
uses information to predict 
23 
00:02:02,350 --> 00:02:07,219 
which customers are likely
to like any book, 
24 
00:02:07,219 --> 00:02:11,560 
and take action to market
the book to those customers. 
25 
00:02:13,840 --> 00:02:16,955 
This is where we see insights
being turned into action. 
26 
00:02:19,834 --> 00:02:24,718 
As we have seen in the book marketing
example, using data science and analysis 
27 
00:02:24,718 --> 00:02:29,240 
of the past and current information,
data science generates actions. 
28 
00:02:30,350 --> 00:02:33,200 
This is not just an analysis of the past,
but 
29 
00:02:33,200 --> 00:02:36,980 
rather generation of actionable
information for the future. 
30 
00:02:38,370 --> 00:02:42,070 
This is what we can call a prediction,
like the weather forecast. 
31 
00:02:43,680 --> 00:02:49,140 
When you decide what to wear for
the day based on the forecast of the day, 
32 
00:02:49,140 --> 00:02:53,880 
you are taking action based
on insight delivered to you. 
33 
00:02:53,880 --> 00:02:58,110 
Just like this, business leaders and
decision makers 
34 
00:02:58,110 --> 00:03:02,170 
take action based on the evidence
provided by their data science teams. 
35 
00:03:04,140 --> 00:03:05,870 
We set data science teams. 
36 
00:03:06,980 --> 00:03:11,290 
This comes from the breadth of information
and skill that it takes to make it happen. 
37 
00:03:12,590 --> 00:03:17,010 
You have probably seen diagrams like
this one that describe data science. 
38 
00:03:18,090 --> 00:03:22,220 
Data science happens at
the intersection of computer science, 
39 
00:03:22,220 --> 00:03:24,820 
mathematics and business expertise. 
40 
00:03:27,170 --> 00:03:29,750 
If we zoom deeper into this diagram and 
41 
00:03:29,750 --> 00:03:35,030 
open the sets of expertise we will
see a variation of this figure. 
42 
00:03:36,410 --> 00:03:42,690 
Even at this level, all of these circles
require deeper knowledge and skills 
43 
00:03:42,690 --> 00:03:48,140 
in areas like domain expertise, data
engineering, statistics and computing. 
44 
00:03:51,185 --> 00:03:55,790 
An even deeper analysis of these
skills will lead you to skills like 
45 
00:03:55,790 --> 00:04:00,110 
machine learning, statistical modeling,
relational algebra, 
46 
00:04:00,110 --> 00:04:04,640 
business passion, problem solving and
data visualization. 
47 
00:04:04,640 --> 00:04:07,582 
That's a lot of skills to have for
a single person. 
48 
00:04:12,922 --> 00:04:17,862 
These wide range of skills and
definitions of data scientists having them 
49 
00:04:17,862 --> 00:04:21,855 
all led to discussions like
are data scientists unicorns? 
50 
00:04:22,895 --> 00:04:24,115 
Meaning they don't exist. 
51 
00:04:25,375 --> 00:04:29,795 
There are data science experts who have
expertise in more than one of these 
52 
00:04:29,795 --> 00:04:31,380 
skills, for sure. 
53 
00:04:31,380 --> 00:04:33,795 
But they're relatively rare, and 
54 
00:04:33,795 --> 00:04:37,470 
still would probably need help from
an expert on some of these areas. 
55 
00:04:38,730 --> 00:04:44,550 
So, in reality, data scientists
are teams of people who act like one. 
56 
00:04:45,560 --> 00:04:48,860 
They are passionate about the story and
the meaning behind data. 
57 
00:04:50,310 --> 00:04:54,700 
They understand they problem
they are trying to solve, and 
58 
00:04:54,700 --> 00:04:57,910 
aim to find the right analytical
methods to solve this problem. 
59 
00:04:57,910 --> 00:05:04,350 
And they all have an interest in
engineering solutions to solve problems. 
60 
00:05:06,230 --> 00:05:11,360 
They also have curiosity about each
others work, and have communication 
61 
00:05:11,360 --> 00:05:15,940 
skills to interact with the team and
present their ideas and results to others. 
62 
00:05:17,900 --> 00:05:23,460 
As a summary, a data science team often
comes together to analyze situations, 
63 
00:05:23,460 --> 00:05:29,370 
business or scientific cases, which none
of the individuals can solve on their own. 
64 
00:05:29,370 --> 00:05:32,090 
There are lots of moving
parts to the solution. 
65 
00:05:32,090 --> 00:05:36,370 
But in the end, all these parts
should come together to provide 
66 
00:05:36,370 --> 00:05:39,055 
actionable insight based on big data. 
67 
00:05:41,807 --> 00:05:46,732 
Being able to use evidence-based
insight in business decisions is more 
68 
00:05:46,732 --> 00:05:48,517 
important now than ever. 
69 
00:05:48,517 --> 00:05:52,594 
Data scientists have a combination
of technical, business and 
70 
00:05:52,594 --> 00:05:54,716 
soft skills to make this happen. 
1 
00:00:00,930 --> 00:00:03,310 
Getting started,
characteristics of big data. 
2 
00:00:21,143 --> 00:00:25,786 
By now you have seen that big data is a
blanket term that is used to refer to any 
3 
00:00:25,786 --> 00:00:30,140 
collection of data so large and
complex that it exceeds the processing 
4 
00:00:30,140 --> 00:00:35,530 
capability of conventional data
management systems and techniques. 
5 
00:00:35,530 --> 00:00:39,040 
The applications of big data are endless. 
6 
00:00:39,040 --> 00:00:44,000 
Every part of business and society
are changing in front our eyes due to that 
7 
00:00:44,000 --> 00:00:48,870 
fact that we now have so much more
data and the ability for analyzing. 
8 
00:00:50,220 --> 00:00:52,120 
But how can we characterize big data? 
9 
00:00:53,310 --> 00:00:56,630 
You can say, I know it when i see it. 
10 
00:00:56,630 --> 00:00:59,440 
But there are easier ways to do it. 
11 
00:00:59,440 --> 00:01:03,030 
Big data is commonly characterized
using a number of V's. 
12 
00:01:04,040 --> 00:01:09,720 
The first three are volume,
velocity, and variety. 
13 
00:01:09,720 --> 00:01:15,580 
Volume refers to the vast amounts of
data that is generated every second, 
14 
00:01:15,580 --> 00:01:19,470 
mInutes, hour, and
day in our digitized world. 
15 
00:01:21,130 --> 00:01:27,480 
Variety refers to the ever increasing
different forms that data can come in 
16 
00:01:27,480 --> 00:01:32,090 
such as text, images,
voice, and geospatial data. 
17 
00:01:34,100 --> 00:01:39,440 
Velocity refers to the speed at
which data is being generated and 
18 
00:01:39,440 --> 00:01:42,980 
the pace at which data moves
from one point to the next. 
19 
00:01:44,290 --> 00:01:46,510 
Volume, variety, and 
20 
00:01:46,510 --> 00:01:52,430 
velocity are the three main dimensions
that characterize big data. 
21 
00:01:52,430 --> 00:01:53,790 
And describe its challenges. 
22 
00:01:55,180 --> 00:02:00,410 
We have huge amounts of data
in different formats, and 
23 
00:02:00,410 --> 00:02:04,160 
varying quality which must
be processed quickly. 
24 
00:02:05,200 --> 00:02:08,470 
More Vs have been introduced
to the big data community 
25 
00:02:08,470 --> 00:02:12,070 
as we discover new challenges and
ways to define big data. 
26 
00:02:13,310 --> 00:02:17,830 
Veracity and
valence are two of these additional V's we 
27 
00:02:17,830 --> 00:02:21,720 
will pay special attention to as
a part of this specialization. 
28 
00:02:22,860 --> 00:02:29,360 
Veracity refers to the biases,
noise, and abnormality in data. 
29 
00:02:29,360 --> 00:02:34,680 
Or, better yet, It refers to the often
unmeasurable uncertainties and 
30 
00:02:34,680 --> 00:02:38,540 
truthfulness and trustworthiness of data. 
31 
00:02:38,540 --> 00:02:44,150 
Valence refers to the connectedness
of big data in the form of graphs, 
32 
00:02:44,150 --> 00:02:45,180 
just like atoms. 
33 
00:02:46,220 --> 00:02:52,180 
Moreover, we must be sure to
never forget our sixth V, value. 
34 
00:02:53,220 --> 00:02:55,890 
How do big data benefit you and
your organization? 
35 
00:02:57,560 --> 00:02:59,640 
Without a clear strategy and 
36 
00:02:59,640 --> 00:03:03,700 
an objective with the value
they are getting from big data. 
37 
00:03:03,700 --> 00:03:07,800 
It is easy to imagine that organizations
will be sidetracked by all these 
38 
00:03:07,800 --> 00:03:12,630 
challenges of big data, and not be
able to turn them into opportunities. 
39 
00:03:13,990 --> 00:03:17,993 
Now let's start looking into the first
five of these V's in detail. 
1 
00:00:02,060 --> 00:00:04,400 
How does data science happen? 
2 
00:00:04,400 --> 00:00:05,850 
Five P's of data science. 
3 
00:00:07,810 --> 00:00:13,220 
Now that we identified what data science
is and how companies can strategize around 
4 
00:00:13,220 --> 00:00:18,660 
big data to start building a purpose,
let's come back to using data science 
5 
00:00:18,660 --> 00:00:23,870 
to get value out of big data around
the purpose or questions they defined. 
6 
00:00:41,348 --> 00:00:43,591 
Our experience with building and 
7 
00:00:43,591 --> 00:00:49,040 
observing successful data science projects
led to a method around the craft with 
8 
00:00:49,040 --> 00:00:55,255 
five distinct components that can be
defined as components of data science. 
9 
00:00:55,255 --> 00:00:59,859 
Here we define data science
as a multi-disciplinary 
10 
00:00:59,859 --> 00:01:03,972 
craft that combines
people teaming up around 
11 
00:01:03,972 --> 00:01:08,910 
application-specific purpose that
can be achieved through a process, 
12 
00:01:10,420 --> 00:01:14,100 
big data computing platforms,
and programmability. 
13 
00:01:17,260 --> 00:01:22,460 
All of these should lead to
products where the focus really 
14 
00:01:22,460 --> 00:01:26,760 
is on the questions or purpose that are
defined by your big data strategy ideas. 
15 
00:01:28,960 --> 00:01:33,540 
There are many technology,
data and analytical research, and 
16 
00:01:33,540 --> 00:01:37,110 
development related activities
around the questions. 
17 
00:01:37,110 --> 00:01:41,490 
But in the end, everything we do
in this phase is to reach to that 
18 
00:01:41,490 --> 00:01:43,820 
final product based on our purposes. 
19 
00:01:44,950 --> 00:01:47,830 
So, it makes sense to start with it and 
20 
00:01:47,830 --> 00:01:50,860 
build a process around how
we make this product happen. 
21 
00:01:52,740 --> 00:01:55,520 
Remember the wild fire
prediction project I described? 
22 
00:01:56,830 --> 00:02:01,070 
One of the products we described
there was the rate of spread and 
23 
00:02:01,070 --> 00:02:02,870 
direction of an ongoing fire. 
24 
00:02:04,460 --> 00:02:06,780 
We have identified questions and 
25 
00:02:06,780 --> 00:02:11,060 
the process that led us to
the product in the end to solve it. 
26 
00:02:12,960 --> 00:02:17,280 
We brought together experts around
the table for fire modeling, 
27 
00:02:17,280 --> 00:02:21,620 
data management, time series analysis,
scalable computing, 
28 
00:02:21,620 --> 00:02:25,060 
Geographical Information Systems,
and emergency response. 
29 
00:02:28,050 --> 00:02:31,770 
I asked them,
let's not dive into the techniques yet. 
30 
00:02:31,770 --> 00:02:33,080 
What is the problem at large? 
31 
00:02:34,100 --> 00:02:40,380 
How do we see ourselves solving it? 
32 
00:02:40,380 --> 00:02:44,780 
A typical conversation around
the process starts with this question. 
33 
00:02:46,230 --> 00:02:51,600 
Then from then on,
drilling down to many areas of expertise, 
34 
00:02:51,600 --> 00:02:53,830 
often we blur lines between the steps. 
35 
00:02:55,510 --> 00:03:00,280 
My wildfire team would start listing
things like, we don't have an integrated 
36 
00:03:00,280 --> 00:03:05,400 
system or we don't have real-time
access to data programmatically, 
37 
00:03:05,400 --> 00:03:08,210 
so we can't analyze fires on the fly. 
38 
00:03:08,210 --> 00:03:12,950 
Or they can say, I can't integrate
sensor data with satellite data. 
39 
00:03:14,370 --> 00:03:20,080 
All of this leads me to challenges
I can then use to define problems. 
40 
00:03:21,300 --> 00:03:25,560 
There are many dimensions of data science
to think about within this discussion. 
41 
00:03:26,570 --> 00:03:31,700 
Let's start with the obvious ones,
people and purpose. 
42 
00:03:34,535 --> 00:03:39,800 
People refers to a data science team or
the projects stakeholders. 
43 
00:03:39,800 --> 00:03:42,700 
As you know by now,
they're expert in data and 
44 
00:03:42,700 --> 00:03:47,028 
analytics, business, computing,
science, or big data management, 
45 
00:03:47,028 --> 00:03:53,154 
like all the set of experts I
listed in my wildfire scenario. 
46 
00:03:53,154 --> 00:03:59,560 
The purpose refers to the challenge or
set of challenges defined by your 
47 
00:03:59,560 --> 00:04:04,940 
big data strategy, like solving the
question related to the rate of spread and 
48 
00:04:04,940 --> 00:04:08,170 
direction of the fire perimeter
in the wildfire case. 
49 
00:04:12,003 --> 00:04:17,099 
Since there's a predefined team
with a purpose, a great place for 
50 
00:04:17,099 --> 00:04:22,190 
this team to start with is
a process they could iterate on. 
51 
00:04:22,190 --> 00:04:27,470 
We can simply say, people with purpose
will define a process to collaborate and 
52 
00:04:27,470 --> 00:04:28,360 
communicate around. 
53 
00:04:30,010 --> 00:04:33,500 
The process is conceptual
in the beginning and 
54 
00:04:33,500 --> 00:04:37,390 
defines the set of steps an how
everyone can contribute to it. 
55 
00:04:41,060 --> 00:04:43,230 
There are many ways to
look at the process. 
56 
00:04:44,890 --> 00:04:50,250 
One way of looking at it is
as two distinct activities, 
57 
00:04:50,250 --> 00:04:53,130 
mainly big data engineering and 
58 
00:04:53,130 --> 00:04:57,500 
big data analytics, or
computational big data science, 
59 
00:04:57,500 --> 00:05:01,910 
as I like to call it, as more than simple
analytics is being performed here. 
60 
00:05:03,840 --> 00:05:10,180 
A more detailed way of looking at
the process reveals five distinct steps or 
61 
00:05:10,180 --> 00:05:15,214 
activities of this data science process, 
62 
00:05:15,214 --> 00:05:21,018 
namely acquire, prepare, 
63 
00:05:21,018 --> 00:05:26,540 
analyze, report, and act. 
64 
00:05:26,540 --> 00:05:29,190 
We can simply say that
data science happens 
65 
00:05:29,190 --> 00:05:31,950 
at the boundary of all these steps. 
66 
00:05:31,950 --> 00:05:36,510 
Ideally, this process should
support experimental work and 
67 
00:05:36,510 --> 00:05:40,980 
dynamic scalability on the big data and
computing platforms. 
68 
00:05:44,220 --> 00:05:49,050 
This five step process can be used in
alternative ways in real life big data 
69 
00:05:49,050 --> 00:05:53,580 
applications, if we add the dependencies
of different tools to each other. 
70 
00:05:54,930 --> 00:05:58,490 
The influence of big data pushes for 
71 
00:05:58,490 --> 00:06:02,930 
alternative scalability approaches
at each step of the process. 
72 
00:06:04,010 --> 00:06:07,080 
Just like you would scale
each step on its own, 
73 
00:06:07,080 --> 00:06:10,780 
you can scale the whole
process as a whole in the end. 
74 
00:06:14,613 --> 00:06:20,900 
One can simply say, all of these steps
have reporting needs in different forms. 
75 
00:06:23,280 --> 00:06:30,290 
Or there is a need to draw all these
activities as an iterating process, 
76 
00:06:30,290 --> 00:06:35,710 
including build, explore, and
scale for big data as steps. 
77 
00:06:38,550 --> 00:06:42,550 
Big data analysis needs alternative
data management techniques and 
78 
00:06:42,550 --> 00:06:47,290 
systems, as well as analytical tools and
methods. 
79 
00:06:49,160 --> 00:06:54,730 
Multiple modes of scalability is needed
based on dynamic data and computing loads. 
80 
00:06:55,750 --> 00:06:59,800 
In addition,
change in physical infrastructure, 
81 
00:06:59,800 --> 00:07:04,320 
streaming data specific urgencies
arising from special events 
82 
00:07:04,320 --> 00:07:07,780 
can also require multiple
modes of scalability. 
83 
00:07:09,230 --> 00:07:11,490 
In this intro course, for simplicity, 
84 
00:07:11,490 --> 00:07:17,470 
we will refer to the process as a set of
five sequential activities that iterate. 
85 
00:07:18,500 --> 00:07:23,550 
However, we'll touch on scalability as
needed in our example applications. 
86 
00:07:26,840 --> 00:07:30,190 
As a part of building
your big data process, 
87 
00:07:30,190 --> 00:07:34,230 
it's important to simply
mention two other P's. 
88 
00:07:34,230 --> 00:07:39,620 
The first one is big data platforms,
like the ones in the Hadoop framework, 
89 
00:07:39,620 --> 00:07:43,610 
or other computing platforms
to scale different steps. 
90 
00:07:43,610 --> 00:07:47,060 
The scalability should be in
the mind of all team members and 
91 
00:07:47,060 --> 00:07:49,030 
get communicated as an expectation. 
92 
00:07:51,390 --> 00:07:55,970 
In addition, the scalable process
should be programmable through 
93 
00:07:55,970 --> 00:08:01,830 
utilization of reusable and
reproducible programming interfaces 
94 
00:08:01,830 --> 00:08:06,308 
to libraries, like systems middleware,
analytical tools, 
95 
00:08:06,308 --> 00:08:10,395 
visualization environments, and
end user reporting environments. 
96 
00:08:14,010 --> 00:08:16,980 
Thinking of big data
applications as a process, 
97 
00:08:16,980 --> 00:08:22,150 
including a set of activities that
the team members can collaborate over, 
98 
00:08:22,150 --> 00:08:26,720 
also helps to build metrics for
accountability to be built into it. 
99 
00:08:26,720 --> 00:08:31,000 
This way, expectations on cost, time, 
100 
00:08:31,000 --> 00:08:34,610 
optimization of deliverables,
and time lines can be discussed 
101 
00:08:34,610 --> 00:08:39,210 
between the the members starting with
the beginning of the data science process. 
102 
00:08:42,325 --> 00:08:45,890 
Sometimes we may not be able
to do this in one step. 
103 
00:08:47,550 --> 00:08:52,420 
And joint explorations like statistical
evaluations of intermediate results or 
104 
00:08:52,420 --> 00:08:55,060 
accuracy of sample data
sets become important. 
105 
00:08:57,150 --> 00:09:02,430 
As a summary, data science can be
defined as a craft of using the five P's 
106 
00:09:02,430 --> 00:09:07,380 
identified in this lecture,
leading to a sixth P, the data product. 
107 
00:09:08,580 --> 00:09:12,870 
Having a process within the more
business-driven Ps, like people and 
108 
00:09:12,870 --> 00:09:17,120 
purpose, and the more technically
driven P's, like platforms and 
109 
00:09:17,120 --> 00:09:22,220 
programmability, leads to
a streamlined approach that starts and 
110 
00:09:22,220 --> 00:09:26,630 
ends with the product, team
accountability, and collaboration in mind. 
111 
00:09:27,740 --> 00:09:31,311 
Data science process
provides guidelines for 
112 
00:09:31,311 --> 00:09:36,621 
implementing big data solution,
as it helps to organize efforts and 
113 
00:09:36,621 --> 00:09:43,137 
ensures all critical steps taken conforms
to pre-define and agreed upon metrics. 
1 
00:00:03,008 --> 00:00:05,098 
Step one, acquiring data. 
2 
00:00:19,898 --> 00:00:24,420 
The first step in the data science
process is to acquire the data. 
3 
00:00:25,660 --> 00:00:29,860 
You need to obtain the source material
before analyzing or acting on it. 
4 
00:00:31,950 --> 00:00:37,690 
The first step in acquiring data is
to determine what data is available. 
5 
00:00:37,690 --> 00:00:41,680 
Leave no stone unturned when it comes
to finding the right data sources. 
6 
00:00:42,770 --> 00:00:46,150 
You want to identify suitable
data related to your problem and 
7 
00:00:47,200 --> 00:00:51,540 
make use of all data that is relevant
to your problem for analysis. 
8 
00:00:52,720 --> 00:00:56,580 
Leaving out even a small
amount of important data 
9 
00:00:56,580 --> 00:00:58,570 
can lead to incorrect conclusions. 
10 
00:01:01,190 --> 00:01:04,460 
Data, comes from, many places, local and 
11 
00:01:04,460 --> 00:01:09,280 
remote, in many varieties,
structured and un-structured. 
12 
00:01:09,280 --> 00:01:12,250 
And, with different velocities. 
13 
00:01:12,250 --> 00:01:17,780 
There are many techniques and technologies
to access these different types of data. 
14 
00:01:17,780 --> 00:01:19,420 
Let's discuss a few examples. 
15 
00:01:21,360 --> 00:01:26,020 
A lot of data exists in
conventional relational databases, 
16 
00:01:26,020 --> 00:01:28,560 
like structure big data
from organizations. 
17 
00:01:29,600 --> 00:01:35,270 
The tool of choice to access data from
databases is structured query language or 
18 
00:01:35,270 --> 00:01:40,360 
SQL, which is supported by all
relational databases management systems. 
19 
00:01:41,680 --> 00:01:47,260 
Additionally, most data base systems
come with a graphical application 
20 
00:01:47,260 --> 00:01:52,250 
environment that allows you to query and
explore the data sets in the database. 
21 
00:01:54,990 --> 00:02:01,870 
Data can also exist in files such as
text files and Excel spreadsheets. 
22 
00:02:01,870 --> 00:02:05,638 
Scripting languages are generally
used to get data from files. 
23 
00:02:05,638 --> 00:02:11,200 
A scripting language is a high
level programming language 
24 
00:02:11,200 --> 00:02:16,020 
that can be either general purpose or
specialized for specific functions. 
25 
00:02:17,960 --> 00:02:24,010 
Common scripting languages with support
for processing files are Java Script, 
26 
00:02:24,010 --> 00:02:29,300 
Python, PHP, Perl, R, and
MATLAB, and are many others. 
27 
00:02:31,780 --> 00:02:36,230 
An increasingly popular way
to get data is from websites. 
28 
00:02:36,230 --> 00:02:40,956 
Web pages are written using
a set of standards approved by 
29 
00:02:40,956 --> 00:02:44,906 
a world wide web consortium or
shortly, W3C. 
30 
00:02:44,906 --> 00:02:48,650 
This includes a variety of formats and
services. 
31 
00:02:49,920 --> 00:02:55,860 
One common format is
the Extensible Markup Language, or XML, 
32 
00:02:55,860 --> 00:03:00,640 
which uses markup symbols or tabs to
describe the contents on a webpage. 
33 
00:03:02,250 --> 00:03:07,920 
Many websites also host web services which
produce program access to their data. 
34 
00:03:10,310 --> 00:03:13,110 
There are several types of web services. 
35 
00:03:13,110 --> 00:03:16,990 
The most popular is REST because it's so
easy to use. 
36 
00:03:18,080 --> 00:03:22,280 
REST stand for
Representational State Transfer. 
37 
00:03:22,280 --> 00:03:26,640 
And it is an approach to implementing
web services with performance, 
38 
00:03:26,640 --> 00:03:29,200 
scalability and maintainability in mind. 
39 
00:03:30,940 --> 00:03:34,400 
Web socket services are also
becoming more popular 
40 
00:03:34,400 --> 00:03:37,440 
since they allow real time
modifications from web sites. 
41 
00:03:40,040 --> 00:03:44,840 
NoSQL storage systems are increasingly
used to manage a variety of data 
42 
00:03:44,840 --> 00:03:45,770 
types in big data. 
43 
00:03:46,950 --> 00:03:50,710 
These data stores are databases
that do not represent data 
44 
00:03:50,710 --> 00:03:55,580 
in a table format with columns and rows as
with conventional relational databases. 
45 
00:03:56,910 --> 00:04:02,180 
Examples of these data stores include
Cassandra, MongoDB and HBASE. 
46 
00:04:03,815 --> 00:04:08,350 
NoSQL data stores provide APIs
to allow users to access data. 
47 
00:04:09,460 --> 00:04:14,940 
These APIs can be used directly or in an
application that needs to access the data. 
48 
00:04:16,660 --> 00:04:21,240 
Additionally, most NoSQL
systems provide data access 
49 
00:04:21,240 --> 00:04:23,960 
via a web service interface, such a REST. 
50 
00:04:26,780 --> 00:04:29,860 
Now, let's discuss our wildfire case study 
51 
00:04:29,860 --> 00:04:34,350 
as a real project that acquires data
using several different mechanisms. 
52 
00:04:35,780 --> 00:04:42,020 
The WIFIRE project stores sensor data from
weather stations in a relational database. 
53 
00:04:43,140 --> 00:04:48,110 
We use SQL to retrieve this data
from the database to create 
54 
00:04:48,110 --> 00:04:52,800 
models to identify weather patterns
associated with Santa Anna conditions. 
55 
00:04:54,450 --> 00:04:58,910 
To determine whether a particular weather
station is currently experiencing 
56 
00:04:58,910 --> 00:05:05,340 
Santa Anna conditions, we access real
time data using a web socket service. 
57 
00:05:06,610 --> 00:05:10,170 
Once we start listening to this service, 
58 
00:05:10,170 --> 00:05:12,980 
we receive weather station
measurements as they occur. 
59 
00:05:14,330 --> 00:05:19,450 
This data is then processed and
compared to patterns found by our models 
60 
00:05:19,450 --> 00:05:23,390 
to determine if a weather station is
experiencing Santa Ana conditions. 
61 
00:05:25,060 --> 00:05:28,130 
At the same time Tweets are retrieved 
62 
00:05:28,130 --> 00:05:32,870 
using hashtags related to any fire
that is occurring in the region. 
63 
00:05:33,950 --> 00:05:38,080 
The Tweet messages are retrieves
using the Twitter REST service. 
64 
00:05:38,080 --> 00:05:43,260 
The idea is to determine the sentiment
of these tweets to see if people 
65 
00:05:43,260 --> 00:05:50,540 
are expressing fear, anger or are simply
nonchalant about the nearby fire. 
66 
00:05:50,540 --> 00:05:55,617 
The combination of sensor data and
tweet sentiments helps 
67 
00:05:55,617 --> 00:06:00,389 
to give us a sense of the urgency
of the fire situation. 
68 
00:06:00,389 --> 00:06:04,570 
As a summary,
big data comes from many places. 
69 
00:06:05,570 --> 00:06:08,000 
Finding and evaluating data 
70 
00:06:08,000 --> 00:06:12,490 
useful to your big data analytics is
important before you start acquiring data. 
71 
00:06:13,740 --> 00:06:15,080 
Depending on the source and 
72 
00:06:15,080 --> 00:06:19,240 
structure of data,
there are alternative ways to access it. 
1 
00:00:02,992 --> 00:00:05,893 
Step 2-A: Exploring Data 
2 
00:00:18,798 --> 00:00:23,194 
After you've put together the data
that you need for your application, 
3 
00:00:23,194 --> 00:00:27,460 
you might be tempted to immediately
build models to analyze the data. 
4 
00:00:28,520 --> 00:00:29,830 
Resist this temptation. 
5 
00:00:30,890 --> 00:00:35,370 
The first step after getting
your data is to explore it. 
6 
00:00:35,370 --> 00:00:39,340 
Exploring data is a part of
the two-step data preparation process. 
7 
00:00:41,450 --> 00:00:44,560 
You want to do some
preliminary investigation 
8 
00:00:44,560 --> 00:00:50,080 
in order to gain a better understanding of
the specific characteristics of your data. 
9 
00:00:50,080 --> 00:00:52,620 
In this step, you'll be looking for 
10 
00:00:52,620 --> 00:00:56,350 
things like correlations,
general trends, and outliers. 
11 
00:00:57,530 --> 00:01:01,700 
Without this step, you will not be
able to use the data effectively. 
12 
00:01:04,100 --> 00:01:07,520 
Correlation graphs can be used
to explore the dependencies 
13 
00:01:07,520 --> 00:01:09,630 
between different variables in the data. 
14 
00:01:11,210 --> 00:01:15,910 
Graphing the general trends of
variables will show you if there is 
15 
00:01:15,910 --> 00:01:21,120 
a consistent direction in which the values
of these variables are moving towards, 
16 
00:01:22,150 --> 00:01:24,320 
like sales prices going up or down. 
17 
00:01:26,200 --> 00:01:34,320 
In statistics, an outlier is a data point
that's distant from other data points. 
18 
00:01:34,320 --> 00:01:37,256 
Plotting outliers will
help you double check for 
19 
00:01:37,256 --> 00:01:39,698 
errors in the data due to measurements. 
20 
00:01:39,698 --> 00:01:45,390 
In some cases, outliers that are not
errors might make you find a rare event. 
21 
00:01:47,670 --> 00:01:53,860 
Additionally, summary statistics provide
numerical values to describe your data. 
22 
00:01:55,040 --> 00:02:00,195 
Summary statistics are quantities
that capture various characteristics 
23 
00:02:00,195 --> 00:02:04,540 
of a set of values with a single number or
a small set of numbers. 
24 
00:02:06,540 --> 00:02:10,674 
Some basic summary statistics
that you should compute for 
25 
00:02:10,674 --> 00:02:15,490 
your data set are mean, median,
range, and standard deviation. 
26 
00:02:17,000 --> 00:02:22,510 
Mean and median are measures of
the location of a set of values. 
27 
00:02:22,510 --> 00:02:26,590 
Mode is the value that occurs
most frequently in your data set. 
28 
00:02:27,920 --> 00:02:32,220 
And range and standard deviation
are measures of spread in your data. 
29 
00:02:33,640 --> 00:02:38,810 
Looking at these measures will give you
an idea of the nature of your data. 
30 
00:02:40,630 --> 00:02:43,340 
They can tell you if there's
something wrong with your data. 
31 
00:02:44,340 --> 00:02:48,957 
For example, if the range of the values
for age in your data includes 
32 
00:02:48,957 --> 00:02:52,926 
negative numbers, or
a number much greater than 100, 
33 
00:02:52,926 --> 00:02:57,708 
there's something suspicious in
the data that needs to be examined. 
34 
00:03:00,629 --> 00:03:05,900 
Visualization techniques also
provide a quick and effective, and 
35 
00:03:05,900 --> 00:03:11,830 
overall a very useful way to look at
data in this preliminary analysis step. 
36 
00:03:11,830 --> 00:03:15,250 
A heat map, such as the one shown here, 
37 
00:03:15,250 --> 00:03:19,530 
can quickly give you the idea
of where the hotspots are. 
38 
00:03:19,530 --> 00:03:22,020 
Many other different types
of graphs can be used. 
39 
00:03:23,340 --> 00:03:27,320 
Histograms show that
the distribution of the data and 
40 
00:03:27,320 --> 00:03:30,520 
can show skewness or unusual dispersion. 
41 
00:03:31,920 --> 00:03:36,168 
Boxplots are another type of plot for
showing data distribution. 
42 
00:03:38,448 --> 00:03:44,410 
Line graphs are useful for seeing how
values in your data change over time. 
43 
00:03:44,410 --> 00:03:47,140 
Spikes in the data are also easy to spot. 
44 
00:03:49,550 --> 00:03:54,070 
Scatter plots can show you
correlation between two variables. 
45 
00:03:54,070 --> 00:03:57,900 
Overall, there are many types
of graph to visualize data. 
46 
00:03:58,935 --> 00:04:02,030 
They are very useful in helping
you understand the data you have. 
47 
00:04:04,120 --> 00:04:08,720 
In summary, what you get by
exploring your data is a better 
48 
00:04:08,720 --> 00:04:12,510 
understanding of the complexity of
the data you have to work with. 
49 
00:04:13,780 --> 00:04:16,418 
This, in turn,
will guide the rest of your process. 
1 
00:00:00,005 --> 00:00:06,278 
Step 2-B: Pre-processing Data 
2 
00:00:20,283 --> 00:00:24,904 
The raw data that you get directly from
your sources are never in the format that 
3 
00:00:24,904 --> 00:00:26,875 
you need to perform analysis on. 
4 
00:00:27,995 --> 00:00:31,885 
There are two main goals in
the data pre-processing step. 
5 
00:00:31,885 --> 00:00:36,455 
The first is to clean the data to
address data quality issues, and 
6 
00:00:36,455 --> 00:00:41,320 
the second is to transform the raw
data to make it suitable for analysis. 
7 
00:00:43,850 --> 00:00:47,050 
A very important part of data preparation 
8 
00:00:47,050 --> 00:00:49,606 
is to address quality
of issues in your data. 
9 
00:00:49,606 --> 00:00:54,000 
Real-world data is messy. 
10 
00:00:54,000 --> 00:01:00,160 
There are many examples of quality issues
with data from real applications including 
11 
00:01:00,160 --> 00:01:05,990 
inconsistent data like a customer with two
different addresses, duplicate customer 
12 
00:01:05,990 --> 00:01:11,460 
records, for example, customers address
recorded at two different sales locations. 
13 
00:01:12,490 --> 00:01:14,600 
And the two recordings don't agree. 
14 
00:01:16,190 --> 00:01:19,160 
Missing customer agent demographics or
studies. 
15 
00:01:21,010 --> 00:01:25,750 
Missing values like missing a customer
age in the demographic studies. 
16 
00:01:27,000 --> 00:01:32,160 
invalid data like an invalid zip code for
example, a six digit code. 
17 
00:01:33,350 --> 00:01:39,060 
And outliers like a sense of failure
causing values to be much higher or 
18 
00:01:39,060 --> 00:01:41,480 
lower than expected for a period of time. 
19 
00:01:43,340 --> 00:01:45,860 
Since we get the data downstream 
20 
00:01:45,860 --> 00:01:48,960 
we usually have little control
over how the data is collected. 
21 
00:01:50,390 --> 00:01:55,120 
Preventing data quality problems as
the data is being collected is not 
22 
00:01:55,120 --> 00:01:56,070 
often an option. 
23 
00:01:57,740 --> 00:01:59,950 
So we have the data that we get and 
24 
00:01:59,950 --> 00:02:03,770 
we have to address quality issues
by detecting and correcting them. 
25 
00:02:05,710 --> 00:02:09,170 
Here are some approaches we can take
to address this quality issues. 
26 
00:02:11,780 --> 00:02:14,630 
We can remove data records
with missing values. 
27 
00:02:16,330 --> 00:02:19,290 
We can merge duplicate records. 
28 
00:02:19,290 --> 00:02:23,430 
This will require a way to determine
how to resolve conflicting values. 
29 
00:02:24,780 --> 00:02:29,320 
Perhaps it makes sense to retain the newer
value whenever there's a conflict. 
30 
00:02:31,030 --> 00:02:34,500 
For invalid values, the best estimate for 
31 
00:02:34,500 --> 00:02:38,330 
a reasonable value can be
used as a replacement. 
32 
00:02:38,330 --> 00:02:42,800 
For example, for
a missing age value for an employee, 
33 
00:02:42,800 --> 00:02:47,320 
a reasonable value can be estimated based
on the employee's length of employment. 
34 
00:02:49,820 --> 00:02:54,340 
Outliers can also be removed if
they are not important to the task. 
35 
00:02:56,590 --> 00:03:00,370 
In order to address data
quality issues effectively, 
36 
00:03:00,370 --> 00:03:04,590 
knowledge about the application,
such as how the data was collected, 
37 
00:03:04,590 --> 00:03:09,880 
the user population, and the intended
uses of the application is important. 
38 
00:03:11,680 --> 00:03:15,280 
This domain knowledge is
essential to making informed 
39 
00:03:15,280 --> 00:03:19,070 
decisions on how to handle incomplete or
incorrect data. 
40 
00:03:22,740 --> 00:03:25,480 
The second part of preparing data 
41 
00:03:25,480 --> 00:03:29,299 
is to manipulate the clean data into
the format needed for analysis. 
42 
00:03:30,500 --> 00:03:32,910 
The step is known by many names. 
43 
00:03:34,940 --> 00:03:39,894 
Data manipulation,
data preprocessing, data wrangling, 
44 
00:03:39,894 --> 00:03:43,419 
and even data munging, some operations for 
45 
00:03:43,419 --> 00:03:49,420 
this type of operation I mean data
munging, wrangling, preprocessing, 
46 
00:03:49,420 --> 00:03:54,373 
include, scaling, transformation,
feature selection, 
47 
00:03:54,373 --> 00:03:58,778 
dimensionality reduction,
and data manipulation. 
48 
00:04:01,628 --> 00:04:08,840 
Scaling involves changing the range of
values to be between a specified range. 
49 
00:04:08,840 --> 00:04:10,970 
Such as from zero to one. 
50 
00:04:12,220 --> 00:04:16,460 
This is done to avoid having certain
features that large values from 
51 
00:04:16,460 --> 00:04:18,570 
dominating the results. 
52 
00:04:18,570 --> 00:04:22,990 
For example,
in analyzing data with height and weight. 
53 
00:04:22,990 --> 00:04:27,350 
To magnitude of weight values is much
greater than of the height values. 
54 
00:04:29,220 --> 00:04:32,880 
So scaling all values
to be between zero and 
55 
00:04:32,880 --> 00:04:37,670 
one will equalize contributions from
both height and weight features. 
56 
00:04:40,900 --> 00:04:46,000 
Various transformations can be performed
on the data to reduce noise and 
57 
00:04:46,000 --> 00:04:46,720 
variability. 
58 
00:04:48,380 --> 00:04:50,970 
One such transformation is aggregation. 
59 
00:04:52,430 --> 00:04:57,260 
Aggregate data generally results
in data with less variability, 
60 
00:04:57,260 --> 00:04:58,890 
which may help with your analysis. 
61 
00:05:00,190 --> 00:05:05,720 
For example, daily sales figures
may have many serious changes. 
62 
00:05:06,730 --> 00:05:11,880 
Aggregating values to weekly or monthly
sales figures will result in similar data. 
63 
00:05:14,720 --> 00:05:19,340 
Other filtering techniques can also be
used to remove variability in the data. 
64 
00:05:19,340 --> 00:05:23,530 
Of course, this comes at
the cost of less detailed data. 
65 
00:05:23,530 --> 00:05:27,760 
So these factors must be weighed for
the specific application. 
66 
00:05:30,570 --> 00:05:36,400 
Future selection can involve removing
redundant or irrelevant features, 
67 
00:05:36,400 --> 00:05:40,300 
combining features, and
creating new features. 
68 
00:05:41,460 --> 00:05:44,070 
During the exploring data step, 
69 
00:05:44,070 --> 00:05:47,860 
you might have discovered that
two features are correlated. 
70 
00:05:49,030 --> 00:05:52,350 
In that case one of these
features can be removed 
71 
00:05:52,350 --> 00:05:55,090 
without negatively affecting
the analysis results. 
72 
00:05:56,170 --> 00:05:59,870 
For example,
the purchase price of a product and 
73 
00:05:59,870 --> 00:06:03,360 
the amount of sales tax paid,
are likely to be correlated. 
74 
00:06:04,800 --> 00:06:08,550 
Eliminating the sales tax amount,
then will be beneficial. 
75 
00:06:10,860 --> 00:06:12,810 
Removing redundant or 
76 
00:06:12,810 --> 00:06:17,010 
irrelevant features will make
the subsequent analysis much simpler. 
77 
00:06:19,320 --> 00:06:25,220 
In other cases, you may want to combine
features or create new ones. 
78 
00:06:25,220 --> 00:06:29,540 
For example,
adding the applicant's education level 
79 
00:06:29,540 --> 00:06:32,820 
as a feature to a loan approval
application would make sense. 
80 
00:06:34,940 --> 00:06:39,790 
There are also algorithms to automatically
determine the most relevant features, 
81 
00:06:39,790 --> 00:06:42,130 
based on various mathematical properties. 
82 
00:06:45,440 --> 00:06:50,200 
Dimensionality reduction is useful when
the data set has a large number of 
83 
00:06:50,200 --> 00:06:50,750 
dimensions. 
84 
00:06:52,020 --> 00:06:55,830 
It involves finding a smaller
subset of dimensions that 
85 
00:06:55,830 --> 00:06:58,350 
captures most of
the variation in the data. 
86 
00:07:00,030 --> 00:07:03,600 
This reduces the dimensions of the data 
87 
00:07:03,600 --> 00:07:08,400 
while eliminating irrelevant features and
makes analysis simpler. 
88 
00:07:10,130 --> 00:07:12,089 
A technique commonly used for 
89 
00:07:12,089 --> 00:07:16,878 
dimensional reduction is called
principle component analysis or PCA. 
90 
00:07:20,528 --> 00:07:26,120 
Raw data often has to be manipulated to
be in the correct format for analysis. 
91 
00:07:27,140 --> 00:07:32,890 
For example, from samples recording
daily changes in stock prices, 
92 
00:07:32,890 --> 00:07:35,520 
we may want the capture price changes for 
93 
00:07:35,520 --> 00:07:38,990 
a particular market segments
like real estate or health care. 
94 
00:07:40,230 --> 00:07:45,150 
This would require determining which
stocks belong to which market segment. 
95 
00:07:45,150 --> 00:07:49,540 
Grouping them together, and
perhaps computing the mean, range, 
96 
00:07:49,540 --> 00:07:51,530 
standard deviation for each group. 
97 
00:07:54,120 --> 00:07:54,760 
In summary, 
98 
00:07:55,910 --> 00:08:00,470 
data preparation is a very important
part of the data science process. 
99 
00:08:00,470 --> 00:08:05,530 
In fact, this is where you will spend most
of your time on any data science effort. 
100 
00:08:06,710 --> 00:08:11,680 
It can be a tedious process,
but it is a crucial step. 
101 
00:08:11,680 --> 00:08:15,200 
Always remember, garbage in, garbage out. 
102 
00:08:15,200 --> 00:08:16,987 
If you don't spend the time and 
103 
00:08:16,987 --> 00:08:21,250 
effort to create good data for the
analysis, you will not get good results 
104 
00:08:21,250 --> 00:08:25,399 
no matter how sophisticated
the analysis technique you're using is. 
1 
00:00:00,005 --> 00:00:03,004 
Step 3: Analyzing Data. 
2 
00:00:12,608 --> 00:00:15,650 
Now that you have your
data nicely prepared, 
3 
00:00:15,650 --> 00:00:18,140 
the next step is to analyze the data. 
4 
00:00:19,350 --> 00:00:23,620 
Data analysis involves building
a model from your data, 
5 
00:00:23,620 --> 00:00:25,350 
which is called input data. 
6 
00:00:26,620 --> 00:00:32,370 
The input data is used by the analysis
technique to build a model. 
7 
00:00:34,010 --> 00:00:38,340 
What your model generates
is the output data. 
8 
00:00:38,340 --> 00:00:41,140 
There are different types of problems, and 
9 
00:00:41,140 --> 00:00:43,890 
so there are different types
of analysis techniques. 
10 
00:00:45,370 --> 00:00:50,424 
The main categories of analysis techniques
are classification, regression, 
11 
00:00:50,424 --> 00:00:56,330 
clustering, association analysis,
and graph analysis. 
12 
00:00:56,330 --> 00:00:58,520 
We will describe each one. 
13 
00:00:58,520 --> 00:01:03,650 
In classification, the goal is to
predict the category of the input data. 
14 
00:01:04,740 --> 00:01:10,102 
An example of this is predicting
the weather as being sunny, 
15 
00:01:10,102 --> 00:01:13,850 
rainy, windy, or cloudy in this case. 
16 
00:01:13,850 --> 00:01:19,820 
Another example is to classify
a tumor as either benign or malignant. 
17 
00:01:21,170 --> 00:01:27,270 
In this case, the classification is
referred to as binary classification, 
18 
00:01:27,270 --> 00:01:29,830 
since there are only two categories. 
19 
00:01:29,830 --> 00:01:32,660 
But you can have many categories as well, 
20 
00:01:32,660 --> 00:01:37,530 
as the weather prediction problem
shown here having four categories. 
21 
00:01:37,530 --> 00:01:41,780 
Another example is to identify
handwritten digits as 
22 
00:01:41,780 --> 00:01:45,860 
being in one of the ten
categories from zero to nine. 
23 
00:01:48,160 --> 00:01:53,310 
When your model has to predict
a numeric value instead of a category, 
24 
00:01:53,310 --> 00:01:56,080 
then the task becomes
a regression problem. 
25 
00:01:57,380 --> 00:02:01,210 
An example of regression is to
predict the price of a stock. 
26 
00:02:02,410 --> 00:02:06,250 
The stock price is a numeric value,
not a category. 
27 
00:02:06,250 --> 00:02:09,560 
So this is a regression task
instead of a classification task. 
28 
00:02:11,300 --> 00:02:15,020 
Other examples of regression
are estimating the weekly sales of a new 
29 
00:02:15,020 --> 00:02:19,740 
product and
predicting the score on a test. 
30 
00:02:19,740 --> 00:02:25,910 
In clustering, the goal is to
organize similar items into groups. 
31 
00:02:25,910 --> 00:02:31,390 
An example is grouping a company's
customer base into distinct segments for 
32 
00:02:31,390 --> 00:02:36,360 
more effective targeted marketing
like seniors, adults and 
33 
00:02:36,360 --> 00:02:38,420 
teenagers, as we see here. 
34 
00:02:38,420 --> 00:02:43,050 
Another such example is identifying
areas of similar topography, 
35 
00:02:43,050 --> 00:02:47,420 
like mountains, deserts,
plains for land use application. 
36 
00:02:47,420 --> 00:02:52,300 
Yet another example is determining
different groups of weather patterns, 
37 
00:02:52,300 --> 00:02:55,150 
like rainy, cold or snowy. 
38 
00:02:55,150 --> 00:02:58,740 
The goal in association analysis
is to come up with a set 
39 
00:02:58,740 --> 00:03:03,330 
of rules to capture associations
within items or events. 
40 
00:03:03,330 --> 00:03:07,935 
The rules are used to determine when
items or events occur together. 
41 
00:03:07,935 --> 00:03:12,300 
A common application of association
analysis is known as market 
42 
00:03:12,300 --> 00:03:17,710 
basket analysis, which is used to
understand customer purchasing behavior. 
43 
00:03:17,710 --> 00:03:22,660 
For example, association analysis
can reveal that banking customers 
44 
00:03:22,660 --> 00:03:27,040 
who have certificate of deposit accounts,
surety CDs, also 
45 
00:03:27,040 --> 00:03:31,790 
tend to be interested in other investment
vehicles, such as money market accounts. 
46 
00:03:31,790 --> 00:03:35,000 
This information can be used for
cross-selling. 
47 
00:03:35,000 --> 00:03:39,140 
If you advertise money market
accounts to your customers with CDs, 
48 
00:03:39,140 --> 00:03:42,100 
they're likely to open such an account. 
49 
00:03:42,100 --> 00:03:47,150 
According to data mining folklore,
a supermarket chain used association 
50 
00:03:47,150 --> 00:03:51,990 
analysis to discover a connection between
two seemingly unrelated products. 
51 
00:03:51,990 --> 00:03:56,940 
They discovered that many customers who go
to the supermarket late on Sunday night 
52 
00:03:56,940 --> 00:04:02,330 
to buy diapers also tend to buy beer,
who are likely to be fathers. 
53 
00:04:02,330 --> 00:04:05,590 
This information was then
used to place beer and 
54 
00:04:05,590 --> 00:04:10,720 
diapers close together and
they saw a jump in sales of both items. 
55 
00:04:10,720 --> 00:04:13,760 
This is the famous diaper beer connection. 
56 
00:04:13,760 --> 00:04:18,090 
When your data can be transformed into
a graph representation with nodes and 
57 
00:04:18,090 --> 00:04:22,330 
links, then you want to use graph
analytics to analyze your data. 
58 
00:04:22,330 --> 00:04:26,220 
This kind of data comes about when
you have a lot of entities and 
59 
00:04:26,220 --> 00:04:30,150 
connections between those entities,
like social networks. 
60 
00:04:30,150 --> 00:04:35,111 
Some examples where graph analytics can
be useful are exploring the spread of 
61 
00:04:35,111 --> 00:04:39,780 
a disease or epidemic by analyzing
hospitals' and doctors' records. 
62 
00:04:39,780 --> 00:04:44,849 
Identification of security threats
by monitoring social media, 
63 
00:04:44,849 --> 00:04:46,483 
email and text data. 
64 
00:04:46,483 --> 00:04:51,015 
And optimization of mobile
communications network traffic. 
65 
00:04:51,015 --> 00:04:56,110 
And optimization of mobile
telecommunications network traffic, 
66 
00:04:56,110 --> 00:04:59,690 
to ensure call quality and
reduce dropped calls. 
67 
00:04:59,690 --> 00:05:03,850 
Modeling starts with selecting,
one of the techniques we listed 
68 
00:05:03,850 --> 00:05:08,680 
as the appropriate analysis technique,
depending on the type of problem you have. 
69 
00:05:08,680 --> 00:05:12,690 
Then you construct the model
using the data you've prepared. 
70 
00:05:12,690 --> 00:05:17,210 
To validate the model,
you apply it to new data samples. 
71 
00:05:17,210 --> 00:05:19,960 
This is to evaluate how well the model 
72 
00:05:19,960 --> 00:05:22,540 
does on data that was
used to construct it. 
73 
00:05:22,540 --> 00:05:26,870 
The common practice is to divide
the prepared data into a set of data for 
74 
00:05:26,870 --> 00:05:30,170 
constructing the model and
reserving some of the data for 
75 
00:05:30,170 --> 00:05:33,330 
evaluating the model after
it has been constructed. 
76 
00:05:33,330 --> 00:05:37,740 
You can also use new data prepared the
same way as with the data that was used to 
77 
00:05:37,740 --> 00:05:39,060 
construct model. 
78 
00:05:39,060 --> 00:05:43,950 
Evaluating the model depends on the type
of analysis techniques you used. 
79 
00:05:43,950 --> 00:05:47,595 
Let's briefly look at how
to evaluate each technique. 
80 
00:05:47,595 --> 00:05:52,595 
For classification and regression,
you will have the correct output for 
81 
00:05:52,595 --> 00:05:54,655 
each sample in your input data. 
82 
00:05:54,655 --> 00:05:56,945 
Comparing the correct output and 
83 
00:05:56,945 --> 00:06:01,880 
the output predicted by the model,
provides a way to evaluate the model. 
84 
00:06:01,880 --> 00:06:05,150 
For clustering,
the groups resulting from clustering 
85 
00:06:05,150 --> 00:06:08,850 
should be examined to see if they
make sense for your application. 
86 
00:06:08,850 --> 00:06:13,250 
For example, do the customer
segments reflect your customer base? 
87 
00:06:13,250 --> 00:06:17,310 
Are they helpful for
use in your targeted marketing campaigns? 
88 
00:06:17,310 --> 00:06:19,170 
For association analysis and 
89 
00:06:19,170 --> 00:06:24,940 
graph analysis, some investigation will be
needed to see if the results are correct. 
90 
00:06:24,940 --> 00:06:27,920 
For example, network traffic delays 
91 
00:06:27,920 --> 00:06:33,220 
need to be investigated to see what your
model predicts is actually happening. 
92 
00:06:33,220 --> 00:06:36,830 
And whether the sources of the delays
are where they are predicted 
93 
00:06:36,830 --> 00:06:38,050 
to be in the real system. 
94 
00:06:39,160 --> 00:06:44,450 
After you have evaluated your model to get
a sense of its performance on your data, 
95 
00:06:44,450 --> 00:06:47,510 
you will be able to
determine the next steps. 
96 
00:06:47,510 --> 00:06:51,000 
Some questions to consider are,
should the analysis be 
97 
00:06:51,000 --> 00:06:55,340 
performed with more data in order
to get a better model performance? 
98 
00:06:55,340 --> 00:06:57,760 
Would using different data types help? 
99 
00:06:57,760 --> 00:06:59,990 
For example, in your clustering results, 
100 
00:06:59,990 --> 00:07:04,070 
is it difficult to distinguish
customers from distinct regions? 
101 
00:07:04,070 --> 00:07:07,050 
Would adding zip code
to your input data help 
102 
00:07:07,050 --> 00:07:10,150 
to generate finer grained
customer segments? 
103 
00:07:10,150 --> 00:07:14,060 
Do the analysis results suggest
a more detailed look at 
104 
00:07:14,060 --> 00:07:15,900 
some aspect of the problem? 
105 
00:07:15,900 --> 00:07:19,690 
For example, predicting sunny
weather gives very good results, 
106 
00:07:19,690 --> 00:07:22,390 
but rainy weather
predictions are just so-so. 
107 
00:07:22,390 --> 00:07:27,840 
This means that you should take a closer
look at your examples for rainy weather. 
108 
00:07:27,840 --> 00:07:31,520 
Perhaps you just need more
samples of rainy weather, or 
109 
00:07:31,520 --> 00:07:34,845 
perhaps there are some
anomalies in those samples. 
110 
00:07:34,845 --> 00:07:40,380 
Or maybe there are some missing data
that needs to be included in order 
111 
00:07:40,380 --> 00:07:42,440 
to completely capture rainy weather. 
112 
00:07:42,440 --> 00:07:47,037 
The ideal situation would be that your
model platforms very well with respect to 
113 
00:07:47,037 --> 00:07:49,620 
the success criteria that were determined 
114 
00:07:49,620 --> 00:07:53,380 
when you defined the problem at
the beginning of the project. 
115 
00:07:53,380 --> 00:07:56,920 
In that case, you're ready to
move on to communicating and 
116 
00:07:56,920 --> 00:08:00,870 
acting on the results that you
obtained from your analysis. 
117 
00:08:00,870 --> 00:08:05,670 
As a summary, data analysis involves
selecting the appropriate technique for 
118 
00:08:05,670 --> 00:08:09,940 
your problem, building the model,
then evaluating the results. 
119 
00:08:11,150 --> 00:08:13,516 
As there are different types of problems, 
120 
00:08:13,516 --> 00:08:16,678 
there are also different
types of analysis techniques. 
1 
00:00:02,925 --> 00:00:05,646 
Step four, reporting insights. 
2 
00:00:15,628 --> 00:00:20,498 
The fourth step in our data science
process is reporting the insights gained 
3 
00:00:20,498 --> 00:00:21,800 
from our analysis. 
4 
00:00:23,660 --> 00:00:27,840 
This is a very important step
to communicate your insights and 
5 
00:00:27,840 --> 00:00:30,320 
make a case for
what actions should follow. 
6 
00:00:32,040 --> 00:00:37,380 
It can change shape based on your
audience and should not be taken lightly. 
7 
00:00:38,510 --> 00:00:39,760 
So how do you get started? 
8 
00:00:43,150 --> 00:00:48,040 
The first thing to do is to look
at your analysis results and 
9 
00:00:48,040 --> 00:00:53,720 
decide what to present or report as the
biggest value or biggest set of values. 
10 
00:00:54,990 --> 00:00:59,530 
In deciding what to present you
should ask yourself these questions. 
11 
00:01:01,260 --> 00:01:02,740 
What is the punchline? 
12 
00:01:02,740 --> 00:01:05,330 
In other words, what are the main results? 
13 
00:01:07,930 --> 00:01:12,590 
What added value do
these results provide or 
14 
00:01:12,590 --> 00:01:14,870 
how can the model add to the application? 
15 
00:01:17,120 --> 00:01:21,840 
How do the results compare to
the success criteria determined at 
16 
00:01:21,840 --> 00:01:23,260 
the beginning of the project? 
17 
00:01:26,030 --> 00:01:30,220 
Answers to these questions are the items
you need to include in your report or 
18 
00:01:30,220 --> 00:01:30,990 
presentation. 
19 
00:01:32,120 --> 00:01:36,336 
So make them the main topics and
gather facts to back them up. 
20 
00:01:39,198 --> 00:01:44,330 
Keep in mind that not all of
your results may be rosy. 
21 
00:01:44,330 --> 00:01:49,650 
Your analysis may show results that are
counter to what you were hoping to find, 
22 
00:01:49,650 --> 00:01:53,410 
or results that are inconclusive or
puzzling. 
23 
00:01:54,600 --> 00:01:56,530 
You need to show these results as well. 
24 
00:01:58,620 --> 00:02:02,950 
Domain experts may find some of
these results to be puzzling, and 
25 
00:02:02,950 --> 00:02:06,932 
inconclusive findings may
lead to additional analysis. 
26 
00:02:08,020 --> 00:02:11,920 
Remember the point of
reporting your findings 
27 
00:02:11,920 --> 00:02:14,410 
is to determine what
the next step should be. 
28 
00:02:17,070 --> 00:02:21,538 
All findings must be presented so
that informed decisions can be made. 
29 
00:02:24,818 --> 00:02:29,180 
Visualization is an important
tool in presenting your results. 
30 
00:02:30,220 --> 00:02:35,400 
The techniques that we discuss and
explore in data can be used here as well. 
31 
00:02:35,400 --> 00:02:36,760 
What were they? 
32 
00:02:36,760 --> 00:02:41,690 
Scatter plots, line graphs,
heat maps, and other types of graphs 
33 
00:02:43,040 --> 00:02:46,390 
are effective ways to present
your results visually. 
34 
00:02:48,330 --> 00:02:52,150 
This time you're not
plotting the input data, but 
35 
00:02:52,150 --> 00:02:55,250 
you're plotting the output
data with similar tools. 
36 
00:02:57,120 --> 00:03:01,238 
You should also have tables with
details from your analysis as backups, 
37 
00:03:01,238 --> 00:03:04,418 
if someone wants to take
a deeper dive into the results. 
38 
00:03:07,398 --> 00:03:10,080 
There are many visualization
tools that are available. 
39 
00:03:11,660 --> 00:03:14,750 
Some of the most popular open
source ones are listed here. 
40 
00:03:16,205 --> 00:03:20,370 
R is a software package for
general data analysis. 
41 
00:03:21,990 --> 00:03:24,830 
It has powerful visualization
capabilities as well. 
42 
00:03:26,300 --> 00:03:30,800 
Python is a general purpose
programming language 
43 
00:03:30,800 --> 00:03:35,090 
that also has a number of packages to
support data analysis and graphics. 
44 
00:03:36,510 --> 00:03:39,690 
D3 is a JavaScript library for 
45 
00:03:39,690 --> 00:03:44,820 
producing interactive web based
visualizations and data driven documents. 
46 
00:03:46,500 --> 00:03:51,325 
Leaflet is a lightweight mobile
friendly JavaScript library 
47 
00:03:51,325 --> 00:03:53,650 
to create interactive maps. 
48 
00:03:55,360 --> 00:04:00,656 
Tableau Public Allows you
to create visualizations, 
49 
00:04:00,656 --> 00:04:07,916 
in your public profile, and share them,
or put them, on a site, or blog. 
50 
00:04:07,916 --> 00:04:12,925 
Google Charts provides
cross-browser compatibility, 
51 
00:04:12,925 --> 00:04:17,940 
and closed platform portability
to iPhones and Android. 
52 
00:04:19,430 --> 00:04:27,865 
Timeline is a JavaScript library
that allows you to create timelines. 
53 
00:04:27,865 --> 00:04:33,787 
In summary, you want to report your
findings by presenting your results and 
54 
00:04:33,787 --> 00:04:37,780 
value add with graphs
using visualization tools. 
1 
00:00:02,010 --> 00:00:05,922 
Step 5, Act, turning insights into action. 
2 
00:00:15,758 --> 00:00:20,048 
Now that you have evaluated
the results from your analysis and 
3 
00:00:20,048 --> 00:00:25,329 
generated reports on the potential value
of the results, the next step is to 
4 
00:00:25,329 --> 00:00:31,075 
determine what action or actions should
be taken, based on the insights gained? 
5 
00:00:31,075 --> 00:00:34,290 
Remember why we started
bringing together the data and 
6 
00:00:34,290 --> 00:00:36,570 
analyzing it in the first place? 
7 
00:00:36,570 --> 00:00:41,100 
To find actionable insights
within all these data sets, 
8 
00:00:41,100 --> 00:00:44,970 
to answer questions, or for
improving business processes. 
9 
00:00:46,120 --> 00:00:47,470 
For example, 
10 
00:00:47,470 --> 00:00:51,440 
is there something in your process that
should change to remove bottle necks? 
11 
00:00:52,580 --> 00:00:56,640 
Is there data that should be added to your
application to make it more accurate? 
12 
00:00:57,810 --> 00:01:02,440 
Should you segment your population
into more well defined groups for 
13 
00:01:02,440 --> 00:01:04,240 
more effective targeted marketing? 
14 
00:01:05,510 --> 00:01:09,508 
This is the first step in
turning insights into action. 
15 
00:01:09,508 --> 00:01:12,733 
Now that you've determined
what action to take, 
16 
00:01:12,733 --> 00:01:17,450 
the next step is figuring out
how to implement the action. 
17 
00:01:17,450 --> 00:01:21,610 
What is necessary to add this action
into your process or application? 
18 
00:01:22,820 --> 00:01:24,080 
How should it be automated? 
19 
00:01:25,440 --> 00:01:31,110 
The stakeholders need to be identified and
become involved in this change. 
20 
00:01:31,110 --> 00:01:36,090 
Just as with any process improvement
changes, we need to monitor and 
21 
00:01:36,090 --> 00:01:40,020 
measure the impact of the action
on the process or application. 
22 
00:01:41,300 --> 00:01:45,600 
Assessing the impact
leads to an evaluation. 
23 
00:01:45,600 --> 00:01:49,710 
Evaluating results from the implemented
action will determine your next steps. 
24 
00:01:50,740 --> 00:01:55,640 
Is there additional analysis that need
to be performed in order to yield 
25 
00:01:55,640 --> 00:01:56,730 
even better results? 
26 
00:01:58,100 --> 00:01:59,740 
What data should be revisited? 
27 
00:02:00,950 --> 00:02:04,980 
Are there additional opportunities
that should be explored? 
28 
00:02:04,980 --> 00:02:09,685 
For example, let's not forget
what big data enables us to do. 
29 
00:02:09,685 --> 00:02:16,553 
Real-time actions based on high
velocity streaming information. 
30 
00:02:16,553 --> 00:02:21,067 
We need to define what part of our
business needs real-time action to be 
31 
00:02:21,067 --> 00:02:25,670 
able to influence the operations or
the interaction with the customer. 
32 
00:02:27,450 --> 00:02:32,430 
Once we define these real time actions,
we need to make sure that 
33 
00:02:32,430 --> 00:02:37,080 
there are automated systems, or
processes to perform such actions, and 
34 
00:02:37,080 --> 00:02:41,320 
provide failure recovery
in case of problems. 
35 
00:02:41,320 --> 00:02:43,547 
As a summary, big data and 
36 
00:02:43,547 --> 00:02:49,320 
data science are only useful if
the insights can be turned into action, 
37 
00:02:49,320 --> 00:02:54,198 
and if the actions are carefully
defined and evaluated. 
1 
00:00:01,360 --> 00:00:03,545 
Steps in the Data Science Process. 
2 
00:00:11,405 --> 00:00:16,739 
We have already seen a simple linear
form of data science process, 
3 
00:00:16,739 --> 00:00:21,900 
including five distinct activities
that depend on each other. 
4 
00:00:23,140 --> 00:00:28,095 
Let's summarize each activity further
before we go into the details of each. 
5 
00:00:28,095 --> 00:00:34,617 
Acquire includes anything that makes
us retrieve data including; finding, 
6 
00:00:34,617 --> 00:00:39,210 
accessing, acquiring, and moving data. 
7 
00:00:39,210 --> 00:00:45,640 
It includes identification of and
authenticated access to all related data. 
8 
00:00:45,640 --> 00:00:50,000 
And transportation of data from
sources to distributed files systems. 
9 
00:00:51,720 --> 00:00:57,340 
It includes way to subset and match
the data to regions or times of interest. 
10 
00:00:57,340 --> 00:01:00,830 
As we sometimes refer to
it as geo-spacial query. 
11 
00:01:02,150 --> 00:01:08,010 
The next activity is prepare data,
we divide the pre-data activity. 
12 
00:01:08,010 --> 00:01:11,574 
Into two steps based on
the nature of the activity. 
13 
00:01:11,574 --> 00:01:16,990 
Namely, explore data and pre-process data. 
14 
00:01:16,990 --> 00:01:21,890 
The first step in data preparation
involves literally looking 
15 
00:01:21,890 --> 00:01:26,750 
at the data to understand its nature,
what it means, its quality and format. 
16 
00:01:28,130 --> 00:01:32,320 
It often takes a preliminary
analysis of data, or 
17 
00:01:32,320 --> 00:01:34,230 
samples of data, to understand it. 
18 
00:01:35,440 --> 00:01:38,230 
This is why this step is called explore. 
19 
00:01:39,350 --> 00:01:43,100 
Once we know more about the data
through exploratory analysis, 
20 
00:01:43,100 --> 00:01:46,625 
the next step is pre-processing
of data for analysis. 
21 
00:01:46,625 --> 00:01:52,180 
Pre-processing includes cleaning data,
sub-setting or 
22 
00:01:52,180 --> 00:01:57,660 
filtering data, creating data,
which programs can read and 
23 
00:01:57,660 --> 00:02:03,660 
understand, such as modeling raw
data into a more defined data model, 
24 
00:02:03,660 --> 00:02:07,260 
or packaging it using
a specific data format. 
25 
00:02:08,810 --> 00:02:11,630 
If there are multiple data sets involved, 
26 
00:02:11,630 --> 00:02:17,300 
this step also includes integration
of multiple data sources, or streams. 
27 
00:02:17,300 --> 00:02:22,070 
The prepared data then would be
passed onto the analysis step, 
28 
00:02:22,070 --> 00:02:24,590 
which involves selection of
analytical techniques to use, 
29 
00:02:25,800 --> 00:02:29,190 
building a model of the data,
and analyzing results. 
30 
00:02:30,330 --> 00:02:34,120 
This step can take a couple
of iterations on its own or 
31 
00:02:34,120 --> 00:02:37,180 
might require data scientists
to go back to steps one and 
32 
00:02:37,180 --> 00:02:42,350 
two to get more data or
package data in a different way. 
33 
00:02:42,350 --> 00:02:48,070 
Step four for communicating results
includes evaluation of analytical results. 
34 
00:02:48,070 --> 00:02:52,400 
Presenting them in a visual way,
creating reports that include 
35 
00:02:52,400 --> 00:02:56,080 
an assessment of results with
respect to success criteria. 
36 
00:02:57,100 --> 00:03:01,690 
Activities in this step can often be
referred to with terms like interpret, 
37 
00:03:01,690 --> 00:03:04,780 
summarize, visualize, or post process. 
38 
00:03:05,900 --> 00:03:10,390 
The last step brings us back to the very
first reason we do data science, 
39 
00:03:11,680 --> 00:03:12,240 
the purpose. 
40 
00:03:13,340 --> 00:03:18,470 
Reporting insights from analysis and
determining actions from insights based 
41 
00:03:18,470 --> 00:03:23,300 
on the purpose you initially defined
is what we refer to as the act step. 
42 
00:03:24,700 --> 00:03:29,010 
We have now seen all the steps in
a typical data science process. 
43 
00:03:30,220 --> 00:03:35,320 
Please note that this is an iterative
process and findings from one step 
44 
00:03:35,320 --> 00:03:38,970 
may require the previous step to
be repeated with new information. 
1 
00:00:02,391 --> 00:00:06,640 
The Sixth V, Value. 
2 
00:00:06,640 --> 00:00:07,970 
In this module, 
3 
00:00:07,970 --> 00:00:13,160 
we described the five ways which are
considered to be dimensions of big data. 
4 
00:00:14,190 --> 00:00:19,947 
Each way presented a challenging
dimension of big data namely, 
5 
00:00:19,947 --> 00:00:25,500 
size, complexity, speed,
quality, and connectedness. 
6 
00:00:27,480 --> 00:00:32,850 
Although we can list some other rays base
on the context, we prefer to list these 
7 
00:00:32,850 --> 00:00:38,490 
five s fundamental dimensions that this
big data specialization helps you work on. 
8 
00:00:39,570 --> 00:00:43,940 
However, at the heart of
the big data challenge 
9 
00:00:43,940 --> 00:00:48,860 
is turning all of the other dimensions
into truly useful business value. 
10 
00:00:50,110 --> 00:00:54,640 
The idea behind processing all
this big data in the first place 
11 
00:00:54,640 --> 00:00:56,640 
is to bring value to the problem at hand. 
12 
00:00:57,830 --> 00:01:02,230 
In week two we will explore how
to take the first steps into 
13 
00:01:02,230 --> 00:01:05,270 
starting to generate
value out of big data. 
14 
00:01:06,430 --> 00:01:12,465 
Now that we saw all the ways, let's focus
on an example of a big data challenge. 
15 
00:01:12,465 --> 00:01:16,940 
Let's imagine now that you're part
of a company called Eglence Inc. 
16 
00:01:18,160 --> 00:01:22,992 
One of the products of Eglence Inc
is a highly popular mobile game 
17 
00:01:22,992 --> 00:01:25,506 
called Catch the Pink Flamingo. 
18 
00:01:25,506 --> 00:01:30,046 
It's a multi-user game where
the users have to catch special types 
19 
00:01:30,046 --> 00:01:34,188 
of pink flamingos that randomly
pop up on the world map on their 
20 
00:01:34,188 --> 00:01:38,670 
screens based on the mission
that gets updated randomly. 
21 
00:01:38,670 --> 00:01:43,120 
The game is played by millions of
people online throughout the world. 
22 
00:01:43,120 --> 00:01:47,110 
One of the goals of the game is to form
a network of players to collectively 
23 
00:01:47,110 --> 00:01:52,470 
cover the world map with pink flamingo
sightings and compete other groups. 
24 
00:01:52,470 --> 00:01:55,670 
Users can pick their groups
based on player stats. 
25 
00:01:56,780 --> 00:02:01,530 
The game's website sends free
cool stuff to registered users. 
26 
00:02:01,530 --> 00:02:07,483 
Registration requires users to enter
demographic information such gender, 
27 
00:02:07,483 --> 00:02:12,542 
year of birth, city, highest education,
and things like that. 
28 
00:02:12,542 --> 00:02:17,675 
However, most of the users enter
inaccurate information about themselves, 
29 
00:02:17,675 --> 00:02:20,140 
just like most of us do. 
30 
00:02:20,140 --> 00:02:24,980 
To help improve the game,
the game collects realtime usage activity 
31 
00:02:24,980 --> 00:02:29,740 
data from each player and
feeds them to it's data servers. 
32 
00:02:29,740 --> 00:02:35,290 
The players of this game are
enthusiastically active on social media, 
33 
00:02:35,290 --> 00:02:38,480 
and have strong
associations with the game. 
34 
00:02:38,480 --> 00:02:43,424 
A popular Twitter hashtag for
this game is, CatchThePinkFlamingo, 
35 
00:02:43,424 --> 00:02:48,780 
which gets more than 200,000
mentions worldwide per day. 
36 
00:02:48,780 --> 00:02:53,695 
There are strong communities of
users who meet via social media and 
37 
00:02:53,695 --> 00:02:55,984 
get together to play the game. 
38 
00:02:55,984 --> 00:03:02,960 
Now, imagine yourself as the big data
solutions architect for Fun Games Inc. 
39 
00:03:02,960 --> 00:03:07,640 
There are definitely examples of all three
types of data sources in this example. 
40 
00:03:08,920 --> 00:03:13,510 
The mobile app generates data for
the analysis of user activity. 
41 
00:03:13,510 --> 00:03:18,407 
Twitter conversations of players form
a rich source of unstructured data from 
42 
00:03:18,407 --> 00:03:19,005 
people. 
43 
00:03:19,005 --> 00:03:20,191 
And the customer and 
44 
00:03:20,191 --> 00:03:24,250 
game records are examples of data
that this organization collects. 
45 
00:03:26,160 --> 00:03:29,010 
This is a challenging big data example 
46 
00:03:29,010 --> 00:03:32,680 
where all characteristics of
big data are represented. 
47 
00:03:32,680 --> 00:03:35,240 
There are high volumes of player, game and 
48 
00:03:35,240 --> 00:03:39,560 
Twitter data,
which also speaks to the variety of data. 
49 
00:03:39,560 --> 00:03:43,465 
The data streams from the mobile app,
website, and 
50 
00:03:43,465 --> 00:03:48,979 
social media in real-time, which can
be defined as high velocity data. 
51 
00:03:48,979 --> 00:03:53,203 
The quality of demographic data
users enter is not clear, and 
52 
00:03:53,203 --> 00:03:58,334 
there are networks of players which
are related to the balance of big data. 
1 
00:00:02,240 --> 00:00:05,918 
Cloud Computing: An Important
Big Data Enabler. 
2 
00:00:20,598 --> 00:00:25,586 
In our first lecture in this course,
we mentioned the cloud as one of 
3 
00:00:25,586 --> 00:00:29,450 
the two influences of
the launch of the big data era. 
4 
00:00:30,930 --> 00:00:34,050 
We called it on-demand computing, and 
5 
00:00:34,050 --> 00:00:38,390 
we said that it enables us to
compute any time any anywhere. 
6 
00:00:38,390 --> 00:00:40,110 
Simply, whenever we demand it. 
7 
00:00:41,170 --> 00:00:42,540 
In this video, 
8 
00:00:42,540 --> 00:00:47,260 
we will see how we deploy the cloud to
our benefit in our big data applications. 
9 
00:00:48,470 --> 00:00:50,990 
The main idea behind cloud computing 
10 
00:00:50,990 --> 00:00:55,340 
is to transform computing
infrastructure into a commodity. 
11 
00:00:55,340 --> 00:01:00,220 
So application developers can focus on
solving application-specific challenges 
12 
00:01:00,220 --> 00:01:03,880 
instead of trying to build
infrastructure to run on. 
13 
00:01:03,880 --> 00:01:05,760 
So how does this happen? 
14 
00:01:05,760 --> 00:01:08,840 
We can simply define
a cloud computing service, 
15 
00:01:08,840 --> 00:01:11,390 
as a rental service for computing. 
16 
00:01:11,390 --> 00:01:14,700 
You rent what you want,
and return upon usage. 
17 
00:01:15,710 --> 00:01:18,650 
Think about this, you wouldn't buy, or 
18 
00:01:18,650 --> 00:01:23,950 
even build, a truck every time you
have to move a piece of furniture. 
19 
00:01:23,950 --> 00:01:24,880 
You would simply rent. 
20 
00:01:26,560 --> 00:01:29,750 
Why build a computing
cluster when you can rent? 
21 
00:01:29,750 --> 00:01:32,930 
Especially if you are not
using it all the time. 
22 
00:01:35,110 --> 00:01:39,900 
Similarly, you can rent a car or
a bike when you are on vacation. 
23 
00:01:39,900 --> 00:01:43,035 
So you can bike anytime, anywhere. 
24 
00:01:44,660 --> 00:01:46,970 
Let's dig into this question. 
25 
00:01:46,970 --> 00:01:51,240 
What factors do you consider when you're
developing a solution for your yourself or 
26 
00:01:51,240 --> 00:01:51,800 
your client? 
27 
00:01:52,860 --> 00:01:55,880 
Should you build a hardware and
software resources yourself? 
28 
00:01:56,930 --> 00:02:01,090 
Or should you rent these
resources from the cloud? 
29 
00:02:02,860 --> 00:02:06,390 
Let's look at in-house hardware and
software resource building first. 
30 
00:02:08,450 --> 00:02:12,975 
If you choose to develop in-house
capabilities, you have to hire people and 
31 
00:02:12,975 --> 00:02:15,630 
buy hardware that suits your requirements. 
32 
00:02:16,660 --> 00:02:22,750 
These includes, but not limited to,
buying networking hardware, 
33 
00:02:22,750 --> 00:02:29,040 
storage disks, upgrading hardware
when it becomes obsolete, and so on. 
34 
00:02:29,040 --> 00:02:32,360 
Not to forget, the real estate
cost of keeping the hardware. 
35 
00:02:33,370 --> 00:02:36,190 
How do you estimate the size
of your hardware needs? 
36 
00:02:36,190 --> 00:02:39,910 
Do you make a five year estimate,
or ten year? 
37 
00:02:41,920 --> 00:02:44,110 
In today's fast changing world, 
38 
00:02:44,110 --> 00:02:46,620 
it is becoming harder to
estimate future demands. 
39 
00:02:48,070 --> 00:02:52,220 
Getting the software that fits
your needs is equally challenging. 
40 
00:02:52,220 --> 00:02:55,700 
Most software installations
require a lot of tweaking and 
41 
00:02:55,700 --> 00:02:58,030 
manual intervention that
require a lot of skills. 
42 
00:02:59,220 --> 00:03:02,025 
You will need your engineers to do this. 
43 
00:03:02,025 --> 00:03:05,300 
Compatibility issues bring
problems that are hard to foresee. 
44 
00:03:06,340 --> 00:03:09,750 
Most software is updated on a daily basis. 
45 
00:03:09,750 --> 00:03:12,210 
You must ensure you're updated. 
46 
00:03:12,210 --> 00:03:16,870 
This insures you avoid security risks and
get the best. 
47 
00:03:18,670 --> 00:03:25,770 
Over all, building your own data center or
computing power house can be expensive. 
48 
00:03:25,770 --> 00:03:32,200 
And it can be time consuming,
maintaining it is a task by itself. 
49 
00:03:32,200 --> 00:03:35,760 
This requires high initial
capital investments and 
50 
00:03:35,760 --> 00:03:39,380 
efficient operation of several
departments in your business, 
51 
00:03:39,380 --> 00:03:42,540 
which you might not have if
you are a startup company. 
52 
00:03:42,540 --> 00:03:46,800 
Most people forget to include
the cost of disposing old hardware. 
53 
00:03:46,800 --> 00:03:48,788 
Now lets see what the cloud can do for us. 
54 
00:03:50,896 --> 00:03:55,660 
Cloud's benefits are similar to what you
would get from a rental car company. 
55 
00:03:57,190 --> 00:04:01,720 
You pay for what you use,
which means a low capital investment. 
56 
00:04:03,160 --> 00:04:07,680 
You don't need to go to the dealership,
do a negotiation, get a bank loan, 
57 
00:04:07,680 --> 00:04:08,420 
get insurance. 
58 
00:04:09,450 --> 00:04:12,680 
That means quick implementation
of your projects. 
59 
00:04:13,930 --> 00:04:17,350 
Just like you don't need to buy
a car if you only need a car for 
60 
00:04:17,350 --> 00:04:22,200 
a limited use, deploying your application
on a server that is geographically 
61 
00:04:22,200 --> 00:04:27,200 
closer to your client can give you
fast service and happy customers. 
62 
00:04:29,060 --> 00:04:34,430 
For startup and small business,
it can be challenging to do so. 
63 
00:04:34,430 --> 00:04:36,620 
Cloud lets you do this with a click. 
64 
00:04:38,030 --> 00:04:42,950 
You can be sitting at a coffeeshop or your
home and starting your Internet business, 
65 
00:04:42,950 --> 00:04:46,700 
without a huge capital investment,
thanks to the cloud. 
66 
00:04:46,700 --> 00:04:51,230 
And you don't need to have a five or
ten year resource estimation plan. 
67 
00:04:51,230 --> 00:04:55,527 
Adapt to your requirements faster if
your business is growing faster than you 
68 
00:04:55,527 --> 00:04:56,130 
thought. 
69 
00:04:57,440 --> 00:05:01,330 
Cloud lets you forget about
the resource management problems and 
70 
00:05:01,330 --> 00:05:06,310 
lets you focus on your business's products
or domain expertise with minimal cost. 
71 
00:05:07,600 --> 00:05:12,500 
Just as you can rent a truck or
a convertible at a rental car company, 
72 
00:05:12,500 --> 00:05:15,520 
you can build your own
custom machine on cloud. 
73 
00:05:15,520 --> 00:05:19,430 
With a custom machine,
we mean a commodity cluster 
74 
00:05:19,430 --> 00:05:22,940 
made out of the right type of
computing nodes for your application. 
75 
00:05:24,020 --> 00:05:27,930 
You pick not only a CPU or a GPU, but 
76 
00:05:27,930 --> 00:05:32,420 
pick from a whole menu of compute,
memory and storage choices. 
77 
00:05:32,420 --> 00:05:34,590 
It's a buffet on the cloud. 
78 
00:05:34,590 --> 00:05:39,900 
Design machines to suit your application
requirements, data size and analytics. 
79 
00:05:41,330 --> 00:05:43,740 
Get what you want, and
pay for what you use. 
80 
00:05:45,130 --> 00:05:47,380 
Compare this with buying and 
81 
00:05:47,380 --> 00:05:51,980 
maintaining all combinations of
hardware that you possibly would use. 
82 
00:05:51,980 --> 00:05:55,860 
That is so costly and
not possible at all times. 
83 
00:05:57,770 --> 00:06:02,710 
Thanks to all these advantages, there
are many cloud server providers today. 
84 
00:06:02,710 --> 00:06:03,990 
And the numbers are growing. 
85 
00:06:05,240 --> 00:06:08,820 
Here we list some of the players
in the cloud computing market. 
86 
00:06:10,620 --> 00:06:12,940 
Take a moment to look at them. 
87 
00:06:12,940 --> 00:06:15,990 
You will probably recognize
some big names and 
88 
00:06:15,990 --> 00:06:18,730 
some others you have not
even heard of before. 
89 
00:06:20,630 --> 00:06:25,670 
As a summary, cloud does the heavy
lifting, so your team can extract 
90 
00:06:25,670 --> 00:06:30,300 
value from data with getting bogged
down in the infrastructure details. 
91 
00:06:31,880 --> 00:06:33,680 
Cloud provides convenient and 
92 
00:06:33,680 --> 00:06:38,485 
viable solutions for scaling your
prototype to a full fledged application. 
93 
00:06:38,485 --> 00:06:43,400 
You can leverage the experts
to handle security, 
94 
00:06:43,400 --> 00:06:46,450 
robustness, and
let them handle the technical issues. 
95 
00:06:47,480 --> 00:06:51,560 
Your team can work on
utilizing your strengths 
96 
00:06:51,560 --> 00:06:54,320 
to solve your domain specific problem. 
1 
00:00:02,690 --> 00:00:05,528 
Cloud Service Models: Exploration
of Choices. 
2 
00:00:20,538 --> 00:00:24,709 
There are many levels of services that
you can get from cloud providers. 
3 
00:00:25,860 --> 00:00:29,880 
Any cloud computing discussion will
involve terms like application 
4 
00:00:29,880 --> 00:00:34,800 
as a service, platform as a service,
and infrastructure as a service. 
5 
00:00:36,450 --> 00:00:40,660 
All of these refer to business
models around using the cloud 
6 
00:00:40,660 --> 00:00:45,460 
with different levels of engagement and
servicing similar to rental agreements. 
7 
00:00:47,860 --> 00:00:55,470 
IaaS, infrastructure as a service, can be
defined as a bare minimum rental service. 
8 
00:00:57,030 --> 00:01:02,630 
This is like renting a truck from
a company that you can assume has hardware 
9 
00:01:02,630 --> 00:01:07,978 
and you do the packing of your furniture,
and drive to your new house. 
10 
00:01:11,118 --> 00:01:15,430 
You as the user of the service install and
maintain an operating system, 
11 
00:01:15,430 --> 00:01:19,400 
and other applications in
the infrastructure as a service model. 
12 
00:01:20,810 --> 00:01:24,180 
The Amazon EC2 cloud is a good example for
this model. 
13 
00:01:26,470 --> 00:01:29,940 
PaaS, platform as a service, 
14 
00:01:29,940 --> 00:01:35,840 
is the model where a user is provided
with an entire computing platform. 
15 
00:01:35,840 --> 00:01:40,130 
This could include the operating system
and programming languages that you need. 
16 
00:01:42,310 --> 00:01:47,440 
It could extend to include the database
of your choice, or even a web server. 
17 
00:01:47,440 --> 00:01:48,920 
You can develop, and 
18 
00:01:48,920 --> 00:01:52,150 
run your own application software,
on top of these layers. 
19 
00:01:54,120 --> 00:01:59,510 
The Google App engine and Microsoft Azure
are two examples of this model. 
20 
00:02:01,460 --> 00:02:07,770 
SaaS, the software as a service model,
is the model 
21 
00:02:07,770 --> 00:02:13,090 
in which the cloud service provider takes
the responsibilities for the hardware and 
22 
00:02:13,090 --> 00:02:18,710 
software environment such as the operating
system and the application software. 
23 
00:02:19,940 --> 00:02:25,720 
This means you can work on using
the application to solve your problem. 
24 
00:02:28,180 --> 00:02:31,848 
Dropbox is a very popular
software as a service platform. 
25 
00:02:34,638 --> 00:02:39,205 
Ultimately, the decision of which service
you want to explore is a function of 
26 
00:02:39,205 --> 00:02:40,460 
several variables. 
27 
00:02:41,470 --> 00:02:45,760 
It depends on the skill level of your
team to handle computing environment, 
28 
00:02:45,760 --> 00:02:48,080 
development and maintenance. 
29 
00:02:48,080 --> 00:02:51,010 
It also depends on how you
might need to use the service. 
30 
00:02:53,170 --> 00:02:55,990 
You need to pick the right
service model that 
31 
00:02:55,990 --> 00:02:58,520 
best fits you in terms of long term goals. 
32 
00:02:59,590 --> 00:03:04,610 
Finally, when you're deploying a cloud
service you also have to understand 
33 
00:03:04,610 --> 00:03:09,480 
all the security risks, since your
data resides on third party service. 
34 
00:03:11,730 --> 00:03:16,026 
Security is a very important
aspect in today's world of growing 
35 
00:03:16,026 --> 00:03:18,207 
digitization of information. 
36 
00:03:18,207 --> 00:03:21,990 
You must make your client's
data safety a top priority, 
37 
00:03:21,990 --> 00:03:25,859 
and hence this should be an important
criteria in your decision. 
38 
00:03:27,790 --> 00:03:31,870 
All the security risks
must be understood and 
39 
00:03:31,870 --> 00:03:35,000 
evaluated as your data resides
on third party servers. 
40 
00:03:37,670 --> 00:03:40,370 
We are seeing other forms
of services being added 
41 
00:03:40,370 --> 00:03:42,080 
to the family of cloud services. 
42 
00:03:43,260 --> 00:03:45,710 
The logic of infrastructure, platform, and 
43 
00:03:45,710 --> 00:03:48,550 
Software as a Service is
getting extended further. 
44 
00:03:50,040 --> 00:03:53,580 
XaaS is an umbrella term that signifies 
45 
00:03:53,580 --> 00:03:57,980 
even finer-grain control over computing
resources that you want to rent. 
46 
00:03:57,980 --> 00:04:03,520 
For example, storage as a service,
communication as a service, 
47 
00:04:03,520 --> 00:04:05,410 
marketing as a service, and so on. 
48 
00:04:06,990 --> 00:04:14,370 
As a summary, infrastructure as a service,
platform as a service, and application 
49 
00:04:14,370 --> 00:04:18,800 
as a service are three main class service
models that are being used with success. 
50 
00:04:19,960 --> 00:04:25,788 
Picking one will depend on the number
of variables which are company's goals. 
51 
00:04:25,788 --> 00:04:28,840 
These three models have
inspired many similar models 
52 
00:04:28,840 --> 00:04:30,880 
to emerge around cloud computing. 
1 
00:00:00,210 --> 00:00:04,331 
In this lecture we'll be learning about
the basic file manipulation commands for 
2 
00:00:04,331 --> 00:00:05,953 
the Hadoop File System or HDFS. 
3 
00:00:05,953 --> 00:00:09,440 
We'll first start by downloading
a text file of words. 
4 
00:00:09,440 --> 00:00:15,040 
We'll use this text file to copy to and
from the local file system in HDFS and 
5 
00:00:15,040 --> 00:00:17,730 
later on we'll use it
to run word count on. 
6 
00:00:18,770 --> 00:00:21,960 
After we download the text file,
we'll open a terminal shell and 
7 
00:00:21,960 --> 00:00:25,220 
copy the text file from
the local file system to HDFS. 
8 
00:00:26,310 --> 00:00:29,870 
Next, we'll copy the file within HDFS and 
9 
00:00:29,870 --> 00:00:34,880 
also see how to copy file from
HDFS to the local file system. 
10 
00:00:34,880 --> 00:00:38,223 
Finally, we'll see how to
delete a file in HDFS. 
11 
00:00:38,223 --> 00:00:39,410 
Let's start. 
12 
00:00:39,410 --> 00:00:42,390 
We're going to download text
file to copy into HDFS. 
13 
00:00:42,390 --> 00:00:46,610 
It doesn't matter what the contents of
the text file is, so we'll download 
14 
00:00:46,610 --> 00:00:50,380 
the complete works of Shakespeare
since it contains interesting text. 
15 
00:00:53,343 --> 00:00:56,840 
First, click on the icon here
to launch a web browser. 
16 
00:01:00,860 --> 00:01:03,780 
Now we'll search Google for
the complete works of Shakespeare. 
17 
00:01:17,633 --> 00:01:19,880 
I'm going to be using
this first link here. 
18 
00:01:19,880 --> 00:01:23,500 
And we'll provide this link
in the reading section. 
19 
00:01:26,500 --> 00:01:30,080 
So this is the complete
works of Shakespeare and 
20 
00:01:30,080 --> 00:01:35,700 
we'll save it to a text file in the local
file system by clicking on the icon here, 
21 
00:01:35,700 --> 00:01:40,790 
the Open Menu and selecting save page. 
22 
00:01:43,690 --> 00:01:47,105 
So we'll call it words.txt. 
23 
00:01:47,105 --> 00:01:52,090 
And the Save in folder, it's going to
save it into the Downloads directory. 
24 
00:01:57,733 --> 00:02:02,800 
Once that completes, we'll open a terminal
window by clicking on the icon here. 
25 
00:02:06,183 --> 00:02:13,480 
So if we go into the downloads
Ddrectory by typing a cd Downloads and 
26 
00:02:13,480 --> 00:02:21,320 
running ls, we can see that words.txt
was successfully downloaded. 
27 
00:02:24,120 --> 00:02:31,100 
Moving on, let's copy words.txt from the
local file system to the HDFS file system. 
28 
00:02:31,100 --> 00:02:38,711 
The command to do this is hadoop
fs- copyFromLocal words.txt. 
29 
00:02:40,850 --> 00:02:43,330 
When I run this,
it'll copy it from the local directory and 
30 
00:02:43,330 --> 00:02:45,595 
local file system to HDFS. 
31 
00:02:47,710 --> 00:02:52,681 
We can see that the file was
copied by running hadoop fs -ls. 
32 
00:02:52,681 --> 00:02:57,630 
You can see that the file
was successfully copied. 
33 
00:03:00,270 --> 00:03:05,673 
Next, we can copy this file
to another file within HDFS. 
34 
00:03:05,673 --> 00:03:14,170 
We can do this by running hadoop
fs -cp words.txt words2.txt. 
35 
00:03:14,170 --> 00:03:20,500 
The first words.txt is the file
that already exists in HDFS. 
36 
00:03:22,160 --> 00:03:24,760 
The second words 2.txt 
37 
00:03:24,760 --> 00:03:27,230 
is the new file that we're going to
create when we run this command. 
38 
00:03:28,340 --> 00:03:33,182 
Let's run it, and again we're can run 
39 
00:03:33,182 --> 00:03:38,033 
hadoopfs-ls to see the files in HDFS. 
40 
00:03:38,033 --> 00:03:44,364 
We can see the original file words.txt,
and the copy that was made, words2.txt. 
41 
00:03:46,723 --> 00:03:52,680 
Let's copy words2.txt from
HDFS to the local filesystem. 
42 
00:03:52,680 --> 00:04:00,909 
We can do this by running hadoop
fs -copyToLocal words2.txt. 
43 
00:04:03,813 --> 00:04:09,210 
After I run this command, I can call ls to
see the contents of the local file system. 
44 
00:04:11,570 --> 00:04:16,455 
So now we have the new file words2.txt
which we've just copied from HDFS. 
45 
00:04:19,640 --> 00:04:22,980 
The last step in this lecture
is to delete a file in HDFS. 
46 
00:04:22,980 --> 00:04:28,881 
We can delete words2.txt
by running hadoop fs, 
47 
00:04:28,881 --> 00:04:32,404 
but is rn words2.txt. 
48 
00:04:35,330 --> 00:04:37,930 
As you can see,
it printed that it deleted the file. 
49 
00:04:37,930 --> 00:04:41,806 
We can also run hadoop fs- ls,
to verify that the file is deleted. 
50 
00:04:45,246 --> 00:04:50,872 
You can see that there's only the original
words.txt, and words2.txt was deleted. 
1 
00:00:00,710 --> 00:00:02,260 
Getting started. 
2 
00:00:02,260 --> 00:00:04,310 
Why do we worry about foundations? 
3 
00:00:14,575 --> 00:00:19,079 
Starting next week you'll start
diving into the details of the Hadoop 
4 
00:00:19,079 --> 00:00:21,650 
framework for big data. 
5 
00:00:21,650 --> 00:00:26,460 
Before you start, taking a little
bit of time to understand some core 
6 
00:00:26,460 --> 00:00:31,580 
concepts will help you to digest the
information on Hadoop better and faster. 
7 
00:00:32,950 --> 00:00:36,670 
Imagine yourself attending
a chemistry lab. 
8 
00:00:36,670 --> 00:00:40,700 
Before you start hearing about
the tubes and mixtures, you really need 
9 
00:00:40,700 --> 00:00:45,650 
to understand the chemistry or theory
of the practical concepts in the lab. 
10 
00:00:45,650 --> 00:00:49,650 
Similarly, learning these
concepts now will help 
11 
00:00:49,650 --> 00:00:53,880 
you with your understanding of the
practical concepts in the Hadoop lectures. 
12 
00:00:55,000 --> 00:01:00,290 
In addition, we want to prepare you to
understand the tools beyond Hadoop. 
13 
00:01:00,290 --> 00:01:06,410 
Any big data system that you find
will be built on these core concepts. 
14 
00:01:06,410 --> 00:01:09,200 
So these foundations will
help you beyond this course. 
15 
00:01:10,280 --> 00:01:11,460 
Now, let's get started. 
1 
00:00:02,022 --> 00:00:03,322 
Getting started. 
2 
00:00:03,322 --> 00:00:05,063 
Why Hadoop? 
3 
00:00:05,063 --> 00:00:07,254 
We have all heard that Hadoop and 
4 
00:00:07,254 --> 00:00:12,150 
related projects in this
ecosystem are great for big data. 
5 
00:00:12,150 --> 00:00:18,332 
This module will answer the four Ws and
an H about why this statement is true. 
6 
00:00:30,473 --> 00:00:33,788 
Before we dive further into
the details of Hadoop, 
7 
00:00:33,788 --> 00:00:38,890 
let's take a moment to analyze the
characteristics of the Hadoop ecosystem. 
8 
00:00:40,560 --> 00:00:42,680 
What's in the ecosystem? 
9 
00:00:42,680 --> 00:00:43,890 
Why is it beneficial? 
10 
00:00:45,150 --> 00:00:46,040 
Where is it used? 
11 
00:00:47,420 --> 00:00:48,130 
Who uses it? 
12 
00:00:49,910 --> 00:00:51,740 
And how do these tools work? 
13 
00:00:54,830 --> 00:00:59,642 
The Hadoop ecosystem frameworks and
applications that we will describe in this 
14 
00:00:59,642 --> 00:01:03,670 
module have several overarching themes and
goals. 
15 
00:01:03,670 --> 00:01:08,930 
First, they provide scalability
to store large volumes of data 
16 
00:01:08,930 --> 00:01:10,180 
on commodity hardware. 
17 
00:01:12,160 --> 00:01:16,820 
As the number of systems increases,
so does the chance for crashes and 
18 
00:01:16,820 --> 00:01:18,510 
hardware failures. 
19 
00:01:18,510 --> 00:01:23,570 
A second goal, supported by most
frameworks in the Hadoop ecosystem, 
20 
00:01:23,570 --> 00:01:26,590 
is the ability to gracefully
recover from these problems. 
21 
00:01:28,730 --> 00:01:34,070 
In addition, as we have mentioned before,
big data comes in 
22 
00:01:34,070 --> 00:01:40,560 
a variety of flavors, such as text files,
graph of social networks, 
23 
00:01:40,560 --> 00:01:46,681 
streaming sensor data and raster images. 
24 
00:01:46,681 --> 00:01:50,250 
A third goal for
the Hadoop ecosystem then, 
25 
00:01:50,250 --> 00:01:55,920 
is the ability to handle these different
data types for any given type of data. 
26 
00:01:57,040 --> 00:02:00,590 
You can find several projects in
the ecosystem that support it. 
27 
00:02:02,350 --> 00:02:05,890 
A fourth goal of the Hadoop ecosystem 
28 
00:02:05,890 --> 00:02:08,998 
is the ability to facilitate
a shared environment. 
29 
00:02:08,998 --> 00:02:15,030 
Since even modest-sized
clusters can have many cores, 
30 
00:02:15,030 --> 00:02:18,640 
it is important to allow multiple
jobs to execute simultaneously. 
31 
00:02:20,070 --> 00:02:23,450 
Why buy servers only to let them sit idle? 
32 
00:02:25,670 --> 00:02:30,750 
Another goal of the Hadoop ecosystem
is providing value for your enterprise. 
33 
00:02:32,910 --> 00:02:37,630 
The ecosystem includes a wide
range of open source projects 
34 
00:02:37,630 --> 00:02:40,270 
backed by a large active community. 
35 
00:02:41,640 --> 00:02:46,500 
These projects are free to use and
easy to find support for. 
36 
00:02:48,640 --> 00:02:50,750 
In the following lectures in this module, 
37 
00:02:50,750 --> 00:02:55,260 
we will take a more detailed
look at the Hadoop ecosystem. 
38 
00:02:55,260 --> 00:02:59,590 
First, we will explore the kinds
of projects available and 
39 
00:02:59,590 --> 00:03:01,760 
the types of capabilities they provide. 
40 
00:03:03,100 --> 00:03:09,300 
Next we will take a deeper look at
the three main parts of Hadoop. 
41 
00:03:09,300 --> 00:03:12,130 
The Hadoop distributed file system,
or HDFS. 
42 
00:03:13,490 --> 00:03:15,910 
YARN, the scheduler and resource manager. 
43 
00:03:17,170 --> 00:03:21,400 
And MapReduce, a programming model for
processing big data. 
44 
00:03:23,210 --> 00:03:28,060 
We will then discuss cloud computing, and
the types of service models it provides. 
45 
00:03:29,980 --> 00:03:34,865 
We will also describe situations in
which Hadoop is not the best solution. 
46 
00:03:36,935 --> 00:03:40,945 
This module then concludes with
two readings involving hands-on 
47 
00:03:40,945 --> 00:03:44,245 
experience with HDFS and MapReduce. 
48 
00:03:44,245 --> 00:03:45,325 
So let's get started. 
1 
00:00:01,873 --> 00:00:05,670 
MapReduce, Simple Programming for
Big Results. 
2 
00:00:21,723 --> 00:00:26,650 
MapReduce is a programming model for
the Hadoop ecosystem. 
3 
00:00:26,650 --> 00:00:29,020 
It relies on YARN to schedule and 
4 
00:00:29,020 --> 00:00:33,430 
execute parallel processing over
the distributed file blocks in HDFS. 
5 
00:00:34,600 --> 00:00:39,292 
There are several tools that use the
MapReduce model to provide a higher level 
6 
00:00:39,292 --> 00:00:41,829 
interface to other programming models. 
7 
00:00:41,829 --> 00:00:46,747 
Hive has a SQL-like interface that
adds capabilities that help with 
8 
00:00:46,747 --> 00:00:49,220 
relational data modeling. 
9 
00:00:49,220 --> 00:00:52,460 
And Pig is a high level data flow language 
10 
00:00:52,460 --> 00:00:55,470 
that adds capabilities that
help with process map modeling. 
11 
00:00:57,070 --> 00:01:02,200 
Traditional parallel programming requires
expertise on a number of computing and 
12 
00:01:02,200 --> 00:01:04,160 
systems concepts. 
13 
00:01:04,160 --> 00:01:09,170 
For example, synchronization
mechanisms like locks, semaphores, 
14 
00:01:09,170 --> 00:01:11,460 
and monitors are essential. 
15 
00:01:11,460 --> 00:01:15,450 
And incorrectly using them can
either crash your program, or 
16 
00:01:15,450 --> 00:01:17,460 
severely impact performance. 
17 
00:01:19,080 --> 00:01:21,600 
This high learning curve
makes it difficult. 
18 
00:01:22,930 --> 00:01:27,465 
It is also error prone,
since your code can run on hundreds, or 
19 
00:01:27,465 --> 00:01:30,813 
thousands of nodes,
each having many cores. 
20 
00:01:30,813 --> 00:01:33,685 
And any problem related to
these parallel processes, 
21 
00:01:33,685 --> 00:01:36,130 
needs to be handled by
your parallel program. 
22 
00:01:37,510 --> 00:01:43,200 
The MapReduce programming model greatly
simplifies running code in parallel 
23 
00:01:43,200 --> 00:01:46,250 
since you don't have to deal
with any of these issues. 
24 
00:01:46,250 --> 00:01:52,450 
Instead, you only need to create and
map and reduce tasks, and you don't 
25 
00:01:52,450 --> 00:01:56,670 
have to worry about multiple threads,
synchronization, or concurrency issues. 
26 
00:01:58,900 --> 00:02:01,110 
So, what is a map and reduce? 
27 
00:02:02,760 --> 00:02:07,522 
Map and reduce are two concepts
based on functional programming 
28 
00:02:07,522 --> 00:02:12,160 
where the output the function
is based solely on the input. 
29 
00:02:14,010 --> 00:02:20,060 
Just like in a mathematical function,
f (x) = y, y depends on x. 
30 
00:02:21,480 --> 00:02:25,360 
You provide a function, or
operation for a map, and reduce. 
31 
00:02:26,890 --> 00:02:30,570 
And the runtime executes it over the data. 
32 
00:02:30,570 --> 00:02:35,890 
For map, the operation is
applied on each data element. 
33 
00:02:35,890 --> 00:02:40,110 
And in reduce, the operation
summarizes elements in some manner. 
34 
00:02:41,510 --> 00:02:47,180 
An example, using map and
reduce will make this concepts more clear. 
35 
00:02:49,230 --> 00:02:52,180 
Hello word is a traditional
first program you code 
36 
00:02:52,180 --> 00:02:54,310 
when you start to learning
programming languages. 
37 
00:02:55,730 --> 00:03:01,661 
The first program to learn, or hello
word of map reduce, is often WordCount. 
38 
00:03:03,735 --> 00:03:08,128 
WordCount reads one or
more text files, and 
39 
00:03:08,128 --> 00:03:14,313 
counts the number of occurrences
of each word in these files. 
40 
00:03:14,313 --> 00:03:17,654 
The output will be a text
file with a list of words and 
41 
00:03:17,654 --> 00:03:20,940 
their occurrence frequencies
in the input data. 
42 
00:03:22,320 --> 00:03:25,560 
Let's examine each step of WordCount. 
43 
00:03:26,820 --> 00:03:31,790 
For simplification we are assuming
we have one big file as an input. 
44 
00:03:33,200 --> 00:03:39,040 
Before WordCount runs,
the input file is stored in HDFS. 
45 
00:03:39,040 --> 00:03:40,880 
As you know now, 
46 
00:03:40,880 --> 00:03:45,200 
HDFS partitions the blocks across
multiple nodes in the cluster. 
47 
00:03:46,470 --> 00:03:53,503 
In this case, four partitions labeled,
A, B, C, and D. 
48 
00:03:53,503 --> 00:03:58,550 
The first step in MapReduce is to
run a map operation on each node. 
49 
00:04:00,100 --> 00:04:05,190 
As the input partitions are read
from HTFS, map is called for 
50 
00:04:05,190 --> 00:04:06,760 
each line in the input. 
51 
00:04:08,020 --> 00:04:12,330 
Let's look at the first lines
of the input partitions, A and 
52 
00:04:12,330 --> 00:04:14,810 
B, and start counting the words. 
53 
00:04:15,920 --> 00:04:21,162 
The first line,
in the partition on node A, 
54 
00:04:21,162 --> 00:04:26,133 
says, My apple is red and my rose is blue. 
55 
00:04:26,133 --> 00:04:32,990 
Similarly, the first line, on partition B,
says, You are the apple of my eye. 
56 
00:04:34,370 --> 00:04:38,170 
Let's now see what happens in
the first map node for partition A. 
57 
00:04:40,138 --> 00:04:42,910 
Map creates a key value for 
58 
00:04:42,910 --> 00:04:49,510 
each word on the line containing
the word as the key, and 1 as the value. 
59 
00:04:49,510 --> 00:04:55,470 
In this example, the word apple is
read from the line in partition A. 
60 
00:04:56,550 --> 00:05:01,447 
Map produces a key value of (apple, 1). 
61 
00:05:01,447 --> 00:05:08,273 
Similarly, the word my is seen
on the first line of A twice. 
62 
00:05:08,273 --> 00:05:14,759 
So, the key values of (my,
1), are created. 
63 
00:05:14,759 --> 00:05:20,040 
Note that map goes to each node
containing a data block for 
64 
00:05:20,040 --> 00:05:25,050 
the file,
instead of the data moving to map. 
65 
00:05:25,050 --> 00:05:27,410 
This is moving computation to data. 
66 
00:05:28,790 --> 00:05:33,863 
Let's now see what the same map
operation generates for partition B. 
67 
00:05:33,863 --> 00:05:37,221 
Since each word only
happens to occur once, 
68 
00:05:37,221 --> 00:05:42,770 
a list of all the words with one
key-value pairing each gets generated. 
69 
00:05:43,950 --> 00:05:48,170 
Please take a moment to
observe the outputs of map and 
70 
00:05:48,170 --> 00:05:52,480 
each key-value pair associated to a word. 
71 
00:06:02,003 --> 00:06:09,570 
Next, all the key-values that were output
from map are sorted based on their key. 
72 
00:06:09,570 --> 00:06:16,810 
And the key values, with the same word,
are moved, or shuffled, to the same node. 
73 
00:06:17,980 --> 00:06:23,850 
To simplify this figure, each node only
has a single word, in orange boxes. 
74 
00:06:24,970 --> 00:06:29,010 
But in general,
a node will have many different words. 
75 
00:06:29,010 --> 00:06:32,890 
Just like our example from the two
lines in A and B partitions. 
76 
00:06:33,960 --> 00:06:39,480 
Here we see that, you and apple,
are assigned to the first node. 
77 
00:06:39,480 --> 00:06:43,543 
The word is, to the second node. 
78 
00:06:43,543 --> 00:06:47,710 
And the words, rose and red, to the third. 
79 
00:06:48,900 --> 00:06:54,960 
Although, for simplicity, we drew four
map nodes and three shuffle nodes. 
80 
00:06:54,960 --> 00:06:58,760 
The number of nodes can be extended
as much as the application demands. 
81 
00:07:01,903 --> 00:07:07,083 
Next, the reduce operation
executes on these nodes to 
82 
00:07:07,083 --> 00:07:12,045 
add values for
key-value pairs with the same keys. 
83 
00:07:12,045 --> 00:07:16,875 
For example, (apple, 1), and 
84 
00:07:16,875 --> 00:07:23,959 
another (apple, 1), becomes (apple, 2). 
85 
00:07:23,959 --> 00:07:28,161 
The result of reduce is
a single key pair for 
86 
00:07:28,161 --> 00:07:32,263 
each word that was read in the input file. 
87 
00:07:32,263 --> 00:07:36,160 
The key is the word, and
the value is the number of occurrences. 
88 
00:07:38,790 --> 00:07:42,650 
If we look back at our WordCount example, 
89 
00:07:42,650 --> 00:07:45,640 
we see that there were
three distinct steps. 
90 
00:07:45,640 --> 00:07:52,818 
Namely, the map step, the shuffle and
sort step, and the reduce step. 
91 
00:07:52,818 --> 00:07:58,840 
Although, the WordCount
example is pretty simple, 
92 
00:07:58,840 --> 00:08:03,320 
it represents a large number of
applications to which these three steps 
93 
00:08:03,320 --> 00:08:07,000 
can be applied in order to achieve
data parallel scalability. 
94 
00:08:08,540 --> 00:08:14,213 
For example, now that you have
seen the WordCount application, 
95 
00:08:14,213 --> 00:08:19,063 
consider changing the WordCount
algorithm to index all 
96 
00:08:19,063 --> 00:08:22,170 
the URLs by words after a web crawl. 
97 
00:08:23,320 --> 00:08:29,225 
This means, instead of pointing to
a number, the keys would refer to URLs. 
98 
00:08:31,100 --> 00:08:36,350 
After the map, with this new function,
which by the way is called a user 
99 
00:08:36,350 --> 00:08:41,760 
defined function, the output of
shuffle and sort would look like this. 
100 
00:08:46,333 --> 00:08:53,450 
Now, when we reduce the URLs, all the URLs
that mention Apple would look like this. 
101 
00:08:55,943 --> 00:09:01,010 
This is, in fact, one of the ways
a search engine like Google works. 
102 
00:09:02,900 --> 00:09:08,150 
So now, if somebody came to the interface
built for this application, 
103 
00:09:08,150 --> 00:09:12,078 
to search for the word apple,
and entered apple, 
104 
00:09:12,078 --> 00:09:17,490 
it would be easy to get all
the URLs as the word itself. 
105 
00:09:18,840 --> 00:09:22,943 
No wonder the first MapReduce
paper was produced by Google. 
106 
00:09:22,943 --> 00:09:27,760 
We will give you a link to the original
Google paper on MapReduce from 
107 
00:09:27,760 --> 00:09:30,390 
2004 at the end of this lecture. 
108 
00:09:32,030 --> 00:09:34,080 
It is pretty technical, but 
109 
00:09:34,080 --> 00:09:38,040 
it gives you a simple overview without
the current system implementations. 
110 
00:09:39,720 --> 00:09:43,440 
We just saw how MapReduce can
be used in search engines 
111 
00:09:43,440 --> 00:09:45,800 
in addition to counting the words and
documents. 
112 
00:09:47,180 --> 00:09:51,900 
Although it's possible to add many
more applications, let's stop here for 
113 
00:09:51,900 --> 00:09:55,150 
a general discussion on how the points of 
114 
00:09:55,150 --> 00:09:59,670 
data parallelism can be used in
search in this three step pattern. 
115 
00:10:01,480 --> 00:10:05,780 
There is definitely parallelization
during the map step. 
116 
00:10:06,810 --> 00:10:09,953 
This parallelization is over the input, 
117 
00:10:09,953 --> 00:10:13,918 
as each partition gets
processed one line at a time. 
118 
00:10:13,918 --> 00:10:18,440 
To achieve this type of data
parallelism we must decide on 
119 
00:10:18,440 --> 00:10:23,430 
the data granularity of
each parallel competition. 
120 
00:10:23,430 --> 00:10:25,540 
In this case, it will be a line. 
121 
00:10:26,860 --> 00:10:33,410 
We also see parallel grouping of
data in the shuffle and sort phase. 
122 
00:10:33,410 --> 00:10:37,710 
This time, the parallelization is
over the intermediate products. 
123 
00:10:38,740 --> 00:10:41,570 
That is, the individual key-value pairs. 
124 
00:10:42,940 --> 00:10:46,400 
And after the grouping of
the intermediate products, 
125 
00:10:46,400 --> 00:10:51,630 
the reduce step gets parallelized
to construct one output file. 
126 
00:10:53,090 --> 00:10:56,340 
You have probably noticed
that the data gets reduced 
127 
00:10:56,340 --> 00:10:58,140 
to a smaller set at each step. 
128 
00:10:59,840 --> 00:11:06,208 
This overview gave us an idea of what
kinds of tasks that MapReduce is good for. 
129 
00:11:06,208 --> 00:11:11,604 
While MapReduce excels at independent
batch tasks similar to our applications, 
130 
00:11:11,604 --> 00:11:16,700 
there are certain kinds of tasks that
you would not want to use MapReduce for. 
131 
00:11:17,965 --> 00:11:22,300 
For example,
if your data is frequently changing, 
132 
00:11:22,300 --> 00:11:27,120 
MapReduce is slow since it reads
the entire input data set each time. 
133 
00:11:28,350 --> 00:11:31,540 
The MapReduce model requires that maps and 
134 
00:11:31,540 --> 00:11:35,320 
reduces execute
independently of each other. 
135 
00:11:35,320 --> 00:11:37,920 
This greatly simplifies
your job as a designer, 
136 
00:11:37,920 --> 00:11:41,910 
since you do not have to deal
with synchronization issues. 
137 
00:11:41,910 --> 00:11:46,060 
However, it means that computations
that do have dependencies, 
138 
00:11:46,060 --> 00:11:47,880 
cannot be expressed with MapReduce. 
139 
00:11:49,600 --> 00:11:53,760 
Finally, MapReduce does
not return any results 
140 
00:11:53,760 --> 00:11:56,490 
until the entire process is finished. 
141 
00:11:56,490 --> 00:11:59,060 
It must read the entire input data set. 
142 
00:12:00,130 --> 00:12:04,240 
This makes it unsuitable for interactive
applications where the results must be 
143 
00:12:04,240 --> 00:12:09,200 
presented to the user very quickly,
expecting a return from the user. 
144 
00:12:10,880 --> 00:12:15,830 
As a summary, MapReduce hides
complexities of parallel programming and 
145 
00:12:15,830 --> 00:12:18,450 
greatly simplifies building
parallel applications. 
146 
00:12:19,530 --> 00:12:21,996 
Many types of tasks suitable for 
147 
00:12:21,996 --> 00:12:27,130 
MapReduce include search engine
page ranking and topic mapping. 
148 
00:12:27,130 --> 00:12:30,929 
Please see the reading after
this lecture on making Pasta Sauce 
149 
00:12:32,382 --> 00:12:38,320 
with MapReduce for another fun application
using the MapReduce programming model. 
1 
00:00:01,058 --> 00:00:03,042 
Programming Models for Big Data. 
2 
00:00:22,264 --> 00:00:26,664 
We have seen that scalable computing
over the internet to achieve 
3 
00:00:26,664 --> 00:00:32,375 
data-parallel scalability for big data
applications is now a possibility. 
4 
00:00:32,375 --> 00:00:34,625 
Thanks to commodity clusters. 
5 
00:00:34,625 --> 00:00:38,970 
Cost-effective commodity clusters
together with advances in distributed 
6 
00:00:38,970 --> 00:00:41,670 
file systems to move computation to data, 
7 
00:00:41,670 --> 00:00:46,280 
provide a potential to conduct
scalable big data analytics. 
8 
00:00:46,280 --> 00:00:49,050 
The next thing we will
talk about is how to take 
9 
00:00:49,050 --> 00:00:51,050 
advantage of these
infrastructure advances. 
10 
00:00:52,100 --> 00:00:53,560 
What are the right programming models? 
11 
00:00:54,680 --> 00:01:00,460 
A programming model is an abstraction or
existing machinery or infrastructure. 
12 
00:01:00,460 --> 00:01:03,970 
It is a set of abstract
runtime libraries and 
13 
00:01:03,970 --> 00:01:08,310 
programming languages that
form a model of computation. 
14 
00:01:08,310 --> 00:01:14,110 
This abstraction level can be low-level
as in machine language in computers. 
15 
00:01:14,110 --> 00:01:20,030 
Or very high as in high-level programming
languages, for example, Java. 
16 
00:01:20,030 --> 00:01:23,470 
So we can say,
if the enabling infrastructure for 
17 
00:01:23,470 --> 00:01:28,320 
big data analysis is distributed
file systems as we mentioned, 
18 
00:01:28,320 --> 00:01:33,120 
then the programming model for
big data should enable the programmability 
19 
00:01:33,120 --> 00:01:37,100 
of the operations within
distributed file systems. 
20 
00:01:37,100 --> 00:01:41,840 
What we mean by this being able to
write computer programs that work 
21 
00:01:41,840 --> 00:01:46,410 
efficiently on top of distributed
file systems using big data and 
22 
00:01:46,410 --> 00:01:50,430 
making it easy to cope with
all the potential issues. 
23 
00:01:50,430 --> 00:01:52,320 
Based on everything we discussed so 
24 
00:01:52,320 --> 00:01:57,680 
far, let's describe the requirements for
big data programming models. 
25 
00:01:57,680 --> 00:02:02,480 
First of all, such a programming model for
big data should support 
26 
00:02:02,480 --> 00:02:07,280 
common big data operations like
splitting large volumes of data. 
27 
00:02:08,420 --> 00:02:12,460 
This means for partitioning and
placement of data in and 
28 
00:02:12,460 --> 00:02:18,220 
out of computer memory along with a model
to synchronize the datasets later on. 
29 
00:02:18,220 --> 00:02:21,463 
The access to data should
be achieved in a fast way. 
30 
00:02:21,463 --> 00:02:26,291 
It should allow fast distribution to nodes
within a rack and these are potentially, 
31 
00:02:26,291 --> 00:02:28,815 
the data nodes we moved
the computation to. 
32 
00:02:28,815 --> 00:02:34,020 
This means scheduling of
many parallel tasks at once. 
33 
00:02:34,020 --> 00:02:37,950 
It should also enable
reliability of the computing and 
34 
00:02:37,950 --> 00:02:40,100 
full tolerance from failures. 
35 
00:02:40,100 --> 00:02:43,768 
This means it should enable
programmable replications and 
36 
00:02:43,768 --> 00:02:45,758 
recovery of files when needed. 
37 
00:02:45,758 --> 00:02:50,518 
It should be easily scalable to
the distributed notes where the data gets 
38 
00:02:50,518 --> 00:02:51,305 
produced. 
39 
00:02:51,305 --> 00:02:56,647 
It should also enable adding new resources
to take advantage of distributive 
40 
00:02:56,647 --> 00:03:01,670 
computers and scale to more or
faster data without losing performance. 
41 
00:03:01,670 --> 00:03:05,280 
This is called scaling out if needed. 
42 
00:03:05,280 --> 00:03:08,220 
Since there are a variety
of different types of data, 
43 
00:03:08,220 --> 00:03:13,140 
such as documents, graphs,
tables, key values, etc. 
44 
00:03:13,140 --> 00:03:17,100 
A programming model should enable
operations over a particular set 
45 
00:03:17,100 --> 00:03:18,680 
of these types. 
46 
00:03:18,680 --> 00:03:23,261 
Not every type of data may be
supported by a particular model, but 
47 
00:03:23,261 --> 00:03:27,020 
the models should be optimized for
at least one type. 
48 
00:03:27,020 --> 00:03:29,540 
Is it getting a little complicated? 
49 
00:03:29,540 --> 00:03:31,153 
It doesn't have to have to be. 
50 
00:03:31,153 --> 00:03:36,696 
In fact, we apply similar models in
our daily lives for everyday tasks. 
51 
00:03:36,696 --> 00:03:41,520 
Let's look at the scenario where you
might unknowingly apply this model. 
52 
00:03:41,520 --> 00:03:45,020 
Imagine a peaceful Saturday afternoon. 
53 
00:03:45,020 --> 00:03:47,840 
You receive a phone call from a friend and
she says, 
54 
00:03:47,840 --> 00:03:50,050 
she they will be at your
house in an hour for dinner. 
55 
00:03:51,290 --> 00:03:56,157 
It seems like you completely forgot that
you had invited your friends for dinner. 
56 
00:03:56,157 --> 00:03:59,309 
So you say, you are looking forward
to it and head to the kitchen. 
57 
00:03:59,309 --> 00:04:05,035 
As a quick solution, you decide to
cook pasta with some tomato sauce. 
58 
00:04:05,035 --> 00:04:07,487 
You need to take advantage
of parallelization, so 
59 
00:04:07,487 --> 00:04:11,315 
that the dinner is ready by the time your
guest arrive, that's within an hour. 
60 
00:04:11,315 --> 00:04:15,593 
You call your spouse and your teenage
kids to action in the kitchen. 
61 
00:04:15,593 --> 00:04:20,464 
Now, you need to give them directions to
start dicing the ingredients for you. 
62 
00:04:20,464 --> 00:04:27,010 
But in the heat of the moment, you end up
mixing the onions, tomatoes and peppers. 
63 
00:04:27,010 --> 00:04:28,890 
Instead of sorting them first, 
64 
00:04:28,890 --> 00:04:33,380 
you give everyone a randomly mixed
batch of different types of vegetables. 
65 
00:04:33,380 --> 00:04:38,360 
They are required to use their computer
powers to chop the vegetables. 
66 
00:04:38,360 --> 00:04:42,210 
They need to ensure not mix
different types of veggies. 
67 
00:04:42,210 --> 00:04:47,562 
When everyone is done chopping, you want
to group the veggies by their types. 
68 
00:04:47,562 --> 00:04:52,877 
You ask each helper to collect items
of the same type, put them in a large 
69 
00:04:52,877 --> 00:04:58,016 
bowl and label this large bowl with
the sum of individual bowl weights 
70 
00:04:58,016 --> 00:05:03,975 
like tomatoes in one bowl, peppers in
another and the onions in the third bowl. 
71 
00:05:03,975 --> 00:05:04,781 
In the end, 
72 
00:05:04,781 --> 00:05:10,209 
you have nice large bowls with the total
weight of each vegetable labeled on it. 
73 
00:05:10,209 --> 00:05:15,378 
Your helpers are soon done with their work
while you're focused on coordinating their 
74 
00:05:15,378 --> 00:05:20,284 
actions and other dinner tasks in the
kitchen, you can start cooking your pasta. 
75 
00:05:20,284 --> 00:05:25,230 
What you have just seen is an excellent
example of big data modeling in action. 
76 
00:05:25,230 --> 00:05:30,439 
Only it is really the data
processed by human processors. 
77 
00:05:30,439 --> 00:05:34,600 
This scenario can be modeled by a common
programming model for big data. 
78 
00:05:34,600 --> 00:05:36,609 
Namely MapReduce. 
79 
00:05:36,609 --> 00:05:40,625 
MapReduce is a big data programming
model that supports all 
80 
00:05:40,625 --> 00:05:44,249 
the requirements of big
data modeling we mentioned. 
81 
00:05:44,249 --> 00:05:47,093 
It can model processing large data, 
82 
00:05:47,093 --> 00:05:51,406 
split complications into
different parallel tasks and 
83 
00:05:51,406 --> 00:05:57,562 
make efficient use of large commodity
clusters and distributed file systems. 
84 
00:05:57,562 --> 00:06:01,542 
In addition, it abstracts out
the details of parallelzation, 
85 
00:06:01,542 --> 00:06:06,050 
full tolerance, data distribution,
monitoring and load balancing. 
86 
00:06:07,140 --> 00:06:08,700 
As a programming model, 
87 
00:06:08,700 --> 00:06:12,590 
it has been implemented in a few
different big data frameworks. 
88 
00:06:12,590 --> 00:06:16,123 
Next week,
we will see more details on MapReduce and 
89 
00:06:16,123 --> 00:06:18,788 
how its Hadoop implementation works. 
90 
00:06:18,788 --> 00:06:21,362 
To summarize, programming models for 
91 
00:06:21,362 --> 00:06:25,267 
big data are abstractions over
distributed file systems. 
92 
00:06:25,267 --> 00:06:30,022 
The desired programming models for
big data should handle large volumes and 
93 
00:06:30,022 --> 00:06:31,289 
varieties of data. 
94 
00:06:31,289 --> 00:06:35,492 
Support full tolerance and
provide scale out functionality. 
95 
00:06:35,492 --> 00:06:37,535 
MapReduce is one of these models, 
96 
00:06:37,535 --> 00:06:41,063 
implemented in a variety of
frameworks including Hadoop. 
97 
00:06:41,063 --> 00:06:45,360 
We will summarize the inner workings
of the Hadoop implementation next week. 
1 
00:00:00,230 --> 00:00:03,230 
In this lecture we will use
Hadoop to run WordCount. 
2 
00:00:03,230 --> 00:00:05,487 
First we will open a terminal shell and 
3 
00:00:05,487 --> 00:00:08,680 
explore the Hadoop-provided MapReduce
programs. 
4 
00:00:08,680 --> 00:00:12,657 
Next we will verify the input
file exists in HDFS. 
5 
00:00:12,657 --> 00:00:17,130 
We will then run WordCount and
explore the WordCount output directory. 
6 
00:00:17,130 --> 00:00:19,560 
After that we will copy
the WordCount results 
7 
00:00:19,560 --> 00:00:22,556 
from HDFS to the local file system and
view them. 
8 
00:00:22,556 --> 00:00:24,493 
Let's begin. 
9 
00:00:24,493 --> 00:00:27,930 
First we'll open a terminal shell by 
10 
00:00:27,930 --> 00:00:30,320 
clicking on the icon at
top of the window here. 
11 
00:00:33,430 --> 00:00:36,811 
Next, we'll look at the map produced
programs that come with Hadoop. 
12 
00:00:36,811 --> 00:00:42,961 
We can do this by running Hadoop,
jars user jars, Hadoop examples .jar. 
13 
00:00:46,666 --> 00:00:51,616 
This command says we're going to use
the jar command to run a program 
14 
00:00:51,616 --> 00:00:54,030 
in Hadoop from a jar file. 
15 
00:00:54,030 --> 00:00:59,730 
And the jar file that we're running from
is in /usr/jars/hadoop-examples.jar. 
16 
00:00:59,730 --> 00:01:03,510 
Many programs written in Java
are distributed via jar files. 
17 
00:01:03,510 --> 00:01:10,520 
If we run this command We'll see a list of
different programs that come with Hadoop. 
18 
00:01:10,520 --> 00:01:13,090 
So for example, wordcount. 
19 
00:01:13,090 --> 00:01:14,780 
Count the words in a text file. 
20 
00:01:15,990 --> 00:01:19,350 
Wordmean, count the average
length of words. 
21 
00:01:19,350 --> 00:01:25,220 
And other programs, such as sorting and
calculating the length of pi. 
22 
00:01:27,748 --> 00:01:31,558 
In the previous lecture we downloaded
the Works of Shakespeare and 
23 
00:01:31,558 --> 00:01:32,721 
saved it into HDFS. 
24 
00:01:32,721 --> 00:01:37,822 
Let's make sure that file is still
there by running hadoop fs -ls. 
25 
00:01:40,387 --> 00:01:44,734 
We can see that the file is still there,
and it's called words.txt. 
26 
00:01:46,900 --> 00:01:51,390 
We can run wordcount by running hadoop jar 
27 
00:01:51,390 --> 00:01:57,180 
/usr/jars/hadoop-examples.jar wordcount. 
28 
00:01:57,180 --> 00:01:59,090 
This command says that
we're going to run a jar, 
29 
00:01:59,090 --> 00:02:02,690 
and this is the name of the jar
containing the program. 
30 
00:02:02,690 --> 00:02:05,690 
And the program we're
going to run is wordcount. 
31 
00:02:05,690 --> 00:02:08,450 
When we run it, we see that it
prints the command line usage for 
32 
00:02:08,450 --> 00:02:10,050 
how to run wordcount. 
33 
00:02:10,050 --> 00:02:17,310 
This says that wordcount takes one or
more input files and an output name. 
34 
00:02:17,310 --> 00:02:21,837 
Now, both the input and
the output are located in HDFS. 
35 
00:02:21,837 --> 00:02:27,010 
So we have the input file that we
just listed, words.txt, in HDFS. 
36 
00:02:27,010 --> 00:02:27,940 
We can run wordcount. 
37 
00:02:29,400 --> 00:02:36,148 
So we'll run hadoop
jar/usr/jars/hadoop-examples.jar 
38 
00:02:36,148 --> 00:02:39,351 
wordcount words.txt out. 
39 
00:02:40,390 --> 00:02:45,250 
This is saying we're going to run
the WordCount program using words.txt 
40 
00:02:45,250 --> 00:02:49,170 
as the input and
put the output in a directory called out. 
41 
00:02:50,470 --> 00:02:51,251 
So we'll run it. 
42 
00:02:58,381 --> 00:03:01,190 
As wordcount is running,
your prints progress to the screen. 
43 
00:03:03,247 --> 00:03:06,760 
It'll print the percentage of map and
reduce completed. 
44 
00:03:06,760 --> 00:03:10,124 
And when both of these reach 100%,
then the job is done. 
45 
00:03:14,664 --> 00:03:17,440 
Now that the job is complete,
let's look at the results. 
46 
00:03:19,900 --> 00:03:23,220 
We can run Hadoop fs
-ls to see the output. 
47 
00:03:26,220 --> 00:03:30,930 
This shows that out was created and
this is where our results are stored. 
48 
00:03:30,930 --> 00:03:34,580 
Notice that it's
a directory with a d here. 
49 
00:03:34,580 --> 00:03:39,870 
So hadoop word count created
the directory to contain the output. 
50 
00:03:39,870 --> 00:03:44,412 
Let's look inside that directory
by running Hadoop fs- ls out. 
51 
00:03:44,412 --> 00:03:49,580 
[BLANK AUDIO] We can see that there
are two files in this directory. 
52 
00:03:49,580 --> 00:03:55,280 
The first is _SUCCESS, this means that
the WordCount job completed successfully. 
53 
00:03:55,280 --> 00:03:59,279 
The other file part-r-00000 is a text file 
54 
00:03:59,279 --> 00:04:03,284 
containing the output from
the WordCount command 
55 
00:04:05,198 --> 00:04:11,260 
Now let's copy this text file to the local
file system from HDFS and then view it. 
56 
00:04:11,260 --> 00:04:18,357 
We could copy it by running
hadoop fs -copytolocal 
57 
00:04:18,357 --> 00:04:23,481 
out/part-r-00000 local. 
58 
00:04:25,760 --> 00:04:28,520 
And we'll say local.txt is the name. 
59 
00:04:28,520 --> 00:04:33,340 
You can view the results of this. 
60 
00:04:33,340 --> 00:04:35,320 
We're running more local.txt. 
61 
00:04:38,170 --> 00:04:39,740 
This will view the contents of the file. 
62 
00:04:42,700 --> 00:04:44,310 
We can hit spacebar to scroll down. 
63 
00:04:47,130 --> 00:04:50,450 
We see the results of
WordCount in this file. 
64 
00:04:50,450 --> 00:04:55,980 
Each line is a particular word and
the second column is the count 
65 
00:04:55,980 --> 00:05:00,020 
of how many words of this particular
word was found in the input file. 
66 
00:05:02,820 --> 00:05:04,159 
You can hit q to quit 
1 
00:00:01,110 --> 00:00:03,128 
Scalable computing over the internet. 
2 
00:00:19,074 --> 00:00:23,040 
Most computing is done on
a single compute node. 
3 
00:00:24,900 --> 00:00:29,960 
If the computation needs more than
a node or parallel processing, 
4 
00:00:29,960 --> 00:00:34,070 
like many scientific computing problems,
we use parallel computers. 
5 
00:00:35,070 --> 00:00:40,420 
Simply put, a parallel computer
is a very large number of 
6 
00:00:40,420 --> 00:00:46,041 
single computing nodes with specialized
capabilities connected to other network. 
7 
00:00:47,130 --> 00:00:54,525 
For example, the Gordon Supercomputer here
at The San Diego Supercomputer Center, 
8 
00:00:54,525 --> 00:01:03,170 
has 1,024 compute nodes with 16 cores each
equalling 16,384 compute cores in total. 
9 
00:01:04,230 --> 00:01:07,890 
This type of specialized
computer is pretty costly 
10 
00:01:07,890 --> 00:01:13,180 
compared to its most recent cousin,
the commodity cluster. 
11 
00:01:13,180 --> 00:01:17,920 
The term, commodity cluster,
is often heard in big data conversations. 
12 
00:01:19,280 --> 00:01:21,560 
Have you ever wondered
what it exactly means? 
13 
00:01:22,960 --> 00:01:27,740 
Commodity clusters are affordable
parallel computers 
14 
00:01:27,740 --> 00:01:29,780 
with an average number of computing nodes. 
15 
00:01:31,070 --> 00:01:35,650 
They are not as powerful as
traditional parallel computers and 
16 
00:01:35,650 --> 00:01:38,280 
are often built out of
less specialized nodes. 
17 
00:01:39,400 --> 00:01:42,660 
In fact,
the nodes in the commodity cluster 
18 
00:01:42,660 --> 00:01:45,640 
are more generic in their
computing capabilities. 
19 
00:01:46,960 --> 00:01:51,320 
The service-oriented computing community
over the internet have pushed for 
20 
00:01:51,320 --> 00:01:56,800 
computing to be done on commodity
clusters as distributed computations. 
21 
00:01:56,800 --> 00:02:02,030 
And in turn, reducing the cost
of computing over the Internet. 
22 
00:02:03,680 --> 00:02:09,390 
In commodity clusters,
the computing nodes are clustered in racks 
23 
00:02:11,400 --> 00:02:15,480 
connected to each other
via a fast network. 
24 
00:02:16,860 --> 00:02:20,150 
There might be many of such
racks in extensible amounts. 
25 
00:02:21,600 --> 00:02:26,410 
Computing in one or
more of these clusters across 
26 
00:02:26,410 --> 00:02:31,910 
a local area network or the internet
is called distributed computing. 
27 
00:02:31,910 --> 00:02:36,860 
Such architectures enable what
we call data-parallelism. 
28 
00:02:36,860 --> 00:02:41,416 
In data-parallelism many jobs
that share nothing can work on 
29 
00:02:41,416 --> 00:02:44,580 
different data sets or
parts of a data set. 
30 
00:02:45,930 --> 00:02:51,060 
This type of parallelism sometimes
gets called as job level parallelism. 
31 
00:02:51,060 --> 00:02:55,950 
But in this specialization,
we will refer to it as data-parallelism 
32 
00:02:55,950 --> 00:03:00,020 
in the context of Big-data computing. 
33 
00:03:00,020 --> 00:03:05,020 
Large volumes and varieties of big
data can be analyzed using this mode 
34 
00:03:05,020 --> 00:03:11,540 
of parallelism, achieving scalability,
performance and cost reduction. 
35 
00:03:11,540 --> 00:03:16,070 
As you can imagine, there are many
points of failure inside systems. 
36 
00:03:17,500 --> 00:03:23,200 
A node, or
an entire rack can fail at any given time. 
37 
00:03:23,200 --> 00:03:27,850 
The connectivity of a rack
to the network can stop or 
38 
00:03:27,850 --> 00:03:31,300 
the connections between
individual nodes can break. 
39 
00:03:32,820 --> 00:03:37,370 
It is not practical to restart everything
every time, if failure happens. 
40 
00:03:38,500 --> 00:03:44,069 
The ability to recover from such
failures is called Fault-tolerance. 
41 
00:03:44,069 --> 00:03:49,245 
For Fault-tolerance of such systems,
two neat solutions emerged. 
42 
00:03:49,245 --> 00:03:54,230 
Namely, Redundant data storage and 
43 
00:03:54,230 --> 00:03:57,480 
restart of failed
individual parallel jobs. 
44 
00:03:58,700 --> 00:04:00,780 
We will explain these two solutions next. 
45 
00:04:02,100 --> 00:04:06,970 
As a summary the commodity
clusters are a cost effective way 
46 
00:04:06,970 --> 00:04:11,360 
of achieving data parallel scalability for
big data applications. 
47 
00:04:11,360 --> 00:04:16,750 
These type of systems have a higher
potential for partial failures. 
48 
00:04:16,750 --> 00:04:19,850 
It is this type of distributed
computing that pushed for 
49 
00:04:19,850 --> 00:04:23,385 
a change towards cost
effective reliable and 
50 
00:04:23,385 --> 00:04:27,550 
Fault-tolerant systems for
management and analysis of big data. 
1 
00:00:02,950 --> 00:00:07,672 
The Hadoop Distributed File System,
a storage system for big data. 
2 
00:00:21,994 --> 00:00:28,920 
As a storage layer, the Hadoop distributed
file system, or the way we call it HDFS. 
3 
00:00:30,070 --> 00:00:33,890 
Serves as the foundation for
most tools in the Hadoop ecosystem. 
4 
00:00:35,700 --> 00:00:39,900 
It provides two capabilities that
are essential for managing big data. 
5 
00:00:40,900 --> 00:00:43,500 
Scalability to large data sets. 
6 
00:00:43,500 --> 00:00:46,570 
And reliability to cope
with hardware failures. 
7 
00:00:48,760 --> 00:00:53,060 
HDFS allows you to store and
access large datasets. 
8 
00:00:53,060 --> 00:00:57,700 
According to Hortonworks,
a leading vendor of Hadoop services, 
9 
00:00:57,700 --> 00:01:03,950 
HDFS has shown production
scalability up to 200 petabytes and 
10 
00:01:03,950 --> 00:01:06,980 
a single cluster of 4,500 servers. 
11 
00:01:06,980 --> 00:01:09,684 
With close to a billion files and blocks. 
12 
00:01:11,504 --> 00:01:16,610 
If you run out of space, you can simply
add more nodes to increase the space. 
13 
00:01:19,000 --> 00:01:22,470 
HDFS achieves scalability
by partitioning or 
14 
00:01:22,470 --> 00:01:26,630 
splitting large files
across multiple computers. 
15 
00:01:26,630 --> 00:01:31,040 
This allows parallel access to very large
files since the computations run in 
16 
00:01:31,040 --> 00:01:34,300 
parallel on each node
where the data is stored. 
17 
00:01:35,760 --> 00:01:38,920 
Typical file size is
gigabytes to terabytes. 
18 
00:01:40,150 --> 00:01:47,900 
The default chunk size, the size of
each piece of a file is 64 megabytes. 
19 
00:01:47,900 --> 00:01:50,390 
But you can configure this to any size. 
20 
00:01:51,950 --> 00:01:55,180 
By spreading the file across many nodes, 
21 
00:01:55,180 --> 00:01:59,740 
the chances are increased that a node
storing one of the blocks will fail. 
22 
00:02:01,650 --> 00:02:03,440 
What happens next? 
23 
00:02:03,440 --> 00:02:05,970 
Do we lose the information
stored in block C? 
24 
00:02:09,560 --> 00:02:12,350 
HDFS is designed for
full tolerance in such case. 
25 
00:02:13,640 --> 00:02:16,080 
HDFS replicates, or 
26 
00:02:16,080 --> 00:02:21,530 
makes a copy of, file blocks on
different nodes to prevent data loss. 
27 
00:02:23,820 --> 00:02:28,945 
In this example,
the node that crashed stored block C. 
28 
00:02:28,945 --> 00:02:37,270 
But block C was replicated on
two other nodes in the cluster. 
29 
00:02:39,820 --> 00:02:44,380 
By default, HDFS maintains
three copies of every block. 
30 
00:02:45,760 --> 00:02:47,850 
This is the default replication factor. 
31 
00:02:48,890 --> 00:02:52,300 
But you can change it globally for
every file, or 
32 
00:02:52,300 --> 00:02:57,180 
on a per file basis. 
33 
00:02:57,180 --> 00:03:01,370 
HDFS is also designed to handle
a variety of data types aligned with 
34 
00:03:01,370 --> 00:03:02,450 
big data variety. 
35 
00:03:04,370 --> 00:03:10,220 
To read a file in HDFS you must
specify the input file format. 
36 
00:03:10,220 --> 00:03:14,610 
Similarly to write the file you must
provide the output file format. 
37 
00:03:16,885 --> 00:03:21,040 
HDFS provides a set of formats for
common data types. 
38 
00:03:21,040 --> 00:03:27,310 
But this is extensible and you can provide
custom formats for your data types. 
39 
00:03:27,310 --> 00:03:30,830 
For example text files can be read. 
40 
00:03:32,030 --> 00:03:34,620 
Line by line or a word at a time. 
41 
00:03:36,950 --> 00:03:42,650 
Geospatial data can be read as vectors or
rasters. 
42 
00:03:43,660 --> 00:03:49,210 
Data formats specific to geospatial data,
or 
43 
00:03:49,210 --> 00:03:52,670 
other domain specific data formats. 
44 
00:03:52,670 --> 00:03:57,588 
Like FASTA, or FASTQ formats for
sequence data genomics. 
45 
00:03:59,660 --> 00:04:02,665 
HDFS is comprised of two components. 
46 
00:04:02,665 --> 00:04:06,582 
NameNode, and DataNode. 
47 
00:04:06,582 --> 00:04:11,180 
These operate using a master
slave relationship. 
48 
00:04:12,350 --> 00:04:16,960 
Where the NameNode issues comments
to DataNodes across the cluster. 
49 
00:04:18,360 --> 00:04:22,110 
The NameNode is responsible for metadata. 
50 
00:04:22,110 --> 00:04:25,072 
And DataNodes provide block storage. 
51 
00:04:27,174 --> 00:04:31,064 
There is usually one NameNode per cluster, 
52 
00:04:31,064 --> 00:04:35,910 
a DataNode however,
runs on each node in the cluster. 
53 
00:04:37,920 --> 00:04:42,280 
In some sense the NameNode
is the administrator or 
54 
00:04:42,280 --> 00:04:44,850 
the coordinator of the HDFS cluster. 
55 
00:04:46,030 --> 00:04:50,560 
When the file is created,
the NameNode records the name, 
56 
00:04:50,560 --> 00:04:54,010 
location in the directory hierarchy and
other metadata. 
57 
00:04:55,820 --> 00:05:01,730 
The NameNode also decides which data nodes
to store the contents of the file and 
58 
00:05:01,730 --> 00:05:03,370 
remembers this mapping. 
59 
00:05:05,420 --> 00:05:08,400 
The DataNode runs on
each node in the cluster. 
60 
00:05:09,460 --> 00:05:12,880 
And is responsible for
storing the file blocks. 
61 
00:05:14,600 --> 00:05:19,328 
The data node listens to commands from
the name node for block creation, 
62 
00:05:19,328 --> 00:05:25,980 
deletion, and replication. 
63 
00:05:25,980 --> 00:05:29,530 
Replication provides two key capabilities. 
64 
00:05:29,530 --> 00:05:32,260 
Fault tolerance and data locality. 
65 
00:05:33,390 --> 00:05:38,730 
As discussed earlier, when a machine in
the cluster has a hardware failure there 
66 
00:05:38,730 --> 00:05:43,100 
are two other copies of each block
that are stored on that node. 
67 
00:05:43,100 --> 00:05:44,470 
So no data is lost. 
68 
00:05:45,660 --> 00:05:49,920 
Replication also means that the same block
will be stored on different nodes on 
69 
00:05:49,920 --> 00:05:54,780 
the system which are in different
geographical locations. 
70 
00:05:56,370 --> 00:06:01,280 
A location may mean a specific rack or
a data center in a different town. 
71 
00:06:03,140 --> 00:06:07,950 
The location is important since we
want to move computation to data and 
72 
00:06:07,950 --> 00:06:08,890 
not the other way around. 
73 
00:06:11,100 --> 00:06:15,030 
We'll talk about what moving computation
to data means later in this module. 
74 
00:06:16,650 --> 00:06:21,120 
As I mentioned earlier, the default
replication factor is three, but 
75 
00:06:21,120 --> 00:06:23,080 
you can change this. 
76 
00:06:23,080 --> 00:06:28,370 
A high replication factor means more
protection against hardware failures, 
77 
00:06:28,370 --> 00:06:31,350 
and better chances for data locality. 
78 
00:06:31,350 --> 00:06:34,660 
But it also means increased
storage space is used. 
79 
00:06:37,190 --> 00:06:40,050 
As a summary HDFS provides 
80 
00:06:40,050 --> 00:06:44,440 
scalable big data storage by
partitioning files over multiple nodes. 
81 
00:06:45,950 --> 00:06:50,160 
This helps to scale big data
analytics to large data volumes. 
82 
00:06:51,530 --> 00:06:54,730 
The application protects
against hardware failures and 
83 
00:06:54,730 --> 00:06:59,420 
provides data locality when we move
analytical complications to data. 
1 
00:00:02,252 --> 00:00:05,592 
The Hadoop Ecosystem: So Much free Stuff! 
2 
00:00:20,275 --> 00:00:24,046 
How did the big data
open-source movement begin? 
3 
00:00:24,046 --> 00:00:28,492 
In 2004 Google published
a paper about their 
4 
00:00:28,492 --> 00:00:33,811 
in-house processing framework
they called MapReduce. 
5 
00:00:33,811 --> 00:00:37,699 
The next year,
Yahoo released an open-source 
6 
00:00:37,699 --> 00:00:42,369 
implementation based on this
framework called Hadoop. 
7 
00:00:42,369 --> 00:00:45,379 
In the following years,
other frameworks and 
8 
00:00:45,379 --> 00:00:49,650 
tools were released to the community
as open-source projects. 
9 
00:00:49,650 --> 00:00:53,764 
These frameworks provided new
capabilities missing in Hadoop, 
10 
00:00:53,764 --> 00:00:56,990 
such as SQL like querying or
high level scripting. 
11 
00:00:59,240 --> 00:01:03,220 
Today, there are over 100
open-source projects for 
12 
00:01:03,220 --> 00:01:06,880 
big data and
this number continues to grow. 
13 
00:01:08,160 --> 00:01:11,181 
Many rely on Hadoop, but
some are independent. 
14 
00:01:13,711 --> 00:01:18,420 
With so many frameworks and tools
available, how do we learn what they do? 
15 
00:01:19,600 --> 00:01:26,590 
We can organize them with a layer diagram
to understand their capabilities. 
16 
00:01:26,590 --> 00:01:30,830 
Sometimes we also used the term
stack instead of a layer diagram. 
17 
00:01:32,560 --> 00:01:37,400 
In a layer diagram,
a component uses the functionality or 
18 
00:01:37,400 --> 00:01:41,958 
capabilities of the components
in the layer below it. 
19 
00:01:41,958 --> 00:01:47,470 
Usually components at the same
layer do not communicate. 
20 
00:01:47,470 --> 00:01:54,669 
And a component never assumes a specific
tool or component is above it. 
21 
00:01:54,669 --> 00:02:00,901 
In this example,
component A is in the bottom layer, 
22 
00:02:00,901 --> 00:02:04,370 
which components B and C use. 
23 
00:02:06,170 --> 00:02:09,390 
Component D uses B, but not C. 
24 
00:02:11,240 --> 00:02:13,790 
And D does not directly use A. 
25 
00:02:17,060 --> 00:02:21,630 
Let's look at one set of tools in
the Hadoop ecosystem as a layer diagram. 
26 
00:02:23,920 --> 00:02:30,795 
This layer diagram is organized
vertically based on the interface. 
27 
00:02:30,795 --> 00:02:35,054 
Low level interfaces, so storage and
scheduling, on the bottom. 
28 
00:02:35,054 --> 00:02:39,315 
And high level languages and
interactivity at the top. 
29 
00:02:41,915 --> 00:02:47,315 
The Hadoop distributed file system,
or HDFS, is the foundation for 
30 
00:02:47,315 --> 00:02:53,570 
many big data frameworks, since it
provides scaleable and reliable storage. 
31 
00:02:54,740 --> 00:03:00,380 
As the size of your data increases,
you can add commodity hardware 
32 
00:03:00,380 --> 00:03:05,660 
to HDFS to increase storage capacity so 
33 
00:03:05,660 --> 00:03:09,250 
it enables scaling out of your resources. 
34 
00:03:11,460 --> 00:03:15,388 
Hadoop YARN provides
flexible scheduling and 
35 
00:03:15,388 --> 00:03:19,220 
resource management over the HDFS storage. 
36 
00:03:20,910 --> 00:03:25,988 
YARN is used at Yahoo to schedule
jobs across 40,000 servers. 
37 
00:03:28,370 --> 00:03:33,430 
MapReduce is a programming model
that simplifies parallel computing. 
38 
00:03:34,720 --> 00:03:39,300 
Instead of dealing with the complexities
of synchronization and scheduling, you 
39 
00:03:39,300 --> 00:03:45,920 
only need to give MapReduce two functions,
map and reduce, as you heard before. 
40 
00:03:48,020 --> 00:03:50,080 
This programming model is so 
41 
00:03:50,080 --> 00:03:55,740 
powerful that Google previously
used it for indexing websites. 
42 
00:03:59,450 --> 00:04:05,009 
MapReduce only assume a limited
model to express data. 
43 
00:04:05,009 --> 00:04:10,094 
Hive and Pig are two additional
programming models on 
44 
00:04:10,094 --> 00:04:15,292 
top of MapReduce to augment
data modeling of MapReduce 
45 
00:04:15,292 --> 00:04:21,398 
with relational algebra and
data flow modeling respectively. 
46 
00:04:21,398 --> 00:04:25,718 
Hive was created at
Facebook to issue SQL-like 
47 
00:04:25,718 --> 00:04:30,038 
queries using MapReduce
on their data in HDFS. 
48 
00:04:30,038 --> 00:04:37,951 
Pig was created at Yahoo to model data
flow based programs using MapReduce. 
49 
00:04:37,951 --> 00:04:41,431 
Thanks to YARN stability
to manage resources, 
50 
00:04:41,431 --> 00:04:45,959 
not just for MapReduce but
other programming models as well. 
51 
00:04:45,959 --> 00:04:50,787 
Giraph was built for
processing large-scale graphs efficiently. 
52 
00:04:50,787 --> 00:04:57,904 
For example, Facebook uses Giraph to
analyze the social graphs of its users. 
53 
00:04:57,904 --> 00:05:03,196 
Similarly, Storm, Spark,
and Flink were built for 
54 
00:05:03,196 --> 00:05:07,899 
real time and
in memory processing of big data on 
55 
00:05:07,899 --> 00:05:12,381 
top of the YARN resource scheduler and
HDFS. 
56 
00:05:12,381 --> 00:05:17,986 
In-memory processing is a powerful way of
running big data applications even faster, 
57 
00:05:17,986 --> 00:05:21,787 
achieving 100x's better performance for
some tasks. 
58 
00:05:24,446 --> 00:05:28,405 
Sometimes, your data or
processing tasks are not easily or 
59 
00:05:28,405 --> 00:05:33,330 
efficiently represented using the file and
directory model of storage. 
60 
00:05:34,620 --> 00:05:39,810 
Examples of this include collections
of key-values or large sparse tables. 
61 
00:05:41,990 --> 00:05:49,381 
NoSQL projects such as Cassandra,
MongoDB, and HBase handle these cases. 
62 
00:05:49,381 --> 00:05:54,180 
Cassandra was created at Facebook,
but Facebook also used HBase for 
63 
00:05:54,180 --> 00:05:56,010 
its messaging platform. 
64 
00:05:58,320 --> 00:06:03,560 
Finally, running all of these tools
requires a centralized management system 
65 
00:06:03,560 --> 00:06:08,490 
for synchronization, configuration and
to ensure high availability. 
66 
00:06:09,860 --> 00:06:13,210 
Zookeeper performs these duties. 
67 
00:06:13,210 --> 00:06:17,700 
It was created by Yahoo to wrangle
services named after animals. 
68 
00:06:20,060 --> 00:06:22,990 
A major benefit of the Hadoop ecosystem 
69 
00:06:22,990 --> 00:06:25,340 
is that all these tools
are open-source projects. 
70 
00:06:26,450 --> 00:06:28,920 
You can download and use them for free. 
71 
00:06:30,220 --> 00:06:34,934 
Each project has a community of users and
developers that 
72 
00:06:34,934 --> 00:06:39,756 
answer questions, fix bugs and
implement new features. 
73 
00:06:39,756 --> 00:06:44,370 
You can mix and match to get only
the tools you need to achieve your goals. 
74 
00:06:46,010 --> 00:06:50,330 
Alternatively, there are several
pre-built stacks of these tools 
75 
00:06:50,330 --> 00:06:54,670 
offered by companies such as Cloudera,
MAPR and Hortonworks. 
76 
00:06:56,090 --> 00:07:00,850 
These companies provide the core
software stacks for free and 
77 
00:07:00,850 --> 00:07:03,740 
offer commercial support for
production environments. 
78 
00:07:05,800 --> 00:07:09,110 
As a summary, the Hadoop ecosystem 
79 
00:07:09,110 --> 00:07:13,672 
consists of a growing number
of open-source tools. 
80 
00:07:13,672 --> 00:07:17,502 
Providing opportunities to
pick the right tool for 
81 
00:07:17,502 --> 00:07:21,788 
the right tasks for
better performance and lower costs. 
82 
00:07:21,788 --> 00:07:25,284 
We will reveal some of these
tools in further detail and 
83 
00:07:25,284 --> 00:07:29,620 
provide an analysis of when to use
which in the next set of lectures. 
1 
00:00:00,610 --> 00:00:03,160 
Generating value from Hadoop and 
2 
00:00:03,160 --> 00:00:07,996 
Pre-Built Hadoop Images that
come as off the shelf products. 
3 
00:00:18,576 --> 00:00:23,922 
Assembling your own software stack
from scratch can be messy and 
4 
00:00:23,922 --> 00:00:26,298 
a lot of work for beginners. 
5 
00:00:26,298 --> 00:00:31,661 
The task of setting up the whole stack
could consume a lot of project time and 
6 
00:00:31,661 --> 00:00:35,550 
man power, reducing time to deployment. 
7 
00:00:35,550 --> 00:00:41,490 
Getting pre-built images is similar
to buying pre-assembled furniture. 
8 
00:00:41,490 --> 00:00:46,701 
You can obtain a ready to go software
stack which contains a pre-installed 
9 
00:00:46,701 --> 00:00:51,510 
operating system, required libraries and
application software. 
10 
00:00:52,830 --> 00:00:55,380 
It saves you from the trouble
of putting the different 
11 
00:00:55,380 --> 00:00:58,090 
parts together in the right orientation. 
12 
00:00:58,090 --> 00:01:00,030 
You can start using
the furniture right away. 
13 
00:01:01,600 --> 00:01:05,620 
Packaging of these pre-built
software images is enabled 
14 
00:01:05,620 --> 00:01:08,420 
by virtual machines using
virtualization software. 
15 
00:01:09,640 --> 00:01:14,380 
Without going into too much detail,
one of the benefits of virtualization 
16 
00:01:14,380 --> 00:01:18,710 
software is that it lets you run a ready
made software stack within minutes. 
17 
00:01:19,890 --> 00:01:23,390 
Your software stack comes as a large file. 
18 
00:01:23,390 --> 00:01:26,800 
Virtualization software provides
a platform where your stack can run. 
19 
00:01:28,160 --> 00:01:33,240 
Many companies provide images for
their version of the Hadoop platform, 
20 
00:01:33,240 --> 00:01:36,100 
including a number of
tools of their choice. 
21 
00:01:37,100 --> 00:01:41,570 
Hortonworks is one of the companies that
provides a pre-built software stack for 
22 
00:01:41,570 --> 00:01:43,370 
both Mac and Windows platforms. 
23 
00:01:44,520 --> 00:01:48,420 
Cloudera is another company
that provides pre-installed and 
24 
00:01:48,420 --> 00:01:51,120 
assembled software stack images. 
25 
00:01:51,120 --> 00:01:54,230 
Cloudera image is what we will
be working with in this course. 
26 
00:01:55,480 --> 00:01:58,020 
Many other companies
provide similar images. 
27 
00:01:59,100 --> 00:02:04,831 
Additionally, lots of online tutorials for
beginners are on vendors websites for 
28 
00:02:04,831 --> 00:02:08,740 
self-training of users
working with these images and 
29 
00:02:08,740 --> 00:02:10,580 
the open source tools they include. 
30 
00:02:12,270 --> 00:02:16,200 
Once you choose the vendor,
you can check out their website for 
31 
00:02:16,200 --> 00:02:18,550 
tutorials on how to get started quickly. 
32 
00:02:19,590 --> 00:02:21,980 
There are plenty of resources online for
that. 
33 
00:02:23,240 --> 00:02:26,080 
You can deploy pre-built
images over the Cloud. 
34 
00:02:27,110 --> 00:02:30,490 
This would further accelerate your
application deployment process. 
35 
00:02:31,850 --> 00:02:36,170 
It is always best to evaluate which
approach is most cost effective for 
36 
00:02:36,170 --> 00:02:38,320 
your business model and organization. 
37 
00:02:39,560 --> 00:02:43,485 
Companies such as Cloudera,
Hortonworks and others, 
38 
00:02:43,485 --> 00:02:49,090 
provide step-by-step guides on how to
set up pre-built images on the Cloud. 
39 
00:02:50,180 --> 00:02:55,460 
As a summary, using pre-built software
packages have a number of benefits and 
40 
00:02:55,460 --> 00:02:58,280 
can significantly accelerate
your big data projects. 
41 
00:02:59,500 --> 00:03:06,558 
Even small teams can quickly prototype,
deploy and validate their project ideas. 
42 
00:03:06,558 --> 00:03:11,480 
The developed analytical solutions
can be scaled to larger volumes and 
43 
00:03:11,480 --> 00:03:15,560 
increase velocities of
data in a matter of hours. 
44 
00:03:15,560 --> 00:03:20,162 
These companies also provide
Enterprise level solutions for large, 
45 
00:03:20,162 --> 00:03:21,837 
full-fledged applications. 
46 
00:03:22,940 --> 00:03:27,395 
An added benefit is that there
are plenty of companies which provide 
47 
00:03:27,395 --> 00:03:29,250 
ready-made solutions. 
48 
00:03:29,250 --> 00:03:35,270 
That means lots of choices for you to
pick the one most suited to your project. 
1 
00:00:00,540 --> 00:00:02,593 
What is a Distributed File System? 
2 
00:00:16,485 --> 00:00:19,336 
Most of us have file
cabinets in our offices or 
3 
00:00:19,336 --> 00:00:22,420 
homes that help us store
our printed documents. 
4 
00:00:24,170 --> 00:00:27,280 
Everyone has their own
method of organizing files, 
5 
00:00:27,280 --> 00:00:31,410 
including the way we bin similar
documents into one file, or 
6 
00:00:31,410 --> 00:00:35,210 
the way we sort them in alphabetical or
date order. 
7 
00:00:35,210 --> 00:00:38,380 
When computers first came out,
the information and 
8 
00:00:38,380 --> 00:00:40,970 
programs were stored in punch cards. 
9 
00:00:42,280 --> 00:00:45,374 
These punch cards were
stored in file cabinets, 
10 
00:00:45,374 --> 00:00:48,760 
just like the physical
file cabinets today. 
11 
00:00:48,760 --> 00:00:51,940 
This is where the name,
file system, comes from. 
12 
00:00:51,940 --> 00:00:55,970 
The need to store information in
files comes from a larger need 
13 
00:00:55,970 --> 00:00:58,720 
to store information in the long-term. 
14 
00:00:58,720 --> 00:01:03,360 
This way the information lives
after the computer program, or 
15 
00:01:03,360 --> 00:01:07,350 
what we call process,
that produced it terminates. 
16 
00:01:07,350 --> 00:01:12,450 
If we don't have files, our access to
such information would not be possible 
17 
00:01:12,450 --> 00:01:15,560 
once a program using or producing it. 
18 
00:01:15,560 --> 00:01:20,430 
Even during the process, we might need
to store large amounts of information 
19 
00:01:20,430 --> 00:01:25,240 
that we cannot store within the program
components or computer memory. 
20 
00:01:25,240 --> 00:01:28,690 
In addition, once the data is in a file, 
21 
00:01:28,690 --> 00:01:33,120 
multiple processes can access
the same information if needed. 
22 
00:01:33,120 --> 00:01:38,940 
For all these reasons, we store
information in files on a hard disk. 
23 
00:01:38,940 --> 00:01:41,350 
There are many of these files, and 
24 
00:01:41,350 --> 00:01:45,750 
they get managed by your operating system,
like Windows or Linux. 
25 
00:01:45,750 --> 00:01:50,640 
How the operating system manages
files is called a file system. 
26 
00:01:50,640 --> 00:01:55,930 
How this information is stored
on disk drives has high impact 
27 
00:01:55,930 --> 00:02:02,090 
on the efficiency and speed of access to
data, especially in the big data case. 
28 
00:02:02,090 --> 00:02:07,420 
While the files have exact addresses for
their locations in the drive, referring 
29 
00:02:07,420 --> 00:02:13,274 
to the data units of sequence of these
blocks, that's called the flat structure, 
30 
00:02:13,274 --> 00:02:18,510 
or hierarchy construction of index
records, that's called the database. 
31 
00:02:18,510 --> 00:02:24,710 
They also have human readable symbolic
names, generally followed by an extension. 
32 
00:02:25,750 --> 00:02:29,140 
Extensions tell what kind of file it is,
in general. 
33 
00:02:29,140 --> 00:02:33,640 
Programs and
users can access files with their names. 
34 
00:02:33,640 --> 00:02:39,010 
The contents of a file can be numeric,
alphabetic, alphanumeric, 
35 
00:02:39,010 --> 00:02:40,160 
or binary executables. 
36 
00:02:41,180 --> 00:02:45,250 
Most computer users work
on personal laptops or 
37 
00:02:45,250 --> 00:02:48,050 
desktop computers with
a single hard drive. 
38 
00:02:49,050 --> 00:02:53,780 
In this model, the user is limited
to the capacity of their hard drive. 
39 
00:02:53,780 --> 00:02:57,060 
The capacity of different devices vary. 
40 
00:02:57,060 --> 00:02:59,460 
For example, while your phone or 
41 
00:02:59,460 --> 00:03:04,380 
tablet might have a storage capacity
in the order of gigabytes, your 
42 
00:03:04,380 --> 00:03:10,210 
laptop computer might have a terabyte of
storage, but what if you have more data? 
43 
00:03:10,210 --> 00:03:13,290 
Some of you probably had
issues in the past with 
44 
00:03:13,290 --> 00:03:15,950 
running out of space on your hard drive. 
45 
00:03:15,950 --> 00:03:19,250 
A way to solve this is to have
an external hard drive and 
46 
00:03:19,250 --> 00:03:23,390 
store your files there or,
you can buy a bigger disk. 
47 
00:03:23,390 --> 00:03:28,580 
Both options are a bit of a hassle, to
copy the data to a new disk, aren't they? 
48 
00:03:28,580 --> 00:03:30,520 
They might not even be
an option sometimes. 
49 
00:03:31,560 --> 00:03:35,620 
Now imagine, you having two computers and 
50 
00:03:35,620 --> 00:03:41,050 
storing some of your data in one and
the rest of your data in another. 
51 
00:03:41,050 --> 00:03:45,560 
How you organize and partition your data
between these computers is up to you. 
52 
00:03:46,570 --> 00:03:50,400 
You might want to store your
work data in one computer and 
53 
00:03:50,400 --> 00:03:53,030 
your personal data in another. 
54 
00:03:53,030 --> 00:03:57,120 
Distributing data on multiple
computers might be an option, but 
55 
00:03:57,120 --> 00:03:59,030 
it raises new issues. 
56 
00:03:59,030 --> 00:04:04,800 
In this situation, you need to know
where to find the files you need, 
57 
00:04:04,800 --> 00:04:06,630 
depending on what you’re doing. 
58 
00:04:06,630 --> 00:04:10,080 
You can find it manageable,
if it’s just your data. 
59 
00:04:10,080 --> 00:04:14,500 
But now imagine having
thousands of computers 
60 
00:04:14,500 --> 00:04:18,373 
to store your data with big volumes and
variety. 
61 
00:04:18,373 --> 00:04:23,240 
Wouldn't it be good to have a system
that can handle the data access and 
62 
00:04:23,240 --> 00:04:24,330 
do this for you? 
63 
00:04:24,330 --> 00:04:29,450 
This is a case that can be handled
by a distributive file system. 
64 
00:04:29,450 --> 00:04:33,590 
Now, let's assume that there
are racks of these computers, 
65 
00:04:33,590 --> 00:04:37,758 
often even distributed across the local or 
66 
00:04:37,758 --> 00:04:43,400 
wide area network, because such file
systems, distributed file systems. 
67 
00:04:44,910 --> 00:04:48,800 
Data sets, or parts of a data set, 
68 
00:04:48,800 --> 00:04:52,900 
can be replicated across the nodes
of a distributed file system. 
69 
00:04:53,900 --> 00:04:58,900 
Since data is already on these nodes,
then analysis of parts of the data 
70 
00:04:58,900 --> 00:05:05,030 
is needed in a data parallel fashion,
computation can be moved to these nodes. 
71 
00:05:06,500 --> 00:05:11,590 
Additionally, distributed file
systems replicate the data 
72 
00:05:11,590 --> 00:05:16,720 
between the racks, and also computers
distributed across geographical regions. 
73 
00:05:18,180 --> 00:05:22,160 
Data replication makes
the system more fault tolerant. 
74 
00:05:23,250 --> 00:05:28,290 
That means, if some nodes or
a rack goes down, 
75 
00:05:28,290 --> 00:05:33,764 
there are other parts of the system,
the same data can be found and analyzed. 
76 
00:05:33,764 --> 00:05:40,080 
Data replication also helps with scaling
the access to this data by many users. 
77 
00:05:41,170 --> 00:05:47,110 
Often, if the data is popular, many
reader processes will want access to it. 
78 
00:05:48,310 --> 00:05:51,240 
In a highly parallelized replication, 
79 
00:05:51,240 --> 00:05:55,680 
each reader can get their own node
to access to and analyze data. 
80 
00:05:56,920 --> 00:05:59,450 
This increases overall system performance. 
81 
00:06:00,935 --> 00:06:05,600 
Note that a problem with having
such a distributive replication is, 
82 
00:06:05,600 --> 00:06:09,490 
that it is hard to make
changes to data over time. 
83 
00:06:09,490 --> 00:06:15,090 
However, in most big data systems,
the data is written once and 
84 
00:06:15,090 --> 00:06:20,570 
the updates to data is maintained
as additional data sets over time. 
85 
00:06:20,570 --> 00:06:25,500 
As a summary, a file system is
responsible from the organization of 
86 
00:06:25,500 --> 00:06:28,970 
the long term information
storage in a computer. 
87 
00:06:28,970 --> 00:06:33,991 
When many storage computers
are connected through the network, 
88 
00:06:33,991 --> 00:06:36,916 
we call it a distributed file system. 
89 
00:06:36,916 --> 00:06:42,035 
Distributed file systems provide data
scalability, fault tolerance, and 
90 
00:06:42,035 --> 00:06:47,321 
high concurrency through partitioning and
replication of data on many nodes. 
1 
00:00:03,148 --> 00:00:05,208 
When to reconsider Hadoop? 
2 
00:00:19,098 --> 00:00:22,960 
The Hadoop ecosystem is
growing at a fast pace. 
3 
00:00:23,970 --> 00:00:26,937 
This means a lot of stuff
that was difficult, or 
4 
00:00:26,937 --> 00:00:30,180 
not supportive, is becoming possible. 
5 
00:00:32,480 --> 00:00:33,990 
In this lecture, 
6 
00:00:33,990 --> 00:00:37,730 
we will look at some aspects that
clearly make a good match for Hadoop. 
7 
00:00:39,200 --> 00:00:43,220 
We will also look at several
aspects that might motivate you 
8 
00:00:43,220 --> 00:00:45,510 
to evaluate Hadoop at a deeper level. 
9 
00:00:47,060 --> 00:00:51,560 
And does Hadoop really make sense for
your specific problem? 
10 
00:00:54,190 --> 00:00:58,790 
First let's look at the key features
that make a problem Hadoop friendly. 
11 
00:00:59,950 --> 00:01:04,390 
If you see a large scale growth in
amount of data you will tackle, 
12 
00:01:04,390 --> 00:01:06,310 
probably it makes sense to use Hadoop. 
13 
00:01:07,610 --> 00:01:12,230 
When you want quick access to your old
data which would otherwise go on tape 
14 
00:01:12,230 --> 00:01:17,180 
drives for archival storage,
Hadoop might provide a good alternative. 
15 
00:01:20,080 --> 00:01:22,650 
Other Hadoop friendly features include 
16 
00:01:22,650 --> 00:01:28,020 
scenarios when you want to use multiple
applications over the same data store. 
17 
00:01:28,020 --> 00:01:31,683 
High volume or
high variety are also great indicators for 
18 
00:01:31,683 --> 00:01:33,598 
Hadoop as a platform choice. 
19 
00:01:37,148 --> 00:01:40,860 
Small data set processing
should raise your eyebrows. 
20 
00:01:40,860 --> 00:01:42,410 
Do you really need Hadoop for that? 
21 
00:01:43,560 --> 00:01:49,178 
Dig deeper, and find out exactly why you
want to use Hadoop before going ahead. 
22 
00:01:53,238 --> 00:01:56,330 
Hadoop is good for data parallelism. 
23 
00:01:56,330 --> 00:02:01,320 
As you know, data parallelism is
the simultaneous execution of the same 
24 
00:02:01,320 --> 00:02:05,520 
function on multiple nodes across
the elements of a dataset. 
25 
00:02:06,630 --> 00:02:11,060 
On the other hand, task parallelism,
as you see in this graph, 
26 
00:02:12,640 --> 00:02:17,160 
is the simultaneous execution
of many different functions 
27 
00:02:17,160 --> 00:02:21,260 
on multiple nodes across the same or
different data sets. 
28 
00:02:22,930 --> 00:02:26,950 
If your problem has task-level
parallelism, you must do further 
29 
00:02:26,950 --> 00:02:30,990 
analysis as to which tools you plan
to deploy from the Hadoop ecosystem. 
30 
00:02:32,910 --> 00:02:36,260 
What are the precise benefits
that these tools provide? 
31 
00:02:37,790 --> 00:02:38,920 
Proceed with caution. 
32 
00:02:41,330 --> 00:02:44,330 
Not all algorithms are scalable in Hadoop,
or 
33 
00:02:44,330 --> 00:02:48,030 
reducible to one of the programming
models supported by YARN. 
34 
00:02:50,090 --> 00:02:55,590 
Hence, if you are looking to deploy
highly coupled data processing algorithms 
35 
00:02:55,590 --> 00:02:56,860 
proceed with caution. 
36 
00:02:58,300 --> 00:03:01,270 
Do a thorough analysis
before using Hadoop. 
37 
00:03:01,270 --> 00:03:05,680 
Are you thinking of 
38 
00:03:05,680 --> 00:03:10,350 
throwing away your existing database
solutions and replacing them with Hadoop? 
39 
00:03:10,350 --> 00:03:10,850 
Think again. 
40 
00:03:12,130 --> 00:03:17,100 
Hadoop may be a good platform where
your diverse data sets can land and 
41 
00:03:17,100 --> 00:03:21,210 
get processed into a form
digestible with your database. 
42 
00:03:22,660 --> 00:03:27,070 
Hadoop may not be the best data store
solution for your business case. 
43 
00:03:27,070 --> 00:03:29,470 
Evaluate, and proceed with caution. 
44 
00:03:30,960 --> 00:03:35,560 
HDFS stores data in blocks
of 64 megabytes or larger, 
45 
00:03:35,560 --> 00:03:40,940 
so you may have to read an entire
file just to pick one data entry. 
46 
00:03:42,580 --> 00:03:45,858 
That makes it a bit harder to
perform random data access. 
47 
00:03:48,078 --> 00:03:52,370 
The Hadoop ecosystem is growing
at a faster pace than ever. 
48 
00:03:53,820 --> 00:03:57,530 
This slide shows some of the moving
targets in the Hadoop ecosystem and 
49 
00:03:57,530 --> 00:04:00,650 
the additional needs which
must be addressed by new tools 
50 
00:04:00,650 --> 00:04:02,470 
to the Hadoop ecosystem. 
51 
00:04:02,470 --> 00:04:05,260 
Mainly, advanced analytical queries, 
52 
00:04:06,270 --> 00:04:10,580 
latency sensitive tasks, and
cyber security of sensitive data. 
53 
00:04:12,050 --> 00:04:16,510 
Here, we give pointers to tools you
might want to look into further 
54 
00:04:16,510 --> 00:04:19,090 
to understand the challenges
these need tools address. 
55 
00:04:21,000 --> 00:04:27,590 
As a summary, although Hadoop is good with
scalability of many algorithms, it is just 
56 
00:04:27,590 --> 00:04:32,520 
one model and does not solve all issues
in managing and processing big data. 
57 
00:04:33,630 --> 00:04:38,590 
Although it would be possible to find
counterexamples, we can generally say 
58 
00:04:38,590 --> 00:04:43,110 
that the Hadoop framework is not the best
for working with small data sets, 
59 
00:04:43,110 --> 00:04:47,120 
advanced algorithms that require
a specific hardware type, 
60 
00:04:47,120 --> 00:04:52,190 
task level parallelism, infrastructure
replacement, or random data access. 
1 
00:00:02,028 --> 00:00:04,748 
YARN, The Resource Manager for Hadoop. 
2 
00:00:21,825 --> 00:00:27,260 
YARN is a resource manage layer that
sits just above the storage layer HDFS. 
3 
00:00:28,510 --> 00:00:33,210 
YARN interacts with applications and
schedules resources for their use. 
4 
00:00:34,390 --> 00:00:39,390 
YARN enables running multiple
applications over HDFC increases 
5 
00:00:39,390 --> 00:00:44,160 
resource efficiency and
let's you go beyond the map reduce or 
6 
00:00:44,160 --> 00:00:46,720 
even beyond the data
parallel programming model. 
7 
00:00:48,910 --> 00:00:52,630 
When Hadoop was first created,
this wasn't the case. 
8 
00:00:52,630 --> 00:00:57,560 
In fact, the original Hadoop
stack had no resource manager. 
9 
00:00:57,560 --> 00:01:02,480 
These two stacked diagrams, show, some of
it's evolution over the last ten years. 
10 
00:01:03,980 --> 00:01:07,580 
One of the biggest limitations
of Hadoop one point zero, 
11 
00:01:07,580 --> 00:01:11,318 
was it's inability to support
non-mapreduce applications. 
12 
00:01:11,318 --> 00:01:15,200 
It had several terrible
resource utilization. 
13 
00:01:15,200 --> 00:01:19,770 
This meant that for advanced
applications such as graph analysis that 
14 
00:01:19,770 --> 00:01:22,020 
required different ways of modelling and 
15 
00:01:22,020 --> 00:01:26,950 
looking at data, you would need to
move your data to another platform. 
16 
00:01:26,950 --> 00:01:29,310 
That's a lot of work if your data is big. 
17 
00:01:31,540 --> 00:01:36,510 
Adding YARN in between HDFS and
the applications enabled 
18 
00:01:36,510 --> 00:01:41,510 
new systems to be built, focusing on
different types of big data applications 
19 
00:01:41,510 --> 00:01:46,280 
such as Giraph for
graph data analysis, Storm for 
20 
00:01:46,280 --> 00:01:50,190 
streaming data analysis, and
Spark for in-memory analysis. 
21 
00:01:51,360 --> 00:01:55,150 
YARN does so
by providing a standard framework 
22 
00:01:55,150 --> 00:01:59,530 
that supports customized application
development in the HADOOP ecosystem. 
23 
00:02:00,990 --> 00:02:04,850 
YARN lets you extract maximum
benefits from your data sets 
24 
00:02:04,850 --> 00:02:09,320 
by letting you use the tools you
think are best for your big data. 
25 
00:02:10,860 --> 00:02:14,810 
Let's take a peek into the architecture
of YARN without getting too technical. 
26 
00:02:16,430 --> 00:02:21,620 
In this picture, notice the resource
manager in the center, and 
27 
00:02:21,620 --> 00:02:25,810 
the node managers on each of
the three nodes on the right. 
28 
00:02:27,910 --> 00:02:32,450 
The resource manager controls all
the resources, and decides who gets what. 
29 
00:02:34,290 --> 00:02:39,580 
Node manager operates at machine level and
is in charge of a single machine. 
30 
00:02:41,490 --> 00:02:44,180 
Together the resource manager and 
31 
00:02:44,180 --> 00:02:47,240 
the node manager form the data
computation framework. 
32 
00:02:48,410 --> 00:02:51,530 
Each application gets
an application master. 
33 
00:02:52,560 --> 00:02:56,190 
It negotiates resource from
the Resource Manager and 
34 
00:02:56,190 --> 00:02:59,640 
it talks to Node Manager
to get its tasks completed. 
35 
00:03:01,820 --> 00:03:06,900 
Notice the ovals labeled
Container The container 
36 
00:03:06,900 --> 00:03:12,240 
is an abstract Notions that
signifies a resource that is 
37 
00:03:12,240 --> 00:03:16,760 
a collection of CPU
memory disk network and 
38 
00:03:16,760 --> 00:03:21,680 
other resources within
the compute note to simplify and 
39 
00:03:21,680 --> 00:03:26,020 
be less precise you can think
of a container and the Machine. 
40 
00:03:27,440 --> 00:03:30,460 
We looked at the essential
gears of the YARN 
41 
00:03:30,460 --> 00:03:34,880 
engine to give you an idea of
the key components of YARN. 
42 
00:03:34,880 --> 00:03:38,580 
Now when you hear terms like
Resource Manager, Node Manager and 
43 
00:03:38,580 --> 00:03:43,670 
Container, you will have an understanding
of what tasks they are responsible for. 
44 
00:03:46,150 --> 00:03:52,400 
Here is a real life example to show
the strength Hadoop 2.0 over 1.0. 
45 
00:03:52,400 --> 00:03:55,940 
Yahoo was able to run almost 
46 
00:03:55,940 --> 00:03:59,910 
twice as many jobs per day with
Yarn than with Hadoop 1.0. 
47 
00:03:59,910 --> 00:04:07,100 
They also experienced a substantial
increase in CPU utilization. 
48 
00:04:07,100 --> 00:04:08,130 
Yahoo! 
49 
00:04:08,130 --> 00:04:11,710 
even claimed that upgrading
to YARN was equal into 
50 
00:04:11,710 --> 00:04:15,850 
adding 1000 machines to
their 2500 machine cluster. 
51 
00:04:15,850 --> 00:04:16,480 
That's big. 
52 
00:04:18,450 --> 00:04:22,790 
YARN success is evident
from an explosive growth of 
53 
00:04:22,790 --> 00:04:25,940 
different application that
the Hadoop ecosystem now has. 
54 
00:04:27,330 --> 00:04:28,660 
New to yarn? 
55 
00:04:28,660 --> 00:04:32,750 
You can use the tool of your choice
over your big data without any hassle. 
56 
00:04:33,920 --> 00:04:39,170 
Compare this with Hadoop 1.0 which
was limited to MapReduce alone. 
57 
00:04:41,260 --> 00:04:45,630 
Let's review a summary of
the key take-aways about yarn. 
58 
00:04:45,630 --> 00:04:50,540 
Yarn gives you many ways for
applications to extract value from data. 
59 
00:04:51,720 --> 00:04:56,490 
It lets you run many distributed
applications over the same Hadoop cluster. 
60 
00:04:57,860 --> 00:05:02,630 
In addition, YARN reduces
the need to move data around and 
61 
00:05:02,630 --> 00:05:07,110 
supports higher resource utilization
resulting in lower costs. 
62 
00:05:08,690 --> 00:05:13,310 
It's a scalable platform that has
enabled growth of several applications 
63 
00:05:13,310 --> 00:05:17,490 
over the HDFS,
enriching the Hadoop ecosystem. 
1 
00:00:00,880 --> 00:00:06,100 
At this point, you hopefully have a good
general idea of what big data means and 
2 
00:00:06,100 --> 00:00:07,380 
why big data is important. 
3 
00:00:08,590 --> 00:00:11,190 
So now, we need to focus on what to do 
4 
00:00:11,190 --> 00:00:13,090 
when we have an application
that uses big data. 
5 
00:00:14,980 --> 00:00:18,780 
In this video, we focus on
the problem of managing big data. 
6 
00:00:19,780 --> 00:00:20,970 
So at the end of the video, 
7 
00:00:20,970 --> 00:00:25,390 
you should be able to describe what data
management means in general, and then 
8 
00:00:25,390 --> 00:00:29,180 
specifically recognize the issues that are
involved in the management of big data. 
9 
00:00:31,760 --> 00:00:34,620 
First, let's see what data
management means in general. 
10 
00:00:35,910 --> 00:00:39,120 
Instead of giving you
definitions of data management, 
11 
00:00:39,120 --> 00:00:41,760 
let's think of some questions
that must be asked and 
12 
00:00:41,760 --> 00:00:44,880 
answered well if you're to manage
a reasonable amount of data. 
13 
00:00:46,250 --> 00:00:49,350 
Now, we can not possibly cover
all questions one should ask for 
14 
00:00:49,350 --> 00:00:54,020 
a data-centric application, but here are
some important ones, which range from how 
15 
00:00:54,020 --> 00:00:59,290 
we get the data, to how we work with it,
to how we secure it from malicious users. 
16 
00:01:00,300 --> 00:01:02,386 
We'll visit these issues one at a time. 
1 
00:00:01,280 --> 00:00:03,990 
Ingestion means the process
of getting the data 
2 
00:00:03,990 --> 00:00:07,040 
into the data system that
we are building or using. 
3 
00:00:07,040 --> 00:00:10,440 
Now you might think,
why is it worth talking about? 
4 
00:00:11,600 --> 00:00:14,560 
We'll just read the data from somewhere,
like a file. 
5 
00:00:14,560 --> 00:00:17,320 
And then using some command,
place it into the data system. 
6 
00:00:18,550 --> 00:00:22,180 
Or we'll have have some
kind of a web form or 
7 
00:00:22,180 --> 00:00:25,870 
other visual interface and just fill it
in so that the data goes into the system. 
8 
00:00:27,500 --> 00:00:31,180 
Both of these ways of
data ingestion are valid. 
9 
00:00:31,180 --> 00:00:32,650 
In fact, they're valid for 
10 
00:00:32,650 --> 00:00:35,420 
some big data systems like your
airline reservation system. 
11 
00:00:36,470 --> 00:00:39,850 
However when you think
of a large scale system 
12 
00:00:39,850 --> 00:00:43,990 
you wold like to have more automation
in the data ingestion processes. 
13 
00:00:43,990 --> 00:00:48,060 
And data ingestion then becomes a part of
the big data management infrastructure. 
14 
00:00:49,200 --> 00:00:53,920 
So here are some questions you might want
to ask when you automate data ingestion. 
15 
00:00:55,140 --> 00:00:56,830 
Now take a minute to read the questions. 
16 
00:00:59,140 --> 00:01:03,350 
We'll look at two examples to
explore them in greater detail. 
17 
00:01:04,660 --> 00:01:08,380 
The first example is that of a hospital
information system that we discussed in 
18 
00:01:08,380 --> 00:01:11,600 
course one in the context
of precision medicine. 
19 
00:01:11,600 --> 00:01:15,650 
We said that hospitals collect
terabytes of medical record 
20 
00:01:15,650 --> 00:01:18,780 
from different departments and
be considered big data systems. 
21 
00:01:20,450 --> 00:01:24,630 
The second example is a cloud based data
store where many people upload their 
22 
00:01:24,630 --> 00:01:29,030 
messages, chats, pictures,
videos, music and so fourth. 
23 
00:01:29,030 --> 00:01:33,100 
The cloud storage also supports active
communication between the members and 
24 
00:01:33,100 --> 00:01:34,920 
store their communication in real time. 
25 
00:01:36,650 --> 00:01:41,060 
So let's think of a hypothetical
hospital information information and 
26 
00:01:41,060 --> 00:01:43,970 
the answer to depressions
that we are putting there. 
27 
00:01:43,970 --> 00:01:45,750 
Now, do not take the numbers
to be very accurate. 
28 
00:01:45,750 --> 00:01:46,630 
They are just examples. 
29 
00:01:47,630 --> 00:01:50,330 
But it illustrates some important points. 
30 
00:01:50,330 --> 00:01:56,260 
One, note that there are two kinds
of likeness associated with data. 
31 
00:01:56,260 --> 00:02:00,410 
Some data like medical images
are large data objects by themselves. 
32 
00:02:02,300 --> 00:02:06,880 
Secondly, the records
themselves are quite small but 
33 
00:02:06,880 --> 00:02:09,740 
the size of the total collection
of records is very high. 
34 
00:02:11,580 --> 00:02:16,546 
Two, while there is a lot of patient data,
the number of data sources that is 
35 
00:02:16,546 --> 00:02:21,517 
the different departmental systems
contributing to the total information 
36 
00:02:21,517 --> 00:02:24,408 
system does not change
very much over time. 
37 
00:02:24,408 --> 00:02:29,739 
Three, the rate of data ingestion is
not enormous and is often proportional 
38 
00:02:29,739 --> 00:02:34,668 
to the number of patient activities
that takes place at the hospital. 
39 
00:02:34,668 --> 00:02:37,993 
Four, the system contains
medical records so 
40 
00:02:37,993 --> 00:02:42,778 
data can never be discarded even
when there are errors in the data. 
41 
00:02:42,778 --> 00:02:48,020 
The errors in this specific case
are flagged but the data is retained. 
42 
00:02:49,590 --> 00:02:53,740 
Now this is the kind of rule
called an error handling policy. 
43 
00:02:53,740 --> 00:02:56,120 
Which might be different for
different application problems. 
44 
00:02:57,960 --> 00:03:03,180 
An air handling policy is part of
a larger scheme of policies called 
45 
00:03:03,180 --> 00:03:04,050 
ingestion policies. 
46 
00:03:06,530 --> 00:03:10,630 
Another kind of ingestion policy involves
decisions regarding what the system should 
47 
00:03:10,630 --> 00:03:14,480 
do if the data rate suddenly increases or
becomes suspiciously low. 
48 
00:03:15,540 --> 00:03:19,420 
In this example we have deliberately
decided not to include it in the design. 
49 
00:03:20,630 --> 00:03:24,150 
Now compare the previous case with
the case of the online store of 
50 
00:03:24,150 --> 00:03:26,000 
personal information. 
51 
00:03:26,000 --> 00:03:28,800 
Again this is just an imaginary example. 
52 
00:03:28,800 --> 00:03:31,270 
So don't think of all
the parameters to be exact. 
53 
00:03:32,670 --> 00:03:37,390 
Now in this case one, the store will
have a fast growing membership. 
54 
00:03:38,680 --> 00:03:42,370 
Each member will use multiple devices
to capture and ingest their data. 
55 
00:03:44,060 --> 00:03:50,510 
Two, over all members together, the site
will be updated at a really fast rate, 
56 
00:03:50,510 --> 00:03:56,182 
making this a large volume data
store with a fast ingest rate. 
57 
00:03:56,182 --> 00:04:01,150 
Three, in this system, our primary
challenge is to keep up with the data 
58 
00:04:01,150 --> 00:04:06,780 
rate, and hence, erroneous data will be
discarded after just one edit to reinvest. 
59 
00:04:08,760 --> 00:04:13,380 
Four, now there is an actual policy for
handling data overflow, 
60 
00:04:13,380 --> 00:04:17,720 
which essentially says,
keep the excess data in a site store. 
61 
00:04:17,720 --> 00:04:20,560 
And ingest them when the data
rate becomes slower. 
62 
00:04:20,560 --> 00:04:24,660 
But if the site store starts getting full 
63 
00:04:24,660 --> 00:04:28,980 
start dropping some incoming data
at a rate of 0.1% at a time. 
64 
00:04:30,470 --> 00:04:33,220 
Now we should see why data
ingestion together with it's 
65 
00:04:33,220 --> 00:04:36,320 
policies should be an integral
part of a big data system. 
66 
00:04:36,320 --> 00:04:39,390 
Especially when it involves
storing fast data. 
1 
00:00:00,770 --> 00:00:05,720 
A very significant aspect of data
management is to document, define, 
2 
00:00:05,720 --> 00:00:09,580 
implement, and test the set of
operations that are required for 
3 
00:00:09,580 --> 00:00:10,610 
a specific application. 
4 
00:00:11,910 --> 00:00:16,570 
As we'll see later in the specialization,
some operations are independent of 
5 
00:00:16,570 --> 00:00:21,770 
the type of data and some others would
require us to know the nature of the data 
6 
00:00:21,770 --> 00:00:24,890 
because the operations make use
of a particular data model. 
7 
00:00:24,890 --> 00:00:26,180 
That is the way that it is structured. 
8 
00:00:27,760 --> 00:00:31,130 
In general, there are two
broad divisions of operations. 
9 
00:00:32,620 --> 00:00:35,950 
Those that work on a singular object and 
10 
00:00:35,950 --> 00:00:38,590 
those that work on
collections of data objects. 
11 
00:00:40,020 --> 00:00:44,520 
In the first case,
an operation that crops an image, 
12 
00:00:44,520 --> 00:00:49,120 
that means extracts a sub
area from an area of pixels, 
13 
00:00:49,120 --> 00:00:53,280 
is a single object operation because we
consider the image as a single object. 
14 
00:00:55,250 --> 00:00:58,860 
One can think of many subclasses
of the second category 
15 
00:00:58,860 --> 00:01:01,790 
where the operations
are on data collections. 
16 
00:01:01,790 --> 00:01:06,300 
We briefly referred to three very
common operations that can be done 
17 
00:01:06,300 --> 00:01:07,640 
regardless of the nature of the data. 
18 
00:01:08,820 --> 00:01:13,610 
The first is to take a collection and
filter out a subset of that collection. 
19 
00:01:13,610 --> 00:01:17,180 
The most obvious case is
selecting a subset from a set. 
20 
00:01:17,180 --> 00:01:21,010 
In this example, we select circles
whose number is greater than three. 
21 
00:01:22,050 --> 00:01:26,830 
A second case is merging two collections
together to form a larger collection. 
22 
00:01:28,220 --> 00:01:29,780 
In the example shown, 
23 
00:01:29,780 --> 00:01:35,380 
two three structure data items are merged
by fusing the node with a common property. 
24 
00:01:35,380 --> 00:01:35,880 
That is two. 
25 
00:01:37,520 --> 00:01:41,650 
In the last case,
we compute a function on a collection and 
26 
00:01:41,650 --> 00:01:43,530 
return the value of the function. 
27 
00:01:43,530 --> 00:01:46,120 
So in this example,
the function is a simple count. 
28 
00:01:47,450 --> 00:01:51,230 
In the real world, this kind of aggregate
function can be very complicated. 
29 
00:01:52,540 --> 00:01:57,470 
We will come back to this issue when
we talk more about map readings, but 
30 
00:01:57,470 --> 00:02:00,670 
in this course, we'll talk about
many different data operations. 
31 
00:02:01,750 --> 00:02:04,580 
Every operator must be efficient. 
32 
00:02:04,580 --> 00:02:09,550 
That means every operator must
perform its task as fast as possible 
33 
00:02:09,550 --> 00:02:13,400 
by taking up as little memory,
or our disk, as possible. 
34 
00:02:14,580 --> 00:02:17,170 
Obviously, the time to
perform an operation 
35 
00:02:17,170 --> 00:02:20,490 
will depend on the size of the input and
the size of the output. 
36 
00:02:21,590 --> 00:02:26,300 
So, if there is an opportunity to use
concurrency where the operator can split 
37 
00:02:26,300 --> 00:02:30,880 
its data and have different threads
operate on the pieces at the same time, 
38 
00:02:30,880 --> 00:02:32,080 
it should definitely do so. 
39 
00:02:33,820 --> 00:02:37,120 
We present a simple example of
an operator we saw on the previous slide. 
40 
00:02:38,290 --> 00:02:40,600 
So this operator, called selection, 
41 
00:02:40,600 --> 00:02:45,390 
refers to choosing a subset of
a set based on some conditions. 
42 
00:02:45,390 --> 00:02:49,120 
Here we are choosing a subset of
circles whose numbers are even. 
43 
00:02:50,460 --> 00:02:55,080 
To make it more efficient,
we can take the input data and 
44 
00:02:55,080 --> 00:02:59,150 
partition it randomly into two groups. 
45 
00:02:59,150 --> 00:03:01,040 
Now, for each group, 
46 
00:03:01,040 --> 00:03:05,850 
we can concurrently run the subset
algorithm and get the partial results. 
47 
00:03:07,020 --> 00:03:10,880 
For this operation, the partial results
can be directly sent to the output 
48 
00:03:10,880 --> 00:03:12,880 
without any additional processing step. 
1 
00:00:00,790 --> 00:00:04,740 
Okay we in essence store
the data efficiently. 
2 
00:00:04,740 --> 00:00:05,670 
But is it any good? 
3 
00:00:06,930 --> 00:00:10,970 
Are there ways of knowing if the data is
potentially error free and useful for 
4 
00:00:10,970 --> 00:00:11,770 
the intended purpose? 
5 
00:00:12,940 --> 00:00:15,500 
This is the issue of data quality. 
6 
00:00:15,500 --> 00:00:18,300 
There are many reasons
why any data application, 
7 
00:00:18,300 --> 00:00:22,470 
especially larger applications need
to be mindful of data quality. 
8 
00:00:23,490 --> 00:00:26,650 
Here are three reasons, of course
there are more that we do not mention. 
9 
00:00:27,730 --> 00:00:32,480 
The first reason emphasizes that
the ultimate use of big data 
10 
00:00:32,480 --> 00:00:35,040 
is its ability to give
us actionable insight. 
11 
00:00:36,130 --> 00:00:40,030 
Poor quality data leads to poor
analysis and hence to poor decisions. 
12 
00:00:41,970 --> 00:00:46,820 
The second related data in regulated
industries in areas like clinical 
13 
00:00:46,820 --> 00:00:50,710 
trials for pharmaceutical companies or
financial data like from banks. 
14 
00:00:51,970 --> 00:00:56,440 
Errors in data in these industries
can regulate regulations leading to 
15 
00:00:56,440 --> 00:00:57,290 
legal complications. 
16 
00:00:58,980 --> 00:01:02,230 
The third factor is different
than the first two. 
17 
00:01:02,230 --> 00:01:06,620 
It says if your big data should
be used by other people or 
18 
00:01:06,620 --> 00:01:09,290 
a third party software
it's very important for 
19 
00:01:09,290 --> 00:01:13,059 
the data to give good quality to
gain trust as a leader provider. 
20 
00:01:14,260 --> 00:01:18,270 
A class of big data applications
is scientific, where large, 
21 
00:01:18,270 --> 00:01:22,180 
integrated collections of data
are created by human experts to 
22 
00:01:22,180 --> 00:01:24,570 
understand scientific questions. 
23 
00:01:24,570 --> 00:01:28,960 
Ensuring accuracy of data will lead
to correct human engagement and 
24 
00:01:28,960 --> 00:01:30,220 
interaction with the data system. 
25 
00:01:32,370 --> 00:01:35,170 
Gartner, the well known
technology research and 
26 
00:01:35,170 --> 00:01:39,720 
advising company created a 2015
industry report on big data qualities. 
27 
00:01:41,300 --> 00:01:44,240 
In this report,
they identify the approaches to meeting 
28 
00:01:44,240 --> 00:01:46,210 
the data quality requirements
in the industry. 
29 
00:01:47,240 --> 00:01:51,690 
This methods include the adherence
to standards where applicable. 
30 
00:01:51,690 --> 00:01:55,150 
It also refers to the need to
create the rules in the data system 
31 
00:01:55,150 --> 00:01:59,870 
that can be use to check if the data
passes a set of correct this qualities. 
32 
00:01:59,870 --> 00:02:02,950 
Like is even employed above 18. 
33 
00:02:02,950 --> 00:02:07,490 
It also includes methods to clean
the data if it's found to have errors or 
34 
00:02:07,490 --> 00:02:08,260 
inconsistencies. 
35 
00:02:09,720 --> 00:02:14,772 
Further the data quality management should
include a well define work flow on how 
36 
00:02:14,772 --> 00:02:19,690 
low quality data could be corrected to
bring it back to a high level of quality. 
1 
00:00:01,920 --> 00:00:03,770 
There are many ways of
looking at scalability. 
2 
00:00:04,880 --> 00:00:07,600 
And we'll consider them as
we go forward in the course. 
3 
00:00:08,930 --> 00:00:12,870 
One way is to consider scaling up and
scaling out. 
4 
00:00:14,360 --> 00:00:18,300 
Simply put,
it is a decision between making a machine 
5 
00:00:18,300 --> 00:00:23,264 
that makes a server more powerful
versus adding more machines. 
6 
00:00:23,264 --> 00:00:27,240 
The first choice will
involve adding more memory, 
7 
00:00:27,240 --> 00:00:31,020 
replacing processes with
process of more course and 
8 
00:00:31,020 --> 00:00:35,060 
adding more processes within a system with
a very fast internet connection speed. 
9 
00:00:36,360 --> 00:00:41,520 
The second choice will involve adding more
machines to a relatively slower network. 
10 
00:00:42,880 --> 00:00:44,290 
Now, there are no absolutes here. 
11 
00:00:45,560 --> 00:00:49,490 
In many cases, we'll choose the former
to get more performance for 
12 
00:00:49,490 --> 00:00:50,740 
large leader systems. 
13 
00:00:52,920 --> 00:00:54,760 
The general trend in the big data world, 
14 
00:00:54,760 --> 00:00:57,590 
however, is to target
the scale out option. 
15 
00:00:59,330 --> 00:01:01,950 
Most big data management systems today 
16 
00:01:01,950 --> 00:01:05,140 
are designed to operate over
a cluster up machines and 
17 
00:01:05,140 --> 00:01:10,490 
have the ability to adjust as more
machines are added and when machines fail. 
18 
00:01:11,740 --> 00:01:16,060 
Cluster management and management
of data operations over a cluster 
19 
00:01:16,060 --> 00:01:19,930 
is an important component in today's
big data management systems. 
20 
00:01:22,110 --> 00:01:25,990 
Now we'll briefly touch upon
the complex issue of data security. 
21 
00:01:27,310 --> 00:01:31,700 
It is obvious that having more sensitive
data implies the need for more security. 
22 
00:01:32,960 --> 00:01:35,650 
If the data is within
the walls of an organization, 
23 
00:01:35,650 --> 00:01:38,180 
we'll still need a security plan. 
24 
00:01:38,180 --> 00:01:41,700 
However, if a big data system
is deployed in the cloud, 
25 
00:01:41,700 --> 00:01:45,900 
over multiple machines, security of
data becomes an even bigger challenge. 
26 
00:01:47,200 --> 00:01:50,800 
So now we need to ensure the security for
not only the machines, 
27 
00:01:50,800 --> 00:01:54,880 
but also the network which will be
heavily used during data transfer 
28 
00:01:54,880 --> 00:01:57,610 
across different phases
of data operations. 
29 
00:01:57,610 --> 00:02:03,200 
For example, if the data store and
the data analysis are performed over 
30 
00:02:03,200 --> 00:02:08,760 
different sets of servers that
every analysis operation gets to 
31 
00:02:08,760 --> 00:02:13,630 
an additional overhead of encrypting the
data as the data gets to the network and 
32 
00:02:13,630 --> 00:02:16,030 
decrypting when it gets
to the processing server. 
33 
00:02:17,190 --> 00:02:19,880 
This effectively increases
operational cost. 
34 
00:02:21,310 --> 00:02:26,050 
As of today, while there are many
security products, the methods for 
35 
00:02:26,050 --> 00:02:31,290 
ensuring security and achieving data
processing efficiency at the same time 
36 
00:02:31,290 --> 00:02:33,620 
remains a research issue
in big data management. 
1 
00:00:00,530 --> 00:00:05,150 
Now the goal of a storage infrastructure,
obviously, is to store data. 
2 
00:00:05,150 --> 00:00:07,330 
There are two storage related
issues we consider here. 
3 
00:00:09,590 --> 00:00:12,920 
The first is the issue of capacity. 
4 
00:00:12,920 --> 00:00:15,110 
How much storage should we allocate? 
5 
00:00:15,110 --> 00:00:18,990 
That means, what should be the size
of the memory, how large and 
6 
00:00:18,990 --> 00:00:21,280 
how many disk units should we have,
and so forth. 
7 
00:00:22,700 --> 00:00:26,760 
There is also the issue of scalability. 
8 
00:00:26,760 --> 00:00:30,150 
Should the storage devices be
attached directly to the computers 
9 
00:00:30,150 --> 00:00:33,690 
to make the direct IO fast but
less scalable? 
10 
00:00:33,690 --> 00:00:36,310 
Or should the storage be
attached to the network 
11 
00:00:37,340 --> 00:00:39,820 
that connect the computers in the cluster? 
12 
00:00:39,820 --> 00:00:42,690 
This will make disk
access a bit slower but 
13 
00:00:42,690 --> 00:00:45,210 
allows one to add more
storage to the system easily. 
14 
00:00:46,230 --> 00:00:49,060 
Now these questions do
not have a simple answer. 
15 
00:00:49,060 --> 00:00:52,200 
If you're interested, you may look up
a website given on your reading list. 
16 
00:00:53,470 --> 00:00:58,080 
A different class of questions deals
with the speed of the IU operation. 
17 
00:00:59,090 --> 00:01:03,620 
This question is often addressed with
this kind of diagram here called a memory 
18 
00:01:03,620 --> 00:01:07,480 
hierarchy, or storage hierarchy, or
sometimes memory storage hierarchy. 
19 
00:01:09,270 --> 00:01:13,740 
The top of the pyramid structure shows
a part of memory called cache memory, 
20 
00:01:14,840 --> 00:01:17,410 
that lives inside the CPU and
is very fast. 
21 
00:01:17,410 --> 00:01:21,650 
There are different levels of cache,
called L1, L2, L3, 
22 
00:01:21,650 --> 00:01:27,229 
where L3 is the slowest but
still faster than what we call memory, 
23 
00:01:27,229 --> 00:01:30,110 
shown here in orange near the middle. 
24 
00:01:31,400 --> 00:01:33,990 
The figure shows their speed
in terms of response times. 
25 
00:01:35,070 --> 00:01:38,410 
Notice the memory streamed here
is 65 nanoseconds per access. 
26 
00:01:39,930 --> 00:01:40,892 
In contrast, 
27 
00:01:40,892 --> 00:01:46,030 
the speed of the traditional hard disk
is of the order of 10 milliseconds. 
28 
00:01:47,510 --> 00:01:51,918 
This gap has prompted the design
of many data structures and 
29 
00:01:51,918 --> 00:01:56,768 
algorithms.that use a hard disk but
tries to minimize the cost of 
30 
00:01:56,768 --> 00:02:01,455 
the IO operations between the fast
memory and the slower disk. 
31 
00:02:01,455 --> 00:02:04,543 
But more recently,
a newer kind of storage. 
32 
00:02:07,329 --> 00:02:09,463 
Very similar to the flash drives or 
33 
00:02:09,463 --> 00:02:13,738 
USBs that we regularly use have made
an entry as a new storage medium. 
34 
00:02:13,738 --> 00:02:17,820 
These devices are called SSDs or
Solid State Devices. 
35 
00:02:17,820 --> 00:02:19,780 
They are much faster
than spinning hard disks. 
36 
00:02:20,990 --> 00:02:24,795 
An even newer addition is
the method called NVMe, 
37 
00:02:24,795 --> 00:02:27,787 
NVM stands for non-volatile memory, 
38 
00:02:27,787 --> 00:02:32,600 
that makes data transfer between SSDs and
memory much faster. 
39 
00:02:33,940 --> 00:02:38,317 
What all this means in a big data system
is that now we have the choice of 
40 
00:02:38,317 --> 00:02:43,077 
architecting a storage infrastructure
by choosing how much of each type of 
41 
00:02:43,077 --> 00:02:44,684 
storage we need to have. 
42 
00:02:44,684 --> 00:02:49,842 
In my own research with large amounts
of data, I have found that using SSDs 
43 
00:02:49,842 --> 00:02:55,597 
speed up all look up operations in data by
at least a factor of ten over hard drives. 
44 
00:02:55,597 --> 00:02:59,057 
Of course the flip side of
this is the cost factor. 
45 
00:02:59,057 --> 00:03:03,305 
The components become increasingly more
expensive as we go from the lower layers 
46 
00:03:03,305 --> 00:03:05,187 
of the pyramid to the upper layers. 
47 
00:03:05,187 --> 00:03:08,972 
So ultimately, it becomes an issue
of cost-benefit tradeoff. 
1 
00:00:00,025 --> 00:00:05,173 
[SOUND] Let us consider
a real life application to 
2 
00:00:05,173 --> 00:00:11,360 
demonstrate the utility and
the challenges of big data. 
3 
00:00:11,360 --> 00:00:14,540 
Many industries naturally deal
with large amounts of data. 
4 
00:00:15,540 --> 00:00:19,585 
For our discussion, we consider
an energy company that provides gas and 
5 
00:00:19,585 --> 00:00:22,248 
electricity to its
consumers in an urban area. 
6 
00:00:24,230 --> 00:00:28,354 
In this news report, you can see
that Commonwealth Edison, or Con Ed, 
7 
00:00:28,354 --> 00:00:30,829 
the gas and electric provider of New York, 
8 
00:00:30,829 --> 00:00:35,440 
decided to place smart meters
all through its jurisdictions. 
9 
00:00:35,440 --> 00:00:38,000 
That comes to 4.7 million smart meters. 
10 
00:00:39,540 --> 00:00:45,150 
Now smart meters are smart because aside
from measuring energy consumption, 
11 
00:00:45,150 --> 00:00:48,730 
they have a two way communication
capability between the meter and 
12 
00:00:48,730 --> 00:00:51,800 
the central system at the gas and
electric company. 
13 
00:00:51,800 --> 00:00:56,590 
In other words, they generate real time
data from the meters to be stored and 
14 
00:00:56,590 --> 00:00:58,060 
processed at the central facility. 
15 
00:00:59,710 --> 00:01:00,280 
How much data? 
16 
00:01:01,610 --> 00:01:03,070 
According to this report, 
17 
00:01:03,070 --> 00:01:06,680 
the number of data received at
the center is 1.5 billion per day. 
18 
00:01:06,680 --> 00:01:13,480 
So the system will not only consume
this data, but process it and 
19 
00:01:14,650 --> 00:01:20,750 
produce output at 15-minute intervals,
and sometimes, 5 minute intervals. 
20 
00:01:20,750 --> 00:01:22,270 
Let's do the math. 
21 
00:01:22,270 --> 00:01:24,650 
That comes to ingesting and processing 
22 
00:01:26,620 --> 00:01:30,360 
about 10.5 million data
points per 15 minutes. 
23 
00:01:32,150 --> 00:01:36,830 
So what kind of computation must
take place within these 15 minutes? 
24 
00:01:36,830 --> 00:01:41,760 
Well, one obvious computation is billing,
where one needs to compute who, especially 
25 
00:01:41,760 --> 00:01:46,290 
in the commercial sector, who actually
owns the meter and should be billed? 
26 
00:01:47,490 --> 00:01:49,980 
This requires combining the meter data 
27 
00:01:49,980 --> 00:01:53,640 
with the data in the customer
database maintained by the company. 
28 
00:01:53,640 --> 00:01:57,390 
But let's just consider
computation related to analytics. 
29 
00:01:57,390 --> 00:02:01,180 
We can list at least four
different kinds of computations. 
30 
00:02:02,610 --> 00:02:07,980 
The first is computing the consumption
pattern per user, not per meter. 
31 
00:02:07,980 --> 00:02:12,840 
Per user, where the output is
a histogram of hourly usage. 
32 
00:02:12,840 --> 00:02:16,270 
So the x axis of the histogram
is hourly intervals And 
33 
00:02:16,270 --> 00:02:19,380 
the y-axis is a number of units consumed. 
34 
00:02:20,950 --> 00:02:25,310 
This leads to the computed both daily and
over larger time periods. 
35 
00:02:25,310 --> 00:02:28,560 
To determine the hourly requirements for
this consumer. 
36 
00:02:30,430 --> 00:02:33,970 
The second computation relates
to estimating the effects of 
37 
00:02:33,970 --> 00:02:37,160 
outdoor temperature on the electricity
consumption of each consumer. 
38 
00:02:38,380 --> 00:02:40,890 
For those you who
are statistically inclined, 
39 
00:02:40,890 --> 00:02:45,860 
this often involves fitting a piece-wise
linear progression model to the data. 
40 
00:02:48,000 --> 00:02:53,080 
The third task is to extract the daily
consumption trends that occur 
41 
00:02:53,080 --> 00:02:55,470 
regardless of the outdoor temperature. 
42 
00:02:55,470 --> 00:02:59,590 
This is again a statistical computation,
and may require something like 
43 
00:02:59,590 --> 00:03:03,570 
a periodic alter regression algorithm,
for time series theta. 
44 
00:03:03,570 --> 00:03:05,290 
The algorithm is not that important there. 
45 
00:03:06,410 --> 00:03:11,360 
What's more important is the ability to
make make good prediction has a direct 
46 
00:03:11,360 --> 00:03:15,690 
economic impact because the company
needs to buy energy from others. 
47 
00:03:15,690 --> 00:03:20,280 
For example, an under-prediction
implies they'll end up paying more for 
48 
00:03:20,280 --> 00:03:23,940 
buying energy at the last moment to
meet the consumer's requirements. 
49 
00:03:25,870 --> 00:03:30,590 
The fourth task is to find groups of
similar consumers based on their usage 
50 
00:03:30,590 --> 00:03:34,860 
pattern so that the company can determine
how many distinct groups of customers 
51 
00:03:34,860 --> 00:03:39,330 
there are and design targeted energy
saving campaigns for each group. 
52 
00:03:40,370 --> 00:03:45,730 
This requires finding similarities
over large number of time series data, 
53 
00:03:45,730 --> 00:03:47,140 
which is a complex computation. 
54 
00:03:48,430 --> 00:03:52,110 
Regardless of the number and
complexity of computation required, 
55 
00:03:52,110 --> 00:03:57,220 
the company's constrained by the fact that
it has only 15 minutes to process the data 
56 
00:03:57,220 --> 00:04:02,942 
before the next and
computation has to be performed. 
57 
00:04:02,942 --> 00:04:06,550 
That issue is not just
the bigness of the data, but 
58 
00:04:06,550 --> 00:04:09,610 
the strip and
strings of the arrival to output time. 
59 
00:04:11,520 --> 00:04:18,150 
The analytics has value, only if it can be
completed within the life-cycle deadline. 
60 
00:04:18,150 --> 00:04:22,220 
So if we were to design a big data system
for such a company, you would need to 
61 
00:04:22,220 --> 00:04:26,730 
understand how much are the computation
can be executed in parallel And 
62 
00:04:26,730 --> 00:04:31,180 
how many machines with what kind of
capability are required to handle the data 
63 
00:04:31,180 --> 00:04:35,510 
rate and the number and complexity of
the analytical computations needed? 
1 
00:00:07,370 --> 00:00:08,820 
Hi, my name is Chad Berkley. 
2 
00:00:08,820 --> 00:00:13,750 
I'm the CTO of FlightStats, and I'm here
today to talk to you a little bit about 
3 
00:00:13,750 --> 00:00:17,740 
our platform and
how we acquire and process data. 
4 
00:00:17,740 --> 00:00:21,320 
But first of all, I'd like to start by
just kind of introducing the company and 
5 
00:00:21,320 --> 00:00:26,540 
telling you a little bit
about what we're all about. 
6 
00:00:26,540 --> 00:00:30,940 
So FlightStats is a data company and 
7 
00:00:30,940 --> 00:00:36,340 
we basically are the leading provider
of global real-time flight status data. 
8 
00:00:36,340 --> 00:00:41,110 
We pull in data from over 500 sources and
we aggregate that data back together, and 
9 
00:00:41,110 --> 00:00:46,140 
we sell it out to our customers which our
other businesses as well as consumers. 
10 
00:00:47,290 --> 00:00:50,920 
So just to give you a little bit
of information about the scope and 
11 
00:00:50,920 --> 00:00:52,460 
scale of what we do. 
12 
00:00:52,460 --> 00:00:55,210 
Like I said,
we have over 500 sources of data. 
13 
00:00:55,210 --> 00:01:00,300 
And on a daily basis,
we process about 15 million flight events, 
14 
00:01:00,300 --> 00:01:04,380 
those that includes landings,
arrivals, departures. 
15 
00:01:05,510 --> 00:01:10,600 
Any time the status of the flight changes,
we got some sort of message on that. 
16 
00:01:10,600 --> 00:01:15,790 
We process about 260 million aircraft
positions per day, so we have an extensive 
17 
00:01:15,790 --> 00:01:21,980 
network that monitors graph positions for
realtime flight tracking applications. 
18 
00:01:21,980 --> 00:01:26,020 
And we also handle about One million PNRs,
or passenger name records, 
19 
00:01:26,020 --> 00:01:31,590 
which are the actual data type
of an itenerary any time you 
20 
00:01:31,590 --> 00:01:36,520 
book travel, a PNR is created for
you, for your travel. 
21 
00:01:36,520 --> 00:01:41,770 
And it includes all of the segments,
like air travel, ferries, 
22 
00:01:41,770 --> 00:01:46,110 
hotels, taxis, Anyhting that
can be scheduled on your trip. 
23 
00:01:47,390 --> 00:01:49,650 
And we basically take in all that data and 
24 
00:01:49,650 --> 00:01:54,080 
we aggregate it together and
we sell it back out. 
25 
00:01:54,080 --> 00:01:57,930 
And how most people kind of know us for
FlightStats.com, that's our 
26 
00:01:57,930 --> 00:02:02,260 
consumer site for business where we
handle about 2 million daily requests. 
27 
00:02:02,260 --> 00:02:04,550 
And we handle about 1
million mobile app requests. 
28 
00:02:05,790 --> 00:02:12,710 
That our b to b side, we cert out a lot
of data by APIs and real time data feeds. 
29 
00:02:12,710 --> 00:02:16,480 
People make about 15 million
API requests to us everyday. 
30 
00:02:16,480 --> 00:02:20,180 
And we also send out about one and half
million flight and trip notifications. 
31 
00:02:20,180 --> 00:02:24,310 
So if you get a push
notification to your phone, 
32 
00:02:24,310 --> 00:02:26,930 
telling you that your flight is delayed or
on time. 
33 
00:02:26,930 --> 00:02:29,010 
That possibly has come from us. 
34 
00:02:29,010 --> 00:02:34,530 
So a little bit about how the data
flows through our company. 
35 
00:02:34,530 --> 00:02:37,820 
We bring in all these
different types of data and 
36 
00:02:37,820 --> 00:02:41,260 
our sources and it flows through
our data acquisition team. 
37 
00:02:41,260 --> 00:02:47,380 
We have a team whose primary purpose
is to pull in all sorts of raw data, 
38 
00:02:47,380 --> 00:02:49,520 
a very heterogeneous datasets. 
39 
00:02:49,520 --> 00:02:55,150 
And process that into a normalized form. 
40 
00:02:55,150 --> 00:02:58,420 
So if you kind of follow the blue arrow
in this diagram you can see that it goes 
41 
00:02:58,420 --> 00:03:00,840 
through this raw data
channel through the data hub. 
42 
00:03:00,840 --> 00:03:03,660 
Which the data hub is a central
component of our system that I'll talk 
43 
00:03:03,660 --> 00:03:06,000 
a little bit more about in a second. 
44 
00:03:06,000 --> 00:03:09,960 
So the blue data, the blue line is
raw data coming in from the source 
45 
00:03:09,960 --> 00:03:12,300 
It goes through our data
acquisition system, 
46 
00:03:12,300 --> 00:03:16,360 
it turns into that purple line
which is a normalized form. 
47 
00:03:16,360 --> 00:03:20,640 
It then goes back through our data hub
again and into our processing engine. 
48 
00:03:20,640 --> 00:03:24,210 
Our processing engine is really where
most of the business logic happens. 
49 
00:03:24,210 --> 00:03:27,210 
The first thing we have to do
is we have to match any piece of 
50 
00:03:27,210 --> 00:03:30,390 
flight information against
a flight that we know about and 
51 
00:03:30,390 --> 00:03:33,220 
primarily the way we know about
flights is through schedules. 
52 
00:03:33,220 --> 00:03:38,560 
So we import schedules on a daily
basis from one of our partner 
53 
00:03:38,560 --> 00:03:39,510 
schedule providers. 
54 
00:03:40,750 --> 00:03:45,065 
That data,
once it's matched is then processed and 
55 
00:03:45,065 --> 00:03:49,635 
the processing basically looks at each
message, and tries to determine if 
56 
00:03:49,635 --> 00:03:52,825 
we think that that message needs
to be passed on to consumers. 
57 
00:03:52,825 --> 00:03:56,940 
So you know it looks at things like,
have we seen that message before. 
58 
00:03:56,940 --> 00:03:58,560 
Or is it a duplicate? 
59 
00:03:58,560 --> 00:04:00,530 
Is it from a data source that we trust? 
60 
00:04:01,530 --> 00:04:05,120 
Are there other things going on
that we need to know about that 
61 
00:04:05,120 --> 00:04:08,300 
may impact whether that message is true or
not? 
62 
00:04:08,300 --> 00:04:11,370 
And once we decide that a message
should be passed through, 
63 
00:04:11,370 --> 00:04:13,020 
if you follow the green line. 
64 
00:04:13,020 --> 00:04:15,860 
It goes into our process
data channel on our hub, and 
65 
00:04:15,860 --> 00:04:18,400 
it's then pushed out to
a couple different places. 
66 
00:04:18,400 --> 00:04:19,080 
So first of all, 
67 
00:04:19,080 --> 00:04:23,510 
it goes into our production database which
is where all of our real time data lives. 
68 
00:04:23,510 --> 00:04:27,750 
That database serves data to our websites, 
69 
00:04:27,750 --> 00:04:32,230 
to our mobile apps, and
a variety of other places. 
70 
00:04:32,230 --> 00:04:34,060 
It also goes into our
data warehouse which, 
71 
00:04:34,060 --> 00:04:37,330 
is where our analytics products use it. 
72 
00:04:37,330 --> 00:04:39,670 
I'll talk a bit more
about that in a minute. 
73 
00:04:39,670 --> 00:04:43,910 
And then, the stream of data actually
goes up to so many of our customers. 
74 
00:04:43,910 --> 00:04:45,490 
We don't need a database on our side. 
75 
00:04:45,490 --> 00:04:47,650 
They would rather build
a database on their side so 
76 
00:04:47,650 --> 00:04:51,490 
we actually just stream all of
the processed data directly to them. 
77 
00:04:51,490 --> 00:04:54,930 
They then host it within
their own systems. 
78 
00:04:57,600 --> 00:05:04,920 
So a little bit more about the hub, the
hub is central to how we move data around. 
79 
00:05:04,920 --> 00:05:07,960 
It's a technology that
we developed in-house. 
80 
00:05:07,960 --> 00:05:11,125 
And it's an object storage based,
scalable, highly available, 
81 
00:05:11,125 --> 00:05:15,810 
multi-channel data queuing and
eventing system. 
82 
00:05:15,810 --> 00:05:21,690 
The object storage part is,
we use Amazon S3 to store this data. 
83 
00:05:21,690 --> 00:05:24,310 
So it's an object storage system. 
84 
00:05:24,310 --> 00:05:26,600 
It's scalable,
we can scale it horizontally or 
85 
00:05:26,600 --> 00:05:32,140 
vertically depending on, but the what
type of data is flowing through it. 
86 
00:05:32,140 --> 00:05:33,250 
It's highly available meaning, 
87 
00:05:33,250 --> 00:05:36,350 
that we have multiple instances
of it in different data centers. 
88 
00:05:36,350 --> 00:05:39,410 
So if one that goes down we can
easily pull another one up or 
89 
00:05:39,410 --> 00:05:42,490 
it's we're going to cross
multiple instances. 
90 
00:05:42,490 --> 00:05:44,600 
And then it's multi channel. 
91 
00:05:44,600 --> 00:05:49,030 
So it's got a rest interface and
any surface can create 
92 
00:05:49,030 --> 00:05:53,760 
a new channel within the system and
start posting data to it. 
93 
00:05:53,760 --> 00:05:58,260 
That data is then queued based on
the time that it comes in, and 
94 
00:05:58,260 --> 00:06:01,490 
other services can be listening for
events on those channels. 
95 
00:06:01,490 --> 00:06:05,240 
So, as soon as a new piece of data
comes into one of those channels, 
96 
00:06:05,240 --> 00:06:09,330 
any service that's listening on that
channel, gets an event notification. 
97 
00:06:09,330 --> 00:06:12,470 
They, that service can then
act upon that piece of data. 
98 
00:06:12,470 --> 00:06:16,700 
And do whatever processing
it may need to do. 
99 
00:06:16,700 --> 00:06:21,690 
This project is open source and
anybody can download it and use it. 
100 
00:06:24,490 --> 00:06:28,230 
So a little bit about some of the data
that we collect and aggregate. 
101 
00:06:28,230 --> 00:06:33,560 
And FLIFO is kind of the industry term for
flight information. 
102 
00:06:33,560 --> 00:06:39,360 
And primarily we look at kind of
the five different parts of flight. 
103 
00:06:39,360 --> 00:06:43,870 
So we pull in information on gate
departure, and then that becomes a runway 
104 
00:06:43,870 --> 00:06:47,855 
departure, basically when the wheels
go up, that is a runway departure. 
105 
00:06:47,855 --> 00:06:54,080 
We do in-flight positional tracking,
so when your flight is moving along, 
106 
00:06:54,080 --> 00:06:58,330 
about once every ten seconds we get
notified of its latitude and longitude. 
107 
00:06:58,330 --> 00:07:00,530 
And its heading and its speed and 
108 
00:07:00,530 --> 00:07:04,790 
its vertical center descent rate and
several other variables. 
109 
00:07:05,960 --> 00:07:07,230 
Then once it lands, 
110 
00:07:07,230 --> 00:07:11,300 
as soon as the wheels touch the ground,
we're notified of a runway arrival and 
111 
00:07:11,300 --> 00:07:15,890 
when the door is opened at the gate,
we have gate arrival information. 
112 
00:07:15,890 --> 00:07:21,480 
All five of these data fields
come in three different forms. 
113 
00:07:21,480 --> 00:07:24,460 
So we have a scheduled,
scheduled departure and arrival. 
114 
00:07:24,460 --> 00:07:29,310 
We have estimated departure and arrival
which can come from a variety of sources, 
115 
00:07:29,310 --> 00:07:33,710 
either airlines, airports,
positional data, et cetera. 
116 
00:07:33,710 --> 00:07:38,120 
And then, we have actuals so
if we have an airport or 
117 
00:07:38,120 --> 00:07:42,460 
an airline that's sending us data about
exactly when the wheels touch down, or 
118 
00:07:42,460 --> 00:07:46,770 
exactly when that door opens on that
aircraft, we push that data as well. 
119 
00:07:46,770 --> 00:07:51,100 
We also generate some
data at flight stops. 
120 
00:07:51,100 --> 00:07:56,850 
So special incidents, if an aircraft
has an issue It's in the news. 
121 
00:07:56,850 --> 00:08:01,920 
We do flag our content with
a message from our support staff. 
122 
00:08:01,920 --> 00:08:04,790 
We do some prediction. 
123 
00:08:04,790 --> 00:08:08,030 
Right now we're just starting to get into
that market, or we're actually trying to 
124 
00:08:08,030 --> 00:08:13,490 
predict 24 hours out whether a flight
will be delayed, disrupted or on time. 
125 
00:08:14,500 --> 00:08:19,550 
We do some synthetic positions,
so over oceans, primarily. 
126 
00:08:19,550 --> 00:08:24,310 
We don't get tracking data
on aircraft over the oceans. 
127 
00:08:24,310 --> 00:08:28,480 
There is currently no satellite-based
tracking system for aircraft. 
128 
00:08:28,480 --> 00:08:34,010 
So we basically take the last
known position a heading, a speed. 
129 
00:08:34,010 --> 00:08:38,330 
And if we have a flight plan, we'll use
a flight plan to synthesize the positions 
130 
00:08:38,330 --> 00:08:42,140 
when we're not getting actual
positions over large bodies of water. 
131 
00:08:43,320 --> 00:08:47,560 
We also generate notifications,
so the push alerts, 
132 
00:08:47,560 --> 00:08:52,680 
the preflight emails,
delay notifications those types of things. 
133 
00:08:52,680 --> 00:08:56,570 
We create those based on what we see
in the data that's coming in to us. 
134 
00:08:58,970 --> 00:09:02,630 
We store all of our historical
data in a data warehouse, 
135 
00:09:02,630 --> 00:09:05,860 
so right now we have six years
of historical flight data. 
136 
00:09:05,860 --> 00:09:08,080 
And that powers our analytics products, 
137 
00:09:08,080 --> 00:09:12,570 
so we allow airlines to do competitive
analysis, and route analysis. 
138 
00:09:13,690 --> 00:09:15,320 
Routes are very important to airlines. 
139 
00:09:15,320 --> 00:09:17,220 
That's how they compete
with each other and 
140 
00:09:17,220 --> 00:09:22,800 
that's primarily how they
are judged by the FAA and 
141 
00:09:22,800 --> 00:09:27,429 
other governmental organizations
on whether they're on-time or not. 
142 
00:09:27,429 --> 00:09:32,070 
We also do airport operations analysis
things like taxi in and taxi out times. 
143 
00:09:32,070 --> 00:09:35,910 
Very important for lots of airports,
runway utilization, 
144 
00:09:35,910 --> 00:09:40,230 
hourly passenger flows through airports,
that type of information. 
145 
00:09:40,230 --> 00:09:42,830 
And we do on-time performance metrics so. 
146 
00:09:42,830 --> 00:09:45,290 
Airlines can look at how they're doing. 
147 
00:09:45,290 --> 00:09:47,020 
How many flights did they complete? 
148 
00:09:47,020 --> 00:09:49,870 
How many flights were on
time within 14 minutes? 
149 
00:09:51,640 --> 00:09:54,630 
And they can compare themselves
to their competitors. 
150 
00:09:56,890 --> 00:10:01,310 
So we host all of this in
a hybrid cloud architecture. 
151 
00:10:01,310 --> 00:10:05,560 
Hybrid cloud basically means that we have
our own private datacenter resources and 
152 
00:10:05,560 --> 00:10:10,120 
we also host resources in
the Amazon Web Services cloud. 
153 
00:10:11,870 --> 00:10:15,680 
Most of our core data processing and
service layer is in our private data 
154 
00:10:15,680 --> 00:10:19,250 
center and we're getting ready to spin
up a second private data center as well. 
155 
00:10:19,250 --> 00:10:24,270 
Right now, our main data center is in
Portland, Oregon, and we're going to spin 
156 
00:10:24,270 --> 00:10:29,499 
another one up on the east coast of
the United States probably in Q2 or Q3. 
157 
00:10:31,210 --> 00:10:35,050 
For our API's we try to keep
those close to our customers so 
158 
00:10:35,050 --> 00:10:38,810 
API end points and
web end points live in Amazon. 
159 
00:10:38,810 --> 00:10:44,170 
And they are automatically routed to
whichever end point is closest to you, 
160 
00:10:44,170 --> 00:10:45,640 
you will automatically be routed to them. 
161 
00:10:46,720 --> 00:10:50,700 
All of our private infrastructure
is virtualized with VMWare. 
162 
00:10:50,700 --> 00:10:54,560 
We pretty much have a fully
virtualized environment. 
163 
00:10:57,530 --> 00:11:04,870 
And we're an Agile shop, so
we have six small, fast teams. 
164 
00:11:04,870 --> 00:11:09,300 
Those are product centric teams, we
allow them to be as customer interactive 
165 
00:11:09,300 --> 00:11:12,610 
as they need to be, and
we try to make our teams semi autonomous. 
166 
00:11:12,610 --> 00:11:16,300 
So, teams get to choose their own tools,
they get to choose their 
167 
00:11:16,300 --> 00:11:21,340 
own development methodologies,
they choose a variety of things. 
168 
00:11:21,340 --> 00:11:24,020 
And We physically allow them 
169 
00:11:24,020 --> 00:11:26,770 
to do what they need to do to get
their job done as quickly as possible. 
170 
00:11:28,030 --> 00:11:30,010 
We try to automate everything. 
171 
00:11:30,010 --> 00:11:33,120 
You do something once manually and
then the next time you write a script or 
172 
00:11:33,120 --> 00:11:34,480 
program to do it. 
173 
00:11:34,480 --> 00:11:35,700 
And we also measure everything. 
174 
00:11:35,700 --> 00:11:39,190 
Right now, we're taking in about
2.5 billion metrics per month off 
175 
00:11:39,190 --> 00:11:40,480 
of our systems. 
176 
00:11:40,480 --> 00:11:44,680 
And we use those metrics to monitor
our application performance, 
177 
00:11:44,680 --> 00:11:48,890 
to monitor revenue, to monitor pretty
much everything we do in the company. 
178 
00:11:48,890 --> 00:11:51,390 
We really try to enable
total system awareness, 
179 
00:11:51,390 --> 00:11:56,200 
everything from the hardware layer
up to the website is monitored. 
180 
00:11:57,240 --> 00:12:01,190 
And we use industry best practices and
tools and of course, we try to recruit and 
181 
00:12:01,190 --> 00:12:05,740 
hire the best talent possible a little
bit about our software stock. 
182 
00:12:05,740 --> 00:12:07,140 
We're primarily a Java shop. 
183 
00:12:08,220 --> 00:12:11,830 
Our core processing services
are all written in Java. 
184 
00:12:11,830 --> 00:12:15,870 
We do use node JS in our
Microservice Edge layer. 
185 
00:12:15,870 --> 00:12:20,090 
And node JS is actually starting to move
more down into the processing service 
186 
00:12:20,090 --> 00:12:21,750 
layer as well. 
187 
00:12:21,750 --> 00:12:23,760 
We use many different types of data bases. 
188 
00:12:23,760 --> 00:12:27,220 
Our primary realtime
database is Post Press. 
189 
00:12:27,220 --> 00:12:31,140 
And we use Mongo for
the backend of our API services. 
190 
00:12:32,280 --> 00:12:36,940 
On the website, we're all HTML5 and
we're moving to React and Redux, and 
191 
00:12:36,940 --> 00:12:41,750 
we're making use of Elasticsearch for
quick searching and indexing on our data. 
192 
00:12:41,750 --> 00:12:45,750 
And, of course, we have iOS and
Android mobile applications. 
193 
00:12:45,750 --> 00:12:51,610 
So you can find out more about
Flightstats on our website. 
194 
00:12:51,610 --> 00:12:53,950 
If you need data for your applications, 
195 
00:12:53,950 --> 00:12:57,860 
please go to the developer center
at developer.flightstats.com. 
196 
00:12:57,860 --> 00:13:00,790 
You can sign up for
a free test account and 
197 
00:13:00,790 --> 00:13:03,980 
be able to pull data
directly off of our APIs. 
198 
00:13:03,980 --> 00:13:06,660 
If you're interested in the Hub,
like I said that's open source. 
199 
00:13:06,660 --> 00:13:10,540 
Please check out the Git Hub page and
if you have any additional questions, 
200 
00:13:10,540 --> 00:13:14,270 
feel free to contact
myself John Berkeley and 
201 
00:13:14,270 --> 00:13:18,360 
I'd be happy to answer any
of your questions via email. 
202 
00:13:18,360 --> 00:13:20,690 
Thanks for listening today and
hope you have a great day. 
203 
00:13:20,690 --> 00:13:21,190 
Bye. 
1 
00:00:10,512 --> 00:00:14,240 
Different data sources in the game
industry include using your finger. 
2 
00:00:14,240 --> 00:00:16,408 
What type of device is coming from? 
3 
00:00:16,408 --> 00:00:18,600 
The amount of headsets. 
4 
00:00:18,600 --> 00:00:19,953 
It's pretty much infinite, 
5 
00:00:19,953 --> 00:00:22,720 
as far as the number of ways we
can bring data in from the game. 
6 
00:00:22,720 --> 00:00:27,602 
And it could be joystick or mouse,
keyboards, there's lots of ways 
7 
00:00:27,602 --> 00:00:31,726 
as well as what happens inside
the game itself as far cars or 
8 
00:00:31,726 --> 00:00:35,537 
the driving, tires,
flying machines, anything. 
9 
00:00:42,179 --> 00:00:45,479 
The volume of data,
it really depends on the type of game and 
10 
00:00:45,479 --> 00:00:47,592 
how often they want to send the data in. 
11 
00:00:47,592 --> 00:00:51,260 
How many types of events they've tagged
and how many users are playing the game. 
12 
00:00:51,260 --> 00:00:54,891 
So if you have a user, you've 5 million
users that are playing your game and 
13 
00:00:54,891 --> 00:00:57,844 
you're tapping and
you're tracking each tap of the screen, 
14 
00:00:57,844 --> 00:01:00,963 
of where they went or each click of
the mouse as they were using it, 
15 
00:01:00,963 --> 00:01:02,982 
you're going to get
a lot of volume of data. 
16 
00:01:02,982 --> 00:01:07,854 
And so
you need to be prepared to bring in a lot 
17 
00:01:07,854 --> 00:01:13,140 
of different data very, very quickly. 
18 
00:01:13,140 --> 00:01:16,322 
As far as the variety of data, it depends. 
19 
00:01:16,322 --> 00:01:18,520 
You have round pizzas and
you have round tires. 
20 
00:01:18,520 --> 00:01:19,513 
I mean, they're completely different. 
21 
00:01:19,513 --> 00:01:23,816 
They're both round, but there's different
ways that you're going to want to know how 
22 
00:01:23,816 --> 00:01:27,104 
many pepperoni are on one pizza and
how many lug nuts go on a tire. 
23 
00:01:27,104 --> 00:01:29,510 
So the variety is really unlimited,
as well. 
24 
00:01:29,510 --> 00:01:31,122 
It really just depends on each game. 
25 
00:01:31,122 --> 00:01:34,430 
So, you have to be prepared to
bring in all kinds of data. 
26 
00:01:34,430 --> 00:01:38,851 
Touch data, wheel data,
track speeds, anything. 
27 
00:01:38,851 --> 00:01:43,628 
And if you put taxonomy together that
you can define as an example of verb, 
28 
00:01:43,628 --> 00:01:49,021 
object, location, value and any number
of other sources, then you can basically 
29 
00:01:49,021 --> 00:01:54,290 
track anything you want as long as they
all fall into the same kind of buckets. 
30 
00:01:54,290 --> 00:01:56,705 
And the buckets can be different
sizes based on the type of events. 
31 
00:01:56,705 --> 00:01:59,804 
They don't all need to be 4,
some can be 2, some can be 20, 
32 
00:01:59,804 --> 00:02:01,218 
it doesn't really matter. 
33 
00:02:08,280 --> 00:02:12,384 
The modeling challenge has really come
down to who designs the structure at which 
34 
00:02:12,384 --> 00:02:15,355 
you store your data and
how you want to retrieve that data. 
35 
00:02:15,355 --> 00:02:19,488 
Those kind of of storage and
retrieval models are very, very important, 
36 
00:02:19,488 --> 00:02:22,098 
because what it really
comes down to is speed. 
37 
00:02:22,098 --> 00:02:25,119 
You can record a lot of data and
it can take you five years to query it, 
38 
00:02:25,119 --> 00:02:26,710 
it doesn't really do you any good. 
39 
00:02:26,710 --> 00:02:31,347 
So you need to make sure you plan for
reporting speed, because that's ultimately 
40 
00:02:31,347 --> 00:02:35,865 
what within the organisation needs is
the ability to report on it very quickly. 
41 
00:02:35,865 --> 00:02:40,781 
The management challenges really come
down to trying to figure out what data to 
42 
00:02:40,781 --> 00:02:41,306 
store. 
43 
00:02:41,306 --> 00:02:44,657 
A lot of times we go into various
companies and you've got producers sitting 
44 
00:02:44,657 --> 00:02:47,810 
across the hall from designers, and
they don't even know each other. 
45 
00:02:47,810 --> 00:02:50,823 
They don't realize what they want and
the programmer says, 
46 
00:02:50,823 --> 00:02:54,606 
I'm going to put these events in and the
product manager who wants to figure out 
47 
00:02:54,606 --> 00:02:58,118 
how many times somebody crashed says,
well, I need these events in. 
48 
00:02:58,118 --> 00:03:02,436 
So unless they're communicating, you're
going to get the wrong type of data. 
49 
00:03:02,436 --> 00:03:06,241 
So the management challenge is trying
to make sure everybody communicates, 
50 
00:03:06,241 --> 00:03:08,639 
they decide on the taxonomy and
the structure and 
51 
00:03:08,639 --> 00:03:12,350 
then we can go forward with tagging and
getting in the entire game working. 
52 
00:03:18,226 --> 00:03:20,469 
We process, stayed in two main ways. 
53 
00:03:20,469 --> 00:03:22,107 
One is streaming data. 
54 
00:03:22,107 --> 00:03:24,667 
One is batched or scheduled data. 
55 
00:03:24,667 --> 00:03:28,653 
Streaming data has scripts that run
instantly the minute the data arrives. 
56 
00:03:28,653 --> 00:03:31,222 
And so as the data come in,
it gets processed and 
57 
00:03:31,222 --> 00:03:33,160 
then stored In a reporting format. 
58 
00:03:33,160 --> 00:03:39,347 
So they can easily generate reports
up to the second very, very quickly. 
59 
00:03:39,347 --> 00:03:42,671 
Batch processing data really depends on
the type of data where it's coming from. 
60 
00:03:42,671 --> 00:03:46,689 
Most of the time when we download
data from iTunes or YouTube or 
61 
00:03:46,689 --> 00:03:50,885 
something like that, it comes in a CSV or
a very similar format. 
62 
00:03:50,885 --> 00:03:53,038 
There's not really a lot of
processing we need to do. 
63 
00:03:53,038 --> 00:03:55,451 
It's more of ingesting that data. 
64 
00:03:55,451 --> 00:03:59,788 
There are processes we need to run
with some type of batch data, but 
65 
00:03:59,788 --> 00:04:03,040 
most of the batch data we
receive comes in as a CSV or 
66 
00:04:03,040 --> 00:04:05,914 
other similar already processed formats. 
67 
00:04:05,914 --> 00:04:10,228 
So typically, while you can do processing
in both modes most processing typically 
68 
00:04:10,228 --> 00:04:14,497 
happens with the streaming real-time data
than it does with offline batch data. 
69 
00:04:20,394 --> 00:04:26,260 
We actually didn't use any
technology in the a big data space. 
70 
00:04:26,260 --> 00:04:28,684 
We created our own, from scratch. 
71 
00:04:28,684 --> 00:04:32,280 
What we did was we decided
what kind of model we wanted. 
72 
00:04:32,280 --> 00:04:33,475 
How we were going to store data? 
73 
00:04:33,475 --> 00:04:35,112 
How we were going to retrieve data? 
74 
00:04:35,112 --> 00:04:37,574 
And ultimately,
how we were going to reduce the data? 
75 
00:04:37,574 --> 00:04:40,772 
Because the more and more and more data
you have, the slower it is to actually do 
76 
00:04:40,772 --> 00:04:43,692 
a query, because you have to look through
all the different pieces of data. 
77 
00:04:43,692 --> 00:04:47,148 
A lot of databases solve this problem for
you, but they were really doing it in more 
78 
00:04:47,148 --> 00:04:50,170 
generic way were we needed
something very specific. 
79 
00:04:50,170 --> 00:04:55,020 
So we started from scratch building our
own data storage and retrieval, and 
80 
00:04:55,020 --> 00:04:57,003 
reporting from the ground up. 
81 
00:04:57,003 --> 00:05:01,097 
When it came to scalability,
it was really about designing the parts of 
82 
00:05:01,097 --> 00:05:03,906 
the system that could be
independently scaled. 
83 
00:05:03,906 --> 00:05:07,773 
So if data's coming in in real-time,
we send that into what we call a gateway. 
84 
00:05:07,773 --> 00:05:10,155 
And the gateway could be three gateways,
let's say. 
85 
00:05:10,155 --> 00:05:11,922 
But if the data starts
getting over loaded, 
86 
00:05:11,922 --> 00:05:13,362 
I just have to add another gateway. 
87 
00:05:13,362 --> 00:05:16,383 
And a gateway is just this little
light layer that just receives data, 
88 
00:05:16,383 --> 00:05:19,605 
passes it on and goes back to it's job,
doesn't do anything else. 
89 
00:05:19,605 --> 00:05:22,296 
So it can receive a lot
of data very quickly, but 
90 
00:05:22,296 --> 00:05:24,124 
also I can just add another one. 
91 
00:05:24,124 --> 00:05:27,309 
And it just automatically logs in and
adds itself to a list and 
92 
00:05:27,309 --> 00:05:30,813 
now the data is being distributed
amongst four gateways instead. 
93 
00:05:30,813 --> 00:05:32,043 
Query engine is the same way. 
94 
00:05:32,043 --> 00:05:34,720 
When you're doing queries to try
to get data out of the system, so 
95 
00:05:34,720 --> 00:05:35,704 
you can build reports. 
96 
00:05:35,704 --> 00:05:39,151 
If I need more query engines,
because people are doing more reporting, 
97 
00:05:39,151 --> 00:05:40,733 
we can add more query engines. 
98 
00:05:40,733 --> 00:05:44,722 
So, the idea behind scalability is
trying to break the services up into 
99 
00:05:44,722 --> 00:05:47,390 
the type of services that
they most make sense. 
100 
00:05:47,390 --> 00:05:48,466 
So that if you need to, 
101 
00:05:48,466 --> 00:05:51,820 
you can add just the service without
rebuilding the entire platform. 
102 
00:05:58,367 --> 00:06:01,825 
My advice for people designing systems for
big data is to first, 
103 
00:06:01,825 --> 00:06:04,398 
try to understand what
you want to accomplish. 
104 
00:06:04,398 --> 00:06:05,763 
What's the goal? 
105 
00:06:05,763 --> 00:06:08,695 
I mean, we're going to ingest everything
and we're going to report on everything. 
106 
00:06:08,695 --> 00:06:15,518 
It's not really something that you can
achieve without some special thought. 
107 
00:06:15,518 --> 00:06:18,790 
If you're going to focus on,
your area's going to be say gardening, 
108 
00:06:18,790 --> 00:06:22,419 
then look at what kind of things you're
going to do in the gardening area and 
109 
00:06:22,419 --> 00:06:24,880 
try to focus on what that
type of data is going to be. 
110 
00:06:24,880 --> 00:06:27,722 
This isn't going to restrict you
to only being a gardener, but 
111 
00:06:27,722 --> 00:06:30,507 
it is going to give you focus
on how to design your system, so 
112 
00:06:30,507 --> 00:06:32,701 
that they're actually going to work for
you. 
113 
00:06:32,701 --> 00:06:36,912 
You're going to continually evolve your
systems, add more things to them and 
114 
00:06:36,912 --> 00:06:37,580 
grow them. 
115 
00:06:37,580 --> 00:06:40,804 
I wouldn't suggest starting with
an unlimited variety of options and 
116 
00:06:40,804 --> 00:06:42,944 
hoping you're going to
solve all the problems. 
117 
00:06:42,944 --> 00:06:49,185 
Start with the goal of what your current
solution is and expand from there. 
118 
00:06:55,600 --> 00:06:58,971 
Data can be fun, but
it can also be overwhelming. 
119 
00:06:58,971 --> 00:07:04,051 
So, try to keep the data in mind
without keeping the world in mind and 
120 
00:07:04,051 --> 00:07:06,291 
I think you'll be just fine. 
1 
00:00:01,860 --> 00:00:06,740 
In this video we will provide a quick
summary of the main points from our 
2 
00:00:06,740 --> 00:00:08,960 
first course on introduction to big data. 
3 
00:00:10,310 --> 00:00:12,930 
If you have just completed
our first course and 
4 
00:00:12,930 --> 00:00:16,810 
do not need a refresher,
you may now skip to the next lecture. 
5 
00:00:18,480 --> 00:00:23,000 
After this video,
you will be able to recall 
6 
00:00:23,000 --> 00:00:27,480 
what started the big data era and
the three main big data sources. 
7 
00:00:29,310 --> 00:00:32,830 
Summarize the volume,
variety, velocity and 
8 
00:00:32,830 --> 00:00:35,350 
veracity issues related to each source. 
9 
00:00:36,790 --> 00:00:42,140 
Explain the five step data science
process to gain value from big data. 
10 
00:00:43,510 --> 00:00:46,680 
Remember the main elements
of the Hadoop Stack. 
11 
00:00:49,670 --> 00:00:54,950 
We began our first course with
an explanation of how a lead torrent of 
12 
00:00:54,950 --> 00:01:00,690 
big data combined with cloud computing
capabilities to process data anytime and 
13 
00:01:00,690 --> 00:01:05,450 
anywhere has been at the core of
the launch of the Big Data Era. 
14 
00:01:08,470 --> 00:01:13,240 
This big torrent of big data is often
boil down to a few varieties of data 
15 
00:01:13,240 --> 00:01:18,282 
generated by machines, people and 
16 
00:01:18,282 --> 00:01:23,810 
organizations with machine generated data. 
17 
00:01:23,810 --> 00:01:28,650 
We refer to the data generated from real
time sensors and industrial machinery or 
18 
00:01:28,650 --> 00:01:30,020 
vehicles. 
19 
00:01:30,020 --> 00:01:33,350 
Web logs that track user behavior online. 
20 
00:01:33,350 --> 00:01:35,470 
environmental sensors, 
21 
00:01:35,470 --> 00:01:38,680 
personal health trackers among
many other sense data sources. 
22 
00:01:40,220 --> 00:01:46,050 
With human generated data, we really refer
to the vast amount of social media data, 
23 
00:01:46,050 --> 00:01:49,760 
status updates, tweets, photos and videos. 
24 
00:01:51,260 --> 00:01:56,600 
With organization generated data,
we refer to more traditional types of data 
25 
00:01:56,600 --> 00:01:59,770 
including transaction
information data bases and 
26 
00:01:59,770 --> 00:02:02,740 
structure data often
stored in data warehouses. 
27 
00:02:03,960 --> 00:02:10,180 
Note that big data can be structured,
semi-structured, and unstructured. 
28 
00:02:10,180 --> 00:02:15,030 
Which is a topic we will talk about
more and in depth later in this course. 
29 
00:02:18,775 --> 00:02:23,975 
Whatever your big data application is and
the types of big data you're using, 
30 
00:02:23,975 --> 00:02:29,905 
the real value will come from integrating
different types of data sources and 
31 
00:02:29,905 --> 00:02:31,755 
analyzing them at scale. 
32 
00:02:33,225 --> 00:02:36,355 
Overall, by modeling, managing and 
33 
00:02:36,355 --> 00:02:40,700 
integrating diverse streams
to improve our business and 
34 
00:02:40,700 --> 00:02:44,880 
add value to our big data even
before we start analyzing it. 
35 
00:02:45,910 --> 00:02:47,591 
As a part of modeling and 
36 
00:02:47,591 --> 00:02:52,804 
managing big data is focusing on
the dimensions of scale availability and 
37 
00:02:52,804 --> 00:02:58,959 
considering the challenges associated with
this dimensions to pick the right tools. 
38 
00:03:02,138 --> 00:03:07,460 
Volume, variety and
velocity are the main dimensions which 
39 
00:03:07,460 --> 00:03:12,480 
we characterized big data and
describe its challenges. 
40 
00:03:13,710 --> 00:03:18,550 
We have huge amounts of data
in different formats and 
41 
00:03:18,550 --> 00:03:21,940 
varying quality which
must be processed quickly 
42 
00:03:24,750 --> 00:03:30,810 
veracity refers to the biases,
noise, and abnormality in data, 
43 
00:03:30,810 --> 00:03:36,190 
or the unmeasurable certainty is in the
truthfulness and trustworthiness of data, 
44 
00:03:37,270 --> 00:03:41,470 
and valence refers to
the connectedness of big data. 
45 
00:03:41,470 --> 00:03:43,790 
Such as in the form of graph networks. 
46 
00:03:46,340 --> 00:03:52,500 
Each V presents a challenging
dimension of big data mainly of size, 
47 
00:03:52,500 --> 00:03:57,160 
complexity, speed, quality,
and consecutiveness. 
48 
00:03:57,160 --> 00:04:00,910 
Although we can list some
other v' based on the context. 
49 
00:04:00,910 --> 00:04:05,390 
We prefer to list these five as
fundamental dimensions which 
50 
00:04:05,390 --> 00:04:08,350 
this big data specialization
helps you work on. 
51 
00:04:09,570 --> 00:04:15,130 
Moreover, we must be sure to
never forget the sixth V: Value, 
52 
00:04:15,130 --> 00:04:18,220 
at the heart of the big
data challenge is turning 
53 
00:04:18,220 --> 00:04:21,590 
all of the other dimensions into
truly useful business value. 
54 
00:04:22,760 --> 00:04:26,490 
How will Big Data benefit you and
your organization? 
55 
00:04:26,490 --> 00:04:30,960 
The idea behind processing all
this Big Data in the first place 
56 
00:04:30,960 --> 00:04:33,100 
is to bring value to the problem at hand. 
57 
00:04:34,360 --> 00:04:38,310 
We need to take steps into
Big Data engineering and 
58 
00:04:38,310 --> 00:04:42,030 
scalable data science to
generate value out of Big Data. 
59 
00:04:43,910 --> 00:04:44,900 
We have all heard it. 
60 
00:04:44,900 --> 00:04:50,370 
Data signs turns big data into insides,
or even actions. 
61 
00:04:51,490 --> 00:04:52,940 
But what does that really mean? 
62 
00:04:54,340 --> 00:04:59,180 
Data signs can be taught of as
the basis for empirical research. 
63 
00:04:59,180 --> 00:05:02,940 
Like data is used to induce
information on the observations. 
64 
00:05:04,120 --> 00:05:06,830 
These observations are mainly data. 
65 
00:05:06,830 --> 00:05:12,280 
In our case, big data related to
a business or scientific use case. 
66 
00:05:14,550 --> 00:05:20,100 
Inside is a term we use to refer to
the data products of data science. 
67 
00:05:20,100 --> 00:05:23,450 
It is extracted from
a diverse amount of data 
68 
00:05:23,450 --> 00:05:27,620 
through a combination of exploratory
data analysis and modeling. 
69 
00:05:28,830 --> 00:05:33,560 
The questions are sometimes less
specific and it can require looking 
70 
00:05:33,560 --> 00:05:38,140 
carefully at the data for patterns in
it to come up with a specific question. 
71 
00:05:40,400 --> 00:05:45,470 
Another important point to recognize
is that data science is not static 
72 
00:05:45,470 --> 00:05:47,450 
one time analysis. 
73 
00:05:47,450 --> 00:05:52,990 
It involves a process where models
where you generate give us insights 
74 
00:05:52,990 --> 00:05:57,155 
are constantly improve to a further and
prequel evidence and iterations. 
1 
00:00:01,390 --> 00:00:03,650 
There are many ways to
look at this process. 
2 
00:00:04,790 --> 00:00:11,120 
One way of looking at it
as two distinct activities. 
3 
00:00:11,120 --> 00:00:17,000 
Mainly, big data engineering and
big data analytics, 
4 
00:00:17,000 --> 00:00:21,940 
or computational big data
science as I like to call it, 
5 
00:00:21,940 --> 00:00:24,890 
since more than simple
analytics are being performed. 
6 
00:00:26,840 --> 00:00:34,053 
A more detailed way of looking at
the process reveals five listing steps or 
7 
00:00:34,053 --> 00:00:38,125 
activities of the data science process, 
8 
00:00:38,125 --> 00:00:43,850 
namely acquire, prepare,
analyze, report, and act. 
9 
00:00:43,850 --> 00:00:48,900 
We can simply say that data science
happens at the boundary of all the steps. 
10 
00:00:48,900 --> 00:00:52,950 
Ideally, this process should
support experimental work, 
11 
00:00:52,950 --> 00:00:58,930 
which is constantly iterated and
leads to more scientific exploration, 
12 
00:00:58,930 --> 00:01:04,130 
as well as producing actionable
results during these explorations 
13 
00:01:04,130 --> 00:01:08,495 
using dynamic scalability on big data and
cloud platforms. 
14 
00:01:11,095 --> 00:01:15,842 
This five step process can be used in
alternative ways in real life big data 
15 
00:01:15,842 --> 00:01:20,685 
applications if we add the dependencies
of different tools to each other. 
16 
00:01:22,325 --> 00:01:25,915 
The influence of big data pushes for 
17 
00:01:25,915 --> 00:01:30,335 
alternative scalability approaches
at each step of the process. 
18 
00:01:31,600 --> 00:01:35,740 
Acquire includes anything
that helps us retrieve data, 
19 
00:01:35,740 --> 00:01:39,740 
including finding, accessing,
acquiring, and moving data. 
20 
00:01:41,500 --> 00:01:49,170 
It includes identification of and
authenticated access to all related data, 
21 
00:01:49,170 --> 00:01:53,490 
as well as transportation of data
from sources to destinations. 
22 
00:01:54,890 --> 00:02:00,760 
It includes ways to subset and
match the data to regions or 
23 
00:02:00,760 --> 00:02:05,350 
times of interest, which we sometimes
refer to as geospatial querying. 
24 
00:02:07,130 --> 00:02:11,234 
We divide the prepare data
step into two sub-steps, 
25 
00:02:11,234 --> 00:02:14,070 
based on the nature of the activity. 
26 
00:02:15,630 --> 00:02:21,060 
The first step in data preparation
involves exploring the data 
27 
00:02:21,060 --> 00:02:25,690 
to understand its nature,
what it means, its quality, and format. 
28 
00:02:27,160 --> 00:02:30,550 
It often takes a preliminary
analysis of data, or 
29 
00:02:30,550 --> 00:02:32,670 
samples of data, to understand it. 
30 
00:02:33,700 --> 00:02:36,770 
This is why this primary
step is called prepare. 
31 
00:02:39,030 --> 00:02:42,990 
Once we know more about the data
through exploratory analysis, 
32 
00:02:42,990 --> 00:02:46,420 
the next step is pre-processing
of data for analysis. 
33 
00:02:47,540 --> 00:02:52,690 
It includes cleaning data,
subsetting, or filtering data, and 
34 
00:02:52,690 --> 00:02:58,600 
creating data, which programs can read and
understand by modelling raw data 
35 
00:02:58,600 --> 00:03:04,710 
into a more defined data model, or
packaging it using a specific data format. 
36 
00:03:05,850 --> 00:03:11,270 
We will learn more about data models and
data formats later in this course. 
37 
00:03:12,940 --> 00:03:15,780 
If there are multiple data sets involved, 
38 
00:03:15,780 --> 00:03:20,220 
this step also includes integration
of different data sources or 
39 
00:03:20,220 --> 00:03:23,745 
streams, which is a topic we will
explore in our course three. 
40 
00:03:26,980 --> 00:03:32,050 
The prepared data then would be
passed on to the analysis step, 
41 
00:03:32,050 --> 00:03:35,670 
which involves selection of
analytical techniques to use, 
42 
00:03:35,670 --> 00:03:38,840 
building a model of the data,
and analyzing results. 
43 
00:03:39,920 --> 00:03:43,520 
This step can take a couple
of iterations on its own or 
44 
00:03:43,520 --> 00:03:47,119 
might require a data scientist
to go back to steps 1 and 
45 
00:03:47,119 --> 00:03:50,410 
2 to get more data or
package data in a different way. 
46 
00:03:51,700 --> 00:03:53,520 
So, exploration never ends. 
47 
00:03:56,050 --> 00:04:02,413 
Step 4 for communicating results includes
evaluation of analytical results, 
48 
00:04:02,413 --> 00:04:07,464 
presenting them in a visual way,
creating reports that include 
49 
00:04:07,464 --> 00:04:12,250 
an assessment of results with
respect to success criteria. 
50 
00:04:12,250 --> 00:04:19,293 
Activities in this step can often be
referred to with terms like interpret, 
51 
00:04:19,293 --> 00:04:23,720 
summarize, visualize, and post-process. 
52 
00:04:23,720 --> 00:04:28,880 
The last step brings us back to the very
first reason we do data science, 
53 
00:04:28,880 --> 00:04:29,550 
for a purpose. 
54 
00:04:31,090 --> 00:04:36,140 
Reporting insights from analysis and
determining actions from insights based 
55 
00:04:36,140 --> 00:04:41,267 
on the purpose you initially defined
is what we refer to as the act step. 
56 
00:04:43,210 --> 00:04:48,140 
We have now seen all of the steps
in a typical data science process. 
57 
00:04:48,140 --> 00:04:52,750 
Please note that this is an iterative
process and findings from 
58 
00:04:52,750 --> 00:04:57,310 
one step may require previous steps
to be repeated, but need information, 
59 
00:04:57,310 --> 00:05:01,250 
leading for further exploration and
application of these steps. 
60 
00:05:02,650 --> 00:05:07,230 
Scalability of this process to big
data analysis requires the use of 
61 
00:05:07,230 --> 00:05:10,099 
big data platforms like Hadoop. 
1 
00:00:00,450 --> 00:00:04,280 
The Hadoop ecosystem frameworks and
applications 
2 
00:00:04,280 --> 00:00:08,870 
provide such functionality through
several overarching themes and goals. 
3 
00:00:11,000 --> 00:00:15,830 
First, they provide scalability
to store large volumes of data 
4 
00:00:15,830 --> 00:00:17,150 
on commodity hardware. 
5 
00:00:18,760 --> 00:00:21,570 
As the number of systems increase, so 
6 
00:00:21,570 --> 00:00:25,620 
does the chance for
crashes and hardware failures. 
7 
00:00:25,620 --> 00:00:30,390 
They handle fault tolerance to
gracefully recover from these problems. 
8 
00:00:32,650 --> 00:00:37,595 
In addition, they are designed to handle
big data capacity and compressing text 
9 
00:00:37,595 --> 00:00:43,185 
files, graphs of social networks,
streaming sensor data and raster images. 
10 
00:00:43,185 --> 00:00:45,635 
We can add more data
types to this variety. 
11 
00:00:46,695 --> 00:00:48,355 
For any given data type, 
12 
00:00:49,515 --> 00:00:53,635 
you can find several projects in
the ecosystem that support it. 
13 
00:00:55,250 --> 00:00:58,890 
Finally, they facilitate
a shared environment, 
14 
00:00:58,890 --> 00:01:02,050 
allow multiple jobs to
execute simultaneously. 
15 
00:01:04,350 --> 00:01:08,930 
Additionally, the Hadoop ecosystem
includes a wide range of open source 
16 
00:01:08,930 --> 00:01:14,170 
projects backed by a large and
active community. 
17 
00:01:14,170 --> 00:01:18,480 
These projects are free to use and
easy to find support for. 
18 
00:01:20,060 --> 00:01:25,690 
Today, there are over 100
Big Data open source projects, 
19 
00:01:25,690 --> 00:01:31,250 
and this continues to grow, many rely
on Hadoop, but some are independent. 
20 
00:01:33,960 --> 00:01:39,230 
Here is, one way of looking at a subset
of tools in the Hadoop Ecosystem. 
21 
00:01:40,360 --> 00:01:45,240 
This layer diagram is organized
vertically based on the interface. 
22 
00:01:46,390 --> 00:01:52,720 
Lower level interfaces to storage and
scheduling on the bottom and 
23 
00:01:52,720 --> 00:01:56,520 
high level languages and
interactivity at the top. 
24 
00:01:59,080 --> 00:02:05,112 
The Hadoop distributed file system,
or HDFS, is the foundation for 
25 
00:02:05,112 --> 00:02:11,777 
many big data frameworks since it
provides scalable and reliable storage. 
26 
00:02:11,777 --> 00:02:16,915 
As the size of your data increases,
you can add commodity 
27 
00:02:16,915 --> 00:02:21,314 
hardware to HDFS to
increase storage capacity. 
28 
00:02:21,314 --> 00:02:27,107 
So it enables what we call
scaling out of your resources. 
29 
00:02:29,267 --> 00:02:32,960 
Hadoop YARN provide
flexible scheduling and 
30 
00:02:32,960 --> 00:02:36,660 
resource management over the HTFS storage. 
31 
00:02:37,660 --> 00:02:43,955 
Yarn is use at Yahoo to schedule
jobs across 40,000 servers. 
32 
00:02:45,105 --> 00:02:50,045 
MapReduce is a programming model
that simplifies parallel computing. 
33 
00:02:50,045 --> 00:02:54,725 
Instead of dealing with the complexities
of synchronization and scheduling you only 
34 
00:02:54,725 --> 00:03:00,325 
need to give MapReduce two
functions map and reduce. 
35 
00:03:00,325 --> 00:03:02,643 
This programming model is so 
36 
00:03:02,643 --> 00:03:07,962 
powerful that Google previously
used it for indexing websites. 
37 
00:03:07,962 --> 00:03:12,820 
MapReduce, only assumes
a limited model to express data. 
38 
00:03:12,820 --> 00:03:16,429 
Hive, and
Pig are two additional programming models, 
39 
00:03:16,429 --> 00:03:20,421 
on top of MapReduce,
to augment data modeling of MapReduce, 
40 
00:03:20,421 --> 00:03:24,744 
with relational algebra and
data flow modeling, respectively. 
41 
00:03:26,869 --> 00:03:31,682 
Hive was created at Facebook to
issue SQL-like queries using 
42 
00:03:31,682 --> 00:03:34,280 
MapReduce on their data in HDFS. 
43 
00:03:35,410 --> 00:03:40,773 
Pig was created at Yahoo to model
dataflow based programs using MapReduce. 
44 
00:03:40,773 --> 00:03:45,345 
Thanks to YARNs ability to
manage resources, not just for 
45 
00:03:45,345 --> 00:03:48,715 
MapReduce but other programming models. 
46 
00:03:48,715 --> 00:03:52,955 
Giraph was built for
processing large scale graphs efficiently. 
47 
00:03:54,335 --> 00:04:00,676 
For example, Facebook uses Giraph to
analyze the social graphs of its users. 
48 
00:04:00,676 --> 00:04:05,181 
Similarly, Storm, Spark and
Flink were built for 
49 
00:04:05,181 --> 00:04:09,595 
real time and
In-memory processing of big data. 
50 
00:04:09,595 --> 00:04:14,013 
On top of the YARN resource scheduler and
HDFS. 
51 
00:04:14,013 --> 00:04:20,555 
In-memory processing is a powerful
way of running big data applications, 
52 
00:04:20,555 --> 00:04:26,598 
even faster, achieving 100x better
performance for some tasks. 
53 
00:04:26,598 --> 00:04:30,845 
Sometimes your data processing or
tasks are not easily or 
54 
00:04:30,845 --> 00:04:36,332 
efficiently represented using the file and
directory model of storage, 
55 
00:04:36,332 --> 00:04:42,010 
examples of this include collections
of key values or large sparse tables. 
56 
00:04:43,282 --> 00:04:48,859 
NoSQL projects such as
Cassandra MongoDB and 
57 
00:04:48,859 --> 00:04:52,681 
HBase handle all these cases. 
58 
00:04:52,681 --> 00:04:57,396 
Cassandra was created at Facebook and
Facebook also use HBase for 
59 
00:04:57,396 --> 00:04:59,290 
its messaging platform. 
60 
00:05:01,934 --> 00:05:07,562 
Finally, running all this tools requires
a centralized management system for 
61 
00:05:07,562 --> 00:05:12,520 
synchronization, configuration And
to ensure high availability. 
62 
00:05:13,660 --> 00:05:18,828 
Zookeeper, created by Yahoo to
wrangle services named after animals, 
63 
00:05:18,828 --> 00:05:20,614 
performs these duties. 
64 
00:05:23,214 --> 00:05:27,023 
Just looking at the small number
of Hadoop stack components, 
65 
00:05:27,023 --> 00:05:31,700 
we can already see that most of them
are dedicated to data modeling. 
66 
00:05:31,700 --> 00:05:36,010 
Management, and
efficient processing of the data. 
67 
00:05:36,010 --> 00:05:40,610 
In the rest of this course,
we will give you fundamental knowledge and 
68 
00:05:40,610 --> 00:05:46,580 
some practical skills on how to start
modeling and managing your data, and 
69 
00:05:46,580 --> 00:05:52,310 
picking the right tools for this activity
from a plethora of big data tools. 
1 
00:00:01,170 --> 00:00:04,780 
Welcome to course two of
the big data specialization. 
2 
00:00:04,780 --> 00:00:06,390 
I'm Amarnath Gupta. 
3 
00:00:06,390 --> 00:00:07,270 
>> And I'm Ilkay Altintas. 
4 
00:00:07,270 --> 00:00:10,830 
We are really excited to work
with you in this course, 
5 
00:00:10,830 --> 00:00:14,820 
to develop your understanding and skills
in big data modeling and management. 
6 
00:00:16,100 --> 00:00:20,470 
You might have just finished our first
course, and see the potential and 
7 
00:00:20,470 --> 00:00:22,570 
challenges of big data. 
8 
00:00:22,570 --> 00:00:25,630 
If you haven't it's not required but for 
9 
00:00:25,630 --> 00:00:29,510 
those with less background in
the area you might find it valuable. 
10 
00:00:30,890 --> 00:00:34,070 
>> Let's explain what we mean by
big data modeling and management. 
11 
00:00:35,400 --> 00:00:39,470 
Suppose you have an application
where the data is big in a sense and 
12 
00:00:39,470 --> 00:00:45,470 
it has a large volume, or has high speed,
or comes with a lot of variations. 
13 
00:00:45,470 --> 00:00:49,220 
Even before you think of
how to handle the bigness, 
14 
00:00:49,220 --> 00:00:51,450 
you need to have a sense of
what the data looks like. 
15 
00:00:52,550 --> 00:00:57,860 
The goal of data modeling is to
formally explore the nature of data, so 
16 
00:00:57,860 --> 00:01:00,870 
that you can figure out what
kind of storage you need, and 
17 
00:01:00,870 --> 00:01:02,470 
what kind of processing you can do on it. 
18 
00:01:04,010 --> 00:01:07,010 
The goal of data management
is to figure out 
19 
00:01:07,010 --> 00:01:10,840 
what kind of infrastructure support
you would need for the data. 
20 
00:01:10,840 --> 00:01:15,130 
For example, does your environment need
to keep multiple replicas of the data? 
21 
00:01:15,130 --> 00:01:18,454 
Do you need to do statistical
computation with the data? 
22 
00:01:18,454 --> 00:01:23,120 
Once these operational requirements,
you'll 
23 
00:01:23,120 --> 00:01:27,110 
be able to choose the right system that
will let you perform these operations. 
24 
00:01:28,990 --> 00:01:33,200 
>> We will also introduce
management of big data as it is 
25 
00:01:33,200 --> 00:01:38,530 
streaming from data sources and talk
about storage architectures for big data. 
26 
00:01:38,530 --> 00:01:42,790 
For example,
how can high velocity data get ingested, 
27 
00:01:42,790 --> 00:01:48,360 
managed, stored in order to enable
real time analytical capabilities. 
28 
00:01:48,360 --> 00:01:53,560 
Or what is the difference between
data at rest and data in motion? 
29 
00:01:53,560 --> 00:01:56,230 
And how can a data system enable both? 
30 
00:01:57,570 --> 00:02:00,680 
>> Once you've understood the basic
concepts of data modeling, 
31 
00:02:00,680 --> 00:02:04,300 
data management, and streaming data,
we will introduce you 
32 
00:02:04,300 --> 00:02:09,120 
to the characteristics of large volume
data and how to think about that. 
33 
00:02:09,120 --> 00:02:13,080 
Thus, we will transition from
classical database management systems, 
34 
00:02:13,080 --> 00:02:18,730 
that is DBMSs,
to big data management systems, or BDMSs. 
35 
00:02:18,730 --> 00:02:23,000 
We'll present brief overviews of
several big data management systems 
36 
00:02:23,000 --> 00:02:24,430 
available in the marketplace today. 
37 
00:02:25,630 --> 00:02:31,310 
We are so excited to show you examples
of archived and streaming big data sets. 
38 
00:02:31,310 --> 00:02:36,500 
Our goal is to provide you with simple
hands on exercises that require 
39 
00:02:36,500 --> 00:02:41,290 
no programming, but show you what
big data looks like, and why various 
40 
00:02:41,290 --> 00:02:47,030 
big data management systems are suitable
for specific kinds of big data. 
41 
00:02:47,030 --> 00:02:50,510 
In the end you will be
able to design a simple 
42 
00:02:50,510 --> 00:02:53,110 
big data information system
using this knowledge. 
43 
00:02:54,110 --> 00:02:56,280 
We wish you a fun time learning and 
44 
00:02:56,280 --> 00:03:00,540 
hope to hear from you in the discussion
forums and learner stories. 
45 
00:03:00,540 --> 00:03:02,529 
>> Happy learning and think big data. 
1 
00:00:01,000 --> 00:00:06,820 
This is the second course in the 2016
version of our big data specialization. 
2 
00:00:06,820 --> 00:00:10,820 
>> After listening to learners like you,
we have changed some of the content and 
3 
00:00:10,820 --> 00:00:12,149 
ordering in the specialization. 
4 
00:00:13,210 --> 00:00:17,200 
This course takes you on the first
step of any big data project, 
5 
00:00:17,200 --> 00:00:19,470 
its modeling and management. 
6 
00:00:19,470 --> 00:00:23,221 
>> We hope that this background
on what big data looks like and 
7 
00:00:23,221 --> 00:00:27,662 
how it is modeled and managed in
a large scale system will make you feel 
8 
00:00:27,662 --> 00:00:32,487 
more prepared to take steps towards
retrieving and processing big data and 
9 
00:00:32,487 --> 00:00:37,338 
perform big data analytics which
are topics coming up in the next courses. 
1 
00:00:03,582 --> 00:00:06,800 
The third component of
a data model is constraints. 
2 
00:00:08,080 --> 00:00:10,418 
A constraint is a logical statement. 
3 
00:00:10,418 --> 00:00:15,770 
That means one can compute and test
whether the statement is true or false. 
4 
00:00:16,980 --> 00:00:21,130 
Constraints are part of the data model
because they can specify something about 
5 
00:00:21,130 --> 00:00:24,240 
the semantics, that is,
the meaning of the data. 
6 
00:00:24,240 --> 00:00:27,705 
For example,
the constraint that a week has seven and 
7 
00:00:27,705 --> 00:00:32,402 
only seven days is something that a data
system would not know unless this 
8 
00:00:32,402 --> 00:00:36,025 
knowledge is passed on to it
in the form of a constraint. 
9 
00:00:36,025 --> 00:00:39,531 
Another constraint, shown here, 
10 
00:00:42,132 --> 00:00:46,690 
Tells the system that the number of
titles for a movie is restricted to one. 
11 
00:00:49,740 --> 00:00:50,800 
Different data models, 
12 
00:00:50,800 --> 00:00:54,260 
as we'll see in the next module,
will have different kinds of constraints. 
13 
00:00:56,140 --> 00:00:58,760 
There may be many different
kinds of constraints. 
14 
00:01:00,020 --> 00:01:04,039 
A value constraint is a logical
statement about data values. 
15 
00:01:05,520 --> 00:01:09,600 
On a previous slide we have said,
that the age, that is, 
16 
00:01:09,600 --> 00:01:13,540 
the value of data elements representing
the age of an entity can not be negative. 
17 
00:01:16,310 --> 00:01:20,840 
We also saw an example of
uniqueness constraint when we said 
18 
00:01:20,840 --> 00:01:23,880 
every movie can have only one title. 
19 
00:01:25,200 --> 00:01:27,120 
In the words of logic, 
20 
00:01:27,120 --> 00:01:32,460 
there should exist no data object that's
a movie and has more than one title. 
21 
00:01:33,860 --> 00:01:37,719 
It's easy to see that enforcing
these constraints requires us 
22 
00:01:37,719 --> 00:01:41,446 
to count the number of titles and
then verify that it's one. 
23 
00:01:41,446 --> 00:01:46,490 
Now, one can generalize this to count
the number of values associated with 
24 
00:01:46,490 --> 00:01:51,310 
each object and check whether it lies
between an upper and lower bound. 
25 
00:01:52,380 --> 00:01:56,010 
This is often called a cardinality
constraint of data property. 
26 
00:01:59,530 --> 00:02:04,388 
In a medical example here,
the constraint has a lower limit of 0 and 
27 
00:02:04,388 --> 00:02:05,931 
an upper limit of 3. 
28 
00:02:10,390 --> 00:02:15,253 
A different kind of value constraint can
be enforced by restricting the type of 
29 
00:02:15,253 --> 00:02:17,030 
the data allowed in a field. 
30 
00:02:18,620 --> 00:02:23,210 
If we do not have such a constraint we
can put any type of data in the field. 
31 
00:02:23,210 --> 00:02:28,300 
For example, you can have -99 as
the value of the last name of a person, 
32 
00:02:28,300 --> 00:02:30,680 
of course that would be wrong. 
33 
00:02:30,680 --> 00:02:34,750 
To ensure that this does not happen,
we can enforce the type of the last name 
34 
00:02:35,780 --> 00:02:40,300 
to be a non-numeric alphabetic string. 
35 
00:02:40,300 --> 00:02:45,420 
This example shows a logical
expression for this constraint. 
36 
00:02:45,420 --> 00:02:50,840 
A type constraint is a special
kind of domain constraint. 
37 
00:02:50,840 --> 00:02:52,420 
The domain of a data property or 
38 
00:02:52,420 --> 00:02:57,900 
attribute is the possible set of values
that are allowed for that attribute. 
39 
00:02:57,900 --> 00:03:01,026 
For example, the possible values for 
40 
00:03:01,026 --> 00:03:05,624 
the day part of the date field
can be between 1 and 31. 
41 
00:03:05,624 --> 00:03:09,746 
While a month may have
the value between 1 and 12. 
42 
00:03:09,746 --> 00:03:16,200 
Or, alternately, a value from the set
January, February, ect till December. 
43 
00:03:17,870 --> 00:03:21,697 
Now, one can devise a more complex
constraint, where the value of the date 
44 
00:03:21,697 --> 00:03:26,099 
for April, June, and September and
November, are restricted between 1 and 30. 
45 
00:03:26,099 --> 00:03:30,609 
And if you think about it,
all three constraints that we have 
46 
00:03:30,609 --> 00:03:35,420 
described in the last slide
are value constraints. 
47 
00:03:35,420 --> 00:03:39,380 
So they only state how to restrict
the values of some data property. 
48 
00:03:40,470 --> 00:03:45,392 
In sharp contrast, structural properties
restrict the structure of the data. 
49 
00:03:45,392 --> 00:03:48,700 
We'll choose a more complex example for
this. 
50 
00:03:50,320 --> 00:03:53,952 
Suppose we are a matrix,
as shown in the example, and 
51 
00:03:53,952 --> 00:03:56,794 
we've restricted to be a square matrix. 
52 
00:03:56,794 --> 00:04:00,919 
So the number of columns is exactly
equal to the number of rows. 
53 
00:04:03,230 --> 00:04:06,570 
We have not put any restriction
on the number of rows or columns. 
54 
00:04:06,570 --> 00:04:07,910 
But just that they have to be the same. 
55 
00:04:09,280 --> 00:04:11,560 
Now this constrains
the structure of the matrix and 
56 
00:04:11,560 --> 00:04:16,140 
implies that the number of entries in
the structure will be a squared number. 
57 
00:04:17,480 --> 00:04:22,600 
If we convert this matrix to a three
column table as shown, and impose 
58 
00:04:22,600 --> 00:04:28,000 
the same squareness constraint, it will
translate to a more complex condition. 
59 
00:04:28,000 --> 00:04:31,540 
That the number of data
rows will be the square of 
60 
00:04:31,540 --> 00:04:34,200 
the number of unique values
in column one of the table. 
61 
00:04:35,900 --> 00:04:39,410 
We'll encounter some more structural
constraints in the next module. 
1 
00:00:03,208 --> 00:00:07,770 
The second component of a data model is
a set of operations that can be performed 
2 
00:00:07,770 --> 00:00:09,390 
on the data. 
3 
00:00:09,390 --> 00:00:10,250 
And in this module, 
4 
00:00:10,250 --> 00:00:15,650 
we'll discuss the operations without
considering the bigness aspect. 
5 
00:00:15,650 --> 00:00:17,110 
In course three, 
6 
00:00:17,110 --> 00:00:20,710 
we'll come back to the issue of performing
these operations when the data is large. 
7 
00:00:22,515 --> 00:00:25,830 
Now,operations specified
the methods to manipulate the data. 
8 
00:00:26,840 --> 00:00:31,220 
SInce different data models are typically
associated with different structures, 
9 
00:00:31,220 --> 00:00:34,210 
the operations on them will be different. 
10 
00:00:34,210 --> 00:00:39,410 
But some types of operations are usually
performed across all data models. 
11 
00:00:39,410 --> 00:00:41,030 
We'll describe a few of them here. 
12 
00:00:43,320 --> 00:00:50,818 
One common operation extract a part of
a collection based on the condition. 
13 
00:00:50,818 --> 00:00:54,422 
In the example here,
we have a set of records and 
14 
00:00:54,422 --> 00:00:59,110 
we're looking for a sub set that
satisfies the condition that 
15 
00:00:59,110 --> 00:01:03,548 
the fifth field has a value
greater than 100,000. 
16 
00:01:03,548 --> 00:01:07,603 
The only one record
satisfies this requirement. 
17 
00:01:07,603 --> 00:01:11,580 
Note that we called this operation
subsetting rather loosely. 
18 
00:01:12,940 --> 00:01:17,210 
Depending on the context,
it's also called selection or filtering. 
19 
00:01:20,410 --> 00:01:26,300 
The next common operation is retrieving
a part of a structure that is specified. 
20 
00:01:26,300 --> 00:01:29,440 
In this case,
we specify that we are interested 
21 
00:01:29,440 --> 00:01:33,130 
in just the first two fields
of a collection off records. 
22 
00:01:34,880 --> 00:01:40,340 
But this produces a new collection of
records which has only these fields. 
23 
00:01:41,350 --> 00:01:44,460 
This operation like before has many names. 
24 
00:01:44,460 --> 00:01:46,910 
The most dominant name is projection. 
25 
00:01:48,340 --> 00:01:52,204 
In the next module, we'll see several
versions of this operation for 
26 
00:01:52,204 --> 00:01:53,588 
different data models. 
27 
00:01:56,108 --> 00:02:00,176 
The next two operations are about
combining two collections 
28 
00:02:00,176 --> 00:02:01,460 
into a larger one. 
29 
00:02:02,540 --> 00:02:06,450 
The term combine may be
interpreted in various ways. 
30 
00:02:06,450 --> 00:02:08,830 
The most straightforward
of them is said union. 
31 
00:02:09,910 --> 00:02:12,500 
The assumption behind the union operation 
32 
00:02:12,500 --> 00:02:15,930 
is that the two collections
involved have the same structure. 
33 
00:02:15,930 --> 00:02:22,050 
In other words, if one collection has
four fields and another has 14 fields, or 
34 
00:02:22,050 --> 00:02:27,230 
if one has four fields on people and
the dates of birth, and the other has four 
35 
00:02:27,230 --> 00:02:31,350 
things about countries and their capitols,
they cannot be combined through union. 
36 
00:02:33,650 --> 00:02:37,340 
In the example here,
their two collections have three and 
37 
00:02:37,340 --> 00:02:42,620 
two records, respectively, with one
record that's common between them. 
38 
00:02:42,620 --> 00:02:43,120 
The green one. 
39 
00:02:45,100 --> 00:02:48,170 
The result collection has four record, 
40 
00:02:48,170 --> 00:02:53,300 
because duplicates are disallowed
because it's a set operation. 
41 
00:02:53,300 --> 00:02:58,110 
There is indeed another version of
union where duplicates are allowed and 
42 
00:02:58,110 --> 00:03:00,540 
will produce five records instead of four. 
43 
00:03:03,040 --> 00:03:08,020 
The second kind of combining,
called a Join, can be done when 
44 
00:03:08,020 --> 00:03:12,460 
the two collections have different data
content but have some common elements. 
45 
00:03:14,430 --> 00:03:16,290 
In the example shown, 
46 
00:03:16,290 --> 00:03:19,920 
the first field is the common element
between the two collections on the left. 
47 
00:03:21,040 --> 00:03:24,630 
In this kind of data combination
there are two stages. 
48 
00:03:24,630 --> 00:03:30,380 
First, for each data item think
of a record of collection one, 
49 
00:03:30,380 --> 00:03:33,490 
one finds a set of matching
data items in collection two. 
50 
00:03:35,120 --> 00:03:38,960 
Thus, the first records of
the two collections match 
51 
00:03:38,960 --> 00:03:40,060 
based on the first field. 
52 
00:03:41,490 --> 00:03:43,750 
In the second phase of the operation, 
53 
00:03:43,750 --> 00:03:46,710 
all fields of the matching
record pairs are put together. 
54 
00:03:47,780 --> 00:03:51,170 
In the first record of the result
collection shown on the right, 
55 
00:03:51,170 --> 00:03:54,890 
one gets the first four fields
on the first collection and 
56 
00:03:54,890 --> 00:03:57,230 
the remaining two fields
from the second collection. 
57 
00:03:58,500 --> 00:04:04,380 
Now in this one example, we found one pair
of matching records from the collections. 
58 
00:04:04,380 --> 00:04:07,790 
In general, one would find more
than one matching record pairs. 
59 
00:04:09,260 --> 00:04:12,415 
As you can see,
this operation is more complex and 
60 
00:04:12,415 --> 00:04:16,863 
can be very expensive when the size
of the true collections are large. 
1 
00:00:00,800 --> 00:00:06,950 
In the big data world, we often hear the
term structured data, that is, data having 
2 
00:00:06,950 --> 00:00:12,490 
a structure which is quite different
from the so-called unstructured data. 
3 
00:00:12,490 --> 00:00:13,500 
But what is a structure? 
4 
00:00:14,910 --> 00:00:16,040 
Let's consider file 1. 
5 
00:00:17,310 --> 00:00:21,960 
It's a typical CSV file that has
three lines with different content. 
6 
00:00:21,960 --> 00:00:26,260 
But the file content is uniform
in the sense that each line, 
7 
00:00:26,260 --> 00:00:30,400 
call it a record,
has exactly three fields, 
8 
00:00:30,400 --> 00:00:33,360 
which we sometimes call data properties or
attributes. 
9 
00:00:34,720 --> 00:00:41,420 
Further, the first two of these fields
are strings and the third one is a date. 
10 
00:00:41,420 --> 00:00:44,770 
We can add more records, that's
lines with the same pattern of data, 
11 
00:00:44,770 --> 00:00:46,210 
to the file in the same fashion. 
12 
00:00:47,240 --> 00:00:52,430 
The content will grow, but the pattern of
data organization will remain identical. 
13 
00:00:53,440 --> 00:00:58,600 
This repeatable pattern of data
organization makes the file structured. 
14 
00:00:58,600 --> 00:01:02,870 
Now let's look at file 2,
which is four records of five fields each, 
15 
00:01:03,910 --> 00:01:07,680 
except that the third record seems
to be missing the last entry. 
16 
00:01:08,710 --> 00:01:09,660 
Is this file structured? 
17 
00:01:10,810 --> 00:01:15,383 
We argue that it is, because the missing
value makes the third record incomplete, 
18 
00:01:15,383 --> 00:01:20,310 
but it does not break the structure or
the pattern of the data organization. 
19 
00:01:20,310 --> 00:01:23,088 
Let's looks at these
two files side by side. 
20 
00:01:23,088 --> 00:01:29,320 
Clearly file 2 has more fields, and hence
is sort of wider than the first file. 
21 
00:01:29,320 --> 00:01:31,980 
Would you say that they
have the same structure? 
22 
00:01:31,980 --> 00:01:33,990 
Well, on the face of it they don't. 
23 
00:01:33,990 --> 00:01:35,740 
But if you think more broadly, 
24 
00:01:35,740 --> 00:01:39,260 
you would notice that they
are both collection of k fields. 
25 
00:01:40,330 --> 00:01:44,770 
The size of the collection,
respectively three and four, differs. 
26 
00:01:44,770 --> 00:01:48,320 
And k is 3 in the first case and
5 in the second. 
27 
00:01:48,320 --> 00:01:52,330 
But we can think of 3 and 5 as parameters. 
28 
00:01:52,330 --> 00:01:57,280 
In that case, we will say that these
files have been generated by a similar 
29 
00:01:57,280 --> 00:02:02,160 
organizational structure, and
hence they have the same data model. 
30 
00:02:03,440 --> 00:02:06,190 
Now in contrast, consider this file. 
31 
00:02:06,190 --> 00:02:10,480 
Just looking at it, it's impossible to
figure out how the data is organized and 
32 
00:02:10,480 --> 00:02:12,420 
how to identify subparts of the data. 
33 
00:02:13,440 --> 00:02:15,850 
We would call this data unstructured. 
34 
00:02:16,860 --> 00:02:21,605 
Often, compressed data like JPEG images,
MP3 audio files, 
35 
00:02:21,605 --> 00:02:26,450 
MPEG3 video files, encrypted data,
are usually unstructured. 
36 
00:02:27,490 --> 00:02:32,098 
In module two, we'll elaborate on data
models that are not fully structured or 
37 
00:02:32,098 --> 00:02:33,900 
are structured differently. 
1 
00:00:00,890 --> 00:00:03,790 
This is the first of two
hands-on exercises involving 
2 
00:00:03,790 --> 00:00:06,170 
sensor data from a weather station. 
3 
00:00:06,170 --> 00:00:09,530 
In this one, we will look at
static data in a text file. 
4 
00:00:09,530 --> 00:00:12,900 
The next one we will look at live data
streaming from the weather station 
5 
00:00:12,900 --> 00:00:13,490 
in real time. 
6 
00:00:13,490 --> 00:00:17,820 
In this exercise, we will begin
by opening a terminal window and 
7 
00:00:17,820 --> 00:00:21,330 
changing into the directory
containing the station measurements. 
8 
00:00:21,330 --> 00:00:24,560 
We will look at these measurements in
a text file and then look at the key for 
9 
00:00:24,560 --> 00:00:27,540 
these measurements so
we can understand what the values mean. 
10 
00:00:27,540 --> 00:00:29,310 
Finally, we will plot the measurements. 
11 
00:00:30,340 --> 00:00:30,840 
Let's begin. 
12 
00:00:31,870 --> 00:00:35,023 
First, we will open a terminal window by
clicking on the Terminal icon on the top 
13 
00:00:35,023 --> 00:00:35,692 
of the toolbar. 
14 
00:00:38,179 --> 00:00:44,814 
Next, we'll cd into the directory 
15 
00:00:44,814 --> 00:00:50,350 
containing the sensor data. 
16 
00:00:51,380 --> 00:00:58,358 
We'll run cd
Downloads/big-data-two/sensor. 
17 
00:01:00,680 --> 00:01:07,750 
We can write ls to see
the contents of this directory. 
18 
00:01:12,274 --> 00:01:18,554 
The data from the weather station is
in a text file called wx-data.txt. 
19 
00:01:21,370 --> 00:01:26,550 
We can run more wx-data.txt to
see the contents of this file. 
20 
00:01:34,970 --> 00:01:37,990 
Each line of this file is
a separate set of measurements. 
21 
00:01:37,990 --> 00:01:42,390 
There are two columns in this file,
the first column is a time stamp and 
22 
00:01:42,390 --> 00:01:45,160 
it's separated by
a second column by a tab. 
23 
00:01:45,160 --> 00:01:51,710 
The second column itself has separate
columns and these are separated by commas. 
24 
00:01:51,710 --> 00:01:56,790 
The time stamp is the number
of seconds since 1970. 
25 
00:01:56,790 --> 00:01:59,450 
You'll notice that it increases for
each time stamp. 
26 
00:02:01,830 --> 00:02:04,520 
You'll notice that it increases for
each measurement. 
27 
00:02:04,520 --> 00:02:06,550 
But sometimes measurements
come in at the same time. 
28 
00:02:06,550 --> 00:02:10,028 
For example this one at 006. 
29 
00:02:10,028 --> 00:02:15,053 
The measurements,
we see that the prefix is 0R1 for 
30 
00:02:15,053 --> 00:02:18,188 
most of them but some have 0R2. 
31 
00:02:18,188 --> 00:02:21,060 
If we look at the other measurements, 
32 
00:02:21,060 --> 00:02:26,646 
we see that all the 0R1 measurements
start with Dn, Dm, Dx, and so on. 
33 
00:02:26,646 --> 00:02:31,670 
Whereas R2 begins with Ta, Ua, and Pa. 
34 
00:02:34,310 --> 00:02:37,419 
If we scroll down in the text
file by hitting the space bar, 
35 
00:02:37,419 --> 00:02:40,728 
we'll see there are other
measurements besides R1 and R2. 
36 
00:02:43,860 --> 00:02:49,360 
For example, there's R5 that has Th,
Vh, Vs and so on. 
37 
00:02:49,360 --> 00:02:53,830 
And there's R0 which has
all the measurements. 
38 
00:02:53,830 --> 00:02:57,510 
So Dn, Dm, Dx, Ta, Ua, Pa. 
39 
00:03:00,550 --> 00:03:01,930 
And the remaining ones. 
40 
00:03:04,160 --> 00:03:07,434 
Next we'll open another internal
window and look at the key to this 
41 
00:03:07,434 --> 00:03:10,598 
measurements click on the tool
bar to open the terminal window. 
42 
00:03:10,598 --> 00:03:15,900 
cd into downloads big data two sensor 
43 
00:03:23,120 --> 00:03:31,131 
And the key to these measurements is
in a file called wxt520format.txt. 
44 
00:03:31,131 --> 00:03:36,290 
We can run more wxt520format.txt
to see this file. 
45 
00:03:44,190 --> 00:03:47,116 
This file says where each
of the prefix is mean, for 
46 
00:03:47,116 --> 00:03:49,307 
example Sn is the wind speed minimum. 
47 
00:03:49,307 --> 00:03:56,430 
Sm is the wind speed average. 
48 
00:03:56,430 --> 00:04:00,700 
And Ta is the air temperature. 
49 
00:04:00,700 --> 00:04:04,530 
So if we go back to our sensor file,
we see here Ta equals 13.9c. 
50 
00:04:04,530 --> 00:04:10,187 
That means the air temperature at this 
51 
00:04:10,187 --> 00:04:15,690 
time was 13.9 degrees celsius. 
52 
00:04:15,690 --> 00:04:21,022 
We can also create a plot of
the data in this text file 
53 
00:04:21,022 --> 00:04:25,994 
by running plot-data.py wx-data.txt Ta. 
54 
00:04:32,727 --> 00:04:37,529 
This says to plot the data in
the wx-data file and the measure that we 
55 
00:04:37,529 --> 00:04:42,680 
want to apply is Ta, which according
to our key, is the air temperature. 
56 
00:04:46,880 --> 00:04:51,520 
When we run it, it displays a plot of the
air temperature found in the text file. 
1 
00:00:00,568 --> 00:00:02,240 
In this hands-on exercise, 
2 
00:00:02,240 --> 00:00:05,870 
we will be looking at an image file,
which uses an array data model. 
3 
00:00:07,050 --> 00:00:11,380 
First, we will open a terminal window and
display an image file on the screen. 
4 
00:00:12,890 --> 00:00:16,630 
Next, we will examine the structure
of the image, and finally, 
5 
00:00:16,630 --> 00:00:19,860 
extract pixel values from
various locations in the image. 
6 
00:00:21,550 --> 00:00:22,660 
Let's begin. 
7 
00:00:22,660 --> 00:00:25,954 
First, we'll open a terminal window by
clicking on the terminal icon at the top 
8 
00:00:25,954 --> 00:00:26,650 
of the toolbar. 
9 
00:00:33,101 --> 00:00:37,989 
Next, we'll CDN2 the directory
containing the image, 
10 
00:00:37,989 --> 00:00:41,390 
cdn2downloads/bigdata2/image. 
11 
00:00:48,410 --> 00:00:51,470 
We can run ls to see the image
in different scripts. 
12 
00:00:54,050 --> 00:00:58,344 
The file, Australia.jpg,
is an image that we want to view, 
13 
00:00:58,344 --> 00:01:00,090 
we can use eog to view it. 
14 
00:01:00,090 --> 00:01:08,830 
Run eog Australia.jpg Eog is
a common image viewer on Linux. 
15 
00:01:11,192 --> 00:01:15,870 
Australia.jpg is a satellite image
of the Australian continent. 
16 
00:01:15,870 --> 00:01:17,560 
Now let's look at the structure
of this image file. 
17 
00:01:21,320 --> 00:01:25,815 
If we go back to our terminal window,
we can run the image 
18 
00:01:25,815 --> 00:01:30,611 
viewer in the background by
hitting CTRL+Z, and then bg. 
19 
00:01:30,611 --> 00:01:35,319 
We can view the dimensions or
structure of the array data model of this 
20 
00:01:35,319 --> 00:01:38,904 
image by running
dimensions.py Australia.jpg. 
21 
00:01:43,740 --> 00:01:50,860 
This says that the image has
5250 columns and 4320 rows. 
22 
00:01:50,860 --> 00:01:53,240 
So it is a two-dimensional image. 
23 
00:01:53,240 --> 00:01:55,671 
Additionally, each cell or 
24 
00:01:55,671 --> 00:02:01,470 
pixel within this image is
composed of three 8-bit pixels. 
25 
00:02:01,470 --> 00:02:06,130 
These pixels are composed of three
elements, red, green, and blue. 
26 
00:02:06,130 --> 00:02:09,410 
We can extract or
view the individual pixel elements 
27 
00:02:09,410 --> 00:02:13,110 
at a specific location in the image
by using the pixel.py script. 
28 
00:02:14,440 --> 00:02:21,240 
We can run pixel.py Australia.jpg 0
0 to see the value at one location. 
29 
00:02:29,430 --> 00:02:33,430 
The 0 0 location is
the corner of the image. 
30 
00:02:33,430 --> 00:02:39,281 
If we go back to the image, the corners
are all the ocean, so they're dark blue. 
31 
00:02:39,281 --> 00:02:43,995 
If we look at the value that was
extracted, we see that blue has a high 
32 
00:02:43,995 --> 00:02:48,490 
value of 50, whereas red and
green are low with 11 and 10. 
33 
00:02:48,490 --> 00:02:51,991 
If we view it at another corner,
by looking at 5000 0, 
34 
00:02:51,991 --> 00:02:53,610 
we'll see the same value. 
35 
00:03:01,136 --> 00:03:05,679 
If we go back to the image, the middle
of the image, which is the land, 
36 
00:03:05,679 --> 00:03:07,070 
is orange or yellow. 
37 
00:03:07,070 --> 00:03:12,261 
It's definitely not blue, so
let's take a look at a pixel value there. 
38 
00:03:12,261 --> 00:03:17,010 
Okay, run pixel.py
Australia.jpg 2000 2000. 
39 
00:03:23,750 --> 00:03:31,070 
This says that the red has a value of 118,
green is 89, and the blue is 57. 
40 
00:03:31,070 --> 00:03:34,130 
So the red and green are higher than blue,
so it's not ocean. 
1 
00:00:02,370 --> 00:00:05,969 
This is the second hands on exercises for
CSV data. 
2 
00:00:05,969 --> 00:00:10,155 
In the first, we saw how to import
a CSV file into a spreadsheet and 
3 
00:00:10,155 --> 00:00:12,430 
make a simple plot. 
4 
00:00:12,430 --> 00:00:15,240 
In this one,
we will learn how to filter data and 
5 
00:00:15,240 --> 00:00:16,830 
perform some aggregate operations. 
6 
00:00:18,390 --> 00:00:21,659 
We will begin by opening a terminal
window and starting a spreadsheet. 
7 
00:00:22,800 --> 00:00:26,440 
Next, we will load the CSV
data into the spreadsheet and 
8 
00:00:26,440 --> 00:00:29,490 
perform a filter over several columns. 
9 
00:00:29,490 --> 00:00:32,398 
Finally, we will calculate an average and
sum from the data. 
10 
00:00:32,398 --> 00:00:35,313 
Let's begin. 
11 
00:00:35,313 --> 00:00:39,470 
First, we open a terminal window by
clicking on the terminal icon in 
12 
00:00:39,470 --> 00:00:40,290 
the top toolbar. 
13 
00:00:42,770 --> 00:00:46,296 
We can start the spreadsheet
by writing oocalc. 
14 
00:00:51,338 --> 00:00:54,590 
Next, let's load our CSV
data into the spreadsheet. 
15 
00:00:54,590 --> 00:01:02,142 
We click on File > Open, CENSUS.CSV. 
16 
00:01:05,719 --> 00:01:08,426 
And click OK on this
dialog to load the data. 
17 
00:01:12,387 --> 00:01:17,344 
Column F in the spreadsheet is the state,
and 
18 
00:01:17,344 --> 00:01:22,704 
column H is the census population for
2010. 
19 
00:01:25,110 --> 00:01:28,480 
Let's create a filter that
just shows the data for 
20 
00:01:28,480 --> 00:01:32,920 
California, for
counties larger than one million people. 
21 
00:01:34,100 --> 00:01:39,030 
We can create this filter by first
selecting both the state name column and 
22 
00:01:39,030 --> 00:01:41,210 
the census 2000 population column. 
23 
00:01:42,790 --> 00:01:48,054 
Next we go to data,
filter, standard filter. 
24 
00:01:52,184 --> 00:01:56,025 
Here we change the field
name to be the state name, 
25 
00:01:56,025 --> 00:02:01,061 
the condition we leave at equals and
the value we use California. 
26 
00:02:06,318 --> 00:02:11,055 
This filters all the rows,
unless the state is California. 
27 
00:02:15,143 --> 00:02:16,498 
We then want to filter for 
28 
00:02:16,498 --> 00:02:20,590 
all the counties whose population
is greater than one million people. 
29 
00:02:20,590 --> 00:02:24,400 
To do that, in this second line here,
we change the operator to AND. 
30 
00:02:26,180 --> 00:02:31,710 
The field name should be
census 2010 population and 
31 
00:02:31,710 --> 00:02:35,040 
the condition should be greater than. 
32 
00:02:35,040 --> 00:02:36,431 
Then set the value to be one million. 
33 
00:02:41,923 --> 00:02:44,893 
And click OK. 
34 
00:02:44,893 --> 00:02:49,603 
We can see that all the data from the
spreadsheet has disappeared except where 
35 
00:02:49,603 --> 00:02:51,559 
the state name is California and 
36 
00:02:51,559 --> 00:02:54,762 
the population is greater
than one million people. 
37 
00:02:56,310 --> 00:03:00,110 
We can reset or remove this
filter to see all the data again 
38 
00:03:00,110 --> 00:03:04,240 
by going to Data > Filter. 
39 
00:03:06,260 --> 00:03:07,020 
Reset filter. 
40 
00:03:08,390 --> 00:03:11,550 
You can perform aggregate operations
on the data in a spreadsheet. 
41 
00:03:13,160 --> 00:03:16,830 
Next, let's perform some aggregate
operations over the data. 
42 
00:03:16,830 --> 00:03:19,220 
We can compute the average and
the sum of the data. 
43 
00:03:20,460 --> 00:03:23,980 
To do this, let's run these
calculations in a separate sheet. 
44 
00:03:25,470 --> 00:03:28,181 
Create a new sheet by clicking
on the green plus button. 
45 
00:03:31,854 --> 00:03:37,656 
To compute the average we select
a cell and enter =Average( and 
46 
00:03:37,656 --> 00:03:43,470 
then we select the data that we
want to compute the average from. 
47 
00:03:44,870 --> 00:03:46,150 
If we go back to sheet one. 
48 
00:03:46,150 --> 00:03:50,190 
We can select some of
the data from the H column. 
49 
00:03:50,190 --> 00:03:54,634 
So let's just choose several
counties in Alabama. 
50 
00:03:54,634 --> 00:03:58,267 
When we hit Enter,
it takes us back to sheet one and 
51 
00:03:58,267 --> 00:04:01,306 
we can see that the average is computing. 
52 
00:04:03,513 --> 00:04:12,220 
Similarly, we can compute the sum
by entering =sum, open parentheses. 
53 
00:04:12,220 --> 00:04:16,120 
Going back to sheet one and
selecting the columns we want to sum. 
54 
00:04:16,120 --> 00:04:21,360 
When we're done hit Enter and
the sum is computed. 
1 
00:00:02,360 --> 00:00:05,130 
In this hands on exercise,
we will be looking at JSON data. 
2 
00:00:07,040 --> 00:00:09,600 
First, we will open a terminal window and 
3 
00:00:09,600 --> 00:00:13,300 
then look at the contents of a JSON
file containing tweets from Twitter. 
4 
00:00:14,860 --> 00:00:17,800 
Next, we will examine
the schema of this JSON file. 
5 
00:00:18,810 --> 00:00:21,920 
Finally, we will extract different
fields from the JSON data. 
6 
00:00:24,340 --> 00:00:25,470 
Let's begin. 
7 
00:00:25,470 --> 00:00:27,307 
First, we'll open a terminal window, 
8 
00:00:27,307 --> 00:00:29,964 
by clicking on the terminal
icon at the top of the toolbar. 
9 
00:00:37,240 --> 00:00:42,005 
Next, we'll see cd into the directory
containing the json data, 
10 
00:00:42,005 --> 00:00:45,594 
by running cddownload/big/data/2/json. 
11 
00:00:52,298 --> 00:00:54,520 
We can run ls, to see the json file. 
12 
00:00:58,836 --> 00:01:04,030 
The json file is called twitter.json. 
13 
00:01:04,030 --> 00:01:07,980 
We can run more twitter.json to
view the contents of this file. 
14 
00:01:16,280 --> 00:01:20,990 
The json data contains semi-structured
data, which is nested several levels. 
15 
00:01:20,990 --> 00:01:25,350 
There are many tweets in this file, and
it's hard to read using the more command. 
16 
00:01:25,350 --> 00:01:29,690 
You can use space to scroll down and
when we're done, hit q. 
17 
00:01:32,420 --> 00:01:37,358 
We can run the jsonschema.pi command
to view the schema of this data. 
18 
00:01:37,358 --> 00:01:42,179 
We run jsonschema.pitwitter.json 
19 
00:01:49,715 --> 00:01:52,670 
And we'll add a pipe more at the end. 
20 
00:01:56,665 --> 00:02:00,572 
This shows the nested
fields within this data, 
21 
00:02:00,572 --> 00:02:06,139 
at the top level there are fields
like contributors text and id and so 
22 
00:02:06,139 --> 00:02:11,900 
on, but there are also fields nested
within these top level fields for 
23 
00:02:11,900 --> 00:02:17,372 
example entities also contains
called symbols media hashtags and 
24 
00:02:17,372 --> 00:02:20,691 
so on If we scroll down by hitting space, 
25 
00:02:20,691 --> 00:02:25,270 
we'll see that there's
several levels of nesting. 
26 
00:02:25,270 --> 00:02:32,320 
For example, user also has
follow_request_sent, id, and so on. 
27 
00:02:36,105 --> 00:02:40,466 
We can run the print json script to view
the contents of a particular tweet and 
28 
00:02:40,466 --> 00:02:42,746 
a particular field within that tweet. 
29 
00:02:42,746 --> 00:02:47,018 
Let's run print_json.py. 
30 
00:02:47,018 --> 00:02:50,812 
It asks for the file name so 
31 
00:02:50,812 --> 00:02:55,300 
we'll enter twitter.json. 
32 
00:02:58,407 --> 00:03:01,434 
And we'll look at tweet 99. 
33 
00:03:03,645 --> 00:03:06,340 
So let's look at the top
level field called text. 
34 
00:03:10,506 --> 00:03:14,884 
So here we see the text for
note the 99th tweet in this file. 
35 
00:03:14,884 --> 00:03:19,110 
We could also look at a nested field
within the file, by running print_json 
36 
00:03:19,110 --> 00:03:24,276 
again The file name is twitter.json. 
37 
00:03:26,975 --> 00:03:28,833 
We'll look at tweet 99 again. 
38 
00:03:32,097 --> 00:03:35,240 
And we'll look at the field
entities hashtags. 
39 
00:03:35,240 --> 00:03:39,565 
The hashtags that are embedded or
nested within the entities field. 
1 
00:00:02,390 --> 00:00:08,427 
This is the first of two hands
on exercises involving CSV Data. 
2 
00:00:08,427 --> 00:00:12,568 
In this exercise, we will import
a CSV file into a spreadsheet and 
3 
00:00:12,568 --> 00:00:13,860 
make a simple plot. 
4 
00:00:15,290 --> 00:00:20,350 
We will begin by opening a terminal window
and looking at a CSV file in the terminal. 
5 
00:00:20,350 --> 00:00:23,330 
Next, we will start
the spreadsheet application and 
6 
00:00:23,330 --> 00:00:25,410 
import the CSV data into the spreadsheet. 
7 
00:00:25,410 --> 00:00:30,760 
We can then look at the rows and columns
of the CSV file and make a simple plot. 
8 
00:00:33,088 --> 00:00:35,837 
Let's begin, first,
open a terminal shell by 
9 
00:00:35,837 --> 00:00:39,423 
clicking on the black terminal
icon at the top of the toolbar. 
10 
00:00:47,759 --> 00:00:52,914 
Next, let's cd into the directory
containing the CSV data. 
11 
00:00:52,914 --> 00:00:58,587 
We'll run cd space
Download/big-data-2/csv. 
12 
00:00:58,587 --> 00:01:04,042 
We can run ls to 
13 
00:01:04,042 --> 00:01:11,325 
see the CSV files. 
14 
00:01:11,325 --> 00:01:16,190 
The file census.csv contains
census data for the United States. 
15 
00:01:16,190 --> 00:01:21,367 
We can run the command more census.csv
to see the contents of this file. 
16 
00:01:29,262 --> 00:01:32,925 
The first line of this file is the header
with the columns separated by commas. 
17 
00:01:32,925 --> 00:01:37,705 
You can go down in
the file by hitting space. 
18 
00:01:41,295 --> 00:01:42,735 
Hit Q to quit more. 
19 
00:01:45,570 --> 00:01:48,210 
Next, let's start
a spreadsheet application. 
20 
00:01:48,210 --> 00:01:52,446 
We run oocalc to start this. 
21 
00:02:00,318 --> 00:02:04,453 
We can import the census
data CSV file into 
22 
00:02:04,453 --> 00:02:08,598 
the spreadsheet by going to File > Open. 
23 
00:02:12,568 --> 00:02:16,067 
Clicking on downloads. 
24 
00:02:16,067 --> 00:02:22,468 
Big Data 2, CSV, census.csv. 
25 
00:02:27,531 --> 00:02:29,109 
In this dialog click OK. 
26 
00:02:44,945 --> 00:02:47,355 
You can see into this spreadsheet, 
27 
00:02:47,355 --> 00:02:50,900 
import of the CSV data to
a bunch of rows and columns. 
28 
00:02:52,880 --> 00:02:56,460 
Each column that was separated
by a comma in the CSV file, 
29 
00:02:56,460 --> 00:02:58,580 
is a column in the spreadsheet. 
30 
00:02:59,700 --> 00:03:02,590 
We can see that our CSV file
was successfully imported into 
31 
00:03:02,590 --> 00:03:03,250 
the spreadsheet. 
32 
00:03:05,650 --> 00:03:07,892 
If we scroll down to
the bottom of the spreadsheet, 
33 
00:03:07,892 --> 00:03:10,094 
we can see how many rows
there were in the CSV file. 
34 
00:03:19,939 --> 00:03:25,372 
There are 3194 rows in the CSV file. 
35 
00:03:25,372 --> 00:03:29,149 
If this file instead had millions or
10 millions of rows, 
36 
00:03:29,149 --> 00:03:33,237 
then we would have to use a big
data system such as Hadoop or HDFS. 
37 
00:03:36,882 --> 00:03:38,089 
Let's scroll back to the top. 
38 
00:03:43,592 --> 00:03:49,351 
Next, let's make a simple plot of
some of the data in the CSV file. 
39 
00:03:49,351 --> 00:03:54,993 
Let's plot the population estimates for
several years for the state of Alabama. 
40 
00:03:54,993 --> 00:04:00,558 
The state of Alabama is
given in the second row and 
41 
00:04:00,558 --> 00:04:06,813 
the population estimates
are given in these columns. 
42 
00:04:06,813 --> 00:04:12,225 
Let's select J through O,
so you get the population 
43 
00:04:12,225 --> 00:04:16,657 
estimate for 2010 through 2015. 
44 
00:04:20,086 --> 00:04:24,331 
We can create a plot of these values
by clicking on the Chart button. 
45 
00:04:29,393 --> 00:04:32,806 
And clicking finish. 
46 
00:04:32,806 --> 00:04:36,442 
In the second hands on for CSV data,
we'll perform some filtering and 
47 
00:04:36,442 --> 00:04:38,669 
some aggregate operations over the data. 
1 
00:00:02,170 --> 00:00:02,670 
Welcome. 
2 
00:00:03,800 --> 00:00:06,200 
In this module,
we'll talk about data models. 
3 
00:00:07,530 --> 00:00:10,770 
If you completed the introductory
course of this specialization, 
4 
00:00:10,770 --> 00:00:12,940 
you might recall our
video on data variety. 
5 
00:00:14,220 --> 00:00:16,840 
One way to characterize data variety 
6 
00:00:16,840 --> 00:00:21,290 
is to identify the different models of
data that are used in any application. 
7 
00:00:23,120 --> 00:00:24,480 
So what is a data model? 
8 
00:00:24,480 --> 00:00:27,980 
And why do we care about data
models in the context of big data? 
9 
00:00:27,980 --> 00:00:32,010 
In this lesson, we'll introduce you to
three components of a data model and 
10 
00:00:32,010 --> 00:00:33,380 
what they tell us about the data. 
11 
00:00:34,550 --> 00:00:39,620 
So after this lesson, you'll be able
to distinguish between structured and 
12 
00:00:39,620 --> 00:00:41,270 
unstructured data. 
13 
00:00:41,270 --> 00:00:46,880 
Describe four basic data operations namely
selection, projection, union, and join. 
14 
00:00:46,880 --> 00:00:51,120 
And enumerate different types of data
constraints like type, value and 
15 
00:00:51,120 --> 00:00:53,240 
structural constraints. 
16 
00:00:53,240 --> 00:00:57,760 
You'll also be able to explain why
constraints are useful to specify 
17 
00:00:57,760 --> 00:01:00,320 
the semantics of data. 
18 
00:01:00,320 --> 00:01:04,980 
Now, regardless of whether the data is big
or small, one needs to know or determine 
19 
00:01:04,980 --> 00:01:10,100 
the characteristics of data before one can
manipulate or analyze them meaningfully. 
20 
00:01:10,100 --> 00:01:14,320 
Let's use a simple example,
suppose you have data is 
21 
00:01:14,320 --> 00:01:19,330 
a file of records with fields called first
name, last name and date of birth of 
22 
00:01:19,330 --> 00:01:24,840 
the employees in the company that this
file consists of records with fields, 
23 
00:01:24,840 --> 00:01:29,290 
and not for instance plain text,
gives us more insight 
24 
00:01:29,290 --> 00:01:34,770 
into the organization of the data in the
file and hence is part of the data model. 
25 
00:01:34,770 --> 00:01:37,810 
This aspect is called Structure. 
26 
00:01:37,810 --> 00:01:41,930 
Similarly, the consideration
that we can perform 
27 
00:01:41,930 --> 00:01:44,980 
data arithmetic with
the date of birth field, and 
28 
00:01:44,980 --> 00:01:50,340 
not with the first name field, is also
part of our understanding of data model. 
29 
00:01:50,340 --> 00:01:52,620 
These are called Operations. 
30 
00:01:52,620 --> 00:01:57,300 
Finally, we may know that in
this company no one's age 
31 
00:01:57,300 --> 00:02:00,950 
that is today's date minus the date
of birth, can't be less than 18. 
32 
00:02:00,950 --> 00:02:06,000 
So it gives us a way to detect records
with blatantly erroneous dates of birth. 
33 
00:02:06,000 --> 00:02:07,940 
In the following three videos, 
34 
00:02:07,940 --> 00:02:11,790 
we'll look at these three aspects
of data models more carefully. 
1 
00:00:01,120 --> 00:00:05,140 
We mentioned before that a data model
is characterized by the structure of 
2 
00:00:05,140 --> 00:00:09,430 
the data that it admits,
the operations on that structure, and 
3 
00:00:09,430 --> 00:00:11,090 
a way to specify constraints. 
4 
00:00:12,290 --> 00:00:13,750 
In this lesson, 
5 
00:00:13,750 --> 00:00:18,950 
we'll present a more detailed description
of a number of common data models. 
6 
00:00:18,950 --> 00:00:20,800 
We'll start with relational data. 
7 
00:00:22,460 --> 00:00:26,780 
It is one of the simplest and most
frequently used data models today, and 
8 
00:00:26,780 --> 00:00:30,320 
forms the basis of many other
traditional database management systems, 
9 
00:00:30,320 --> 00:00:33,930 
like MySQL, Oracle,
Teradata, and so forth. 
10 
00:00:35,480 --> 00:00:38,940 
So after this video you'll be able to 
11 
00:00:38,940 --> 00:00:41,680 
describe the structural components
of a relational data model. 
12 
00:00:43,320 --> 00:00:46,430 
Demonstrate which components
become a data model's schema. 
13 
00:00:48,760 --> 00:00:51,760 
Explain the purpose of primary and
foreign keys. 
14 
00:00:52,770 --> 00:00:55,370 
And describe Join and other operations. 
15 
00:00:58,070 --> 00:00:59,610 
The primary data structure for 
16 
00:00:59,610 --> 00:01:03,380 
a relational model is a table, like
the one shown here for a toy application. 
17 
00:01:05,030 --> 00:01:09,740 
But we need to be careful about relational
tables, which are also called relations. 
18 
00:01:11,110 --> 00:01:15,695 
This table actually
represents a set of tuples. 
19 
00:01:17,550 --> 00:01:22,390 
This is a relational tuple,
represented as a row in the table. 
20 
00:01:23,500 --> 00:01:26,140 
We were informally calling
this a record before. 
21 
00:01:27,350 --> 00:01:33,110 
But a relational tuple implies that unless
otherwise stated, the elements of it 
22 
00:01:33,110 --> 00:01:37,600 
like 203 or 204, Mary and
so forth, are atomic. 
23 
00:01:38,950 --> 00:01:43,300 
That is,
they represent one unit of information and 
24 
00:01:43,300 --> 00:01:44,740 
cannot be decomposed further. 
25 
00:01:45,830 --> 00:01:48,150 
We'll return to this issue
in the next few slides. 
26 
00:01:49,490 --> 00:01:53,935 
Thus, this is a relation of six tuples. 
27 
00:01:55,468 --> 00:01:58,490 
Remember, the definition of sets, 
28 
00:01:58,490 --> 00:02:03,270 
it's a collection of distinct
elements of the same type. 
29 
00:02:04,420 --> 00:02:10,920 
That means I cannot add
this tuple to the solution. 
30 
00:02:10,920 --> 00:02:14,350 
Because if I do,
it will be introducing a duplicate. 
31 
00:02:15,820 --> 00:02:21,330 
Now in practice, many systems will allow
duplicate tuples in a relation, but 
32 
00:02:21,330 --> 00:02:26,450 
mechanisms are provided to prevent
duplicate entries if the user so chooses. 
33 
00:02:26,450 --> 00:02:28,170 
So I cannot add it. 
34 
00:02:28,170 --> 00:02:31,170 
Here is another tuple I cannot add. 
35 
00:02:31,170 --> 00:02:33,680 
It has all the right
pieces of information, but 
36 
00:02:33,680 --> 00:02:35,410 
it's all in the wrong order. 
37 
00:02:35,410 --> 00:02:39,300 
So it is a tuple dissimilar with
the other six tuples in the relation. 
38 
00:02:40,300 --> 00:02:43,690 
Okay, so how does the system know
that this tuple is different? 
39 
00:02:45,390 --> 00:02:49,700 
This brings our attention to the very
first row that is the header of this table 
40 
00:02:49,700 --> 00:02:50,710 
painted in black. 
41 
00:02:52,680 --> 00:02:55,910 
This row is part of
the scheme of the table. 
42 
00:02:55,910 --> 00:02:56,420 
Lets look at it. 
43 
00:02:57,890 --> 00:03:01,190 
It tells us the name of the table,
in this case employee. 
44 
00:03:02,580 --> 00:03:06,560 
This also tells us the names of the six
columns called attributes of the relation. 
45 
00:03:07,740 --> 00:03:09,090 
And for each column, 
46 
00:03:09,090 --> 00:03:13,370 
it tells us the allowed data type, that
is the type constraint for each column. 
47 
00:03:14,610 --> 00:03:16,340 
Given this schema, 
48 
00:03:16,340 --> 00:03:20,890 
it should now be clear why the last
red row does not belong to this table. 
49 
00:03:21,940 --> 00:03:26,180 
The schema in a relational table
can also specify constraints, 
50 
00:03:27,330 --> 00:03:30,150 
shown in yellow in the third
line of the schema row. 
51 
00:03:31,720 --> 00:03:36,050 
It says that the minimum salary of
a person has to be greater than 25k. 
52 
00:03:38,150 --> 00:03:44,670 
Further, it states that every employee
must have a first and last name. 
53 
00:03:44,670 --> 00:03:46,660 
They cannot be left null,
that means without a value. 
54 
00:03:48,840 --> 00:03:51,900 
Why doesn't department or
title column have this constraint? 
55 
00:03:53,500 --> 00:03:57,990 
One answer can be that a newly
hired employee may not be assigned 
56 
00:03:57,990 --> 00:04:01,350 
a department or a title yet but
can still be an entry in the table. 
57 
00:04:02,560 --> 00:04:06,040 
However, the department column
has another constraint. 
58 
00:04:07,130 --> 00:04:11,390 
It restricts the possible values
that is the domain of the attribute 
59 
00:04:11,390 --> 00:04:14,060 
to only four possibilities. 
60 
00:04:14,060 --> 00:04:17,900 
HR, IT, research, and business. 
61 
00:04:19,620 --> 00:04:24,840 
Finally, the first says
that ID is a primary key. 
62 
00:04:26,180 --> 00:04:29,960 
This means it is unique for each employee. 
63 
00:04:29,960 --> 00:04:33,200 
And for
every employee knowing the primary key for 
64 
00:04:33,200 --> 00:04:38,720 
the employee will also uniquely know the
other five attributes of that employee. 
65 
00:04:40,180 --> 00:04:43,240 
You should now see that
a table with a primary key 
66 
00:04:43,240 --> 00:04:48,590 
logically implies that the table cannot
have a duplicate record because if we do, 
67 
00:04:48,590 --> 00:04:51,870 
it will violate the uniqueness constraint
associated with the primary key. 
68 
00:04:53,330 --> 00:04:57,140 
Let us introduce a new table containing
the salary history of employees. 
69 
00:04:58,410 --> 00:05:02,010 
The employees are identified
with the column EmpID, but 
70 
00:05:02,010 --> 00:05:04,760 
these are not new values that
this table happens to have. 
71 
00:05:05,900 --> 00:05:10,460 
They are the same IDs that are present in
the ID column of the employee's table, 
72 
00:05:10,460 --> 00:05:11,140 
presented earlier. 
73 
00:05:13,420 --> 00:05:15,570 
This is reflected in
the statement made on the right. 
74 
00:05:16,760 --> 00:05:23,530 
The term References means,
the values in this column can exist 
75 
00:05:23,530 --> 00:05:28,760 
only if the same values if you are in
employees the table being referenced, 
76 
00:05:28,760 --> 00:05:30,700 
also called the parent table. 
77 
00:05:32,150 --> 00:05:38,920 
So in the terminology of the relational
model, the EmpID column of EmpSalaries 
78 
00:05:38,920 --> 00:05:44,660 
table is called a foreign key that refers
to the primary key of the Employees table. 
79 
00:05:46,560 --> 00:05:52,650 
Note that EmpID is not a primary
key in this EmpSalaries table. 
80 
00:05:52,650 --> 00:05:56,420 
Because it is multiple to
post with the same EmpID 
81 
00:05:56,420 --> 00:05:58,920 
reflecting the salary of
the employee at different times. 
82 
00:06:00,700 --> 00:06:04,070 
You will remember join is a common
operation that we discussed before. 
83 
00:06:05,150 --> 00:06:09,200 
So here is an example of
a relational join performed 
84 
00:06:09,200 --> 00:06:13,810 
on the first three columns of employee and
EmpSalaries table. 
85 
00:06:13,810 --> 00:06:20,630 
Where employees.ID, and EmpSalaries.EmpID
columns are matched for equality. 
86 
00:06:21,770 --> 00:06:24,130 
The output table shows
all the columns involved. 
87 
00:06:25,360 --> 00:06:27,360 
The common column is represented once. 
88 
00:06:28,680 --> 00:06:31,210 
This form of join is
called a Natural Join. 
89 
00:06:32,690 --> 00:06:37,770 
It is important to understand that
join is one of the most expensive 
90 
00:06:37,770 --> 00:06:41,570 
that means time consuming and
space consuming operations. 
91 
00:06:42,770 --> 00:06:48,270 
As data becomes larger, and tables contain
hundreds of millions of tuples, the join 
92 
00:06:48,270 --> 00:06:52,480 
operation can easily become a bottleneck
in a larger analytic application. 
93 
00:06:53,880 --> 00:06:59,280 
So for analytical big data application
that needs joins, it's very important to 
94 
00:06:59,280 --> 00:07:04,890 
choose a suitable data management platform
that makes this operation efficient. 
95 
00:07:04,890 --> 00:07:07,090 
We will return to this
issue in module four. 
96 
00:07:08,760 --> 00:07:10,600 
We end this video on a practical note. 
97 
00:07:11,830 --> 00:07:13,060 
In many scientific and 
98 
00:07:13,060 --> 00:07:16,990 
business applications,
people start with CSV files, 
99 
00:07:16,990 --> 00:07:21,650 
manipulate them with the spreadsheet,
then migrate their relational system only 
100 
00:07:21,650 --> 00:07:25,790 
as an afterthought where the data becomes
too large to handle the spreadsheet. 
101 
00:07:27,490 --> 00:07:30,500 
While the spreadsheet offers
many useful features. 
102 
00:07:30,500 --> 00:07:36,037 
It does not conform and enforce many
principles of relational data models. 
103 
00:07:37,700 --> 00:07:42,210 
Consequently, a large amount of time
may be spent in cleaning up and 
104 
00:07:42,210 --> 00:07:45,480 
correcting data errors after
the migration actually happens. 
105 
00:07:46,670 --> 00:07:51,860 
Let me show a few examples from
a spreadsheet that has 125,000 rows and 
106 
00:07:51,860 --> 00:07:52,430 
over 100 columns. 
107 
00:07:53,950 --> 00:07:59,280 
The spreadsheet here lists terrorism
attacks gathered from news media. 
108 
00:07:59,280 --> 00:08:02,030 
So each row represents one attack. 
109 
00:08:03,080 --> 00:08:07,210 
This is a valuable piece of data for
people who study terrorism. 
110 
00:08:07,210 --> 00:08:10,420 
But we are going to look at it from
a relational data modelling viewpoint. 
111 
00:08:12,200 --> 00:08:15,160 
First, notice the column marked in green. 
112 
00:08:16,750 --> 00:08:20,900 
It lists two weapons used in
the attack separated by a semicolon. 
113 
00:08:22,330 --> 00:08:24,390 
Why is this really common? 
114 
00:08:24,390 --> 00:08:26,950 
It makes this column non-atomic. 
115 
00:08:26,950 --> 00:08:31,450 
It means that this column actually
has two different values. 
116 
00:08:32,570 --> 00:08:37,140 
In a relational design, this information
will be moved to another table 
117 
00:08:37,140 --> 00:08:40,870 
just like the multiple salaries of
employees were placed in a separate table. 
118 
00:08:42,100 --> 00:08:45,740 
Next, notice the column outlined in red. 
119 
00:08:47,290 --> 00:08:52,370 
It describes the amount of property
damaged by a possible terrorist attack. 
120 
00:08:53,890 --> 00:08:59,880 
In this column, the intended
legitimate values are unknown, 
121 
00:08:59,880 --> 00:09:01,540 
minor, major and catastrophic. 
122 
00:09:02,670 --> 00:09:07,350 
However, the value in the highlighted
part of the spreadsheet is minor and 
123 
00:09:07,350 --> 00:09:12,012 
then within bracket,
likely less than $1 million. 
124 
00:09:12,012 --> 00:09:16,700 
Which means a query like
find all attacks for 
125 
00:09:16,700 --> 00:09:20,390 
which the property damage is equal to
minor cannot be answered directly. 
126 
00:09:21,680 --> 00:09:25,150 
Instead, we need to perform
a substring search for 
127 
00:09:25,150 --> 00:09:27,630 
minor in the beginning of the description. 
128 
00:09:27,630 --> 00:09:30,420 
Which is doable, but
it's a more expensive operation. 
129 
00:09:31,960 --> 00:09:35,730 
This shows the columns of the spreadsheet. 
130 
00:09:35,730 --> 00:09:37,950 
So this is part of the schema of the data. 
131 
00:09:39,120 --> 00:09:42,310 
If you observe carefully you
will see a recurring pattern. 
132 
00:09:44,120 --> 00:09:49,290 
The designer of the data table determined
that there can be at most three types of 
133 
00:09:49,290 --> 00:09:54,380 
attacks within a single encounter and
represented with three separate columns. 
134 
00:09:55,790 --> 00:10:01,220 
Now in proper relational modeling,
one would say that there is a one to many 
135 
00:10:01,220 --> 00:10:06,650 
relationship between the attack and
the number of attack types. 
136 
00:10:08,210 --> 00:10:10,980 
In such a case, it would be more prudent 
137 
00:10:10,980 --> 00:10:15,030 
to place these attack type
columns in a separate table and 
138 
00:10:15,030 --> 00:10:19,480 
connect with the parent using a primary
key, foreign key relationship. 
139 
00:10:20,920 --> 00:10:24,240 
Here's another block,
with a similar pattern, 
140 
00:10:25,490 --> 00:10:29,420 
this time this is about the types and
subtypes of weapons used. 
141 
00:10:30,620 --> 00:10:33,690 
Now can you determine how you might
be able to reorganize this block? 
142 
00:10:35,130 --> 00:10:36,761 
We'll leave this as an exercise. 
1 
00:00:03,030 --> 00:00:05,956 
It can be said without a doubt,
and the Internet and 
2 
00:00:05,956 --> 00:00:08,960 
the worldwide web changed
everything in our lives. 
3 
00:00:10,450 --> 00:00:15,100 
The worldwide web is indeed the largest
information source there is today. 
4 
00:00:15,100 --> 00:00:16,630 
But what's the data model behind the web? 
5 
00:00:17,800 --> 00:00:20,740 
We will say that it is
the semi-structure data model. 
6 
00:00:22,310 --> 00:00:26,560 
So after going through this video you
will be able to distinguish between 
7 
00:00:26,560 --> 00:00:29,870 
the structured data model that we
talked about the last time and 
8 
00:00:29,870 --> 00:00:32,805 
semi-structured data model. 
9 
00:00:32,805 --> 00:00:36,110 
Further, you will recognize that the most 
10 
00:00:36,110 --> 00:00:40,680 
times the semi-structured data
refers to tree structured data. 
11 
00:00:41,720 --> 00:00:47,160 
And you can explain why tree
navigation operations are important for 
12 
00:00:47,160 --> 00:00:49,370 
formats like XML and JSON. 
13 
00:00:51,050 --> 00:00:53,290 
Let's a take a very simple web page. 
14 
00:00:55,090 --> 00:00:58,740 
Now this page does not have
a lot of content or stylization. 
15 
00:00:58,740 --> 00:01:01,970 
It doesn't even have links to other pages,
but 
16 
00:01:01,970 --> 00:01:04,390 
let's look at the corresponding HTML code. 
17 
00:01:06,560 --> 00:01:11,340 
This code is used by the browser so
that it can render the HTML, and 
18 
00:01:11,340 --> 00:01:13,580 
notice a few things in this data. 
19 
00:01:13,580 --> 00:01:19,710 
The entire data comes within the HTML and
slash HTML blocks. 
20 
00:01:21,760 --> 00:01:28,590 
And we similarly have a body begin and
end, a header begin and 
21 
00:01:28,590 --> 00:01:33,950 
end, a list begin and end and
a paragraph begin and end. 
22 
00:01:36,020 --> 00:01:40,330 
Everywhere here a block is
nested within a larger block. 
23 
00:01:41,770 --> 00:01:46,350 
The second item to notice is that
unlike a relational structure 
24 
00:01:46,350 --> 00:01:49,940 
there are multiple list items and
multiple paragraphs. 
25 
00:01:49,940 --> 00:01:53,040 
And any single document would
have a different number of them. 
26 
00:01:54,100 --> 00:01:59,250 
This means while the date object has
some structure it is more flexible. 
27 
00:02:00,360 --> 00:02:05,930 
So this is the hallmark office
semi structure date model. 
28 
00:02:05,930 --> 00:02:07,020 
Now XML, or 
29 
00:02:07,020 --> 00:02:12,100 
the extensible markup language, is another
well known standard to represent data. 
30 
00:02:12,100 --> 00:02:16,880 
You can think of XML as a generalization
of HTML where the elements, that's 
31 
00:02:16,880 --> 00:02:21,270 
the beginning and end markers within
the angular brackets, can be any string. 
32 
00:02:21,270 --> 00:02:24,240 
And not like the ones
allowed by standard HTML. 
33 
00:02:25,530 --> 00:02:28,840 
Let's see an example
from a biological case. 
34 
00:02:30,140 --> 00:02:34,770 
As you can see, there are two
elements called sample attribute. 
35 
00:02:36,100 --> 00:02:39,130 
They do structurally
different because they 
36 
00:02:39,130 --> 00:02:42,320 
have different numbers of sub
elements called the value. 
37 
00:02:44,030 --> 00:02:46,960 
Another interesting issue
about XML data processing 
38 
00:02:46,960 --> 00:02:50,530 
is that you can actually credit for
the structure elements. 
39 
00:02:50,530 --> 00:02:55,560 
For example, it is perfectly fine to ask,
what is the name of the element 
40 
00:02:55,560 --> 00:02:59,250 
which contains a sub-element whose
textual content is cell type? 
41 
00:03:00,390 --> 00:03:06,990 
As you can see, you'll get two results,
sample attribute. 
42 
00:03:06,990 --> 00:03:11,604 
An experimental factor because sample
attribute has a sub-element called 
43 
00:03:11,604 --> 00:03:15,940 
category and experimental factor
has a subelement called link and 
44 
00:03:15,940 --> 00:03:18,860 
each of these subelements
have the value celltape. 
45 
00:03:20,090 --> 00:03:23,860 
Now we cannot perform an operation
like this in a relational data model. 
46 
00:03:23,860 --> 00:03:28,090 
For example, we cannot say which relation
has a column with a value, John. 
47 
00:03:30,730 --> 00:03:35,740 
The same idea can also be seen in JSON or
the Java Script Object Notation, which 
48 
00:03:35,740 --> 00:03:41,610 
is a very popular format used for many
different data like Twitter and Facebook. 
49 
00:03:41,610 --> 00:03:47,170 
Consider the example here,
all of the format looks different. 
50 
00:03:47,170 --> 00:03:50,390 
We have a similar nested
structure varies that is lists 
51 
00:03:50,390 --> 00:03:55,030 
containing other lists which will contain
topples Which consists of p value ps. 
52 
00:03:57,080 --> 00:04:01,160 
So the key value pairs at atomic
property names and their values. 
53 
00:04:02,660 --> 00:04:07,760 
But one way to generalize about all these
different forms of semi structured data 
54 
00:04:07,760 --> 00:04:10,460 
is to model them as trees. 
55 
00:04:10,460 --> 00:04:12,380 
Let's go back to .xml. 
56 
00:04:12,380 --> 00:04:15,210 
The left side shows an XML document, and 
57 
00:04:15,210 --> 00:04:17,440 
the right side shows
the corresponding tree. 
58 
00:04:18,660 --> 00:04:22,030 
Since the top object of
the root element is document, 
59 
00:04:22,030 --> 00:04:23,230 
it is also the root of the tree. 
60 
00:04:24,550 --> 00:04:28,620 
Now under document we have
a report element with author and 
61 
00:04:28,620 --> 00:04:34,450 
date under it, and also a paper element
with title, author, and source under it. 
62 
00:04:34,450 --> 00:04:38,670 
The actual values,
like is the textual content of an element. 
63 
00:04:40,480 --> 00:04:43,760 
Since a text data item cannot
have any further components, 
64 
00:04:43,760 --> 00:04:46,590 
these text values are always
the leaves of the tree. 
65 
00:04:48,760 --> 00:04:53,950 
Now, modeling a document as a tree
has significant advantages. 
66 
00:04:53,950 --> 00:04:55,740 
A tree is a well-known data structure, 
67 
00:04:55,740 --> 00:05:00,100 
that allows what's called
a navigational access to data. 
68 
00:05:00,100 --> 00:05:02,150 
Imagine you are standing
on the note paper. 
69 
00:05:03,460 --> 00:05:08,610 
Now you can perform a getParent
operation and navigate the document. 
70 
00:05:08,610 --> 00:05:14,460 
Or you can perform a getChildren operation
to get to the title, author and source. 
71 
00:05:14,460 --> 00:05:18,550 
You can even perform a getSiblings
operation and get to the report. 
72 
00:05:19,810 --> 00:05:24,970 
You can also ask a textual query like
which strings have the substring data and 
73 
00:05:24,970 --> 00:05:31,340 
seek their root-to-node path to get to
the path from document to the text nodes. 
74 
00:05:31,340 --> 00:05:34,590 
You can possibly see how queries
can be evaluated on the tree, 
75 
00:05:35,670 --> 00:05:37,250 
now let us take the query. 
76 
00:05:37,250 --> 00:05:39,990 
Who is the author of XML query data model. 
77 
00:05:41,900 --> 00:05:47,987 
In one evaluation scheme we can navigate
up from the text note to title, 
78 
00:05:47,987 --> 00:05:53,580 
to paper, and then navigate down
to author and then to Don Robie. 
79 
00:05:53,580 --> 00:05:56,990 
Well how do we know that we have to get up
to paper before reversing the direction? 
80 
00:05:58,050 --> 00:06:01,470 
Well, paper is the least,
that's the lowest in the tree, 
81 
00:06:01,470 --> 00:06:07,170 
common ancestor of the author note,
and the XM query data model note. 
82 
00:06:07,170 --> 00:06:10,207 
We will come back to semi
structure data in a later module. 
1 
00:00:01,851 --> 00:00:05,973 
In this hands-on activity,
we will be looking at graph data in Gephi. 
2 
00:00:05,973 --> 00:00:10,510 
First, we will import data into Gephi and
then examine the properties of the graph. 
3 
00:00:11,720 --> 00:00:15,160 
Next, we will perform some statistical
operations on the graph data, 
4 
00:00:15,160 --> 00:00:17,750 
and then run some different
layout algorithms. 
5 
00:00:20,090 --> 00:00:21,585 
Let's begin. 
6 
00:00:21,585 --> 00:00:26,370 
We're not running Gephi in the Cloudera
virtual machine, on the Coursera website, 
7 
00:00:26,370 --> 00:00:30,450 
there will be a reading with instructions
on how to download, install, and 
8 
00:00:30,450 --> 00:00:34,050 
run Gephi on your native hardware,
instead of in the virtual machine. 
9 
00:00:35,960 --> 00:00:39,693 
Once you have Gephi started,
let's import the data into Gephi. 
10 
00:00:39,693 --> 00:00:46,908 
We'll go to File Import spreadsheet,
and in the CSV dialog, 
11 
00:00:46,908 --> 00:00:52,338 
we'll click the button with dot, dot, dot. 
12 
00:00:52,338 --> 00:00:57,679 
We'll choose diseasegraph.csv and
click open. 
13 
00:01:01,336 --> 00:01:06,420 
Make sure that as table says
edges table Click next. 
14 
00:01:09,570 --> 00:01:13,535 
And make sure Create
missing nodes is checked. 
15 
00:01:13,535 --> 00:01:17,215 
We'll click finish to import
the CSV file as a graph. 
16 
00:01:17,215 --> 00:01:19,630 
Gephi now shows the graph
in the center pane. 
17 
00:01:21,270 --> 00:01:23,930 
The little black circles
are the nodes of the graph and 
18 
00:01:23,930 --> 00:01:26,500 
the lines between them are the edges. 
19 
00:01:26,500 --> 00:01:32,707 
In the top right we can see that
there are 777 nodes and 998 edges. 
20 
00:01:32,707 --> 00:01:36,380 
Next let's perform some statistical
operations on this graph. 
21 
00:01:37,510 --> 00:01:42,145 
In the statistics pane we
can see average degree. 
22 
00:01:42,145 --> 00:01:44,980 
Let's compute the average degree
of the graph by clicking on Run. 
23 
00:01:49,320 --> 00:01:53,260 
This says that the average
degree is 2.569. 
24 
00:01:53,260 --> 00:01:58,693 
Let's close this,
let's compute the connected 
25 
00:01:58,693 --> 00:02:02,583 
components, we'll click on Run. 
26 
00:02:02,583 --> 00:02:05,400 
We'll leave this as a directed,
since the graph is directed. 
27 
00:02:07,140 --> 00:02:09,930 
Click OK.
It says that there 
28 
00:02:09,930 --> 00:02:14,940 
are 5 weakly connected components,
and 761 strongly connected components. 
29 
00:02:16,440 --> 00:02:17,400 
Let's close this. 
30 
00:02:18,470 --> 00:02:21,710 
Next, let's run some different
layout algorithms over the graph. 
31 
00:02:23,240 --> 00:02:25,760 
The bottom left,
we'll go to choose layout. 
32 
00:02:27,550 --> 00:02:32,019 
We'll choose Force Atlas and click run. 
33 
00:02:50,345 --> 00:02:52,080 
Click stop to stop the layout. 
34 
00:02:53,655 --> 00:02:58,200 
We can see that Gephi has grouped
strongly connected components together 
35 
00:02:58,200 --> 00:02:59,320 
in different clusters. 
36 
00:02:59,320 --> 00:03:01,920 
We can also see that they're parts
of the graph that are not connected. 
37 
00:03:03,440 --> 00:03:04,830 
Let's run a different layout algorithm. 
38 
00:03:06,080 --> 00:03:11,395 
The combo box,
choose Fruchterman Reingold, click run. 
39 
00:03:18,234 --> 00:03:23,125 
After it runs for a few seconds, click
stop, then click the magnifying glass, 
40 
00:03:23,125 --> 00:03:25,680 
center on graph, to see the whole graph. 
41 
00:03:28,320 --> 00:03:31,700 
In this layout,
all the nodes appear to be equally spaced. 
42 
00:03:31,700 --> 00:03:34,537 
But we can also see
the nodes with many edges. 
1 
00:00:01,350 --> 00:00:04,350 
In this hands on activity,
we will be working with Lucene, 
2 
00:00:04,350 --> 00:00:07,310 
a search engine that uses a vector
space model to index data. 
3 
00:00:08,430 --> 00:00:10,580 
First, we will open a terminal window and 
4 
00:00:10,580 --> 00:00:14,200 
change into the directory
containing the data and scripts. 
5 
00:00:14,200 --> 00:00:18,700 
Next, we will index some text
documents and query terms in Lucene. 
6 
00:00:19,960 --> 00:00:22,538 
After that we'll query
using weighted terms or 
7 
00:00:22,538 --> 00:00:25,810 
boosting to see how this
changes the rankings. 
8 
00:00:25,810 --> 00:00:30,045 
Finally, we will show the term
frequency-inverse document frequency or 
9 
00:00:30,045 --> 00:00:32,988 
TF-IDF in terms. 
10 
00:00:32,988 --> 00:00:35,300 
Let's begin. 
11 
00:00:35,300 --> 00:00:39,139 
First, let's open a terminal window by
clicking on the terminal icon at the top 
12 
00:00:39,139 --> 00:00:39,953 
of the tool bar. 
13 
00:00:43,413 --> 00:00:47,090 
Next, let's cd into the directory
containing the scripts and data. 
14 
00:00:47,090 --> 00:00:48,679 
We'll run cd 
15 
00:00:48,679 --> 00:00:58,560 
Downloads/big-data-2/vector/. 
16 
00:00:58,560 --> 00:01:02,201 
We'll run ls to see the scripts. 
17 
00:01:06,543 --> 00:01:09,670 
The data directory
contains three text files. 
18 
00:01:09,670 --> 00:01:13,310 
Each of these text files contains
news data about elections. 
19 
00:01:15,730 --> 00:01:21,497 
Let's index these files by
running runLuceneQuery.sh data. 
20 
00:01:28,974 --> 00:01:35,740 
Next, let's query Lucene for some terms in
these text documents and see the rankings. 
21 
00:01:35,740 --> 00:01:37,970 
Let's query for the term voters. 
22 
00:01:43,242 --> 00:01:47,973 
You can see the rankings and
scores that news1.csv ranked first, 
23 
00:01:47,973 --> 00:01:52,900 
news2.csv was the second ranking,
and the third was news3.csv. 
24 
00:01:52,900 --> 00:01:56,359 
Let's query for delegates. 
25 
00:01:56,359 --> 00:02:01,324 
For this term, we see that news2.csv 
26 
00:02:01,324 --> 00:02:06,288 
was first, and news1 was second, and 
27 
00:02:06,288 --> 00:02:11,260 
news3 did not contain the term at all. 
28 
00:02:13,710 --> 00:02:17,275 
Now, let's query for both terms,
voters and delegates. 
29 
00:02:17,275 --> 00:02:24,935 
In this result we see that
news2 was ranked first, 
30 
00:02:24,935 --> 00:02:29,566 
news1 was ranked second, and 
31 
00:02:29,566 --> 00:02:33,500 
news3 was ranked third. 
32 
00:02:35,230 --> 00:02:37,000 
Now lets use query term waiting or 
33 
00:02:37,000 --> 00:02:40,920 
boosting to increase
the relevance of voters. 
34 
00:02:40,920 --> 00:02:45,900 
I can do this by ensuring
voters carat 5 delegates. 
35 
00:02:45,900 --> 00:02:53,524 
The carat 5 notation is a syntax for 
36 
00:02:53,524 --> 00:02:58,360 
Lucene for boosting. 
37 
00:02:59,890 --> 00:03:05,990 
When we run this, we see that now,
news1 is ranked first, 
38 
00:03:05,990 --> 00:03:10,170 
news2 is ranked second,
and news3 is ranked third. 
39 
00:03:10,170 --> 00:03:14,500 
Notice this is different from the original
query with voters and delegates, 
40 
00:03:14,500 --> 00:03:18,100 
where news2 is ranked first and
news1 was ranked second. 
41 
00:03:20,210 --> 00:03:23,890 
Now let's look at the term frequency,
inverse document frequency or TF-IDF. 
42 
00:03:23,890 --> 00:03:28,465 
We'll enter q to quit this and 
43 
00:03:28,465 --> 00:03:33,960 
we'll run Lucene TF-IDF SH data. 
44 
00:03:45,380 --> 00:03:51,000 
Let's look at the TF-IDF for voters. 
45 
00:03:51,000 --> 00:03:53,320 
You can see that it ranked number 1 for
news1. 
46 
00:03:54,370 --> 00:03:58,160 
Second news 2 and news 3 is last. 
47 
00:03:58,160 --> 00:03:59,170 
Lets try delegates. 
48 
00:04:05,110 --> 00:04:09,040 
Here we see that news 2 had
a higher score than news 1, 
49 
00:04:09,040 --> 00:04:12,850 
and news 3 is not listed because
news 3 does not contain this term. 
50 
00:04:13,870 --> 00:04:14,590 
Hit q to quit. 
1 
00:00:00,800 --> 00:00:05,529 
So the next category of data we discuss
has the form of graphs or networks, 
2 
00:00:05,529 --> 00:00:08,749 
the most obvious example
being social networks. 
3 
00:00:08,749 --> 00:00:10,782 
Now speaking of social networks, 
4 
00:00:10,782 --> 00:00:15,760 
Tim Libzek created a social network
from the Lord of the Rings Trilogy. 
5 
00:00:15,760 --> 00:00:18,545 
This graph represents
the characters' allegiances, 
6 
00:00:18,545 --> 00:00:20,744 
that is who is faithful
to whom in the books. 
7 
00:00:20,744 --> 00:00:24,670 
So the nodes are characters and
other entities, like cities, and 
8 
00:00:24,670 --> 00:00:29,070 
the edges connecting pairs of
nodes represent allegiances. 
9 
00:00:29,070 --> 00:00:35,480 
So after this video, you'll be able to
identify graph data in practical problems 
10 
00:00:35,480 --> 00:00:41,150 
and describe path, neighborhood, and
connectivity operations in graphs. 
11 
00:00:41,150 --> 00:00:44,860 
But this specialization includes
a separate course in graph analytics 
12 
00:00:44,860 --> 00:00:48,470 
that provides a much more detailed
treatment on the subject. 
13 
00:00:48,470 --> 00:00:51,810 
Now what distinguishes a graph
from other data models 
14 
00:00:51,810 --> 00:00:55,400 
is that it bears two kinds of information. 
15 
00:00:55,400 --> 00:01:00,080 
One, properties and attributes of
entities and relationships, and 
16 
00:01:00,080 --> 00:01:04,130 
two, the connectivity structure that
constitutes the network itself. 
17 
00:01:05,480 --> 00:01:08,260 
One way to look at this data
is shown in the figure, 
18 
00:01:08,260 --> 00:01:10,070 
borrowed from the Apache Spark system. 
19 
00:01:11,130 --> 00:01:12,360 
In this representation, 
20 
00:01:12,360 --> 00:01:16,390 
the graph on the left is represented
by two tables on the right. 
21 
00:01:16,390 --> 00:01:22,095 
The vertex, or node table, gives IDs
to nodes and lists their properties. 
22 
00:01:23,170 --> 00:01:25,720 
The edge table has two parts. 
23 
00:01:25,720 --> 00:01:29,210 
The colored part represents
the properties of the edge, 
24 
00:01:29,210 --> 00:01:33,600 
whereas the white part contains just the
direction of the arrows in the network. 
25 
00:01:33,600 --> 00:01:39,280 
Thus, since there is a directed
edge going from node 3 to node 7, 
26 
00:01:39,280 --> 00:01:43,830 
there is a tupple 3,
7 in that part of the edge table. 
27 
00:01:43,830 --> 00:01:48,540 
Now this form of the graph model is
called the property graph model, 
28 
00:01:48,540 --> 00:01:52,760 
which we'll see many times in this
course and in the specialization. 
29 
00:01:52,760 --> 00:01:57,740 
Now representing connectivity information
gives graph data a new kind of 
30 
00:01:57,740 --> 00:02:01,910 
computing ability that's different from
other data models we have seen so far. 
31 
00:02:03,090 --> 00:02:07,450 
Even without looking at the properties
of the nodes and edges, one can get very 
32 
00:02:07,450 --> 00:02:12,640 
interesting information just by analyzing
or querying this connectivity structure. 
33 
00:02:13,800 --> 00:02:18,990 
Consider a social network with three
types of nodes, user, city, and 
34 
00:02:18,990 --> 00:02:24,250 
restaurant, and three types of edges,
friend, likes, and lives in. 
35 
00:02:25,300 --> 00:02:28,110 
The leftmost node, AG, represents me. 
36 
00:02:28,110 --> 00:02:31,930 
And I'm interested in finding a good
Italian restaurant in New York 
37 
00:02:31,930 --> 00:02:36,360 
that my friends, or their friends,
who also live in New York, like. 
38 
00:02:36,360 --> 00:02:41,765 
I shall possibly choose IT3 because
it has the highest number of 
39 
00:02:41,765 --> 00:02:47,700 
like edges coming into it from people
who have a lives in edge to New York. 
40 
00:02:47,700 --> 00:02:49,008 
And at the same time, 
41 
00:02:49,008 --> 00:02:52,869 
can be reached by following
the friend edges going out from me. 
42 
00:02:52,869 --> 00:02:57,306 
Now this shows a very important class
of operations and ground data, namely 
43 
00:02:57,306 --> 00:03:01,830 
traversal, that involves edge following
based on some sort of conditions. 
44 
00:03:03,090 --> 00:03:07,660 
A number of path operations
required some sort of optimization. 
45 
00:03:07,660 --> 00:03:12,885 
The simplest among these is the well known
shortest path query, which is applied to 
46 
00:03:12,885 --> 00:03:17,750 
node networks to find the best route from
a source location to a target location. 
47 
00:03:17,750 --> 00:03:21,215 
The second class of optimization
operations is required to find 
48 
00:03:21,215 --> 00:03:26,050 
an optimal path that must include
some user specified nodes, for 
49 
00:03:26,050 --> 00:03:30,420 
the operation has to determine the order
in which the nodes once we visited. 
50 
00:03:30,420 --> 00:03:32,795 
The classical application
is a trip planner, 
51 
00:03:32,795 --> 00:03:36,610 
where the user specifies the cities
she wishes to visit, and 
52 
00:03:36,610 --> 00:03:40,740 
the operation will optimize the criterion,
like the total distance covered. 
53 
00:03:40,740 --> 00:03:45,260 
The third category is a case where
the system must find the best possible 
54 
00:03:45,260 --> 00:03:47,690 
path in the network, given two or 
55 
00:03:47,690 --> 00:03:52,070 
more optimization criteria,
which cannot be satisfied simultaneously. 
56 
00:03:52,070 --> 00:03:56,350 
For example, if I want to travel
from my house to the airport 
57 
00:03:56,350 --> 00:04:01,150 
using the shortest distance, but also
minimizing the amount of highway travel, 
58 
00:04:01,150 --> 00:04:03,710 
the algorithm must find a best compromise. 
59 
00:04:03,710 --> 00:04:07,510 
This is called a pareto-optimality
problem on graphs. 
60 
00:04:07,510 --> 00:04:13,120 
The neighborhood of a node N in a graph is
a set of edges directly connected to it. 
61 
00:04:13,120 --> 00:04:18,360 
A K neighborhood of N is a collection
of edges between nodes that are, 
62 
00:04:18,360 --> 00:04:21,160 
at most, K steps away from N. 
63 
00:04:21,160 --> 00:04:24,710 
So going back to our mini social
network graph, Bob, Jill, 
64 
00:04:24,710 --> 00:04:29,630 
and Sarah are the first neighbors of AG,
while Max, Tim and 
65 
00:04:29,630 --> 00:04:34,570 
Pam belong to the second neighborhood and
not the first neighborhood of AG. 
66 
00:04:34,570 --> 00:04:37,910 
Finally, Jen is a third level neighbor. 
67 
00:04:38,910 --> 00:04:43,790 
An important class of analysis to perform
with neighborhoods is community finding. 
68 
00:04:43,790 --> 00:04:47,740 
A community and a social network can
be a very close group of friends. 
69 
00:04:47,740 --> 00:04:51,340 
So the graph shown in this
figure has four communities. 
70 
00:04:51,340 --> 00:04:56,327 
One can see in the figure that each
community has a higher density of edges 
71 
00:04:56,327 --> 00:05:02,480 
within the community and a lower density
across two different communities. 
72 
00:05:02,480 --> 00:05:05,440 
Finding densely connected parts of a graph 
73 
00:05:05,440 --> 00:05:09,860 
helps identify neighborhoods that
can be recognized as communities. 
74 
00:05:09,860 --> 00:05:15,032 
A more complex class of operations include
finding the best possible clusters, 
75 
00:05:15,032 --> 00:05:17,620 
which is another name for
communities in a graph, so 
76 
00:05:17,620 --> 00:05:22,030 
that any other grouping of nodes into
communities will be less effective. 
77 
00:05:22,030 --> 00:05:27,430 
Now, as graphs become bigger and denser,
these methods become harder to compute. 
78 
00:05:27,430 --> 00:05:32,240 
Thus, neighborhood-based
optimization operation present 
79 
00:05:32,240 --> 00:05:34,520 
significant scalability challenges. 
80 
00:05:34,520 --> 00:05:38,650 
If we inspect the neighborhood of
every node in a graph, sometimes, 
81 
00:05:38,650 --> 00:05:42,650 
we'll find neighborhoods that
are different from all others. 
82 
00:05:42,650 --> 00:05:45,390 
These neighborhoods are called anomalous. 
83 
00:05:45,390 --> 00:05:49,720 
Consider the following four graphs and
on the central red node. 
84 
00:05:49,720 --> 00:05:54,177 
The first graph is odd because
it's almost perfectly star shaped. 
85 
00:05:54,177 --> 00:05:58,751 
That is, the nodes that the red node
is connected to are almost unconnected 
86 
00:05:58,751 --> 00:06:00,137 
amongst themselves. 
87 
00:06:00,137 --> 00:06:04,000 
That's really odd because it
doesn't happen in reality much. 
88 
00:06:04,000 --> 00:06:05,620 
So it's an anomalous node. 
89 
00:06:05,620 --> 00:06:09,630 
The second figure shows
a neighborhood to which 
90 
00:06:09,630 --> 00:06:13,770 
a significantly large number of neighbors
has connected amongst themselves. 
91 
00:06:13,770 --> 00:06:18,860 
This makes the graph very cliquish,
where a clique refers to a neighborhood 
92 
00:06:18,860 --> 00:06:23,280 
where each node is connected to all other
neighborhood nodes in the neighborhood. 
93 
00:06:23,280 --> 00:06:25,770 
The third figure shows a neighborhood, 
94 
00:06:25,770 --> 00:06:31,040 
where some edges have an unusually
heavy weight compared to the others. 
95 
00:06:31,040 --> 00:06:34,578 
The fourth figure shows
a special case of the third, 
96 
00:06:34,578 --> 00:06:38,930 
where one edge is predominantly high
rate compared to all the other edges. 
97 
00:06:40,120 --> 00:06:42,890 
Connectedness is a fundamental
property of a graph. 
98 
00:06:43,960 --> 00:06:45,390 
In a connected graph, 
99 
00:06:45,390 --> 00:06:49,570 
each node is reachable from every
other node through some path. 
100 
00:06:49,570 --> 00:06:54,750 
If a graph is not connected, but there
are subgraphs of it, which are connected, 
101 
00:06:54,750 --> 00:06:58,745 
then these subgraphs are called connected
components of the original graph. 
102 
00:06:58,745 --> 00:07:01,799 
In the figure on the right,
there are four connected components. 
103 
00:07:01,799 --> 00:07:05,805 
A search gradient like
finding optimal paths 
104 
00:07:05,805 --> 00:07:09,075 
should be performed only within
each component and not across them. 
105 
00:07:10,215 --> 00:07:14,325 
For large graphs, there are several
new parallelized techniques for 
106 
00:07:14,325 --> 00:07:15,835 
the detection of connected components. 
107 
00:07:17,070 --> 00:07:19,680 
We will discuss a map
reduce based technique for 
108 
00:07:19,680 --> 00:07:21,830 
connected components in a later course. 
1 
00:00:03,200 --> 00:00:05,569 
We have discussed quite a few data models,
but 
2 
00:00:05,569 --> 00:00:08,669 
there are many other data models
that have been developed for 
3 
00:00:08,669 --> 00:00:13,500 
various purposes, and we really cannot
cover all of them in a single course. 
4 
00:00:13,500 --> 00:00:17,360 
We'll end these lectures on data models
with an example that may give you 
5 
00:00:17,360 --> 00:00:21,560 
an insight into a class of objects that
define in many different applications. 
6 
00:00:22,850 --> 00:00:27,972 
So after this video you'll be able
to describe how arrays can serve 
7 
00:00:27,972 --> 00:00:33,278 
as a data model, explain why images
can be modeled as vector arrays, 
8 
00:00:33,278 --> 00:00:37,689 
specify a set of operations on scalar and
vector arrays. 
9 
00:00:39,969 --> 00:00:42,690 
Now, we have all seen the arrays. 
10 
00:00:42,690 --> 00:00:46,390 
In the simplest case,
an array is a matrix like this. 
11 
00:00:46,390 --> 00:00:48,380 
Let's call this array A. 
12 
00:00:50,960 --> 00:00:55,140 
The top row in yellow,
gives the column numbers and 
13 
00:00:55,140 --> 00:00:57,890 
the left column, also in yellow,
gives the row numbers. 
14 
00:00:58,970 --> 00:01:03,263 
When we need to refer to a value
of the array as A(3, 2), 
15 
00:01:03,263 --> 00:01:07,530 
we mean the value of the cell in row 3 and
column 2. 
16 
00:01:07,530 --> 00:01:12,270 
This is called indexed structure,
where 3 and 
17 
00:01:12,270 --> 00:01:18,070 
2 are the row and column indices that are
necessary to get the value of a data item. 
18 
00:01:19,430 --> 00:01:21,830 
The area has two dimensions. 
19 
00:01:21,830 --> 00:01:24,050 
So hence there are two indexes. 
20 
00:01:24,050 --> 00:01:27,650 
If these were a three dimensional array,
we would have three indexes. 
21 
00:01:28,860 --> 00:01:29,390 
Now earlier, 
22 
00:01:29,390 --> 00:01:34,360 
we have seen that we can represent the two
dimensional array as a three column table. 
23 
00:01:34,360 --> 00:01:37,790 
One column for the row index,
one column for the column index, and 
24 
00:01:37,790 --> 00:01:38,849 
the last column for the value. 
25 
00:01:40,310 --> 00:01:45,310 
Thus a k dimensional array can be
represented as a relation with k 
26 
00:01:45,310 --> 00:01:46,090 
plus one columns. 
27 
00:01:47,690 --> 00:01:51,100 
The number of tuples in this
representation will be the product of 
28 
00:01:51,100 --> 00:01:55,220 
the size of the first dimension times the
size of the second dimension and so forth. 
29 
00:01:56,670 --> 00:02:01,000 
Then in this case,
the size is five in each dimension. 
30 
00:02:01,000 --> 00:02:08,070 
So there are 25 C column tuples in
a relation representing the array. 
31 
00:02:08,070 --> 00:02:11,890 
A more useful situation occurs
when the cells of an array have 
32 
00:02:11,890 --> 00:02:13,490 
a vectors as values. 
33 
00:02:14,620 --> 00:02:19,610 
As you can see in the 2D vector array
here, each cell has a three vector. 
34 
00:02:19,610 --> 00:02:21,340 
That is a vector with three elements. 
35 
00:02:22,400 --> 00:02:25,150 
Therefore, if we want to
receive a cell value and 
36 
00:02:25,150 --> 00:02:27,930 
treat it like before,
we'll get back the whole vector. 
37 
00:02:29,220 --> 00:02:31,680 
Now, this type of data
should look familiar to you, 
38 
00:02:31,680 --> 00:02:35,840 
because images often have a red,
green and blue channels per pixel. 
39 
00:02:37,030 --> 00:02:38,050 
In other words, 
40 
00:02:38,050 --> 00:02:43,780 
images of vector valued arrays where each
array cell has a three color vector. 
41 
00:02:45,270 --> 00:02:49,000 
We can also think of the array model
in the context of satellite images. 
42 
00:02:49,000 --> 00:02:52,850 
Where there are many more channels
depending on the range of wavelengths 
43 
00:02:52,850 --> 00:02:54,100 
each channel catches. 
44 
00:02:55,360 --> 00:02:58,710 
Let us consider the operations
on arrays of vectors. 
45 
00:02:58,710 --> 00:03:03,430 
Because it is a combination of two models,
one can create different combinations of 
46 
00:03:03,430 --> 00:03:08,140 
array operations, vector operations and
composite operations. 
47 
00:03:08,140 --> 00:03:08,650 
Here are some. 
48 
00:03:09,810 --> 00:03:12,580 
The dimension of the array here,
the first operation, is two. 
49 
00:03:13,940 --> 00:03:16,289 
If we pick up any dimension, say one. 
50 
00:03:16,289 --> 00:03:20,741 
The size of it is also two
because they're two elements, 
51 
00:03:20,741 --> 00:03:23,260 
zero and one in each dimension. 
52 
00:03:23,260 --> 00:03:31,259 
As we saw before, the value of the cell
(1,1) is a vector 16, 301, 74. 
53 
00:03:31,259 --> 00:03:37,979 
While the value of A11 component 2 is 74. 
54 
00:03:37,979 --> 00:03:42,279 
The length of the vector is a square root
of the sum of the elements of the vector. 
55 
00:03:42,279 --> 00:03:48,999 
So length of A11 would come to 310.375. 
56 
00:03:48,999 --> 00:03:50,623 
The distance function can be so 
57 
00:03:50,623 --> 00:03:54,184 
simple like the Euclidean distance
function between two vectors or 
58 
00:03:54,184 --> 00:03:57,820 
the cosine of an angle between them
as we saw in the previous lecture. 
59 
00:03:59,080 --> 00:04:02,580 
But it can also be something more complex,
based on the needs of the application. 
60 
00:04:04,320 --> 00:04:08,390 
Obviously one can also perform operations
like selection over indices so 
61 
00:04:08,390 --> 00:04:13,170 
we can ask which cells had
the zero value greater than 25. 
62 
00:04:13,170 --> 00:04:16,060 
Giving as the result zero one and
one zero. 
63 
00:04:17,190 --> 00:04:20,050 
You will experience some of these
operations in your hands on session. 
1 
00:00:02,265 --> 00:00:06,497 
Next, we'll look at a data model
that has been successfully used to 
2 
00:00:06,497 --> 00:00:10,150 
retrieve data from large
collections of text and images. 
3 
00:00:11,170 --> 00:00:12,230 
Let's stay with text for now. 
4 
00:00:13,590 --> 00:00:17,490 
Text is often thought of
as unstructured data. 
5 
00:00:17,490 --> 00:00:20,315 
Primarily because it doesn't really
have attributes and relationships. 
6 
00:00:21,530 --> 00:00:25,740 
Instead, it is a sequence of strings
punctuated by line and parent of breaks. 
7 
00:00:27,810 --> 00:00:34,370 
So one is to think of a different
way to find and analyze text data. 
8 
00:00:34,370 --> 00:00:39,130 
In this video, we'll describe that
finding text from a huge collection of 
9 
00:00:39,130 --> 00:00:42,880 
text data is a little different from
the data modules we have seen so far. 
10 
00:00:44,080 --> 00:00:48,710 
To find text,
we not only need the text data itself, but 
11 
00:00:48,710 --> 00:00:53,810 
we need a different structure that
is computed from the text data. 
12 
00:00:53,810 --> 00:00:55,020 
To create the structure, 
13 
00:00:55,020 --> 00:01:01,000 
we'll introduce the notion of the document
vector model which we call a vector model. 
14 
00:01:02,760 --> 00:01:05,510 
Further you will see
that finding a document 
15 
00:01:05,510 --> 00:01:08,090 
is not really an exact search problem. 
16 
00:01:09,380 --> 00:01:13,090 
Here, we'll give a query document and 
17 
00:01:13,090 --> 00:01:17,130 
ask the system to find all
documents that are similar to it. 
18 
00:01:18,440 --> 00:01:20,990 
After this video,
you'll be able to describe 
19 
00:01:20,990 --> 00:01:25,380 
how the similarity is computed and
how it is used to search documents. 
20 
00:01:26,930 --> 00:01:31,770 
Finally, you will see that search engines
use some form of vector models and 
21 
00:01:31,770 --> 00:01:33,850 
similarity search to locate text data. 
22 
00:01:35,480 --> 00:01:39,970 
And you will see that the same principle
can be use for finding similar images. 
23 
00:01:42,740 --> 00:01:45,410 
Let us describe the concept
of a document to an example. 
24 
00:01:46,540 --> 00:01:49,170 
So lets consider three types
of document shown here. 
25 
00:01:51,040 --> 00:01:52,470 
Now we'll create a matrix. 
26 
00:01:56,040 --> 00:01:57,790 
The rows of the matrix stand for 
27 
00:01:57,790 --> 00:02:02,950 
the documents and columns represent
the words in the documents. 
28 
00:02:02,950 --> 00:02:05,770 
We put the number of occurrences
of returning the document in 
29 
00:02:05,770 --> 00:02:07,400 
the appropriate cell of the matrix. 
30 
00:02:08,630 --> 00:02:14,380 
In this case, the count of each term
in each document happens to be one. 
31 
00:02:14,380 --> 00:02:16,700 
This is called the term frequency matrix. 
32 
00:02:19,340 --> 00:02:22,620 
So now that we have created
the term frequency matrix, 
33 
00:02:22,620 --> 00:02:24,540 
which we call tf for short. 
34 
00:02:25,595 --> 00:02:30,350 
We'll create a new vector called the
inverse document frequency for each term. 
35 
00:02:31,810 --> 00:02:34,990 
We'll explain why we need this
vector on the next slide. 
36 
00:02:34,990 --> 00:02:37,160 
First, let's see how it's computed. 
37 
00:02:39,280 --> 00:02:43,435 
The number of documents n here is 3. 
38 
00:02:43,435 --> 00:02:46,270 
The term new occurs
twice in the collection. 
39 
00:02:47,520 --> 00:02:52,950 
So the inverse document frequency or
IDF of the term new 
40 
00:02:52,950 --> 00:02:57,810 
is log to the base 2,
n divided by term count. 
41 
00:02:57,810 --> 00:03:04,464 
That is, log to the base 2,
3 divided by 2, which is 0.584. 
42 
00:03:05,980 --> 00:03:09,210 
We'll show the ideal score for
all six terms here. 
43 
00:03:10,770 --> 00:03:15,735 
Now some of you may wonder why we use
log to the base 2 instead of let's 
44 
00:03:15,735 --> 00:03:17,540 
say log to the base 10. 
45 
00:03:17,540 --> 00:03:20,350 
There is no deep scientific reason for it. 
46 
00:03:20,350 --> 00:03:23,970 
It's more of a convention in
many areas of Computer Science 
47 
00:03:23,970 --> 00:03:26,110 
when many important
numbers are powers of two. 
48 
00:03:27,150 --> 00:03:32,153 
In reality,
log to the base two of x is the same 
49 
00:03:32,153 --> 00:03:39,156 
number as log to the base ten of x
times log to the base two of ten. 
50 
00:03:39,156 --> 00:03:44,340 
The second number, that is log to
the base two of ten is a constant. 
51 
00:03:44,340 --> 00:03:50,080 
So the relative score of IDF does not
change regardless of the base we use. 
52 
00:03:50,080 --> 00:03:52,170 
Now let's understand this
number one more time. 
53 
00:03:53,400 --> 00:03:57,829 
The document frequency of a term is
the count of that term in the whole 
54 
00:03:57,829 --> 00:04:01,029 
collection divided by
the number of documents. 
55 
00:04:02,748 --> 00:04:07,168 
Here, we take the inverse of
the document frequency, so 
56 
00:04:07,168 --> 00:04:11,510 
that n, the number of documents,
is in the numerator. 
57 
00:04:13,580 --> 00:04:18,270 
Now before we continue, let's understand
the intuition behind the IDF vector. 
58 
00:04:19,500 --> 00:04:25,210 
Now suppose you have 100
random newspaper articles and 
59 
00:04:25,210 --> 00:04:28,110 
let's say 10 of them cover elections. 
60 
00:04:29,210 --> 00:04:33,710 
Which means all the others
cover all other subjects. 
61 
00:04:33,710 --> 00:04:39,730 
Now in this article, let's say we'll find
the term election 50 times in total. 
62 
00:04:41,660 --> 00:04:46,000 
How often do you think you'll find
the term is as in the verb is? 
63 
00:04:47,320 --> 00:04:50,440 
You can imagine that it will
occur in all hundred of them and 
64 
00:04:50,440 --> 00:04:52,160 
that too, multiple times. 
65 
00:04:53,370 --> 00:04:57,240 
We can safely assume that
the number of occurrences of is 
66 
00:04:57,240 --> 00:04:59,700 
will be 300 at the very least. 
67 
00:05:01,000 --> 00:05:06,940 
Thus the document frequency of is is six
times the document frequency of election. 
68 
00:05:07,940 --> 00:05:10,460 
But that doesn't sound right, does it? 
69 
00:05:10,460 --> 00:05:15,030 
Is is such a common word that it's
prevalence has a negative impact 
70 
00:05:15,030 --> 00:05:17,240 
on its informativeness. 
71 
00:05:17,240 --> 00:05:21,520 
So, now if you want to
compute the IDF of is and 
72 
00:05:21,520 --> 00:05:24,990 
election, the IDF of is will be far lower. 
73 
00:05:26,170 --> 00:05:29,930 
So, IDF acts like a penalty factor for 
74 
00:05:29,930 --> 00:05:33,570 
terms which are too widely used
to be considered informative. 
75 
00:05:36,340 --> 00:05:39,890 
Now that we have understood
that IDF is a penalty factor, 
76 
00:05:39,890 --> 00:05:45,800 
we will multiply the tf numbers through
the IDF numbers giving us this. 
77 
00:05:48,610 --> 00:05:53,550 
This is a column-wise multiplication
of the tf numbers with the IDF 
78 
00:05:53,550 --> 00:05:57,677 
numbers giving us what we
call the tf-idf matrix. 
79 
00:05:59,120 --> 00:06:06,070 
Therefore for each document, we have
a vector represented here as a row. 
80 
00:06:06,070 --> 00:06:10,910 
So that row represents the relative
importance of each term in the vocabulary. 
81 
00:06:10,910 --> 00:06:14,370 
Vocabulary means the collection of all
words that appear in this collection. 
82 
00:06:16,160 --> 00:06:20,800 
If the vocabulary has 3 million entries,
then this vector can get quite long. 
83 
00:06:22,150 --> 00:06:26,890 
Also, if the number of document
grows let's just say to 1 billion, 
84 
00:06:26,890 --> 00:06:28,720 
then it becomes a big data problem. 
85 
00:06:30,460 --> 00:06:33,660 
Now the last column after
each document of vector here 
86 
00:06:33,660 --> 00:06:35,180 
is the length of the document vector. 
87 
00:06:36,300 --> 00:06:41,060 
Which is really the square root of the sum
of squares of the individual term scores 
88 
00:06:41,060 --> 00:06:42,060 
as shown in the formula. 
89 
00:06:45,560 --> 00:06:48,170 
To perform a search in the vector space, 
90 
00:06:48,170 --> 00:06:52,010 
we write a query just like
we type terms in Google. 
91 
00:06:52,010 --> 00:06:55,170 
Here, the number of terms is three. 
92 
00:06:55,170 --> 00:06:57,910 
Out of which the term
new appears two times. 
93 
00:06:59,260 --> 00:07:03,049 
In fact, this is the maximum frequency
out of all terms in the query. 
94 
00:07:04,220 --> 00:07:08,660 
So we take the document vector of
the query and multiply each term 
95 
00:07:08,660 --> 00:07:14,570 
by the number of occurrences divided by
two which is the maximum term frequency. 
96 
00:07:14,570 --> 00:07:18,090 
Now in this case,
it gives us two non-zero terms. 
97 
00:07:18,090 --> 00:07:20,950 
0.584 and 
98 
00:07:20,950 --> 00:07:26,460 
0.292 for new and york. 
99 
00:07:26,460 --> 00:07:30,726 
Then we compute the length of
the query vector just like we did for 
100 
00:07:30,726 --> 00:07:33,730 
the document vectors
on the previous slide. 
101 
00:07:33,730 --> 00:07:39,443 
Next, we will compute the similarity
between the query vector and each document 
102 
00:07:39,443 --> 00:07:45,170 
with the idea that we'll measure how far
the query vector is from each document. 
103 
00:07:47,730 --> 00:07:53,150 
Now there are many similar functions
defined and used for different things. 
104 
00:07:53,150 --> 00:07:56,940 
A popular similarity measure
is the cosine function, 
105 
00:07:56,940 --> 00:08:03,200 
which measures the cosine function of
the angle between these two vectors. 
106 
00:08:03,200 --> 00:08:07,080 
The mathematical formula for
computing the function is given here. 
107 
00:08:07,080 --> 00:08:11,250 
The intuition is that if
the vectors are identical, 
108 
00:08:11,250 --> 00:08:14,330 
then the angle between them is zero. 
109 
00:08:14,330 --> 00:08:16,230 
And therefore,
the cosine function evaluates to one. 
110 
00:08:18,250 --> 00:08:20,910 
As the angle increases, 
111 
00:08:20,910 --> 00:08:25,450 
the value of the cosine function
decreases to make them more dissimilar. 
112 
00:08:26,780 --> 00:08:30,694 
The way to compute the function is to
multiply the corresponding elements of 
113 
00:08:30,694 --> 00:08:32,170 
the two vectors. 
114 
00:08:32,170 --> 00:08:34,850 
That is the first element
of one with the first 
115 
00:08:34,850 --> 00:08:37,340 
element of the second one and so forth. 
116 
00:08:37,340 --> 00:08:38,790 
And then sum of these products. 
117 
00:08:39,960 --> 00:08:45,610 
Here, the only contributing
terms are from new and 
118 
00:08:45,610 --> 00:08:49,230 
york because, these are the only two
non-zero terms in the query vector. 
119 
00:08:50,660 --> 00:08:55,630 
This sum is then divided by
the product of the document length and 
120 
00:08:55,630 --> 00:08:58,040 
the query length that we
have computed earlier. 
121 
00:08:59,310 --> 00:09:02,650 
Look at the result of the distance
function and you will notice that 
122 
00:09:02,650 --> 00:09:07,570 
the document 1 is much more similar
to the query than the other two. 
123 
00:09:08,810 --> 00:09:13,640 
So while similarity scoring and document
ranking process working effectively, 
124 
00:09:13,640 --> 00:09:16,100 
the method is a little cotton dry. 
125 
00:09:16,100 --> 00:09:19,920 
More often than not users would
like a little more control 
126 
00:09:19,920 --> 00:09:20,910 
over the ranking of terms. 
127 
00:09:22,270 --> 00:09:27,450 
One way of accomplishing this is to put
different weights on each query term. 
128 
00:09:27,450 --> 00:09:33,460 
And in this example, the query term
york has a default weight of one, 
129 
00:09:33,460 --> 00:09:36,530 
times has a weight of two. 
130 
00:09:36,530 --> 00:09:40,340 
And post has a weight of five
as specified by the user. 
131 
00:09:41,350 --> 00:09:43,662 
So relatively speaking, 
132 
00:09:43,662 --> 00:09:49,765 
york has a weight of 1 divided
by 1 + 5 + 2 is equal to 0.125. 
133 
00:09:49,765 --> 00:09:55,630 
Times has a weight of 0.25 and
post has a weight of 0.625. 
134 
00:09:55,630 --> 00:09:59,280 
Now the scoring method we showed
before will change a bit. 
135 
00:09:59,280 --> 00:10:04,091 
The query of vector and its length
were exactly as computed before. 
136 
00:10:04,091 --> 00:10:08,777 
However, now each term in the query
vector is further multiplied by these 
137 
00:10:08,777 --> 00:10:10,810 
relative weights. 
138 
00:10:10,810 --> 00:10:14,490 
In our case,
the term york now has a much higher rate. 
139 
00:10:15,500 --> 00:10:21,237 
So as expected, this will change
the ranking of the documents and 
140 
00:10:21,237 --> 00:10:24,900 
new york post will have the highest rank. 
141 
00:10:24,900 --> 00:10:30,580 
Now similarity search is often used for
images using a vector space model. 
142 
00:10:30,580 --> 00:10:33,210 
One can compute futures from images. 
143 
00:10:33,210 --> 00:10:36,180 
And one common feature
is a scatter histogram. 
144 
00:10:36,180 --> 00:10:38,000 
Consider the image here. 
145 
00:10:38,000 --> 00:10:40,820 
One can create the histogram of the red,
green and 
146 
00:10:40,820 --> 00:10:46,170 
blue channels where histogram is the count
of pixels having a certain density value. 
147 
00:10:47,360 --> 00:10:52,600 
This picture is mostly bright, so the
count of dark pixels is relatively small. 
148 
00:10:53,690 --> 00:10:56,746 
Now one can think of
histograms like a vector. 
149 
00:10:56,746 --> 00:11:01,430 
Very often the pixel values will
be bend before creating a vector. 
150 
00:11:01,430 --> 00:11:06,424 
The table shown is a feature vector
where the numbers for each row have been 
151 
00:11:06,424 --> 00:11:10,960 
normalized with the size of the image
to make the row sum equal to one. 
152 
00:11:10,960 --> 00:11:15,320 
Similar vectors can be computed of
the image texture, shapes of objects and 
153 
00:11:15,320 --> 00:11:16,790 
any other properties. 
154 
00:11:16,790 --> 00:11:20,700 
Thus making a vector space model
significant for unstructured data. 
1 
00:00:01,646 --> 00:00:06,170 
In our experience as educators
we have observed that 
2 
00:00:06,170 --> 00:00:10,940 
learners often make the assumption
that the format of the data 
3 
00:00:10,940 --> 00:00:15,820 
is the same as the logical model of the
data in the way that you operate on it. 
4 
00:00:15,820 --> 00:00:18,740 
The goal of this very
short lecture is to ensure 
5 
00:00:18,740 --> 00:00:21,590 
that we can clearly
distinguish between the two. 
6 
00:00:21,590 --> 00:00:26,550 
So, after watching this video you will be
able to explain the difference between 
7 
00:00:26,550 --> 00:00:31,460 
format, which is a serialized
representation of the data, as opposed to 
8 
00:00:31,460 --> 00:00:35,010 
data model which we have discussed
at length in the previous months. 
9 
00:00:36,930 --> 00:00:41,050 
Perhaps the simplest example of a data
format is a csv file, so here is 
10 
00:00:41,050 --> 00:00:45,359 
a snippet of a csv file from the global
terrorism database we discussed earlier. 
11 
00:00:46,640 --> 00:00:50,520 
We know that CSV or
common separative values means 
12 
00:00:50,520 --> 00:00:53,910 
that the term between two commas
is the value of an attribute. 
13 
00:00:53,910 --> 00:00:54,740 
But what is this value? 
14 
00:00:56,200 --> 00:01:00,110 
The common notion is that it's
a content of a single relation 
15 
00:01:01,280 --> 00:01:04,870 
where each line is a record
that's a tuple and 
16 
00:01:04,870 --> 00:01:09,030 
the iod value in the CSV corresponds
to the iod attribute as shown here. 
17 
00:01:10,340 --> 00:01:14,850 
Now that might very well be true but
let's look at a different example. 
18 
00:01:14,850 --> 00:01:17,440 
Let's say this snippet
here is my CSV file. 
19 
00:01:18,790 --> 00:01:21,960 
There is no difference between
the previous file and this one. 
20 
00:01:21,960 --> 00:01:24,780 
However, here is how I
like to see the data. 
21 
00:01:26,630 --> 00:01:28,600 
As you can see this is a graph, 
22 
00:01:28,600 --> 00:01:33,190 
and the data model is the same
although the format is still CSV. 
1 
00:00:00,870 --> 00:00:03,200 
This is the second hands on exercise for
sensor data. 
2 
00:00:03,200 --> 00:00:06,220 
In the first we looked at
static data in a text file. 
3 
00:00:06,220 --> 00:00:10,410 
In this one we'll be looking at
real-time streaming measurements. 
4 
00:00:10,410 --> 00:00:12,675 
First, we will open a terminal window, and 
5 
00:00:12,675 --> 00:00:15,770 
cd into the directory containing
the data and the scripts. 
6 
00:00:15,770 --> 00:00:18,110 
Next we'll connect to
the weather station and 
7 
00:00:18,110 --> 00:00:20,990 
look at the real-time
data as it streams in. 
8 
00:00:20,990 --> 00:00:25,280 
After that, we will look at the key to
remind ourselves what the fields mean. 
9 
00:00:25,280 --> 00:00:28,140 
And finally, we will plot the data
streaming from the weather station. 
10 
00:00:29,400 --> 00:00:30,490 
Let's begin. 
11 
00:00:30,490 --> 00:00:33,830 
First, open a terminal window,
by clicking on the terminal icon. 
12 
00:00:33,830 --> 00:00:34,861 
Top of the toolbar. 
13 
00:00:36,498 --> 00:00:42,919 
[NOISE] Let's run cd
Downloads/big-data-2/sensor. 
14 
00:00:51,421 --> 00:00:54,021 
You can run ls to see
the name of the scripts. 
15 
00:00:57,772 --> 00:01:02,114 
Let's run stream-data.py
to see the real-time data. 
16 
00:01:10,240 --> 00:01:15,860 
This shows us the real-time measurements
coming from the weather station. 
17 
00:01:15,860 --> 00:01:17,738 
By looking at the time stamps, 
18 
00:01:17,738 --> 00:01:22,733 
we can see that each measurement arrives
about one second after the previous one. 
19 
00:01:25,078 --> 00:01:28,439 
Additionally, we can see
that R1 comes fairly often, 
20 
00:01:28,439 --> 00:01:32,172 
whereas other measurements,
such as R2, are not as often. 
21 
00:01:37,361 --> 00:01:38,880 
We can open another terminal and 
22 
00:01:38,880 --> 00:01:42,040 
look at the key to remind ourselves
what these measurements mean. 
23 
00:01:53,639 --> 00:01:58,875 
The key is in wxt-520-format.txt. 
24 
00:01:58,875 --> 00:02:03,693 
We could run more
wxt-520-format.txt to view it. 
25 
00:02:14,792 --> 00:02:19,535 
If we go back to our live data,
we can see that the 19th 
26 
00:02:19,535 --> 00:02:24,500 
measurement here says Ta
was 22.5 degrees Celsius. 
27 
00:02:25,930 --> 00:02:30,116 
And look up here,
see that Ta is the air temperature. 
28 
00:02:30,116 --> 00:02:34,737 
The next measure we can see
that Dn was equal to 255D. 
29 
00:02:34,737 --> 00:02:39,340 
According to our key,
Dn is the wind direction minimum, and 
30 
00:02:39,340 --> 00:02:41,157 
the units are degrees. 
31 
00:02:46,417 --> 00:02:51,316 
We can also plot specific measurements
streaming live from the weather station. 
32 
00:02:54,056 --> 00:02:55,740 
Let's plot the wind speed average. 
33 
00:02:57,300 --> 00:03:02,819 
If we look at our key,
we see that the wind speed average is Sm. 
34 
00:03:02,819 --> 00:03:08,554 
So we can plot this by running
stream-plot-data.py sm. 
35 
00:03:20,501 --> 00:03:23,659 
This plots the data as the weather
station sends it to us. 
36 
00:03:27,501 --> 00:03:28,843 
If we look at the x-axis, 
37 
00:03:28,843 --> 00:03:31,960 
we can see that one measurement
comes in about every second. 
38 
00:03:43,199 --> 00:03:46,620 
We can plot other measurements by
choosing different fields from the key. 
39 
00:03:47,650 --> 00:03:56,350 
For example, we can plot the air pressure
by running stream-plot-data.py Pa. 
40 
00:03:56,350 --> 00:03:58,289 
Since Pa is the air pressure. 
41 
00:04:14,810 --> 00:04:18,720 
First thing we notice is that there's
only one measurement so far in the graph. 
42 
00:04:20,950 --> 00:04:23,710 
This means that the air pressure
measurements are not coming as 
43 
00:04:23,710 --> 00:04:25,840 
fast as the wind measurements. 
44 
00:04:25,840 --> 00:04:27,127 
In fact, we only got one. 
1 
00:00:00,008 --> 00:00:01,736 
In this hands-on activity, 
2 
00:00:01,736 --> 00:00:05,270 
we'll be looking at real time
data streaming from Twitter. 
3 
00:00:06,360 --> 00:00:08,750 
First, we'll open a terminal window and 
4 
00:00:08,750 --> 00:00:12,520 
cd into the directory containing
Python scripts to access this data. 
5 
00:00:13,880 --> 00:00:17,750 
Next, we'll look at the contents
of tweets streaming from Twitter 
6 
00:00:17,750 --> 00:00:19,220 
containing specific words. 
7 
00:00:20,400 --> 00:00:23,230 
Finally, we will plot the frequency
of these streaming tweets. 
8 
00:00:24,500 --> 00:00:25,810 
Let's begin. 
9 
00:00:25,810 --> 00:00:29,259 
First, click on the terminal
icon at the top of the toolbar. 
10 
00:00:33,485 --> 00:00:38,414 
Let cd into the directory containing
the python scripts to access the real time 
11 
00:00:38,414 --> 00:00:40,400 
data from Twitter. 
12 
00:00:40,400 --> 00:00:45,473 
We'll run CD downloads big-data-2 json. 
13 
00:00:52,538 --> 00:00:55,828 
We can run ls to see
the files in this directory. 
14 
00:01:01,678 --> 00:01:07,200 
The file auth should be created containing
your Twitter authentication information. 
15 
00:01:08,200 --> 00:01:11,434 
You could see the separate reading for
how to setup the Twitter app and 
16 
00:01:11,434 --> 00:01:12,617 
how to create this file. 
17 
00:01:16,023 --> 00:01:20,958 
Next, let's run the script live tweets
to view tweets in real time containing 
18 
00:01:20,958 --> 00:01:22,093 
a specific word. 
19 
00:01:22,093 --> 00:01:27,819 
Let's run LiveTweets.py president. 
20 
00:01:33,534 --> 00:01:36,267 
This will show the real time tweets
containing the word president. 
21 
00:01:42,171 --> 00:01:45,280 
[INAUDIBLE] Runs, in the first column,
you can see the time stamp of the tweet. 
22 
00:01:45,280 --> 00:01:48,500 
In the second column,
you can see the text. 
23 
00:01:54,220 --> 00:01:56,471 
When you're done, hit Ctrl + C. 
24 
00:02:01,208 --> 00:02:04,836 
Let's run LiveTweet again using
a different keyword that appear more 
25 
00:02:04,836 --> 00:02:06,260 
frequently. 
26 
00:02:06,260 --> 00:02:10,448 
Let's use the word time,
we'll run LiveTweet time 
27 
00:02:24,118 --> 00:02:30,780 
When we're done, hit Ctrl + C. 
28 
00:02:30,780 --> 00:02:34,900 
We can plot the frequency of these Tweets
by running the script plot Tweets. 
29 
00:02:37,390 --> 00:02:41,373 
Let's run plot, tweet, president to see
the frequency for the word president. 
30 
00:02:57,849 --> 00:03:01,466 
As this runs, we could see
the frequency changes over time. 
31 
00:03:19,557 --> 00:03:21,960 
I can see that the maximum was one. 
32 
00:03:23,130 --> 00:03:25,540 
And a few times,
there were just one tweet in that second. 
33 
00:03:29,380 --> 00:03:31,180 
When you're done looking at the graph, 
34 
00:03:31,180 --> 00:03:32,930 
click in the terminal window and
hit Enter. 
35 
00:03:35,940 --> 00:03:38,010 
Now, let's plot the frequency for
the word time. 
36 
00:03:39,040 --> 00:03:42,645 
Run PlotTweets.py time. 
37 
00:03:58,468 --> 00:04:02,500 
This plot shows that the word time appears
a lot more frequently than president. 
38 
00:04:03,800 --> 00:04:09,514 
We can see spikes in the frequency
of 40 and a maximum of around 65. 
39 
00:04:09,514 --> 00:04:12,663 
When you're done looking at the graph, 
40 
00:04:12,663 --> 00:04:16,840 
click on the terminal window and
press enter to quit. 
1 
00:00:01,000 --> 00:00:05,890 
With big data streaming from different
sources in varying formats, models, and 
2 
00:00:05,890 --> 00:00:10,650 
speeds it is no surprise that we
need to be able to ingest this data 
3 
00:00:10,650 --> 00:00:13,970 
into a fast and scalable storage system 
4 
00:00:13,970 --> 00:00:18,900 
that is flexible enough to serve many
current and future analytical processes. 
5 
00:00:20,410 --> 00:00:25,730 
This is when traditional data warehouses
with strict data models and data 
6 
00:00:25,730 --> 00:00:31,160 
formats don't fit the big data challenges
for streaming and batch applications. 
7 
00:00:32,760 --> 00:00:38,700 
The concept of a data lake was created in
response of these data big storage and 
8 
00:00:38,700 --> 00:00:39,890 
processing challenges. 
9 
00:00:41,140 --> 00:00:45,460 
After this video you
will be able to describe 
10 
00:00:45,460 --> 00:00:48,880 
how data lakes enable batch
processing of streaming data. 
11 
00:00:50,410 --> 00:00:55,390 
Explain the difference between
schema on write and schema on read. 
12 
00:00:56,680 --> 00:01:00,320 
Organize data streams and data lakes and 
13 
00:01:00,320 --> 00:01:04,720 
data warehouses on a spectrum of
big data management and storage. 
14 
00:01:07,080 --> 00:01:08,080 
What is a data lake? 
15 
00:01:09,140 --> 00:01:15,450 
Simply speaking, a data lake is
a part of a big data infrastructure 
16 
00:01:15,450 --> 00:01:20,270 
that many streams can flow into and 
17 
00:01:20,270 --> 00:01:23,680 
get stored for
processing in their original form. 
18 
00:01:23,680 --> 00:01:28,140 
We can think of it as a massive
storage depository with huge 
19 
00:01:28,140 --> 00:01:33,560 
processing power and ability to handle
a very large number of concurrence, 
20 
00:01:33,560 --> 00:01:35,970 
data management and analytical tasks. 
21 
00:01:37,370 --> 00:01:42,660 
In 2010,
the Pentaho Corporation's CTO James Dixon 
22 
00:01:42,660 --> 00:01:44,510 
defined a data link as follows. 
23 
00:01:45,930 --> 00:01:50,830 
If you think of a datamart as a store
of bottled water, cleansed and 
24 
00:01:50,830 --> 00:01:54,090 
packaged and structured for
easy consumption, 
25 
00:01:54,090 --> 00:01:59,320 
the data lake is a large body of
water in a more natural state. 
26 
00:01:59,320 --> 00:02:04,940 
The contents of the data lake stream
in from a source to fill the lake, 
27 
00:02:04,940 --> 00:02:11,980 
and various users of the lake can come
to examine it, dive in, or take samples. 
28 
00:02:11,980 --> 00:02:14,000 
A data lake works as follows. 
29 
00:02:15,140 --> 00:02:17,600 
The data gets loaded from its source, 
30 
00:02:19,160 --> 00:02:22,140 
stored in its native
format until it is needed 
31 
00:02:23,330 --> 00:02:28,240 
at which time the applications can freely
read the data and add structure to it. 
32 
00:02:29,500 --> 00:02:33,250 
This is what we call schema on read. 
33 
00:02:33,250 --> 00:02:38,070 
In a traditional data warehouse,
the data is loaded into the warehouse 
34 
00:02:38,070 --> 00:02:42,670 
after transforming it into a well
defined and structured format. 
35 
00:02:42,670 --> 00:02:45,398 
This is what we call schema on write. 
36 
00:02:45,398 --> 00:02:51,078 
Any application using the data needs to
know this format in order to retrieve and 
37 
00:02:51,078 --> 00:02:52,110 
use the data. 
38 
00:02:53,330 --> 00:02:54,990 
In this approach, 
39 
00:02:54,990 --> 00:02:58,882 
data is not loaded into the warehouse
unless there is a use for it. 
40 
00:02:58,882 --> 00:03:04,030 
However, schema on read
approach of data lakes ensures 
41 
00:03:04,030 --> 00:03:09,040 
all data is stored for
a potentially unknown use at a later time. 
42 
00:03:09,040 --> 00:03:12,110 
So how is a data lake
from a data warehouse? 
43 
00:03:13,540 --> 00:03:18,650 
A traditional data warehouse
stores data in a hierarchical file 
44 
00:03:19,820 --> 00:03:22,973 
system with a well-defined structure. 
45 
00:03:22,973 --> 00:03:29,590 
However, a data lake stores data as
flat files with a unique identifier. 
46 
00:03:29,590 --> 00:03:33,550 
This often gets referred to as
object storage in big data systems. 
47 
00:03:36,250 --> 00:03:42,330 
In data lakes each data is stored
as a binary large object or 
48 
00:03:42,330 --> 00:03:45,460 
BLOB and is assigned a unique identifier. 
49 
00:03:46,460 --> 00:03:53,620 
In addition, each data object is
tagged with a number of metadata tags. 
50 
00:03:54,860 --> 00:03:58,230 
The data can be searched
using these metadata tags 
51 
00:03:58,230 --> 00:04:00,400 
to retrieve it when there
is a need to access it. 
52 
00:04:01,820 --> 00:04:03,405 
From a users perspective, 
53 
00:04:03,405 --> 00:04:08,672 
metadata is stored is not a problem as
long as it is accessible when needed. 
54 
00:04:08,672 --> 00:04:15,780 
In Hadoop data architectures,
data is loaded into HDFS and processed 
55 
00:04:15,780 --> 00:04:20,410 
using the appropriate data management and
analytical systems on commodity clusters. 
56 
00:04:21,860 --> 00:04:27,080 
The selection of the tools is based on the
nature of the problem being solved, and 
57 
00:04:27,080 --> 00:04:28,540 
the data format being accessed. 
58 
00:04:30,090 --> 00:04:32,920 
We will talk more about
the processing of data streams and 
59 
00:04:32,920 --> 00:04:35,820 
data lakes in the next course
in this specialization. 
60 
00:04:37,520 --> 00:04:43,250 
To summarize a data lake is
a storage architecture for 
61 
00:04:43,250 --> 00:04:45,190 
big data collection and processing. 
62 
00:04:46,670 --> 00:04:50,060 
It enables collection of
all data suitable for 
63 
00:04:50,060 --> 00:04:53,060 
analysis today and
potentially in the future. 
64 
00:04:54,410 --> 00:04:57,450 
Regardless of the data source,
structure, and 
65 
00:04:57,450 --> 00:05:03,120 
format it supports storage of data and
transforms it only when it is needed. 
66 
00:05:04,500 --> 00:05:09,940 
A data lake ideally supports all parts
of the user base to benefit from 
67 
00:05:09,940 --> 00:05:15,790 
this architecture, including business,
storage, analytics and computing experts. 
68 
00:05:17,240 --> 00:05:21,310 
Finally, And perhaps most importantly, 
69 
00:05:21,310 --> 00:05:25,790 
data lakes are infrastructure components
within a big data architecture 
70 
00:05:25,790 --> 00:05:29,980 
that can evolve over time based
on application-specific needs. 
1 
00:00:01,870 --> 00:00:04,473 
What is a data stream? 
2 
00:00:04,473 --> 00:00:05,913 
After this video, 
3 
00:00:05,913 --> 00:00:11,323 
you will be able to summarize the key
characteristics of a data stream. 
4 
00:00:11,323 --> 00:00:16,263 
Identify the requirements of
streaming data systems, and 
5 
00:00:16,263 --> 00:00:20,323 
recognize the data streams
you use in your life. 
6 
00:00:20,323 --> 00:00:23,262 
When we talked about how
big data is generated and 
7 
00:00:23,262 --> 00:00:26,640 
the characteristics of the big
data using sound waves. 
8 
00:00:26,640 --> 00:00:32,460 
One of the challenges we mentioned was the
velocity of data coming in varying rates. 
9 
00:00:34,560 --> 00:00:39,360 
For some applications this
presents the need to process data 
10 
00:00:39,360 --> 00:00:43,540 
as it is generated, or
in other words, as it streams. 
11 
00:00:44,760 --> 00:00:48,770 
We call these types of applications
Streaming Data Processing Applications. 
12 
00:00:49,840 --> 00:00:55,630 
This terminology refers to a constant
stream of data flowing from a source, 
13 
00:00:55,630 --> 00:01:00,380 
for example data from a sensory machine or
data from social media. 
14 
00:01:02,753 --> 00:01:07,149 
An example application would be making
data-driven marketing decisions in 
15 
00:01:07,149 --> 00:01:08,390 
real time. 
16 
00:01:08,390 --> 00:01:12,790 
Through the use of data from
real-time sales trends, 
17 
00:01:12,790 --> 00:01:15,670 
social media analysis,
and sales distributions. 
18 
00:01:17,430 --> 00:01:23,070 
Another example for streaming data
processing is monitoring of industrial or 
19 
00:01:23,070 --> 00:01:25,250 
farming machinery in real time. 
20 
00:01:25,250 --> 00:01:30,075 
For monitoring and
detection of potential system failures. 
21 
00:01:30,075 --> 00:01:32,837 
In fact, any sensor network or 
22 
00:01:32,837 --> 00:01:38,576 
internet of things environment
controlled by another entity, 
23 
00:01:38,576 --> 00:01:42,733 
or set of entities falls
under this category. 
24 
00:01:42,733 --> 00:01:46,304 
For example,
as you have seen in an earlier video, 
25 
00:01:46,304 --> 00:01:48,640 
FlightStats is an application. 
26 
00:01:48,640 --> 00:01:52,942 
That processes about 60
million weekly flight events 
27 
00:01:52,942 --> 00:01:56,610 
that come into their
data acquisition system. 
28 
00:01:56,610 --> 00:02:00,978 
And turns it into real-time
intelligence for airlines and 
29 
00:02:00,978 --> 00:02:04,480 
millions of travelers
around the world daily. 
30 
00:02:07,520 --> 00:02:10,180 
So, how then do we define a data stream? 
31 
00:02:12,300 --> 00:02:17,268 
A stream is defined as
a possibly unbounded sequence of 
32 
00:02:17,268 --> 00:02:19,433 
data items or records. 
33 
00:02:19,433 --> 00:02:25,880 
That may or may not be related to,
or correlated with each other. 
34 
00:02:25,880 --> 00:02:30,952 
Each data is generally timestamped and 
35 
00:02:30,952 --> 00:02:34,670 
in some cases geo-tagged. 
36 
00:02:34,670 --> 00:02:39,063 
As you have seen in our examples,
the data can stream from many sources. 
37 
00:02:39,063 --> 00:02:44,403 
Including instruments, and
many internet of things application areas, 
38 
00:02:44,403 --> 00:02:48,480 
computer programs, websites,
or social media posts. 
39 
00:02:49,900 --> 00:02:54,360 
Streaming data sometimes get
referred to as event data as 
40 
00:02:54,360 --> 00:02:58,940 
each data item is treated as an individual
event in a synchronized sequence. 
41 
00:03:01,200 --> 00:03:04,070 
Streams pose very difficult challenges for 
42 
00:03:04,070 --> 00:03:06,940 
conventional data
management architectures. 
43 
00:03:06,940 --> 00:03:13,860 
Which are built primarily on the concept
of persistence, static data collections. 
44 
00:03:13,860 --> 00:03:18,219 
Due to the fact that most often we
have only one chance to look at and 
45 
00:03:18,219 --> 00:03:21,643 
process streaming data
before more gets piled on. 
46 
00:03:21,643 --> 00:03:26,597 
Streaming data management systems cannot
be separated from real-time processing 
47 
00:03:26,597 --> 00:03:27,160 
of data. 
48 
00:03:29,130 --> 00:03:30,160 
Managing and 
49 
00:03:30,160 --> 00:03:35,030 
processing data in motion is a typical
capability of streaming data systems. 
50 
00:03:36,060 --> 00:03:38,791 
However, the sheer size, variety and 
51 
00:03:38,791 --> 00:03:43,305 
velocity of big data adds further
challenges to these systems. 
52 
00:03:43,305 --> 00:03:49,260 
Such systems are designed to manage
relatively simple computations. 
53 
00:03:49,260 --> 00:03:52,640 
Such as one record at a time or 
54 
00:03:52,640 --> 00:03:57,070 
a set of objects in a short time
window of the most recent data. 
55 
00:03:58,460 --> 00:04:02,090 
The computations are done
in near-real-time, 
56 
00:04:02,090 --> 00:04:06,910 
sometimes in memory, and
as independent computations. 
57 
00:04:08,950 --> 00:04:14,099 
The processing components
often subscribe to a system, 
58 
00:04:14,099 --> 00:04:18,074 
or a stream source, non-interactively. 
59 
00:04:19,440 --> 00:04:24,580 
This means they sent nothing
back to the source, nor 
60 
00:04:24,580 --> 00:04:26,710 
did they establish
interaction with the source. 
61 
00:04:29,933 --> 00:04:35,133 
The concept of dynamic steering involves
dynamically changing the next steps or 
62 
00:04:35,133 --> 00:04:38,345 
direction of an application
through a continuous 
63 
00:04:38,345 --> 00:04:41,110 
computational process using streaming. 
64 
00:04:42,460 --> 00:04:48,817 
Dynamic steering is often a part of
streaming data management and processing. 
65 
00:04:48,817 --> 00:04:55,280 
A self-driving car is a perfect example
of a dynamic steering application. 
66 
00:04:56,350 --> 00:05:00,330 
But all streaming data applications
fall into this category. 
67 
00:05:00,330 --> 00:05:04,585 
Such as the online gaming example we
discussed earlier in this course. 
68 
00:05:04,585 --> 00:05:08,893 
Amazon Kinesis an other open-source Apache 
69 
00:05:08,893 --> 00:05:13,993 
projects like Storm, Flink,
Spark Streaming, and 
70 
00:05:13,993 --> 00:05:18,880 
Samza are examples of big
data streaming systems. 
71 
00:05:20,650 --> 00:05:23,770 
Many other companies also
provide streaming systems for 
72 
00:05:23,770 --> 00:05:26,920 
big data that are frequently
updated in response 
73 
00:05:26,920 --> 00:05:30,280 
to the rapidly changing
nature of these technologies. 
74 
00:05:32,440 --> 00:05:38,300 
As a summary, dynamic near-real-time
streaming data management, 
75 
00:05:38,300 --> 00:05:43,640 
processing, and steering is an important
part of today's big data applications. 
76 
00:05:44,710 --> 00:05:47,590 
Next, we will look at some
of the challenges for 
77 
00:05:47,590 --> 00:05:49,830 
streaming data management and processing. 
1 
00:00:01,280 --> 00:00:04,810 
Now that we have seen what
streaming data means, 
2 
00:00:04,810 --> 00:00:08,690 
let’s look at what makes streaming data
different and what some management and 
3 
00:00:08,690 --> 00:00:11,320 
processing challenges for
streaming data are. 
4 
00:00:12,510 --> 00:00:16,130 
After this video you will
be able to compare and 
5 
00:00:16,130 --> 00:00:18,980 
contrast data in motion and data at rest. 
6 
00:00:20,450 --> 00:00:23,990 
Differentiate between streaming and
batch data processing. 
7 
00:00:25,250 --> 00:00:31,070 
And list management and
processing challenges for streaming data. 
8 
00:00:31,070 --> 00:00:33,560 
We often hear the terms data addressed and 
9 
00:00:33,560 --> 00:00:36,665 
data in motion,
when talking about big data management. 
10 
00:00:36,665 --> 00:00:43,260 
Data-at-rest refers to mostly
static data collected from one or 
11 
00:00:43,260 --> 00:00:49,186 
more data sources, and the analysis
happens after the data is collected. 
12 
00:00:49,186 --> 00:00:54,920 
The term data-in-motion refers to a mode 
13 
00:00:54,920 --> 00:00:58,210 
although similar data
collection methods apply, 
14 
00:00:58,210 --> 00:01:02,490 
the data gets analyzed at the same
time it is being generated. 
15 
00:01:03,670 --> 00:01:09,300 
Just like the sensor data processing
in a plane or a self-driving car. 
16 
00:01:09,300 --> 00:01:14,610 
Analysis of data addressed is called
batch or static processing and 
17 
00:01:14,610 --> 00:01:18,080 
the analysis of streaming data
is called stream processing. 
18 
00:01:19,640 --> 00:01:24,633 
The run time and memory usage of most
algorithms that process static data, 
19 
00:01:24,633 --> 00:01:28,890 
is usually dependent on the data size, and 
20 
00:01:28,890 --> 00:01:33,010 
this size can easily be calculated
from files or databases. 
21 
00:01:34,670 --> 00:01:41,200 
A key property, of streaming data
processing is the size of the data 
22 
00:01:41,200 --> 00:01:46,650 
is unbounded and this changes the types
of algorithms that can be used. 
23 
00:01:48,050 --> 00:01:52,760 
Algorithms that require iterating or
looping over the whole data set are not 
24 
00:01:52,760 --> 00:01:57,470 
possible since with stream data,
you never get to the end. 
25 
00:01:58,730 --> 00:02:04,710 
The modeling and management of streaming
data should enable computations on 
26 
00:02:04,710 --> 00:02:09,720 
one data element or a small window
of group of recent data elements. 
27 
00:02:10,880 --> 00:02:14,820 
These computations can update metrics,
monitor and 
28 
00:02:14,820 --> 00:02:18,050 
plot statistics on the streaming data. 
29 
00:02:18,050 --> 00:02:22,120 
Or apply analysis techniques
to the streaming data 
30 
00:02:22,120 --> 00:02:26,010 
to learn about the dynamics of
the system as a time series. 
31 
00:02:27,120 --> 00:02:30,820 
Since computations need to
be completed in real time, 
32 
00:02:30,820 --> 00:02:35,460 
the analysis tasks processing
streaming data should be quicker or 
33 
00:02:35,460 --> 00:02:39,370 
not much longer than
the streaming rate of the data. 
34 
00:02:39,370 --> 00:02:41,200 
Which we define by it's velocity. 
35 
00:02:42,460 --> 00:02:45,970 
In most streaming systems,
the management, and 
36 
00:02:45,970 --> 00:02:51,020 
processing system subscribe to
the data source, but doesn't 
37 
00:02:51,020 --> 00:02:55,310 
send anything back to the stream source
in terms of feedback or interactions. 
38 
00:02:57,070 --> 00:03:01,530 
These requirements for streaming data
processing are quite different than batch 
39 
00:03:01,530 --> 00:03:06,330 
processing where the analytical steps
have access to often, all data and 
40 
00:03:06,330 --> 00:03:11,890 
can take more time to complete a complex
analytical task with less pressure 
41 
00:03:11,890 --> 00:03:16,360 
on the completion time of individual
data management and processing tasks. 
42 
00:03:18,120 --> 00:03:21,756 
Most organizations today
use a hybrid architecture. 
43 
00:03:21,756 --> 00:03:26,990 
Sometimes get referred to as
the lambda architecture for 
44 
00:03:26,990 --> 00:03:31,490 
processing streaming and
back jobs at the same time. 
45 
00:03:31,490 --> 00:03:38,130 
In these systems, streaming wheel over
the real-time data is managed and 
46 
00:03:38,130 --> 00:03:43,230 
kept until those data elements are pushed 
47 
00:03:43,230 --> 00:03:48,510 
to a batch system and become available
to access and process as batch data. 
48 
00:03:49,870 --> 00:03:54,770 
In such systems, a stream storage
layer is used to enable fast 
49 
00:03:54,770 --> 00:03:59,591 
trees of streams and
ensure data ordering and consistency. 
50 
00:03:59,591 --> 00:04:01,398 
And a processing layer for 
51 
00:04:01,398 --> 00:04:06,076 
data is used to retrieve data from
the storage layer to analyze it and 
52 
00:04:06,076 --> 00:04:11,003 
most probably little bit to a batch
data stream and notify the streaming 
53 
00:04:11,003 --> 00:04:16,810 
storage that the data set does no
longer need to be in streaming storage. 
54 
00:04:16,810 --> 00:04:22,200 
The big data challenges we discussed
were scalability, data replication, 
55 
00:04:22,200 --> 00:04:27,490 
and durability, and fault tolerance arise
in this type of data very significantly. 
56 
00:04:28,690 --> 00:04:34,260 
Among many there are two main
challenges that needs to be overcome 
57 
00:04:34,260 --> 00:04:39,820 
to avoid data loss, and
enable real time analytical tasks. 
58 
00:04:39,820 --> 00:04:44,950 
One challenge in streaming data
process is that the size and 
59 
00:04:44,950 --> 00:04:49,020 
frequency of the mean data can
significantly change over time. 
60 
00:04:50,330 --> 00:04:55,910 
These changes can be unpredictable and
may be driven by human behavior. 
61 
00:04:57,060 --> 00:05:02,340 
For example, streaming data found on
social networks such as Facebook and 
62 
00:05:02,340 --> 00:05:05,025 
Twitter can increase in
volume during holidays, 
63 
00:05:05,025 --> 00:05:08,160 
sports matches, or major news events. 
64 
00:05:09,980 --> 00:05:15,910 
These changes can be periodic and occur,
for example, in the evenings or weekends. 
65 
00:05:17,280 --> 00:05:22,470 
For example, people may post messages
on Facebook more in the evening 
66 
00:05:22,470 --> 00:05:25,940 
instead of during the day working hours. 
67 
00:05:25,940 --> 00:05:30,750 
Streaming data changes may also
be unpredictable and sporadic. 
68 
00:05:30,750 --> 00:05:33,510 
There can be an increase in data size and 
69 
00:05:33,510 --> 00:05:38,930 
frequency during during major events,
sporting matches and things like that. 
70 
00:05:38,930 --> 00:05:44,950 
Other changes include dropping or
missing data or even no data 
71 
00:05:44,950 --> 00:05:50,320 
when there are network problems or device
generating the data has hardware problems. 
72 
00:05:50,320 --> 00:05:52,880 
As an example of streaming
data fluctuation, 
73 
00:05:52,880 --> 00:05:54,964 
consider the number of Tweets per second. 
74 
00:05:54,964 --> 00:06:01,050 
On average,
there are 6,000 tweets sent every second. 
75 
00:06:01,050 --> 00:06:05,810 
However, in August 2013,
the world record was 
76 
00:06:05,810 --> 00:06:10,980 
set when over 144,000 tweets
were sent in a second. 
77 
00:06:10,980 --> 00:06:13,210 
That's a factor of 24 increase. 
78 
00:06:15,320 --> 00:06:20,250 
At the end of this lesson we will ask
you to focus on Twitter streams for 
79 
00:06:20,250 --> 00:06:23,160 
trending topics and any other topic. 
80 
00:06:24,480 --> 00:06:27,810 
You will notice how the rates
of Tweets streaming 
81 
00:06:27,810 --> 00:06:31,600 
changes between different times and
different topics. 
82 
00:06:31,600 --> 00:06:38,010 
To summarize, streaming data must be
handled differently than static data. 
83 
00:06:38,010 --> 00:06:43,150 
Unlike static data, where you can
determine the size, streaming data is 
84 
00:06:43,150 --> 00:06:48,570 
continually generated, and
you can not process it all at once. 
85 
00:06:50,250 --> 00:06:56,580 
Streaming data can unpredictably
change in both size and frequency. 
86 
00:06:56,580 --> 00:06:58,330 
This can be due to human behavior. 
87 
00:06:59,420 --> 00:07:01,910 
Finally, algorithms for 
88 
00:07:01,910 --> 00:07:06,830 
processing streaming data must
be relatively fast and simple. 
89 
00:07:06,830 --> 00:07:09,170 
Since you don't know when
the next data arrives. 
1 
00:00:02,380 --> 00:00:03,730 
In a previous lecture, 
2 
00:00:03,730 --> 00:00:08,440 
we said that new interesting solutions are
emerging in the big data product space. 
3 
00:00:09,780 --> 00:00:13,718 
While these solutions do not have
the full fledged power of a DBMS, 
4 
00:00:13,718 --> 00:00:18,800 
they offer novel feature combinations that
suit some application space just right. 
5 
00:00:19,970 --> 00:00:23,380 
One of these products is Aerospike, 
6 
00:00:23,380 --> 00:00:28,040 
which calls itself a distributed
NoSQL database and 
7 
00:00:28,040 --> 00:00:32,340 
key value store, and
goes on to say that it is architected for 
8 
00:00:32,340 --> 00:00:36,700 
the performance needs of
today's web scale applications. 
9 
00:00:36,700 --> 00:00:40,630 
The diagram here is from
an Aerospike whitepaper. 
10 
00:00:41,920 --> 00:00:45,990 
It shows how Aerospike relates to
the ecosystem for which it is designed. 
11 
00:00:47,340 --> 00:00:52,585 
The top layer shows several applications
for real time consumer facing systems, 
12 
00:00:52,585 --> 00:00:57,115 
such as travel recommendation systems,
pricing engines used for 
13 
00:00:57,115 --> 00:01:00,455 
stock market applications,
real time decision systems that 
14 
00:01:00,455 --> 00:01:04,145 
analyze data to figure out whether
an investment should be done and so forth. 
15 
00:01:05,405 --> 00:01:08,215 
Now, all of these systems
have the common need 
16 
00:01:08,215 --> 00:01:12,385 
that large amounts of data should be
accessible to them at any point of time. 
17 
00:01:14,090 --> 00:01:18,939 
The Aerospike system can interoperate
with Hadoop-based systems, so 
18 
00:01:18,939 --> 00:01:23,170 
Spark, or a Legacy database, or
even a real time data source. 
19 
00:01:23,170 --> 00:01:27,761 
It can exchange large volumes
of data with any such source and 
20 
00:01:27,761 --> 00:01:32,274 
serve fast lookups and
queries to the applications above. 
21 
00:01:32,274 --> 00:01:36,420 
Now that translates to a very
high availability robust and 
22 
00:01:36,420 --> 00:01:38,460 
strong consistency needs. 
23 
00:01:40,720 --> 00:01:46,600 
The figure here presents a high level
architecture diagram of Aerospike. 
24 
00:01:46,600 --> 00:01:51,010 
The first item to notice here
is what they call a fast bat, 
25 
00:01:52,410 --> 00:01:55,500 
which essentially refers to
the left side of the architecture. 
26 
00:01:57,200 --> 00:02:00,700 
The client system processes transactions. 
27 
00:02:00,700 --> 00:02:05,730 
That is data that are primarily managed in
a primary index that is a key value store. 
28 
00:02:07,180 --> 00:02:11,950 
This index stays in memory for
operational purposes. 
29 
00:02:11,950 --> 00:02:16,430 
However, the server also interacts with
the storage layer for persistence. 
30 
00:02:17,910 --> 00:02:21,290 
The storage layer uses three
kinds of storage systems, 
31 
00:02:22,770 --> 00:02:27,940 
in memory with a dynamic RAM or
DRAM, a regular spinning disk, 
32 
00:02:29,040 --> 00:02:34,190 
and flash/SSD, which is solid state device
for fast loading of data when needed. 
33 
00:02:35,410 --> 00:02:38,160 
In fact the Aerospike system 
34 
00:02:38,160 --> 00:02:42,210 
has optimized its performance with
characteristics of an SSD in mind. 
35 
00:02:44,320 --> 00:02:48,890 
For those of who are not sure
what an SSD is, you can think of 
36 
00:02:48,890 --> 00:02:53,500 
an SSD as a kind of storage device
whose random read performance is much 
37 
00:02:53,500 --> 00:02:57,610 
faster than speeding hard disk and the
write performance is just a little slower. 
38 
00:02:59,150 --> 00:03:04,556 
One vendor recently advertised its SSD
has sequence share read speeds of up to 
39 
00:03:04,556 --> 00:03:09,980 
2,500 MBPS and the sequential write
speeds as fast as 1,500 MBPS. 
40 
00:03:12,435 --> 00:03:14,810 
Now why is this important
in a big data discussion? 
41 
00:03:16,890 --> 00:03:21,300 
When we speak of scalability, grade
efficiency, fast transactions, and so 
42 
00:03:21,300 --> 00:03:25,940 
forth, we often do not mention
that part of the performance 
43 
00:03:25,940 --> 00:03:29,950 
guarantee is governed by the combination
of hardware and software. 
44 
00:03:31,200 --> 00:03:36,200 
So the ability to offer more efficient
persistent storage with fast IO 
45 
00:03:36,200 --> 00:03:41,540 
implies that while a significant amount
of information can be stored on disk, 
46 
00:03:41,540 --> 00:03:45,714 
it can be done without compromising
the overall system performance for 
47 
00:03:45,714 --> 00:03:48,819 
an environment that
needs fast data loading. 
48 
00:03:51,490 --> 00:03:55,410 
The second point of uniqueness
is a secondary index there. 
49 
00:03:56,760 --> 00:04:01,022 
Aerospike built secondary index
fields that are non-primary keys. 
50 
00:04:01,022 --> 00:04:05,192 
A non-primary key is a key attribute
that makes a tuple unique, 
51 
00:04:05,192 --> 00:04:07,560 
but it has not been
chosen as a primary key. 
52 
00:04:08,940 --> 00:04:12,980 
In Aerospike, secondary indices
are stored in main memory. 
53 
00:04:12,980 --> 00:04:17,770 
They are built on every node in a cluster
and co-located with the primary index. 
54 
00:04:18,960 --> 00:04:23,224 
Each secondary index entry
contains references to records, 
55 
00:04:23,224 --> 00:04:25,196 
which are local to the node. 
56 
00:04:27,240 --> 00:04:32,414 
As a key value store,
Aerospike uses standard database like 
57 
00:04:32,414 --> 00:04:39,450 
scalar types like integer, string, and
so forth, as well as lists like Reddis. 
58 
00:04:39,450 --> 00:04:46,340 
The map type is similar to the hashtag of
Reddis and contains attribute value pairs. 
59 
00:04:46,340 --> 00:04:52,390 
Since it is focused on real time web
application, it supports geospatial data, 
60 
00:04:52,390 --> 00:04:56,930 
like places with latitude and
longitude values or regency polygons. 
61 
00:04:59,020 --> 00:05:04,300 
This allows them to perform KV store
operations, for example, like is 
62 
00:05:04,300 --> 00:05:09,640 
the location of this in La Jolla,
which is a point-in-polygon query. 
63 
00:05:09,640 --> 00:05:14,120 
Or a distance query, like find hotels
within three miles of my location. 
64 
00:05:15,700 --> 00:05:18,230 
KV queries are constructed
programmatically. 
65 
00:05:19,750 --> 00:05:26,506 
Now interestingly, Aerospike also provides
a more declarative language called AQL. 
66 
00:05:26,506 --> 00:05:31,840 
AQL looks very similar to SQL, 
67 
00:05:31,840 --> 00:05:34,520 
the Standard Query Language for
relational databases. 
68 
00:05:35,830 --> 00:05:37,360 
A query like select name and 
69 
00:05:37,360 --> 00:05:43,460 
age from user star profiles projects
out the name and age values 
70 
00:05:43,460 --> 00:05:48,250 
from the profile's record set that
belongs to the ding space called users. 
71 
00:05:49,750 --> 00:05:53,080 
The language also allows
advocate functions, like sum and 
72 
00:05:53,080 --> 00:05:56,800 
average, and other user defined functions, 
73 
00:05:56,800 --> 00:06:00,379 
which the system may evaluate through
a map produced time operation. 
74 
00:06:03,160 --> 00:06:06,880 
We mentioned earlier that while most
medium assists today offer base 
75 
00:06:06,880 --> 00:06:11,850 
guarantees, Aerospike, despite being
a distributor information system, 
76 
00:06:11,850 --> 00:06:13,740 
actually offers ACID guarantees. 
77 
00:06:14,750 --> 00:06:17,250 
This is accomplished using
a number of techniques. 
78 
00:06:19,338 --> 00:06:23,100 
We'll consider a few of them to give you
a flavor of the mechanisms that current 
79 
00:06:23,100 --> 00:06:28,380 
systems use to balance between
large scale data management and 
80 
00:06:28,380 --> 00:06:32,010 
transaction management in a cluster
where nodes can join or leave. 
81 
00:06:33,800 --> 00:06:37,499 
You may recall that consistency
means two different things, 
82 
00:06:38,640 --> 00:06:42,460 
one is to ensure that all constraints,
like domain constraints, are satisfied. 
83 
00:06:43,940 --> 00:06:46,905 
The second meaning is applied
to distributor systems and 
84 
00:06:46,905 --> 00:06:51,320 
ensures all copies of a data
item in a cluster are in sync. 
85 
00:06:52,950 --> 00:06:56,040 
For operations and
single keys with replication and 
86 
00:06:56,040 --> 00:07:01,070 
secondary indices,
Aerospike provides immediate consistency 
87 
00:07:01,070 --> 00:07:04,720 
using synchronous writes to
replicas within the cluster. 
88 
00:07:05,860 --> 00:07:10,083 
Synchronous write means the write
process will be considered 
89 
00:07:10,083 --> 00:07:13,420 
successful only if
the replica is all subdated. 
90 
00:07:14,768 --> 00:07:18,829 
No other write is allowed on the record
while the object of replica is pending. 
91 
00:07:20,520 --> 00:07:24,949 
So what happens if there is an increase
in the number of write operations due to 
92 
00:07:24,949 --> 00:07:26,800 
an increase in ingestion rate? 
93 
00:07:27,890 --> 00:07:33,490 
In Aerospike, it is possible to relax
this immediate consistency condition 
94 
00:07:33,490 --> 00:07:37,950 
by bypassing some of
the consistency checks. 
95 
00:07:37,950 --> 00:07:42,670 
But if this is done, eventual
consistency will still be enforced. 
96 
00:07:45,080 --> 00:07:50,890 
Durability is achieved by storing data
in the flash SSD on every node and 
97 
00:07:50,890 --> 00:07:52,530 
performing direct reads from the flash. 
98 
00:07:54,000 --> 00:07:57,920 
Now durability is also maintained through
the process of replication because we have 
99 
00:07:57,920 --> 00:07:59,790 
multiple copies of data. 
100 
00:07:59,790 --> 00:08:05,870 
So even if one node fails, the latest copy
of the last data is available from one or 
101 
00:08:05,870 --> 00:08:08,190 
more replica nodes in the same cluster, 
102 
00:08:08,190 --> 00:08:11,350 
as well as in nodes residing
in remote clusters. 
103 
00:08:13,550 --> 00:08:16,250 
But does that just
contradict the CAP theorem? 
104 
00:08:17,525 --> 00:08:22,700 
The CAP theorem holds when
the network is partitioned. 
105 
00:08:24,220 --> 00:08:27,700 
That means when nodes in
different parts of the network 
106 
00:08:27,700 --> 00:08:29,360 
have different data content. 
107 
00:08:30,810 --> 00:08:35,750 
Aerospike reduces and tries to completely
eliminate the situation by making 
108 
00:08:35,750 --> 00:08:41,250 
sure that the master knows exactly
where all the other nodes are. 
109 
00:08:41,250 --> 00:08:44,260 
And the replication is
happening properly even when 
110 
00:08:44,260 --> 00:08:46,300 
the new nodes are joining the network. 
1 
00:00:02,850 --> 00:00:07,270 
In the previous modules, we talked
about data variety and streaming data. 
2 
00:00:08,410 --> 00:00:13,310 
In this module, we'll focus on a central
issue in large scale data processing and 
3 
00:00:13,310 --> 00:00:19,540 
management and that is when should
we use Hadoop or Yarn style system? 
4 
00:00:19,540 --> 00:00:24,230 
And when should we use a database system
that can perform parallel operations? 
5 
00:00:24,230 --> 00:00:28,840 
And then we'll explore how the state
of the art big data management systems 
6 
00:00:28,840 --> 00:00:31,370 
address these issues of volume and
variety. 
7 
00:00:32,930 --> 00:00:36,510 
We start with the problem
of high volume data and 
8 
00:00:36,510 --> 00:00:38,699 
two contrasting approaches for
handling them. 
9 
00:00:40,150 --> 00:00:42,590 
So after this video, you'll be able to 
10 
00:00:43,650 --> 00:00:47,270 
explain the various advantages of
using a DBMS over a file system. 
11 
00:00:48,560 --> 00:00:52,500 
Specify the differences between
parallel and distributed DBMS. 
12 
00:00:52,500 --> 00:00:57,362 
And briefly describe
a MapReduce-style DBMS and 
13 
00:00:57,362 --> 00:01:01,537 
its relationship with the current DBMSs. 
14 
00:01:01,537 --> 00:01:06,005 
In the early days,
when database systems weren't around or 
15 
00:01:06,005 --> 00:01:11,450 
just came in, databases were designed
as a set of application programs. 
16 
00:01:12,600 --> 00:01:17,390 
They were written to handle data that
resided in files in a file system. 
17 
00:01:18,450 --> 00:01:21,310 
However, soon this
approach led to problems. 
18 
00:01:22,520 --> 00:01:26,560 
First, there are multiple file formats. 
19 
00:01:26,560 --> 00:01:32,370 
And often, there was a duplication
of information in different files. 
20 
00:01:32,370 --> 00:01:36,940 
Or the files simply had inconsistent
information that was very hard to 
21 
00:01:36,940 --> 00:01:41,150 
determine, especially when the data was
large and the file content was complex. 
22 
00:01:43,550 --> 00:01:47,050 
Secondly, there wasn't
a uniform way to access data. 
23 
00:01:48,090 --> 00:01:52,120 
Each data access task, like finding
employees in a department sorted by their 
24 
00:01:52,120 --> 00:01:55,990 
salary versus finding
employees in all departments 
25 
00:01:55,990 --> 00:02:00,060 
sorted by their start date needed to
be written as a separate program. 
26 
00:02:01,160 --> 00:02:04,040 
So people ended up writing
different programs for 
27 
00:02:04,040 --> 00:02:06,200 
data access, as well as data update. 
28 
00:02:08,350 --> 00:02:11,890 
A third problem was rooted to
the enforcement of constraints, 
29 
00:02:11,890 --> 00:02:14,050 
often called integrity constraints. 
30 
00:02:14,050 --> 00:02:20,000 
For example, to say something like every
employee has exactly one job title. 
31 
00:02:20,000 --> 00:02:23,530 
One had arrived that condition,
as part of an application program called. 
32 
00:02:24,700 --> 00:02:27,670 
So if you want to change the constraint,
you need to look for 
33 
00:02:27,670 --> 00:02:29,620 
the programs where such
a rule is hard coded. 
34 
00:02:31,590 --> 00:02:35,300 
The fourth problem has to
do with system failures. 
35 
00:02:35,300 --> 00:02:39,420 
Supposed Joe, an employee becomes
the leader of a group and moves in to 
36 
00:02:39,420 --> 00:02:44,840 
the office of the old leader, Ben who has
now become the director of the division. 
37 
00:02:44,840 --> 00:02:49,788 
So we update Joe's details and
move on to update, Ben's new office for 
38 
00:02:49,788 --> 00:02:51,280 
the system crashes. 
39 
00:02:51,280 --> 00:02:54,501 
So the files are incompletely updated and 
40 
00:02:54,501 --> 00:02:58,096 
there is no way to go back,
and start all over. 
41 
00:02:58,096 --> 00:03:04,138 
The term atomicity means that all of
the changes that we need to do for 
42 
00:03:04,138 --> 00:03:09,657 
these promotions must happen altogether,
as a single unit. 
43 
00:03:09,657 --> 00:03:14,874 
They should either fully go through or
not go through at all. 
44 
00:03:14,874 --> 00:03:21,038 
This atomicity is very difficult to handle
when the data reside in one or more files. 
45 
00:03:23,936 --> 00:03:29,173 
So, a prime reason for the transition
to a DBMS is to alleviate these and 
46 
00:03:29,173 --> 00:03:30,842 
other difficulties. 
47 
00:03:30,842 --> 00:03:34,507 
If we look at the current DBMS,
especially relational DBMS, 
48 
00:03:34,507 --> 00:03:36,836 
we will notice a number of advantages. 
49 
00:03:39,165 --> 00:03:42,710 
DBMSs offer query languages,
which are declarative. 
50 
00:03:44,060 --> 00:03:48,200 
Declarative means that we
state what we want to retrieve 
51 
00:03:48,200 --> 00:03:51,060 
without telling the DBMS
how exactly to retrieve it. 
52 
00:03:52,220 --> 00:03:56,682 
In a relational DBMS, we can say,
find the average set of salary of 
53 
00:03:56,682 --> 00:04:01,800 
employees in the R&D division for
every job title and sort from high to low. 
54 
00:04:01,800 --> 00:04:05,779 
We don't have to tell the system how
to group these records by job title or 
55 
00:04:05,779 --> 00:04:07,879 
how to extract just the salary field. 
56 
00:04:10,073 --> 00:04:14,404 
A typical user of a DBMS who issues
queries does not worry about how 
57 
00:04:14,404 --> 00:04:19,604 
the relations are structured or whether
they are located in the same machine, 
58 
00:04:19,604 --> 00:04:21,835 
or spread across five machines. 
59 
00:04:21,835 --> 00:04:25,835 
The goal of data independence is to
isolate the users from the record 
60 
00:04:25,835 --> 00:04:28,993 
layout so long as the logical
definition of the data, 
61 
00:04:28,993 --> 00:04:33,083 
which means the tables and
their attributes are clearly specified. 
62 
00:04:35,195 --> 00:04:39,754 
Now most importantly, relational DBMSs
have developed a very mature and 
63 
00:04:39,754 --> 00:04:44,389 
continually improving methodology of
how to answer a query efficiently, 
64 
00:04:44,389 --> 00:04:47,255 
even when there are a large
number of cables and 
65 
00:04:47,255 --> 00:04:50,590 
the number of records
exceeds hundreds of millions. 
66 
00:04:51,800 --> 00:04:56,880 
From a 2009 account,
EB uses the tera data system with 
67 
00:04:56,880 --> 00:05:02,280 
72 machines to manage approximately
2.4 terabytes of relational data. 
68 
00:05:03,840 --> 00:05:08,496 
These systems have built powerful
data structures, algorithms and 
69 
00:05:08,496 --> 00:05:12,991 
sound principles to determine how
a specific array should be onset 
70 
00:05:12,991 --> 00:05:18,070 
efficiently despite the size of the data
and the complexity of the tables. 
71 
00:05:18,070 --> 00:05:22,790 
Now with any system,
bad things can happen. 
72 
00:05:22,790 --> 00:05:25,266 
Systems fail in the middle
of an operation. 
73 
00:05:25,266 --> 00:05:29,489 
Malicious processes try to get
unauthorized access to data. 
74 
00:05:29,489 --> 00:05:33,405 
One large can often underappreciated
aspect of a DBMS is 
75 
00:05:33,405 --> 00:05:39,060 
the implementation of transaction
safety and failure recovery. 
76 
00:05:39,060 --> 00:05:40,950 
Now, recall our discussion of atomicity. 
77 
00:05:42,110 --> 00:05:47,082 
In databases, a single logical operation
on the data is called a transaction. 
78 
00:05:47,082 --> 00:05:51,314 
For example, a transfer of funds
from one bank account to another, 
79 
00:05:51,314 --> 00:05:55,253 
even involving multiple changes
like debiting one account and 
80 
00:05:55,253 --> 00:05:58,091 
crediting another is a single transaction. 
81 
00:05:58,091 --> 00:06:02,894 
Now, atomicity is one of the four
properties that a transaction should 
82 
00:06:02,894 --> 00:06:04,350 
provide. 
83 
00:06:04,350 --> 00:06:09,740 
The four properties,
collectively called ACID are atomicity, 
84 
00:06:09,740 --> 00:06:13,370 
consistency, isolation and durability. 
85 
00:06:14,730 --> 00:06:19,370 
Consistency means any data
written to the database must be 
86 
00:06:19,370 --> 00:06:24,301 
valid according to all defined
rules including constrains. 
87 
00:06:24,301 --> 00:06:29,747 
The durability property ensures that
once a transaction has been committed, 
88 
00:06:29,747 --> 00:06:34,709 
it will remain so, even in the event
of power loss, crashes or errors. 
89 
00:06:36,816 --> 00:06:40,573 
The isolation property comes
in the context of concurrency, 
90 
00:06:40,573 --> 00:06:44,920 
which refers to multiple people
updating a database simultaneously. 
91 
00:06:45,970 --> 00:06:50,640 
To understand concurrency, think of
an airline or a railway reservation system 
92 
00:06:50,640 --> 00:06:54,330 
where hundreds and thousands of
people are buying, cancelling and 
93 
00:06:54,330 --> 00:06:57,110 
changing their reservations and
tickets all at the same time. 
94 
00:06:58,360 --> 00:07:02,710 
The DBMS must be sure that
a ticket should no be sold twice. 
95 
00:07:02,710 --> 00:07:06,920 
Or if one person is in the middle
of buying the last ticket, 
96 
00:07:06,920 --> 00:07:09,840 
another person does not see
that ticket as available. 
97 
00:07:11,380 --> 00:07:15,480 
These are guaranteed by
the isolation property that says, 
98 
00:07:15,480 --> 00:07:19,340 
not withstanding the number of people
accessing the system at the same time. 
99 
00:07:19,340 --> 00:07:26,310 
The transactions must happen as if they're
done serially, that is one after another. 
100 
00:07:26,310 --> 00:07:33,949 
Providing these capabilities is
an important part of the M in DBMS. 
101 
00:07:33,949 --> 00:07:38,390 
So next, we consider how traditional
databases handle large data volumes. 
102 
00:07:39,920 --> 00:07:44,093 
The classical way in which DBMSs
have handled the issue of large 
103 
00:07:44,093 --> 00:07:48,120 
volumes is by created parallel and
distributed databases. 
104 
00:07:48,120 --> 00:07:52,244 
In a parallel database, for
example, parallel Oracle, 
105 
00:07:52,244 --> 00:07:54,357 
parallel DB2 or post SQL XE. 
106 
00:07:54,357 --> 00:07:59,966 
The tables are spread across multiple
machines and operations like selection, 
107 
00:07:59,966 --> 00:08:03,853 
and join use parallel algorithms
to be more efficient. 
108 
00:08:03,853 --> 00:08:07,905 
These systems also allow a user
to create a replication. 
109 
00:08:07,905 --> 00:08:10,400 
That is multiple copies of tables. 
110 
00:08:10,400 --> 00:08:13,420 
Thus, introducing data redundancy, so 
111 
00:08:13,420 --> 00:08:18,180 
that failure on replica can be
compensated for by using another. 
112 
00:08:19,740 --> 00:08:23,678 
Further, it replicates in
sync with each other and 
113 
00:08:23,678 --> 00:08:27,260 
a query can result into
any of the replicates. 
114 
00:08:27,260 --> 00:08:32,021 
This increases the number of simultaneous
that is conquer into queries 
115 
00:08:32,021 --> 00:08:34,332 
that can be handled by the system. 
116 
00:08:34,332 --> 00:08:39,942 
In contrast, a distributed DBMS, which
we'll not discuss in detail in this course 
117 
00:08:39,942 --> 00:08:46,200 
is a network of independently running
DBMSs that communicate with each other. 
118 
00:08:46,200 --> 00:08:51,275 
In this case, one component knows some
part of the schema of it is neighboring 
119 
00:08:51,275 --> 00:08:55,670 
DBMS and can pass a query or part of
a query to the neighbor when needed. 
120 
00:08:57,520 --> 00:09:01,360 
So the important takeaway issue here is, 
121 
00:09:01,360 --> 00:09:04,890 
are all of these facilities
offered by a DBMS important for 
122 
00:09:04,890 --> 00:09:07,430 
the big data application that
you are planning to build? 
123 
00:09:08,590 --> 00:09:11,131 
And the answer in many
cases can be negative. 
124 
00:09:11,131 --> 00:09:16,304 
However, if these issues are important,
then the database management 
125 
00:09:16,304 --> 00:09:20,726 
systems may offer a viable option for
a big data application. 
126 
00:09:23,034 --> 00:09:27,726 
Now, let's take a little more time to
address an issue that's often discussed in 
127 
00:09:27,726 --> 00:09:28,819 
the big data word. 
128 
00:09:28,819 --> 00:09:32,300 
The question is if DBMSs are so powerful, 
129 
00:09:32,300 --> 00:09:37,178 
why do we see the emergence
of MapReduce-style Systems? 
130 
00:09:37,178 --> 00:09:41,800 
Unfortunately, the answer to this
question is not straightforward. 
131 
00:09:43,660 --> 00:09:49,180 
For a long while now, DBMSs have
effectively used parallelism, specifically 
132 
00:09:49,180 --> 00:09:54,030 
parallel databases in addition to
replication would also create partitions. 
133 
00:09:55,220 --> 00:10:00,017 
So that different parts of a logical
table can physically reside on different 
134 
00:10:00,017 --> 00:10:05,403 
machines,, then different parts of a query
can access the partitions in parallel and 
135 
00:10:05,403 --> 00:10:07,501 
speed up creative performance. 
136 
00:10:07,501 --> 00:10:11,753 
Now these algorithms not only improve
the operating efficiency, but 
137 
00:10:11,753 --> 00:10:16,882 
simultaneously optimize algorithms to
take into account the communication cost. 
138 
00:10:16,882 --> 00:10:21,511 
That is the time needed to
exchange data between machines. 
139 
00:10:21,511 --> 00:10:28,390 
However, classical parallel DBMSs did
not take into account machine failure. 
140 
00:10:29,700 --> 00:10:35,070 
And in contrast, MapReduce was
originally developed not for storage and 
141 
00:10:35,070 --> 00:10:38,850 
retrieval, but for distributive
processing of large amounts of data. 
142 
00:10:39,890 --> 00:10:44,783 
Specifically, its goal was to support
complex custom computations that could 
143 
00:10:44,783 --> 00:10:47,569 
be performed efficiently on many machines. 
144 
00:10:47,569 --> 00:10:50,259 
So in a MapReduce or MR setting, 
145 
00:10:50,259 --> 00:10:54,357 
the number of machines
could go up to thousands. 
146 
00:10:54,357 --> 00:10:58,710 
Now since MR implementations were
done over Hadoop file systems, 
147 
00:10:58,710 --> 00:11:02,760 
issues like node failure were
automatically accounted for. 
148 
00:11:04,492 --> 00:11:09,017 
So MR effectively used in complex
applications like data mining or 
149 
00:11:09,017 --> 00:11:13,703 
data clustering, and these algorithms
are often very complex, and 
150 
00:11:13,703 --> 00:11:17,202 
typically require problem
specific techniques. 
151 
00:11:17,202 --> 00:11:20,554 
Very often,
these algorithms have multiple stages. 
152 
00:11:20,554 --> 00:11:25,891 
That is the output from one processing
stage is the input to the next. 
153 
00:11:25,891 --> 00:11:29,519 
It is difficult to develop these
multistage algorithms in a standard 
154 
00:11:29,519 --> 00:11:30,650 
relational system. 
155 
00:11:31,990 --> 00:11:36,080 
But since these were genetic operations,
many of them were designed to work 
156 
00:11:36,080 --> 00:11:39,880 
with unstructured data like text and
nonstandard custom data formats. 
157 
00:11:41,730 --> 00:11:46,139 
Now, it's now amply clear that this
mixture of data management requirements 
158 
00:11:46,139 --> 00:11:50,818 
and data processing analysis requirements
have created an interesting tension in 
159 
00:11:50,818 --> 00:11:52,439 
the data management world. 
160 
00:11:52,439 --> 00:11:56,030 
Just look at a few of
these tension points. 
161 
00:11:56,030 --> 00:12:00,990 
Now, DBMSs perform storage and
retrieval operations very efficiently. 
162 
00:12:02,210 --> 00:12:05,170 
But first,
the data must be loaded into the DBMS. 
163 
00:12:06,230 --> 00:12:07,490 
So, how much time does loading take? 
164 
00:12:08,690 --> 00:12:12,900 
In one study,
scientists use two CVS files. 
165 
00:12:12,900 --> 00:12:17,850 
One had 92 attributes with
about 165 million tuples for 
166 
00:12:17,850 --> 00:12:19,890 
a total size of 85 gigabytes. 
167 
00:12:21,190 --> 00:12:25,710 
And the other had 227 attributes
with 5 million tuples for 
168 
00:12:25,710 --> 00:12:27,390 
a total size of 5 gigabytes. 
169 
00:12:28,850 --> 00:12:35,610 
The time to load and index this data in
MySQL and PostgreSQL, took 15 hours each. 
170 
00:12:36,690 --> 00:12:41,240 
In a commercial database running on
three machines, it took two hours. 
171 
00:12:41,240 --> 00:12:44,980 
Now there are applications like
the quantities in the case we discussed 
172 
00:12:44,980 --> 00:12:49,180 
earlier where this kind of loading
time is simply not acceptable, 
173 
00:12:49,180 --> 00:12:51,880 
because the analysis on
the data must be performed 
174 
00:12:51,880 --> 00:12:54,070 
within a given time limit
after it's arrival. 
175 
00:12:56,600 --> 00:12:59,780 
A second problem faced by
some application is that for 
176 
00:12:59,780 --> 00:13:03,140 
them, the DBMSs offer
too much functionality. 
177 
00:13:04,160 --> 00:13:08,460 
For example, think of an application
that only looks at the price of an item 
178 
00:13:08,460 --> 00:13:10,700 
if you provide it with a product name or
product code. 
179 
00:13:12,210 --> 00:13:14,850 
The number of products it serves
is let's say, 250 million. 
180 
00:13:16,170 --> 00:13:19,899 
This lookup operation happens
only on a single table and 
181 
00:13:19,899 --> 00:13:23,164 
does not mean anything
more complex like a join. 
182 
00:13:23,164 --> 00:13:27,963 
Further, consider that while there
are several hundred thousand customers 
183 
00:13:27,963 --> 00:13:31,810 
who access this data,
none of them really update the tables. 
184 
00:13:31,810 --> 00:13:36,685 
So, do we need a full function DBMS for
this read-only application? 
185 
00:13:36,685 --> 00:13:41,175 
Or can we get a simpler solution which
can use a cluster of machines, but 
186 
00:13:41,175 --> 00:13:44,275 
does not provide all the wonderful
guarantees that a DBMS provides? 
187 
00:13:46,600 --> 00:13:51,940 
At the other end of the spectrum,
there is an emerging class of optimization 
188 
00:13:51,940 --> 00:13:56,590 
that meets all the nice transactional
guarantees that a DBMS provides. 
189 
00:13:56,590 --> 00:14:00,500 
And at the same time, meets the support
for efficient analytical operations. 
190 
00:14:01,720 --> 00:14:05,741 
These are often required for
systems like Real-Time Decision Support. 
191 
00:14:05,741 --> 00:14:10,141 
That will accept real-time data like
customer purchases on a newly released 
192 
00:14:10,141 --> 00:14:13,188 
product will perform some
statistical analysis, so 
193 
00:14:13,188 --> 00:14:15,365 
that it can determine buying trends. 
194 
00:14:15,365 --> 00:14:17,969 
And then decide whether in real-time, 
195 
00:14:17,969 --> 00:14:21,040 
a discount can be offered
to this customer now. 
196 
00:14:23,860 --> 00:14:28,368 
It turns out that the combination
of traditional requirements and 
197 
00:14:28,368 --> 00:14:32,072 
new requirements is leading
to new capabilities, and 
198 
00:14:32,072 --> 00:14:35,465 
products in the big data
management technology. 
199 
00:14:35,465 --> 00:14:40,283 
On the one hand, DBMS technologies
are creating new techniques that make 
200 
00:14:40,283 --> 00:14:43,164 
use of MapReduce-style data processing. 
201 
00:14:43,164 --> 00:14:46,473 
Many of them are being
developed to run on HDFS and 
202 
00:14:46,473 --> 00:14:50,279 
take advantage of his data
replication capabilities. 
203 
00:14:50,279 --> 00:14:54,423 
More strikingly, DBMSs are beginning
to have a side door for 
204 
00:14:54,423 --> 00:14:58,405 
a user to perform and
MR-style operation on HDFS files and 
205 
00:14:58,405 --> 00:15:02,399 
exchange data between the Hadoop
subsystem and the DBMS. 
206 
00:15:02,399 --> 00:15:06,862 
Thus, giving the user the flexibility
to use both forms of data processing. 
207 
00:15:09,674 --> 00:15:12,562 
It has now been recognized
that a simple map and 
208 
00:15:12,562 --> 00:15:17,155 
reduce operations are not sufficient for
many data operations leading to 
209 
00:15:17,155 --> 00:15:22,740 
a significant expansion in the number
of operations in the MR ecosystems. 
210 
00:15:22,740 --> 00:15:26,510 
For example,
Spark has several kinds of join and 
211 
00:15:26,510 --> 00:15:29,350 
data grouping operations in
addition to map and reduce. 
212 
00:15:31,650 --> 00:15:34,900 
Sound DBMSs are making use of large 
213 
00:15:34,900 --> 00:15:38,190 
distributed memory management
operations to accept streaming data. 
214 
00:15:39,400 --> 00:15:43,510 
These systems are designed with the idea
that the analysis they need to perform on 
215 
00:15:43,510 --> 00:15:45,400 
the data are known before. 
216 
00:15:45,400 --> 00:15:49,860 
And as new data records arrive,
they keep a record of the data 
217 
00:15:49,860 --> 00:15:53,820 
in the memory long enough to finish
the computation needed on that record. 
218 
00:15:55,310 --> 00:15:57,610 
And finally, computer scientists and 
219 
00:15:57,610 --> 00:16:00,770 
data scientists are working
towards new solutions 
220 
00:16:00,770 --> 00:16:05,770 
where large scale distributed algorithms
are beginning to emerge to solve different 
221 
00:16:05,770 --> 00:16:09,280 
kinds of analytics problems like
finding dense regions of a graph. 
222 
00:16:10,610 --> 00:16:14,210 
These algorithms use
a MR-style computing and 
223 
00:16:14,210 --> 00:16:17,880 
are becoming a part of a new
generation of DBMS products 
224 
00:16:17,880 --> 00:16:20,860 
that invoke these algorithms
from inside the database system. 
225 
00:16:21,890 --> 00:16:25,720 
In the next video, we'll take a look at
some of the modern day data management 
226 
00:16:25,720 --> 00:16:28,030 
systems that have some
of these capabilities. 
1 
00:00:02,600 --> 00:00:07,300 
As we mentioned in the last lesson, there
is no single irricuteous solution for 
2 
00:00:07,300 --> 00:00:08,200 
big data problems. 
3 
00:00:08,200 --> 00:00:10,960 
So in this lesson, 
4 
00:00:10,960 --> 00:00:14,900 
our goal will be to explore some existing
solutions in a little more depth. 
5 
00:00:16,780 --> 00:00:19,290 
So after this lesson, you'll be able to. 
6 
00:00:20,540 --> 00:00:23,250 
Explain the at least five
desirable characteristics of 
7 
00:00:23,250 --> 00:00:29,150 
a Big Data Management System,
explain the differences between acid and 
8 
00:00:29,150 --> 00:00:32,070 
base, and 
9 
00:00:32,070 --> 00:00:37,210 
list examples of BDMSs and describe some
of their similarities and differences. 
10 
00:00:39,260 --> 00:00:44,890 
So we start from high level, suppose there
were an ideal big data management system. 
11 
00:00:46,170 --> 00:00:48,610 
What capabilities or
features should such a system have? 
12 
00:00:50,000 --> 00:00:53,700 
Professor Michael Carey of
the University of California Irvine 
13 
00:00:53,700 --> 00:00:56,750 
has described a number of characteristics. 
14 
00:00:56,750 --> 00:01:00,590 
We'll go through them and
use them as an idealistic yardstick 
15 
00:01:00,590 --> 00:01:03,190 
against which we compare
existing solutions. 
16 
00:01:04,500 --> 00:01:10,820 
First, the ideal BDMS would allow for
a semi-structured data model. 
17 
00:01:10,820 --> 00:01:15,060 
Now that does not mean it will only
support a specific format like XML. 
18 
00:01:16,170 --> 00:01:18,550 
The operative word here is flexible. 
19 
00:01:19,660 --> 00:01:22,390 
The flexibility can take many forms, 
20 
00:01:23,600 --> 00:01:27,920 
one of which Is the degree to which
schemas should be supported by the system. 
21 
00:01:29,190 --> 00:01:31,810 
In a perfect world, it should support 
22 
00:01:31,810 --> 00:01:35,550 
a completely traditional application which
requires the development of a schema. 
23 
00:01:37,250 --> 00:01:41,840 
At the same time, it should also support
applications which require no schema, 
24 
00:01:41,840 --> 00:01:45,420 
because the data can vary in terms
of its attributes and relationships. 
25 
00:01:47,790 --> 00:01:53,290 
A different axis of flexibility
is in the data types it supports. 
26 
00:01:53,290 --> 00:01:56,810 
For example, it should support
operations on text and documents. 
27 
00:01:57,920 --> 00:02:03,010 
It should also permit social media and
other data that have a time component and 
28 
00:02:03,010 --> 00:02:06,820 
need temporal operations, like before,
after, during, and so on. 
29 
00:02:08,260 --> 00:02:11,340 
Similarly, it should
allow spacial data and 
30 
00:02:11,340 --> 00:02:15,530 
allow operations like find all data
within a five mile radius of a landmark. 
31 
00:02:17,960 --> 00:02:20,290 
As we saw in a previous lesson, 
32 
00:02:20,290 --> 00:02:24,260 
a big advantage of a DBMS is that
is provides a query language. 
33 
00:02:25,800 --> 00:02:30,360 
There is a notion that query languages
present a steep learning curve for 
34 
00:02:30,360 --> 00:02:33,690 
data science people with no
background in computer science. 
35 
00:02:33,690 --> 00:02:37,900 
However, to effectively
manage large volumes of data, 
36 
00:02:37,900 --> 00:02:41,190 
it's often more convenient
to use a query language and 
37 
00:02:41,190 --> 00:02:45,720 
let the query processor automatically
determine optimal ways to receive data. 
38 
00:02:47,270 --> 00:02:50,440 
Now this query language may or
may not look like SQL, 
39 
00:02:50,440 --> 00:02:55,120 
which is the standard query language used
by the modern relational systems, but 
40 
00:02:55,120 --> 00:02:57,870 
it should at least be equally powerful. 
41 
00:02:59,080 --> 00:03:04,770 
Now this is not an unreasonable feature
given that most DBMS vendors offer their 
42 
00:03:04,770 --> 00:03:12,120 
own extension of SQL Of course it's not
enough to just have a good query language. 
43 
00:03:13,430 --> 00:03:18,230 
Today's big data systems must
have a parallel query engine 
44 
00:03:18,230 --> 00:03:20,080 
which will run on multiple machines. 
45 
00:03:21,480 --> 00:03:25,240 
The machines can be connected to
a shared nothing architecture, or 
46 
00:03:25,240 --> 00:03:28,760 
shared memory architecture,
or a shared cluster. 
47 
00:03:28,760 --> 00:03:34,260 
The shared nothing means two machines
do not share a disk or memory. 
48 
00:03:34,260 --> 00:03:36,750 
But this is a critical requirement for 
49 
00:03:36,750 --> 00:03:41,850 
any BDMS regardless of how complete
the supported query languages is for 
50 
00:03:41,850 --> 00:03:46,970 
efficiency sake Continuing with our list,
the next 
51 
00:03:46,970 --> 00:03:51,620 
capability of a BDMS is often
not emphasized as much as it should be. 
52 
00:03:52,890 --> 00:03:56,880 
Some applications working on top
of a BDMS will issue queries 
53 
00:03:56,880 --> 00:04:01,200 
which will only have a few conditions and
a few small objects to return. 
54 
00:04:01,200 --> 00:04:06,740 
But some applications, especially those
generated by other software tools or 
55 
00:04:06,740 --> 00:04:10,380 
machine learning algorithms can
have many many conditions and 
56 
00:04:10,380 --> 00:04:11,990 
can return many large objects. 
57 
00:04:13,090 --> 00:04:18,320 
In my own work, we have seen how internet
bots can blast an information system 
58 
00:04:18,320 --> 00:04:22,230 
with really large queries that
can potentially choke the system. 
59 
00:04:22,230 --> 00:04:23,580 
But that should not happen in a BDMS. 
60 
00:04:25,810 --> 00:04:28,080 
Now, we discussed streaming
data in a previous lesson. 
61 
00:04:29,600 --> 00:04:33,830 
In many cases,
a BDMS will have both streaming data, 
62 
00:04:33,830 --> 00:04:37,590 
which adds to the volume,
as well as to the large data that 
63 
00:04:37,590 --> 00:04:40,380 
need to be combined with the streaming
data to solve a problem. 
64 
00:04:41,430 --> 00:04:45,140 
An example would be to combine
streaming data from weather stations 
65 
00:04:45,140 --> 00:04:48,870 
with historical data to make
better predictions for wild fire. 
66 
00:04:51,610 --> 00:04:54,510 
We have discussed the definition,
significance and 
67 
00:04:54,510 --> 00:04:57,140 
importance of scalability before. 
68 
00:04:57,140 --> 00:05:00,130 
However, what a BDMS needs to guarantee 
69 
00:05:00,130 --> 00:05:05,260 
that it is designed to operate over a
cluster, possibly a cluster of hundred or 
70 
00:05:05,260 --> 00:05:09,450 
thousand of machines and
that it knows how to handle a failure. 
71 
00:05:11,040 --> 00:05:15,200 
Further the system should be able
to handling new machines joining or 
72 
00:05:15,200 --> 00:05:16,829 
existing machines leaving the cluster. 
73 
00:05:18,910 --> 00:05:23,530 
Finally, our BDMS must have
data management capabilities. 
74 
00:05:23,530 --> 00:05:29,080 
It should be easy to install, restart and
configure, provide high availability and 
75 
00:05:29,080 --> 00:05:33,340 
make operational management as
simple as possible even when 
76 
00:05:33,340 --> 00:05:37,880 
a BDMS is declined across data centers
that are possibly geographically apart 
77 
00:05:40,770 --> 00:05:45,580 
In a prior module, we discussed
the ACID properties of transactions and 
78 
00:05:45,580 --> 00:05:47,880 
said that BDMSs guarantee them. 
79 
00:05:49,250 --> 00:05:53,270 
For big data systems,
there is too much data and 
80 
00:05:53,270 --> 00:05:56,540 
too many updates from too many users. 
81 
00:05:56,540 --> 00:05:59,730 
So the effort to maintain ACID properties 
82 
00:05:59,730 --> 00:06:02,330 
May lead to a significant
slowdown of the system. 
83 
00:06:03,550 --> 00:06:07,590 
Now, this lead to the idea, that while
the ACID properties are still desirable, 
84 
00:06:08,840 --> 00:06:14,410 
it might be more practical to relax
the ACID conditions and replace them 
85 
00:06:14,410 --> 00:06:19,770 
with what's called the BASE properties,
beginning with Basic availability. 
86 
00:06:21,410 --> 00:06:24,060 
This states that the system 
87 
00:06:24,060 --> 00:06:26,600 
does guarantee the availability
of data in the following sense. 
88 
00:06:28,140 --> 00:06:32,710 
If you make a request,
there will be a response to that request. 
89 
00:06:32,710 --> 00:06:36,310 
But, the response could still
be failure to obtain data or 
90 
00:06:36,310 --> 00:06:39,800 
the data isn't Inconsistent state or
changing state. 
91 
00:06:41,370 --> 00:06:44,970 
Well this is not unusual because
it's much like waiting for 
92 
00:06:44,970 --> 00:06:46,540 
a check to clear a bank account. 
93 
00:06:49,270 --> 00:06:51,580 
Second, there is a soft state. 
94 
00:06:52,620 --> 00:06:57,650 
Which means the state of the system
is very likely to change over time. 
95 
00:06:57,650 --> 00:07:00,030 
So even during times without input, 
96 
00:07:00,030 --> 00:07:04,330 
there may be changes going on through
the system due to eventual consistency. 
97 
00:07:05,680 --> 00:07:08,190 
Thus the state of
the system is always soft. 
98 
00:07:09,640 --> 00:07:12,080 
And finally there's eventual consistency. 
99 
00:07:13,330 --> 00:07:18,010 
This means that the system will
eventually become consistent 
100 
00:07:18,010 --> 00:07:19,770 
once it stops receiving input. 
101 
00:07:21,870 --> 00:07:23,730 
When it stops receiving input, 
102 
00:07:23,730 --> 00:07:28,800 
the data will propagate to everywhere
that it should sooner or later go to but 
103 
00:07:28,800 --> 00:07:32,840 
in reality the system will
continue to receive input. 
104 
00:07:32,840 --> 00:07:36,840 
And it's not checking the consistency
of every transaction at every moment 
105 
00:07:36,840 --> 00:07:40,610 
because there's still lots
of transactions to process. 
106 
00:07:40,610 --> 00:07:45,904 
So, if you make a new Facebook post,
your friend in Zambia, who is supposed 
107 
00:07:45,904 --> 00:07:51,200 
to see your update but is served by a very
different data center in a different 
108 
00:07:51,200 --> 00:07:56,116 
geographic region, will certainly
see it but may not see right away. 
109 
00:07:58,272 --> 00:08:02,066 
Now for those of you who have a bit
of computer science background, 
110 
00:08:02,066 --> 00:08:06,744 
we just want to mention in passing that
there is actually some theoretical results 
111 
00:08:06,744 --> 00:08:08,260 
behind this relaxation. 
112 
00:08:09,490 --> 00:08:13,090 
The result comes from what's
called the CAP Theorem. 
113 
00:08:13,090 --> 00:08:16,840 
Also named Bauer's theorem after
the computer scientist Eric Bauer. 
114 
00:08:18,200 --> 00:08:23,410 
He states, that it is impossible for
a distributed computer system 
115 
00:08:23,410 --> 00:08:27,420 
to simultaneously provide all
three of the following guarantees. 
116 
00:08:28,680 --> 00:08:30,300 
Consistency. 
117 
00:08:30,300 --> 00:08:34,970 
It means all nodes see the
same data at any time. 
118 
00:08:37,080 --> 00:08:42,000 
Availability, which is a guarantee
that every request receives a response 
119 
00:08:42,000 --> 00:08:43,948 
about whether it succeeded or failed. 
120 
00:08:43,948 --> 00:08:48,150 
And Partition Tolerance. 
121 
00:08:48,150 --> 00:08:51,270 
Which means the system
continues to operate 
122 
00:08:51,270 --> 00:08:55,610 
despite arbitrary partitioning
due to network failures. 
123 
00:08:55,610 --> 00:08:59,740 
Now, most of the big data systems
available today will adhere 
124 
00:08:59,740 --> 00:09:04,130 
to these BASE properties,
although several modern systems 
125 
00:09:04,130 --> 00:09:08,010 
do offer the stricter ACID properties,
or at least several of them. 
126 
00:09:10,180 --> 00:09:13,060 
Now given the idealistic background
of what is desirable and 
127 
00:09:13,060 --> 00:09:17,060 
achievable in a big data system,
today's marketplace for 
128 
00:09:17,060 --> 00:09:20,100 
big data related products
looks somewhat like this. 
129 
00:09:21,680 --> 00:09:26,350 
Now this is Matt Turk's depiction of big
data products from a couple of years back. 
130 
00:09:27,750 --> 00:09:31,730 
You would notice that the products
are grouped into categories, like no SQL, 
131 
00:09:31,730 --> 00:09:36,340 
massively parallel databases, analytic
systems, real time systems, and so forth. 
132 
00:09:37,430 --> 00:09:42,050 
In this lesson, we'll do a quick
tour through a few of these products 
133 
00:09:43,690 --> 00:09:45,470 
from different areas of this landscape. 
134 
00:09:47,140 --> 00:09:53,070 
In each case our goal will to be assess
what aspects of our ideal BDMS they cover, 
135 
00:09:53,070 --> 00:09:56,240 
and whether they have obvious limitations. 
136 
00:09:56,240 --> 00:10:01,650 
Now we will not cover all features
of every system, but will highlight 
137 
00:10:01,650 --> 00:10:05,130 
those aspects of the system that
are relevant to our discussion on BDMS. 
1 
00:00:02,542 --> 00:00:07,110 
The first up in our rapid tour
of modern systems is Redis. 
2 
00:00:08,420 --> 00:00:12,690 
Redis calls itself an in-memory
data structure store. 
3 
00:00:12,690 --> 00:00:18,540 
In simple terms, Redis is not a full blown
DBMS, in the sense we discussed earlier. 
4 
00:00:19,620 --> 00:00:24,490 
It can persist data on disks, and
does so to save its state, but 
5 
00:00:24,490 --> 00:00:29,840 
it's intended use is to optimally
use memory and memory based methods 
6 
00:00:29,840 --> 00:00:33,840 
to make a number of common data
structures very fast for lots of users. 
7 
00:00:35,000 --> 00:00:37,649 
Here is a list of data
structures that Redis supports. 
8 
00:00:41,084 --> 00:00:46,410 
A good way to think about them is
to think of a data lookup problem. 
9 
00:00:46,410 --> 00:00:50,160 
Now, in the simplest case,
a lookup needs a key value pair 
10 
00:00:50,160 --> 00:00:54,650 
where the key is a string and
the value is also a string. 
11 
00:00:54,650 --> 00:00:59,000 
So for a lookup we provide the key and
get back the value. 
12 
00:00:59,000 --> 00:01:00,550 
Simple, right? 
13 
00:01:00,550 --> 00:01:01,050 
Let's see. 
14 
00:01:02,760 --> 00:01:05,510 
I'm sure you've seen captures like this. 
15 
00:01:07,090 --> 00:01:11,770 
These are small images used by websites
to ensure that the user is a human and 
16 
00:01:11,770 --> 00:01:12,370 
not a robot. 
17 
00:01:13,470 --> 00:01:17,810 
The images presented to the user,
who is supposed to write the text he or 
18 
00:01:17,810 --> 00:01:20,990 
she sees in the image, into a text box. 
19 
00:01:20,990 --> 00:01:23,010 
Upon success, the user is let in. 
20 
00:01:24,100 --> 00:01:28,900 
To implement this,
one obviously needs a key value stored, 
21 
00:01:28,900 --> 00:01:31,500 
where the key is the idea of the image. 
22 
00:01:31,500 --> 00:01:33,230 
And the value is the desired text. 
23 
00:01:34,610 --> 00:01:39,900 
Now what if we wanted to use
the image itself as the key, 
24 
00:01:39,900 --> 00:01:42,940 
instead of an ID number that
you generate separately? 
25 
00:01:44,580 --> 00:01:46,160 
The content of the image, 
26 
00:01:46,160 --> 00:01:51,400 
which let's say is a .jpg file,
can be thought of as a binary string. 
27 
00:01:51,400 --> 00:01:52,706 
But can it serve as a key then? 
28 
00:01:52,706 --> 00:01:56,050 
According to the Redis specification,
it can. 
29 
00:01:57,360 --> 00:02:00,050 
The Redis string can be binary and 
30 
00:02:00,050 --> 00:02:05,770 
can have a size of up to 512 megabytes,
although its internal limit is higher. 
31 
00:02:06,840 --> 00:02:12,169 
So, small images like this can indeed
be used as binary string keys. 
32 
00:02:14,040 --> 00:02:19,100 
In some application scenarios,
keys may have an internal structure. 
33 
00:02:19,100 --> 00:02:23,090 
For example,
product codes may have product family, 
34 
00:02:23,090 --> 00:02:28,040 
manufacturing batch, and the actual
product ID strung together into one ID. 
35 
00:02:29,250 --> 00:02:33,710 
The example shown here is a typical
Twitter style key to store the response 
36 
00:02:33,710 --> 00:02:35,500 
to comment one, two, three, four. 
37 
00:02:37,240 --> 00:02:40,980 
How long do we want to
keep the comment around, 
38 
00:02:40,980 --> 00:02:45,240 
this is a standard big data issue
when it comes to streaming data 
39 
00:02:45,240 --> 00:02:48,680 
whose data values have limited
utility beyond the certain period. 
40 
00:02:50,540 --> 00:02:52,000 
One typically would not look for 
41 
00:02:52,000 --> 00:02:55,110 
this response possibly three
months after the conversation. 
42 
00:02:56,410 --> 00:03:00,490 
One would certainly not like to keep
such a value in memory for a long time. 
43 
00:03:00,490 --> 00:03:04,250 
Because the memory is being used as
a cache for rapid access to current data. 
44 
00:03:05,510 --> 00:03:11,250 
In fact Redis has the ability
to delete an expired key and 
45 
00:03:11,250 --> 00:03:14,230 
can be made to call a function
to generate a new key. 
46 
00:03:16,210 --> 00:03:19,580 
An interesting side
benefits to structured keys 
47 
00:03:19,580 --> 00:03:22,830 
that it can encode
a hierarchy to the structure. 
48 
00:03:22,830 --> 00:03:23,490 
In the example, 
49 
00:03:23,490 --> 00:03:28,449 
we show keys that represent
increasingly finer subgroups of users. 
50 
00:03:30,290 --> 00:03:31,730 
With this structure, 
51 
00:03:31,730 --> 00:03:36,433 
a lookup on user.commercial.entertainment
will also retrieve values 
52 
00:03:36,433 --> 00:03:40,390 
from user.commercial.entertainment.movie
industry. 
53 
00:03:43,330 --> 00:03:49,390 
A slightly more complex case occurs
when the value is not atomic, 
54 
00:03:49,390 --> 00:03:54,300 
but a collection object like a list
which by definition is an ordered set. 
55 
00:03:55,720 --> 00:03:58,218 
An example of such a list
can come from Twitter, 
56 
00:03:58,218 --> 00:04:02,760 
that uses Redis list to store timelines. 
57 
00:04:02,760 --> 00:04:05,860 
Now borrowed from a Twitter presentation
about their timeline architecture 
58 
00:04:06,920 --> 00:04:10,880 
our timeline service can
take a specific ID, and 
59 
00:04:10,880 --> 00:04:14,550 
it quickly identifies all tweets of
this user that are in the cache. 
60 
00:04:16,010 --> 00:04:19,310 
These tweets are then populated
by the content of the tweet, 
61 
00:04:19,310 --> 00:04:21,460 
then returned as a result. 
62 
00:04:21,460 --> 00:04:26,000 
This list can be long, but the insertion
and delete operations on the list 
63 
00:04:26,000 --> 00:04:28,970 
can be performed in constant
time which is in milliseconds. 
64 
00:04:30,220 --> 00:04:35,110 
If a tweet is retweeted,
the IDs of those tweets are also added 
65 
00:04:35,110 --> 00:04:38,820 
to the list of the first tweet, as you can
see for the three cases in the figure. 
66 
00:04:41,099 --> 00:04:45,210 
When the lists are long,
space saving becomes an important issue. 
67 
00:04:46,480 --> 00:04:50,370 
So Redis employs a method called
Ziplists which essentially 
68 
00:04:50,370 --> 00:04:54,700 
compacts the size of the list in
memory without changing the content. 
69 
00:04:54,700 --> 00:04:57,620 
Often producing significant
reduction in memory used. 
70 
00:04:59,150 --> 00:05:01,890 
Of course,
while Ziplists are very efficient for 
71 
00:05:01,890 --> 00:05:06,240 
retrieval, they are a little more complex
for insertion and deletion operations. 
72 
00:05:08,100 --> 00:05:11,100 
Since Redis is an open source system, 
73 
00:05:11,100 --> 00:05:15,070 
Twitter made a few innovations
on the Redis data structures. 
74 
00:05:15,070 --> 00:05:20,470 
One of these innovations is that
they created lists of Ziplists. 
75 
00:05:20,470 --> 00:05:24,480 
This gave them the flexibility of
having constant timing insertions and 
76 
00:05:24,480 --> 00:05:28,970 
deletions and at the same time used the
compressed representation to save space. 
77 
00:05:30,880 --> 00:05:35,830 
In 2012, the timeline service has about 
78 
00:05:35,830 --> 00:05:41,220 
40 terabytes of main memory, serving
30 million user queries per second. 
79 
00:05:41,220 --> 00:05:44,589 
Running on over 6,000
machines in one data center. 
80 
00:05:45,870 --> 00:05:51,280 
For those interested I would like to point
you to two wonderful Twitter presentation 
81 
00:05:51,280 --> 00:05:55,200 
explaining Twitter's use of Redis
among other design issue for 
82 
00:05:55,200 --> 00:05:56,790 
a realtime data system like Twitter. 
83 
00:05:58,740 --> 00:06:01,800 
We also introduce links in
the supplemental readings for this lesson. 
84 
00:06:05,843 --> 00:06:10,949 
Now the value to be looked up by the keys
can actually be more complicated and 
85 
00:06:10,949 --> 00:06:15,260 
can be records containing
attribute value pairs themselves. 
86 
00:06:17,110 --> 00:06:18,939 
Redis values can be hashes 
87 
00:06:20,010 --> 00:06:24,970 
which are essentially named containers
of unique fields and their values. 
88 
00:06:26,010 --> 00:06:29,210 
In the example, the key, std:101, 
89 
00:06:29,210 --> 00:06:33,960 
is associated with five
attributed value pairs. 
90 
00:06:35,150 --> 00:06:38,330 
The hashed attributes
are stored very efficiently. 
91 
00:06:38,330 --> 00:06:43,570 
And even when the list of attributed
value pairs in a hash is really long, 
92 
00:06:43,570 --> 00:06:45,110 
retrieval is efficient. 
93 
00:06:48,250 --> 00:06:50,240 
Horizontal scalability or 
94 
00:06:50,240 --> 00:06:54,710 
scale out capabilities refers
to the ability of a system 
95 
00:06:54,710 --> 00:06:59,170 
to achieve scalability when the number
of machines it operates on is increased. 
96 
00:07:00,720 --> 00:07:06,380 
Redis allows data partitioning through
range partitioning and hash partitioning. 
97 
00:07:07,600 --> 00:07:13,110 
Rate partitioning takes a numeric key and
breaks up the range of keys into bins. 
98 
00:07:13,110 --> 00:07:18,610 
In this case, by bins of 10,000
each of bin is assigned a machine. 
99 
00:07:20,730 --> 00:07:24,940 
And ultimately, a partitioning is where
computing a hashing function on a key. 
100 
00:07:26,060 --> 00:07:28,360 
Suppose we have 10 machines. 
101 
00:07:28,360 --> 00:07:31,690 
We pick the key and use the hash
function to get back a number. 
102 
00:07:33,050 --> 00:07:37,560 
We represent the number, modular 10,
and the result, in this case, 
103 
00:07:37,560 --> 00:07:40,920 
2, is the machine to which
the record will be allocated. 
104 
00:07:43,979 --> 00:07:48,110 
Replication is accomplished in
Redis through master-slave mode. 
105 
00:07:50,350 --> 00:07:54,110 
The slaves have a copy of the master node. 
106 
00:07:54,110 --> 00:07:55,940 
And can serve read queries. 
107 
00:07:58,090 --> 00:08:00,850 
Clients write to the master node. 
108 
00:08:00,850 --> 00:08:03,280 
And master node replicates to the slaves. 
109 
00:08:05,310 --> 00:08:08,090 
Clients read from the slaves to
scale up the read performance. 
110 
00:08:09,190 --> 00:08:11,760 
The replication processes are synchronous. 
111 
00:08:11,760 --> 00:08:16,040 
That is that slaves do not get replicated
data, it locks them with each other. 
112 
00:08:17,360 --> 00:08:20,540 
However, the replication
process does ensure 
113 
00:08:20,540 --> 00:08:22,120 
that they're consistent with each other. 
1 
00:00:01,310 --> 00:00:04,510 
The next system we'll explore is Vertica. 
2 
00:00:04,510 --> 00:00:08,810 
Which is the relational DBMS
designed to operate on top of HTFS. 
3 
00:00:10,450 --> 00:00:13,960 
It belongs to a family of DBMS
architectures called column stores. 
4 
00:00:15,320 --> 00:00:18,410 
Other products in
the same family are UCDV, 
5 
00:00:18,410 --> 00:00:21,540 
Carrot Cell Xvelocity from Microsoft and
so forth. 
6 
00:00:23,070 --> 00:00:25,620 
The primary difference
between a row store and 
7 
00:00:25,620 --> 00:00:28,010 
a column store is shown
in the diagram here. 
8 
00:00:29,160 --> 00:00:32,094 
Logically, this table has five columns. 
9 
00:00:32,094 --> 00:00:35,935 
Emp number, department number, 
10 
00:00:35,935 --> 00:00:38,810 
hire date, employee last name and
employee first name. 
11 
00:00:40,940 --> 00:00:45,760 
In a row oriented design the database
internally organizes the record 
12 
00:00:45,760 --> 00:00:46,300 
two four by two. 
13 
00:00:48,390 --> 00:00:52,030 
In a column store,
the data is organized column wise. 
14 
00:00:53,500 --> 00:00:58,100 
So the nth number column is stored
separately from the department id 
15 
00:00:58,100 --> 00:00:59,280 
column and so forth. 
16 
00:01:00,740 --> 00:01:03,730 
Now suppose a query
needs to find the ID and 
17 
00:01:03,730 --> 00:01:09,100 
department ID of all employees who were
hired after first of January, 2001. 
18 
00:01:09,100 --> 00:01:12,990 
The system only needs to look up 
19 
00:01:12,990 --> 00:01:16,720 
the hire date column to figure
out which records qualify. 
20 
00:01:16,720 --> 00:01:18,670 
And then pick up the values of the ID, 
21 
00:01:18,670 --> 00:01:22,450 
and the department of ID columns for
the qualifying records. 
22 
00:01:22,450 --> 00:01:26,060 
The other columns are not touched. 
23 
00:01:26,060 --> 00:01:31,270 
So if a table has 30 to 50 columns, very
often only a few of them are needed for 
24 
00:01:31,270 --> 00:01:31,950 
any single query. 
25 
00:01:33,730 --> 00:01:40,440 
So for tables with 500 million rows and
40 columns a typical query is very fast, 
26 
00:01:40,440 --> 00:01:45,400 
and uses much less memory because a full
record is not used most of the time. 
27 
00:01:46,990 --> 00:01:51,780 
My own experience is that with
an application that needed a database 
28 
00:01:51,780 --> 00:01:53,830 
with 150 billion tuples in a table. 
29 
00:01:54,870 --> 00:01:58,860 
Accounting operation took a little
under three minutes to complete. 
30 
00:02:00,330 --> 00:02:03,890 
A second advantage of the column store
comes from the nature of the data. 
31 
00:02:05,350 --> 00:02:09,220 
In a column store,
data in every column is sorted. 
32 
00:02:10,400 --> 00:02:13,900 
The figure on the right shows
three sorted columns of a table 
33 
00:02:13,900 --> 00:02:15,110 
with three visible columns. 
34 
00:02:16,550 --> 00:02:18,940 
The bottom of the first column shown here 
35 
00:02:18,940 --> 00:02:22,670 
has many accurate sixteen
transactions on the same day. 
36 
00:02:24,340 --> 00:02:29,170 
The second column has customer ids which
can be numerically close to each other. 
37 
00:02:29,170 --> 00:02:31,930 
They are all within seventy
values of the first customer. 
38 
00:02:33,430 --> 00:02:38,020 
So in the first case we can just
say that the value is one, one, 
39 
00:02:38,020 --> 00:02:41,730 
2007, and
the next 16 records have this value. 
40 
00:02:42,800 --> 00:02:48,580 
That means we do not have to store
the next 16 values thus saving space. 
41 
00:02:50,770 --> 00:02:54,280 
Now this form of shortened
representation is called compression. 
42 
00:02:55,300 --> 00:03:00,070 
And this specific variety is
called run-length encoding or RLE. 
43 
00:03:01,760 --> 00:03:05,010 
Another form of encoding can
be seen in the second column. 
44 
00:03:07,030 --> 00:03:10,990 
Here the customer ids a long integer,
but for 
45 
00:03:10,990 --> 00:03:14,460 
the records shown there
are nearby numbers. 
46 
00:03:14,460 --> 00:03:18,420 
So, if we pick a value in the column and
just put the difference between 
47 
00:03:18,420 --> 00:03:22,340 
this value and
other values The difference will be small. 
48 
00:03:22,340 --> 00:03:25,380 
It means,
we'll need fewer bites to represent them. 
49 
00:03:27,330 --> 00:03:30,800 
This one of compression is called
Frame-of-reference encoding. 
50 
00:03:32,440 --> 00:03:36,520 
The lesson to remember here is
that compressed data presentation 
51 
00:03:36,520 --> 00:03:41,760 
can significantly reduce the total
size of a database and if BDMS. 
52 
00:03:41,760 --> 00:03:44,970 
Should use all such tricks
to improve space efficiency. 
53 
00:03:46,730 --> 00:03:50,020 
While the space efficiency and
performance of Vertica is impressive for 
54 
00:03:50,020 --> 00:03:54,490 
large data, one has to be careful
about how to design a system with it. 
55 
00:03:55,620 --> 00:03:57,570 
Like Google's Web Table, and 
56 
00:03:57,570 --> 00:04:02,844 
Apache Cassandra, Vertica allows
the declaration of column-groups. 
57 
00:04:04,180 --> 00:04:05,830 
These are columns like first name and 
58 
00:04:05,830 --> 00:04:08,730 
last name which are very
often accessed together. 
59 
00:04:09,980 --> 00:04:13,040 
In Vertica,
a column group is like a mini table 
60 
00:04:13,040 --> 00:04:15,640 
which is treated like a little
row storied in the system. 
61 
00:04:16,750 --> 00:04:22,070 
But because these groups represent
activists that are frequently co accessed 
62 
00:04:22,070 --> 00:04:24,500 
the grouping actually
improves performance. 
63 
00:04:25,560 --> 00:04:29,660 
So an application developer is
better off when the nature and 
64 
00:04:29,660 --> 00:04:33,150 
the frequency of user queries
are known to some degree. 
65 
00:04:33,150 --> 00:04:37,190 
And this knowledge is applies
to designing the column groups. 
66 
00:04:37,190 --> 00:04:41,200 
An important side effect of column
structure organization of vertical 
67 
00:04:41,200 --> 00:04:44,390 
Is that the writing of data into
Vertica is a little slower. 
68 
00:04:45,590 --> 00:04:50,110 
When rows are added to a table,
Vertica initially places 
69 
00:04:50,110 --> 00:04:53,750 
them in a row-wise data structure and 
70 
00:04:53,750 --> 00:04:58,029 
then converts them into a column-wise
data structure, which is then compressed. 
71 
00:04:59,370 --> 00:05:02,920 
This lowness can be perceptible for
large uploads or updates. 
72 
00:05:04,140 --> 00:05:08,050 
Vertica belongs to a new breed of
database systems that call themselves 
73 
00:05:08,050 --> 00:05:09,220 
analytical databases. 
74 
00:05:10,620 --> 00:05:14,780 
This means two slightly different things. 
75 
00:05:14,780 --> 00:05:18,110 
First, Vertica offers many more
statistical functions than in 
76 
00:05:18,110 --> 00:05:18,950 
classical DBMS. 
77 
00:05:20,140 --> 00:05:24,500 
For example one can perform
operations over a Window 
78 
00:05:25,728 --> 00:05:32,150 
to see an example consider a table of
stock ticks having a time attribute, 
79 
00:05:32,150 --> 00:05:35,520 
a stock name and
a value of the stock called bid here. 
80 
00:05:36,570 --> 00:05:41,610 
This shows that data values in
the table now we would like to compute 
81 
00:05:41,610 --> 00:05:45,220 
a moving average of
the bit every 40 seconds. 
82 
00:05:46,510 --> 00:05:48,770 
We show the query in a frame below. 
83 
00:05:48,770 --> 00:05:53,450 
Now we aren't going into the details of
the query just consider the blue part. 
84 
00:05:53,450 --> 00:05:57,930 
It says the average,
which is the AVG function in yellow. 
85 
00:05:57,930 --> 00:06:00,760 
It must be computed on
the last column which is bit. 
86 
00:06:01,920 --> 00:06:03,060 
But, this computation 
87 
00:06:04,200 --> 00:06:08,270 
must be over the range that is
computed on the timestamp call. 
88 
00:06:09,740 --> 00:06:15,877 
So, the range is defined by a 40
second row before that current row so, 
89 
00:06:15,877 --> 00:06:21,404 
here the computation of the average
advances for the stock abc. 
90 
00:06:21,404 --> 00:06:26,293 
And for each computation,
the system only considers the rows whose 
91 
00:06:26,293 --> 00:06:30,510 
timestamp is within 40 seconds
before the current row. 
92 
00:06:31,840 --> 00:06:35,630 
The table on the right shows
the result of the query. 
93 
00:06:35,630 --> 00:06:38,130 
The average value, 10.12. 
94 
00:06:38,130 --> 00:06:41,910 
Is the same as the actual value, because
there are no other rows within 40 seconds. 
95 
00:06:41,910 --> 00:06:45,963 
The next two result rows average
over the preceding rows, 
96 
00:06:45,963 --> 00:06:49,530 
was times R within 40
seconds of the current row. 
97 
00:06:50,650 --> 00:06:51,670 
When we get to the blue row, 
98 
00:06:51,670 --> 00:06:58,020 
that we notice that it occurs 1 minute
16 seconds after the previous row. 
99 
00:06:58,020 --> 00:07:00,950 
So we cannot consider the previous
sort in the computation. 
100 
00:07:00,950 --> 00:07:04,660 
Instead, the result is just the value
of the bid in the current row. 
101 
00:07:06,190 --> 00:07:10,180 
The takeaway from this example is
that analytical computations like 
102 
00:07:10,180 --> 00:07:14,260 
this are happening inside the database and
not in an external application. 
103 
00:07:15,880 --> 00:07:19,850 
This brings us to the second feature
of Vertica as an analytical database. 
104 
00:07:21,200 --> 00:07:25,710 
R is a well known free statistics
package that's used by statisticians, 
105 
00:07:25,710 --> 00:07:28,050 
data minors, predictive analytics experts. 
106 
00:07:28,050 --> 00:07:33,840 
Today, R can not only
read data from files, 
107 
00:07:33,840 --> 00:07:37,660 
but it can go to an SQL database and
grab data to perform statistical analysis. 
108 
00:07:39,300 --> 00:07:42,160 
Over time, R has evolved and 
109 
00:07:42,160 --> 00:07:47,830 
given rights to distributed R which
is a high performance platform for R. 
110 
00:07:49,430 --> 00:07:51,810 
As expected in this distributed setting, 
111 
00:07:51,810 --> 00:07:55,050 
the system operates in
a master slave mode. 
112 
00:07:55,050 --> 00:07:59,650 
The master node coordinates computations
by sending commands to the workers. 
113 
00:07:59,650 --> 00:08:02,520 
The worker nodes maintain
data partitions and 
114 
00:08:02,520 --> 00:08:05,470 
apply the computation
functions to the data. 
115 
00:08:05,470 --> 00:08:09,070 
Just getting the data parallelly, 
116 
00:08:09,070 --> 00:08:12,310 
the essential data structure
is a distributed array. 
117 
00:08:12,310 --> 00:08:15,266 
That is an array that is
partitioned as shown here. 
118 
00:08:15,266 --> 00:08:18,480 
Now in this diagram
the partitions are equal, but 
119 
00:08:18,480 --> 00:08:20,960 
in practice they may be
all different sizes. 
120 
00:08:20,960 --> 00:08:26,320 
On which, one can compute a function for
each of these mini-arrays. 
121 
00:08:26,320 --> 00:08:30,200 
The bottom diagram, shows a simple
work flow of constructing and 
122 
00:08:30,200 --> 00:08:31,690 
deploying a predictive model. 
123 
00:08:32,730 --> 00:08:34,460 
The role of Vertica here, 
124 
00:08:34,460 --> 00:08:38,620 
is that it's a data supplier to the worker
nodes of R, and a model consumer. 
125 
00:08:39,880 --> 00:08:43,730 
The data to be analyzed is
the output of the vertica query, 
126 
00:08:43,730 --> 00:08:48,010 
which is transferred in memory through
a protocol called vertica fast transfer 
127 
00:08:48,010 --> 00:08:49,780 
through distributed R as a dArray. 
128 
00:08:50,940 --> 00:08:52,120 
When the model is created in R, 
129 
00:08:52,120 --> 00:08:56,900 
it should come back as a code that
goes through vertica as a function. 
130 
00:08:58,070 --> 00:09:00,870 
This function can be
called from inside Vertica 
131 
00:09:00,870 --> 00:09:03,310 
as if it was a user defined function. 
132 
00:09:04,330 --> 00:09:07,690 
Now in sophisticated applications,
the features of the data needed for 
133 
00:09:07,690 --> 00:09:13,670 
predicted modeling will also be
computed inside the DBMS possibly 
134 
00:09:13,670 --> 00:09:16,389 
using the new analytical operations
of Vertica that we've just shown. 
135 
00:09:17,770 --> 00:09:21,190 
Now this will make future
computation much faster and 
136 
00:09:21,190 --> 00:09:24,930 
improve the efficiency of
the entire analytics process. 
137 
00:09:26,240 --> 00:09:31,920 
Going forward, we believe that most DBMS's
will want to play in the analytics field, 
138 
00:09:31,920 --> 00:09:33,290 
will support similar functions. 
1 
00:00:02,390 --> 00:00:06,010 
Most of you have heard of
MongoDB as a dominant store for 
2 
00:00:06,010 --> 00:00:07,770 
JSON style semi-structured data. 
3 
00:00:09,070 --> 00:00:11,200 
MongoDB is very popular and 
4 
00:00:11,200 --> 00:00:13,860 
there are a number of excellent
tutorials on it on the web. 
5 
00:00:15,460 --> 00:00:20,290 
In this module we would like to discuss
a relatively new big data management 
6 
00:00:20,290 --> 00:00:25,910 
system for semistructured data that's
currently being incubated by Apache. 
7 
00:00:25,910 --> 00:00:27,040 
It's called AsterixDB. 
8 
00:00:28,260 --> 00:00:32,500 
Originally, AsterixDB was conceived by
the University of California Irvine. 
9 
00:00:33,910 --> 00:00:36,800 
Since it is a full fledged DBMS, 
10 
00:00:36,800 --> 00:00:42,310 
it provides ACID guarantees to
understand the basic design of AsterixDB, 
11 
00:00:43,330 --> 00:00:48,140 
let's consider this incomplete JSON
snippet taken from an actual tweet. 
12 
00:00:50,120 --> 00:00:51,900 
We have seen the structure of JSON before. 
13 
00:00:53,000 --> 00:00:59,090 
Here we point out that entities and
user, the two parts 
14 
00:00:59,090 --> 00:01:03,910 
in blue are nested, that means embedded,
within the structure of the tweet. 
15 
00:01:05,640 --> 00:01:10,540 
If we represent a part of the schema of
this abbreviated structure in AsterixDB, 
16 
00:01:12,010 --> 00:01:13,270 
it will look like this. 
17 
00:01:14,890 --> 00:01:17,950 
Here a dataverse is like a name space for
data. 
18 
00:01:19,610 --> 00:01:22,860 
Data is declared in terms of data types. 
19 
00:01:22,860 --> 00:01:28,030 
The top type, which looks like
a standard data with stable declaration, 
20 
00:01:28,030 --> 00:01:32,030 
represents the user portion of the JSON
object that we highlighted before. 
21 
00:01:33,130 --> 00:01:35,520 
The type below represents the message. 
22 
00:01:36,750 --> 00:01:39,820 
Now, instead of nesting it like JSON. 
23 
00:01:39,820 --> 00:01:44,520 
The user attribute highlighted in
blue is declared to have the type 
24 
00:01:44,520 --> 00:01:49,240 
TwitterUserType, thus it captures
the hierarchical structure of JSON. 
25 
00:01:51,450 --> 00:01:55,560 
We should also notice that
the first type is declared as open. 
26 
00:01:57,160 --> 00:02:02,100 
It means that the actual data can have
more attributes than specified here. 
27 
00:02:03,640 --> 00:02:08,900 
In contrast, the TweetMessage
type is declared as closed, 
28 
00:02:08,900 --> 00:02:13,130 
meaning that the data instance must have
the same attributes as in the schema. 
29 
00:02:15,020 --> 00:02:19,890 
AsterixDB can handle spatial data as given
by the point data types shown in green. 
30 
00:02:21,380 --> 00:02:27,120 
The question mark at the end of the point
type says that this attribute is optional. 
31 
00:02:27,120 --> 00:02:29,960 
That means all instances need not have it. 
32 
00:02:31,780 --> 00:02:37,810 
Finally, the create dataset actually
asks the system to create a dataset 
33 
00:02:37,810 --> 00:02:43,010 
called TweetMessages, whose type is
the just declared quick message type. 
34 
00:02:44,730 --> 00:02:50,060 
AstrerixDB which runs on HDFS provides
several options for credit support. 
35 
00:02:53,350 --> 00:02:58,440 
First it has its own query language
called the Asterix query language 
36 
00:02:58,440 --> 00:03:01,450 
which resembles the XML
credit language query. 
37 
00:03:03,000 --> 00:03:05,340 
The details of this query language
are not important right now. 
38 
00:03:06,720 --> 00:03:11,150 
We are illustrating the structure of
a query just to show what it looks like. 
39 
00:03:12,330 --> 00:03:15,040 
This particular query asks for 
40 
00:03:15,040 --> 00:03:20,020 
all user objects from the dataset
TwitterUsers in descending order of their 
41 
00:03:20,020 --> 00:03:25,010 
follower count and in alphabetical
order of the user's preferred language. 
42 
00:03:26,740 --> 00:03:30,650 
What is more interesting and distinctive
is that AsterixDB has a creative 
43 
00:03:30,650 --> 00:03:36,409 
processing engine that can process
queries in multiple languages. 
44 
00:03:37,750 --> 00:03:41,680 
For its supported language
they've developed a way to 
45 
00:03:41,680 --> 00:03:46,550 
transfer the query into a set of low
level operations like select and 
46 
00:03:46,550 --> 00:03:49,895 
join which their query
exchange can support. 
47 
00:03:49,895 --> 00:03:54,520 
Further, they've determined how
a record described in one of these 
48 
00:03:54,520 --> 00:03:58,280 
languages can be transformed
into an Asterix. 
49 
00:03:58,280 --> 00:04:02,449 
In this manner, the support hive queries, 
50 
00:04:02,449 --> 00:04:06,410 
which is expressed in like this. 
51 
00:04:06,410 --> 00:04:11,630 
Xquery, Hadoop map reduce,
as wall as a new language 
52 
00:04:11,630 --> 00:04:15,930 
called SQL++ which extends SQL for JSON. 
53 
00:04:19,107 --> 00:04:20,781 
Like a typical DB BDms, 
54 
00:04:20,781 --> 00:04:25,660 
AsterixDB is designed to operate
on a cluster of machines. 
55 
00:04:25,660 --> 00:04:31,395 
The basic idea, not surprisingly,
is to use partition data parallellism. 
56 
00:04:31,395 --> 00:04:35,415 
Each data set is divided into
instances of various types 
57 
00:04:35,415 --> 00:04:39,685 
which can be decomposed to different
machines by either range partitioning or 
58 
00:04:39,685 --> 00:04:41,565 
hash partitioning like
we discussed earlier. 
59 
00:04:43,572 --> 00:04:47,292 
A runtime distributed execution
engine called Hyracks is used for 
60 
00:04:47,292 --> 00:04:51,172 
partitioned parallel
execution of query plans. 
61 
00:04:51,172 --> 00:04:54,762 
For example, let's assume we have
two relations, customers and orders, 
62 
00:04:54,762 --> 00:04:55,522 
as you can see here. 
63 
00:04:57,402 --> 00:05:00,249 
Our query is find the number of orders for 
64 
00:05:00,249 --> 00:05:03,946 
every market segment that
the customers belong to. 
65 
00:05:05,526 --> 00:05:10,936 
Now this query need a join operation
between the two relations, 
66 
00:05:10,936 --> 00:05:16,260 
using the O_CUSTKEY as a foreign
key of customer into orders. 
67 
00:05:17,450 --> 00:05:21,090 
It also needs a grouping operation,
which for 
68 
00:05:21,090 --> 00:05:25,330 
each market segment will pull together all
the orders which will then be counted. 
69 
00:05:26,530 --> 00:05:29,960 
You don't have to understand the details
of this diagram at this point. 
70 
00:05:29,960 --> 00:05:33,580 
We just want to point out that
the different parts of the query 
71 
00:05:33,580 --> 00:05:38,180 
that are being marked,
the customer filed here has two partitions 
72 
00:05:38,180 --> 00:05:41,760 
that reside on two nodes,
NC one and NC two respectively. 
73 
00:05:43,300 --> 00:05:47,200 
The orders file also has two partitions. 
74 
00:05:47,200 --> 00:05:49,787 
But each partition is dually replicated. 
75 
00:05:51,567 --> 00:05:58,660 
One can be accessed either of nodes NC3 or
NC2 and the other on NC1 and NC5. 
76 
00:06:01,540 --> 00:06:06,970 
Hyracks will also break up
the query into a number of jobs and 
77 
00:06:06,970 --> 00:06:10,740 
then fill it out which tasks can
be performed in parallel and 
78 
00:06:10,740 --> 00:06:13,020 
which ones must be
executed stage by stage. 
79 
00:06:14,220 --> 00:06:17,650 
This whole thing will be managed
by the cluster controller. 
80 
00:06:19,080 --> 00:06:22,930 
The cluster controller is also
responsible for replanning and 
81 
00:06:22,930 --> 00:06:27,050 
reexecuting of a job
if there is a failure. 
82 
00:06:27,050 --> 00:06:32,300 
AsteriskDB also has the provision 
83 
00:06:32,300 --> 00:06:35,990 
to accept real time data from external
data sources at multiple rates. 
84 
00:06:37,610 --> 00:06:40,160 
One way is from files in a directory path. 
85 
00:06:41,400 --> 00:06:43,040 
Consider the example of tweets. 
86 
00:06:44,040 --> 00:06:46,480 
As you have seen with the hands-on demo, 
87 
00:06:46,480 --> 00:06:52,350 
usually people acquire tweets by accessing
data through an api that twitter provides. 
88 
00:06:52,350 --> 00:06:55,590 
Very typically a certain volume of tweets,
lets say for 
89 
00:06:55,590 --> 00:07:00,010 
every 5 minutes, is accumulated into
a .json file in a specific directory. 
90 
00:07:00,010 --> 00:07:02,861 
The next 5 minutes,
in another .json file, and so forth. 
91 
00:07:04,891 --> 00:07:07,205 
The way to get this
data into asterisks DB, 
92 
00:07:07,205 --> 00:07:10,240 
is to first create an empty
data set called Tweets here. 
93 
00:07:11,490 --> 00:07:13,420 
The next task is to create a feed. 
94 
00:07:14,470 --> 00:07:16,830 
That is an externally resource. 
95 
00:07:16,830 --> 00:07:21,450 
One has to specify that it's coming from
the local file system called local fs here 
96 
00:07:22,640 --> 00:07:25,390 
and the location of the directory,
the format and 
97 
00:07:25,390 --> 00:07:27,850 
the data type it's going to copy it. 
98 
00:07:27,850 --> 00:07:31,100 
Next, the feed is connected
to the data set and 
99 
00:07:31,100 --> 00:07:33,770 
the system starts reading unread
files from the directory. 
100 
00:07:35,960 --> 00:07:41,170 
Another way for AsteriskDB to access
external data is directly from an API, 
101 
00:07:41,170 --> 00:07:43,020 
such as the Twitter API. 
102 
00:07:43,020 --> 00:07:47,750 
To do this,
one would create a dataset as before. 
103 
00:07:47,750 --> 00:07:51,460 
But this time the data feed is
not on the local file system. 
104 
00:07:52,580 --> 00:07:57,960 
Instead it uses the push Twitter
method which invokes the Twitter 
105 
00:07:57,960 --> 00:08:01,830 
client with the four authentication
parameters required by the API. 
106 
00:08:03,360 --> 00:08:06,700 
Once the feed is defined it is
connected to the data set as before. 
1 
00:00:02,986 --> 00:00:05,186 
Now we move to another Apache product for 
2 
00:00:05,186 --> 00:00:07,720 
large scale text data
searching called Solr. 
3 
00:00:09,100 --> 00:00:10,430 
Systems like Solr, and 
4 
00:00:10,430 --> 00:00:16,300 
its underlying text indexing engine, are
typically designed for search problems. 
5 
00:00:16,300 --> 00:00:18,629 
So they would typically be
part of a search engine. 
6 
00:00:19,860 --> 00:00:21,410 
But before we talk about Solr or 
7 
00:00:21,410 --> 00:00:26,770 
large scale text, we need to first
appreciate some fundamental challenges 
8 
00:00:26,770 --> 00:00:29,590 
when it comes to storing,
indexing, and matching text data. 
9 
00:00:31,690 --> 00:00:36,030 
The basic challenge comes from
the numerous ways in which 
10 
00:00:36,030 --> 00:00:40,660 
a text string may vary, making it
hard to define what a good match is. 
11 
00:00:41,830 --> 00:00:43,560 
Let us show some of these challenges. 
12 
00:00:44,750 --> 00:00:49,810 
Remember in each case,
we're asking whether the strings shown on 
13 
00:00:49,810 --> 00:00:52,560 
either side of the double
tilde sign should match. 
14 
00:00:55,100 --> 00:00:58,541 
The first issue is about spelling
variations and capitalization. 
15 
00:01:02,049 --> 00:01:05,600 
The second issue relates
structured strings. 
16 
00:01:05,600 --> 00:01:10,080 
Where different parts of a string
represent different kind of information. 
17 
00:01:10,080 --> 00:01:16,420 
We have seen this before for file paths,
URLs, and like in this case, product IDs. 
18 
00:01:16,420 --> 00:01:21,081 
The problem is that the searcher may
not always know the structure or 
19 
00:01:21,081 --> 00:01:24,775 
my job on misposition,
the internal punctuations. 
20 
00:01:28,014 --> 00:01:32,674 
The next problem is very common with
proper nouns which are represented in 
21 
00:01:32,674 --> 00:01:38,248 
various ways, including dropping a part of
the string, picking up only the initials. 
22 
00:01:38,248 --> 00:01:43,497 
But consider the last variation, should
BH Obama really match the full name? 
23 
00:01:46,797 --> 00:01:50,120 
The next example is about
frequently used synonyms. 
24 
00:01:51,210 --> 00:01:55,512 
If the document has one of the synonyms,
while the query uses another, 
25 
00:01:55,512 --> 00:01:56,742 
should they match? 
26 
00:01:59,679 --> 00:02:03,400 
Example 5 illustrates
a very common problem. 
27 
00:02:03,400 --> 00:02:05,710 
People use abbreviations all the time. 
28 
00:02:06,970 --> 00:02:09,160 
If we look at text and social media and 
29 
00:02:09,160 --> 00:02:12,930 
instant messaging, we see a much
wider variety of abbreviations. 
30 
00:02:14,270 --> 00:02:17,270 
How well should they met with
the real correct version of the term? 
31 
00:02:19,580 --> 00:02:22,510 
Now problem six is a special
case of problem five. 
32 
00:02:24,350 --> 00:02:26,690 
Many long nouns are shortened 
33 
00:02:26,690 --> 00:02:30,440 
because we take the first initial
letter of each significant word. 
34 
00:02:31,570 --> 00:02:37,580 
We say significant because we
drop words like of as shown here. 
35 
00:02:37,580 --> 00:02:38,870 
This is called initialism. 
36 
00:02:40,390 --> 00:02:44,490 
Just so you know, when an initialism
can be said like a real word, 
37 
00:02:44,490 --> 00:02:45,480 
it's called an acronym. 
38 
00:02:46,480 --> 00:02:51,620 
Thus IBM is an initialization,
but NATO is an acronym. 
39 
00:02:53,160 --> 00:02:58,250 
Now problem seven and
problem eight show two 
40 
00:02:58,250 --> 00:03:02,690 
different situation where we first must
decide what to do with the period sign. 
41 
00:03:04,100 --> 00:03:09,322 
In the first case, we should not
find a match because students and 
42 
00:03:09,322 --> 00:03:13,250 
American are in two different
sentences punctuated by a period sign. 
43 
00:03:14,720 --> 00:03:18,330 
But in the second case we should
find a match because the period 
44 
00:03:18,330 --> 00:03:20,310 
does not designate a sentence boundary. 
45 
00:03:23,055 --> 00:03:28,510 
Lucene, the engine on which Solr is
built is effectively not a database, 
46 
00:03:28,510 --> 00:03:30,030 
but a modern inverted index. 
47 
00:03:31,040 --> 00:03:32,020 
What's an inverted index? 
48 
00:03:33,620 --> 00:03:37,590 
Let's first define a vocabulary
as a collection of terms. 
49 
00:03:37,590 --> 00:03:42,090 
Where a term may be a single word or
it can be multiple words. 
50 
00:03:42,090 --> 00:03:45,030 
It can a single term or
it can be a collection of synonyms. 
51 
00:03:46,510 --> 00:03:48,730 
But how would a search
engine know what a term is? 
52 
00:03:49,795 --> 00:03:52,558 
We'll revisit this question
in a couple of slides. 
53 
00:03:52,558 --> 00:03:57,390 
For now, let's just say that if we
have a corpus of documents, we can 
54 
00:03:57,390 --> 00:04:01,690 
extract most of the terms and construct
a vocabulary for that collection. 
55 
00:04:03,500 --> 00:04:08,080 
We can then define occurrence, as a list 
56 
00:04:08,080 --> 00:04:12,270 
containing all the information necessary
for each term in the vocabulary. 
57 
00:04:13,780 --> 00:04:17,790 
It would include information like,
which documents have the term? 
58 
00:04:17,790 --> 00:04:21,050 
The positions in the document
where the term occurs. 
59 
00:04:21,050 --> 00:04:25,404 
We can then go on to compute the count of
the term in the document and the corpus. 
60 
00:04:26,540 --> 00:04:30,760 
Referring back to a previous module
in this course, we can also compute 
61 
00:04:30,760 --> 00:04:34,370 
the term frequency and inverse
document frequency for the collection. 
62 
00:04:36,470 --> 00:04:40,250 
An inverted index is
essentially an index which for 
63 
00:04:40,250 --> 00:04:45,850 
every term stores at least the ID of
the document where the term occurs. 
64 
00:04:45,850 --> 00:04:48,440 
Practically, other computed numbers or 
65 
00:04:48,440 --> 00:04:52,060 
properties associated with the terms
will also be included in the index. 
66 
00:04:54,570 --> 00:04:58,820 
Solr is an open source
enterprise search platform. 
67 
00:04:58,820 --> 00:05:04,190 
The heart of Solr is its search
functionality built for full text search. 
68 
00:05:04,190 --> 00:05:07,110 
However, Solr provides much
more than tech search. 
69 
00:05:08,620 --> 00:05:14,780 
It can take any structured document,
even CSV files 
70 
00:05:14,780 --> 00:05:19,490 
which can be broken up into fields,
and can index each field separately. 
71 
00:05:20,850 --> 00:05:24,640 
Full text indexes where text columns
are supplemented by indexes for 
72 
00:05:24,640 --> 00:05:29,735 
other types of data, including numeric
data, dates, geographic coordinates, and 
73 
00:05:29,735 --> 00:05:33,390 
fields where domains are limited
to an emergent set of values. 
74 
00:05:35,190 --> 00:05:40,230 
Solr provides other facilities
like faceted search and 
75 
00:05:40,230 --> 00:05:43,440 
highlighting of terms that match a query. 
76 
00:05:43,440 --> 00:05:46,970 
Now if you're not familiar
with the term faceted search, 
77 
00:05:46,970 --> 00:05:50,550 
let's look at the screenshot
from amazon.com. 
78 
00:05:50,550 --> 00:05:52,960 
I performed a search on the string,
Dell laptop. 
79 
00:05:54,260 --> 00:05:56,680 
Consider the highlighted
part of the image. 
80 
00:05:58,400 --> 00:06:02,230 
Each laptop record carries a lot of
attributes like the display size, 
81 
00:06:02,230 --> 00:06:04,499 
the processor speed,
the amount of memory, and so forth. 
82 
00:06:05,680 --> 00:06:10,480 
These attributes can be put into builds
like processor type has Intel i5, i7, etc. 
83 
00:06:11,950 --> 00:06:17,098 
Faceted search essentially extracts
the individual values of these fields and 
84 
00:06:17,098 --> 00:06:22,188 
displays them back to the user, usually
with a count of the number of records. 
85 
00:06:22,188 --> 00:06:26,466 
We see this in the upper
part of the marked portion, 
86 
00:06:26,466 --> 00:06:31,452 
which says there are 5619
laptops per 411 tablets. 
87 
00:06:31,452 --> 00:06:32,600 
These are called facets. 
88 
00:06:33,750 --> 00:06:37,810 
If a user clicks on a facet,
documents with only those values, 
89 
00:06:37,810 --> 00:06:40,250 
that's just the tablets,
will be presented back to the user. 
90 
00:06:42,380 --> 00:06:45,860 
Now let's get back to the question
of what a term is, and 
91 
00:06:45,860 --> 00:06:47,550 
how a Solr system should know it. 
92 
00:06:49,870 --> 00:06:55,730 
Solr allows the system designer to
specify how to parse a document, 
93 
00:06:55,730 --> 00:07:01,770 
by instructing how to tokenize
a document and how to filter it. 
94 
00:07:01,770 --> 00:07:06,720 
Tokenization is the process of
breaking down the characters read. 
95 
00:07:06,720 --> 00:07:10,540 
For example, one can break
the stream at white spaces, and 
96 
00:07:10,540 --> 00:07:13,170 
get all the words as tokens. 
97 
00:07:13,170 --> 00:07:17,803 
Then, it can filter out the punctuation
like the period, the apostrophe, and so 
98 
00:07:17,803 --> 00:07:19,631 
on, just to get the pure words. 
99 
00:07:21,984 --> 00:07:24,410 
The code snippet on the right
essentially achieves this. 
100 
00:07:25,510 --> 00:07:29,960 
It uses a standard tokenizer that gets
the words with immediate punctuation. 
101 
00:07:31,100 --> 00:07:34,730 
The first filter removes the punctuations. 
102 
00:07:34,730 --> 00:07:38,040 
The second filter turns
everything into lowercase. 
103 
00:07:38,040 --> 00:07:41,960 
And the third filter uses
a synonym file to ensure that 
104 
00:07:41,960 --> 00:07:46,140 
all the synonyms get the same
token after ignoring the case. 
105 
00:07:46,140 --> 00:07:50,828 
The last filter removes common
English words like a and the. 
106 
00:07:52,401 --> 00:07:56,330 
While a similar process would need to
happen when we get the query string. 
107 
00:07:58,080 --> 00:08:01,245 
It will also need to go through a
tokenization and token filtering process. 
108 
00:08:03,933 --> 00:08:07,918 
In the query analyzer example, there
are six filters within the tokenizer. 
109 
00:08:09,060 --> 00:08:12,130 
We use a pattern tokenizer which
will remove the white spaces and 
110 
00:08:12,130 --> 00:08:15,020 
periods and semi-colon. 
111 
00:08:15,020 --> 00:08:19,710 
The common grams filter creates
tokens out of pairs of terms, and 
112 
00:08:19,710 --> 00:08:23,470 
in doing so, makes sure that words
in the stopword file are used. 
113 
00:08:23,470 --> 00:08:24,750 
So if we have the string, 
114 
00:08:24,750 --> 00:08:29,560 
the cat, the term the should
not be ignored in this filter. 
115 
00:08:31,240 --> 00:08:34,520 
Now, the filters here
are executed in order. 
116 
00:08:36,180 --> 00:08:39,720 
After the common grams
filter is already done, 
117 
00:08:39,720 --> 00:08:42,500 
the next filter removes all
the stopwords in the file. 
118 
00:08:44,320 --> 00:08:49,190 
The fifth filter makes sure that if
the queries coming from a web form, 
119 
00:08:49,190 --> 00:08:51,240 
all the HTML characters are stripped off. 
120 
00:08:52,490 --> 00:08:57,360 
Finally, the remaining words are stemmed,
so that runs and 
121 
00:08:57,360 --> 00:09:01,440 
running in the query would match
the word run in the document. 
122 
00:09:04,170 --> 00:09:08,935 
We'll end this section with a discussion
on Solr queries, that is searches. 
123 
00:09:12,370 --> 00:09:16,450 
We present a CSV file with nine
records and seven attributes. 
124 
00:09:18,280 --> 00:09:23,900 
We issue queries against
the system by posing a web query. 
125 
00:09:23,900 --> 00:09:27,850 
In these examples, we show some of
the queries one can post to the system. 
126 
00:09:29,060 --> 00:09:32,187 
This will be covered in more detail
during the hands-on session. 
127 
00:09:33,220 --> 00:09:39,200 
Just notice that the q equal to is a query
and the fl is what you want to back. 
1 
00:00:01,230 --> 00:00:06,440 
Listens on activity who will be performing
queries assessing the Postgres database. 
2 
00:00:06,440 --> 00:00:10,650 
First, will open a terminal window and
start the Postgres shell. 
3 
00:00:10,650 --> 00:00:14,430 
Next, we will look at table and
column definitions in database. 
4 
00:00:15,670 --> 00:00:19,000 
Who’s in query the content
of the buy-clicks table? 
5 
00:00:19,000 --> 00:00:24,170 
And see how to do this query, but filter
specific rows and columns from the table. 
6 
00:00:24,170 --> 00:00:25,950 
Next, we will perform average and 
7 
00:00:25,950 --> 00:00:28,780 
some aggregation operations
on a specific column. 
8 
00:00:30,040 --> 00:00:30,590 
And finally, 
9 
00:00:30,590 --> 00:00:33,960 
we will see how to combine two tables
by joining them on a single column. 
10 
00:00:35,900 --> 00:00:37,130 
Let's begin. 
11 
00:00:37,130 --> 00:00:40,300 
First, click on the terminal
icon at the top of the toolbar 
12 
00:00:40,300 --> 00:00:41,390 
to open a Terminal window. 
13 
00:00:43,930 --> 00:00:48,347 
Next, let's start the Postgres
shell by running psql. 
14 
00:00:51,821 --> 00:00:56,410 
The Postgres shell allows us to
enter queries and run commands for 
15 
00:00:56,410 --> 00:00:58,180 
the postgres database. 
16 
00:00:58,180 --> 00:01:02,028 
We can see what tables are in
the database by running \d. 
17 
00:01:06,453 --> 00:01:11,350 
This says that there are three
tables in the database, 
18 
00:01:11,350 --> 00:01:14,980 
adclicks, buyclicks and gameclicks. 
19 
00:01:14,980 --> 00:01:19,689 
We could use \d table name to see
the definition of one of these tables. 
20 
00:01:20,830 --> 00:01:26,786 
Let's look at the definition of buyclicks, 
21 
00:01:26,786 --> 00:01:30,084 
we enter \d buyclicks. 
22 
00:01:32,395 --> 00:01:35,090 
This shows that there's seven
columns in the database. 
23 
00:01:36,400 --> 00:01:41,490 
These are the column names and here's
the data type for each of the columns. 
24 
00:01:44,240 --> 00:01:46,630 
Now let's look at the contents
of the buyclicks table. 
25 
00:01:48,310 --> 00:01:56,718 
We can query the contents by running
the command select * from buyclicks;. 
26 
00:01:56,718 --> 00:02:00,050 
The select says that
we want to do a query. 
27 
00:02:00,050 --> 00:02:03,730 
The star means we want to
retrieve all the columns and 
28 
00:02:03,730 --> 00:02:07,450 
from buyclicks says,
which table to perform the query from. 
29 
00:02:08,660 --> 00:02:14,226 
And finally, all commands in Postgres
shell need to end with a semicolon. 
30 
00:02:14,226 --> 00:02:19,299 
When we run this command we see
the contents of the buyclicks table. 
31 
00:02:21,120 --> 00:02:24,760 
Column header is at the top and
the contents are below. 
32 
00:02:26,530 --> 00:02:31,609 
Again hit Space to scroll through
the contents, and hit Q when we're done. 
33 
00:02:34,341 --> 00:02:37,250 
Now, let's view the contents
of only two of the columns. 
34 
00:02:38,370 --> 00:02:42,889 
Let's query only the price and
the user id from the buyclicks table. 
35 
00:02:42,889 --> 00:02:49,180 
To do this we run select price,
userid from buyclicks. 
36 
00:02:52,020 --> 00:02:55,245 
This command says,
we want to query only the price and 
37 
00:02:55,245 --> 00:02:57,747 
userid columns from the buyclicks table 
38 
00:03:00,568 --> 00:03:05,166 
When we run this,
We only get those two columns. 
39 
00:03:07,416 --> 00:03:10,892 
We can also perform queries that
just select certain rows and 
40 
00:03:10,892 --> 00:03:12,400 
meet a certain criteria. 
41 
00:03:13,570 --> 00:03:14,630 
For example, 
42 
00:03:14,630 --> 00:03:19,744 
let's make a query that only shows
the rows containing the price over $10. 
43 
00:03:20,770 --> 00:03:25,538 
You can do this by running select price, 
44 
00:03:25,538 --> 00:03:30,736 
userid from buyclicks where price > 10. 
45 
00:03:30,736 --> 00:03:35,159 
This query says, we only want to see
the price and userid columns from 
46 
00:03:35,159 --> 00:03:40,762 
the buyclicks table where the value in the
price column has a value greater than 10. 
47 
00:03:43,008 --> 00:03:49,437 
We run this, we see that we always
have price values greater than 10. 
48 
00:03:51,588 --> 00:03:55,650 
The SQL language has a number of
aggregation operations built into it. 
49 
00:03:57,250 --> 00:04:04,277 
For example,
we can take the average price by running, 
50 
00:04:04,277 --> 00:04:09,219 
select avg(price) from buyclicks. 
51 
00:04:09,219 --> 00:04:17,925 
This command will show the average price
from all the data in the buyclicks table. 
52 
00:04:17,925 --> 00:04:20,920 
Another aggregate operation is sum. 
53 
00:04:20,920 --> 00:04:25,531 
We can see the total price by running, 
54 
00:04:25,531 --> 00:04:30,297 
select sum(price) from buyclicks. 
55 
00:04:34,474 --> 00:04:39,547 
We can also combine two tables
by joining on a common column. 
56 
00:04:39,547 --> 00:04:44,050 
If you recall we have three
tables in our database adclicks, 
57 
00:04:44,050 --> 00:04:46,180 
buyclicks and gameclicks. 
58 
00:04:47,870 --> 00:04:55,594 
We look at the description the definition
of adclicks buy running \d adclicks. 
59 
00:04:59,061 --> 00:05:04,810 
You can see that it also
has a column called userid. 
60 
00:05:04,810 --> 00:05:09,030 
Let's combine buyclicks and
adclicks based on this common column. 
61 
00:05:10,180 --> 00:05:16,252 
You can do this by running, select adid, 
62 
00:05:16,252 --> 00:05:23,019 
buyid, adclicks.userid from adclicks join 
63 
00:05:23,019 --> 00:05:31,011 
buyclicks on adclicks.userid
= buyclicks.userid 
64 
00:05:33,704 --> 00:05:38,278 
This query says,
we want to just see the adid, buyid and 
65 
00:05:38,278 --> 00:05:43,252 
userid columns, and
we want to combine the adclicks table, 
66 
00:05:43,252 --> 00:05:47,729 
the buyclicks table, and
we will be combining them on 
67 
00:05:47,729 --> 00:05:52,524 
the userid column and
it's common in both those tables. 
68 
00:05:55,767 --> 00:05:58,809 
When we run it,
you see just the three columns. 
1 
00:00:02,160 --> 00:00:05,540 
Next we'll consider queries
where two tables are used. 
2 
00:00:06,610 --> 00:00:08,540 
Let's consider the query, 
3 
00:00:08,540 --> 00:00:13,320 
find the beers liked by drinkers who
frequent The Great American Bar. 
4 
00:00:15,070 --> 00:00:21,540 
For this query, we need
the relation's Frequents and Likes. 
5 
00:00:21,540 --> 00:00:24,330 
Now look at the scheme of these
relations in the light blue box. 
6 
00:00:25,840 --> 00:00:28,390 
They have a common
attribute called drinker. 
7 
00:00:29,940 --> 00:00:32,740 
So if we use the attribute drinker, 
8 
00:00:32,740 --> 00:00:35,760 
we need to tell the system
which one we are referring to. 
9 
00:00:37,980 --> 00:00:44,710 
Now look at the SQL query, the FROM clause
in the query has these two relations. 
10 
00:00:44,710 --> 00:00:46,530 
To handle a common attribute name issue, 
11 
00:00:46,530 --> 00:00:51,380 
we need to give nicknames,
aliases to these relations. 
12 
00:00:51,380 --> 00:00:56,305 
Therefore in the FROM clause we say,
Likes has the alias L and 
13 
00:00:56,305 --> 00:00:58,060 
Frequents has the alias F. 
14 
00:00:59,950 --> 00:01:05,290 
Since we want to find beers like before,
we use a SELECT DISTINCT clause for beer. 
15 
00:01:06,880 --> 00:01:11,740 
As we saw before, using SELECT DISTINCT
avoids duplicates in the result. 
16 
00:01:13,300 --> 00:01:16,850 
The WHERE clause has two
kinds of conditions, 
17 
00:01:17,870 --> 00:01:22,280 
the first kind is
a single table condition. 
18 
00:01:22,280 --> 00:01:30,170 
In this case, bar = The Great American Bar
on the Frequents relation. 
19 
00:01:31,820 --> 00:01:38,215 
The second kind is a joined condition
which says that the drinker's attribute in 
20 
00:01:38,215 --> 00:01:45,090 
the frequency relation is the same as the
drinker's attribute of the Likes relation. 
21 
00:01:46,248 --> 00:01:50,420 
We encode this in SQL in the last
line of the query using aliases. 
22 
00:01:52,140 --> 00:01:57,780 
Why did we not say L.beer in the SELECT
clause or F.bar in the first condition? 
23 
00:01:57,780 --> 00:02:00,410 
We could have,
the query would have been equally right. 
24 
00:02:01,480 --> 00:02:06,520 
But we are using a shortcut because we
know that these attributes are unique 
25 
00:02:06,520 --> 00:02:07,680 
already in the query. 
26 
00:02:09,570 --> 00:02:11,690 
Now let's look at the query again, 
27 
00:02:11,690 --> 00:02:14,980 
this time from the viewpoint
of evaluating the query. 
28 
00:02:16,280 --> 00:02:19,080 
There are many ways to evaluate the query,
but 
29 
00:02:19,080 --> 00:02:21,789 
the way it's most likely
to be evaluated is this. 
30 
00:02:23,610 --> 00:02:28,110 
The query will first look at the tables
that have single table conditions. 
31 
00:02:29,260 --> 00:02:32,640 
So it would perform a select operation 
32 
00:02:33,980 --> 00:02:38,360 
on the Frequents table to match
the records of the condition 
33 
00:02:38,360 --> 00:02:41,700 
that The Great American Bar
equal to The Great American Bar. 
34 
00:02:42,950 --> 00:02:43,990 
Why is this strategy good? 
35 
00:02:45,120 --> 00:02:48,590 
It's because the selection operative
reduces the number of triples to consider. 
36 
00:02:49,720 --> 00:02:53,390 
Thus, if there are thousand
triples in the relation frequents, 
37 
00:02:53,390 --> 00:02:55,760 
maybe 60 of them matches the desired bar. 
38 
00:02:57,240 --> 00:03:03,010 
So in the next step, we have to deal with
a fewer number of records than thousand. 
39 
00:03:04,170 --> 00:03:09,216 
All right, the next step will
be a Join with a Likes relation. 
40 
00:03:09,216 --> 00:03:12,970 
A Join requires two relations
in a Join condition, 
41 
00:03:14,070 --> 00:03:15,620 
the Join condition comes from the query. 
42 
00:03:17,410 --> 00:03:21,102 
The first relation shown with
an underscore symbol here 
43 
00:03:24,008 --> 00:03:27,665 
is a result of the previous operation. 
44 
00:03:27,665 --> 00:03:31,610 
Another way of saying this
is that the result of 
45 
00:03:31,610 --> 00:03:36,470 
the selection is piped
into the Join operation. 
46 
00:03:36,470 --> 00:03:39,930 
That means we do not create 
47 
00:03:39,930 --> 00:03:43,400 
an intermediate table from
the result of the selection. 
48 
00:03:43,400 --> 00:03:49,060 
The results are directly supplied to the
next operator, which in this case is Join. 
49 
00:03:51,340 --> 00:03:56,017 
Now the result of the Join operator is an
intermediate structure with columns beer 
50 
00:03:56,017 --> 00:03:57,373 
from Likes relation and 
51 
00:03:57,373 --> 00:04:01,116 
the drinker from the Frequents
relation that we've processed. 
52 
00:04:03,510 --> 00:04:08,492 
This intermediate set of triples is
piped to the Project operation that 
53 
00:04:08,492 --> 00:04:10,326 
picks up the beer column. 
54 
00:04:12,846 --> 00:04:17,216 
Now we need to process
the DISTINCT clause for 
55 
00:04:17,216 --> 00:04:22,970 
Deduplicate elimination,
which then goes to the Output. 
56 
00:04:24,950 --> 00:04:28,614 
We have already seen how the select
project queries on single tables 
57 
00:04:28,614 --> 00:04:32,610 
are evaluated when the tables
are partitioned across several machines. 
58 
00:04:33,720 --> 00:04:37,070 
We'll now see how we process Join
queries in the same setting. 
59 
00:04:38,780 --> 00:04:44,546 
For our case, consider that the Likes and 
60 
00:04:44,546 --> 00:04:48,470 
Frequents tables are on
two different machines. 
61 
00:04:49,830 --> 00:04:51,740 
In the first part of the query, 
62 
00:04:51,740 --> 00:04:54,699 
the selection happens on the machine
with the Frequents table. 
63 
00:04:55,870 --> 00:05:00,550 
The output of the query is a smaller
table with the same scheme as Frequents, 
64 
00:05:00,550 --> 00:05:03,550 
that is with drinkers and bars. 
65 
00:05:03,550 --> 00:05:05,480 
Now we define an operation
called Semijoin, 
66 
00:05:07,420 --> 00:05:11,070 
in which we need to move data
from one machine to another. 
67 
00:05:13,050 --> 00:05:17,400 
The goal of the Semijoin operation is
to reduce the cost of data movement. 
68 
00:05:18,480 --> 00:05:22,420 
That is to move data from
the machine which has 
69 
00:05:22,420 --> 00:05:27,196 
the Frequents data to
the machine with the Likes data. 
70 
00:05:27,196 --> 00:05:30,670 
The cost is reduced if we ship less data. 
71 
00:05:30,670 --> 00:05:38,090 
The way to it is to first find which
data the join operation actually needs. 
72 
00:05:38,090 --> 00:05:43,570 
Clearly, it needs only the drinkers
column and not the bars column. 
73 
00:05:43,570 --> 00:05:49,070 
So the drinkers column is projected out,
then 
74 
00:05:49,070 --> 00:05:52,440 
just this column is transmitted
to the second machine. 
75 
00:05:54,140 --> 00:05:59,130 
Finally, the join is performed by looking
at the values in the Likes column 
76 
00:05:59,130 --> 00:06:02,080 
that only matches the values
in the shipped data. 
77 
00:06:04,270 --> 00:06:09,720 
That means only the data from Likes that
matches the drinkers that are chosen. 
78 
00:06:11,410 --> 00:06:15,090 
These are then the join results which
would go to the output of the operation. 
79 
00:06:16,460 --> 00:06:19,680 
Now here you can see the Semijoin
operation graphically. 
80 
00:06:20,990 --> 00:06:24,800 
The red table on the left is the output
of the selection operations on the left. 
81 
00:06:27,520 --> 00:06:31,820 
The white table on the right
is the table to be joined to. 
82 
00:06:33,260 --> 00:06:36,120 
Since we need only the Drinkers column, 
83 
00:06:36,120 --> 00:06:38,870 
it is projected to create
a one-column relation. 
84 
00:06:40,380 --> 00:06:46,490 
Notice that the red table has two entries
for Pete, who frequented two bars. 
85 
00:06:46,490 --> 00:06:49,770 
But the output of the project
is condensed in the yellow table 
86 
00:06:49,770 --> 00:06:52,490 
to just show the Drinkers,
where Pete appears only once. 
87 
00:06:54,170 --> 00:06:57,940 
For those of you with
a background in computer science, 
88 
00:06:57,940 --> 00:07:00,450 
this can be done using a hash
map like data structure. 
89 
00:07:02,400 --> 00:07:06,180 
This one-column table is now shipped
to Site2, which has the Likes relation. 
90 
00:07:07,290 --> 00:07:11,834 
Now at Site2, the Shipped relation
is used to find matches from 
91 
00:07:11,834 --> 00:07:16,222 
the Drinkers column and
it finds only one match called Sally. 
92 
00:07:16,222 --> 00:07:19,960 
So the corresponding result triples,
in this case, 
93 
00:07:19,960 --> 00:07:23,956 
only one triple is produced
at the end of this operation. 
94 
00:07:23,956 --> 00:07:29,414 
Now, the original table and
the matching table are shipped 
95 
00:07:29,414 --> 00:07:35,350 
to the last of the operation
to finish the Join operation. 
96 
00:07:35,350 --> 00:07:39,120 
And more efficient version of
this is shown in the next slide. 
97 
00:07:40,120 --> 00:07:47,250 
In this version, the first two
steps this and that are the same. 
98 
00:07:48,620 --> 00:07:52,938 
Then the result of the reduce
is also shipped to Site1 to find 
99 
00:07:52,938 --> 00:07:55,490 
the matches from the red relation. 
100 
00:07:56,510 --> 00:08:00,060 
Another reduce operation
is performed on Site1 now 
101 
00:08:00,060 --> 00:08:03,020 
to get the matching records
on the red relation. 
102 
00:08:03,020 --> 00:08:06,103 
Finally, these two reduced
relations are shipped to 
103 
00:08:06,103 --> 00:08:09,200 
the site where the final join happens. 
104 
00:08:09,200 --> 00:08:11,920 
And all of this may seem
like a lot of detail. 
105 
00:08:13,070 --> 00:08:15,960 
Let me repeat something I've said before. 
106 
00:08:17,600 --> 00:08:23,850 
If we have a system like DB2 or Spark SQL
that implements multi-site joins, it 
107 
00:08:23,850 --> 00:08:27,200 
will perform this kind of operation under
the hood, you don't have to know them. 
108 
00:08:28,460 --> 00:08:34,020 
However, if we were to implement
a similar operation and all that you have 
109 
00:08:34,020 --> 00:08:38,240 
is Hadoop, you may end up implementing
this kind of algorithm yourself. 
1 
00:00:01,642 --> 00:00:06,010 
The queries in real life are little more
complex than what we have seen before. 
2 
00:00:07,080 --> 00:00:09,180 
So let's consider a more complex query. 
3 
00:00:11,010 --> 00:00:17,190 
Let’s find bars where the price
of Miller is the same as or 
4 
00:00:17,190 --> 00:00:21,600 
less than what the great American bar
called TGAB here charges for Bud. 
5 
00:00:23,090 --> 00:00:28,026 
You may say, but we do not really
know what TGAB charges for Bud. 
6 
00:00:28,026 --> 00:00:34,600 
That's correct, so
we can break up the query into two parts. 
7 
00:00:34,600 --> 00:00:42,060 
First, we will find this unknown price,
and then we'll use that price 
8 
00:00:42,060 --> 00:00:47,160 
to find the bars that would sell Miller
for the same price or better price. 
9 
00:00:47,160 --> 00:00:52,119 
Now this is a classic situation where
the result from the first part of 
10 
00:00:52,119 --> 00:00:56,326 
the query should be fed as
a parameter to the second query. 
11 
00:00:56,326 --> 00:00:59,670 
Now this situation is called a subquery. 
12 
00:01:01,510 --> 00:01:04,090 
We write this in SQL as
shown in the slide here. 
13 
00:01:05,730 --> 00:01:10,730 
What makes this query different is that
the part where price is less than equal 
14 
00:01:10,730 --> 00:01:15,440 
to, instead of specifying
a constant like $8, 
15 
00:01:15,440 --> 00:01:21,610 
we actually place another query which
computes the price of a Bud at TGAB. 
16 
00:01:24,300 --> 00:01:28,600 
The shaded part is called the inner query,
or the subquery. 
17 
00:01:29,870 --> 00:01:32,750 
In this case, both the outer query and 
18 
00:01:32,750 --> 00:01:36,110 
the inner query use the same relation,
which is Sells. 
19 
00:01:37,820 --> 00:01:43,100 
Now in terms of evaluation,
the inner query is evaluated first, 
20 
00:01:43,100 --> 00:01:45,520 
and the outer query uses its output. 
21 
00:01:46,750 --> 00:01:50,110 
Now while it may not be
obvious at this time, 
22 
00:01:50,110 --> 00:01:53,930 
notice that the inner query is
independent of the outer query. 
23 
00:01:54,960 --> 00:01:56,270 
In other words, 
24 
00:01:56,270 --> 00:02:00,930 
even if we did not have the outer query,
we can still evaluate the inner query. 
25 
00:02:02,270 --> 00:02:06,550 
We say in this case that
the subquery is uncorrelated. 
26 
00:02:09,180 --> 00:02:11,260 
Let's look at another
example of a subquery. 
27 
00:02:12,350 --> 00:02:15,504 
In this example,
we want to find the name and 
28 
00:02:15,504 --> 00:02:19,185 
manufacturer of each beer
that Fred didn't like. 
29 
00:02:19,185 --> 00:02:22,953 
So how do we know what Fred didn't like? 
30 
00:02:22,953 --> 00:02:27,545 
We do however know that the set of
beers that Fred likes because they 
31 
00:02:27,545 --> 00:02:29,925 
are listed in the Likes relation. 
32 
00:02:29,925 --> 00:02:34,324 
So we need to subtract this set from the
total set of beers that the company has 
33 
00:02:34,324 --> 00:02:35,030 
recorded. 
34 
00:02:36,260 --> 00:02:40,180 
This subtraction of sets can
be performed in several ways. 
35 
00:02:40,180 --> 00:02:43,660 
One of them is to use
the NOT IN construct. 
36 
00:02:45,130 --> 00:02:50,270 
So the query class's job is to take
every name from the Beers table and 
37 
00:02:50,270 --> 00:02:55,730 
output it only if it does not appear
in the set produced by the inner query. 
38 
00:02:57,180 --> 00:03:01,990 
Similar to the previous query,
the subquery here is also uncorrelated. 
39 
00:03:04,920 --> 00:03:06,640 
Now this is a more sophisticated query. 
40 
00:03:07,745 --> 00:03:11,490 
The intention is to find
beers that are more expensive 
41 
00:03:11,490 --> 00:03:13,820 
than the average price of beer. 
42 
00:03:13,820 --> 00:03:17,250 
But since beers have different
prices in different bars, 
43 
00:03:17,250 --> 00:03:19,520 
we have to find the average for every bar. 
44 
00:03:20,720 --> 00:03:24,340 
Therefore the idea is to find
the average price of beer for 
45 
00:03:24,340 --> 00:03:29,340 
every bar and then compare the price of
each beer with respect to this average. 
46 
00:03:30,670 --> 00:03:32,580 
Now look at the query and the table. 
47 
00:03:34,265 --> 00:03:37,110 
Let's assume we are processing
the first table. 
48 
00:03:39,250 --> 00:03:43,134 
The beer is Bud, and the price is $5. 
49 
00:03:43,134 --> 00:03:50,741 
Now we need to know if $5 is greater than
the average price of beer sold at HGAT. 
50 
00:03:50,741 --> 00:03:55,120 
To do this,
we need to compute the inner query, okay? 
51 
00:03:55,120 --> 00:03:57,410 
So now let's look at the fourth row. 
52 
00:03:57,410 --> 00:04:03,490 
The price of Guinness needs to be
compared to that average again for HGAT. 
53 
00:04:04,550 --> 00:04:07,869 
In fact for
every table processed by the outer query, 
54 
00:04:07,869 --> 00:04:10,980 
one needs to compute the inner query for
that bar. 
55 
00:04:12,240 --> 00:04:16,620 
This makes the inner subquery
correlated with the outer query. 
56 
00:04:17,910 --> 00:04:22,013 
Now a smart query processor will store
the average once it's computed and 
57 
00:04:22,013 --> 00:04:25,929 
then reuse the stored value instead
of computing over and over again. 
58 
00:04:29,376 --> 00:04:31,521 
What's an aggregate query? 
59 
00:04:31,521 --> 00:04:36,440 
Let's use a simple example of
finding the average price of Bud. 
60 
00:04:37,980 --> 00:04:42,100 
This is like a simple select project
query with the additional aspect 
61 
00:04:42,100 --> 00:04:46,010 
that it takes a list of price values
of Bud from different bars and 
62 
00:04:46,010 --> 00:04:47,250 
then computes an average. 
63 
00:04:48,280 --> 00:04:52,749 
In the example shown,
the average of the five prices is 4.2. 
64 
00:04:52,749 --> 00:04:55,230 
In other words the average function, 
65 
00:04:55,230 --> 00:05:00,750 
the AVG function, takes a list of
values and produces a single value. 
66 
00:05:00,750 --> 00:05:03,700 
Now there are many functions
that have this behavior. 
67 
00:05:03,700 --> 00:05:06,090 
The SUM function takes
a list of values and 
68 
00:05:06,090 --> 00:05:08,280 
adds them up to produce a single value. 
69 
00:05:08,280 --> 00:05:11,000 
The COUNT function takes
a list of list of values and 
70 
00:05:11,000 --> 00:05:13,490 
counts the number of items
in that list and so on. 
71 
00:05:16,020 --> 00:05:19,170 
These are called aggregate functions. 
72 
00:05:21,160 --> 00:05:26,208 
Now if we wanted to count only the price
values that are different, that is 3, 
73 
00:05:26,208 --> 00:05:31,690 
4, and 5 just once, we can write
the SELECT clause a little differently. 
74 
00:05:33,350 --> 00:05:38,852 
We would say that the average
is over distinct values 
75 
00:05:38,852 --> 00:05:43,612 
of price which in this
case will result in 4. 
76 
00:05:43,612 --> 00:05:48,481 
You should recognize that most analytical
operations need to use statistical 
77 
00:05:48,481 --> 00:05:50,630 
functions which are aggregates. 
78 
00:05:52,150 --> 00:05:54,850 
So another important
analytical requirement 
79 
00:05:54,850 --> 00:05:58,140 
is computing the statistical
aggregate by groups. 
80 
00:05:58,140 --> 00:06:02,660 
For example, we often compute the average
salaries of employees per department. 
81 
00:06:03,670 --> 00:06:09,070 
Now back to our example here,
we want to find the average price paid for 
82 
00:06:09,070 --> 00:06:15,670 
Bud per drinker, where we know
that a drinker visits many bars. 
83 
00:06:15,670 --> 00:06:18,520 
So the grouping variable here is drinker. 
84 
00:06:19,740 --> 00:06:24,900 
So we have three attributes at play,
price which we need to aggregate, 
85 
00:06:24,900 --> 00:06:29,230 
drinker which we need to group by,
and bar which is a join attribute. 
86 
00:06:30,660 --> 00:06:34,380 
The fourth attribute, namely beer,
is used for selection and 
87 
00:06:34,380 --> 00:06:37,100 
does not participate in grouping. 
88 
00:06:37,100 --> 00:06:42,340 
So after the selection we will
get an intermediate relation 
89 
00:06:42,340 --> 00:06:45,220 
containing drinker, bar, and price. 
90 
00:06:46,420 --> 00:06:50,660 
With this, the GROUP BY operation
will create one result row for 
91 
00:06:50,660 --> 00:06:55,040 
each drinker and place the average
price over all such rows. 
92 
00:06:57,650 --> 00:06:59,090 
Now how does GROUP BY and 
93 
00:06:59,090 --> 00:07:01,630 
aggregate computation work
when the data is partitioned? 
94 
00:07:02,670 --> 00:07:03,570 
Let's take the same query. 
95 
00:07:04,680 --> 00:07:09,071 
We are looking for the average
price of Bud grouped by drinker. 
96 
00:07:11,859 --> 00:07:17,090 
But this time the result of the selection
are in two different machines. 
97 
00:07:17,090 --> 00:07:20,270 
Imagine that this time they are range
partitioned by row numbers, 
98 
00:07:20,270 --> 00:07:22,170 
which we have not shown
to maintain clarity. 
99 
00:07:23,480 --> 00:07:27,374 
Now with the GROUP BY operation the data
will get repartitioned by the grouping 
100 
00:07:27,374 --> 00:07:28,970 
attribute, that's drinker. 
101 
00:07:31,730 --> 00:07:37,340 
And then the aggregate
function is computed locally. 
102 
00:07:38,890 --> 00:07:44,453 
To accomplish this repartitioning task,
each machine groups its own data locally, 
103 
00:07:44,453 --> 00:07:49,783 
determines which portions of data should
be transmitted to a different machine, 
104 
00:07:49,783 --> 00:07:52,580 
and accordingly ships it to that machine. 
105 
00:07:53,630 --> 00:07:56,390 
Now there are several variants
of this general scheme 
106 
00:07:56,390 --> 00:07:58,680 
which are even more efficient. 
107 
00:07:58,680 --> 00:08:02,870 
Now if this reminds you of the map
operation you saw in your previous course, 
108 
00:08:02,870 --> 00:08:03,790 
you are exactly right. 
109 
00:08:04,910 --> 00:08:10,164 
This fundamental process of grouping,
partitioning, and redistribution of data 
110 
00:08:10,164 --> 00:08:15,500 
is inherent in data-parallel computing and
implemented inside database systems. 
1 
00:00:02,070 --> 00:00:02,600 
Welcome back. 
2 
00:00:03,850 --> 00:00:08,705 
In this video, we will provide you
a quick summary of the main points from 
3 
00:00:08,705 --> 00:00:12,303 
our last course on big data modeling and
management. 
4 
00:00:12,303 --> 00:00:16,882 
If you had just completed our second
course and do not need a refresher, 
5 
00:00:16,882 --> 00:00:19,261 
you may now skip to the next lecture. 
6 
00:00:19,261 --> 00:00:24,311 
After this video, you will be able
to recall why big data modeling and 
7 
00:00:24,311 --> 00:00:29,536 
management is essential in preparing
to gain insights from your data, 
8 
00:00:29,536 --> 00:00:32,693 
summarize different kids of data models. 
9 
00:00:32,693 --> 00:00:38,351 
Describe streaming data and the different challenges
it presents, and explain the differences 
10 
00:00:38,351 --> 00:00:43,403 
between a database management system and
a big data management system. 
11 
00:00:46,510 --> 00:00:51,140 
In the second course, we described
a data model as a specification 
12 
00:00:51,140 --> 00:00:55,194 
that precisely characterizes
the structure of the data, 
13 
00:00:55,194 --> 00:01:00,260 
the operations on the data, and
the constraints that may apply on data. 
14 
00:01:01,360 --> 00:01:05,070 
For example, a data model may state that 
15 
00:01:05,070 --> 00:01:09,030 
a data is structured like
a two-dimensional array or a matrix. 
16 
00:01:10,760 --> 00:01:16,050 
For this structure,
one may have a data access operation, 
17 
00:01:16,050 --> 00:01:21,720 
which given an index of the array,
we use the cell of the array to refer to. 
18 
00:01:24,210 --> 00:01:28,820 
A data model may also specify
constraints on the data. 
19 
00:01:28,820 --> 00:01:33,720 
For example, while a total
data set may have many arrays, 
20 
00:01:33,720 --> 00:01:36,640 
the name of each array must be unique and 
21 
00:01:36,640 --> 00:01:41,420 
the values of a specific array
must always be greater than zero. 
22 
00:01:43,070 --> 00:01:48,335 
Database management systems handle
low level data management operations, 
23 
00:01:48,335 --> 00:01:51,980 
help organization of the data
using a data model, and 
24 
00:01:51,980 --> 00:01:55,147 
provide an open programmable
access to data. 
25 
00:01:57,975 --> 00:02:01,346 
We covered a number of data models. 
26 
00:02:01,346 --> 00:02:04,911 
We showed four models that were
discussed in more details. 
27 
00:02:07,330 --> 00:02:10,810 
The relational data to date
is the most used data model. 
28 
00:02:11,990 --> 00:02:17,189 
Here, data is structured like tables
which are formally called relations. 
29 
00:02:18,260 --> 00:02:21,840 
The relational data model has been
implemented in traditional database 
30 
00:02:21,840 --> 00:02:23,120 
systems. 
31 
00:02:23,120 --> 00:02:28,240 
But they are being refreshly implemented
in modern data systems over Hadoop and 
32 
00:02:28,240 --> 00:02:31,400 
Spark and
are getting deployed on cloud platforms. 
33 
00:02:32,700 --> 00:02:37,930 
The second category of data gaining
popularity is semi-structured data, 
34 
00:02:37,930 --> 00:02:42,840 
which includes documents like HTML pages,
XML data and 
35 
00:02:42,840 --> 00:02:45,960 
JSON data that are used by
many Internet applications. 
36 
00:02:47,470 --> 00:02:50,201 
This data can have one element nested or 
37 
00:02:50,201 --> 00:02:55,674 
embedded within another data element and
hence can often be modeled as a tree. 
38 
00:02:58,779 --> 00:03:03,480 
The third category of data
models is called graph data. 
39 
00:03:03,480 --> 00:03:08,700 
A graph is a network where
nodes represent entities and 
40 
00:03:08,700 --> 00:03:12,540 
edges represent relationships
between pairs of such entities. 
41 
00:03:13,800 --> 00:03:19,675 
For example, in a social network,
nodes may represent users and 
42 
00:03:19,675 --> 00:03:23,160 
edges may represent their friendship. 
43 
00:03:23,160 --> 00:03:27,990 
The operations performed on graph data
includes traversing the network so 
44 
00:03:27,990 --> 00:03:32,000 
that one can find friend of
a friend of a friend if needed. 
45 
00:03:34,200 --> 00:03:39,890 
In contrast to the previous three models,
that there is a structure to the data, 
46 
00:03:39,890 --> 00:03:45,280 
the text data is much more unstructured
because an entire data item 
47 
00:03:45,280 --> 00:03:47,780 
like a new article can
be just a text string. 
48 
00:03:49,180 --> 00:03:53,540 
However, text is the primary form of data 
49 
00:03:53,540 --> 00:03:57,120 
in information retrieval systems or
search engines like Google. 
50 
00:04:00,250 --> 00:04:05,290 
We also discussed streaming data,
or data with velocity, as a special 
51 
00:04:05,290 --> 00:04:10,580 
class of data that continually come
to the system at some data rate. 
52 
00:04:12,350 --> 00:04:17,308 
Examples can be found in data coming
from road sensors that measure traffic 
53 
00:04:17,308 --> 00:04:21,636 
patterns or stock price data from
the stock exchange that may come 
54 
00:04:21,636 --> 00:04:25,111 
in volumes from stock
exchanges all over the world. 
55 
00:04:27,563 --> 00:04:35,090 
Streaming data is special because a stream
is technically an infinite data source. 
56 
00:04:35,090 --> 00:04:38,050 
And therefore,
we keep filling up memory and 
57 
00:04:38,050 --> 00:04:42,390 
storage and will eventually go
beyond the capacity of any system. 
58 
00:04:43,930 --> 00:04:47,720 
Streaming data, therefore, needs
a different kind of management system. 
59 
00:04:48,960 --> 00:04:54,135 
For this reason,
streaming data is processed in memory, 
60 
00:04:54,135 --> 00:04:57,841 
in chunks which are also called windows. 
61 
00:04:57,841 --> 00:05:01,226 
Often only the necessary
part of the data stream or 
62 
00:05:01,226 --> 00:05:05,186 
the results of queries against
the data stream is stored. 
63 
00:05:07,145 --> 00:05:13,180 
A typical type of query against streaming
data are alerts or notifications. 
64 
00:05:13,180 --> 00:05:17,933 
The system notices an event like multiple
stock price changing within a short time. 
65 
00:05:21,658 --> 00:05:24,160 
Streaming data is also used for
prediction. 
66 
00:05:26,330 --> 00:05:31,287 
For instance, based on wind direction and
temperature data streams, 
67 
00:05:31,287 --> 00:05:34,768 
one can predict how a wildfire
is going to spread. 
68 
00:05:37,640 --> 00:05:42,522 
In the last course, we also covered
a number of data systems that we called 
69 
00:05:42,522 --> 00:05:44,590 
big data management systems. 
70 
00:05:46,510 --> 00:05:49,890 
These systems use
different data models and 
71 
00:05:49,890 --> 00:05:55,030 
have different capabilities, but
are characterized by some common features. 
72 
00:05:56,210 --> 00:06:00,430 
They are also designed from the start for
parallel and distributed processing. 
73 
00:06:01,610 --> 00:06:07,100 
Most of them implement data partition
parallelism, which, if you can recall, 
74 
00:06:07,100 --> 00:06:11,890 
refers to the process of segmenting
the data into multiple machines so 
75 
00:06:11,890 --> 00:06:16,839 
data retrieval and manipulations can be
performed in parallel on these machines. 
76 
00:06:18,880 --> 00:06:24,360 
Many of these systems allow a large
number of users who constantly update and 
77 
00:06:24,360 --> 00:06:25,510 
query the system. 
78 
00:06:27,590 --> 00:06:32,190 
Some of the systems do not maintain
transaction consistency with every update. 
79 
00:06:33,300 --> 00:06:34,224 
That means, 
80 
00:06:34,224 --> 00:06:39,437 
not all the machines may have all
the updates guaranteed at every moment. 
81 
00:06:41,091 --> 00:06:46,460 
However, most of them provide
a guarantee of eventual consistency, 
82 
00:06:46,460 --> 00:06:51,780 
which means all the machines will
get all updates sooner or later. 
83 
00:06:51,780 --> 00:06:54,920 
Therefore, providing better accuracy and
time. 
84 
00:06:57,610 --> 00:07:02,740 
The third common characteristic
of big data management systems is 
85 
00:07:02,740 --> 00:07:06,750 
that they are often built on top of
a Hadoop-like platform that provides 
86 
00:07:06,750 --> 00:07:11,040 
automatic replication and
a map-reduce style processing ability. 
87 
00:07:12,270 --> 00:07:16,440 
Some of the data operations performed
within these systems make use of these 
88 
00:07:16,440 --> 00:07:17,829 
lower level capabilities. 
89 
00:07:20,200 --> 00:07:22,520 
After this refresher on data modeling and 
90 
00:07:22,520 --> 00:07:26,650 
management, let's start big data
integration and processing. 
1 
00:00:02,188 --> 00:00:03,689 
So, hi. 
2 
00:00:03,689 --> 00:00:08,521 
In the previous course, we saw
examples of different data models and 
3 
00:00:08,521 --> 00:00:12,290 
talked about a few current
data management systems. 
4 
00:00:12,290 --> 00:00:15,617 
In this module,
we'll focus on data retrieval. 
5 
00:00:35,163 --> 00:00:40,012 
Data retrieval refers to
the way in which data desired 
6 
00:00:40,012 --> 00:00:44,980 
by a user is specified and
retrieved from a data store. 
7 
00:00:46,280 --> 00:00:50,720 
Note that in this course, we are using
the term data retrieval in two ways. 
8 
00:00:51,760 --> 00:00:56,430 
Assume that your data is stored in a data
store that follows a specific data model, 
9 
00:00:56,430 --> 00:00:58,190 
like for
example the relational data model. 
10 
00:00:59,340 --> 00:01:03,300 
By data retrieval, we will refer to, one, 
11 
00:01:03,300 --> 00:01:07,950 
the way you specify how to get
the desired data out of the system, 
12 
00:01:07,950 --> 00:01:12,910 
this is called the query
specification method, and two, 
13 
00:01:12,910 --> 00:01:18,300 
the internal processing that occurs within
the data management system to compute or 
14 
00:01:18,300 --> 00:01:21,520 
evaluate that specified retrieval request. 
15 
00:01:23,170 --> 00:01:28,780 
While query specification can apply to
small data stores or large data stores, 
16 
00:01:28,780 --> 00:01:32,600 
we'll keep an eye on the nature of
query evaluation when the data is big. 
17 
00:01:34,230 --> 00:01:38,370 
Further, we'll consider how
the query specification changes 
18 
00:01:38,370 --> 00:01:40,770 
when we deal with faster streaming data. 
19 
00:01:43,380 --> 00:01:48,590 
A query language is a language in which
a retrieval request is specified. 
20 
00:01:50,730 --> 00:01:56,232 
A query language is often called
declarative, which means it lets you 
21 
00:01:56,232 --> 00:02:01,600 
specify what you want to retrieve without
having to tell the system how to retrieve. 
22 
00:02:02,670 --> 00:02:04,660 
For example, you can say, 
23 
00:02:04,660 --> 00:02:10,550 
find all data from relation employee
where the salary is more than 50k. 
24 
00:02:10,550 --> 00:02:14,970 
Now, you don't have to write a program
which will tell the system to open a file, 
25 
00:02:14,970 --> 00:02:20,006 
skip the first 250 bytes,
then in a loop pick the next 1024 bytes, 
26 
00:02:20,006 --> 00:02:24,340 
probe into the 600th byte and
read an integer, and so forth. 
27 
00:02:25,480 --> 00:02:28,160 
Instead of writing such
a complicated procedure, 
28 
00:02:28,160 --> 00:02:32,450 
you just specify the data items that
you need and the system does the rest. 
29 
00:02:33,840 --> 00:02:38,167 
For example, SQL,
structured query language, 
30 
00:02:38,167 --> 00:02:42,810 
is the most used query language for
relational data. 
31 
00:02:42,810 --> 00:02:47,156 
Now, in contrast to a query language,
a database programming 
32 
00:02:47,156 --> 00:02:52,076 
language like Oracle's PL/SQL or
Postgres's PgSQL are high-level 
33 
00:02:52,076 --> 00:02:56,680 
procedural programming languages
that embed query operations. 
34 
00:02:57,870 --> 00:03:00,648 
We will look at some query
languages in detail and 
35 
00:03:00,648 --> 00:03:03,649 
show examples of database
programming languages. 
36 
00:03:06,040 --> 00:03:08,881 
The first query language
we'll look at is SQL, 
37 
00:03:08,881 --> 00:03:13,002 
which is the ubiquitous query
language when the data is structured, 
38 
00:03:13,002 --> 00:03:18,120 
but has been extended in many ways
to accommodate other types of data. 
39 
00:03:18,120 --> 00:03:22,329 
For this course, we'll stick to
the structured aspect of the language. 
40 
00:03:22,329 --> 00:03:26,939 
Now, you should know that SQL is used for
classical database management systems 
41 
00:03:26,939 --> 00:03:31,440 
like Oracle as well as modern Hadoop
style distributed systems such as Spark. 
42 
00:03:32,650 --> 00:03:35,220 
Now, we will work with
an illustrative example. 
43 
00:03:37,140 --> 00:03:40,360 
First, we need to define
the schema of the database. 
44 
00:03:40,360 --> 00:03:45,640 
Now, think of a business called the
Beer Drinkers Club that owns many bars, 
45 
00:03:45,640 --> 00:03:46,990 
and each bar sells beer. 
46 
00:03:48,060 --> 00:03:52,646 
Our schema for
this business has six relations of tables. 
47 
00:03:52,646 --> 00:03:57,448 
The first table lists these bars,
the names, addresses, and 
48 
00:03:57,448 --> 00:03:59,814 
the license number of the bar. 
49 
00:03:59,814 --> 00:04:04,469 
Notice that the attribute name is
underlined because it is the primary key 
50 
00:04:04,469 --> 00:04:05,930 
of the bars relation. 
51 
00:04:05,930 --> 00:04:09,800 
Recall that the primary key
refers to a set of attributes, 
52 
00:04:09,800 --> 00:04:13,350 
in this case just the name,
that makes a record unique. 
53 
00:04:15,060 --> 00:04:17,720 
Note that the relation bars
with the attribute name 
54 
00:04:19,080 --> 00:04:22,241 
within parenthesis is the same
as the table shown on the right. 
55 
00:04:23,280 --> 00:04:27,147 
We will use both representations
as we go forward. 
56 
00:04:27,147 --> 00:04:32,030 
The second table called Beers, this is
the names and manufacturers of beer. 
57 
00:04:32,030 --> 00:04:36,266 
Now, not every bar sells the same
brands of beer, and even when they do, 
58 
00:04:36,266 --> 00:04:38,280 
they may have different prices for 
59 
00:04:38,280 --> 00:04:42,390 
the same product because of differences
in the establishment costs. 
60 
00:04:43,550 --> 00:04:48,960 
So the Sells table records which
bar sells which beer at what price. 
61 
00:04:50,970 --> 00:04:53,268 
Now, our business is special. 
62 
00:04:53,268 --> 00:04:57,358 
It also keeps information about
the regular member customers. 
63 
00:04:57,358 --> 00:05:02,750 
So the Drinkers relation has the name,
address, and phone of these customers. 
64 
00:05:02,750 --> 00:05:07,032 
Well, not only that,
it knows which member visits 
65 
00:05:07,032 --> 00:05:11,720 
which bars and
which beer each member likes. 
66 
00:05:11,720 --> 00:05:14,980 
Clearly, the Beer Drinkers Club
knows its customers. 
67 
00:05:17,670 --> 00:05:24,490 
The most basic structure of an SQL
query is a SELECT-FROM-WHERE clause. 
68 
00:05:25,500 --> 00:05:28,920 
In this example, we're looking for
beer names that are made by Heineken. 
69 
00:05:30,060 --> 00:05:33,160 
So we need to specify
our output attribute, 
70 
00:05:33,160 --> 00:05:35,520 
in this case the name of the beer. 
71 
00:05:35,520 --> 00:05:41,040 
The logical table which will be used to
answer the query, in this case, Beers. 
72 
00:05:42,370 --> 00:05:46,972 
And the condition that all the desired
data items should satisfy, 
73 
00:05:46,972 --> 00:05:51,260 
namely, the value of the attribute
called manf is Heineken. 
74 
00:05:52,600 --> 00:05:55,570 
Now, there are few things to notice here. 
75 
00:05:55,570 --> 00:05:59,311 
First, the literal Heineken
is put within quotes, 
76 
00:05:59,311 --> 00:06:02,210 
because it's a single string literal. 
77 
00:06:03,470 --> 00:06:04,550 
Remember that in this case, 
78 
00:06:04,550 --> 00:06:08,830 
the string is supposed to match exactly,
including the case. 
79 
00:06:10,910 --> 00:06:16,590 
Secondly, if you go back to the data
operations discussed in course two, 
80 
00:06:16,590 --> 00:06:21,610 
you will recognize that this form
of query can also be represented 
81 
00:06:21,610 --> 00:06:27,070 
as a selection operation on the relation
Beers with a condition on the manf 
82 
00:06:27,070 --> 00:06:30,800 
attribute, followed by
a projection operation 
83 
00:06:30,800 --> 00:06:34,349 
that outputs the name attribute from
the result of the selection operation. 
84 
00:06:35,680 --> 00:06:40,340 
So the selection operation finds all
tuples of beer for which the manufacturer 
85 
00:06:40,340 --> 00:06:45,710 
is Heineken, and from those tuples
it projects only the name column. 
86 
00:06:47,300 --> 00:06:51,580 
The result of the query is a table
with one single attribute called name. 
87 
00:06:54,250 --> 00:06:58,883 
We illustrate some more features of SQL,
using two example queries. 
88 
00:06:58,883 --> 00:07:02,330 
The first looks for
expensive beer and its price. 
89 
00:07:03,370 --> 00:07:06,856 
Let's say we consider a beer to be
expensive if it costs more than $15 
90 
00:07:06,856 --> 00:07:07,569 
per bottle. 
91 
00:07:08,810 --> 00:07:10,200 
From the schema, 
92 
00:07:10,200 --> 00:07:13,830 
we know that the price information is
available in the table called Sells. 
93 
00:07:14,900 --> 00:07:17,470 
So the FROM clause should use Sells. 
94 
00:07:18,620 --> 00:07:20,490 
The WHERE clause is intuitive and 
95 
00:07:20,490 --> 00:07:24,200 
specifies the price of the beer
to be greater than 15. 
96 
00:07:24,200 --> 00:07:28,873 
Now notice that the Sells relation
also has a column called bar. 
97 
00:07:28,873 --> 00:07:35,170 
Now, if two different bars sell
the same beer at the same price, 
98 
00:07:35,170 --> 00:07:37,570 
we'll get both entries in the result. 
99 
00:07:37,570 --> 00:07:39,620 
But that's not what we want. 
100 
00:07:39,620 --> 00:07:43,850 
Now regardless of the multiplicity
of bars that have the same price for 
101 
00:07:43,850 --> 00:07:46,340 
the same beer,
we want the result just once. 
102 
00:07:48,320 --> 00:07:53,688 
So this is achieved through
the SELECT DISTINCT statement, 
103 
00:07:53,688 --> 00:07:59,384 
which ensures that the result
relation will have no duplicate. 
104 
00:07:59,384 --> 00:08:03,736 
The second example shows the case
where more than one condition 
105 
00:08:03,736 --> 00:08:06,045 
must be specified by the result. 
106 
00:08:06,045 --> 00:08:09,774 
In this query,
the business must be in San Diego and 
107 
00:08:09,774 --> 00:08:13,764 
at the same time it must be
a temporary license holder, 
108 
00:08:13,764 --> 00:08:17,870 
which means the license
number should start with 32. 
109 
00:08:17,870 --> 00:08:23,280 
As we see here, these conditions
are put together by the AND operator. 
110 
00:08:25,230 --> 00:08:29,330 
Thus, the query will pick
the third record in the table 
111 
00:08:29,330 --> 00:08:33,560 
because the first record satisfy the first
condition and not the second condition. 
112 
00:08:34,950 --> 00:08:38,780 
In a few slides, we'll come back to
the evaluation of this type of queries 
113 
00:08:38,780 --> 00:08:40,625 
in the context of big data. 
114 
00:08:40,625 --> 00:08:47,340 
Now, remember, one can also place a limit
on the number of results to return. 
115 
00:08:47,340 --> 00:08:52,163 
If our database is large, and
we need only five results, for 
116 
00:08:52,163 --> 00:08:56,620 
example, for a sample to display,
we can say LIMIT 5. 
117 
00:08:56,620 --> 00:09:03,706 
Now, the exact syntax of this LIMIT
clause may vary between DBMS vendors. 
1 
00:00:02,450 --> 00:00:06,680 
Now if the table of beers was large and
had millions of entries, 
2 
00:00:07,820 --> 00:00:11,480 
the table would possibly need
to be split over many machines. 
3 
00:00:12,600 --> 00:00:16,720 
Another way of saying that is that
the table will be partitioned 
4 
00:00:16,720 --> 00:00:18,960 
across a number of machines. 
5 
00:00:18,960 --> 00:00:21,210 
Since a query simply
performs a selection and 
6 
00:00:21,210 --> 00:00:24,330 
projection here,
it can be evaluated in parallel. 
7 
00:00:25,860 --> 00:00:28,860 
Remember that name is
the primary key of the table. 
8 
00:00:30,070 --> 00:00:32,790 
One standard way of partitioning the data 
9 
00:00:32,790 --> 00:00:35,260 
is called a range partitioning
by the primary key. 
10 
00:00:37,210 --> 00:00:41,760 
This simply means that the rows
of the table are put in groups 
11 
00:00:41,760 --> 00:00:45,440 
depending on the alphabetical
order of the name value. 
12 
00:00:46,570 --> 00:00:50,950 
So beers with names starting with E and
B here are placed in Machine 1. 
13 
00:00:50,950 --> 00:00:53,910 
Those starting with C and
D are in Machine 2. 
14 
00:00:53,910 --> 00:00:58,337 
And if there are too many rows for
entries where the name starts with H, 
15 
00:00:58,337 --> 00:01:00,902 
maybe H is split into Machines 5 and 6. 
16 
00:01:00,902 --> 00:01:02,260 
This is shown in the sketch here. 
17 
00:01:03,640 --> 00:01:08,620 
Next, we will show how queries
are performed over partition tables. 
18 
00:01:08,620 --> 00:01:13,230 
But before we do that, you should know
that all database management companies, 
19 
00:01:13,230 --> 00:01:18,090 
like IBM, Chair Data, Microsoft, and
others, have a solution like this for 
20 
00:01:18,090 --> 00:01:21,670 
large volumes of data,
where data partitioning is used. 
21 
00:01:21,670 --> 00:01:26,030 
Newer systems, like Spark and SQL,
are naturally distributed, and 
22 
00:01:26,030 --> 00:01:27,419 
therefore, offer data partitioning. 
23 
00:01:30,470 --> 00:01:33,620 
So, we show the same partition
tables as we saw before. 
24 
00:01:33,620 --> 00:01:35,180 
Now we'll ask two queries. 
25 
00:01:36,550 --> 00:01:41,340 
The first query asks for
all tuples as records from the beers table 
26 
00:01:42,500 --> 00:01:45,290 
where the name of the beer starts with Am. 
27 
00:01:46,970 --> 00:01:49,400 
And the second query is
exactly what we asked before. 
28 
00:01:52,544 --> 00:01:55,560 
The first query in
the SQL looks like this. 
29 
00:01:56,730 --> 00:02:01,434 
We said SELECT* FROM Beers to mean
all attributes from table beers. 
30 
00:02:01,434 --> 00:02:05,790 
The WHERE clause shows the syntax for
a partial match query. 
31 
00:02:06,940 --> 00:02:10,850 
In this query,
there are two new syntax elements. 
32 
00:02:10,850 --> 00:02:12,630 
The first is a predicate called like. 
33 
00:02:14,480 --> 00:02:18,080 
When we use like,
we're telling the query engine 
34 
00:02:18,080 --> 00:02:21,870 
that we only have partial information
about the string we want to match. 
35 
00:02:22,870 --> 00:02:25,870 
This partly specified string
is called a string pattern. 
36 
00:02:27,290 --> 00:02:30,320 
That means, there is this part
of the string we know and 
37 
00:02:30,320 --> 00:02:31,460 
a part that we do not know. 
38 
00:02:33,050 --> 00:02:39,300 
In this case, we know that our design
string starts with Am, so we'd write Am, 
39 
00:02:39,300 --> 00:02:44,250 
and then we put % to refer to the part
of the string that we do not know. 
40 
00:02:44,250 --> 00:02:48,410 
Putting them together, we get Am%. 
41 
00:02:48,410 --> 00:02:52,671 
If we wanted to find, say,
Am somewhere in the middle of the string, 
42 
00:02:52,671 --> 00:02:55,027 
we would write the pattern as %Am%. 
43 
00:02:55,027 --> 00:03:01,680 
The second query is not new. 
44 
00:03:01,680 --> 00:03:03,580 
We saw it in the last slide. 
45 
00:03:03,580 --> 00:03:08,490 
However, as we'll see next, evaluating
the second query will be a little more 
46 
00:03:08,490 --> 00:03:12,930 
tricky in a partition database than
that we usually see for big data. 
47 
00:03:17,390 --> 00:03:20,780 
Let's talk about the first query
in this data partition setting. 
48 
00:03:20,780 --> 00:03:27,160 
The question to ask is, do we need to
touch all partitions to answer the query? 
49 
00:03:27,160 --> 00:03:33,720 
Of course not, we know that the name is
a primary key for the table of beers. 
50 
00:03:33,720 --> 00:03:37,710 
We also know that the system did arrange
partitioning on the name attribute. 
51 
00:03:38,710 --> 00:03:43,260 
This means that the evaluation
process should only access Machine 1 
52 
00:03:43,260 --> 00:03:47,370 
because no other machine will have
records for names starting with A. 
53 
00:03:48,670 --> 00:03:50,600 
Now this is exactly what we, as humans, 
54 
00:03:50,600 --> 00:03:55,780 
do when we look up an entry in
a multivolume encyclopedia. 
55 
00:03:55,780 --> 00:03:59,300 
We look for the starting words, then
figure out which specific volume would 
56 
00:03:59,300 --> 00:04:01,410 
have that entry,
then pick up just that volume. 
57 
00:04:02,830 --> 00:04:06,330 
Thus, so long as the system
knows the partitioning strategy, 
58 
00:04:07,380 --> 00:04:09,310 
it can make its job much more efficient. 
59 
00:04:10,760 --> 00:04:14,920 
When a system processes
thousands of queries per second, 
60 
00:04:14,920 --> 00:04:17,430 
this kind of efficiency actually matters. 
61 
00:04:18,970 --> 00:04:22,785 
Now raised partitioning is
only one of many partitioning 
62 
00:04:22,785 --> 00:04:25,650 
schemes used in a database system, okay. 
63 
00:04:26,790 --> 00:04:29,760 
Let's try to answer the second query
in the same partition setting. 
64 
00:04:30,820 --> 00:04:34,650 
Now the query condition is on
the second attribute, manf. 
65 
00:04:35,690 --> 00:04:37,900 
Now in one sense, it's a simpler query. 
66 
00:04:37,900 --> 00:04:39,392 
There is no light pattern here, and 
67 
00:04:39,392 --> 00:04:42,789 
we know exactly the string that we are
looking for, namely the string Heineken. 
68 
00:04:43,930 --> 00:04:47,310 
However, this time,
we really cannot get away 
69 
00:04:47,310 --> 00:04:51,520 
by using the partitioning information
because the partitioning activity is 
70 
00:04:51,520 --> 00:04:55,130 
different from the attribute on which
the query condition is applied. 
71 
00:04:56,300 --> 00:05:00,023 
So this query will need
to go to all partitions. 
72 
00:05:00,023 --> 00:05:05,200 
Technically speaking,
the query needs to be broadcast 
73 
00:05:05,200 --> 00:05:08,870 
from the primary machine to all machines,
as shown here. 
74 
00:05:11,500 --> 00:05:15,320 
Next, this broadcast query
will be independently, and 
75 
00:05:15,320 --> 00:05:19,470 
in parallel,
execute the query on the local machine. 
76 
00:05:20,840 --> 00:05:24,420 
Then, these results need to be
brought back into the primary machine. 
77 
00:05:25,690 --> 00:05:28,760 
And then,
they need to be unioned together. 
78 
00:05:28,760 --> 00:05:31,880 
And only then, the results can be
formed and returned to the client. 
79 
00:05:33,130 --> 00:05:35,490 
Now, this might seem
like a lot of extra work. 
80 
00:05:36,510 --> 00:05:41,440 
However, remember, the shaded part of
the query is executed in parallel, 
81 
00:05:41,440 --> 00:05:43,830 
which is the essence of
dealing with large data. 
82 
00:05:46,350 --> 00:05:51,770 
Now, at this point, you might be thinking,
wait a minute, what if I had 100 machines, 
83 
00:05:52,940 --> 00:05:55,800 
and the desired data
is only in 20 of them? 
84 
00:05:57,340 --> 00:06:01,870 
Should we needlessly go through all 100
machines, find nothing in 80 of them, and 
85 
00:06:01,870 --> 00:06:04,080 
return 0 results from those machines? 
86 
00:06:04,080 --> 00:06:05,760 
Then why do the extra work? 
87 
00:06:05,760 --> 00:06:06,790 
Can it not be avoided? 
88 
00:06:08,560 --> 00:06:12,280 
Well, to do this, it would need
one more piece in the solution, 
89 
00:06:12,280 --> 00:06:13,700 
it's called an index structure. 
90 
00:06:14,990 --> 00:06:16,290 
Very simply, 
91 
00:06:16,290 --> 00:06:20,930 
an index can be thought of as a reverse
table, where given the value in a column, 
92 
00:06:20,930 --> 00:06:25,220 
you would get back the records where the
value appears as shown in the figure here. 
93 
00:06:26,810 --> 00:06:31,147 
Using an index speeds up query
processing significantly. 
94 
00:06:31,147 --> 00:06:34,940 
With indexes, we can solve this
problem in many different ways. 
95 
00:06:37,110 --> 00:06:41,930 
The top table shows the case where
each machine has its own index for 
96 
00:06:41,930 --> 00:06:43,400 
the manf column. 
97 
00:06:43,400 --> 00:06:47,980 
This is called a local index because
the index is in every machine 
98 
00:06:47,980 --> 00:06:51,050 
that holds the data for
that table on that machine. 
99 
00:06:53,070 --> 00:06:55,670 
In this case,
looking up Heineken in the index, 
100 
00:06:55,670 --> 00:06:57,950 
we would know which records
would have the data. 
101 
00:06:59,500 --> 00:07:04,750 
Since the index is local, the main query
will indeed go to all machines, but 
102 
00:07:04,750 --> 00:07:08,660 
the lookup will be really instant, and the
empty results would return very quickly. 
103 
00:07:11,010 --> 00:07:13,410 
In the second case,
we adopted a different solution. 
104 
00:07:14,610 --> 00:07:20,500 
Here, there is an index on the main
machine, all on a separate index server. 
105 
00:07:20,500 --> 00:07:23,720 
Now when we place a data
record in a machine, 
106 
00:07:23,720 --> 00:07:27,959 
this index keeps an account of the machine
that contains the record with that value. 
107 
00:07:29,240 --> 00:07:30,720 
Look at the second table to the right. 
108 
00:07:32,510 --> 00:07:37,398 
Given the value of Heineken,
we know that it is only in three machines, 
109 
00:07:37,398 --> 00:07:41,960 
and therefore,
we can avoid going to the other machines. 
110 
00:07:41,960 --> 00:07:45,840 
Clearly, we can always use
both indexing schemes. 
111 
00:07:45,840 --> 00:07:49,736 
This will use more space,
but queries will be faster. 
112 
00:07:49,736 --> 00:07:54,717 
Now this gives you some of the choices
you may need to make with big data, 
113 
00:07:54,717 --> 00:07:59,220 
whether you use a parallel DBMS or
a distributed data solution. 
1 
00:00:01,850 --> 00:00:05,240 
Welcome to Big Data Integration and
Processing. 
2 
00:00:05,240 --> 00:00:08,190 
 Welcome to course three of
the big data specialization. 
3 
00:00:08,190 --> 00:00:09,710 
I'm Amarnath Gupta. 
4 
00:00:09,710 --> 00:00:11,650 
 And I'm Ilkay Altintas. 
5 
00:00:11,650 --> 00:00:14,590 
We are really excited to
work with you in this course 
6 
00:00:14,590 --> 00:00:19,820 
to develop your understanding and skills
in big data integration and processing. 
7 
00:00:19,820 --> 00:00:24,180 
By now you might have just
finished our first two courses and 
8 
00:00:24,180 --> 00:00:27,870 
learned the basics of big data
modelling and management. 
9 
00:00:27,870 --> 00:00:30,480 
If you haven't, it's not required. 
10 
00:00:30,480 --> 00:00:33,560 
But for those that less background
in the data modelling and 
11 
00:00:33,560 --> 00:00:37,680 
data management areas you
might find it valuable. 
12 
00:00:37,680 --> 00:00:42,030 
 We understand that you may not have
any background on data management. 
13 
00:00:42,030 --> 00:00:47,382 
We are going to introduce Query languages,
we'll first look at SQL in some detail, 
14 
00:00:47,382 --> 00:00:51,740 
and then move to Query languages for
MongoDB which is a semi structured 
15 
00:00:51,740 --> 00:00:56,260 
data management system, and Aerospike,
which is a key value store. 
16 
00:00:57,740 --> 00:01:02,200 
 We will also introduce concepts
related to processing of big data as 
17 
00:01:02,200 --> 00:01:03,890 
big data pipelines. 
18 
00:01:03,890 --> 00:01:08,160 
We talk about data structures and
transformations related to batch and 
19 
00:01:08,160 --> 00:01:11,230 
stream processing as steps in a pipeline. 
20 
00:01:11,230 --> 00:01:14,100 
Providing us a way to talk
about big data processing 
21 
00:01:14,100 --> 00:01:17,180 
without getting into the details
of the underlying technologies. 
22 
00:01:18,630 --> 00:01:23,710 
Once we have reviewed the concepts and
related systems, we will switch gears 
23 
00:01:23,710 --> 00:01:28,370 
to hands on exercises with Spark,
one of the most popular big data engines. 
24 
00:01:29,470 --> 00:01:34,060 
We will show you examples of patch and
stream processing using Spark. 
25 
00:01:35,780 --> 00:01:39,410 
 As you know for
many data science applications 
26 
00:01:39,410 --> 00:01:44,140 
one has to use many different databases
and analyze the integrated data. 
27 
00:01:44,140 --> 00:01:50,160 
In fact, data integration is one leading
cause leading to the bigness of data. 
28 
00:01:51,410 --> 00:01:54,780 
We'll give you a rapid exposure to
information integration systems 
29 
00:01:54,780 --> 00:01:59,530 
through use cases and point out the big
data aspects one should pay attention to. 
30 
00:02:00,760 --> 00:02:06,620 
 We are also excited to show you
examples of data processing using Splunk. 
31 
00:02:06,620 --> 00:02:10,510 
Our goal here is to provide you
with simple hands on exercises 
32 
00:02:10,510 --> 00:02:12,550 
that require no programming, but 
33 
00:02:12,550 --> 00:02:18,660 
show you how one can use interfaces like
Splunk to manage and process big data. 
34 
00:02:18,660 --> 00:02:20,820 
We wish you a fun time learning and 
35 
00:02:20,820 --> 00:02:26,080 
hope to hear from you in the discussions
forums and learner stories as usual. 
36 
00:02:26,080 --> 00:02:27,880 
 Well, happy learning and think big data 
1 
00:00:02,627 --> 00:00:07,359 
In this video, we will talk about
the challenges of ingesting and 
2 
00:00:07,359 --> 00:00:12,178 
processing big data and
remind ourselves why need any paradigm and 
3 
00:00:12,178 --> 00:00:14,690 
programming models for big data. 
4 
00:00:15,900 --> 00:00:20,710 
After this video, you will be able to
summarize the requirements of programming 
5 
00:00:20,710 --> 00:00:23,650 
models for big data and
why you should care about them. 
6 
00:00:24,730 --> 00:00:27,800 
You will also be able to explain
how the challenges of big 
7 
00:00:27,800 --> 00:00:32,750 
data related to its variety, volume and
velocity affects its processing. 
8 
00:00:36,090 --> 00:00:41,880 
Before we start,
let's imagine an online gaming newscase, 
9 
00:00:41,880 --> 00:00:44,870 
just like the one we have for
Catch the Pink Flamingo. 
10 
00:00:47,140 --> 00:00:52,123 
You just introduced the game,
and users started signing up. 
11 
00:00:52,123 --> 00:00:55,101 
You start with a traditional
relational database, 
12 
00:00:55,101 --> 00:00:57,950 
keeping track of user sessions and
other events. 
13 
00:01:00,130 --> 00:01:05,030 
Your game server receives
an event notification every time 
14 
00:01:05,030 --> 00:01:08,710 
a user opens his session and
makes a point in the game. 
15 
00:01:09,990 --> 00:01:13,690 
Initially, everything is great,
your game is working and 
16 
00:01:13,690 --> 00:01:17,230 
the database is able to handle the event
streams coming into the server. 
17 
00:01:18,490 --> 00:01:23,960 
However, suddenly your game becomes
highly popular a good problem to have. 
18 
00:01:25,890 --> 00:01:29,670 
The database management system in
your game server won't be able to 
19 
00:01:29,670 --> 00:01:31,660 
handle the load anymore. 
20 
00:01:31,660 --> 00:01:35,630 
You start getting errors that the events
can't be inserted into the database 
21 
00:01:35,630 --> 00:01:37,240 
at the speed they are coming in. 
22 
00:01:38,650 --> 00:01:45,240 
You decide that you will have a buffer or
a queue to process the advancing chunks. 
23 
00:01:45,240 --> 00:01:50,600 
Maybe also at the same time processing
them to be organized in windows of time or 
24 
00:01:50,600 --> 00:01:51,260 
game sessions. 
25 
00:01:53,390 --> 00:01:58,916 
However, in time as the demand goes up,
you will need more processing nodes and 
26 
00:01:58,916 --> 00:02:02,638 
even more database servers
that can handle the load. 
27 
00:02:02,638 --> 00:02:08,042 
This is, a typical scenario that
most web sites face when confronted 
28 
00:02:08,042 --> 00:02:13,373 
with big data issues related to
volume and velocity of information. 
29 
00:02:13,373 --> 00:02:15,894 
As this scenario demonstrates, 
30 
00:02:15,894 --> 00:02:20,580 
solving the problem in one step
might be possible initially. 
31 
00:02:20,580 --> 00:02:25,382 
But the more reactive fixes
the game developers add, the system 
32 
00:02:25,382 --> 00:02:29,630 
becomes less robust and
more complicated to evolve. 
33 
00:02:31,880 --> 00:02:35,210 
While the developers initially
started with an application and 
34 
00:02:35,210 --> 00:02:36,360 
the database to manage. 
35 
00:02:37,400 --> 00:02:41,614 
Now they have to manage a number
of issues related to this 
36 
00:02:41,614 --> 00:02:46,920 
infrastructure management just to
keep up with the load on the system. 
37 
00:02:46,920 --> 00:02:52,282 
Similarly, the database servers
can be effected and corrupted. 
38 
00:02:52,282 --> 00:02:57,150 
The replication and fault tolerance of
them need to be handled separately. 
39 
00:02:58,240 --> 00:03:01,745 
Let's start by going through these issues. 
40 
00:03:01,745 --> 00:03:05,300 
Let's say,
one of the processing nodes went down. 
41 
00:03:06,470 --> 00:03:11,970 
The system needs to manage and
restart the processing and 
42 
00:03:11,970 --> 00:03:14,920 
there will be potentially some
data loss in the meantime. 
43 
00:03:16,400 --> 00:03:19,560 
The system would need to
check every processing node 
44 
00:03:19,560 --> 00:03:21,020 
before it can discard data. 
45 
00:03:22,040 --> 00:03:28,373 
Each note and each database has
to be replicated separately. 
46 
00:03:28,373 --> 00:03:35,255 
Batch computations that need data from
multiple data servers need to access and 
47 
00:03:35,255 --> 00:03:42,453 
maintain use of the data separately which
might end up being quite slow and costly. 
48 
00:03:42,453 --> 00:03:46,705 
Big data processing techniques
we will address in this course, 
49 
00:03:46,705 --> 00:03:51,430 
will help you to reduce the management
of the mentioned complexities, 
50 
00:03:51,430 --> 00:03:55,226 
including failing servers and
breaking compute nodes. 
51 
00:03:55,226 --> 00:04:00,840 
While helping with the scalability of the
management and processing infrastructure. 
52 
00:04:02,610 --> 00:04:07,170 
We will talk about using big data systems
like Spark to achieve data parallel 
53 
00:04:07,170 --> 00:04:12,130 
processing scalability for
data applications on commodity clusters. 
54 
00:04:13,450 --> 00:04:18,080 
We will use to Spark Runtime Libraries and
Programming Models to 
55 
00:04:18,080 --> 00:04:22,230 
demonstrate how big data systems can
be used for application management. 
56 
00:04:23,630 --> 00:04:28,260 
To summarize, what our imaginary game
application needs from big data system. 
57 
00:04:29,830 --> 00:04:35,021 
First of all, there needs to be a way
to use common big data operations 
58 
00:04:35,021 --> 00:04:39,778 
to manage and split large volumes
of events data streaming in. 
59 
00:04:39,778 --> 00:04:43,740 
This means the partitioning and
placement of data in and 
60 
00:04:43,740 --> 00:04:49,400 
out of computer memory along with a model
to synchronize the datasets later on. 
61 
00:04:50,960 --> 00:04:54,410 
The access to data should
be achieved in a fast way. 
62 
00:04:56,060 --> 00:04:58,710 
The game developers need
to be able to deploy 
63 
00:04:58,710 --> 00:05:03,630 
many event processing jobs to
distributed processing nodes at once. 
64 
00:05:03,630 --> 00:05:07,350 
And these are potentially the data
nodes we move the computations to. 
65 
00:05:08,850 --> 00:05:13,430 
It should also enable
reliability of the computing and 
66 
00:05:13,430 --> 00:05:16,190 
enable fault tolerance from failures. 
67 
00:05:16,190 --> 00:05:19,998 
This means enabling
programmable replications and 
68 
00:05:19,998 --> 00:05:22,685 
recovery of event data when needed. 
69 
00:05:22,685 --> 00:05:24,119 
It should be easily 
70 
00:05:24,119 --> 00:05:29,238 
scalable to a distributed set of
nodes where the data gets produced. 
71 
00:05:29,238 --> 00:05:32,639 
It should also enable scaling out. 
72 
00:05:32,639 --> 00:05:38,963 
Scaling out is simply adding new
resources like distributed computers to 
73 
00:05:38,963 --> 00:05:44,681 
process more or faster data at
scale without losing performance. 
74 
00:05:44,681 --> 00:05:47,360 
There are many data
types in an online game. 
75 
00:05:48,360 --> 00:05:51,440 
Although, we talked about
time click events and 
76 
00:05:51,440 --> 00:05:56,390 
scores, it would be easy to imagine
there are graphs of players, 
77 
00:05:56,390 --> 00:05:59,790 
text-based chats, and
images that need to be processed. 
78 
00:06:01,390 --> 00:06:04,700 
Our big data system should
enable processing of such 
79 
00:06:04,700 --> 00:06:09,500 
a mixed variety of data and
potentially optimize handling of 
80 
00:06:09,500 --> 00:06:12,870 
each type separately as well
as together when needed. 
81 
00:06:15,180 --> 00:06:19,750 
In addition, our system should
have been able both streaming and 
82 
00:06:19,750 --> 00:06:25,140 
batch processing, enabling all
the processing to be debuggable and 
83 
00:06:25,140 --> 00:06:27,950 
extensible with minimal effort. 
84 
00:06:27,950 --> 00:06:32,110 
That means being able to handle
operations at small chunks of data 
85 
00:06:32,110 --> 00:06:36,450 
streams with minimal delay,
that is what we call low latency. 
86 
00:06:37,810 --> 00:06:44,050 
While at the same time handle processing
of potentially all available data 
87 
00:06:44,050 --> 00:06:49,370 
in batch form and
all through the same system architecture. 
88 
00:06:51,160 --> 00:06:56,330 
Latency is a word that we use and
hear a lot in big data processing, 
89 
00:06:57,370 --> 00:07:02,340 
here we refer to how fast the data
is being processed, or simply 
90 
00:07:02,340 --> 00:07:09,400 
the difference between production or event
time and processing time of a data entry. 
91 
00:07:09,400 --> 00:07:13,560 
In other words, latency is quantification 
92 
00:07:13,560 --> 00:07:17,330 
of the delay in the processing of
the streaming data in the system. 
93 
00:07:19,400 --> 00:07:22,700 
While some big data
systems are good at it. 
94 
00:07:22,700 --> 00:07:27,220 
Hadoop for instance is not a great choice
for operations that require low latency. 
95 
00:07:29,540 --> 00:07:32,962 
Let's finish by remembering
the real reasons for 
96 
00:07:32,962 --> 00:07:36,310 
all these requirements
of big data processing. 
97 
00:07:36,310 --> 00:07:41,559 
Making a different from processing
in a traditional data architecture. 
98 
00:07:41,559 --> 00:07:47,148 
Big data has varying volume and
velocity requiring the dynamic and 
99 
00:07:47,148 --> 00:07:50,575 
scalable batch and stream processing. 
100 
00:07:50,575 --> 00:07:55,048 
Big data has a variety requiring
management of data in many 
101 
00:07:55,048 --> 00:07:59,626 
different data systems and
integration of it all at scale. 
1 
00:00:01,250 --> 00:00:03,485 
Next, we'll describe
aggregation functions. 
2 
00:00:04,760 --> 00:00:07,190 
We have seen the first query before. 
3 
00:00:07,190 --> 00:00:11,460 
Select count(*) simply
translates to a count function. 
4 
00:00:12,710 --> 00:00:18,351 
Now we could also say
db.Drinkers.find.count. 
5 
00:00:18,351 --> 00:00:20,880 
But using count directly
is more straightforward. 
6 
00:00:23,190 --> 00:00:27,720 
Now, let's ask to count the number
of unique addresses for drinkers. 
7 
00:00:29,160 --> 00:00:32,740 
So, we don't care what the address is. 
8 
00:00:32,740 --> 00:00:34,210 
We just care if it exists. 
9 
00:00:35,660 --> 00:00:41,605 
This is accomplished through
the $exists:true expression. 
10 
00:00:41,605 --> 00:00:45,590 
Thus, if an address exists for
a drinker, it will be counted. 
11 
00:00:47,740 --> 00:00:52,190 
Another area where we need to count is
when we have an array valued attribute, 
12 
00:00:52,190 --> 00:00:53,040 
like places. 
13 
00:00:54,580 --> 00:00:59,200 
If we just want the number
of elements in the raw list, 
14 
00:00:59,200 --> 00:01:05,267 
we'll write db.country.findplaces.length
and we'll get six. 
15 
00:01:05,267 --> 00:01:10,538 
However, if we want distinct values, we'll
use distinct instead of find and then 
16 
00:01:10,538 --> 00:01:15,531 
use the length for counting the number
of distinct elements, in this case 4. 
17 
00:01:17,927 --> 00:01:24,178 
Now, MongoDB uses an internal machinery
called the aggregation framework, 
18 
00:01:24,178 --> 00:01:29,208 
which is modeled on the concept
of data processing pipelines. 
19 
00:01:29,208 --> 00:01:34,840 
That means documents enter
a multi-stage pipeline which transforms 
20 
00:01:34,840 --> 00:01:40,286 
the documents at each stage until
it becomes an aggregated result. 
21 
00:01:40,286 --> 00:01:44,078 
Now we have seeing a similar mechanism for
relational data. 
22 
00:01:44,078 --> 00:01:48,440 
The aggregation pipelines starts
by using the aggregate primitive. 
23 
00:01:49,680 --> 00:01:55,480 
The most basic pipeline stages provides
filters that operate like queries and 
24 
00:01:55,480 --> 00:01:59,130 
the document transformations that
modify the form of the output document. 
25 
00:02:00,310 --> 00:02:06,890 
The primary filter operation is $match,
which is followed by a query condition. 
26 
00:02:06,890 --> 00:02:10,401 
In this case, status is A. 
27 
00:02:10,401 --> 00:02:14,652 
And expectedly, the $match operation
produces a smaller number of documents 
28 
00:02:14,652 --> 00:02:16,500 
to be processed at the next stage. 
29 
00:02:17,960 --> 00:02:22,730 
This is usually followed
by the $group operation. 
30 
00:02:22,730 --> 00:02:26,040 
Now this operation needs to know which
attributes should be grouped together. 
31 
00:02:27,130 --> 00:02:31,258 
In the example here cust_id
is the grouping attribute so 
32 
00:02:31,258 --> 00:02:35,750 
it is passed as a parameter
to the $group function. 
33 
00:02:35,750 --> 00:02:38,143 
Now notice the syntax. 
34 
00:02:38,143 --> 00:02:44,649 
_id:$cust_id says that the grouped
data will have an _id attribute, 
35 
00:02:44,649 --> 00:02:48,573 
and its value will be
picked from the cust_id 
36 
00:02:48,573 --> 00:02:53,128 
attribute from the previous
stage of computation. 
37 
00:02:53,128 --> 00:02:57,466 
Thus, the $ before the cust_id
is telling the system that 
38 
00:02:57,466 --> 00:03:01,990 
cust_id is a known variable in
the system and not a constant. 
39 
00:03:01,990 --> 00:03:04,639 
The $group operation also needs a reducer, 
40 
00:03:04,639 --> 00:03:10,250 
which is a function that operates on an
activity to produce an aggregate result. 
41 
00:03:10,250 --> 00:03:13,590 
In this case, the reduce function is sum, 
42 
00:03:14,980 --> 00:03:17,720 
which operates on the amount
attribute from the previous stage. 
43 
00:03:18,790 --> 00:03:23,560 
Like $cust_id, we use $amount to
indicate that it's a variable. 
44 
00:03:25,070 --> 00:03:27,590 
As we saw in the relational case, 
45 
00:03:27,590 --> 00:03:32,580 
data can be partitioned into chunks on
the same machine or on different machines. 
46 
00:03:32,580 --> 00:03:35,320 
These chunks are called chards. 
47 
00:03:35,320 --> 00:03:42,284 
The aggregation pipeline of MongoDB
can operate on a charded collection. 
48 
00:03:42,284 --> 00:03:47,156 
The grouping operation in MongoDB
can accept multiple attributes like 
49 
00:03:47,156 --> 00:03:48,650 
the four shown here. 
50 
00:03:48,650 --> 00:03:53,915 
Also shown is a post grouping directive
to sort on the basis of two attributes. 
51 
00:03:55,380 --> 00:04:00,150 
The first is a computed count
variable in ascending order. 
52 
00:04:00,150 --> 00:04:03,728 
So the one designates the ascending order. 
53 
00:04:03,728 --> 00:04:05,870 
The next sorting attribute is secondary. 
54 
00:04:05,870 --> 00:04:08,966 
That means if two groups have
the same value for count, 
55 
00:04:08,966 --> 00:04:13,110 
then they'll be further sorted
based on the category value. 
56 
00:04:13,110 --> 00:04:18,860 
But this time the order is descending
because of the -1 directive 
57 
00:04:22,207 --> 00:04:27,280 
In course two we have seen Solar,
a text search engine from Apache. 
58 
00:04:28,410 --> 00:04:32,830 
MongoDB has a built in text search engine
which can be invoked through the same 
59 
00:04:32,830 --> 00:04:34,360 
aggregation framework we saw before. 
60 
00:04:35,360 --> 00:04:40,006 
Imagine that MongoDB documents in this
case are really text documents placed in 
61 
00:04:40,006 --> 00:04:41,730 
a collection called articles. 
62 
00:04:43,270 --> 00:04:48,560 
In this case, the $match directive of
the aggregate function must be told 
63 
00:04:48,560 --> 00:04:51,955 
it's going to perform a text
function on the article's corpus. 
64 
00:04:53,320 --> 00:04:55,884 
The actual text function is $search. 
65 
00:04:55,884 --> 00:05:01,080 
We set search terms like "Hillary
Democrat" such that having 
66 
00:05:01,080 --> 00:05:07,210 
either term in a document will
satisfy the search requirement. 
67 
00:05:07,210 --> 00:05:11,260 
As is the case of any text engine, 
68 
00:05:11,260 --> 00:05:15,410 
the results of any search returns
a list of documents, each with a score. 
69 
00:05:16,600 --> 00:05:21,780 
The next task is to tell MongDB to
sort the results based on textScore. 
70 
00:05:23,050 --> 00:05:24,290 
What's the $meta here? 
71 
00:05:25,420 --> 00:05:29,532 
Meta stands for metadata,
that is additional information. 
72 
00:05:29,532 --> 00:05:33,860 
Remember that the aggregation
operations are executed in a pipeline. 
73 
00:05:35,090 --> 00:05:38,705 
Any step in the pipeline can
produce some extra data, or 
74 
00:05:38,705 --> 00:05:41,780 
metadata, for each processed document. 
75 
00:05:41,780 --> 00:05:45,470 
In this example, the metadata
produced by the search function 
76 
00:05:45,470 --> 00:05:48,670 
is a computed attribute called textScore. 
77 
00:05:48,670 --> 00:05:54,627 
So this directive tells the system to pick
up this specific metadata attribute and 
78 
00:05:54,627 --> 00:05:59,828 
use it to populate the score attribute
which would be used for sorting. 
79 
00:05:59,828 --> 00:06:04,501 
Finally, the $project class
does exactly what is expected. 
80 
00:06:04,501 --> 00:06:09,006 
It tells the system to output only
the title of each document and 
81 
00:06:09,006 --> 00:06:10,284 
suppress its id. 
82 
00:06:14,760 --> 00:06:19,120 
The last item in our
discussion of MongoDB is join. 
83 
00:06:19,120 --> 00:06:22,820 
We have seen that join is a vital
operation for data management operations. 
84 
00:06:25,010 --> 00:06:32,062 
Interestingly, MongoDB introduced this
equivalent of join only in version 3.2. 
85 
00:06:32,062 --> 00:06:36,320 
So, the joining in MongoDB also
happens in the aggregation framework. 
86 
00:06:37,740 --> 00:06:40,420 
There are a few ways of
expressing joins in MongoDB. 
87 
00:06:41,470 --> 00:06:46,980 
We show one here that explicitly performs
a join to a function called look up. 
88 
00:06:48,690 --> 00:06:51,445 
We use an example form
the MongoDB documentation. 
89 
00:06:51,445 --> 00:06:58,380 
Now here are two document collections,
order and inventory. 
90 
00:06:58,380 --> 00:07:03,600 
Notice that the item key in
orders has values abc, jkl, etc. 
91 
00:07:04,830 --> 00:07:10,100 
And the sku key in the inventory has
comparable values abc, def, etc. 
92 
00:07:10,100 --> 00:07:12,270 
So these two are joinable by value. 
93 
00:07:13,550 --> 00:07:19,167 
The way to specify this join,
one can use this query. 
94 
00:07:19,167 --> 00:07:24,124 
The db.orders.aggregate
declaration states that orders is 
95 
00:07:24,124 --> 00:07:27,311 
sort of the home, or local collection. 
96 
00:07:27,311 --> 00:07:32,592 
Now in the aggregate, the function
$lookup needs to know what to look up for 
97 
00:07:32,592 --> 00:07:34,420 
each document in orders. 
98 
00:07:36,050 --> 00:07:41,010 
The from attribute specifies the name
of the collection as inventory. 
99 
00:07:42,590 --> 00:07:44,410 
The next two parameters are the local and 
100 
00:07:44,410 --> 00:07:48,591 
foreign matching keys,
which are item and sku, respectively. 
101 
00:07:49,976 --> 00:07:54,760 
The last item, as:,
is a construction part of 
102 
00:07:54,760 --> 00:07:59,380 
the join operation which says how to
structure the match items into the result. 
103 
00:08:00,780 --> 00:08:03,770 
Now before we show you the results,
let's see what should match. 
104 
00:08:05,740 --> 00:08:10,020 
The abc item in order should
match the abc in sku. 
105 
00:08:11,120 --> 00:08:16,050 
Similarly, the jkl item
should match the jkl in sku. 
106 
00:08:17,260 --> 00:08:19,540 
Okay, but there is one more twist. 
107 
00:08:21,020 --> 00:08:22,920 
Here is the actual result. 
108 
00:08:22,920 --> 00:08:27,090 
The first two records show
exactly what we expect. 
109 
00:08:27,090 --> 00:08:29,760 
There is a new field called
inventory-docs in the batching record. 
110 
00:08:31,640 --> 00:08:34,580 
The third record however,
shows something interesting. 
111 
00:08:36,170 --> 00:08:40,250 
Inventory has two records shown here,
what do they match? 
112 
00:08:41,670 --> 00:08:46,890 
Now they match the empty
document in orders because 
113 
00:08:46,890 --> 00:08:50,700 
orders has a document
whose item field is null. 
114 
00:08:51,980 --> 00:08:57,355 
So it matches documents and
inventory where the sku item is also null, 
115 
00:08:57,355 --> 00:09:02,290 
explicitly as in document 5, or
implicitly as in document 6. 
116 
00:09:04,801 --> 00:09:08,948 
This concludes our discussion of
queries in the context of MongoDB. 
1 
00:00:00,720 --> 00:00:05,150 
In this hands on activity, we'll be
using Pandas to read CSV files and 
2 
00:00:05,150 --> 00:00:07,300 
perform various queries on them. 
3 
00:00:07,300 --> 00:00:10,070 
Pandas is a data analysis library for
Python. 
4 
00:00:12,140 --> 00:00:14,730 
First, we'll create a new
Jupyter Python Notebook. 
5 
00:00:16,070 --> 00:00:20,060 
Next, we will use Pandas to read
a CSV file into a DataFrame. 
6 
00:00:21,375 --> 00:00:25,490 
We'll then view the contents of the
DataFrame and see how to filter rows and 
7 
00:00:25,490 --> 00:00:26,130 
columns of it. 
8 
00:00:27,600 --> 00:00:31,410 
Next, we will perform average and
sum operations on the DataFrame. 
9 
00:00:31,410 --> 00:00:36,760 
And finally, show how to merge two
DataFrames by joining on a single column. 
10 
00:00:39,160 --> 00:00:39,660 
Let's begin. 
11 
00:00:41,420 --> 00:00:44,370 
We'll start by creating
a new iPython notebook. 
12 
00:00:44,370 --> 00:00:48,570 
Clicking on New and
selecting Python 3 under notebooks. 
13 
00:00:51,900 --> 00:00:56,350 
First, we'll import the Pandas
library by writing import pandas. 
14 
00:00:58,010 --> 00:01:00,710 
Remember that in iPython notebooks,
to run a command, 
15 
00:01:00,710 --> 00:01:02,980 
we hold down the shift key and hit Enter. 
16 
00:01:06,599 --> 00:01:12,395 
Next, let's read buyclicks.csv
into a Pandas DataFrame. 
17 
00:01:12,395 --> 00:01:14,730 
We'll put it in a variable
called buyclicksDF. 
18 
00:01:18,135 --> 00:01:22,751 
We'll read it using pandas.read_csv, 
19 
00:01:22,751 --> 00:01:27,630 
and we'll read the buy-clicks.csv file. 
20 
00:01:30,966 --> 00:01:34,870 
We can see the contents of the file by
just running the variable by itself. 
21 
00:01:39,948 --> 00:01:44,398 
And notice that the file has many rows and
then iPython truncates this, 
22 
00:01:44,398 --> 00:01:45,740 
the dot, dot, dot. 
23 
00:01:49,227 --> 00:01:52,027 
We can see only the top five
rows by calling .head(5). 
24 
00:01:53,417 --> 00:01:58,343 
Next, let's look at only two 
25 
00:01:58,343 --> 00:02:04,500 
columns in the buyclicks data. 
26 
00:02:04,500 --> 00:02:06,310 
Let's look at price and user ID. 
27 
00:02:07,330 --> 00:02:13,680 
We can do these by first entering
buyclicks DataFrame in the same text for 
28 
00:02:13,680 --> 00:02:19,040 
specifying only certain columns to
show is open bracket open bracket and 
29 
00:02:19,040 --> 00:02:20,950 
then the name of the columns
you want to view. 
30 
00:02:20,950 --> 00:02:25,700 
So, want your price and
user ID, and again, 
31 
00:02:25,700 --> 00:02:31,395 
we only want to see the first five rows,
so we'll do .head(5). 
32 
00:02:35,016 --> 00:02:39,350 
Now, let's query the buyclicks data for
only the prices less than 3. 
33 
00:02:39,350 --> 00:02:44,494 
First, we'll enter buyclicksDF,
One square bracket, 
34 
00:02:44,494 --> 00:02:49,007 
to filter our particular column,
we enter buyclicksDF and then column name. 
35 
00:02:50,440 --> 00:02:54,440 
Now, we specify the limit of
the query by entering <3. 
36 
00:03:02,378 --> 00:03:06,080 
This shows first five rows where
the price is less than three. 
37 
00:03:08,409 --> 00:03:12,205 
We can also perform aggregate
operations on Panda's DataFrames. 
38 
00:03:13,650 --> 00:03:21,673 
We can sum all the price data by
entering buyclicksDF['price'].sum. 
39 
00:03:25,760 --> 00:03:29,470 
Another aggregate operation we can
perform is looking at the average. 
40 
00:03:29,470 --> 00:03:31,820 
Let's look at the average for price. 
41 
00:03:31,820 --> 00:03:33,540 
The function is called mean. 
42 
00:03:34,720 --> 00:03:40,782 
So, once your buyclicksDF ['price'].mean. 
43 
00:03:45,341 --> 00:03:48,460 
Can also join two DataFrames
on a single column. 
44 
00:03:49,870 --> 00:03:54,620 
First, let's read in another
CSV into a different DataFrame. 
45 
00:03:54,620 --> 00:03:57,700 
We'll read in adclicks.csv. 
46 
00:03:57,700 --> 00:04:04,723 
So we'll says adclicksDF
= pandas.read_csv, 
47 
00:04:04,723 --> 00:04:10,286 
we'll say ('ad-clicks.csv'). 
48 
00:04:16,538 --> 00:04:20,870 
To verify that we read this data
successfully, let's look at the contents. 
49 
00:04:29,077 --> 00:04:32,063 
Now, let's combine
the buyclicks DataFrame and 
50 
00:04:32,063 --> 00:04:34,700 
the adclicks data frame
on the user ID call. 
51 
00:04:36,690 --> 00:04:39,480 
We'll put the result in the new
DataFrame called mergeDF. 
52 
00:04:39,480 --> 00:04:46,400 
So we'll say mergeDF = adclicksDF.merge 
53 
00:04:50,465 --> 00:04:54,820 
Then, we need to say which
DataFrame we're merging with, 
54 
00:04:54,820 --> 00:04:59,590 
buyclicksDF, and
the column that we're merging on. 
55 
00:04:59,590 --> 00:05:02,080 
So we'll say on='userid'. 
56 
00:05:08,461 --> 00:05:11,681 
Finally, we can look at the contents
with this merged DataFrame. 
1 
00:00:01,510 --> 00:00:05,380 
So now from MongoDB we will go to
Aerospike which is a key value store. 
2 
00:00:07,030 --> 00:00:09,120 
Key value stores typically offer an API. 
3 
00:00:10,480 --> 00:00:14,630 
That is the way to access data using a
programming language like Python or Java. 
4 
00:00:15,850 --> 00:00:19,620 
We will take a very brief look
at Aerospike, which offers both 
5 
00:00:19,620 --> 00:00:23,550 
a programmatic access and
a limited amount of query access to data. 
6 
00:00:25,550 --> 00:00:27,910 
The data model of Aerospike
is illustrated here. 
7 
00:00:29,310 --> 00:00:34,960 
Data are organized in lean spaces which
can be in memory or on flash disks. 
8 
00:00:36,410 --> 00:00:38,840 
Name spaces are top level data containers. 
9 
00:00:39,980 --> 00:00:45,080 
The way you collect data in name spaces
relates to how data is stored and managed. 
10 
00:00:46,270 --> 00:00:50,395 
So name space contains records,
indexes, and policies. 
11 
00:00:50,395 --> 00:00:55,130 
Now policies dictate name space
behavior like how data is stored, 
12 
00:00:55,130 --> 00:00:57,790 
whether it's on RAM or disk, or 
13 
00:00:57,790 --> 00:01:02,150 
how how many replicas exist for
a record, and when records expire. 
14 
00:01:04,840 --> 00:01:08,606 
A name space can contain sets,
you can think of them as tables. 
15 
00:01:08,606 --> 00:01:11,470 
So here there are two sets,
people and places, 
16 
00:01:11,470 --> 00:01:15,140 
and a set of records
which are not in any set. 
17 
00:01:16,750 --> 00:01:20,620 
Within a record,
data is stored in one or many bins. 
18 
00:01:21,980 --> 00:01:25,490 
Bins consist of a name and a value. 
19 
00:01:27,820 --> 00:01:31,970 
The example written here is in Java, and 
20 
00:01:31,970 --> 00:01:34,230 
you don't have to know Java to
follow the main point here. 
21 
00:01:36,068 --> 00:01:40,470 
Here we are creating indexes on key
value data that's handled by Aerospike. 
22 
00:01:41,780 --> 00:01:43,500 
This data set comes from Twitter. 
23 
00:01:44,990 --> 00:01:50,310 
Each field of a tweet is extracted and
put into Aerospike as a key value pair. 
24 
00:01:50,310 --> 00:01:55,420 
So we declare the namespace to be 
25 
00:01:55,420 --> 00:01:59,880 
example, and the record set to be tweet. 
26 
00:02:01,510 --> 00:02:07,460 
The name of the index to be TestIndex,
and the name of the bin as user name. 
27 
00:02:09,490 --> 00:02:14,330 
Since this index is stored
on disk an SQL-like command, 
28 
00:02:14,330 --> 00:02:18,638 
like SHOW INDEX, shows the content
of the index as you can see here. 
29 
00:02:18,638 --> 00:02:23,220 
This routine shows 
30 
00:02:23,220 --> 00:02:27,419 
how data can be inserted into
Aerospike programmatically. 
31 
00:02:27,419 --> 00:02:31,500 
Again, the goal is to point out a few
salient aspects of data insertion 
32 
00:02:31,500 --> 00:02:33,090 
regardless of the syntax of the language. 
33 
00:02:33,090 --> 00:02:38,120 
Now since this is a key value store,
one first needs to define the key. 
34 
00:02:40,220 --> 00:02:45,414 
This line here says that the key in
the namespace call example, and set call 
35 
00:02:45,414 --> 00:02:50,699 
tweet, is the value of the function getId,
which returns the ID of a tweet. 
36 
00:02:53,288 --> 00:02:57,028 
When the data is populated,
we are essentially creating bins. 
37 
00:02:57,028 --> 00:03:01,500 
Here, user name is the attribute and 
38 
00:03:01,500 --> 00:03:04,290 
the screen name obtained
from the tweet is the value. 
39 
00:03:05,510 --> 00:03:09,440 
The actual insertion
happens here in the client. 
40 
00:03:11,200 --> 00:03:15,574 
The client.put statement,
where we need to mention the key and 
41 
00:03:15,574 --> 00:03:18,510 
the bins we have just created. 
42 
00:03:18,510 --> 00:03:21,260 
Now why are we inserting
two bins at a time? 
43 
00:03:21,260 --> 00:03:23,140 
Two bins with two ids and user name? 
44 
00:03:24,150 --> 00:03:27,180 
This is an idiosyncrasy
of the Aerospike client. 
45 
00:03:29,810 --> 00:03:34,190 
After data is inserted one can
create other data using AQL 
46 
00:03:34,190 --> 00:03:35,450 
which is very much like SQL. 
47 
00:03:36,870 --> 00:03:41,920 
This screenshot shows a part of
the output of a simple select star query. 
48 
00:03:44,680 --> 00:03:49,100 
Now in your hands-on exercise, you'll be
able to play with the Aerospike data. 
49 
00:03:50,520 --> 00:03:56,740 
This is just a screenshot showing
the basic query syntax of AQL, 
50 
00:03:56,740 --> 00:04:00,110 
that is Aerospike Query Language,
and a few examples. 
51 
00:04:01,370 --> 00:04:04,960 
The last two lines show a couple of
interesting features of the language. 
52 
00:04:06,090 --> 00:04:12,200 
The operation between 0 and 99,
is a nicer way of stating a range query, 
53 
00:04:12,200 --> 00:04:14,850 
which gives a lower and
upper limits on a variable. 
54 
00:04:16,030 --> 00:04:20,550 
The last line shows the operation cost, 
55 
00:04:20,550 --> 00:04:24,670 
which transforms one type
of data to another type. 
56 
00:04:24,670 --> 00:04:29,047 
Here it transforms coordinates,
that is latitude and longitude data, 
57 
00:04:29,047 --> 00:04:33,784 
to a JSON format called GeoJSON which is
designed to represent geographic data 
58 
00:04:33,784 --> 00:04:35,092 
in a JSON structure. 
59 
00:04:38,762 --> 00:04:42,421 
We will finish our coverage of queries
with a quick reference to an advanced 
60 
00:04:42,421 --> 00:04:44,870 
topic, which is beyond
the scope of this course. 
61 
00:04:46,068 --> 00:04:50,780 
Now you have seen in prior courses that
streaming data is complex to process 
62 
00:04:50,780 --> 00:04:53,310 
because a stream is infinite in nature. 
63 
00:04:54,480 --> 00:04:57,640 
Now does this have any impact on
query languages and evaluation? 
64 
00:04:58,790 --> 00:05:01,840 
The answer is that it absolutely does. 
65 
00:05:01,840 --> 00:05:03,590 
We'll mention one such impact here. 
66 
00:05:05,150 --> 00:05:08,700 
This shows a pictorial
depiction of streaming data. 
67 
00:05:09,890 --> 00:05:14,170 
The data is segmented into five pieces,
shown in the white boxes in the upper row. 
68 
00:05:15,620 --> 00:05:20,280 
These can be gathered, for example every
few seconds, or every 200 data objects. 
69 
00:05:21,980 --> 00:05:24,970 
The query defines a window to select 
70 
00:05:24,970 --> 00:05:27,770 
key of these data objects
as a unit of processing. 
71 
00:05:27,770 --> 00:05:28,860 
Here case three. 
72 
00:05:30,000 --> 00:05:34,120 
So three units of data
are picked up in a window unit. 
73 
00:05:34,120 --> 00:05:37,020 
To get the next window it
is moved by two units, 
74 
00:05:38,030 --> 00:05:40,280 
this is called a slide of the window. 
75 
00:05:41,970 --> 00:05:46,030 
Since the window size is three and
the slide is two, 
76 
00:05:46,030 --> 00:05:50,130 
one unit of information overlaps
between the two consecutive windows. 
77 
00:05:51,450 --> 00:05:56,056 
The lower line in this diagram shows
the initialized item, let's ignore it, 
78 
00:05:56,056 --> 00:06:03,980 
followed by two window sets of
data records for processing. 
79 
00:06:05,090 --> 00:06:08,670 
Thus the query language therefore,
will have to specify a query 
80 
00:06:08,670 --> 00:06:14,650 
that's an SQL query, over a window,
which is also specified in the query. 
81 
00:06:14,650 --> 00:06:18,980 
Now in a traffic data stream example,
the SQL statement might look like this. 
82 
00:06:20,330 --> 00:06:23,490 
Where the window size is 30 second, and 
83 
00:06:23,490 --> 00:06:26,520 
the slide is the same size as window
giving output every 30 seconds. 
84 
00:06:27,760 --> 00:06:32,650 
So streaming data results in changes
in both the query language and 
85 
00:06:32,650 --> 00:06:34,560 
the way queries are processed. 
1 
00:00:00,008 --> 00:00:04,900 
In this hands-on activity,
we will be querying documents in MongoDB. 
2 
00:00:06,020 --> 00:00:10,480 
First, we will start a MongoDB server and
then run the MongoDB Shell. 
3 
00:00:10,480 --> 00:00:15,730 
We will see how to show the databases and
collections in the MongoDB Server. 
4 
00:00:15,730 --> 00:00:19,520 
We will look at an example
document in the database and 
5 
00:00:19,520 --> 00:00:21,529 
see how to find distinct values for
a field. 
6 
00:00:23,360 --> 00:00:26,540 
Next, we will search for
specific field values and 
7 
00:00:26,540 --> 00:00:28,620 
see how to filter fields
returned in a query. 
8 
00:00:30,530 --> 00:00:34,070 
Finally, we will search using
regular expressions and operators. 
9 
00:00:36,903 --> 00:00:41,361 
Let's begin,
first we'll start the MongoDB server. 
10 
00:00:41,361 --> 00:00:46,830 
Open the terminal window by clicking on
the terminal icon, the top of the toolbar. 
11 
00:00:46,830 --> 00:00:53,680 
We'll cd into
Downloads/big-data-3-mongodb. 
12 
00:00:53,680 --> 00:00:59,495 
We'll start the MongoDB server by running 
13 
00:00:59,495 --> 00:01:05,675 
./mongodb/bin/mongod --dbpath db. 
14 
00:01:05,675 --> 00:01:08,930 
The arguments dbpath db 
15 
00:01:08,930 --> 00:01:13,330 
specified that the database on
MongoDB is in the directory named db. 
16 
00:01:15,260 --> 00:01:16,780 
Run this to start the server. 
17 
00:01:20,530 --> 00:01:24,413 
Next, we'll start the MongoDB Shell so
that we can perform queries. 
18 
00:01:24,413 --> 00:01:29,416 
We'll open another terminal window. 
19 
00:01:29,416 --> 00:01:36,417 
Again, cd into
Downloads/big-data-3-mongodb. 
20 
00:01:36,417 --> 00:01:41,955 
We'll start the shell by
running ./mongodb/bin/mongo. 
21 
00:01:49,734 --> 00:01:55,237 
We can see what databases are available in
the MongoDB server by running the command, 
22 
00:01:55,237 --> 00:01:55,933 
show dbs. 
23 
00:01:59,521 --> 00:02:02,680 
We've created the sample database
with JSON data from Twitter. 
24 
00:02:04,400 --> 00:02:08,400 
We can use the use command
to change to this database. 
25 
00:02:08,400 --> 00:02:10,245 
We'll run use sample. 
26 
00:02:13,078 --> 00:02:16,828 
We can see the collections in this
database by running, show collections. 
27 
00:02:19,858 --> 00:02:22,656 
There's only one collection called users. 
28 
00:02:22,656 --> 00:02:26,987 
So, all of the queries will be
using the users collection. 
29 
00:02:26,987 --> 00:02:31,945 
Let's count how many documents there
are in the users collection by writing 
30 
00:02:31,945 --> 00:02:33,349 
db.users.count. 
31 
00:02:38,225 --> 00:02:43,241 
We can look at one of these documents
by writing db.users.findOne. 
32 
00:02:51,078 --> 00:02:55,443 
This document contains
Jason from a Twitter tweet. 
33 
00:02:55,443 --> 00:02:58,740 
You can see the field names
here with their values. 
34 
00:02:58,740 --> 00:03:02,380 
There are also nested fields
under the user field. 
35 
00:03:02,380 --> 00:03:04,410 
And each of these fields also has values. 
36 
00:03:05,730 --> 00:03:08,870 
You can find the distinct values for 
37 
00:03:08,870 --> 00:03:12,320 
particular field,
by using the distinct command. 
38 
00:03:12,320 --> 00:03:15,058 
Let's find the distinct values for
the username field. 
39 
00:03:15,058 --> 00:03:21,234 
We'll write
db.users.distinct("user_name"). 
40 
00:03:30,324 --> 00:03:35,193 
Next, let's find all the documents in
this collection where the field username 
41 
00:03:35,193 --> 00:03:36,990 
matches the specific value. 
42 
00:03:38,050 --> 00:03:41,360 
The value we'll search for
is ActionSportsJax. 
43 
00:03:42,730 --> 00:03:46,851 
We'll run the command db.users.find. 
44 
00:03:46,851 --> 00:03:49,980 
And the field name is username, and 
45 
00:03:49,980 --> 00:03:54,688 
the value we're searching for
is ActionSportsJax. 
46 
00:04:04,405 --> 00:04:07,050 
Results of this query is
compressed all in one line. 
47 
00:04:08,650 --> 00:04:11,401 
If we append the .pretty
to the previous query. 
48 
00:04:11,401 --> 00:04:13,874 
We can see the formatted output. 
49 
00:04:13,874 --> 00:04:16,776 
So we'll run the same
command with append .pretty. 
50 
00:04:23,289 --> 00:04:25,980 
We can filter the fields
returned from the queries. 
51 
00:04:27,560 --> 00:04:31,709 
Let's perform the same query again but
only show the tweet_id field. 
52 
00:04:33,200 --> 00:04:36,090 
We can do this by adding a second
argument to the find command. 
53 
00:04:37,440 --> 00:04:42,648 
So we'll run the same command again, 
54 
00:04:42,648 --> 00:04:49,827 
but add a second argument,
saying tweet_ID: 1. 
55 
00:04:49,827 --> 00:04:55,748 
The underscore ID field is a primary
key used in all documents in MongoDB. 
56 
00:04:55,748 --> 00:04:58,280 
You can turn this off by adding
another field to our filter. 
57 
00:05:00,130 --> 00:05:03,287 
We'll run the same command again,
but turn of _ID. 
58 
00:05:11,820 --> 00:05:16,550 
Next, we use the regular expression search
to find strings containing the document. 
59 
00:05:17,680 --> 00:05:21,640 
For example, if we want to find all
the tweets containing the text FIFA, 
60 
00:05:21,640 --> 00:05:25,171 
we can run db.users.find tweet_text FIFA. 
61 
00:05:32,170 --> 00:05:35,850 
There are no results in this query
because this query is searching for 
62 
00:05:35,850 --> 00:05:37,580 
tweet text equals FIFA. 
63 
00:05:39,030 --> 00:05:43,590 
That is the entire contents and
the value of tweet_text must be FIFA. 
64 
00:05:44,720 --> 00:05:46,110 
Instead, if we want to look for 
65 
00:05:46,110 --> 00:05:51,230 
FIFA anywhere in the tweet_text,
we can do a regular expression search. 
66 
00:05:51,230 --> 00:05:55,750 
To do this,
replace the double quotes with slashes. 
67 
00:05:55,750 --> 00:05:57,850 
So we'll run the same command again. 
68 
00:05:57,850 --> 00:06:03,573 
But replacing double quotes with slashes. 
69 
00:06:03,573 --> 00:06:07,423 
We can count how many documents are
returned by this query by running the same 
70 
00:06:07,423 --> 00:06:09,540 
command again, but appending .count. 
71 
00:06:13,325 --> 00:06:17,948 
We can also search for documents in
MongoDB where the field values are greater 
72 
00:06:17,948 --> 00:06:20,020 
than or less than a certain value. 
73 
00:06:21,170 --> 00:06:26,162 
For example, lets find tweet
mention count greater than six. 
74 
00:06:26,162 --> 00:06:32,977 
We'll run db.users.find, and
the field name is tweet_mentioned_count. 
75 
00:06:35,670 --> 00:06:41,265 
And we want to look for the value
where this field is greater than six. 
76 
00:06:41,265 --> 00:06:47,940 
So I'll enter a { $gt : 6 }. 
1 
00:00:01,500 --> 00:00:06,870 
In a prior course we looked at JSON as
an example of semi-structured data, 
2 
00:00:06,870 --> 00:00:10,020 
and we demonstrated that JSON
data can be thought of as a tree. 
3 
00:00:11,200 --> 00:00:14,180 
In this course,
we'll focus on querying JSON data. 
4 
00:00:15,330 --> 00:00:20,340 
Before we start, lets review
the details of the JSON structure, and 
5 
00:00:20,340 --> 00:00:24,180 
get an initial sense of how
to query this form of data. 
6 
00:00:25,720 --> 00:00:28,590 
Let's consider a simple
JSON collection and 
7 
00:00:28,590 --> 00:00:32,330 
look at the structures,
substructures actually, it's composed of. 
8 
00:00:34,160 --> 00:00:37,990 
The atomic element in the structure
is a key value pair, for example, 
9 
00:00:37,990 --> 00:00:43,520 
name is the key and sue is the value,
in this case, an atomic string value. 
10 
00:00:45,560 --> 00:00:47,178 
To query a key value pair, 
11 
00:00:47,178 --> 00:00:52,123 
we should be able perform one basic
operation given the key, return the value. 
12 
00:00:54,140 --> 00:01:00,830 
Now, the value can also be an array,
an array is a list. 
13 
00:01:02,060 --> 00:01:06,940 
So the query operations on it can either
be on its position in the list or 
14 
00:01:06,940 --> 00:01:08,370 
on the value. 
15 
00:01:08,370 --> 00:01:12,980 
Thus, we should be able to ask for the
second element of the array called badges. 
16 
00:01:12,980 --> 00:01:17,180 
Or we should be able to seek objects
of which the key called badges 
17 
00:01:17,180 --> 00:01:18,270 
has a value of blue. 
18 
00:01:20,130 --> 00:01:25,360 
Notice here that the document
collection here is itself an array, 
19 
00:01:25,360 --> 00:01:29,130 
within square brackets and
it's just two elements in it. 
20 
00:01:29,130 --> 00:01:33,664 
The top level array does not have a key,
by default it's called db. 
21 
00:01:36,820 --> 00:01:41,135 
These key value peers
are structured as tuples, 
22 
00:01:41,135 --> 00:01:44,607 
often with a name In the snippet shown, 
23 
00:01:44,607 --> 00:01:48,644 
favorites has a tuple
of two key value pairs. 
24 
00:01:48,644 --> 00:01:53,530 
Now, tuples can be thought of as
relational records, as the operations 
25 
00:01:53,530 --> 00:01:57,800 
would include, projection of an attribute,
and selection over a set of tables. 
26 
00:02:00,330 --> 00:02:03,060 
On the other hand,
the area called 'points', 
27 
00:02:03,060 --> 00:02:07,180 
has two tuples, these two tuples named. 
28 
00:02:07,180 --> 00:02:10,880 
As you will see, we'll address
these tuples by their positions. 
29 
00:02:12,570 --> 00:02:16,000 
Finally, this one has nesting, 
30 
00:02:16,000 --> 00:02:20,110 
that means a mini structure can be
embedded within another structure. 
31 
00:02:21,250 --> 00:02:24,220 
So we need operations
that will let us navigate 
32 
00:02:24,220 --> 00:02:27,350 
from one structure to any of
it's embedded structures. 
33 
00:02:30,030 --> 00:02:35,180 
Now just like a basic SQL query states,
which parts of which records from one or 
34 
00:02:35,180 --> 00:02:39,840 
more people should be reported,
a MongoDB query states 
35 
00:02:39,840 --> 00:02:43,700 
which parts of which documents from
a document collection should be returned. 
36 
00:02:45,010 --> 00:02:51,088 
The primary query is expressed as a find
function, which contains two arguments and 
37 
00:02:51,088 --> 00:02:56,494 
an optional qualifier, there are four
things to notice in this function. 
38 
00:02:58,142 --> 00:03:00,773 
The first is the term collection, 
39 
00:03:00,773 --> 00:03:05,402 
this tells the system which
document collection to use, and 
40 
00:03:05,402 --> 00:03:11,410 
therefore is roughly similar to the From
clause when restricted to one table. 
41 
00:03:11,410 --> 00:03:18,629 
So if the name of the collection is beers,
the first part would say db.beers.find. 
42 
00:03:18,629 --> 00:03:23,098 
The second item is a query filter
which lists all conditions that 
43 
00:03:23,098 --> 00:03:28,159 
the retrieved documents should satisfy,
so it's like a Where clause. 
44 
00:03:30,170 --> 00:03:34,370 
Now, if we want to return everything,
then this filter is left blank. 
45 
00:03:35,550 --> 00:03:39,719 
Otherwise, we'll fill it in a couple
of ways shown in the next few slides. 
46 
00:03:41,330 --> 00:03:45,652 
The third term is a projection class
which is essentially a list of variables 
47 
00:03:45,652 --> 00:03:47,550 
that we want to see in the output. 
48 
00:03:49,520 --> 00:03:53,950 
The fourth and last item sits
after the find function ends and 
49 
00:03:53,950 --> 00:03:57,370 
is separated by a dot,
it's called a cursor modifier. 
50 
00:03:58,370 --> 00:04:02,760 
The word cursor relates back
to SQL where cursor is defined 
51 
00:04:02,760 --> 00:04:06,120 
as a block of results that is
returned to the user in one chunk. 
52 
00:04:07,410 --> 00:04:11,357 
This becomes important when the set of
results is too large to be returned all 
53 
00:04:11,357 --> 00:04:14,256 
together, and
the user may need to specify how much, or 
54 
00:04:14,256 --> 00:04:16,614 
what portion of results
they actually want. 
55 
00:04:18,710 --> 00:04:21,776 
So, we'll start out with a few queries, 
56 
00:04:21,776 --> 00:04:26,156 
where we show how the same query
can be expressed in SQL, and 
57 
00:04:26,156 --> 00:04:31,220 
in MongoDB The first query
wants everything from Beers. 
58 
00:04:31,220 --> 00:04:34,740 
The SQL query is structured
on the table Beers, and 
59 
00:04:34,740 --> 00:04:37,660 
the SELECT * asks to return all rows. 
60 
00:04:38,740 --> 00:04:43,230 
In MongoDB the same query
is more succincted, 
61 
00:04:43,230 --> 00:04:45,830 
since the name of the collection
is already specified in 
62 
00:04:45,830 --> 00:04:49,650 
calling the find function,
the body of the find function is empty. 
63 
00:04:50,650 --> 00:04:55,020 
That means there are no query conditions
and no projection clauses in it. 
64 
00:04:55,020 --> 00:04:59,550 
The second query 
65 
00:04:59,550 --> 00:05:04,160 
needs to return the variables beer and
price for all records. 
66 
00:05:05,810 --> 00:05:12,120 
So the find function here needs an empty
query condition denoted by the open and 
67 
00:05:12,120 --> 00:05:17,860 
closed brace symbols, but the projection
clauses are specifically identified. 
68 
00:05:17,860 --> 00:05:21,760 
There is a 1 if an attribute is output and
a 0 if it is not. 
69 
00:05:21,760 --> 00:05:26,500 
As a shortcut,
only variables with 1 are required. 
70 
00:05:28,000 --> 00:05:29,204 
Okay, so when do you use 0? 
71 
00:05:30,360 --> 00:05:31,870 
A common situation is the following. 
72 
00:05:33,160 --> 00:05:37,548 
Every MongoDB document has
an identifier named _id. 
73 
00:05:40,139 --> 00:05:44,410 
By default every query will
return the id of the document. 
74 
00:05:45,700 --> 00:05:49,150 
If you don't want it to return
this designated attribute, 
75 
00:05:49,150 --> 00:05:55,970 
you should explicitly say, _id:0. 
76 
00:05:55,970 --> 00:05:58,630 
Next we will add query conditions. 
77 
00:05:58,630 --> 00:06:02,640 
That is the equivalent of
the Where clause in SQL. 
78 
00:06:02,640 --> 00:06:07,490 
Our query number three has the query
condition where name is equal to a value. 
79 
00:06:08,600 --> 00:06:15,020 
In MongoDB, that equal to translate
to a variable colon value form. 
80 
00:06:16,070 --> 00:06:21,135 
Notice the symbol used for
a string is quotes. 
81 
00:06:22,987 --> 00:06:25,720 
Query four is more interesting for
two reasons. 
82 
00:06:26,770 --> 00:06:30,980 
First, we see a way in which
the distinct operation is specified. 
83 
00:06:32,580 --> 00:06:38,670 
Notice here that the primary query
function is not find any more but 
84 
00:06:38,670 --> 00:06:40,310 
a new function called distinct. 
85 
00:06:41,790 --> 00:06:45,990 
As we'll see later again in our slides, 
86 
00:06:45,990 --> 00:06:49,660 
MongoDB uses a few special query
functions for some of the operations. 
87 
00:06:51,520 --> 00:06:55,620 
So, you need to know, which function
should be used in what context, 
88 
00:06:55,620 --> 00:06:57,270 
when you write MogoDB queries. 
89 
00:06:58,640 --> 00:07:03,191 
Secondly, in this query,
we have a non-equality condition, 
90 
00:07:03,191 --> 00:07:06,153 
namely, the price is greater than 15. 
91 
00:07:08,370 --> 00:07:12,418 
This example shows MongoDB style
of using operators in a query. 
92 
00:07:12,418 --> 00:07:17,090 
It's always variable: 
93 
00:07:17,090 --> 00:07:21,470 
followed by MongoDB's name for the
operator, and then the comparison value. 
94 
00:07:22,580 --> 00:07:24,959 
So where would you find
MongoDB's operators? 
95 
00:07:26,620 --> 00:07:28,890 
Here are some of the operators
supported in MongoDB. 
96 
00:07:30,300 --> 00:07:33,860 
These operators and others are listed
in the URL shown at the bottom. 
97 
00:07:35,210 --> 00:07:37,430 
The operators shown here are color coded. 
98 
00:07:38,470 --> 00:07:41,866 
The top blue set
are the comparison operators. 
99 
00:07:41,866 --> 00:07:47,760 
We see the $gt, greater than,
operation that we used in the last slide. 
100 
00:07:50,100 --> 00:07:54,780 
The green colored operations are array
operations which we'll see shortly. 
101 
00:07:54,780 --> 00:07:58,370 
And the yellow operators at the bottom
are logical operations that 
102 
00:07:58,370 --> 00:08:02,070 
combine two conditions in different ways
like the AND operation we saw in SQL. 
103 
00:08:02,070 --> 00:08:07,860 
Now, the last operator $nor,
is interesting, 
104 
00:08:07,860 --> 00:08:13,110 
because it is used to specify queries
when neither of two conditions must hold. 
105 
00:08:13,110 --> 00:08:16,960 
For example, find all beer
whose name is neither bad nor 
106 
00:08:16,960 --> 00:08:20,080 
is the price less than $6 per bottle. 
107 
00:08:20,080 --> 00:08:23,021 
Now I would strongly encourage you
to play with these operators in 
108 
00:08:23,021 --> 00:08:24,098 
your hands on session. 
109 
00:08:27,220 --> 00:08:29,980 
Now I'm sure you remember
the like query in SQL. 
110 
00:08:31,420 --> 00:08:37,000 
MongoDB uses regular expressions
to specify partial string matches. 
111 
00:08:37,000 --> 00:08:39,120 
Now some of you may not know
what a regular expression is. 
112 
00:08:39,120 --> 00:08:41,660 
Let's first use some examples. 
113 
00:08:43,680 --> 00:08:48,150 
The first example is the same query
we saw before when we're asking for 
114 
00:08:48,150 --> 00:08:53,514 
beer manufacturers,
whose name has a sub string A-M in it, 
115 
00:08:53,514 --> 00:08:56,620 
so A-M can appear
anywhere within the name. 
116 
00:08:57,630 --> 00:08:58,550 
To do this, 
117 
00:08:58,550 --> 00:09:03,890 
the query condition first states that
it is going to use a $regex operation. 
118 
00:09:04,980 --> 00:09:10,340 
And then we have to give
the partial string as /am/. 
119 
00:09:10,340 --> 00:09:16,250 
Then it gives the directive
that this match 
120 
00:09:16,250 --> 00:09:21,560 
should be case insensitive by placing
an i after the partial string. 
121 
00:09:21,560 --> 00:09:28,070 
And if we just wanted to do names we
would stop right after the find function. 
122 
00:09:28,070 --> 00:09:33,120 
But here we also want to do a count,
which is a post operation 
123 
00:09:33,120 --> 00:09:38,140 
after the find, so we use .count
at the end of the find function. 
124 
00:09:39,860 --> 00:09:46,560 
Now, what if we have the same query,
but we want the partial string A-m? 
125 
00:09:46,560 --> 00:09:49,610 
To appear at the beginning of the name and 
126 
00:09:49,610 --> 00:09:52,070 
you'd like the A to really
be a capital letter. 
127 
00:09:53,460 --> 00:09:57,540 
In this case we use
the caret sign to indicate 
128 
00:09:57,540 --> 00:09:59,950 
that the partial string is at
the beginning of the name. 
129 
00:10:01,090 --> 00:10:05,786 
Naturally we also drop the i at the end
because the match is no longer case 
130 
00:10:05,786 --> 00:10:10,389 
insensitive A more complex 
131 
00:10:10,389 --> 00:10:15,445 
partial string pattern will be a case
where our name starts with capital A-m, 
132 
00:10:15,445 --> 00:10:19,600 
then has a number of characters
in the middle and ends with corp. 
133 
00:10:20,810 --> 00:10:26,140 
So for the first part,
the string pattern is ^Am. 
134 
00:10:26,140 --> 00:10:31,370 
For the second part, that is,
any character in the middle, we use dot 
135 
00:10:31,370 --> 00:10:37,480 
to represent any character, and star
to represent zero or more occurrences. 
136 
00:10:37,480 --> 00:10:40,090 
For the third part, we say corp but 
137 
00:10:40,090 --> 00:10:44,580 
put a dollar at the end to say that it
must appear at the end of the string. 
138 
00:10:45,830 --> 00:10:49,330 
The regular expression pattern is
a sub-language, in itself, and 
139 
00:10:49,330 --> 00:10:52,500 
is supported by most
programming languages today. 
140 
00:10:52,500 --> 00:10:56,530 
We will refer you to the following
URL to learn more about it. 
141 
00:10:58,120 --> 00:11:01,270 
Also, note an example that,
instead of saying, find.count, 
142 
00:11:01,270 --> 00:11:06,200 
we can directly use the count function,
natively defined in MongoDB. 
143 
00:11:08,680 --> 00:11:13,070 
One important feature of JSON is that
everything contain arrays, as a type of 
144 
00:11:13,070 --> 00:11:19,390 
collection objects, this enables us
to query arrays in multiple ways. 
145 
00:11:19,390 --> 00:11:23,460 
One of them,
is to consider an array as a list and 
146 
00:11:23,460 --> 00:11:28,310 
perform intersection operations,
the first query shows this. 
147 
00:11:28,310 --> 00:11:29,840 
The data item is shown on the right. 
148 
00:11:31,190 --> 00:11:35,600 
It has the area value attribute
called tags with three entries. 
149 
00:11:35,600 --> 00:11:40,390 
The first query asks if two specific
strings belong to the array. 
150 
00:11:41,660 --> 00:11:46,510 
In other words, it wants to get
the document whose tagged attribute 
151 
00:11:46,510 --> 00:11:48,480 
intersects with the query supplied array. 
152 
00:11:49,520 --> 00:11:53,640 
In this case, there is an intersection and
the document is returned. 
153 
00:11:55,250 --> 00:11:57,950 
In the second case, it is asking for 
154 
00:11:57,950 --> 00:12:01,570 
a documents who's tags
attribute has no intersection. 
155 
00:12:02,920 --> 00:12:08,880 
Now notice the $nin operator so
there is no intersection with this list. 
156 
00:12:10,730 --> 00:12:14,810 
So in this document there exists and
intersection so nothing will be returned. 
157 
00:12:16,940 --> 00:12:21,560 
A different kind of array query uses
the positions of the list elements and 
158 
00:12:21,560 --> 00:12:23,680 
wants to extract a portion of the array. 
159 
00:12:24,950 --> 00:12:28,000 
This is illustrated in
the third query which asks for 
160 
00:12:28,000 --> 00:12:31,140 
the second and third items of the array. 
161 
00:12:31,140 --> 00:12:35,890 
To encode this in MongoDB,
we use the $slice operator 
162 
00:12:35,890 --> 00:12:41,890 
which needs two parameters,
the number of variable limits to skip, 
163 
00:12:41,890 --> 00:12:44,370 
and the number of variable limits
to extract after skipping. 
164 
00:12:45,420 --> 00:12:50,910 
In this case, we need to extract items two
and three, so the skip value is one and 
165 
00:12:50,910 --> 00:12:55,130 
the number of items is two,
thus returning summer and Japanese. 
166 
00:12:56,320 --> 00:13:00,990 
Now, we could get the same result if we
pose the query using the last statement. 
167 
00:13:02,080 --> 00:13:06,740 
In this case, the minus says that
the system should count from the end and 
168 
00:13:07,800 --> 00:13:10,360 
the true says that it should
extract two elements. 
169 
00:13:11,990 --> 00:13:16,096 
Now if we omitted the minus sign,
it will come from the beginning and 
170 
00:13:16,096 --> 00:13:17,981 
fetch the first two elements. 
171 
00:13:21,300 --> 00:13:27,570 
Finally, we can also ask for a document
who's second element in tags is summer. 
172 
00:13:27,570 --> 00:13:33,529 
In this case we use an array index
tags.1 to denote the second element. 
173 
00:13:37,580 --> 00:13:42,232 
Compound statements are queries
with multiple query conditions that 
174 
00:13:42,232 --> 00:13:45,750 
are combined using logical operations. 
175 
00:13:45,750 --> 00:13:51,167 
The query shown here has one condition,
which is the and,are in terms of MongoDB, 
176 
00:13:51,167 --> 00:13:53,640 
the $and of three different clauses. 
177 
00:13:54,710 --> 00:13:57,310 
The last clause is
the most straight forward, 
178 
00:13:57,310 --> 00:14:00,368 
it states that the desired
item should not be Coors. 
179 
00:14:00,368 --> 00:14:07,630 
The first clause is an or, that is,
a $or, between two sub-conditions, 
180 
00:14:07,630 --> 00:14:13,622 
A the prices either 3.99, or B it is 4.99. 
181 
00:14:13,622 --> 00:14:18,154 
The second clause is also an or
of two sub conditions, 
182 
00:14:18,154 --> 00:14:23,101 
A the rating is good, and
B the quantity is less than 20. 
183 
00:14:24,474 --> 00:14:27,019 
This query shows that the $and and 
184 
00:14:27,019 --> 00:14:32,170 
the $or operators need a list
that is an array of arguments. 
185 
00:14:32,170 --> 00:14:34,343 
To draw a quick comparison, 
186 
00:14:34,343 --> 00:14:38,609 
here's the example of the same
query imposed with SQL. 
187 
00:14:43,250 --> 00:14:47,612 
Now, an important feature of
semi-structured data is that 
188 
00:14:47,612 --> 00:14:49,070 
it allows nesting. 
189 
00:14:50,120 --> 00:14:54,750 
We showed three documents here,
where there is an area named points, 
190 
00:14:54,750 --> 00:14:58,620 
which in turn has two tuples with
the elements points and bonus. 
191 
00:14:58,620 --> 00:15:04,060 
Let's assume that these three
documents are part of a collection, so 
192 
00:15:04,060 --> 00:15:07,630 
they form three items in
an area called users. 
193 
00:15:08,800 --> 00:15:13,060 
Our goal is to show how we can
write queries to extract data 
194 
00:15:13,060 --> 00:15:15,350 
from these documents with nesting. 
195 
00:15:16,620 --> 00:15:19,300 
The first query wants
to find documents for 
196 
00:15:19,300 --> 00:15:23,860 
which, the value of points should
be less than or equal to 80. 
197 
00:15:23,860 --> 00:15:26,052 
Now, which ones? 
198 
00:15:26,052 --> 00:15:29,120 
Now, points.0, 
199 
00:15:29,120 --> 00:15:34,016 
refers to the first tuple
under the outer points, and 
200 
00:15:34,016 --> 00:15:39,800 
points.0.points, refers to
the first element of that tuple. 
201 
00:15:39,800 --> 00:15:42,980 
Clearly, only the second
documents satisfies this query. 
202 
00:15:45,280 --> 00:15:48,150 
Now, what happens if we have
the same query but we drop the zero? 
203 
00:15:49,410 --> 00:15:54,260 
Now, we are looking for points.points
without specifying the array index. 
204 
00:15:54,260 --> 00:15:56,830 
This means that the points element in 
205 
00:15:56,830 --> 00:16:00,050 
any of the tuples should
have a value of 80 or less. 
206 
00:16:01,170 --> 00:16:05,403 
So now, the first and the second
document will satisfy the query. 
207 
00:16:07,900 --> 00:16:11,340 
We can put multiple conditions
as seen in the third query. 
208 
00:16:12,490 --> 00:16:16,740 
It looks for a document where the points
element of a tuple, should be utmost 81, 
209 
00:16:16,740 --> 00:16:23,440 
and the bonus should be exactly 20, and
clearly the second document qualifies. 
210 
00:16:23,440 --> 00:16:25,500 
But does the third document qualify? 
211 
00:16:26,930 --> 00:16:32,640 
In this case, the first tuple
satisfies points greater than 81 and 
212 
00:16:32,640 --> 00:16:34,870 
the second tuple satisfies
bonus equal to 20. 
213 
00:16:34,870 --> 00:16:41,190 
The answer is no, because the comma
is treated as an implicit and 
214 
00:16:41,190 --> 00:16:45,280 
condition within the same double,
as shown in the yellow braces. 
215 
00:16:47,320 --> 00:16:50,770 
Now remember that we said in course
two that all semi-structured 
216 
00:16:50,770 --> 00:16:52,070 
data can be viewed as a tree. 
217 
00:16:53,490 --> 00:16:57,730 
Now, what if I pick a node of a tree and
ask for all the descendents of that node? 
218 
00:16:58,880 --> 00:17:02,660 
That would require the system
to recursively get chideNodes, 
219 
00:17:02,660 --> 00:17:04,510 
over increasing depth from the given node. 
220 
00:17:05,820 --> 00:17:11,141 
Unfortunately, at this time,
MongoDB does not support recursive search. 
1 
00:00:02,463 --> 00:00:07,381 
A different kind of scaling problem arises
when we try to answer queries over a large 
2 
00:00:07,381 --> 00:00:12,158 
number of data sources, but before we do
that let's see how a query is answered in 
3 
00:00:12,158 --> 00:00:14,570 
the virtual data integration setting. 
4 
00:00:15,990 --> 00:00:18,819 
We are going to use a toy
scenario in a medical setting 
5 
00:00:19,890 --> 00:00:22,070 
simple as we have four data sources. 
6 
00:00:22,070 --> 00:00:24,320 
Each with one table for
the sake of simplicity. 
7 
00:00:25,570 --> 00:00:30,760 
Notice that two sources, S1 and
S3, have the same schema. 
8 
00:00:30,760 --> 00:00:34,380 
Now this is entirely possible because
sources may be independent of each other. 
9 
00:00:35,750 --> 00:00:39,310 
Further, there is no guarantee that
they would have the same exact content. 
10 
00:00:40,390 --> 00:00:44,400 
Maybe these two sources represent
clinics at different locations. 
11 
00:00:46,260 --> 00:00:48,860 
So next, we look at the target schema. 
12 
00:00:50,020 --> 00:00:54,630 
For simplicity, let's consider that
it's not an algorithmically creative 
13 
00:00:54,630 --> 00:00:59,090 
probabilistic mediator schema but just a
manually designed schema with five tables. 
14 
00:01:00,670 --> 00:01:05,620 
But while we assume that
the target schema is fixed 
15 
00:01:05,620 --> 00:01:08,710 
we want the possibility that
we can add more sources. 
16 
00:01:08,710 --> 00:01:11,280 
That means more clinics
as the system grows. 
17 
00:01:13,970 --> 00:01:16,210 
Now I'm beginning to
add the schema mapping. 
18 
00:01:17,370 --> 00:01:21,430 
Now there are several techniques
of specifying schema mappings. 
19 
00:01:21,430 --> 00:01:24,090 
One of them is called Local-as-View. 
20 
00:01:24,090 --> 00:01:29,830 
This means we write the relations in each
source as a view over the target schema. 
21 
00:01:31,590 --> 00:01:35,398 
But this way of writing the query,
as we can see here, may seem odd to you. 
22 
00:01:35,398 --> 00:01:39,280 
It's called syntax, but
you don't need to know it. 
23 
00:01:39,280 --> 00:01:45,300 
Just as an example, the first few things
the treats relation in S1 maps to, 
24 
00:01:45,300 --> 00:01:47,220 
so you see that arrow, that means maps to. 
25 
00:01:48,790 --> 00:01:53,030 
So it maps to the query select doctor, 
26 
00:01:53,030 --> 00:01:58,890 
chronic disease from treats patient,
has chronic disease. 
27 
00:01:58,890 --> 00:02:04,550 
Where treatspatient.patient Is equal
to has chronic disease dot patient. 
28 
00:02:07,050 --> 00:02:08,520 
We see the query here in the yellow box. 
29 
00:02:09,760 --> 00:02:14,200 
The only thing we should notice here
is that the select class on the query 
30 
00:02:14,200 --> 00:02:15,880 
has two attributes doctor and 
31 
00:02:15,880 --> 00:02:21,040 
chronic disease which are exactly the same
attributes of the treats relation in S1. 
32 
00:02:23,360 --> 00:02:25,820 
Now let's ask a query that
gives the target schema. 
33 
00:02:26,820 --> 00:02:30,620 
Which doctors are responsible for
discharging patients? 
34 
00:02:30,620 --> 00:02:32,777 
Which translates to
the SQL query shown here. 
35 
00:02:34,491 --> 00:02:39,344 
Now, the problem is how to translate
this query to a query that can 
36 
00:02:39,344 --> 00:02:41,120 
be sent to the sources. 
37 
00:02:42,160 --> 00:02:47,985 
Now ideally this should be simplest query
with no extra operations as shown here. 
38 
00:02:47,985 --> 00:02:55,020 
S3 treats means treats
relation in sources 3. 
39 
00:02:55,020 --> 00:02:57,500 
Now you can see the ideal answer. 
40 
00:02:58,600 --> 00:03:03,200 
To find such an optimal query
reformulation, it turns out that this 
41 
00:03:03,200 --> 00:03:08,550 
process is very complex and becomes
worse as a number of sources increases. 
42 
00:03:10,090 --> 00:03:12,840 
Thus query reformulation 
43 
00:03:12,840 --> 00:03:17,200 
becomes a significant scalability problem
in a big data integration scenario. 
44 
00:03:21,000 --> 00:03:22,640 
Let's look at the second use case 
45 
00:03:23,860 --> 00:03:27,070 
public health is a significant
component of our healthcare system. 
46 
00:03:28,150 --> 00:03:33,630 
Public health systems monitor detect and
take action when epidemics strike. 
47 
00:03:33,630 --> 00:03:38,500 
Not so long ago we have witnessed public
health concerns due to Anthrax virus, 
48 
00:03:38,500 --> 00:03:40,240 
the swine flu and the bird flu. 
49 
00:03:41,510 --> 00:03:46,180 
But these epidemics caused a group
called WADDS to develop a system for 
50 
00:03:46,180 --> 00:03:47,070 
disease surveillance. 
51 
00:03:48,230 --> 00:03:52,670 
This system would connect all local
hospitals in the Washington, DC area, and 
52 
00:03:52,670 --> 00:03:55,290 
is designed to exchange
disease information. 
53 
00:03:55,290 --> 00:04:00,030 
For example, if a hospital lab has
identified a new strain of a virus, 
54 
00:04:00,030 --> 00:04:02,630 
other hospitals and the Centers for
Disease Control, CDC, 
55 
00:04:02,630 --> 00:04:05,260 
in the network,
should be able to know about it. 
56 
00:04:07,300 --> 00:04:12,020 
It should be clear that this needs a data
integration solution where the data 
57 
00:04:12,020 --> 00:04:17,490 
sources would be the labs, the data would
be the lab tests medical records and 
58 
00:04:17,490 --> 00:04:21,170 
even genetic profiles of the virus and
the subjects who might be infected. 
59 
00:04:22,180 --> 00:04:26,630 
The table here shows the different
components with this architecture. 
60 
00:04:26,630 --> 00:04:29,590 
We will just digest the necessary
parts for our requirement. 
61 
00:04:30,620 --> 00:04:35,488 
Just know that RIM which stands for
Reference Information Model is global 
62 
00:04:35,488 --> 00:04:40,370 
schema that this industry has developed
and expects to use as a standard. 
63 
00:04:43,820 --> 00:04:47,330 
Why we want to exchange and combine new
information from different hospitals? 
64 
00:04:48,430 --> 00:04:50,990 
Every hospital is independent and 
65 
00:04:50,990 --> 00:04:54,190 
can implement their own information
system any way they see fit. 
66 
00:04:55,450 --> 00:04:59,080 
Therefore even when there
are standards like HL-7 that 
67 
00:04:59,080 --> 00:05:03,230 
specify what kind of data a held
cache system should have an exchange. 
68 
00:05:03,230 --> 00:05:08,520 
There are considerable variations in
the implementation of the standard itself. 
69 
00:05:08,520 --> 00:05:12,660 
For example the two wide boxes show
a difference in representation 
70 
00:05:12,660 --> 00:05:17,620 
of the same kind of data, this should
remind you of the data variety problem. 
71 
00:05:18,800 --> 00:05:23,810 
Let's say, we have a patient
with ID 19590520 whose lab 
72 
00:05:25,280 --> 00:05:29,020 
reports containing her plasma protein
measurements are required for 
73 
00:05:29,020 --> 00:05:30,440 
analyzing her health condition. 
74 
00:05:31,590 --> 00:05:35,550 
The problem is that the patient
went to three different clinics and 
75 
00:05:35,550 --> 00:05:38,990 
four different labs which all
implement the standards differently. 
76 
00:05:40,290 --> 00:05:41,540 
On top of it? 
77 
00:05:41,540 --> 00:05:45,420 
Each clinic uses its own
electronic medical record system 
78 
00:05:45,420 --> 00:05:47,230 
which we have a very large amount of data. 
79 
00:05:48,510 --> 00:05:53,300 
So the data integration system's job is to
transform the data from the source schema 
80 
00:05:53,300 --> 00:05:58,150 
to the schema of the receiving
system in this case the rim system. 
81 
00:05:59,410 --> 00:06:02,730 
This is sometimes called
the data exchange problem. 
82 
00:06:05,380 --> 00:06:09,140 
Informally a data exchange
problem can be defined like this. 
83 
00:06:10,780 --> 00:06:14,300 
Suppose we have a given database
whose relations are known. 
84 
00:06:15,750 --> 00:06:18,920 
Let us say we also know
the target database's schema and 
85 
00:06:18,920 --> 00:06:20,700 
the constraints the schema will satisfy. 
86 
00:06:22,330 --> 00:06:26,780 
Further we know the desired schema
mappings between the source and 
87 
00:06:26,780 --> 00:06:28,300 
this target schema. 
88 
00:06:28,300 --> 00:06:34,040 
What we do not know is how to populate
the tuples in the target database. 
89 
00:06:34,040 --> 00:06:38,493 
From the tuples in the socialization in
such a way that both schema mappings and 
90 
00:06:38,493 --> 00:06:41,540 
target constraints
are simultaneously satisfied. 
91 
00:06:44,729 --> 00:06:48,854 
In many domains like healthcare,
a significant amount of effort has been 
92 
00:06:48,854 --> 00:06:52,260 
spend by the industry in
standardizing schemas and values. 
93 
00:06:53,440 --> 00:06:57,320 
For example LOINC is a standard for
medical lab observations. 
94 
00:06:58,720 --> 00:07:01,320 
Here item like systolic blood pressure or 
95 
00:07:01,320 --> 00:07:05,580 
gene mutation are encoded in this
specific way as given by this standard. 
96 
00:07:06,630 --> 00:07:11,060 
So, if we want to write that
the systolic/diastolic pressure of 
97 
00:07:11,060 --> 00:07:17,190 
a individual is 132 by 90, we'll not write
out the string systolic blood pressure, 
98 
00:07:17,190 --> 00:07:19,920 
but use the code for it. 
99 
00:07:19,920 --> 00:07:24,070 
The ability to use standard code
is not unique to healthcare data. 
100 
00:07:24,070 --> 00:07:26,919 
The 50 states of the US all
have two letter abbreviations. 
101 
00:07:28,210 --> 00:07:34,210 
Generalizing therefore, whenever we have
data such as the domain is finite and have 
102 
00:07:34,210 --> 00:07:39,910 
a standard set of code available, we give
a new opportunity of handling big deal. 
103 
00:07:41,610 --> 00:07:45,260 
Mainly, reducing the data
size through compression. 
104 
00:07:47,110 --> 00:07:49,700 
The compression refers to a way of 
105 
00:07:49,700 --> 00:07:52,920 
creating an encoded
representation of data. 
106 
00:07:52,920 --> 00:07:56,980 
So that this encoder form is smaller
than the original representation. 
107 
00:07:58,970 --> 00:08:02,420 
A common encoding method is
called dictionary encoding. 
108 
00:08:02,420 --> 00:08:05,980 
Consider a database with 10 million
record of patient visits a lab. 
109 
00:08:07,250 --> 00:08:11,140 
Each record indicates a test and
its results. 
110 
00:08:11,140 --> 00:08:14,140 
Now we show it this way like
in a columnar structure 
111 
00:08:15,220 --> 00:08:20,040 
to make the point that the data is kept
in a column stored relational database 
112 
00:08:20,040 --> 00:08:22,430 
rather than a row store
relational database. 
113 
00:08:23,650 --> 00:08:26,380 
Now consider the column for test code. 
114 
00:08:26,380 --> 00:08:29,470 
Where the type of test is codified
according to the standard. 
115 
00:08:31,470 --> 00:08:35,340 
We replace a string representation
of the standard by a number. 
116 
00:08:37,750 --> 00:08:40,540 
The mapping between
the original test code and 
117 
00:08:40,540 --> 00:08:43,870 
the encoded number are also
stored separately. 
118 
00:08:43,870 --> 00:08:45,810 
Now suppose there
are a total of 500 tests. 
119 
00:08:47,240 --> 00:08:52,780 
So this separate table called
the dictionary here has 500 rows, 
120 
00:08:52,780 --> 00:08:57,500 
which is clearly much smaller
than ten million right? 
121 
00:08:57,500 --> 00:09:02,300 
Now 500 distinct values can be
represented by encoding them in 9 bits, 
122 
00:09:02,300 --> 00:09:04,114 
because 2 to the power of 9 is 512. 
123 
00:09:06,060 --> 00:09:09,750 
Other encoding techniques would be applied
to attributes like date and patient ID. 
124 
00:09:10,920 --> 00:09:15,890 
That's full large data we cannot reduce
the number of total actual rules. 
125 
00:09:15,890 --> 00:09:18,090 
So we have to store all ten million rules. 
126 
00:09:18,090 --> 00:09:23,893 
But we can reduce the amount of space
required by storing data in a column 
127 
00:09:23,893 --> 00:09:28,925 
oriented data store and
by using compression, indeed modern 
128 
00:09:28,925 --> 00:09:35,621 
systems use credit processing algorithms
to operate directly on compress data. 
129 
00:09:39,025 --> 00:09:42,636 
Data compression is an important
technology for big data. 
130 
00:09:45,555 --> 00:09:50,560 
And just like is a set of qualified
terms for lab tests, clinical data 
131 
00:09:50,560 --> 00:09:55,500 
also uses SNOMED which stands for
systematized nomenclature of medicine. 
132 
00:09:56,770 --> 00:09:59,780 
SNOMED is a little more
than just a vocabulary. 
133 
00:10:00,900 --> 00:10:02,599 
It does have a vocabulary of course. 
134 
00:10:03,620 --> 00:10:08,656 
The vocabulary is the collection
of medical terms in human and 
135 
00:10:08,656 --> 00:10:14,790 
medicine to provide codes, terms, synonyms
and definitions that cover anatomy, 
136 
00:10:14,790 --> 00:10:18,980 
diseases, findings, procedures,
micro organisms, substances etcetera. 
137 
00:10:19,990 --> 00:10:21,590 
But it also has relationships. 
138 
00:10:23,060 --> 00:10:27,277 
As you can see,
a renal cyst is related to kidney because 
139 
00:10:27,277 --> 00:10:30,615 
kidney's the finding site of a renal cyst. 
140 
00:10:32,890 --> 00:10:37,080 
If we query against an ontology,
it would look like a graph grid. 
141 
00:10:38,150 --> 00:10:38,920 
In this box, 
142 
00:10:38,920 --> 00:10:43,120 
we are asking to find all patient
findings with a benign tumor morphology. 
143 
00:10:44,350 --> 00:10:49,100 
In terms of querying, we are looking for
edges of a graph where one noticed 
144 
00:10:49,100 --> 00:10:54,470 
the concept that we need to find which is
connected to a node called benign neoplasm 
145 
00:10:54,470 --> 00:10:59,050 
that is benign tumor through an edge
called associated morphology 
146 
00:11:00,070 --> 00:11:04,700 
that applying this query against
the data here produces all benign tumors 
147 
00:11:04,700 --> 00:11:08,970 
of specific organs as you can
see by the orange rounded group. 
148 
00:11:11,170 --> 00:11:13,410 
But now that we have these terms, 
149 
00:11:13,410 --> 00:11:17,760 
we can use these terms to search
outpatient records with these terms would 
150 
00:11:17,760 --> 00:11:22,910 
have been used so
what's the essence of this used case. 
151 
00:11:24,030 --> 00:11:29,170 
This is can shows that an in division
system in a public health domain and 
152 
00:11:29,170 --> 00:11:32,320 
in many other domains must
be able to handle variety. 
153 
00:11:33,910 --> 00:11:40,520 
In this case there's a Global Schema
called RiM shown here all queries and 
154 
00:11:40,520 --> 00:11:45,000 
analyses performed by a data analysts
should be against this global schema. 
155 
00:11:46,230 --> 00:11:50,870 
However, the actual data which is
generated by different medical facilities 
156 
00:11:50,870 --> 00:11:55,970 
would need to be transformed
into data in this schema. 
157 
00:11:55,970 --> 00:12:00,220 
This would require not only
format conversions, but 
158 
00:12:00,220 --> 00:12:05,630 
it would need to respect all constraints
imposed by the source and by the target. 
159 
00:12:05,630 --> 00:12:10,520 
For example, a source may not distinguish
between an emergency surgical procedure 
160 
00:12:10,520 --> 00:12:12,430 
and a regular surgical procedure. 
161 
00:12:12,430 --> 00:12:16,429 
But the target may want to
put them in different tables. 
162 
00:12:18,850 --> 00:12:20,850 
We also saw that
the integration system for 
163 
00:12:20,850 --> 00:12:24,920 
this used case would need to use
quantified data but this gives 
164 
00:12:24,920 --> 00:12:28,970 
us the opportunity to use data compression
to gauge story and query efficiency. 
165 
00:12:30,120 --> 00:12:35,352 
In terms of variety, we saw how
relational data like patient records XML 
166 
00:12:35,352 --> 00:12:40,603 
data like HL7 events, and
graph data like ontologies, are co-used. 
167 
00:12:42,866 --> 00:12:47,407 
To support this, the integration
system must be able to do both model 
168 
00:12:47,407 --> 00:12:50,400 
transformation and query transformation. 
169 
00:12:51,560 --> 00:12:55,260 
Query transformation is the process of
taking a query on the target schema. 
170 
00:12:56,490 --> 00:13:00,540 
Converting it to a query
against a different data model. 
171 
00:13:00,540 --> 00:13:06,100 
For example, part of an SQL query against
the RIM may need to go to snowmad and 
172 
00:13:06,100 --> 00:13:09,240 
hence when you to be converted to
a graph query in the snowmad system. 
173 
00:13:10,590 --> 00:13:13,933 
Model transformation is
a process of taking data 
174 
00:13:13,933 --> 00:13:17,274 
represented in one model
in one source system and 
175 
00:13:17,274 --> 00:13:22,263 
converting it to an equivalent data
in another model the target system. 
1 
00:00:00,820 --> 00:00:02,600 
Hello, my name is Victor Lou. 
2 
00:00:02,600 --> 00:00:07,090 
I am the Solutions Engineer of
Global Philanthropy of Daameer.org. 
3 
00:00:07,090 --> 00:00:08,440 
Helping researchers, academia, 
4 
00:00:08,440 --> 00:00:11,750 
and non-profits find the insights
that matter using data. 
5 
00:00:11,750 --> 00:00:15,280 
Today I am here to give you
a production environment, 
6 
00:00:15,280 --> 00:00:17,410 
personalized music recommendation project. 
7 
00:00:17,410 --> 00:00:21,150 
That was done for PonoMusic and
so what is Pono Music? 
8 
00:00:23,120 --> 00:00:26,355 
Pono Music is revolutionizing music
listening by bringing back native, 
9 
00:00:26,355 --> 00:00:28,290 
high-resolution audio. 
10 
00:00:28,290 --> 00:00:31,550 
It's the way that artists intended and
recorded their music. 
11 
00:00:31,550 --> 00:00:36,710 
Neil Young, the iconic artist and
musician is also the founder and CEO. 
12 
00:00:36,710 --> 00:00:40,020 
Pono Music is the only complete
high-resolution music ecosystem. 
13 
00:00:40,020 --> 00:00:43,950 
That includes the Pono player, the Pono
Music store, the Pono Music community. 
14 
00:00:45,200 --> 00:00:48,030 
They have been super
generous in allowing us to 
15 
00:00:48,030 --> 00:00:51,020 
take a look behind the scenes as how
we built out the recommendations. 
16 
00:00:53,630 --> 00:00:56,540 
So, David Meer supports their mission
by providing a music recommendation 
17 
00:00:56,540 --> 00:01:00,900 
engine that is both scalable and
flexible as Pono grows their user base. 
18 
00:01:00,900 --> 00:01:05,220 
Let's begin by visiting Pono
music's website as you can see, 
19 
00:01:05,220 --> 00:01:07,740 
they have a few shelves that focus on 
20 
00:01:07,740 --> 00:01:11,320 
various recommendations that are not
tailored to each individual user. 
21 
00:01:11,320 --> 00:01:14,670 
If you login,
then an additional shelf would appear 
22 
00:01:14,670 --> 00:01:17,690 
that would deliver the recommendations
that are created in data gear. 
23 
00:01:17,690 --> 00:01:19,029 
So how does DataMirror accomplish this. 
24 
00:01:19,029 --> 00:01:24,279 
In a nutshell, DataMirror is a end to end,
antalis platform which contains ingestion, 
25 
00:01:24,279 --> 00:01:29,543 
preparation analysis, visualization and
operationalization all the same platform. 
26 
00:01:29,543 --> 00:01:32,300 
And having all the capabilities
in one platform is superior to 
27 
00:01:32,300 --> 00:01:34,240 
traditional technology stack. 
28 
00:01:34,240 --> 00:01:39,300 
Integration between disparate technologies
brings a lot of unnecessary challenges. 
29 
00:01:39,300 --> 00:01:41,880 
We take advantage of open
source big data technologies. 
30 
00:01:41,880 --> 00:01:46,261 
Specifically, we run natively and
Hadoop and Spark for our back end. 
31 
00:01:46,261 --> 00:01:49,600 
And we leverage D3JS on the front end for
visualizations. 
32 
00:01:49,600 --> 00:01:51,821 
Our Excel-like interface makes it easy for 
33 
00:01:51,821 --> 00:01:55,960 
folks who don't know how to code to
also get in with the data modeling. 
34 
00:01:55,960 --> 00:01:57,720 
It is very much self-service. 
35 
00:01:57,720 --> 00:02:00,001 
In the case of Pono Music's
data mirror deployment, 
36 
00:02:00,001 --> 00:02:02,790 
we have deployed a Hadoop
on Amazon on web services. 
37 
00:02:02,790 --> 00:02:04,410 
But we can also be deployed off premise if 
38 
00:02:04,410 --> 00:02:06,290 
needed it doesn't have
to be in the clouds. 
39 
00:02:06,290 --> 00:02:09,820 
In fact, we could work with many many
different distributions of Hadoop. 
40 
00:02:09,820 --> 00:02:12,320 
We can access dean rear through
any modern web browser. 
41 
00:02:12,320 --> 00:02:16,300 
As you can see here we're using Google
Chrome but it works on any browser, right? 
42 
00:02:16,300 --> 00:02:18,460 
So lets go ahead and take a look at
the a; let's log into data grip, 
43 
00:02:18,460 --> 00:02:21,640 
so we're going to go ahead and
log in here.. 
44 
00:02:23,390 --> 00:02:28,250 
So what you're seeing here is the browser
tab and we can see various artifacts 
45 
00:02:28,250 --> 00:02:32,364 
which include connections such
as connections to the pull nose, 
46 
00:02:32,364 --> 00:02:36,434 
salesforce.com instance or
a connection to AWS S3 instance. 
47 
00:02:36,434 --> 00:02:38,226 
Because DataMirror comes with so 
48 
00:02:38,226 --> 00:02:42,708 
many prebuilt connectors which include
Connections to salesforce.com and S3. 
49 
00:02:42,708 --> 00:02:46,034 
We can easily grant access to
those systems in order to pull or 
50 
00:02:46,034 --> 00:02:48,550 
push data in and out of the dev. 
51 
00:02:48,550 --> 00:02:51,760 
For example, let's take a quick look at
the salesforce.com configuration here. 
52 
00:02:51,760 --> 00:02:59,290 
As you can see, we've already configured
salesforce as the connector type. 
53 
00:02:59,290 --> 00:03:00,530 
When we hit next here, 
54 
00:03:00,530 --> 00:03:04,370 
we'll we that you can authorize
a data manager to retrieve data. 
55 
00:03:04,370 --> 00:03:06,380 
We're not going to do it because
this is a production environment, so 
56 
00:03:06,380 --> 00:03:09,300 
I don't want to disrupt the pools. 
57 
00:03:09,300 --> 00:03:13,210 
But if we were to click it, it'll give
us a salesfloor.com blogging stream. 
58 
00:03:13,210 --> 00:03:17,830 
And so as soon as we login, the Datameer
would have access via the O-Off token. 
59 
00:03:17,830 --> 00:03:20,130 
And so this is valid for a period of time. 
60 
00:03:20,130 --> 00:03:20,650 
I'm going to go ahead and 
61 
00:03:20,650 --> 00:03:23,910 
hit the cancel button as this
is a production environment. 
62 
00:03:23,910 --> 00:03:28,290 
And so now we're ready to look
at an import job artifact. 
63 
00:03:28,290 --> 00:03:31,888 
So this is the sales force import job and 
64 
00:03:31,888 --> 00:03:36,152 
as you can see we go in here and
we configure it. 
65 
00:03:36,152 --> 00:03:41,830 
We can see that it's connected to
the Pono Music SMDC connection. 
66 
00:03:44,753 --> 00:03:49,269 
I'm going to go ahead and
hit next here.with salesforce.com, 
67 
00:03:49,269 --> 00:03:52,079 
we just have to simply define the SOQL, 
68 
00:03:52,079 --> 00:03:56,526 
which is basically a querying
language that's based on SQL. 
69 
00:03:56,526 --> 00:03:59,860 
To locate data contained within
the salesforce.com objects. 
70 
00:03:59,860 --> 00:04:04,997 
You can see the select statement here,
you can also see the familiar 
71 
00:04:04,997 --> 00:04:10,059 
from statement, and you can also
see the where statement as well. 
72 
00:04:10,059 --> 00:04:12,459 
So in order to protect
the privacy of the users, 
73 
00:04:12,459 --> 00:04:16,830 
Datameer has the capability to obfuscate
sort of columns that are sensitive. 
74 
00:04:16,830 --> 00:04:19,770 
And so in other words, we can scramble
sensitive fields such as email or 
75 
00:04:19,770 --> 00:04:21,370 
physical addresses. 
76 
00:04:21,370 --> 00:04:25,228 
And at this point in time, I'm going to
go ahead and cancel out, and I'm going to 
77 
00:04:25,228 --> 00:04:28,866 
show the next part of this demo in
an obfuscated workbook environment. 
78 
00:04:28,866 --> 00:04:33,813 
The artifacts contained in the obfuscated
folders are duplicates of the import job 
79 
00:04:33,813 --> 00:04:38,130 
of the workbook from their production
counterparts as you can see here. 
80 
00:04:40,360 --> 00:04:44,510 
So as you can see in the import
job settings we have configured 
81 
00:04:44,510 --> 00:04:51,320 
the same connections here but
for the obfuscated columns here. 
82 
00:04:51,320 --> 00:04:54,970 
Now we're obfuscating the owner
email as well as the street address. 
83 
00:04:54,970 --> 00:04:55,690 
Now we hit Next here. 
84 
00:04:59,259 --> 00:05:03,140 
And so you can see, various fields are
being pulled here from Salesforce, right. 
85 
00:05:03,140 --> 00:05:05,130 
And in particular,
I want to highlight the. 
86 
00:05:06,150 --> 00:05:09,780 
The owner email, that field, so as you
can see these are the obfuscated fields. 
87 
00:05:09,780 --> 00:05:13,420 
Similarly, the actual street address,
that's also masked as well. 
88 
00:05:13,420 --> 00:05:17,375 
I'm going to go ahead and hit Cancel. 
89 
00:05:17,375 --> 00:05:20,780 
I will go back to, rather we're going to
go check out the workbook environment, 
90 
00:05:20,780 --> 00:05:23,300 
which is where we do the preparation and
analysis. 
91 
00:05:28,465 --> 00:05:31,070 
To the obfuscated workbook environment. 
92 
00:05:31,070 --> 00:05:34,260 
As you can see we're starting
with a raw source of data here. 
93 
00:05:34,260 --> 00:05:38,930 
This was the data that was
contained in the data sheet. 
94 
00:05:38,930 --> 00:05:43,500 
Take note that there's a variety of icons
that indicate different types of sheets. 
95 
00:05:43,500 --> 00:05:45,610 
So you could see here this
is the raw data type. 
96 
00:05:45,610 --> 00:05:48,360 
This is a regular worksheet. 
97 
00:05:48,360 --> 00:05:51,400 
You have a union sheet and
you have a joint sheet and 
98 
00:05:51,400 --> 00:05:54,440 
we'll go into a little bit
more detail as we get to them. 
99 
00:05:54,440 --> 00:05:57,700 
So, as you can see,
the interface is very much like Excel. 
100 
00:05:57,700 --> 00:06:02,830 
Looking at the second sheet pairs, and
it's using a function called group by. 
101 
00:06:02,830 --> 00:06:07,380 
So the group by function is a type of
aggregation function in Datameer speak. 
102 
00:06:07,380 --> 00:06:10,640 
As you can see, you can build
the functions by pointing and clicking. 
103 
00:06:10,640 --> 00:06:14,040 
But you can also enter the formula
to create nested functions or 
104 
00:06:14,040 --> 00:06:19,310 
Apply arithmetic
operations just as a cell. 
105 
00:06:19,310 --> 00:06:21,770 
We take all the unique combinations
of albums purchased for 
106 
00:06:21,770 --> 00:06:24,730 
each of the unique email
using a group pair function. 
107 
00:06:25,890 --> 00:06:28,710 
Then we pull out each of the elements
of the pairings using a list element 
108 
00:06:28,710 --> 00:06:30,310 
function, so let's see here. 
109 
00:06:31,360 --> 00:06:34,230 
If you were to build a function
from scratch, you can see this. 
110 
00:06:34,230 --> 00:06:39,950 
So then you create a group by function,
and then direct it at a column. 
111 
00:06:39,950 --> 00:06:41,120 
You basically pass it to argument. 
112 
00:06:43,890 --> 00:06:47,856 
To find out the occurrence
of each pairing of albums, 
113 
00:06:47,856 --> 00:06:52,188 
we're going to go ahead and
look at this LR sheet. 
114 
00:06:52,188 --> 00:06:55,477 
And so these are, we're using group
by functions as well here, so 
115 
00:06:55,477 --> 00:06:58,372 
we're grouping by the first item one,
and then item two. 
116 
00:06:58,372 --> 00:07:00,088 
And then, we're doing a group count, 
117 
00:07:00,088 --> 00:07:03,430 
which is counting the frequency
of occurrences of those cells. 
118 
00:07:03,430 --> 00:07:06,350 
Similarly, we're going to
do a similar thing but 
119 
00:07:06,350 --> 00:07:11,096 
with the item two first now, and then
item one, and then doing a group count. 
120 
00:07:11,096 --> 00:07:13,780 
And the reason is,
we don't care about the order. 
121 
00:07:13,780 --> 00:07:17,810 
And so, we repeat this on the next sheet
reversing the order of the albums to make 
122 
00:07:17,810 --> 00:07:21,460 
sure that we capture all the combinations
since the order doesn't matter to us. 
123 
00:07:24,260 --> 00:07:28,887 
And then we use our unit sheet function
to append the second frequency 
124 
00:07:28,887 --> 00:07:34,140 
counting sheet to the first frequency
counting sheet, as you can see here. 
125 
00:07:34,140 --> 00:07:36,962 
It's drag and drop, of course, and
so we've connected them together. 
126 
00:07:36,962 --> 00:07:39,248 
You can do multi-sheets but
in this case we only have two, right. 
127 
00:07:39,248 --> 00:07:42,260 
So it's, okay. 
128 
00:07:42,260 --> 00:07:47,770 
We ignore the co-occurrent set sheet as it
is currently not used in the final output. 
129 
00:07:47,770 --> 00:07:51,010 
We were kind of experimenting
with other recommendation models. 
130 
00:07:51,010 --> 00:07:52,660 
We're rebuilding those workbooks. 
131 
00:07:52,660 --> 00:07:56,780 
So one of the really cool features for
we can execute the workbook, but 
132 
00:07:56,780 --> 00:07:59,162 
deselect the storage of
intermediary sheets. 
133 
00:07:59,162 --> 00:08:02,958 
This means that we only need
relevant functions are executed 
134 
00:08:02,958 --> 00:08:07,740 
at a to the data lineage of columns that
are contained in the selector sheet. 
135 
00:08:07,740 --> 00:08:12,620 
Moving on, we have to create
a separate sheet from the raw data, 
136 
00:08:12,620 --> 00:08:16,770 
that counts the number of times an album
occurs across all of the purchases. 
137 
00:08:16,770 --> 00:08:20,780 
So you can see here, which students will
buy an album and then we do a count. 
138 
00:08:24,890 --> 00:08:29,768 
We bring the code occurrences and
the frequency together using the join 
139 
00:08:29,768 --> 00:08:33,599 
feature as you can see,
it is a drag and drop interface. 
140 
00:08:33,599 --> 00:08:36,601 
You got the obfuscated workbook
each of the worksheets here and 
141 
00:08:36,601 --> 00:08:39,100 
within each worksheets,
you got various columns. 
142 
00:08:39,100 --> 00:08:45,484 
You can bring those, you can drag and
drop them into here to do the joins. 
143 
00:08:45,484 --> 00:08:50,730 
You can do different types of joins,
you got the inner, outer, etc. 
144 
00:08:50,730 --> 00:08:53,206 
You can also do multi column joins. 
145 
00:08:53,206 --> 00:08:54,491 
You can do multi sheet joins. 
146 
00:08:54,491 --> 00:08:59,168 
You can also select which columns
you want to keep during the join. 
147 
00:08:59,168 --> 00:09:02,705 
Go ahead and cancel out here. 
148 
00:09:02,705 --> 00:09:06,580 
Datameer does not allow us to add
additional functions to a joint sheet so 
149 
00:09:06,580 --> 00:09:09,269 
we duplicate it by right clicking and
then hitting the duplicate button. 
150 
00:09:11,820 --> 00:09:14,780 
And then we create the next sheet,
which contains a link to columns, so 
151 
00:09:14,780 --> 00:09:17,570 
it will be linked to the rating sheet. 
152 
00:09:17,570 --> 00:09:22,480 
This is, see the link columns back
to the joint sheets and then. 
153 
00:09:23,760 --> 00:09:28,880 
We simply add a calculation
where it's the number of times 
154 
00:09:28,880 --> 00:09:32,730 
the co-occurrence occurs divided by
the number of times that the first album 
155 
00:09:32,730 --> 00:09:35,950 
in the column appears
throughout the data set. 
156 
00:09:35,950 --> 00:09:40,284 
The idea is to give a high recommendation
for albums that appear frequently 
157 
00:09:40,284 --> 00:09:44,503 
together While simultaneously
penalizing albums that are too common. 
158 
00:09:44,503 --> 00:09:48,513 
And so at this point,
I would like to write a few more joins 
159 
00:09:48,513 --> 00:09:51,773 
that bring together the album ID,
the email, 
160 
00:09:51,773 --> 00:09:56,980 
recommended album ID based on the album
ID and email, so take a look here. 
161 
00:09:59,310 --> 00:10:03,472 
So, as you can see here and then, 
162 
00:10:03,472 --> 00:10:08,460 
before we finish,
we also do an anti joint by performing 
163 
00:10:08,460 --> 00:10:14,170 
the outer left joint with
the almighty in the email. 
164 
00:10:14,170 --> 00:10:18,830 
And then filtering, so this is,
by the way, this is a double column joint. 
165 
00:10:20,090 --> 00:10:24,060 
And then we do a filter, so
then we filter out all the, for 
166 
00:10:24,060 --> 00:10:26,840 
the obfuscated owner
fields that are empty. 
167 
00:10:26,840 --> 00:10:30,192 
We apply a filter to it. 
168 
00:10:30,192 --> 00:10:35,996 
So finally, we use a few group I
functions to do some deep duplication, 
169 
00:10:35,996 --> 00:10:38,915 
so we use group by functions again. 
170 
00:10:38,915 --> 00:10:42,637 
And then, so we make sure that the album
functions are unique for each user and 
171 
00:10:42,637 --> 00:10:44,250 
then we use a group max function. 
172 
00:10:45,780 --> 00:10:49,292 
In order to preserve the highest
rating for each of the unique 
173 
00:10:50,950 --> 00:10:55,310 
recommendations the final desired
output is on the recommenders sheet. 
174 
00:10:55,310 --> 00:11:01,710 
It leverages the group top end function,
so you can see here group top numbers. 
175 
00:11:01,710 --> 00:11:05,349 
So we only want to look at the to
20 recommendations for each user. 
176 
00:11:05,349 --> 00:11:09,759 
And in addition,
we also create a email row number field, 
177 
00:11:09,759 --> 00:11:13,720 
which is a key that needed for
salesforce.com, and 
178 
00:11:13,720 --> 00:11:18,142 
it is generated by a simple
concatenation function here. 
179 
00:11:18,142 --> 00:11:22,642 
Something that note that [SOUND] is
big on data governance, data lineage. 
180 
00:11:22,642 --> 00:11:26,632 
As you can see, we can keep track of
how all the raw data flows through 
181 
00:11:26,632 --> 00:11:29,650 
from the raw data set all
the way to the end product. 
182 
00:11:31,950 --> 00:11:36,651 
So each workbook functions can be
also exported in JSON format to keep 
183 
00:11:36,651 --> 00:11:38,605 
track of revision history. 
184 
00:11:38,605 --> 00:11:41,470 
We're not ready to operationalize
things and so how do we do it? 
185 
00:11:41,470 --> 00:11:45,700 
Get our workbook configurations. 
186 
00:11:45,700 --> 00:11:46,860 
Go ahead and exit out of the workbook. 
187 
00:11:49,420 --> 00:11:50,710 
Let's go back to
the production environment. 
188 
00:11:56,340 --> 00:11:57,128 
As you can see, 
189 
00:11:57,128 --> 00:12:01,670 
the workbook recalculation retriggers
when the import job is completed. 
190 
00:12:01,670 --> 00:12:03,820 
We also only retain the latest results, so 
191 
00:12:03,820 --> 00:12:09,440 
we select purge historical results,
as you can see here. 
192 
00:12:09,440 --> 00:12:14,970 
As I mentioned earlier, a data mirror
makes it easy to operationalize a workbook 
193 
00:12:14,970 --> 00:12:17,630 
because we only select
the final output sheets. 
194 
00:12:17,630 --> 00:12:21,000 
And, so you see everything's unchecked
except for the recommended sheet. 
195 
00:12:22,730 --> 00:12:26,420 
And Datameer basically will automatically
determine what is a critical path 
196 
00:12:26,420 --> 00:12:28,990 
of the intermediary sheets to
produce the desired output sheet. 
197 
00:12:30,430 --> 00:12:33,031 
Once the recommendation
workbook is calculated, 
198 
00:12:33,031 --> 00:12:35,029 
then this triggers the export job run. 
199 
00:12:35,029 --> 00:12:38,032 
I'm going to go ahead and
move to the export job artifact. 
200 
00:12:38,032 --> 00:12:40,194 
Exit out of this. 
201 
00:12:40,194 --> 00:12:42,361 
Go ahead and save, or not save. 
202 
00:12:42,361 --> 00:12:47,223 
We'll see the export job,
let's go ahead and configure that. 
203 
00:12:52,087 --> 00:12:55,127 
Hit Next, Share. 
204 
00:12:55,127 --> 00:13:00,240 
CSV outputs, so. 
205 
00:13:02,817 --> 00:13:04,557 
This triggers the export job to run, 
206 
00:13:04,557 --> 00:13:08,600 
which is how we basically push
the results of the recommendations to S3. 
207 
00:13:08,600 --> 00:13:14,310 
We elected to use S3 because we
are already hosting at services and 
208 
00:13:14,310 --> 00:13:16,630 
S3 is an affordable storage solution. 
209 
00:13:16,630 --> 00:13:19,695 
We cannot push directly to
salesforce.com at the present because 
210 
00:13:19,695 --> 00:13:22,920 
It's a only connection. 
211 
00:13:22,920 --> 00:13:26,726 
Therefore, we need to push
data to the storage area, 
212 
00:13:26,726 --> 00:13:30,123 
which can be scheduled for using Apex. 
213 
00:13:30,123 --> 00:13:33,796 
A general challenge for
is the role limitation on each pool. 
214 
00:13:33,796 --> 00:13:38,386 
Fortunately, DataMirror comes
with the option to push 
215 
00:13:38,386 --> 00:13:41,873 
files that are broken
into one megabyte chunks. 
216 
00:13:41,873 --> 00:13:46,796 
So as you can see here in
advanced settings 1 megabyte, and 
217 
00:13:46,796 --> 00:13:51,140 
then we set a type of
consecutive numbering scheme. 
218 
00:13:51,140 --> 00:13:53,852 
And so it's dynamic naming conventions
using time stamps and numbering for 
219 
00:13:53,852 --> 00:13:55,530 
each of the chunks. 
220 
00:13:55,530 --> 00:13:58,830 
So that sums up the current
co-curds/d deployment. 
221 
00:13:58,830 --> 00:14:03,018 
As you've seen, we have easily
integrated data from Salesforce.com, 
222 
00:14:03,018 --> 00:14:07,272 
created a recommendation model, and
set up a push to S3 automatically for 
223 
00:14:07,272 --> 00:14:09,856 
easy consumption back to Salesforce.com. 
224 
00:14:09,856 --> 00:14:13,469 
Finally, we operationalized this by
triggering each of the steps sequentially 
225 
00:14:13,469 --> 00:14:16,297 
all in sitting on top of
the powerful Hadoop platform. 
226 
00:14:16,297 --> 00:14:19,070 
What's next? 
227 
00:14:19,070 --> 00:14:23,590 
Well, Pono Music is working on
implementing Google Analytics tracking for 
228 
00:14:23,590 --> 00:14:26,450 
each user at the Apple pages. 
229 
00:14:26,450 --> 00:14:29,450 
So let's go back to
the Apple Music Store here. 
230 
00:14:29,450 --> 00:14:32,620 
Just for an example, let's take a look
at the Train Led Zeppelin Two Album. 
231 
00:14:33,670 --> 00:14:36,650 
As you can see all of those album
IDs that we were looking at earlier, 
232 
00:14:36,650 --> 00:14:38,390 
these are actually embedded in the URL. 
233 
00:14:40,300 --> 00:14:44,517 
This means that in the near future, we
can actually adapt recommendations based 
234 
00:14:44,517 --> 00:14:49,053 
on the buying behavior to recommendations
based on both buying and browser behavior. 
235 
00:14:49,053 --> 00:14:53,313 
We can look at recommendations based on
genre, perhaps we can try to look at most 
236 
00:14:53,313 --> 00:14:57,526 
recent purchases or browsing behavior in
the last three to six to nine months. 
237 
00:14:57,526 --> 00:15:01,541 
Or we could use album metadata such as
album release date to give additional 
238 
00:15:01,541 --> 00:15:03,270 
recommendations. 
239 
00:15:03,270 --> 00:15:07,698 
Also, it is quite simple to just
duplicate the recommendations workbook in 
240 
00:15:07,698 --> 00:15:10,409 
[INAUDIBLE] to try any
number of these options. 
241 
00:15:10,409 --> 00:15:13,172 
And so,
that means we can do a lot of AB testing, 
242 
00:15:13,172 --> 00:15:18,600 
as we track how users react to each of
the modified recommendation algorithms. 
243 
00:15:18,600 --> 00:15:21,170 
The possibilities are literally endless. 
244 
00:15:21,170 --> 00:15:24,051 
So anyhow, thanks again for
checking out how Data Mirror 
245 
00:15:24,051 --> 00:15:27,305 
builds a simple code current
recommendation engine for music. 
246 
00:15:27,305 --> 00:15:29,636 
Please feel free to direction questions or 
247 
00:15:29,636 --> 00:15:32,492 
comments to me at
victor.liu@datamirror.com. 
248 
00:15:32,492 --> 00:15:34,220 
Thanks again. 
1 
00:00:01,380 --> 00:00:09,185 
Amarnath just finished overviewing
querying and integrational big data. 
2 
00:00:09,185 --> 00:00:13,160 
Although, fundamental understanding
of these concepts are important. 
3 
00:00:14,300 --> 00:00:17,520 
Some of these tasks can be accomplished 
4 
00:00:17,520 --> 00:00:21,650 
using graphical user interface
based tools or software products. 
5 
00:00:22,800 --> 00:00:27,870 
In this short module,
we introduce you to two of those tools. 
6 
00:00:27,870 --> 00:00:29,810 
Splunk and Datameer. 
7 
00:00:31,860 --> 00:00:37,720 
We have selected a few videos from
the site of our sponsors Splunk, 
8 
00:00:37,720 --> 00:00:41,900 
to give you an overview of using such
tools for different applications. 
9 
00:00:42,930 --> 00:00:45,940 
You will also have a short
hands-on activity on Splunk. 
10 
00:00:47,810 --> 00:00:52,210 
In addition,
we provide you a comprehensive video on 
11 
00:00:52,210 --> 00:00:56,110 
using Datamere in
the digital music industry. 
12 
00:00:56,110 --> 00:01:01,150 
Keep in mind, Splunk and
Datamere are just two of the many 
13 
00:01:01,150 --> 00:01:06,613 
tools in this category, but
they represent a huge industry. 
1 
00:00:15,850 --> 00:00:20,205 
Open XC is an open source hardware-software platform. 
2 
00:00:20,205 --> 00:00:25,230 
We wanted to see what correlations we could actually create within this model dashboards. 
3 
00:00:25,230 --> 00:00:30,915 
It plugs into your car, and it gives you all the data that you can possibly want. 
4 
00:00:30,915 --> 00:00:33,510 
What we're interested in is looking at how you 
5 
00:00:33,510 --> 00:00:36,070 
can make the car much more modular and customizable, 
6 
00:00:36,070 --> 00:00:38,365 
getting a lot of this data and afford cars 
7 
00:00:38,365 --> 00:00:40,750 
out and letting developers do whatever they want with it. 
8 
00:00:40,750 --> 00:00:44,170 
You could kind of like update technology as technology progresses. 
9 
00:00:44,170 --> 00:00:45,930 
So what we're really trying to do is like catch up with 
10 
00:00:45,930 --> 00:00:50,060 
the consumer electronics design cycle not the automotive design cycle types. 
11 
00:00:50,060 --> 00:00:52,265 
The clock hasn't started yet. 
12 
00:00:52,265 --> 00:00:56,120 
One of the tests that we did was a gas car versus an electric car. 
13 
00:00:56,120 --> 00:00:58,230 
So we're trying to see which is faster, 
14 
00:00:58,230 --> 00:00:59,965 
which is expecting the most energy, 
15 
00:00:59,965 --> 00:01:02,070 
which is the most cost efficient. 
16 
00:01:02,070 --> 00:01:05,240 
And we wanted to 
17 
00:01:05,240 --> 00:01:09,025 
measure doesn't electric car driver drive different than a gas car driver. 
18 
00:01:09,025 --> 00:01:11,345 
So actually by using the Open XC data, 
19 
00:01:11,345 --> 00:01:13,120 
and by splunking that creating links in 
20 
00:01:13,120 --> 00:01:17,190 
dashboards will give it to match ups and correlations. 
21 
00:01:23,110 --> 00:01:29,965 
It all comes down to how much are they mashing the gas. 
22 
00:01:29,965 --> 00:01:35,410 
We've been internally calling that dashboard the "Lead Foot Dashboard". 
23 
00:01:45,700 --> 00:01:53,060 
Who's the most efficient driver? 
24 
00:01:53,060 --> 00:01:56,740 
There's one of our drivers actually going low and 
25 
00:01:56,740 --> 00:02:00,085 
slow and actually using into the brakes not really gnashing on the gas, 
26 
00:02:00,085 --> 00:02:02,635 
and driving more like a normal person would. 
27 
00:02:02,635 --> 00:02:07,265 
When you drive an electric car, it's a lot quicker off the line than a gas powered car. 
28 
00:02:07,265 --> 00:02:10,215 
So, we find people kind of smashing 
29 
00:02:10,215 --> 00:02:13,975 
on the gas pedal but sort of easing up on the electric pedal. 
30 
00:02:13,975 --> 00:02:15,595 
So I've got some data for you here. 
31 
00:02:15,595 --> 00:02:17,950 
We see. What's this I'm doing? 
32 
00:02:17,950 --> 00:02:19,995 
It says you're doing. 
33 
00:02:19,995 --> 00:02:20,645 
There we go. 
34 
00:02:20,645 --> 00:02:24,570 
Battery car is high. There you go. 
35 
00:02:24,570 --> 00:02:27,230 
Hurry up. 
36 
00:02:27,230 --> 00:02:29,735 
There we go. We don't even need because Open XC, We just cut it. 
37 
00:02:29,735 --> 00:02:31,670 
These are basically all of 
38 
00:02:31,670 --> 00:02:35,160 
the sensor readings that are being broadcast from that Firefly box. 
39 
00:02:35,160 --> 00:02:38,455 
It's plugged into the diagnostics port of the car. 
40 
00:02:38,455 --> 00:02:41,350 
It's about how much power can we kind of give 
41 
00:02:41,350 --> 00:02:45,190 
away in order to incentivize a community of makers. 
42 
00:02:45,190 --> 00:02:47,645 
We want to be able to actually harness that and create 
43 
00:02:47,645 --> 00:02:50,605 
an environment where they can much more quickly, 
44 
00:02:50,605 --> 00:02:51,990 
experiment with our ideas, 
45 
00:02:51,990 --> 00:02:54,725 
enrich the platform in general, 
46 
00:02:54,725 --> 00:02:59,130 
make our vehicles in that sense more valuable. 
1 
00:00:02,180 --> 00:00:05,380 
In this hands on activity,
we will be performing queries in Splunk. 
2 
00:00:06,620 --> 00:00:09,980 
First, we will open a browser and
login to Splunk. 
3 
00:00:09,980 --> 00:00:13,590 
Next, we will import a CSV file and
search its contents. 
4 
00:00:13,590 --> 00:00:18,030 
We will see how to filter fields for
specific values and 
5 
00:00:18,030 --> 00:00:20,532 
also perform statistical
calculations on the data. 
6 
00:00:20,532 --> 00:00:23,678 
Let's begin. 
7 
00:00:23,678 --> 00:00:28,236 
First, open a web browser and
navigate to the Splunk web page. 
8 
00:00:28,236 --> 00:00:33,103 
We'll enter localhost:8000. 
9 
00:00:36,793 --> 00:00:41,870 
Next we'll log in to Splunk using
admin and the default password. 
10 
00:00:51,160 --> 00:00:55,231 
Next, we'll import a CSV file into Splunk. 
11 
00:00:55,231 --> 00:00:56,952 
We'll click on Add Data. 
12 
00:01:00,159 --> 00:01:05,704 
Upload We'll click on Select File. 
13 
00:01:08,366 --> 00:01:11,838 
And we'll choose the census.csv
file that we downloaded. 
14 
00:01:16,364 --> 00:01:17,583 
Click Next. 
15 
00:01:22,148 --> 00:01:25,010 
On the left,
it should say source type csv. 
16 
00:01:25,010 --> 00:01:30,066 
If it does not, click on the button and 
17 
00:01:30,066 --> 00:01:35,128 
go down to Structured and select csv. 
18 
00:01:35,128 --> 00:01:37,290 
In this table,
we see a preview of the data. 
19 
00:01:39,050 --> 00:01:42,100 
You should see the column names
of the CSV file at the top. 
20 
00:01:44,260 --> 00:01:45,360 
Click on Next. 
21 
00:01:48,372 --> 00:01:49,888 
Review. 
22 
00:01:51,026 --> 00:01:52,227 
And Submit. 
23 
00:01:54,602 --> 00:01:58,797 
Now that the file has been imported
successfully, click on Start Searching. 
24 
00:02:04,511 --> 00:02:09,539 
In the search box,
it fills in the default query, 
25 
00:02:09,539 --> 00:02:16,260 
source="cencus.csv" the host name and
sourcetype="csv" 
26 
00:02:18,161 --> 00:02:22,067 
We could change these fields
to search other files or 
27 
00:02:22,067 --> 00:02:25,894 
other data types if we
imported those into Splunk. 
28 
00:02:25,894 --> 00:02:31,135 
Now let's search census.csv for
particular values in the fields. 
29 
00:02:31,135 --> 00:02:34,478 
Let's search for all the data
where the state is California. 
30 
00:02:37,251 --> 00:02:40,908 
We'll enter STNAME="California". 
31 
00:02:46,621 --> 00:02:50,521 
You'll see the results down here. 
32 
00:02:50,521 --> 00:02:53,604 
You could search for
other states by using or. 
33 
00:02:53,604 --> 00:02:57,581 
For example, we can add OR
STNAME="Alaska". 
34 
00:02:57,581 --> 00:03:01,840 
This will search for
state names equal to California or Alaska. 
35 
00:03:05,450 --> 00:03:07,270 
We can add conditions
to our query as well. 
36 
00:03:07,270 --> 00:03:14,930 
Let's search for state equals California
whose population was over one million. 
37 
00:03:14,930 --> 00:03:21,467 
We'll do this by saying
STNAME = "California" 
38 
00:03:21,467 --> 00:03:26,925 
CENSUS2010POP > 1000000. 
39 
00:03:32,477 --> 00:03:35,900 
Can limit the results to one or
more columns. 
40 
00:03:35,900 --> 00:03:42,446 
You do this by adding pipe table
CTYNAME to the end of our query. 
41 
00:03:47,573 --> 00:03:52,336 
In Spunk queries, the pipe command is used
to send the outputs from the first part of 
42 
00:03:52,336 --> 00:03:53,778 
the query into the next. 
43 
00:03:56,139 --> 00:03:58,441 
You can also show more than
one column from the output. 
44 
00:03:58,441 --> 00:04:02,982 
If we add a comma CENSUS2010POP
to the end of this, 
45 
00:04:02,982 --> 00:04:07,145 
we'll see both the city name and
the population. 
46 
00:04:07,145 --> 00:04:10,944 
You can also see a visualization of this
data by clicking on the Visualization tab. 
47 
00:04:13,883 --> 00:04:18,344 
At the bottom, on the x-axis,
we see the county names, and 
48 
00:04:18,344 --> 00:04:21,420 
the y values are the population numbers. 
49 
00:04:24,300 --> 00:04:27,994 
Now let's perform some
statistics on this data. 
50 
00:04:27,994 --> 00:04:31,630 
We'll begin by counting the number of
records where the state is California. 
51 
00:04:33,570 --> 00:04:39,951 
You can do this by saying
STNAME="California" pipe stats count. 
52 
00:04:44,282 --> 00:04:49,468 
Now switch back to the statistics tab,
see the result here 
53 
00:04:52,360 --> 00:04:55,775 
Now let's see the total population for
California. 
54 
00:04:55,775 --> 00:05:01,241 
You can replace count
with sum(CENUS2010POP) 
55 
00:05:05,290 --> 00:05:07,923 
You can also calculate
the average population. 
56 
00:05:07,923 --> 00:05:09,781 
We'll replace sum with mean. 
1 
00:00:01,970 --> 00:00:06,280 
Hello, My name is Mitch Fleichmann,
senior instructor here at Splunk. 
2 
00:00:06,280 --> 00:00:08,670 
Today we are going to
install Splunk on Linux. 
3 
00:00:10,230 --> 00:00:13,760 
I'm using one of
the Splunk Education Linux servers. 
4 
00:00:13,760 --> 00:00:14,564 
Let's examine the environment. 
5 
00:00:17,146 --> 00:00:21,040 
First of all,
notice I'm logged in as the user splunker. 
6 
00:00:21,040 --> 00:00:24,600 
As a best practice,
do not install Splunk as the root user. 
7 
00:00:27,730 --> 00:00:33,498 
I'm currently in the /opt directory,
where I've already downloaded 
8 
00:00:33,498 --> 00:00:38,426 
the tarball from splunk.com/download for
this platform. 
9 
00:00:40,954 --> 00:00:45,755 
As a final check, let's check the system
to make sure we have the correct operating 
10 
00:00:45,755 --> 00:00:46,932 
system and kernel. 
11 
00:00:49,680 --> 00:00:56,072 
This is indeed a Linux machine, let's
get confirmation that it is also 64 bit. 
12 
00:01:02,580 --> 00:01:04,090 
So we are good to go. 
13 
00:01:04,090 --> 00:01:07,519 
The next step is to unzip and
untar the installer and 
14 
00:01:07,519 --> 00:01:10,729 
that we can do with the gunzip and
tar commands. 
15 
00:01:18,205 --> 00:01:23,650 
When everything untucks, you'll notice
a new sub directory created named splunk. 
16 
00:01:24,960 --> 00:01:28,670 
And we can navigate to the splunk/bin
directory to start up Splunk. 
17 
00:01:31,450 --> 00:01:36,000 
Couple of ways to start
Splunk with no switches. 
18 
00:01:37,330 --> 00:01:41,890 
On the first startup, you'll be prompted
to read and agree to the software license. 
19 
00:01:43,610 --> 00:01:47,141 
Or as a shortcut, you can start
up Splunk and accept the license. 
20 
00:01:55,707 --> 00:02:00,315 
And notice a couple of port numbers
being grabbed, port 8000 for 
21 
00:02:00,315 --> 00:02:03,902 
splunk web and port 8089 for
splunk management. 
22 
00:02:08,003 --> 00:02:09,528 
The splunk daemon has started. 
23 
00:02:12,675 --> 00:02:14,603 
Splunk is generating its own keys. 
24 
00:02:17,707 --> 00:02:21,520 
And we can see the splunk web interface,
the URL. 
25 
00:02:26,050 --> 00:02:26,950 
As a best practice, 
26 
00:02:26,950 --> 00:02:31,050 
you may also want to consider automating
splunk to start when the machine boots. 
27 
00:02:32,060 --> 00:02:37,635 
That you can do as the root user. 
28 
00:02:37,635 --> 00:02:41,841 
By issuing the splunk enable command, 
29 
00:02:41,841 --> 00:02:46,686 
enable boot-start as the -user splunker, 
30 
00:02:46,686 --> 00:02:51,793 
the same user that we
just consult splunk with. 
31 
00:03:00,498 --> 00:03:03,960 
So lets log in to splunk web and
see how the system looks. 
32 
00:03:03,960 --> 00:03:05,410 
And for that we'll go back to the browser. 
33 
00:03:07,980 --> 00:03:09,230 
Go to the appropriate URL. 
34 
00:03:13,340 --> 00:03:15,360 
And we can see upon first login, 
35 
00:03:15,360 --> 00:03:20,730 
you are prompted to login with
the credentials admin, password changeme. 
36 
00:03:28,180 --> 00:03:29,780 
Since this is the first login, 
37 
00:03:29,780 --> 00:03:33,170 
you also coach to change your
password to something more secure. 
38 
00:03:33,170 --> 00:03:36,218 
And it's highly recommended to
follow this best practice as well. 
39 
00:03:46,435 --> 00:03:48,027 
And also on first connection, 
40 
00:03:48,027 --> 00:03:51,715 
you'll also see a splash screen
showing you what's new in version 6. 
41 
00:03:53,160 --> 00:03:54,950 
In this case, Powerful Analytics. 
42 
00:03:56,190 --> 00:03:58,790 
Some changes to the UI to
make it more intuitive. 
43 
00:04:01,400 --> 00:04:04,919 
Simplified component management for
cluster management, 
44 
00:04:04,919 --> 00:04:06,647 
folder management and so on. 
45 
00:04:08,737 --> 00:04:10,651 
Also a richer developer experience. 
46 
00:04:14,147 --> 00:04:17,560 
Close down this window and
you can explore the navigation options. 
47 
00:04:18,940 --> 00:04:20,410 
And notice in the left side, 
48 
00:04:20,410 --> 00:04:23,470 
you see a panel showing you
the apps to navigate to and manage. 
49 
00:04:24,960 --> 00:04:29,060 
And on the right side, you see some
panel showing you data in the system and 
50 
00:04:29,060 --> 00:04:30,380 
various links for help. 
51 
00:04:32,730 --> 00:04:34,936 
Let's go to the search and
reporting app by clicking here. 
52 
00:04:39,527 --> 00:04:43,879 
And you can see the search far up top, 
53 
00:04:43,879 --> 00:04:49,788 
some tips on how to search and
then data to search. 
54 
00:04:49,788 --> 00:04:53,720 
Since this is a fresh install,
there is no data to search, so 
55 
00:04:53,720 --> 00:04:57,049 
the next step is to index data and
begin searching. 
56 
00:04:57,049 --> 00:04:57,890 
Good luck. 
1 
00:00:00,025 --> 00:00:04,660 
[SOUND] Hello, this is Chris Busheers, 
2 
00:00:04,660 --> 00:00:08,881 
part of the Splunk education team. 
3 
00:00:08,881 --> 00:00:13,530 
In this video, I'll show you how
install Splunk onto a Window's server. 
4 
00:00:13,530 --> 00:00:16,869 
First we need to get the software
from the splunk.com download page. 
5 
00:00:18,010 --> 00:00:22,710 
We will need to select that
the platform is 32 or 64-bit. 
6 
00:00:22,710 --> 00:00:25,605 
If you aren't sure if your system is 32 or 
7 
00:00:25,605 --> 00:00:28,200 
64-bit, you can check
your system properties. 
8 
00:00:29,320 --> 00:00:34,750 
As you can see, this server is 64-bit,
so we can install that version. 
9 
00:00:34,750 --> 00:00:39,720 
If we saw a system type of 32-bit,
we would download the 32-bit version. 
10 
00:00:40,720 --> 00:00:43,020 
We run the installer by
double clicking on it. 
11 
00:00:44,220 --> 00:00:48,920 
There is button to view the license
agreement, and a check box to accept it. 
12 
00:00:50,310 --> 00:00:53,800 
At this point we can either install
Splunk with the defaults, or 
13 
00:00:53,800 --> 00:00:55,160 
customize our installation. 
14 
00:00:56,290 --> 00:01:00,379 
Let's click on the customize options
to see what settings can be selected. 
15 
00:01:01,660 --> 00:01:05,860 
The first option is to change
the installation location of Splunk. 
16 
00:01:05,860 --> 00:01:09,940 
We are fine with this location,
so we click Next. 
17 
00:01:09,940 --> 00:01:15,100 
Now we must choose what account type
to install Splunk as, Local System or 
18 
00:01:15,100 --> 00:01:15,830 
Domain Account. 
19 
00:01:17,070 --> 00:01:21,420 
A Local System account will allow
Splunk to access all data on, or 
20 
00:01:21,420 --> 00:01:23,550 
forwarded, to this machine. 
21 
00:01:23,550 --> 00:01:26,140 
A Domain Account will allow
you to collect logs and 
22 
00:01:26,140 --> 00:01:31,070 
metrics from remote machines as
well as local and forwarded data. 
23 
00:01:31,070 --> 00:01:35,010 
You are required to provide a Domain
Account with the proper domain rights 
24 
00:01:35,010 --> 00:01:36,730 
to use this type. 
25 
00:01:36,730 --> 00:01:40,530 
Local System works well for
us, so we click Next. 
26 
00:01:40,530 --> 00:01:45,012 
We can select to have a shortcut to Splunk
added, and click Install to continue. 
27 
00:01:45,012 --> 00:01:48,319 
[MUSIC] 
28 
00:01:48,319 --> 00:01:52,750 
Once installed, we can select to have
Splunk launch, and click Finish. 
29 
00:01:52,750 --> 00:01:55,720 
This Splunk web interface
opens in our default browser. 
30 
00:01:56,750 --> 00:02:01,130 
We enter the default user name of admin,
and a password of change made. 
31 
00:02:01,130 --> 00:02:04,400 
A dialog box appears asking
us to change our password. 
32 
00:02:04,400 --> 00:02:07,310 
It is always best practice to do this. 
33 
00:02:07,310 --> 00:02:10,820 
Once logged in, we are taken to
the Splunk launcher homepage. 
34 
00:02:10,820 --> 00:02:13,780 
And that's all it takes to get
Splunk installed on Windows. 
35 
00:02:13,780 --> 00:02:15,895 
Now dig in, and start exploring. 
36 
00:02:15,895 --> 00:02:21,000 
[SOUND] 
1 
00:00:03,530 --> 00:00:04,180 
Our third and 
2 
00:00:04,180 --> 00:00:09,080 
final case is applicable to most companies
that create customer-focused products. 
3 
00:00:11,030 --> 00:00:15,270 
They want to understand how their
customers are responding to the products, 
4 
00:00:15,270 --> 00:00:17,800 
how the product marketing
efforts are performing, 
5 
00:00:17,800 --> 00:00:21,470 
what kind of problems customers
are encountering, and what new features or 
6 
00:00:21,470 --> 00:00:24,330 
feature improvements the customers
are seeking, and so forth. 
7 
00:00:25,380 --> 00:00:28,010 
But how does the company
get this information? 
8 
00:00:28,010 --> 00:00:31,060 
What kind of data sources
would carry this information? 
9 
00:00:31,060 --> 00:00:33,710 
The figure show some of these sources. 
10 
00:00:33,710 --> 00:00:40,210 
They are in focused user surveys,
emails sent by the customers, in blogs and 
11 
00:00:40,210 --> 00:00:46,350 
product review forums, specialized
groups on social media and user forums. 
12 
00:00:46,350 --> 00:00:52,990 
In short, they are on the Internet or
in material received through the Internet. 
13 
00:00:52,990 --> 00:00:54,250 
Now, how many sources are there? 
14 
00:00:55,830 --> 00:00:57,200 
Two. 
15 
00:00:57,200 --> 00:00:58,940 
The number would vary. 
16 
00:00:58,940 --> 00:01:01,060 
A new sites, a new postings, and 
17 
00:01:01,060 --> 00:01:02,950 
new discussion threads
would come up all the time. 
18 
00:01:04,000 --> 00:01:07,870 
In all of these,
the goal is to identify information that 
19 
00:01:07,870 --> 00:01:12,130 
truly relates to the companies product,
its features and its utility. 
20 
00:01:14,470 --> 00:01:17,530 
To cast this as a type
of big data problem, 
21 
00:01:17,530 --> 00:01:20,871 
we look at a task that computer
scientists called Data Fusion. 
22 
00:01:22,690 --> 00:01:27,840 
Consider a set of data sources, S,
as we mentioned on the last slide and 
23 
00:01:27,840 --> 00:01:29,320 
a set of data items, D. 
24 
00:01:30,630 --> 00:01:35,420 
A data item represents a particular
aspect of a real world entity 
25 
00:01:35,420 --> 00:01:37,290 
which in our case is
a product of the company. 
26 
00:01:39,180 --> 00:01:44,860 
For each data item, a source can, but
not necessarily will, provide a value. 
27 
00:01:44,860 --> 00:01:46,020 
For example, 
28 
00:01:46,020 --> 00:01:51,450 
the usability of an ergonomically
split keyboard can have a value good. 
29 
00:01:52,590 --> 00:01:57,500 
The value can be atomic,
like good, or a set, or a list or 
30 
00:01:57,500 --> 00:01:58,910 
sometimes embedded in the string. 
31 
00:02:00,370 --> 00:02:04,270 
For example, the cursor sometimes
freezes when using the touchpad, 
32 
00:02:05,410 --> 00:02:09,300 
is a string which has
a value about the touchpad. 
33 
00:02:11,400 --> 00:02:16,130 
The goal of Data Fusion is to find
the values of Data Items from a source. 
34 
00:02:18,060 --> 00:02:23,160 
In many cases, the system would find
a unique true value of an item. 
35 
00:02:23,160 --> 00:02:27,580 
For example, the launch data of a product
in Europe should be the same true value 
36 
00:02:27,580 --> 00:02:29,530 
regardless of the data
source one looks at. 
37 
00:02:30,700 --> 00:02:34,520 
In other cases, we could find
a value distribution of an item. 
38 
00:02:34,520 --> 00:02:37,960 
For example, the usability of our
keyboard may have a value distribution. 
39 
00:02:39,070 --> 00:02:43,960 
That's with Data Fusion, we should be
able to collect the values of real world 
40 
00:02:43,960 --> 00:02:46,790 
items from a subset of data sources. 
41 
00:02:46,790 --> 00:02:49,850 
It is a subset because
not all Data Sources 
42 
00:02:49,850 --> 00:02:51,940 
will have relevant information
about the Data Item. 
43 
00:02:53,440 --> 00:02:56,212 
There are some other versions
of what a Data Fusion is but for 
44 
00:02:56,212 --> 00:02:58,992 
our purposes we'll stick with
this general description. 
45 
00:03:01,190 --> 00:03:05,856 
Now one obvious problem with the Internet
is that there are too many data 
46 
00:03:05,856 --> 00:03:09,530 
sources at any time,
these lead to many difficulties. 
47 
00:03:10,910 --> 00:03:14,570 
First, it is to be understood
that with too many data sources 
48 
00:03:14,570 --> 00:03:17,310 
there will be many values for
the same item. 
49 
00:03:18,530 --> 00:03:21,190 
Often these will differ and
sometimes they will conflict. 
50 
00:03:22,580 --> 00:03:25,740 
A standard technique in this case
is to use a voting mechanism. 
51 
00:03:27,200 --> 00:03:31,820 
However, even a voting
mechanism can be complex 
52 
00:03:31,820 --> 00:03:33,350 
due to problems with the data source. 
53 
00:03:35,040 --> 00:03:38,800 
One of the problems is to estimate
the trustworthiness of the source. 
54 
00:03:40,130 --> 00:03:42,080 
For each data source, 
55 
00:03:42,080 --> 00:03:48,260 
we need to evaluate whether it's reporting
some basic or known facts correctly. 
56 
00:03:48,260 --> 00:03:51,510 
If a source mentions details
about a rainbow colored iPhone, 
57 
00:03:51,510 --> 00:03:55,010 
which does not exist,
it's trustworthiness reduces 
58 
00:03:55,010 --> 00:03:57,960 
because of the falsity of
the provided value of this data item. 
59 
00:03:59,270 --> 00:04:03,140 
Accordingly, a higher vote count can be
assigned to a more trustworthy source. 
60 
00:04:04,590 --> 00:04:07,640 
And then, this can be used in voting. 
61 
00:04:09,560 --> 00:04:11,260 
The second aspect is Copy Detection. 
62 
00:04:12,700 --> 00:04:16,670 
Detecting weather once was has copied
information from another can be very 
63 
00:04:16,670 --> 00:04:19,490 
important for
detail fusion task in customer analytics. 
64 
00:04:20,660 --> 00:04:22,680 
If a source has copied information, 
65 
00:04:23,750 --> 00:04:28,900 
it's such that discounted vote count
can be assigned to a copy value and 
66 
00:04:28,900 --> 00:04:34,190 
voting that means the copy in
source will have less weight. 
67 
00:04:35,210 --> 00:04:40,160 
Now this is especially relevant when we
compute value distributions, because if we 
68 
00:04:40,160 --> 00:04:45,680 
treat copies as genuine information, we
will statistically bias the distribution. 
69 
00:04:45,680 --> 00:04:50,279 
Now here is active research on how to
detect copies, how to determine bias and 
70 
00:04:50,279 --> 00:04:54,685 
then arrive at a statistically sound
estimation of value distribution. 
71 
00:04:54,685 --> 00:04:59,628 
But to our knowledge, these methods are
yet to be applied to existing software for 
72 
00:04:59,628 --> 00:05:01,070 
big data integration. 
73 
00:05:04,473 --> 00:05:06,240 
It should be very clear by now but 
74 
00:05:06,240 --> 00:05:10,240 
there are two kinds of big data
situations when it comes to information. 
75 
00:05:11,400 --> 00:05:16,180 
The first two uses cases that we
saw requires an integration system 
76 
00:05:16,180 --> 00:05:20,380 
to consider all sources because
the application demand so. 
77 
00:05:21,620 --> 00:05:27,660 
In contrast, problems where data comes
from too many redundant, potentially 
78 
00:05:27,660 --> 00:05:32,380 
unreliable sources like the Internet, the
best results can be obtained if we have 
79 
00:05:32,380 --> 00:05:36,490 
a way of evaluating the worthiness of
sources before information integration. 
80 
00:05:37,660 --> 00:05:40,790 
But this problem is
called Source Selection. 
81 
00:05:40,790 --> 00:05:44,570 
The picture on the right shows the result
of a cost benefit analysis for 
82 
00:05:44,570 --> 00:05:46,080 
data fusion. 
83 
00:05:46,080 --> 00:05:49,239 
The x-axis indicates the number
of sources used, and 
84 
00:05:49,239 --> 00:05:53,400 
the y-axis measures the proportion
of true results that were returned. 
85 
00:05:55,090 --> 00:05:59,480 
We can clearly see that the plot peaks
around six-to-eight sources, and 
86 
00:05:59,480 --> 00:06:01,990 
that the efficiency falls
as more sources are added. 
87 
00:06:03,790 --> 00:06:08,930 
In a cost benefit analysis,
the cost must include both the human and 
88 
00:06:08,930 --> 00:06:10,580 
the computational costs, 
89 
00:06:10,580 --> 00:06:14,270 
while the benefit is a function of
the accuracy of the fusion result. 
90 
00:06:14,270 --> 00:06:19,210 
The technique for
solving this problem comes from economics. 
91 
00:06:20,520 --> 00:06:23,870 
Assuming that cost and
benefits are measure in the same unit, for 
92 
00:06:23,870 --> 00:06:24,660 
example, dollars. 
93 
00:06:25,780 --> 00:06:28,580 
They proposed to continue
selecting sources 
94 
00:06:28,580 --> 00:06:32,740 
until the marginal benefit is
less than the marginal cost. 
95 
00:06:34,150 --> 00:06:38,210 
Now recent techniques were performing
this computation at quite scalable. 
96 
00:06:38,210 --> 00:06:41,910 
In one setting,
selecting the most beneficial sources 
97 
00:06:41,910 --> 00:06:45,520 
from a total of one million
sources took less than one hour. 
98 
00:06:47,900 --> 00:06:51,980 
This completes our coverage of
the big data integration problems. 
1 
00:00:00,830 --> 00:00:02,180 
Hello and welcome. 
2 
00:00:02,180 --> 00:00:07,180 
My name is Rene and I'll be sharing with
you, how to create a report using Pivot. 
3 
00:00:07,180 --> 00:00:11,920 
To access the Pivot interface,
click Pivot on the navigation menu. 
4 
00:00:13,630 --> 00:00:17,370 
The first step is to select
a prebuilt data model. 
5 
00:00:17,370 --> 00:00:21,260 
Now the data model allows you
to create compelling reports and 
6 
00:00:21,260 --> 00:00:26,090 
dashboards without having to know
how to write complex search queries. 
7 
00:00:26,090 --> 00:00:31,000 
As a side note data models are typically
created by a knowledge manager that 
8 
00:00:31,000 --> 00:00:34,200 
understands their
organization's index data. 
9 
00:00:34,200 --> 00:00:38,760 
Grasps the search language and
are familiar with lookups, 
10 
00:00:38,760 --> 00:00:42,640 
transactions, field extractions and
calculated fields. 
11 
00:00:43,750 --> 00:00:48,260 
For our example, we're going to
use Buttercup Games Online Sales. 
12 
00:00:50,200 --> 00:00:54,960 
Now, this data model includes
the online sales activities. 
13 
00:00:54,960 --> 00:00:58,100 
It's made up of nine objects. 
14 
00:00:58,100 --> 00:01:03,660 
Each object represents a specific set
of events in a hierarchical structure. 
15 
00:01:05,290 --> 00:01:12,390 
The http request would include the largest
number of events in this data model. 
16 
00:01:12,390 --> 00:01:15,540 
The next object, successful request, 
17 
00:01:15,540 --> 00:01:20,950 
would be a subset of the http
request events and so on. 
18 
00:01:20,950 --> 00:01:25,710 
Let's say that we want to create a report
for purchases over the last seven days. 
19 
00:01:25,710 --> 00:01:28,560 
We would select the most
appropriate object. 
20 
00:01:28,560 --> 00:01:32,200 
In this case we're going to
select successful purchase. 
21 
00:01:33,910 --> 00:01:37,940 
After you have selected the object
you will notice that a new 
22 
00:01:37,940 --> 00:01:39,460 
Pivot view will display. 
23 
00:01:40,720 --> 00:01:45,950 
If you notice, the count of
successful purchase is the total 
24 
00:01:45,950 --> 00:01:50,580 
number of events for this specific object. 
25 
00:01:50,580 --> 00:01:52,780 
Now, it is based off of all time. 
26 
00:01:54,470 --> 00:01:59,840 
Before you begin, you should have an idea
of what type of report you want to create. 
27 
00:01:59,840 --> 00:02:02,720 
So let's say that we
want to create a bar chart. 
28 
00:02:02,720 --> 00:02:08,120 
We're going to select that
from the left navigation pane. 
29 
00:02:08,120 --> 00:02:12,375 
So the third icon will allow me
to click and select Bar Chart. 
30 
00:02:14,278 --> 00:02:18,130 
So at this point we are ready
to start building our report. 
31 
00:02:19,560 --> 00:02:21,310 
The first selection is Time Range. 
32 
00:02:21,310 --> 00:02:24,480 
Let's set that to the last seven days. 
33 
00:02:26,030 --> 00:02:30,780 
The next one is Filter, now we don't need
to define Filter, because we've already 
34 
00:02:30,780 --> 00:02:36,210 
selected an object of successful purchase
and that's what we're reporting against. 
35 
00:02:37,720 --> 00:02:42,570 
Now, for my x axis,
I'm going to select product name. 
36 
00:02:43,620 --> 00:02:46,040 
You'll notice that I can set the label. 
37 
00:02:46,040 --> 00:02:48,170 
I can set the sort order. 
38 
00:02:48,170 --> 00:02:51,091 
I can even set how many
maximum bars that I want. 
39 
00:02:52,728 --> 00:02:56,988 
The next thing to define is the y axis. 
40 
00:02:56,988 --> 00:03:01,970 
At this time it's set to
count of successful purchase. 
41 
00:03:01,970 --> 00:03:05,325 
But I want the total sales by product. 
42 
00:03:05,325 --> 00:03:07,785 
So I'm going to select price and 
43 
00:03:07,785 --> 00:03:12,655 
then you'll notice that my
value is already set to sum. 
44 
00:03:12,655 --> 00:03:14,475 
So I'm going to keep it as sum. 
45 
00:03:16,340 --> 00:03:18,030 
My report is starting to look really good. 
46 
00:03:18,030 --> 00:03:25,370 
But after reviewing it I might decide that
I want to see values segmented by host. 
47 
00:03:25,370 --> 00:03:30,670 
So let's do that,
in order to set the segmentation 
48 
00:03:30,670 --> 00:03:38,290 
of a specific product name I could go and
set the color to the field that I want. 
49 
00:03:38,290 --> 00:03:39,824 
So I'm going to select Host. 
50 
00:03:42,936 --> 00:03:49,490 
Now, you'll see that my total
sales are segmented by host. 
51 
00:03:50,966 --> 00:03:52,566 
So we really like this report. 
52 
00:03:52,566 --> 00:03:54,190 
Let's go ahead and save it. 
53 
00:03:55,240 --> 00:03:57,860 
So we're going to click on Save As. 
54 
00:03:59,050 --> 00:04:01,560 
Select report and give it a title. 
55 
00:04:01,560 --> 00:04:06,810 
Let's call it Product Sales by Host. 
56 
00:04:06,810 --> 00:04:13,684 
We're going to click Save and
then we're going to view our report. 
57 
00:04:16,563 --> 00:04:19,880 
It was that easy to create
a report through Pivot. 
58 
00:04:21,850 --> 00:04:26,030 
Now, let's just say that we want to go and
work with another Pivot. 
59 
00:04:26,030 --> 00:04:28,810 
This time we're going to just go and 
60 
00:04:28,810 --> 00:04:32,070 
create a statistic table
to view some of that data. 
61 
00:04:32,070 --> 00:04:34,917 
Let's go ahead and click Pivot again. 
62 
00:04:36,700 --> 00:04:39,090 
We'll select the same data model. 
63 
00:04:39,090 --> 00:04:43,637 
Buttercup games online sales and
let's keep to the object that 
64 
00:04:43,637 --> 00:04:47,855 
we're familiar with right now,
successful purchases. 
65 
00:04:54,902 --> 00:04:59,920 
As I mentioned, I want to just
simply create a statistic table. 
66 
00:05:00,990 --> 00:05:03,400 
I want to show you how
we'd go about doing that. 
67 
00:05:04,620 --> 00:05:11,700 
So the first step is to think about what
are some of the row data that you want? 
68 
00:05:11,700 --> 00:05:18,790 
Let's just assume that we want to view
the product names within their categories. 
69 
00:05:18,790 --> 00:05:24,500 
So, let's go ahead, under Split Rows,
we're going to select category first. 
70 
00:05:25,530 --> 00:05:28,060 
Notice that I can set a label if I want. 
71 
00:05:28,060 --> 00:05:32,500 
Let's go ahead and
set it capitalized category. 
72 
00:05:32,500 --> 00:05:33,897 
I'm going to add that to my table. 
73 
00:05:33,897 --> 00:05:38,403 
You'll see that it
becomes the first column. 
74 
00:05:38,403 --> 00:05:42,240 
Now I want to add another column. 
75 
00:05:42,240 --> 00:05:46,535 
Let's go ahead and
click the plus right next to categories. 
76 
00:05:46,535 --> 00:05:49,995 
Again we're still working
with the split rows. 
77 
00:05:51,065 --> 00:05:56,375 
I'm going to select product name, again if
you want to set a label you can do that. 
78 
00:05:56,375 --> 00:05:58,955 
Now this is the label of our column. 
79 
00:06:00,175 --> 00:06:00,675 
Add to table. 
80 
00:06:04,367 --> 00:06:09,282 
And I want you to notice that
now we are viewing each product 
81 
00:06:09,282 --> 00:06:11,400 
within their category. 
82 
00:06:12,580 --> 00:06:16,130 
Now at this point if you look at
the statistic that is being defined, 
83 
00:06:16,130 --> 00:06:19,850 
it's the count of successful purchases. 
84 
00:06:19,850 --> 00:06:22,570 
That is defined as column values. 
85 
00:06:23,940 --> 00:06:28,960 
Now if I want to add to this,
I can append and have another column. 
86 
00:06:28,960 --> 00:06:34,050 
Let's go ahead and click the plus next
to the Count of successful purchase. 
87 
00:06:35,480 --> 00:06:37,300 
This time I want to add the price. 
88 
00:06:38,520 --> 00:06:41,216 
And you'll notice that I have
different values available. 
89 
00:06:41,216 --> 00:06:44,662 
I could do a Sum, a Count,
an Average, Max, Min. 
90 
00:06:44,662 --> 00:06:51,274 
Let's keep it as a Sum and
let's add a label of Total Sales. 
91 
00:06:51,274 --> 00:06:53,795 
And let's Add To Table. 
92 
00:06:55,955 --> 00:06:59,675 
So this is just to show you
how quick it is to go and 
93 
00:06:59,675 --> 00:07:03,955 
build a table with the statistics
that you would need. 
94 
00:07:03,955 --> 00:07:07,977 
Now at this point, if ever you decide,
I only want to view the data for 
95 
00:07:07,977 --> 00:07:09,260 
the last seven days. 
96 
00:07:09,260 --> 00:07:13,590 
Just note that you could go
back up to your filter and 
97 
00:07:13,590 --> 00:07:16,630 
your first filter is
based off of all time. 
98 
00:07:16,630 --> 00:07:20,870 
So we could set that to
the last seven days. 
99 
00:07:20,870 --> 00:07:23,610 
So of course that the data
in our table will change. 
100 
00:07:24,750 --> 00:07:29,030 
Now, the last thing I want to show you
here is if ever you did want to see 
101 
00:07:30,120 --> 00:07:37,075 
the count and the total sales per host,
as we've defined in our previous example. 
102 
00:07:37,075 --> 00:07:41,615 
That is where you could go
to the split columns and 
103 
00:07:41,615 --> 00:07:44,325 
define how you wish to
split that information. 
104 
00:07:44,325 --> 00:07:47,825 
So let's go ahead and select host again. 
105 
00:07:47,825 --> 00:07:50,731 
And I'm just going to
click on add to table. 
106 
00:07:55,074 --> 00:08:02,820 
Now I could see that the information
has been split by host. 
107 
00:08:02,820 --> 00:08:08,460 
I have www1 count of successful
purchases as well as total sales. 
108 
00:08:08,460 --> 00:08:15,810 
And then I have the split of www2 I have
the count and the total sales and etc. 
109 
00:08:15,810 --> 00:08:20,950 
So this concludes our short
video hope you enjoy it. 
110 
00:08:20,950 --> 00:08:21,450 
Thank you. 
1 
00:00:01,860 --> 00:00:03,350 
Welcome. 
2 
00:00:03,350 --> 00:00:08,570 
In this short module we'll talk about
information integration which refers 
3 
00:00:08,570 --> 00:00:13,470 
to the problem of using many different
information sources to accomplish a task. 
4 
00:00:14,780 --> 00:00:19,939 
In this module, we'll look at the problems
and solutions through a few use cases. 
5 
00:00:22,330 --> 00:00:27,830 
So after this video, you'll be able to
explain the data integration problem, 
6 
00:00:29,250 --> 00:00:34,930 
define integrated views and schema
mapping, describe the impact of increasing 
7 
00:00:34,930 --> 00:00:40,306 
the number of data sources, appreciate
the need to use data compression, 
8 
00:00:40,306 --> 00:00:46,910 
And describe record linking,
data exchange, and data fusion tasks. 
9 
00:00:50,008 --> 00:00:54,230 
Our first use case starts with
an example given at an IBM website for 
10 
00:00:54,230 --> 00:00:55,760 
their information integration products. 
11 
00:00:57,150 --> 00:01:00,580 
It represents a very common
scenario in today's business world. 
12 
00:01:02,100 --> 00:01:06,040 
Due to the changing market dynamics,
companies 
13 
00:01:06,040 --> 00:01:09,620 
are always selling off a part of their
company or acquiring another company. 
14 
00:01:11,330 --> 00:01:15,950 
As these mergers and acquisitions happen,
databases which were developed and 
15 
00:01:15,950 --> 00:01:19,900 
stored separately in different companies
would now need to be brought together. 
16 
00:01:21,680 --> 00:01:23,640 
Now take a minute to read this case. 
17 
00:01:28,134 --> 00:01:32,644 
This is the case of an expanding financial
services group that's growing its customer 
18 
00:01:32,644 --> 00:01:34,260 
base in different countries. 
19 
00:01:36,420 --> 00:01:41,980 
And all they want is a single view
of their entire customer base. 
20 
00:01:41,980 --> 00:01:46,590 
In other words, it does not matter
which previous company originally had 
21 
00:01:46,590 --> 00:01:51,920 
the customers, Suncorp-Metway
want to consolidate all customer 
22 
00:01:51,920 --> 00:01:58,130 
information as if they were
in one single database. 
23 
00:01:58,130 --> 00:02:02,223 
And in reality, of course, they may
not want to buy a huge machine and 
24 
00:02:02,223 --> 00:02:05,340 
migrate every subsidiary
company's data into it. 
25 
00:02:06,680 --> 00:02:12,338 
What they're looking to create is possibly
a software solution which would make all 
26 
00:02:12,338 --> 00:02:18,710 
customer-related data to appear as though
they were together as a single database. 
27 
00:02:18,710 --> 00:02:22,290 
This software solution is called
an information integration system. 
28 
00:02:23,590 --> 00:02:27,920 
This will help them ensure that they have
a uniform set of marketing campaigns for 
29 
00:02:27,920 --> 00:02:29,040 
all their customers. 
30 
00:02:31,780 --> 00:02:36,893 
Let's try to see, hypothetically, of
course, what might be involved in creating 
31 
00:02:36,893 --> 00:02:41,511 
this combined data and what kind of use
the integrated data might result in. 
32 
00:02:41,511 --> 00:02:44,330 
So we first create
a hypothetical scenario. 
33 
00:02:45,552 --> 00:02:48,920 
Although Suncorp have a large
number of data sources, 
34 
00:02:48,920 --> 00:02:51,290 
we will take a much simpler situation and 
35 
00:02:51,290 --> 00:02:55,610 
have only two data sources from two
different financial service companies. 
36 
00:02:57,010 --> 00:02:58,190 
The first data source, 
37 
00:02:58,190 --> 00:03:03,090 
which is an insurance company that manages
it's data with a relation of DBMS, 
38 
00:03:03,090 --> 00:03:08,660 
this database has nine tables where the
primary object of information is a policy. 
39 
00:03:10,040 --> 00:03:13,090 
The company offers many
different types of policies 
40 
00:03:13,090 --> 00:03:16,380 
sold to individual people by their agents. 
41 
00:03:16,380 --> 00:03:20,740 
Now as it's true for all insurance
companies, policyholders pay their monthly 
42 
00:03:20,740 --> 00:03:26,130 
dues, and sometimes people make claims
against their insurance policies. 
43 
00:03:26,130 --> 00:03:30,080 
When they do, the details of the claims
are maintained in the database. 
44 
00:03:31,170 --> 00:03:35,220 
These claims can belong to different
categories, and when the claims have 
45 
00:03:35,220 --> 00:03:39,780 
paid to the claimants, the transaction
is recorded in the transactions table. 
46 
00:03:41,350 --> 00:03:43,640 
As we have done several times now, 
47 
00:03:43,640 --> 00:03:46,930 
the primary keys of the table
are the underlined attributes here. 
48 
00:03:48,900 --> 00:03:51,000 
The second company in
our example is a bank, 
49 
00:03:52,060 --> 00:03:53,850 
which also uses a relational database. 
50 
00:03:55,240 --> 00:03:59,500 
In this bank, both individuals and
businesses called corporations here, 
51 
00:03:59,500 --> 00:04:00,140 
can have accounts. 
52 
00:04:02,900 --> 00:04:05,050 
Now accounts can be of different types. 
53 
00:04:05,050 --> 00:04:07,900 
For example, a money market account
is different from a savings account. 
54 
00:04:09,330 --> 00:04:13,910 
A bank also maintains its transactions
in a table, which can be really large. 
55 
00:04:15,620 --> 00:04:20,050 
But the dispute in a bank record
case happens when the bank is 
56 
00:04:20,050 --> 00:04:24,690 
charged a customer, or the customer has
declined responsibility of the charge. 
57 
00:04:24,690 --> 00:04:29,030 
This can happen, for example, if a
customer's Internet account was hacked or 
58 
00:04:29,030 --> 00:04:30,260 
a debit card got stolen. 
59 
00:04:31,700 --> 00:04:33,800 
The bank keeps a record
of these anomalies and 
60 
00:04:33,800 --> 00:04:38,260 
fraudulent events in a disputes table,
all right. 
61 
00:04:38,260 --> 00:04:42,730 
Let's see what happens after the data
from these two subsidiary companies 
62 
00:04:42,730 --> 00:04:43,640 
are integrated. 
63 
00:04:46,410 --> 00:04:50,970 
After the merger the company wants to
do a promotional goodwill activity. 
64 
00:04:51,970 --> 00:04:54,490 
They would like to offer
a small discount to their 
65 
00:04:54,490 --> 00:04:59,130 
insurance policyholders if they're also
customers of the newly acquired bank. 
66 
00:05:00,350 --> 00:05:02,650 
How do you identify these customers? 
67 
00:05:02,650 --> 00:05:03,960 
Let's see. 
68 
00:05:03,960 --> 00:05:08,882 
In other words, we need to use the table
shown on the left to create the table 
69 
00:05:08,882 --> 00:05:12,103 
shown on the right called
discount candidates. 
70 
00:05:12,103 --> 00:05:16,547 
One is to create a yellow tables from
the insurance company database, and 
71 
00:05:16,547 --> 00:05:21,351 
the blue table from the bank database,
and then join them to construct the table 
72 
00:05:21,351 --> 00:05:25,960 
with a common customer ID, and both
the policyKey and bank account number. 
73 
00:05:27,290 --> 00:05:31,600 
Now, this relation, which is derived that
is computed by querying two different 
74 
00:05:31,600 --> 00:05:36,260 
data sources and combining their results,
is called an integrated view. 
75 
00:05:37,330 --> 00:05:42,400 
It is integrated because the data is
retrieved from different data sources, 
76 
00:05:43,610 --> 00:05:47,640 
and it's called a view because
in database terminology 
77 
00:05:47,640 --> 00:05:50,680 
it is a relation computed
from other relations. 
78 
00:05:53,498 --> 00:05:56,780 
To populate the integrated
view discount candidates, 
79 
00:05:56,780 --> 00:05:59,470 
we need to go through a step
called schema mapping. 
80 
00:06:00,580 --> 00:06:04,550 
The term mapping means to
establish correspondence between 
81 
00:06:04,550 --> 00:06:08,850 
the attributes of the view, which is
also called a target relation, and 
82 
00:06:08,850 --> 00:06:10,120 
that of the source relations. 
83 
00:06:11,200 --> 00:06:16,270 
For example, we can map the full address
from individuals to the address attribute 
84 
00:06:16,270 --> 00:06:21,380 
in discountCandidates, but
this would only be true for 
85 
00:06:21,380 --> 00:06:25,300 
customers whose names and
addresses match in the two databases. 
86 
00:06:27,040 --> 00:06:32,120 
As you can see, policyholders uses
the full name of a customer, whereas 
87 
00:06:32,120 --> 00:06:36,050 
individuals has it broken down into first
name, middle initial, and last name. 
88 
00:06:37,520 --> 00:06:42,760 
On the other hand, full address is
a single field in individuals, but 
89 
00:06:42,760 --> 00:06:46,397 
represented in full attributes
in the policyholders relation. 
90 
00:06:48,090 --> 00:06:51,650 
The mappings of account number and
policyKey are more straightforward. 
91 
00:06:52,680 --> 00:06:55,752 
Well, what about customer ID which doesn't
correspond to anything in the four 
92 
00:06:55,752 --> 00:06:57,300 
input relations? 
93 
00:06:57,300 --> 00:06:58,959 
We'll come back to this later on. 
94 
00:07:01,019 --> 00:07:04,470 
Okay, now we'll define
an integrated relation. 
95 
00:07:04,470 --> 00:07:06,020 
How do we query? 
96 
00:07:06,020 --> 00:07:07,090 
For example, 
97 
00:07:07,090 --> 00:07:11,170 
how do you find the bank account number
of a person whose policyKey is known? 
98 
00:07:12,530 --> 00:07:15,020 
You might think, what's the problem here? 
99 
00:07:15,020 --> 00:07:16,590 
We have a table. 
100 
00:07:16,590 --> 00:07:22,020 
Just say select account number from
discount candidates where policyKey 
101 
00:07:22,020 --> 00:07:26,680 
is equal to 4-937528734, and we're done. 
102 
00:07:28,730 --> 00:07:35,310 
Well, yes, you can write this query,
but how the query be evaluated? 
103 
00:07:35,310 --> 00:07:37,820 
That depends on what's called the query 
104 
00:07:37,820 --> 00:07:40,610 
architecture of the data
integration system. 
105 
00:07:40,610 --> 00:07:43,130 
The figure on the left shows
the elements of this architecture. 
106 
00:07:44,290 --> 00:07:47,694 
We'll discover it in more detail,
but on this slide, 
107 
00:07:47,694 --> 00:07:50,810 
we'll just describe
the three axes of this cube. 
108 
00:07:51,830 --> 00:07:57,090 
The vertical z axis specifies
whether we have one data source or 
109 
00:07:57,090 --> 00:07:58,320 
multiple data sources. 
110 
00:07:59,320 --> 00:08:02,390 
Our interest is in the case where
there are multiple data sources. 
111 
00:08:04,218 --> 00:08:10,110 
The x axis asks whether the integrated
data is actually stored physically 
112 
00:08:10,110 --> 00:08:16,280 
in some place or whether it is computed
on the fly, each time a query is asked. 
113 
00:08:16,280 --> 00:08:22,120 
If it is all precomputed and stored,
we say that the data is materialized. 
114 
00:08:22,120 --> 00:08:24,940 
And if it is computed on the fly,
we say it's virtual. 
115 
00:08:26,480 --> 00:08:30,174 
The y axis asks whether
there is a single schema or 
116 
00:08:30,174 --> 00:08:34,319 
global schema defined all
over the data integrated for 
117 
00:08:34,319 --> 00:08:39,364 
an application or whether the data
stay in different computers and 
118 
00:08:39,364 --> 00:08:43,531 
it is accessed in a peer-to-peer
manner at runtime. 
119 
00:08:43,531 --> 00:08:48,267 
Thus, the seemingly simple select
project query will be evaluated 
120 
00:08:48,267 --> 00:08:53,870 
depending on which part of the cube
our architecture implements. 
121 
00:08:53,870 --> 00:08:56,880 
But for now,
let's return to our example use case. 
122 
00:08:59,900 --> 00:09:03,920 
An obvious goal of an information
integration system 
123 
00:09:03,920 --> 00:09:05,260 
is to be complete and accurate. 
124 
00:09:06,770 --> 00:09:10,790 
Complete means no eligible record
from the source should be absent in 
125 
00:09:10,790 --> 00:09:11,670 
the target relation. 
126 
00:09:12,970 --> 00:09:18,668 
Accurate means all the entries in
the integrated relation should be correct. 
127 
00:09:18,668 --> 00:09:23,148 
Now we said on the previous
slide that a matching 
128 
00:09:23,148 --> 00:09:27,740 
customer is a person who
was in both databases and 
129 
00:09:27,740 --> 00:09:32,590 
has the same name and
address in the two databases. 
130 
00:09:32,590 --> 00:09:34,310 
Now let's look at some example records. 
131 
00:09:35,390 --> 00:09:39,110 
Specifically, consider the records
marked by the three arrows. 
132 
00:09:40,510 --> 00:09:45,320 
The two bank accounts and
the policy record do not match for name or 
133 
00:09:45,320 --> 00:09:47,050 
for address. 
134 
00:09:47,050 --> 00:09:49,360 
So our previous method would discard them. 
135 
00:09:50,510 --> 00:09:52,030 
But look at the records closely. 
136 
00:09:53,380 --> 00:09:56,070 
Do you think they might all
belong to the same customer? 
137 
00:09:57,560 --> 00:09:59,740 
Maybe this lady has a maiden name and 
138 
00:09:59,740 --> 00:10:03,150 
a married name, and
has moved from one address to another. 
139 
00:10:04,290 --> 00:10:07,289 
Maybe she changed her Social Security
number somewhere along the way. 
140 
00:10:08,820 --> 00:10:11,640 
So this is called
a record linkage problem. 
141 
00:10:12,730 --> 00:10:17,510 
That means we would like to ensure that
the set of data records that belong to 
142 
00:10:17,510 --> 00:10:22,040 
a single entity are recognized,
perhaps by clustering 
143 
00:10:22,040 --> 00:10:26,790 
the values of different attributes or
by using a set of matching rules so 
144 
00:10:26,790 --> 00:10:31,130 
that we know how to deal with it
during the integration process. 
145 
00:10:31,130 --> 00:10:33,600 
For example, we need to determine 
146 
00:10:33,600 --> 00:10:36,250 
which of the addresses should be
used in the integrated relation. 
147 
00:10:37,570 --> 00:10:38,650 
Which of the two bank accounts? 
148 
00:10:40,070 --> 00:10:44,800 
If the answer is both accounts 102 and
103, we will need to change 
149 
00:10:44,800 --> 00:10:50,380 
the schema of the target's relation
to a list instead of an atomic number 
150 
00:10:50,380 --> 00:10:53,290 
to avoid creating multiple tuples for
the same entity. 
151 
00:10:55,990 --> 00:11:00,380 
As we saw, the schema of adding
process is a task of figuring out 
152 
00:11:00,380 --> 00:11:04,850 
how elements of the schema from two
sources would relate to each other and 
153 
00:11:04,850 --> 00:11:07,940 
determining how they would
map the target schema. 
154 
00:11:08,980 --> 00:11:13,180 
You also saw that this is not really
a simple process, that we are trying to 
155 
00:11:13,180 --> 00:11:18,600 
produce one integrated relation using
a couple of relations from each source. 
156 
00:11:20,130 --> 00:11:25,250 
In a Big Data situation,
there are dozens of data sources, or 
157 
00:11:25,250 --> 00:11:31,880 
more because the company's growing and
each source may have a few hundred tables. 
158 
00:11:31,880 --> 00:11:35,370 
So it becomes very hard to actually solve 
159 
00:11:35,370 --> 00:11:40,419 
this correspondence-making problem
completely and accurately just because 
160 
00:11:40,419 --> 00:11:44,920 
the number of combinations one has to
go through is really, really high. 
161 
00:11:47,120 --> 00:11:52,250 
One practical way to tackle this problem
is not to do a full-scale detail 
162 
00:11:52,250 --> 00:11:57,147 
integration in the beginning but
adopt what's called a pay-as-you-go model. 
163 
00:11:57,147 --> 00:12:00,189 
The pay-as-you-go data
management principle is simple. 
164 
00:12:01,380 --> 00:12:06,214 
The system should provide some basic
integration services at the outset and 
165 
00:12:06,214 --> 00:12:11,123 
then evolve the schema mappings between
the different sources on an as needed 
166 
00:12:11,123 --> 00:12:12,660 
basis. 
167 
00:12:12,660 --> 00:12:17,190 
So given a query, the system should
generate a best effort or approximate 
168 
00:12:17,190 --> 00:12:21,310 
answers from the data sources for
a perfect schema mappings do not exist. 
169 
00:12:22,310 --> 00:12:26,015 
When it discovers a large number
of sophisticated queries or 
170 
00:12:26,015 --> 00:12:30,375 
data mining tasks over certain sources,
it will guide the users to make 
171 
00:12:30,375 --> 00:12:34,387 
additional efforts to integrate
these sources more precisely. 
172 
00:12:36,401 --> 00:12:40,270 
Okay, so how does the first
approximate schema mapping performed? 
173 
00:12:41,660 --> 00:12:46,920 
One approach to do this is called
Probabilistic Schema Mapping. 
174 
00:12:46,920 --> 00:12:48,920 
We'll describe it in more detail next. 
175 
00:12:51,120 --> 00:12:52,450 
In the previous step, 
176 
00:12:52,450 --> 00:12:58,050 
we just decided to create the disk count
candidates rrelation in an ad hoc way. 
177 
00:12:58,050 --> 00:12:59,990 
And in a Big Data situation, 
178 
00:12:59,990 --> 00:13:03,920 
we need to carefully determine
what the integrated schema, 
179 
00:13:03,920 --> 00:13:08,000 
also called mediated schemas, should be,
and we should evaluate them properly. 
180 
00:13:09,640 --> 00:13:15,550 
Since our toy company is trying
to create a single customer view, 
181 
00:13:15,550 --> 00:13:19,660 
it's natural to create an integrated
table called customers. 
182 
00:13:19,660 --> 00:13:21,690 
But how can we design this table? 
183 
00:13:21,690 --> 00:13:22,550 
Here are some options. 
184 
00:13:23,820 --> 00:13:29,487 
We can create the customer table to
include individuals and corporations and 
185 
00:13:29,487 --> 00:13:34,384 
then use a flag called customer
type to distinguish between them. 
186 
00:13:34,384 --> 00:13:39,051 
Now in the mediated schema then,
the individuals first name, 
187 
00:13:39,051 --> 00:13:43,191 
middle initial, last name,
policyholder's name and 
188 
00:13:43,191 --> 00:13:48,140 
corporation's name would all
map to Customer_Name similarly. 
189 
00:13:49,390 --> 00:13:53,978 
The individual's full address,
the corporation's registered address, 
190 
00:13:53,978 --> 00:13:57,440 
and the policyholder's address,
plus city, plus state, 
191 
00:13:57,440 --> 00:14:01,530 
plus zip would all map
to customer address. 
192 
00:14:01,530 --> 00:14:06,010 
Now we can enumerate all such choices of
which attributes to group together and 
193 
00:14:06,010 --> 00:14:09,230 
map for each single attribute
in the target schema. 
194 
00:14:10,620 --> 00:14:15,630 
But no matter how you'll do it,
it will never be a perfect fit because 
195 
00:14:15,630 --> 00:14:18,590 
not all these combinations
would go well together. 
196 
00:14:18,590 --> 00:14:22,160 
For example, should that date of
birth be included in this table? 
197 
00:14:23,550 --> 00:14:24,870 
Would it make sense for corporations? 
198 
00:14:26,810 --> 00:14:32,310 
In Probabilistic Mediated Schema Design,
we answer this question by 
199 
00:14:32,310 --> 00:14:35,410 
associating probability values
with each of these options. 
200 
00:14:38,040 --> 00:14:42,410 
To compute these values, we need to
quantify the relationships between 
201 
00:14:42,410 --> 00:14:46,202 
attributes by figuring out which
attributes should be grouped or 
202 
00:14:46,202 --> 00:14:48,710 
clustered together? 
203 
00:14:48,710 --> 00:14:52,140 
Now two pieces of information
available in the source schemas 
204 
00:14:52,140 --> 00:14:55,497 
can serve as evidence for
attribute clustering. 
205 
00:14:55,497 --> 00:14:59,740 
One, the parallel similarity
of source attributes, and two, 
206 
00:14:59,740 --> 00:15:04,640 
statistical properties
of service attributes. 
207 
00:15:05,800 --> 00:15:10,260 
The first piece of information indicates
when two attributes are likely to be 
208 
00:15:10,260 --> 00:15:14,459 
similar and is used for
creating multiple mediated schemas. 
209 
00:15:15,680 --> 00:15:19,050 
One can apply a collection of
attribute matching modules 
210 
00:15:19,050 --> 00:15:21,650 
to compute pairwise similarity. 
211 
00:15:21,650 --> 00:15:23,755 
For example, individual names and 
212 
00:15:23,755 --> 00:15:26,700 
policyholder names
are possibly quite similar. 
213 
00:15:27,930 --> 00:15:32,878 
Are individual names versus
corporation names similar? 
214 
00:15:32,878 --> 00:15:37,283 
Now, the similarity between two
source attributes, EIN and EZ, 
215 
00:15:37,283 --> 00:15:42,560 
measure how closely the two attributes
represent the same real world concept. 
216 
00:15:44,520 --> 00:15:47,230 
The second piece of information indicates 
217 
00:15:47,230 --> 00:15:51,650 
when two attributes are likely
to be different, and is used for 
218 
00:15:51,650 --> 00:15:54,599 
assigning probabilities to
each of the mediated schemas. 
219 
00:15:56,370 --> 00:15:59,770 
For example, date of birth and 
220 
00:15:59,770 --> 00:16:02,410 
corporation name possibly
will never co-occur together. 
221 
00:16:04,050 --> 00:16:07,250 
But for
large schemas with large data volumes, 
222 
00:16:07,250 --> 00:16:11,980 
one can estimate these measures by
taking samples from the actual database 
223 
00:16:11,980 --> 00:16:16,312 
to come up with reasonable
similarity co-occurrence scores. 
224 
00:16:16,312 --> 00:16:22,658 
To illustrate attribute regrouping, we
take a significant re-simplified example. 
225 
00:16:22,658 --> 00:16:27,543 
Here we want to create customer
transactions as a mediated relation 
226 
00:16:27,543 --> 00:16:32,180 
based upon the bank transactions and
insurance transactions. 
227 
00:16:33,530 --> 00:16:36,159 
Each attribute is given
an abbreviation for simplicity. 
228 
00:16:38,030 --> 00:16:41,575 
Below, you can see three
possible mediated schemas. 
229 
00:16:42,770 --> 00:16:47,140 
In the first one,
the transaction begin and end times 
230 
00:16:47,140 --> 00:16:52,270 
from bank transactions are grouped into
the same cluster as transaction date time 
231 
00:16:52,270 --> 00:16:56,040 
from the insurance transactions,
because all of them are the same type. 
232 
00:16:57,280 --> 00:17:02,000 
Similarly, transaction party, that
means who is giving or receiving money, 
233 
00:17:03,010 --> 00:17:06,610 
and transaction description are grouped
together with transaction details. 
234 
00:17:08,490 --> 00:17:11,130 
The second schema keeps
all of them separate. 
235 
00:17:12,320 --> 00:17:16,145 
And the third candidate schema
groups some of them and not others. 
236 
00:17:19,998 --> 00:17:23,860 
Now that we have multiple mediated
schemas, which one should we choose? 
237 
00:17:25,030 --> 00:17:27,990 
Now I'm presenting here
a qualitative account of the method. 
238 
00:17:29,280 --> 00:17:32,730 
The primary goal is to look for
what we can call consistency. 
239 
00:17:34,430 --> 00:17:38,260 
A source schema is consistent
with a mediated schema if 
240 
00:17:38,260 --> 00:17:43,070 
two different attributes of the source
schema do not occur in one cluster. 
241 
00:17:44,190 --> 00:17:48,990 
And in the example, Med3,
that means related schema three, 
242 
00:17:48,990 --> 00:17:54,560 
is more consistent with bank
transactions because, unlike Med1, 
243 
00:17:54,560 --> 00:17:58,700 
it keeps TBT,
TET in two different clusters. 
244 
00:17:59,800 --> 00:18:06,090 
Once this is done, we can count the number
of consistent sources for each candidate 
245 
00:18:06,090 --> 00:18:10,650 
mediated schema and then use this count
to come up with a probability estimate. 
246 
00:18:11,890 --> 00:18:16,100 
This estimate can then be used
to choose the k best schemas. 
247 
00:18:17,590 --> 00:18:22,030 
Should one ever choose more
than one just best schema? 
248 
00:18:22,030 --> 00:18:23,860 
Well, that's a hard question
to answer in general. 
249 
00:18:25,280 --> 00:18:29,932 
It is done when the top capability
estimates are very close to each other. 
1 
00:00:00,025 --> 00:00:04,576 
[SOUND] The Splunk platform for
operational intelligence is 
2 
00:00:04,576 --> 00:00:10,493 
a revolutionary suite of products that
is unlocking unprecedented value for 
3 
00:00:10,493 --> 00:00:13,779 
thousands of customers around the world. 
4 
00:00:13,779 --> 00:00:15,970 
Why Splunk? 
5 
00:00:15,970 --> 00:00:18,420 
It all starts with machine data. 
6 
00:00:18,420 --> 00:00:20,970 
Machine data is the big data generated by 
7 
00:00:20,970 --> 00:00:23,710 
all the technologies that
power our businesses. 
8 
00:00:23,710 --> 00:00:28,320 
From the applications, servers, websites,
and network devices in the data center and 
9 
00:00:28,320 --> 00:00:31,300 
the cloud, to the mobile device
in the palm of your hand. 
10 
00:00:32,340 --> 00:00:37,460 
Thermostats, train sensors, electric cars,
and the Internet of things. 
11 
00:00:37,460 --> 00:00:39,412 
Machine data is everywhere. 
12 
00:00:39,412 --> 00:00:41,710 
It's fast-growing and complex. 
13 
00:00:41,710 --> 00:00:44,550 
It's also incredibly valuable, why? 
14 
00:00:44,550 --> 00:00:48,630 
Because it contains a definitive
record of all activity and behavior. 
15 
00:00:49,970 --> 00:00:54,380 
Splunk software collects and
indexes this data at massive scale, 
16 
00:00:54,380 --> 00:00:58,340 
from wherever it's generated,
regardless of format or source. 
17 
00:00:58,340 --> 00:01:02,290 
Users can quickly and
easily monitor, search, analyze, and 
18 
00:01:02,290 --> 00:01:05,680 
report on their data, all in real time. 
19 
00:01:05,680 --> 00:01:07,640 
Machine data is different. 
20 
00:01:07,640 --> 00:01:11,450 
It can't be processed and
analyzed using traditional methods. 
21 
00:01:11,450 --> 00:01:16,540 
Splunk software does not rely on brittle
schemas and inflexible databases. 
22 
00:01:16,540 --> 00:01:19,660 
Splunk is easy to deploy, easy to use, and 
23 
00:01:19,660 --> 00:01:25,290 
easy to scale, whether on premises or
in a public, private, or hybrid cloud. 
24 
00:01:25,290 --> 00:01:27,960 
Splunk is also available
as a cloud service. 
25 
00:01:27,960 --> 00:01:31,942 
And for big data environments that
used Hadoop for cheap app storage, 
26 
00:01:31,942 --> 00:01:34,522 
we have Hunk, Splunk analytics for Hadoop. 
27 
00:01:34,522 --> 00:01:36,950 
[MUSIC] 
28 
00:01:36,950 --> 00:01:39,843 
Our customers from around
the world illustrate why so 
29 
00:01:39,843 --> 00:01:42,070 
many organizations use Splunk. 
30 
00:01:42,070 --> 00:01:46,710 
Intuit has standardized on Splunk,
delivering operational visibility for 
31 
00:01:46,710 --> 00:01:52,060 
their leading online products, including
QuickBooks, Quicken, and TurboTax. 
32 
00:01:52,060 --> 00:01:55,770 
Intuit considers Splunk one of
their cornerstone technologies 
33 
00:01:55,770 --> 00:01:59,950 
that is helping them innovate and
deliver better service to their customers. 
34 
00:01:59,950 --> 00:02:03,210 
Cisco, one of the world's
largest technology providers, 
35 
00:02:03,210 --> 00:02:06,940 
empowers their global security
team with Splunk enterprise 
36 
00:02:06,940 --> 00:02:11,220 
to gain a centralized view into
end user and system activities. 
37 
00:02:11,220 --> 00:02:15,000 
Splunk has dramatically helped
improve their incident detection and 
38 
00:02:15,000 --> 00:02:15,690 
response rate. 
39 
00:02:17,000 --> 00:02:20,650 
Splunk was key to Domino's Pizza's
success during the Super Bowl, 
40 
00:02:20,650 --> 00:02:24,020 
by monitoring database uptime and
order response. 
41 
00:02:24,020 --> 00:02:26,710 
Armed with new levels of
customer understanding, 
42 
00:02:26,710 --> 00:02:30,490 
Domino's was able to strengthen their
online business, in addition to saving 
43 
00:02:30,490 --> 00:02:34,960 
hundreds of thousands of dollars replacing
legacy technologies with Splunk. 
44 
00:02:34,960 --> 00:02:39,350 
We are thrilled to hear Domino's
call Splunk their secret sauce. 
45 
00:02:39,350 --> 00:02:43,200 
Another great Splunk story includes
cars and the Internet of things. 
46 
00:02:44,810 --> 00:02:48,813 
Working together with the Ford Motor
Company and Ford's OpenXC platform, 
47 
00:02:48,813 --> 00:02:52,816 
Splunk delivered connected car dashboards
to examine driving behavior and 
48 
00:02:52,816 --> 00:02:54,079 
vehicle performance. 
49 
00:02:54,079 --> 00:02:58,529 
UK-based Tesco is one of
the world's largest retailers, 
50 
00:02:58,529 --> 00:03:04,650 
with nearly 1 million customers and
a half billion online orders per week. 
51 
00:03:04,650 --> 00:03:07,480 
Customer satisfaction
is a critical metric. 
52 
00:03:07,480 --> 00:03:12,580 
Tesco deployed Splunk to gain a unified
view across their websites, transactions, 
53 
00:03:12,580 --> 00:03:17,410 
and business, gaining valuable digital
intelligence about their customers. 
54 
00:03:17,410 --> 00:03:22,820 
Splunk was founded to pursue a disruptive
vision to make machine data accessible, 
55 
00:03:22,820 --> 00:03:25,490 
usable, and valuable to everyone. 
56 
00:03:25,490 --> 00:03:27,280 
Find out more about Splunk and 
57 
00:03:27,280 --> 00:03:30,820 
operational intelligence by
downloading the executive summary. 
58 
00:03:30,820 --> 00:03:31,660 
Or better yet, 
59 
00:03:31,660 --> 00:03:36,230 
experience the value firsthand by
downloading Splunk software for free. 
60 
00:03:36,230 --> 00:03:39,565 
Chances are someone in your
organization already has. 
61 
00:03:39,565 --> 00:03:45,649 
[SOUND] 
1 
00:00:02,630 --> 00:00:04,630 
Aggregations in Big Data Pipelines. 
2 
00:00:07,220 --> 00:00:10,680 
After this video you will
be able to compare and 
3 
00:00:10,680 --> 00:00:15,740 
select the aggregation operation that
you require to solve your problem. 
4 
00:00:15,740 --> 00:00:21,650 
Explain how you can use aggregations to
compact your dataset and reduce volume. 
5 
00:00:21,650 --> 00:00:22,890 
That is in many cases. 
6 
00:00:24,260 --> 00:00:29,586 
And design complex operations in your
pipeline, using a series of aggregations. 
7 
00:00:32,313 --> 00:00:39,189 
Aggregation is any operation on a data set
that performs a specific transformation, 
8 
00:00:39,189 --> 00:00:43,979 
taking all the related data
elements into consideration. 
9 
00:00:45,060 --> 00:00:49,550 
Let's say we have a bunch of stars
which are of different colors. 
10 
00:00:50,570 --> 00:00:54,550 
Different colors denote diversity or
variety in the data. 
11 
00:00:56,260 --> 00:01:02,610 
To keep things simple, we will use
letter 'f' to denote a transformation. 
12 
00:01:03,700 --> 00:01:04,720 
In the following slides, 
13 
00:01:04,720 --> 00:01:10,360 
we will see examples of how 'f' can take
the shape of different transformations. 
14 
00:01:13,550 --> 00:01:18,390 
If we apply a transformation that does
something using the information of 
15 
00:01:18,390 --> 00:01:22,430 
all the stars here,
we are performing an aggregation. 
16 
00:01:24,070 --> 00:01:29,850 
Loosely speaking, we can say
that applying a transformation 'f' 
17 
00:01:29,850 --> 00:01:34,830 
that takes all the elements of data
as input is called 'aggregation'. 
18 
00:01:38,490 --> 00:01:44,400 
One of the simplest aggregations is
summation over all the data elements. 
19 
00:01:44,400 --> 00:01:48,750 
In this case,
let's say every star counted as 1. 
20 
00:01:48,750 --> 00:01:52,586 
Summing over all the stars gives 14, 
21 
00:01:52,586 --> 00:01:57,119 
which is the summation of 3 stars for
yellow, 
22 
00:01:57,119 --> 00:02:01,897 
5 stars for green, and
6 stars for color pink. 
23 
00:02:04,779 --> 00:02:10,513 
Another aggregation that you could perform
is summation of individual star colors, 
24 
00:02:10,513 --> 00:02:13,070 
that is, grouping the sums by color. 
25 
00:02:14,200 --> 00:02:20,360 
So, If each star is a 1,
adding each group will result in 3 for 
26 
00:02:20,360 --> 00:02:25,530 
yellow stars, 5 for green stars,
and 6 for pink stars. 
27 
00:02:26,860 --> 00:02:27,952 
In this case, 
28 
00:02:27,952 --> 00:02:33,611 
the aggregation function 'f' will output
3 tuples of star colors and counts. 
29 
00:02:36,065 --> 00:02:41,190 
In a sales scenario, each color could
denote a different product type. 
30 
00:02:42,470 --> 00:02:47,320 
And the number 1 could be replaced
by revenue generated by a product 
31 
00:02:47,320 --> 00:02:49,230 
in each city the product is sold. 
32 
00:02:50,600 --> 00:02:54,250 
In fact,
we will keep coming back to this analogy. 
33 
00:02:56,420 --> 00:03:01,188 
You can also perform average
over items of similar kind, 
34 
00:03:01,188 --> 00:03:06,150 
such as sums grouped by color. 
35 
00:03:08,060 --> 00:03:10,630 
Continuing the example earlier, 
36 
00:03:10,630 --> 00:03:15,060 
you can calculate average revenue per
product type using this aggregation. 
37 
00:03:17,530 --> 00:03:22,270 
Other simple yet useful aggregational
operations to help you extract meaning 
38 
00:03:22,270 --> 00:03:29,010 
from large data sets are maximum,
minimum, and standard deviation. 
39 
00:03:29,010 --> 00:03:34,090 
Remember, you can always perform
aggregation as a series of operations, 
40 
00:03:34,090 --> 00:03:38,140 
such as maximum of the sums per product. 
41 
00:03:38,140 --> 00:03:41,510 
That is, summation followed by maximum. 
42 
00:03:42,725 --> 00:03:46,920 
If you first sum sales for
each city, that is for 
43 
00:03:46,920 --> 00:03:51,640 
each product,
then you can take the maximum of it 
44 
00:03:51,640 --> 00:03:57,050 
by applying maximum function to
the result of the summation function. 
45 
00:03:57,050 --> 00:04:01,870 
In this case, you get the product,
which has maximum sales in the country. 
46 
00:04:04,040 --> 00:04:08,084 
Aggregation over Boolean data
sets that can have true-false or 
47 
00:04:08,084 --> 00:04:14,430 
one-zero values could be a complex mixture
of AND, OR, and NOT logical operations. 
48 
00:04:16,030 --> 00:04:20,110 
A lot of problems become easy
to manipulate using sets. 
49 
00:04:20,110 --> 00:04:24,008 
Because sets don't allow duplicate values. 
50 
00:04:24,008 --> 00:04:27,710 
Depending on your application,
this could be very useful. 
51 
00:04:28,800 --> 00:04:33,730 
For example, to count the number
of products from a sales table, 
52 
00:04:33,730 --> 00:04:38,920 
you can simply take all
the sales tables and create sets 
53 
00:04:38,920 --> 00:04:44,130 
of these products in those tables,
and take a union of these sets. 
54 
00:04:46,090 --> 00:04:52,130 
To summarize, by choosing the right
aggregation, you can generate compact and 
55 
00:04:52,130 --> 00:04:58,720 
meaningful insights that enable faster and
effective decision making in business. 
56 
00:04:58,720 --> 00:05:00,960 
You will find that in most cases, 
57 
00:05:00,960 --> 00:05:04,270 
aggregation results in
smaller output data sets. 
58 
00:05:05,450 --> 00:05:10,140 
Hence, aggregation is an important tool
set to keep in pocket when dealing with 
59 
00:05:10,140 --> 00:05:12,669 
large data sets, and big data pipelines. 
1 
00:00:02,723 --> 00:00:05,960 
Big Data Processing Pipelines:
A Dataflow Approach. 
2 
00:00:07,160 --> 00:00:11,420 
Most big data applications
are composed of a set of operations 
3 
00:00:11,420 --> 00:00:13,980 
executed one after another as a pipeline. 
4 
00:00:14,980 --> 00:00:17,520 
Data flows through these operations, 
5 
00:00:17,520 --> 00:00:20,177 
going through various
transformations along the way. 
6 
00:00:20,177 --> 00:00:24,080 
We also call this dataflow graphs. 
7 
00:00:24,080 --> 00:00:27,625 
So to understand big data
processing we should start by 
8 
00:00:27,625 --> 00:00:30,100 
understanding what dataflow means. 
9 
00:00:31,820 --> 00:00:36,781 
After this video you will be able to
summarize what dataflow means and 
10 
00:00:36,781 --> 00:00:38,844 
it's role in data science. 
11 
00:00:38,844 --> 00:00:44,096 
Explain split->do->merge as a big
data pipeline with examples, 
12 
00:00:44,096 --> 00:00:46,770 
and define the term data parallel. 
13 
00:00:48,910 --> 00:00:53,484 
Let's consider the hello
world MapReduce example for 
14 
00:00:53,484 --> 00:00:57,461 
WordCount which reads one or
more text files and 
15 
00:00:57,461 --> 00:01:03,043 
counts the number of occurrences
of each word in these text files. 
16 
00:01:03,043 --> 00:01:07,483 
You are by now very familiar with
this example, but as a reminder, 
17 
00:01:07,483 --> 00:01:10,891 
the output will be a text
file with a list of words and 
18 
00:01:10,891 --> 00:01:14,250 
their occurrence frequencies
in the input data. 
19 
00:01:16,451 --> 00:01:20,985 
In this application,
the files were first split into HDFS 
20 
00:01:20,985 --> 00:01:25,990 
cluster nodes as partitions of
the same file or multiple files. 
21 
00:01:27,560 --> 00:01:30,680 
Then a map operation, in this case, 
22 
00:01:30,680 --> 00:01:36,340 
a user defined function to count words
was executed on each of these nodes. 
23 
00:01:36,340 --> 00:01:42,460 
And all the key values that were output
from map were sorted based on the key. 
24 
00:01:42,460 --> 00:01:47,380 
And the key values with the same word
were moved or shuffled to the same node. 
25 
00:01:48,920 --> 00:01:54,130 
Finally, the reduce operation
was executed on these nodes 
26 
00:01:54,130 --> 00:01:57,900 
to add the values for
key-value pairs with the same keys. 
27 
00:01:59,920 --> 00:02:05,880 
If you look back at this example, we see
that there were four distinct steps, 
28 
00:02:05,880 --> 00:02:11,070 
namely the data split step,
the map step, the shuffle and 
29 
00:02:11,070 --> 00:02:13,030 
sort step, and the reduce step. 
30 
00:02:14,310 --> 00:02:18,417 
Although, the word count
example is pretty simple it 
31 
00:02:18,417 --> 00:02:22,980 
represents a large number of
applications that these three 
32 
00:02:22,980 --> 00:02:27,652 
steps can be applied to achieve
data parallel scalability. 
33 
00:02:27,652 --> 00:02:33,290 
We refer in general to this
pattern as "split-do-merge". 
34 
00:02:35,040 --> 00:02:41,490 
In these applications, data flows
through a number of steps, going through 
35 
00:02:41,490 --> 00:02:46,990 
transformations with various scalability
needs, leading to a final product. 
36 
00:02:48,180 --> 00:02:50,310 
The data first gets partitioned. 
37 
00:02:51,430 --> 00:02:56,450 
The split data goes through a set of
user-defined functions to do something, 
38 
00:02:57,840 --> 00:03:02,420 
ranging from statistical operations to
data joins to machine learning functions. 
39 
00:03:03,870 --> 00:03:08,510 
Depending on the application's
data processing needs, 
40 
00:03:08,510 --> 00:03:13,350 
these "do something" operations can
differ and can be chained together. 
41 
00:03:14,620 --> 00:03:20,130 
In the end results can be combined
using a merging algorithm or 
42 
00:03:20,130 --> 00:03:22,300 
a higher-order function like reduce. 
43 
00:03:23,580 --> 00:03:27,520 
We call the stitched-together
version of these sets of steps for 
44 
00:03:27,520 --> 00:03:30,970 
big data processing "big data pipelines". 
45 
00:03:33,700 --> 00:03:38,180 
The term pipe comes from
a UNIX separation that 
46 
00:03:38,180 --> 00:03:43,490 
the output of one running program gets
piped into the next program as an input. 
47 
00:03:43,490 --> 00:03:47,870 
As you might imagine,
one can string multiple programs together 
48 
00:03:47,870 --> 00:03:52,560 
to make longer pipelines with various
scalability needs at each step. 
49 
00:03:53,740 --> 00:03:56,750 
However, for big data processing, 
50 
00:03:56,750 --> 00:04:02,110 
the parallelism of each step in
the pipeline is mainly data parallelism. 
51 
00:04:02,110 --> 00:04:07,284 
We can simply define data parallelism
as running the same functions 
52 
00:04:07,284 --> 00:04:13,380 
simultaneously for the elements or
partitions of a dataset on multiple cores. 
53 
00:04:13,380 --> 00:04:17,447 
For example,
in our word count example, data 
54 
00:04:17,447 --> 00:04:21,720 
parallelism occurs in every
step of the pipeline. 
55 
00:04:22,770 --> 00:04:26,530 
There's definitely parallelization
during map over the input 
56 
00:04:26,530 --> 00:04:30,460 
as each partition gets
processed as a line at a time. 
57 
00:04:30,460 --> 00:04:32,920 
To achieve this type of data parallelism, 
58 
00:04:32,920 --> 00:04:37,560 
we must decide on the data granularity
of each parallel computation. 
59 
00:04:37,560 --> 00:04:38,930 
In this case, it is a line. 
60 
00:04:41,022 --> 00:04:46,560 
We also see a parallel grouping of
data in the shuffle and sort phase. 
61 
00:04:46,560 --> 00:04:50,630 
This time, the parallelization is
over the intermediate products, 
62 
00:04:50,630 --> 00:04:53,050 
that is, the individual key-value pairs. 
63 
00:04:55,140 --> 00:04:58,810 
And after the grouping of
the intermediate products 
64 
00:04:58,810 --> 00:05:03,270 
the reduce step gets parallelized
to construct one output file. 
65 
00:05:04,360 --> 00:05:08,470 
You have probably noticed that
the data gets reduced to a smaller set 
66 
00:05:08,470 --> 00:05:09,060 
at each step. 
67 
00:05:11,370 --> 00:05:13,890 
Although, the example we have given is for 
68 
00:05:13,890 --> 00:05:18,020 
batch processing, similar techniques
apply to stream processing. 
69 
00:05:19,060 --> 00:05:20,740 
Let's discuss this for 
70 
00:05:20,740 --> 00:05:24,690 
our simplified advanced stream
data from an online game example. 
71 
00:05:25,910 --> 00:05:30,070 
In this case, your event gets ingested 
72 
00:05:30,070 --> 00:05:34,960 
through a real time big data ingestion
engine, like Kafka or Flume. 
73 
00:05:36,380 --> 00:05:40,400 
Then they get passed into
a Streaming Data Platform for 
74 
00:05:40,400 --> 00:05:44,670 
processing like Samza,
Storm or Spark streaming. 
75 
00:05:45,750 --> 00:05:51,910 
This is a valid choice for processing
data one event at a time or chunking 
76 
00:05:51,910 --> 00:05:57,390 
the data into Windows or Microbatches
of time or other features. 
77 
00:05:58,940 --> 00:06:04,470 
Any pipeline processing of data can
be applied to the streaming data here 
78 
00:06:04,470 --> 00:06:07,410 
as we wrote in a batch-
processing Big Data engine. 
79 
00:06:09,350 --> 00:06:15,034 
The process stream data can then be
served through a real-time view or 
80 
00:06:15,034 --> 00:06:17,279 
a batch-processing view. 
81 
00:06:17,279 --> 00:06:22,380 
Real-time view is often subject to change
as potentially delayed new data comes in. 
82 
00:06:23,590 --> 00:06:28,490 
The storage of the data can be
accomplished using H-Base, Cassandra, 
83 
00:06:28,490 --> 00:06:32,660 
HDFS, or
many other persistent storage systems. 
84 
00:06:34,120 --> 00:06:39,110 
To summarize, big data pipelines
get created to process data 
85 
00:06:39,110 --> 00:06:43,990 
through an aggregated set of steps that
can be represented with the split- 
86 
00:06:43,990 --> 00:06:48,370 
do-merge pattern with
data parallel scalability. 
87 
00:06:48,370 --> 00:06:51,040 
This pattern can be
applied to many batch and 
88 
00:06:51,040 --> 00:06:54,100 
streaming data processing applications. 
89 
00:06:54,100 --> 00:06:58,958 
Next we will go through some processing
steps in a big data pipeline in 
90 
00:06:58,958 --> 00:07:03,589 
more detail, first conceptually,
then practically in Spark. 
1 
00:00:01,140 --> 00:00:05,070 
Now that we went through an overview
of the Spark ecosystem and 
2 
00:00:05,070 --> 00:00:09,640 
the components of the Spark stack, it is
time for us to start learning more about 
3 
00:00:09,640 --> 00:00:13,960 
its architecture and run our first
Spark program in the cloud VM. 
4 
00:00:15,580 --> 00:00:19,710 
After this video you will be
able to describe how Spark 
5 
00:00:19,710 --> 00:00:24,690 
does in-memory processing using the
Resilient Distributed Dataset abstraction, 
6 
00:00:25,890 --> 00:00:29,740 
explain the inner workings of
the Spark architecture, and 
7 
00:00:29,740 --> 00:00:34,090 
summarize how Spark manages and
executes code on Clusters. 
8 
00:00:35,510 --> 00:00:39,531 
I mentioned a few times that
Spark is efficient because it 
9 
00:00:39,531 --> 00:00:43,739 
uses an abstraction called RDDs for
in memory processing. 
10 
00:00:45,704 --> 00:00:48,970 
What this means might not be
clear to some of you yet. 
11 
00:00:50,170 --> 00:00:52,660 
Let's remember the alternative. 
12 
00:00:52,660 --> 00:00:57,153 
In Hadoop MapReduce,
each step, also pipeline, 
13 
00:00:57,153 --> 00:01:02,272 
reads from disk to memory,
performs the computations and 
14 
00:01:02,272 --> 00:01:06,770 
writes back its output from
the memory to the disk. 
15 
00:01:08,290 --> 00:01:13,520 
However, writing data to disk
is a costly operation and 
16 
00:01:13,520 --> 00:01:17,220 
this cost becomes even more
with large volumes of data. 
17 
00:01:18,650 --> 00:01:20,010 
Here is an interesting fact. 
18 
00:01:21,320 --> 00:01:25,664 
Memory operations can
be up to 100,000 times 
19 
00:01:25,664 --> 00:01:28,250 
faster than disk operations in some cases. 
20 
00:01:29,510 --> 00:01:33,430 
Spark instead takes advantage of this and
allows for 
21 
00:01:33,430 --> 00:01:37,810 
immediate results of transformations
in different stages of the pipeline and 
22 
00:01:37,810 --> 00:01:40,650 
memory, like MAP and REDUCE here. 
23 
00:01:41,980 --> 00:01:45,540 
Here, we see that the outputs
of MAP operations 
24 
00:01:45,540 --> 00:01:49,890 
are shared with reduce operations
without being written to the disk. 
25 
00:01:50,980 --> 00:01:55,900 
The containers where the data
gets stored in memory 
26 
00:01:55,900 --> 00:02:00,370 
are called resilient distributed
datasets or RDDs for short. 
27 
00:02:02,210 --> 00:02:06,730 
RDDs are how Spark distributes data and
computations across 
28 
00:02:06,730 --> 00:02:12,180 
the nodes of a commodity cluster,
preferably with large memory. 
29 
00:02:12,180 --> 00:02:14,380 
Thanks to this abstraction, 
30 
00:02:14,380 --> 00:02:18,790 
Spark has proven to be 100 times
faster for some applications. 
31 
00:02:20,360 --> 00:02:23,960 
Let's define all the words
in this interesting name 
32 
00:02:23,960 --> 00:02:26,510 
beginning with the last one, datasets. 
33 
00:02:28,196 --> 00:02:36,790 
Datasets that RDD distributes comes
from a batch data storage like HDFS, 
34 
00:02:36,790 --> 00:02:42,120 
no SQL databases, text files or streaming
data ingestion systems like Cafco. 
35 
00:02:43,330 --> 00:02:47,760 
It can even conveniently read and
distribute the data from your local disc 
36 
00:02:47,760 --> 00:02:52,600 
like text files into Spark, or
even a hierarchy of folders. 
37 
00:02:54,750 --> 00:03:01,200 
When Spark reads data from these sources,
it generates RDDs for them. 
38 
00:03:01,200 --> 00:03:07,930 
The Spark operations can transform RDDs
into other RDDs like any other data. 
39 
00:03:07,930 --> 00:03:13,650 
Here it's important to mention
that RDDs are immutable. 
40 
00:03:13,650 --> 00:03:17,610 
This means that you cannot
change them partially. 
41 
00:03:17,610 --> 00:03:22,970 
However, you can create new RDDs by
a series of one or many transformations. 
42 
00:03:24,570 --> 00:03:28,200 
Next, let's look at what distributed
means in the RDD context. 
43 
00:03:29,480 --> 00:03:34,680 
As I mentioned before, RDDs distribute
partitioned data collections and 
44 
00:03:34,680 --> 00:03:40,740 
computations on clusters even
across a number of machines. 
45 
00:03:41,770 --> 00:03:44,860 
For example, running on the Amazon Cloud. 
46 
00:03:46,160 --> 00:03:50,880 
The complexity of this operation is
hidden by this very simple interface. 
47 
00:03:52,140 --> 00:03:57,230 
Computations are a diverse set of
transformations of RDDs like map, 
48 
00:03:57,230 --> 00:03:59,230 
filter and join. 
49 
00:03:59,230 --> 00:04:05,940 
And also actions on the RDDs like counting
and saving them persistently on disk. 
50 
00:04:05,940 --> 00:04:10,560 
The partitioning of data can be changed
dynamically to optimize Spark's 
51 
00:04:10,560 --> 00:04:11,170 
performance. 
52 
00:04:12,620 --> 00:04:17,420 
The last element is resilient, and
it's very important because in a large 
53 
00:04:17,420 --> 00:04:21,640 
scale computing environment it is
pretty common to have node failures. 
54 
00:04:22,850 --> 00:04:25,710 
It's very important to be
able to recover from these 
55 
00:04:25,710 --> 00:04:28,480 
situations without losing
any work already done. 
56 
00:04:29,570 --> 00:04:34,360 
For full tolerance in such situations,
Spark tracks the history of each 
57 
00:04:34,360 --> 00:04:39,220 
partition, keeping a lineage
over RDDs over time, 
58 
00:04:39,220 --> 00:04:44,170 
so every point in your calculations,
Spark knows which are the partitions 
59 
00:04:44,170 --> 00:04:47,560 
needed to recreate the partition
in case it gets lost. 
60 
00:04:48,710 --> 00:04:50,480 
And if that happens, 
61 
00:04:50,480 --> 00:04:55,730 
then Spark automatically figures out
where it can start the recompute from and 
62 
00:04:55,730 --> 00:04:59,890 
optimizes the amount of processing
needed to recover from the failure. 
63 
00:05:01,410 --> 00:05:06,087 
Before we create our first Spark program
using RDDs in Pi Spark in the cloud 
64 
00:05:06,087 --> 00:05:09,343 
layer VM,
let's review Spark's architecture. 
65 
00:05:12,239 --> 00:05:16,772 
From a bird's eye view,
Spark has two main components, 
66 
00:05:16,772 --> 00:05:19,610 
a driver program and worker nodes. 
67 
00:05:21,900 --> 00:05:25,300 
The driver program is where
your application starts. 
68 
00:05:26,900 --> 00:05:30,760 
It distributes RDDs on your
computational cluster and 
69 
00:05:30,760 --> 00:05:35,435 
makes sure the transformations and
actions on these RDDs are performed. 
70 
00:05:37,530 --> 00:05:42,250 
Driver programs create
a connection to a Spark cluster or 
71 
00:05:42,250 --> 00:05:45,950 
your local Spark through
a Spark context object. 
72 
00:05:47,010 --> 00:05:50,770 
The default Spark context
in the Spark shell is 
73 
00:05:50,770 --> 00:05:54,680 
an object called SC for Spark context. 
74 
00:05:55,980 --> 00:06:01,260 
For example, in the upcoming reading for
creating word counts in Spark, 
75 
00:06:01,260 --> 00:06:05,252 
we will use SC as the context to 
76 
00:06:05,252 --> 00:06:10,210 
generate RDDs for a text file
using the line of code shown here. 
77 
00:06:11,650 --> 00:06:17,340 
The driver program manages a potentially
large number of nodes called worker nodes. 
78 
00:06:18,780 --> 00:06:23,740 
On a local computer, we can assume
that there's only one worker node and 
79 
00:06:23,740 --> 00:06:25,590 
it is where the Spark operations execute. 
80 
00:06:27,100 --> 00:06:32,230 
A worker node in Spark keeps
a running Java virtual machine, 
81 
00:06:32,230 --> 00:06:36,000 
called JVM commonly, called the executor. 
82 
00:06:38,080 --> 00:06:40,913 
Depending on the illustration, 
83 
00:06:40,913 --> 00:06:45,566 
executor can execute task
related to mapping stages or 
84 
00:06:45,566 --> 00:06:50,130 
reducing stages or
other Spark specific pipelines. 
85 
00:06:50,130 --> 00:06:56,642 
This Java virtual machine is the core
that all the computation is executed, 
86 
00:06:56,642 --> 00:07:03,780 
and this is the interface also to the rest
of the Big Data storage systems and tools. 
87 
00:07:05,320 --> 00:07:10,560 
For example, if we ever had the Hadoop
file system, HTFS, as the storage system, 
88 
00:07:10,560 --> 00:07:14,650 
then on each worker node,
some of the data will be stored locally. 
89 
00:07:14,650 --> 00:07:19,445 
As you know, the most important point
of this computing framework is to bring 
90 
00:07:19,445 --> 00:07:21,084 
the computation to data. 
91 
00:07:21,084 --> 00:07:24,770 
So Spark will send some
computational jobs to be executed 
92 
00:07:24,770 --> 00:07:30,190 
on the data that are already available
on the machine thanks to HDFS. 
93 
00:07:30,190 --> 00:07:32,930 
Data will be read from HDFS and 
94 
00:07:32,930 --> 00:07:38,580 
get processed in memory,
the results will be stored as one RDD. 
95 
00:07:38,580 --> 00:07:45,012 
The actual computation is running
straight in the executor, 
96 
00:07:45,012 --> 00:07:50,214 
that is the JVM that runs your Scala or
Java codes. 
97 
00:07:50,214 --> 00:07:54,970 
Instead, if you are using PySpark
then there will be several Python 
98 
00:07:54,970 --> 00:07:57,723 
processes generally, one for task but 
99 
00:07:57,723 --> 00:08:01,490 
you can configure it depending
on your application. 
100 
00:08:03,999 --> 00:08:09,770 
In a real Big Data scenario, we have many
worker nodes running tasks internally. 
101 
00:08:10,990 --> 00:08:15,220 
It is important to have a system that can
automatically manage provisioning and 
102 
00:08:15,220 --> 00:08:16,420 
restarting of these nodes. 
103 
00:08:17,540 --> 00:08:20,550 
The cluster manager in
Spark has this capability. 
104 
00:08:21,970 --> 00:08:27,660 
Spark currently supports mainly three
interfaces for cluster management, 
105 
00:08:27,660 --> 00:08:34,620 
namely Spark's standalone cluster manager,
the Apache Mesos, and Hadoop YARN. 
106 
00:08:36,190 --> 00:08:40,441 
Standalone means that there's a special
Spark process that takes care of 
107 
00:08:40,441 --> 00:08:42,567 
restarting nodes that are failing or 
108 
00:08:42,567 --> 00:08:45,668 
starting nodes at the beginning
of the computation. 
109 
00:08:45,668 --> 00:08:50,276 
YARN and Mesos are two external research
measures that can be used also for 
110 
00:08:50,276 --> 00:08:51,400 
these purposes. 
111 
00:08:54,080 --> 00:08:57,400 
Choosing a cluster manager
to fit your application and 
112 
00:08:57,400 --> 00:09:00,280 
infrastructure can be quite confusing. 
113 
00:09:00,280 --> 00:09:04,020 
Here, we give you a good
article as a starting point on 
114 
00:09:04,020 --> 00:09:07,030 
how to pick the right cluster manager for
your organization. 
115 
00:09:08,470 --> 00:09:13,130 
To summarize, the Spark architecture
includes a driver program. 
116 
00:09:14,270 --> 00:09:18,918 
The driver program communicates
with the cluster manager for 
117 
00:09:18,918 --> 00:09:22,404 
monitoring and
provisioning of resources and 
118 
00:09:22,404 --> 00:09:27,783 
communicates directly with worker
nodes to submit and execute tasks. 
119 
00:09:27,783 --> 00:09:33,410 
RDDs get created and passed within
transformations running in the executable. 
120 
00:09:34,900 --> 00:09:39,691 
Finally, let's see how this
setup works on the Cloudera VM. 
121 
00:09:39,691 --> 00:09:43,797 
In the Cloudera VM,
we are using Spark in standalone mode and 
122 
00:09:43,797 --> 00:09:46,820 
everything is running locally. 
123 
00:09:46,820 --> 00:09:53,615 
So it's a single machine and on the same
machine we have our driver program, 
124 
00:09:53,615 --> 00:09:58,089 
the executor JVM and
our single PySpark process. 
125 
00:09:58,089 --> 00:10:02,771 
With that, we are ready to start with
our first reading to install Spark and 
126 
00:10:02,771 --> 00:10:06,946 
then running our word count program
using the Spark environment. 
1 
00:00:01,430 --> 00:00:05,170 
After a brief overview of some
of the processing systems 
2 
00:00:05,170 --> 00:00:09,480 
in the Big Data Landscape, it is time for
us to dive deeper into Spark. 
3 
00:00:10,820 --> 00:00:15,170 
Spark was initiated at
UC Berkeley in 2009 and 
4 
00:00:15,170 --> 00:00:19,840 
was transferred to
Apache Software Foundation in 2013. 
5 
00:00:19,840 --> 00:00:24,240 
Since then, Spark has become a top
level project with many users and 
6 
00:00:24,240 --> 00:00:25,700 
contributors worldwide. 
7 
00:00:27,550 --> 00:00:32,530 
After this video, you will be able
to list the main motivations for 
8 
00:00:32,530 --> 00:00:36,980 
the development of Spark,
draw the Spark stack as a layer diagram, 
9 
00:00:38,320 --> 00:00:42,130 
And explain the functionality of
the components in the Spark stack. 
10 
00:00:44,520 --> 00:00:49,810 
As we have discussed in our earlier
discussions, while Hadoop is great for 
11 
00:00:49,810 --> 00:00:53,460 
batch processing using
the MapReduce programming module, 
12 
00:00:53,460 --> 00:00:55,710 
it has shortcomings in a number of ways. 
13 
00:00:57,110 --> 00:01:02,000 
First of all, since it is limited to
Map and Reduce based transformations, 
14 
00:01:02,000 --> 00:01:07,020 
one has to restrict their big data
pipeline to map and reduce steps. 
15 
00:01:08,520 --> 00:01:11,680 
But the number of applications
can be implemented using Map and 
16 
00:01:11,680 --> 00:01:14,970 
Reduce, it's not always possible and 
17 
00:01:14,970 --> 00:01:18,810 
it is often not the most efficient
way to express a big data pipeline. 
18 
00:01:20,460 --> 00:01:25,920 
For example, you might want to do a join
operation between different data sets or 
19 
00:01:25,920 --> 00:01:28,430 
you might want to filter or
sample your data. 
20 
00:01:29,440 --> 00:01:32,460 
Or you might have a more
complicated data pipeline with 
21 
00:01:32,460 --> 00:01:36,680 
several steps including joins and
group byes. 
22 
00:01:36,680 --> 00:01:41,280 
It might have a Map and Reduce face,
but maybe another map face after that. 
23 
00:01:42,660 --> 00:01:48,140 
These types of operations are hard or
impossible to express using MapReduce and 
24 
00:01:48,140 --> 00:01:51,760 
cannot be accommodated by
the MapReduce framework in Hadoop. 
25 
00:01:53,080 --> 00:01:56,830 
Another important bottleneck in
Hadoop MapReduce that is critical for 
26 
00:01:56,830 --> 00:02:01,880 
performance, is that MapReduce relies
heavily on reading data from disc. 
27 
00:02:03,680 --> 00:02:08,470 
This is especially a problem for iterative
algorithms that require taking several 
28 
00:02:08,470 --> 00:02:12,800 
passes through the data using
a number of transformations. 
29 
00:02:12,800 --> 00:02:16,940 
Since each transformation will need
to read its inputs from the disk, 
30 
00:02:16,940 --> 00:02:20,670 
this will end up in a performance
bottleneck due to IO. 
31 
00:02:22,730 --> 00:02:25,610 
Most machine learning pipelines
are in this category, 
32 
00:02:25,610 --> 00:02:28,850 
making Hadoop MapReduce not ideal for
machine learning. 
33 
00:02:29,900 --> 00:02:34,070 
And as I mentioned in the system overview,
the only programming language 
34 
00:02:34,070 --> 00:02:38,190 
that MapReduce provides
a native interface for is Java. 
35 
00:02:38,190 --> 00:02:43,500 
Although, it's possible to run Python code
to implementation for it is more complex 
36 
00:02:43,500 --> 00:02:48,020 
and not very efficient especially when
you are running not with text data, but 
37 
00:02:48,020 --> 00:02:49,320 
with floating point numbers. 
38 
00:02:50,650 --> 00:02:54,417 
The programming language issue
also affects how interactive 
39 
00:02:54,417 --> 00:02:55,830 
the environment is. 
40 
00:02:55,830 --> 00:02:58,490 
Most data scientist
prefer to use scripting 
41 
00:02:58,490 --> 00:03:02,120 
languages due to their
interactive shell capabilities. 
42 
00:03:02,120 --> 00:03:06,670 
Not having such an interface in Hadoop
really makes it difficult to use and 
43 
00:03:06,670 --> 00:03:07,940 
adapt my many in the field. 
44 
00:03:09,530 --> 00:03:13,410 
In addition in the big data
era having support for 
45 
00:03:13,410 --> 00:03:16,280 
streaming data processing is a key for 
46 
00:03:16,280 --> 00:03:20,920 
being able to run similar analysis on
both real time and historical data. 
47 
00:03:22,380 --> 00:03:26,160 
Spark came out of the need to
extend the MapReduce framework 
48 
00:03:26,160 --> 00:03:28,690 
to overcome this shortcomings and 
49 
00:03:28,690 --> 00:03:33,080 
provide an expressive cluster computing
environment that can provide interactive 
50 
00:03:33,080 --> 00:03:37,820 
querying, efficient iterative analytics
and streaming data processing. 
51 
00:03:39,140 --> 00:03:42,970 
So, how does Apache Spark provide
solutions for these problems? 
52 
00:03:44,810 --> 00:03:48,820 
Spark provides a very rich and
expressive programming module that 
53 
00:03:48,820 --> 00:03:53,690 
gives you more than 20 highly efficient
distributed operations or transformations. 
54 
00:03:54,695 --> 00:03:58,810 
Pipe-lining any of these steps in Spark
simply takes a few lines of code. 
55 
00:04:00,470 --> 00:04:05,040 
Another important feature of Spark is
the ability to run these computations 
56 
00:04:05,040 --> 00:04:05,550 
in memory. 
57 
00:04:06,610 --> 00:04:10,350 
It's ability to cache and
process data in memory, 
58 
00:04:10,350 --> 00:04:14,325 
makes it significantly faster for
iterative applications. 
59 
00:04:14,325 --> 00:04:19,665 
This is proven to provide a factor
of ten or even 100 speed-up 
60 
00:04:19,665 --> 00:04:23,805 
in the performance of some algorithms,
especially using large data sets. 
61 
00:04:25,635 --> 00:04:30,625 
Additionally, Spark provides support for
batch and streaming workloads at once. 
62 
00:04:31,790 --> 00:04:36,675 
Last but not least,
Spark provides simple APIs for Python, 
63 
00:04:36,675 --> 00:04:41,570 
Scala, Java and SQL programming
through an interactive shell to 
64 
00:04:41,570 --> 00:04:46,590 
accomplish analytical tasks through both
external and its built-in libraries. 
65 
00:04:48,750 --> 00:04:52,500 
The Spark layer diagram,
also called Stack, 
66 
00:04:52,500 --> 00:04:57,250 
consists of components that build on
top of the Spark computational engine. 
67 
00:04:58,510 --> 00:05:04,860 
This engine distributes and monitors tasks
across the nodes of a commodity cluster. 
68 
00:05:05,960 --> 00:05:10,400 
The components built on top of this
engine are designed to interact and 
69 
00:05:10,400 --> 00:05:12,320 
communicate through this common engine. 
70 
00:05:13,430 --> 00:05:17,520 
Any improvements to
the underlying engine becomes 
71 
00:05:17,520 --> 00:05:21,980 
an improvement in the other components,
thanks to such close interaction. 
72 
00:05:23,390 --> 00:05:28,790 
This also enables building applications
that's span across these different 
73 
00:05:28,790 --> 00:05:33,950 
components like querying data using
Spark SQL and applying machine learning 
74 
00:05:33,950 --> 00:05:39,230 
algorithms, the query results using Sparks
machine learning library and MLlib. 
75 
00:05:41,120 --> 00:05:45,240 
The Spark Core is where
the core capability is 
76 
00:05:45,240 --> 00:05:48,520 
of the Spark Framework are implemented. 
77 
00:05:48,520 --> 00:05:50,010 
This includes support for 
78 
00:05:50,010 --> 00:05:54,460 
distributed scheduling,
memory management and full tolerance. 
79 
00:05:55,570 --> 00:05:59,460 
Interaction with different schedulers,
like YARN and Mesos and 
80 
00:05:59,460 --> 00:06:04,030 
various NoSQL storage systems like
HBase also happen through Spark Core. 
81 
00:06:06,020 --> 00:06:10,068 
A very important part of
Spark Core is the APIs for 
82 
00:06:10,068 --> 00:06:15,422 
defining resilient distributed data sets,
or RDDs for short. 
83 
00:06:15,422 --> 00:06:18,858 
RDDs are the main programming
abstraction in Spark, 
84 
00:06:18,858 --> 00:06:23,910 
which carry data across many computing
nodes in parallel, and transform it. 
85 
00:06:23,910 --> 00:06:29,614 
Spark SQL is the component of Spark
that provides querying structured and 
86 
00:06:29,614 --> 00:06:33,763 
unstructured data through
a common query language. 
87 
00:06:33,763 --> 00:06:38,830 
It can connect to many data sources and
provide APIs to convert 
88 
00:06:38,830 --> 00:06:43,714 
query results to RDDs in Python,
Scala and Java programs. 
89 
00:06:43,714 --> 00:06:49,780 
Spark Streaming is where data
manipulations take place in Spark. 
90 
00:06:51,060 --> 00:06:57,120 
Although, not a native real-time interface
to datastreams, Spark streaming enables 
91 
00:06:57,120 --> 00:07:01,140 
creating small aggregates of data coming
from streaming data ingestion systems. 
92 
00:07:03,490 --> 00:07:08,000 
These aggregate datasets
are called micro-batches and 
93 
00:07:08,000 --> 00:07:11,680 
they can be converted into RDBs in
Spark Streaming for processing. 
94 
00:07:13,740 --> 00:07:16,740 
MLlib is Sparks native library for 
95 
00:07:16,740 --> 00:07:21,240 
machine learning algorithms
as well as model evaluation. 
96 
00:07:21,240 --> 00:07:26,050 
All of the functionality is potentially
ported to any programming language Sparks 
97 
00:07:26,050 --> 00:07:30,030 
supports and
is designed to scale out using Spark. 
98 
00:07:31,190 --> 00:07:35,840 
GraphX is the graph analytics
library of Spark and 
99 
00:07:35,840 --> 00:07:41,270 
enables the Vertex edge data model of
graphs to be converted into RDDs as 
100 
00:07:41,270 --> 00:07:45,328 
well as providing scalable implementations
of graph processing algorithms. 
101 
00:07:47,390 --> 00:07:52,655 
To summarize, through these
layers Spark provides diverse, 
102 
00:07:52,655 --> 00:07:57,731 
scalable interactive management and
analyses of big data. 
103 
00:07:57,731 --> 00:08:03,166 
The interactive shell enables data
scientists to conduct exploratory 
104 
00:08:03,166 --> 00:08:08,416 
analysis and create big data pipelines,
while also enabling the big 
105 
00:08:08,416 --> 00:08:13,941 
data system integration engineers
to scale these analytical pipelines 
106 
00:08:13,941 --> 00:08:18,850 
across commodity computing clusters and
cloud environments. 
1 
00:00:01,670 --> 00:00:05,470 
There are many big data
processing systems, but 
2 
00:00:05,470 --> 00:00:09,260 
how do we make sense of them in order to
take full advantage of these systems? 
3 
00:00:10,320 --> 00:00:13,820 
In this video,
we will review some of them, and 
4 
00:00:13,820 --> 00:00:17,600 
a way to categorize big data processing
systems as we go through our review. 
5 
00:00:19,290 --> 00:00:24,330 
After this video, you will be able
to recall the Hadoop ecosystem, 
6 
00:00:25,490 --> 00:00:30,050 
draw a layer diagram with three layers for
data storage, data processing and 
7 
00:00:30,050 --> 00:00:31,930 
workflow management. 
8 
00:00:31,930 --> 00:00:36,800 
Summarize an evaluation criteria for
big data processing systems and 
9 
00:00:36,800 --> 00:00:41,363 
explain the properties of Hadoop,
Spark, Flink, 
10 
00:00:41,363 --> 00:00:45,720 
Beam, and Storm as major big
data processing systems. 
11 
00:00:46,750 --> 00:00:49,900 
In our introduction to big data course, 
12 
00:00:49,900 --> 00:00:53,500 
we talked about a version of
the layer diagram for the tools 
13 
00:00:53,500 --> 00:00:59,030 
in the Hadoop ecosystem organized
vertically based on the interface. 
14 
00:00:59,030 --> 00:01:03,225 
Lower level interface is the storage and
scheduling on the bottom and 
15 
00:01:03,225 --> 00:01:06,560 
higher level languages and
interactivity at the top. 
16 
00:01:07,830 --> 00:01:12,040 
Most of the tools in the Hadoop ecosystem
are initially built to complement 
17 
00:01:12,040 --> 00:01:17,410 
the capabilities of Hadoop for distributed
filesystem management using HDFS. 
18 
00:01:18,440 --> 00:01:23,240 
Data processing using the MapReduce
engine, and resource scheduling and 
19 
00:01:23,240 --> 00:01:25,210 
negotiation using the YARN engine. 
20 
00:01:26,310 --> 00:01:30,170 
Over time,
a number of new projects were built 
21 
00:01:30,170 --> 00:01:35,150 
either to add to these
complimentary tools or to handle 
22 
00:01:35,150 --> 00:01:39,750 
additional types of big data management
and processing not available in Hadoop. 
23 
00:01:41,520 --> 00:01:46,250 
Arguably, the most important
change to Hadoop over time 
24 
00:01:46,250 --> 00:01:51,210 
was the separation of YARN from
the MapReduce programming model 
25 
00:01:51,210 --> 00:01:54,510 
to solely handle resource
management concerns. 
26 
00:01:55,780 --> 00:02:00,660 
This allowed for Hadoop to be extensible
to different programming models. 
27 
00:02:00,660 --> 00:02:04,950 
And enabled the development of
a number of processing engines for 
28 
00:02:04,950 --> 00:02:06,920 
batch and stream processing. 
29 
00:02:08,250 --> 00:02:12,580 
Another way to look at the vast number of
tools that have been added to the Hadoop 
30 
00:02:12,580 --> 00:02:15,170 
ecosystem, is from the point of view of 
31 
00:02:15,170 --> 00:02:17,820 
their functionality in the big
data processing pipeline. 
32 
00:02:18,870 --> 00:02:24,500 
Simply put, these associate to three
distinct layers for data management and 
33 
00:02:24,500 --> 00:02:31,380 
storage, data processing, and resource
coordination and workflow management. 
34 
00:02:32,550 --> 00:02:37,760 
In our second course,
we talked about the bottom layer 
35 
00:02:37,760 --> 00:02:42,770 
in this diagram in detail,
namely the data management and storage. 
36 
00:02:44,600 --> 00:02:50,440 
While this layer includs Hadoop's HDFS
there are a number of other systems that 
37 
00:02:50,440 --> 00:02:56,250 
rely on HDFS as a file system or implement
their own no SQL storage options. 
38 
00:02:58,290 --> 00:03:02,940 
As big data can have a variety of
structure, semi structured and 
39 
00:03:02,940 --> 00:03:07,858 
unstructured formats, and
gets analyzed through a variety of tools, 
40 
00:03:07,858 --> 00:03:12,890 
many tools were introduced to
fit this variety of needs. 
41 
00:03:12,890 --> 00:03:15,180 
We call these big data management systems. 
42 
00:03:18,040 --> 00:03:22,630 
We reviewed redis and
Aerospike as key value stores, 
43 
00:03:22,630 --> 00:03:25,670 
where each data item is
identified with a unique key. 
44 
00:03:27,150 --> 00:03:30,827 
We got some practical
experience with Lucene and 
45 
00:03:30,827 --> 00:03:35,048 
Gephi as vector and
graph data stores, respectively. 
46 
00:03:36,800 --> 00:03:40,505 
We also talked about Vertica
as a column store database, 
47 
00:03:40,505 --> 00:03:44,300 
where information is stored
in columns rather than rows. 
48 
00:03:45,880 --> 00:03:49,070 
Cassandra and
Hbase are also in this category. 
49 
00:03:50,270 --> 00:03:56,402 
Finally, we introduce Solr and
Asterisk DB for managing unstructured and 
50 
00:03:56,402 --> 00:04:02,260 
semi-structured text, and
mongoDB as a document store. 
51 
00:04:02,260 --> 00:04:07,547 
The processing layer is where these
different varieties of data gets 
52 
00:04:07,547 --> 00:04:14,030 
retrieved, integrated, and analyzed,
which is the primary focus of this class. 
53 
00:04:16,232 --> 00:04:19,061 
In the integration and processing layer 
54 
00:04:19,061 --> 00:04:23,966 
we roughly refer to the tools that
are built on top of the HDFS and YARN, 
55 
00:04:23,966 --> 00:04:28,480 
although some of them work with
other storage and file systems. 
56 
00:04:30,180 --> 00:04:37,140 
YARN is a significant enabler of many of
these tools making a number of batch and 
57 
00:04:37,140 --> 00:04:42,111 
stream processing engines like Storm,
Spark, Flink and being possible. 
58 
00:04:43,700 --> 00:04:47,340 
We will revisit these processing
engines and explain why we have so 
59 
00:04:47,340 --> 00:04:48,890 
many later in this lecture. 
60 
00:04:49,970 --> 00:04:54,940 
This layer also includes tools like Hive,
or Spark SQL, for 
61 
00:04:54,940 --> 00:04:59,120 
bringing a query interface
on top of the storage layer. 
62 
00:04:59,120 --> 00:05:04,280 
Pig, for scripting simple big data
pipelines using the MapReduce framework. 
63 
00:05:04,280 --> 00:05:09,009 
And a number of specialized analytical
libraries for machine learning and 
64 
00:05:09,009 --> 00:05:14,262 
graph analytics, like Giraph as GraphX of
Spark are examples of such libraries for 
65 
00:05:14,262 --> 00:05:15,551 
graph processing. 
66 
00:05:15,551 --> 00:05:20,187 
And Mahout on top of the Hadoop stack and
MLlib of Spark are two options for 
67 
00:05:20,187 --> 00:05:21,480 
machine learning. 
68 
00:05:22,640 --> 00:05:26,760 
Although we have a basic overview of
graph processing and machine learning for 
69 
00:05:26,760 --> 00:05:31,700 
big data analytics later in this course,
we won't go into the details here. 
70 
00:05:31,700 --> 00:05:36,530 
Instead, we will have a dedicated
course on each of them 
71 
00:05:36,530 --> 00:05:38,670 
later in this specialization. 
72 
00:05:38,670 --> 00:05:46,720 
The third and top layer in our diagram is
the coordination and management layer. 
73 
00:05:46,720 --> 00:05:50,890 
This is where integration,
scheduling, coordination, and 
74 
00:05:50,890 --> 00:05:56,410 
monitoring of applications across many
tools in the bottom two layers take place. 
75 
00:05:57,440 --> 00:06:01,870 
This layer is also where the results of
the big data analysis gets communicated to 
76 
00:06:01,870 --> 00:06:07,630 
other programs, websites, visualization
tools, and business intelligence tools. 
77 
00:06:07,630 --> 00:06:12,830 
Workflow management systems help to
develop automated solutions that can 
78 
00:06:12,830 --> 00:06:17,960 
manage and coordinate the process of
combining data management and analytical 
79 
00:06:17,960 --> 00:06:23,264 
tests in a big data pipeline, as
a configurable, structured set of steps. 
80 
00:06:25,020 --> 00:06:29,544 
The workflow driven thinking also
matches this basic process of data 
81 
00:06:29,544 --> 00:06:31,970 
science that we overviewed before. 
82 
00:06:33,220 --> 00:06:37,420 
Oozie is an example of a workflow
scheduler that can interact with 
83 
00:06:37,420 --> 00:06:40,295 
many of the tools in the integration and
processing layer. 
84 
00:06:41,690 --> 00:06:46,390 
Zookeeper is the resource coordination and
monitoring tool and 
85 
00:06:46,390 --> 00:06:50,990 
manages and coordinates all these tools
and middleware named after animals. 
86 
00:06:52,460 --> 00:06:56,429 
Although virtual management is
my personal research area and 
87 
00:06:56,429 --> 00:06:58,793 
I talk more about it in other venues, 
88 
00:06:58,793 --> 00:07:03,999 
in this specialization we focus mainly
on big data integration and processing. 
89 
00:07:03,999 --> 00:07:09,450 
And we will not have a specific
lecture on this layer in this course. 
90 
00:07:09,450 --> 00:07:13,430 
We give you a reading on big data
workflows after this video as 
91 
00:07:13,430 --> 00:07:16,750 
further information and
a starting point for the subject. 
1 
00:00:02,380 --> 00:00:06,830 
In data integration and
processing pipelines, 
2 
00:00:06,830 --> 00:00:11,910 
data goes through a number of operations,
which can apply 
3 
00:00:11,910 --> 00:00:17,960 
a specific function to it, can work
the data from one format to another, 
4 
00:00:17,960 --> 00:00:24,097 
join data with other data sets, or
filter some values out of a data set. 
5 
00:00:25,240 --> 00:00:31,350 
We generally refer to these as
transformations, some of which can also 
6 
00:00:31,350 --> 00:00:36,950 
be specially named aggregations as you
have seen in Amarnath's earlier lectures. 
7 
00:00:36,950 --> 00:00:43,010 
In this video we will reveal some common
transformation operations that we see 
8 
00:00:43,010 --> 00:00:48,490 
in these pipelines, some of which,
we refer to as data parallel patterns. 
9 
00:00:50,575 --> 00:00:55,265 
After this video you will
be able to list common data 
10 
00:00:55,265 --> 00:01:00,355 
transformations within big data pipelines,
and design 
11 
00:01:00,355 --> 00:01:05,235 
a conceptual data processing pipeline
using the basic data transformations. 
12 
00:01:07,551 --> 00:01:12,770 
Simply speaking, transformations
are higher order functions or 
13 
00:01:12,770 --> 00:01:17,960 
tools to convert your data from
one form to another, just like 
14 
00:01:17,960 --> 00:01:23,530 
we would use tools at the wood shop
to transform logs into furniture. 
15 
00:01:24,900 --> 00:01:27,630 
When we look at big data
pipelines used today, 
16 
00:01:28,650 --> 00:01:32,840 
map is probably the most
common transformation we find. 
17 
00:01:34,340 --> 00:01:39,230 
The map operation is one of the basic
building blocks of the big data pipeline. 
18 
00:01:40,760 --> 00:01:46,200 
When you want to apply a process
to each member of a collection, 
19 
00:01:46,200 --> 00:01:49,370 
such as adding 10% bonus to each 
20 
00:01:49,370 --> 00:01:53,770 
person's salary on a given month a map
operation comes in very handy. 
21 
00:01:55,520 --> 00:02:00,490 
It takes your process and
understand that it is required to perform 
22 
00:02:00,490 --> 00:02:04,370 
the same operation or
process to each member of the set. 
23 
00:02:06,610 --> 00:02:10,970 
The figure on the left
here shows the application 
24 
00:02:10,970 --> 00:02:15,396 
of a map function to data
depicted in grey color. 
25 
00:02:15,396 --> 00:02:22,800 
Here colors red, blue, and
yellow are keys to identify each data set. 
26 
00:02:24,480 --> 00:02:31,130 
As you see, each data set is executed
separately even for the same colored key. 
27 
00:02:35,010 --> 00:02:39,944 
The reduce operation helps you then
to collectively apply the same 
28 
00:02:39,944 --> 00:02:42,860 
process to objects of similar nature. 
29 
00:02:44,680 --> 00:02:50,030 
For example, when you want to add your
monthly spending in different categories, 
30 
00:02:50,030 --> 00:02:56,350 
like grocery, fuel, and dining out,
the reduce operation is very useful. 
31 
00:02:58,500 --> 00:03:01,900 
In our figure here on the top left, 
32 
00:03:01,900 --> 00:03:06,472 
we see that data sets in
grey with the same color 
33 
00:03:06,472 --> 00:03:11,760 
are keys grouped together
using a reduced function. 
34 
00:03:12,940 --> 00:03:16,320 
Reds together, blues together,
and yellows together. 
35 
00:03:18,780 --> 00:03:23,579 
It would be a good idea to check out
the Spark word count hands-on to see 
36 
00:03:23,579 --> 00:03:28,400 
how map and reduce can be used
effectively for getting things done. 
37 
00:03:29,560 --> 00:03:34,550 
Map and reduce are types of
transformations that work on a single 
38 
00:03:34,550 --> 00:03:39,460 
list of key and data pairings just
like we see on the left of our figure. 
39 
00:03:42,460 --> 00:03:47,520 
Now let's consider a scenario where
we have two data sets identified 
40 
00:03:47,520 --> 00:03:53,090 
by the same keys just like the two
sets and colors in our diagram. 
41 
00:03:55,140 --> 00:04:00,120 
Many operations have such needs where
we have to look at all the pairings of 
42 
00:04:00,120 --> 00:04:04,740 
all key value pairs,
just like crossing two matrices. 
43 
00:04:06,900 --> 00:04:11,800 
For a practical example
Imagine you have two teams, 
44 
00:04:11,800 --> 00:04:16,870 
a sales team with two people, and
an operations team with four people. 
45 
00:04:18,050 --> 00:04:22,940 
In an event you would want each
person to meet every other person. 
46 
00:04:24,030 --> 00:04:27,210 
In this case, a cross product, or 
47 
00:04:27,210 --> 00:04:32,200 
a cartesian product, becomes a good
choice for organizing the event and 
48 
00:04:32,200 --> 00:04:36,000 
sharing each pairs' meeting location and
travel time to them. 
49 
00:04:37,770 --> 00:04:43,470 
In a cross or cartesian product operation,
each data partition gets 
50 
00:04:43,470 --> 00:04:49,450 
paired with all other data partitions,
regardless of its key. 
51 
00:04:49,450 --> 00:04:52,370 
This sometimes gets
referred to as all pairs. 
52 
00:04:54,990 --> 00:05:00,160 
Now add to the cross product by
just grouping together the data 
53 
00:05:00,160 --> 00:05:05,490 
partitions with the same key,
just like the red data. 
54 
00:05:07,090 --> 00:05:09,128 
And the yellow data partitions here. 
55 
00:05:12,005 --> 00:05:16,930 
This is a typical match or join operation. 
56 
00:05:16,930 --> 00:05:22,190 
As we see in the figure here, match
is very similar to the cross product, 
57 
00:05:22,190 --> 00:05:25,760 
except that it is more
selective in forming pairs. 
58 
00:05:26,810 --> 00:05:29,420 
Every pair must have something in common. 
59 
00:05:30,510 --> 00:05:34,920 
This something in common is
usually referred to as a key. 
60 
00:05:36,490 --> 00:05:40,330 
For example,
each person in your operations team and 
61 
00:05:40,330 --> 00:05:43,510 
sales team is assigned
to a different product. 
62 
00:05:43,510 --> 00:05:48,500 
You only want those people to meet
who are working on the same product. 
63 
00:05:48,500 --> 00:05:52,490 
In this case your key is product. 
64 
00:05:52,490 --> 00:05:55,290 
And you can perform and
match operation and 
65 
00:05:55,290 --> 00:05:59,670 
send e-mails to those people
who share a common product. 
66 
00:06:01,530 --> 00:06:06,756 
The number of e-mails is likely to be less
than when you performed a cartesian or 
67 
00:06:06,756 --> 00:06:10,980 
a cross product, therefore reducing
the cost of the operation. 
68 
00:06:12,270 --> 00:06:19,170 
In a match operation, only the keys
with data in both sets get joined, 
69 
00:06:20,240 --> 00:06:23,920 
and become a part of the final
output of the transformation. 
70 
00:06:26,600 --> 00:06:31,600 
Now let's consider listing
the data sets with all the keys, 
71 
00:06:31,600 --> 00:06:33,790 
even if they don't exist in both sets. 
72 
00:06:35,850 --> 00:06:40,550 
Consider a scenario where you
want to do brainstorming sessions 
73 
00:06:40,550 --> 00:06:43,690 
of people from operations and sales, and 
74 
00:06:43,690 --> 00:06:47,560 
get people who work on the same
products in the same rooms. 
75 
00:06:50,390 --> 00:06:52,750 
A co-group operation will do this for you. 
76 
00:06:54,290 --> 00:06:58,160 
You give it a product name
as they key to work with and 
77 
00:06:58,160 --> 00:07:01,530 
the two tables, the sales team and
operations team. 
78 
00:07:02,800 --> 00:07:07,740 
The co-group will create groups
which contain team members 
79 
00:07:07,740 --> 00:07:13,690 
working on common products even if a
product doesn't exist in one of the sets. 
80 
00:07:16,520 --> 00:07:20,520 
The last operation we will
see is the filter operation. 
81 
00:07:20,520 --> 00:07:23,760 
Filter works much like a test 
82 
00:07:23,760 --> 00:07:28,340 
where only elements that pass
a test are shown in the output. 
83 
00:07:30,040 --> 00:07:34,770 
Consider as a set that contains teams and
a number of members in their teams. 
84 
00:07:35,890 --> 00:07:38,930 
If your game requires people to pair up, 
85 
00:07:38,930 --> 00:07:42,570 
you may want to select teams which
have an even number of members. 
86 
00:07:43,570 --> 00:07:49,130 
In this case, you can create a test
that only passes the teams which have 
87 
00:07:49,130 --> 00:07:56,690 
an even number of team members shown as
divided by 2 with 0 in the remainder. 
88 
00:07:59,608 --> 00:08:04,147 
The real effectiveness of the basic
transformation we saw here 
89 
00:08:04,147 --> 00:08:09,025 
is in pipelining them in a way that
helps you to solve your specific 
90 
00:08:09,025 --> 00:08:13,819 
problem just as you would perform
a series of tasks on a real block 
91 
00:08:13,819 --> 00:08:18,610 
of wood to make a fine piece of
woodwork that you can use to steer your 
92 
00:08:18,610 --> 00:08:22,495 
ship, which in this case is
your business or research. 
1 
00:00:00,550 --> 00:00:03,512 
Now that we revealed all three layers, 
2 
00:00:03,512 --> 00:00:08,556 
we are ready to come back to
the Integration and Processing layer. 
3 
00:00:08,556 --> 00:00:12,652 
Just a simple Google search for
Big Data Processing Pipelines 
4 
00:00:12,652 --> 00:00:17,378 
will bring a vast number of pipelines
with large number of technologies 
5 
00:00:17,378 --> 00:00:22,041 
that support scalable data cleaning,
preparation, and analysis. 
6 
00:00:24,091 --> 00:00:28,286 
How do we make sense of it all to
make sure we use the right tools for 
7 
00:00:28,286 --> 00:00:29,540 
our application? 
8 
00:00:30,660 --> 00:00:35,490 
We will continue our lecture to review
a set of evaluation criteria for 
9 
00:00:35,490 --> 00:00:40,750 
these systems and some of the big data
processing systems based on this criteria. 
10 
00:00:42,250 --> 00:00:47,850 
Depending on the resources we have access
to and characteristics of our application, 
11 
00:00:47,850 --> 00:00:51,130 
we apply several
considerations to evaluate and 
12 
00:00:51,130 --> 00:00:52,920 
pick a software stack for big data. 
13 
00:00:54,010 --> 00:00:59,410 
Of these, first one we consider
is the Execution Model, 
14 
00:00:59,410 --> 00:01:04,450 
and the expressivity of it to support for
various transformations of batch or 
15 
00:01:04,450 --> 00:01:07,430 
streaming data, or
sometimes interactive computing. 
16 
00:01:08,920 --> 00:01:12,550 
Semantics of streaming,
including exactly once or 
17 
00:01:12,550 --> 00:01:17,620 
at least one processing for each event, or
being able to keep the state of the data, 
18 
00:01:17,620 --> 00:01:21,680 
is an important concern for
this execution model. 
19 
00:01:21,680 --> 00:01:26,110 
Latency is another important criteria,
depending on the application. 
20 
00:01:27,210 --> 00:01:30,760 
Having a low latency system
is very important for 
21 
00:01:30,760 --> 00:01:34,345 
applications like online gaming and
hazards management. 
22 
00:01:34,345 --> 00:01:38,340 
Whereas most applications
are less time critical, 
23 
00:01:38,340 --> 00:01:43,800 
like search engine indexing, and would
be fine with a batch processing ability. 
24 
00:01:44,820 --> 00:01:49,760 
Scalability for both small and
large datasets and different 
25 
00:01:49,760 --> 00:01:54,070 
analytical methods and algorithms,
is also an important evaluation criteria. 
26 
00:01:55,290 --> 00:01:59,430 
As well as support for
different programming language 
27 
00:01:59,430 --> 00:02:03,470 
of the libraries used by the analytical
tools that we have access to. 
28 
00:02:04,700 --> 00:02:09,030 
Finally, while all big data
tools provide fault tolerance, 
29 
00:02:09,030 --> 00:02:14,390 
the mechanics of how the fault tolerance is
handled is an important issue to consider. 
30 
00:02:15,790 --> 00:02:18,900 
Let's review five of
the big data processing 
31 
00:02:18,900 --> 00:02:23,390 
engines supported by the Apache Foundation
using this evaluation criteria. 
32 
00:02:25,300 --> 00:02:30,230 
The MapReduce implementation of
Hadoop provides a batch execution 
33 
00:02:30,230 --> 00:02:35,230 
model where the data from HDFS gets
loaded into mappers before processing. 
34 
00:02:36,440 --> 00:02:39,380 
There is no in-memory processing support, 
35 
00:02:39,380 --> 00:02:44,490 
meaning the mappers write the data on
files before the reducers can read it, 
36 
00:02:44,490 --> 00:02:48,250 
resulting in a high-latency and
less scalable execution. 
37 
00:02:49,960 --> 00:02:53,690 
This also hinders
the performance of iterative and 
38 
00:02:53,690 --> 00:02:58,827 
interactive applications that require many
steps of transformations using MapReduce. 
39 
00:03:01,150 --> 00:03:04,835 
Although the only native
programming interface for 
40 
00:03:04,835 --> 00:03:09,870 
MapReduce is in Java, other programming
languages like Python provide modules or 
41 
00:03:09,870 --> 00:03:14,370 
libraries for Hadoop MapReduce
programming, however with less efficiency. 
42 
00:03:16,180 --> 00:03:20,730 
Data replication is the primary
method of fault tolerance, 
43 
00:03:20,730 --> 00:03:26,270 
which in turn affects the scalability and
execution speed further. 
44 
00:03:26,270 --> 00:03:29,560 
Spark was built to support iterative and 
45 
00:03:29,560 --> 00:03:34,730 
interactive big data processing
pipelines efficiently using an in-memory 
46 
00:03:34,730 --> 00:03:39,800 
structure called Resilient Distributed
Datasets, or shortly, RDDs. 
47 
00:03:41,040 --> 00:03:45,860 
In addition to map and reduce operations,
it provides support for 
48 
00:03:45,860 --> 00:03:49,890 
a range of transformation
operations like join and filter. 
49 
00:03:49,890 --> 00:03:55,711 
Any pipeline of transformations can
be applied to these RDD's in-memory, 
50 
00:03:55,711 --> 00:04:00,829 
making Spark's performance very high for
iterative processing. 
51 
00:04:02,538 --> 00:04:07,480 
The RDD extraction is also designed
to handle fault tolerance with 
52 
00:04:07,480 --> 00:04:09,230 
less impact on performance. 
53 
00:04:10,886 --> 00:04:13,170 
In addition to HDFS, 
54 
00:04:13,170 --> 00:04:18,200 
Spark can read data from many storage
platforms and it provides support for 
55 
00:04:18,200 --> 00:04:24,155 
streaming data applications using
a technique called micro-batching. 
56 
00:04:24,155 --> 00:04:30,250 
Its latency can be on the order of
seconds depending on the batch size, 
57 
00:04:30,250 --> 00:04:34,030 
which is relatively slower compared
to native streaming platforms. 
58 
00:04:35,600 --> 00:04:40,320 
Spark has support for a number of
programming languages, including Scala and 
59 
00:04:40,320 --> 00:04:44,260 
Python as the most popular ones,
as well as built-in libraries for 
60 
00:04:44,260 --> 00:04:46,500 
graph processing and machine learning. 
61 
00:04:47,530 --> 00:04:51,415 
Although Flink has very
similar transformations and 
62 
00:04:51,415 --> 00:04:55,890 
in-memory data extractions with Spark,
it provides direct support for 
63 
00:04:55,890 --> 00:04:59,517 
streaming data,
making it a lower-latency framework. 
64 
00:05:00,620 --> 00:05:04,720 
It provides connection interfaces to
streaming data ingestion engines like 
65 
00:05:04,720 --> 00:05:06,020 
Kafka and Flume. 
66 
00:05:07,460 --> 00:05:11,340 
Flink supports application
programming interfaces in Java and 
67 
00:05:11,340 --> 00:05:12,966 
Scala just like Spark. 
68 
00:05:12,966 --> 00:05:17,760 
Starting with it's original
version called Stratosphere, 
69 
00:05:17,760 --> 00:05:23,010 
Flink had it's own execution engine
called Nephele, and had an ability 
70 
00:05:23,010 --> 00:05:27,670 
to run both on Hadoop and also separately
in its own execution environment. 
71 
00:05:29,060 --> 00:05:33,110 
In addition to map and reduce,
Flink provides abstractions for 
72 
00:05:33,110 --> 00:05:37,055 
other data parallel database
patterns like join and group by. 
73 
00:05:39,210 --> 00:05:44,010 
One of the biggest advantage of using
Flink comes from it's optimizer 
74 
00:05:44,010 --> 00:05:47,460 
to pick and apply the best pattern and
execution strategy. 
75 
00:05:48,490 --> 00:05:52,080 
There has been experiments comparing
fault tolerance features of Flink 
76 
00:05:52,080 --> 00:05:56,720 
to those of Sparks, which conclude
that Sparks slightly better for Spark. 
77 
00:05:57,840 --> 00:06:03,080 
The Beam system, from Google,
is a relatively new system for 
78 
00:06:03,080 --> 00:06:06,890 
batch and stream processing with
a data flow programming model. 
79 
00:06:08,080 --> 00:06:13,090 
It initially used Google's own Cloud data
flow as an execution environment, but 
80 
00:06:13,090 --> 00:06:17,228 
Spark and Flink backends for
it have been implemented recently. 
81 
00:06:17,228 --> 00:06:22,370 
It's a low-latency environment with
high reviews on fault tolerance. 
82 
00:06:23,460 --> 00:06:28,456 
It currently provides application
programming interfaces in Java and 
83 
00:06:28,456 --> 00:06:31,295 
Scala, and a Python SDK is in the works. 
84 
00:06:31,295 --> 00:06:34,532 
SDK means software development kit. 
85 
00:06:34,532 --> 00:06:40,690 
Beam provides a very strong streaming and
windowing framework for streaming data. 
86 
00:06:40,690 --> 00:06:45,417 
And it is highly scalable and reliable,
allowing it to make trade-off 
87 
00:06:45,417 --> 00:06:49,690 
decisions between accuracy,
speed, and cost of processing. 
88 
00:06:51,710 --> 00:06:53,730 
Storm has been designed for 
89 
00:06:53,730 --> 00:06:58,960 
stream processing in real
time with very low-latency. 
90 
00:06:58,960 --> 00:07:03,930 
It defined input stream interface
abstractions called spouts, and 
91 
00:07:03,930 --> 00:07:06,080 
computation abstractions called bolts. 
92 
00:07:07,420 --> 00:07:11,700 
Spouts and bolts can be pipelined
together using a data flow approach. 
93 
00:07:11,700 --> 00:07:15,430 
That data gets queued until
the computation acknowledges 
94 
00:07:15,430 --> 00:07:16,190 
the receipt of it. 
95 
00:07:18,310 --> 00:07:21,080 
A master node tracks running jobs and 
96 
00:07:21,080 --> 00:07:24,860 
ensures all data is processed
by the computations on workers. 
97 
00:07:26,430 --> 00:07:31,952 
Nathan Mars, the lead developer for
Storm, built the Lambda Architecture 
98 
00:07:31,952 --> 00:07:37,744 
using Storm for stream processing and
Hadoop MapReduce for batch processing. 
99 
00:07:40,660 --> 00:07:45,740 
The Lambda Architecture
originally used Storm for 
100 
00:07:45,740 --> 00:07:50,229 
speed layer and Hadoop and
HBase for batch and 
101 
00:07:50,229 --> 00:07:54,490 
serving layers, as seen in this diagram. 
102 
00:07:55,840 --> 00:08:00,760 
However, it was later used as a more
general framework that can combine 
103 
00:08:00,760 --> 00:08:06,990 
the results of stream and batch processing
executed in multiple big data systems. 
104 
00:08:06,990 --> 00:08:12,700 
This diagram shows a generalized Lambda
Architecture containing some of the tools 
105 
00:08:12,700 --> 00:08:19,930 
we discussed earlier, including using
Spark for both batch and speed layers. 
106 
00:08:21,540 --> 00:08:25,530 
In this course, we picked Spark
as a big data integration and 
107 
00:08:25,530 --> 00:08:30,600 
processing environment since it supports
most of our evaluation criteria. 
108 
00:08:30,600 --> 00:08:35,862 
And this hybrid data processing
architecture using built-in data querying, 
109 
00:08:35,862 --> 00:08:38,619 
streaming, and analytical libraries. 
110 
00:08:38,619 --> 00:08:43,586 
We will continue our discussion with
Spark and hands-on exercises in Spark. 
1 
00:00:01,740 --> 00:00:04,460 
Analytical operations
in big data pipelines. 
2 
00:00:05,940 --> 00:00:07,730 
After this video, 
3 
00:00:07,730 --> 00:00:13,080 
you will be able to list common analytical
operations within big data pipelines and 
4 
00:00:13,080 --> 00:00:16,650 
describe sample applications for
these analytical operations. 
5 
00:00:18,550 --> 00:00:24,200 
In this lesson, we will be
looking at analytical operations. 
6 
00:00:24,200 --> 00:00:28,810 
These are operations used in analytics,
which is the process of 
7 
00:00:28,810 --> 00:00:33,370 
transforming data into insights for
making more informed decisions. 
8 
00:00:34,690 --> 00:00:39,720 
The purpose of analytical operations is to
analyze the data to discover meaningful 
9 
00:00:39,720 --> 00:00:44,089 
trends and patterns, in order to gain
insights into the problem being studied. 
10 
00:00:45,138 --> 00:00:47,810 
The knowledge gained from these insights 
11 
00:00:47,810 --> 00:00:51,890 
ultimately lead to more informed
decisions driven by data. 
12 
00:00:54,150 --> 00:00:58,326 
Here are some common analytical operations
that we will discuss in this lecture. 
13 
00:00:58,326 --> 00:01:02,704 
Classification, clustering, 
14 
00:01:02,704 --> 00:01:06,380 
path analysis and connectivity analysis. 
15 
00:01:07,630 --> 00:01:09,490 
Let's start with classification. 
16 
00:01:10,730 --> 00:01:18,130 
In classification, the goal is to predict
a categorical target from the input data. 
17 
00:01:18,130 --> 00:01:21,290 
A categorical target is one
with discreet values or 
18 
00:01:21,290 --> 00:01:23,820 
categories, instead of continuous values. 
19 
00:01:25,470 --> 00:01:28,460 
For example, this diagram shows 
20 
00:01:28,460 --> 00:01:34,260 
a classification task to determine the
risk associated with a loan application. 
21 
00:01:34,260 --> 00:01:37,801 
The input consists of the loan amount, 
22 
00:01:37,801 --> 00:01:44,571 
applicant information such as income,
age, debts, and a down payment. 
23 
00:01:46,202 --> 00:01:50,903 
From this input data,
the task is to determine whether 
24 
00:01:50,903 --> 00:01:54,898 
the loan application is low risk or
high risk. 
25 
00:01:57,216 --> 00:02:00,852 
There are many classification techniques
or algorithms that can be used for 
26 
00:02:00,852 --> 00:02:02,480 
this problem. 
27 
00:02:02,480 --> 00:02:07,160 
We will discuss a specific one, namely,
decision tree in the next slide. 
28 
00:02:09,330 --> 00:02:13,590 
The decision tree algorithm is
one technique for classification. 
29 
00:02:13,590 --> 00:02:15,000 
With this technique, 
30 
00:02:15,000 --> 00:02:20,040 
decisions to perform the classification
task are modeled as a tree structure. 
31 
00:02:22,070 --> 00:02:26,770 
For the loan risk assessment problem,
a simple decision tree is shown here, 
32 
00:02:27,960 --> 00:02:32,470 
where the loan application is
classified as being either low risk, or 
33 
00:02:32,470 --> 00:02:34,930 
high risk, based on the loan amount. 
34 
00:02:34,930 --> 00:02:37,750 
The applicant's income,
and the applicant's age. 
35 
00:02:40,090 --> 00:02:44,110 
The decision tree algorithm is implemented
in many machine learning tools. 
36 
00:02:45,310 --> 00:02:51,718 
This diagram shows how to specify
decision tree from input data, KNIME. 
37 
00:02:51,718 --> 00:02:55,650 
A graphical user-interface-based
machine learning platform. 
38 
00:02:57,370 --> 00:03:02,400 
Some examples of classification
are the prediction of whether cells from 
39 
00:03:02,400 --> 00:03:07,540 
a tumor are benign or
malignant, categorization of 
40 
00:03:07,540 --> 00:03:12,630 
handwritten digits as being zero,
one, two, etc, up to nine. 
41 
00:03:14,180 --> 00:03:18,760 
And determining whether a credit card
transaction is legitimate or fraudulent, 
42 
00:03:20,940 --> 00:03:25,255 
and classification of a loan application
as being low-risk, medium-risk or 
43 
00:03:25,255 --> 00:03:26,540 
high-risk, as you've seen. 
44 
00:03:28,370 --> 00:03:31,970 
Another common analytical
operation is cluster analysis. 
45 
00:03:33,180 --> 00:03:35,880 
In cluster analysis, or clustering, 
46 
00:03:35,880 --> 00:03:40,320 
the goal is to organize similar
items in to groups of association. 
47 
00:03:41,880 --> 00:03:47,045 
This diagram shows an example of cluster
analysis in which customers are clustered 
48 
00:03:47,045 --> 00:03:51,510 
into groups according to their
preferences of movie genre. 
49 
00:03:53,080 --> 00:03:56,450 
So, customers who like Sci-Fi
movies are grouped together. 
50 
00:03:58,010 --> 00:04:01,814 
Those who like drama movies
are grouped together, 
51 
00:04:01,814 --> 00:04:06,337 
and customers who like horror
movies are grouped together. 
52 
00:04:06,337 --> 00:04:10,482 
With this grouping, new movies,
as well as other products, 
53 
00:04:10,482 --> 00:04:15,184 
such as books, can be offered to
the right type of costumers in order to 
54 
00:04:15,184 --> 00:04:18,073 
generate interest and increase revenue. 
55 
00:04:20,775 --> 00:04:25,516 
A simple and commonly used algorithm for
cluster analysis is k-means. 
56 
00:04:27,163 --> 00:04:31,670 
With k-means,
samples are divided into k clusters. 
57 
00:04:31,670 --> 00:04:35,100 
This clustering is done in order
to minimize the variance or 
58 
00:04:35,100 --> 00:04:38,620 
similarity between samples
within the same cluster 
59 
00:04:38,620 --> 00:04:41,590 
using some similarity
measures such as distance. 
60 
00:04:42,690 --> 00:04:47,210 
In this example, k is equal to three, and 
61 
00:04:47,210 --> 00:04:53,170 
k-means divides the original data shown
on the left into three clusters, 
62 
00:04:53,170 --> 00:04:57,620 
shown as blue, green, and
red on the chart on the right. 
63 
00:04:59,700 --> 00:05:03,530 
The k-means clustering algorithm is
implemented on many machine-learning 
64 
00:05:03,530 --> 00:05:04,180 
platforms. 
65 
00:05:05,360 --> 00:05:08,550 
The code here shows how to read in and 
66 
00:05:08,550 --> 00:05:13,190 
parse input data, and
perform k-means clustering on the data. 
67 
00:05:13,190 --> 00:05:18,210 
Other examples of cluster analysis
are grouping a company’s customer base 
68 
00:05:18,210 --> 00:05:24,560 
into distinct segments for more effective
targeted marketing, finding articles or 
69 
00:05:24,560 --> 00:05:28,240 
webpages with similar topics for
retrieving relevant information. 
70 
00:05:30,060 --> 00:05:35,844 
Identification of areas in the city with
rates of particular types of crimes for 
71 
00:05:35,844 --> 00:05:40,077 
effective management of law
enforcement resources, and 
72 
00:05:40,077 --> 00:05:46,055 
determining different groups of weather
patterns such as rainy, cold or snowy. 
73 
00:05:48,190 --> 00:05:51,632 
Classification and cluster analysis
are considered machine learning and 
74 
00:05:51,632 --> 00:05:53,620 
analytical operations. 
75 
00:05:53,620 --> 00:05:56,790 
There are also analytical
operations from graph analytics, 
76 
00:05:56,790 --> 00:06:01,790 
which is the field of analytics where
the underlying data is structured as, or 
77 
00:06:01,790 --> 00:06:03,670 
can be modeled as the set of graphs. 
78 
00:06:05,020 --> 00:06:09,190 
One analytical operation using
graphs as path analysis, 
79 
00:06:09,190 --> 00:06:13,000 
which analyzes sequences of nodes and
edges in a graph. 
80 
00:06:14,390 --> 00:06:17,500 
A common application of path analysis 
81 
00:06:17,500 --> 00:06:21,980 
is to find routes from one
location to another location. 
82 
00:06:21,980 --> 00:06:27,440 
For example, you might want to find the
shortest path from your home to your work. 
83 
00:06:27,440 --> 00:06:32,100 
This path may be different depending on
conditions such as the day of the week, 
84 
00:06:32,100 --> 00:06:36,450 
time of day, traffic congestion,
weather and etc. 
85 
00:06:38,390 --> 00:06:43,530 
This code shows some operations for
path analysis on neo4j, 
86 
00:06:43,530 --> 00:06:48,033 
which is a graph database system
using a query language called Cypher. 
87 
00:06:49,210 --> 00:06:54,440 
The first operation finds the shortest
path between specific nodes in a graph. 
88 
00:06:55,610 --> 00:06:59,460 
The second operation finds all
the shortest paths in a graph. 
89 
00:07:00,890 --> 00:07:04,770 
Connectivity analysis of graphs
has to do with finding and 
90 
00:07:04,770 --> 00:07:07,970 
tracking groups to determine
interactions between entities. 
91 
00:07:09,110 --> 00:07:13,330 
Entities in highly interacting
groups are more connected 
92 
00:07:13,330 --> 00:07:17,680 
to each other than to entities
of other groups in a graph. 
93 
00:07:18,820 --> 00:07:21,860 
These groups are called communities, and 
94 
00:07:21,860 --> 00:07:26,280 
are interesting to analyze as they
give insights into the degree and 
95 
00:07:26,280 --> 00:07:31,410 
patterns of the interaction between
entities, and also between communities. 
96 
00:07:32,880 --> 00:07:39,570 
Some applications of connectivity analysis
are to extract conversation threads. 
97 
00:07:39,570 --> 00:07:42,030 
For example,
by looking at tweets and retweets. 
98 
00:07:43,250 --> 00:07:47,720 
To find interacting groups, for example,
to determine which users are interacting 
99 
00:07:47,720 --> 00:07:53,900 
with each other users, to find
influencers, for example, to understand 
100 
00:07:53,900 --> 00:07:58,720 
who are the main users leading to
the conversation about a particular topic. 
101 
00:07:58,720 --> 00:08:01,550 
Or, who do people pay attention to? 
102 
00:08:01,550 --> 00:08:05,020 
This information can be used to
identify the fewest number of 
103 
00:08:05,020 --> 00:08:06,990 
people with the greatest influence. 
104 
00:08:06,990 --> 00:08:11,830 
For example, for political campaigns,
or marketing on social media. 
105 
00:08:13,360 --> 00:08:15,680 
This code shows some operations for 
106 
00:08:15,680 --> 00:08:20,210 
connectivity analysis on neo4j using
the query language, Cypher, again. 
107 
00:08:21,980 --> 00:08:25,660 
The first operation finds the degree
of all the nodes in a graph, 
108 
00:08:25,660 --> 00:08:30,450 
and the second creates a histogram
of degrees for all nodes in a graph 
109 
00:08:30,450 --> 00:08:35,990 
to determine how connected a node in
a graph is, we need to look at its degree. 
110 
00:08:35,990 --> 00:08:39,860 
The degree of a node is the number
of edges connected to the node. 
111 
00:08:41,220 --> 00:08:45,250 
A degree histogram shows the distribution
of node degrees in the graph and 
112 
00:08:45,250 --> 00:08:50,330 
is useful in comparing graphs and
identifying types of users, for 
113 
00:08:50,330 --> 00:08:55,410 
example, those who follow, versus those
who are followed in social networks. 
114 
00:08:56,940 --> 00:09:01,970 
To summarize and add to these techniques,
the decision tree algorithm for 
115 
00:09:01,970 --> 00:09:06,470 
classification and k-means algorithm for
cluster analysis that we covered in this 
116 
00:09:06,470 --> 00:09:09,040 
lecture are techniques
from machine learning. 
117 
00:09:10,210 --> 00:09:15,650 
Machine learning is a field of analytics
focused on the study and construction of 
118 
00:09:15,650 --> 00:09:20,819 
computer systems that can learn from data
without being explicitly programmed. 
119 
00:09:22,140 --> 00:09:25,540 
Our course on machine learning in
this specialization will cover these 
120 
00:09:25,540 --> 00:09:29,230 
algorithms in more detail,
along with other algorithms used for 
121 
00:09:29,230 --> 00:09:31,660 
classification and cluster analysis. 
122 
00:09:31,660 --> 00:09:34,690 
As well as algorithms for
other machine learning tasks, 
123 
00:09:34,690 --> 00:09:39,360 
such as regression,
association analysis, and tools for 
124 
00:09:39,360 --> 00:09:42,480 
implementing and
executing machine learning algorithms. 
125 
00:09:44,380 --> 00:09:49,130 
As a summary of the Graph Analytics,
the Path Analytics technique for finding 
126 
00:09:49,130 --> 00:09:53,390 
the shortest path and the connectivity
analysis technique for analyzing communities 
127 
00:09:53,390 --> 00:09:58,260 
that we discussed earlier,
are techniques used in graph analytics. 
128 
00:09:58,260 --> 00:10:02,970 
As explained earlier,
graph analytics is the field of analytics, 
129 
00:10:02,970 --> 00:10:07,520 
where the underlying data is structured or
can be modeled as a set of graphs. 
130 
00:10:08,760 --> 00:10:13,200 
Our graph analytics course in the
specialization will cover these and other 
131 
00:10:13,200 --> 00:10:18,174 
graph techniques, and we'll also cover
tools and platforms for graph analytics. 
132 
00:10:18,174 --> 00:10:25,610 
In summary, analytic operations
are used to discover meaningful 
133 
00:10:25,610 --> 00:10:31,115 
patterns in the data in order to provide
insights into the problem being studied. 
134 
00:10:31,115 --> 00:10:36,635 
We looked at some of the examples of
analytical operations for classification, 
135 
00:10:36,635 --> 00:10:41,355 
cluster analysis, path analysis and
connectivity analysis in this lecture. 
1 
00:00:01,226 --> 00:00:02,488 
In this hands-on activity, 
2 
00:00:02,488 --> 00:00:05,520 
we'll be performing word count on
the complete works of Shakespeare. 
3 
00:00:07,110 --> 00:00:11,350 
First, we will copy the Shakespeare
text into the Hadoop file system. 
4 
00:00:11,350 --> 00:00:14,847 
Next, we will create a new
Jupyter Notebook, and 
5 
00:00:14,847 --> 00:00:17,937 
read the Shakespeare
text into a Spark RDD. 
6 
00:00:17,937 --> 00:00:21,177 
We will then perform WordCount
using map and reduce, 
7 
00:00:21,177 --> 00:00:24,352 
and write the results to HDFS and
view the contents. 
8 
00:00:27,553 --> 00:00:29,301 
Let's begin. 
9 
00:00:29,301 --> 00:00:33,300 
In the intro to big data course,
we copy the Shakespeare text into HDFS. 
10 
00:00:33,300 --> 00:00:36,630 
Let's see if it's still there. 
11 
00:00:36,630 --> 00:00:38,150 
If not, we can copy it now. 
12 
00:00:38,150 --> 00:00:42,180 
Click on the terminal icon
at the top of the toolbar. 
13 
00:00:43,950 --> 00:00:49,830 
Now we can run hadoop fs- ls to see what's
in our hadoop filesystem directory. 
14 
00:00:51,900 --> 00:00:54,380 
There are no files in HTFS,
so let's copy it. 
15 
00:00:54,380 --> 00:01:01,070 
If you already have words.txt in your HTFS
directory, you can skip this next step. 
16 
00:01:02,450 --> 00:01:06,837 
Cd into downloads, 
17 
00:01:06,837 --> 00:01:15,359 
big-data-3/spark-wordcount. 
18 
00:01:15,359 --> 00:01:17,450 
We can do ls to see the file. 
19 
00:01:18,960 --> 00:01:21,390 
Let's copy this file to HTFS. 
20 
00:01:21,390 --> 00:01:28,221 
We run Hadoop,
fs copy from local, words.txt. 
21 
00:01:33,314 --> 00:01:37,800 
We can write Hadoop fs -ls again
to verify that the file is there. 
22 
00:01:41,290 --> 00:01:42,600 
Now let's do work count in spark. 
23 
00:01:44,220 --> 00:01:47,380 
We will do this in an iPython
notebook using Jupyter server. 
24 
00:01:49,070 --> 00:01:51,587 
Look on the web browser icon,
the top of the toolbar. 
25 
00:01:55,112 --> 00:01:59,706 
And go to the Jupyter server URL,
which is local host port 8889. 
26 
00:02:02,360 --> 00:02:04,789 
Next, let's create a new iPython notebook 
27 
00:02:10,685 --> 00:02:16,855 
The first step is to read the words.txt
files in HTFS into a spark RDD. 
28 
00:02:16,855 --> 00:02:18,572 
We'll call the RDD, lines. 
29 
00:02:22,140 --> 00:02:27,128 
We can read it using the spark context SC,
in calling the text file method. 
30 
00:02:32,609 --> 00:02:37,041 
The argument is the URL of
the word set TXT file and HDFS. 
31 
00:02:47,270 --> 00:02:51,998 
Let's run this We can 
32 
00:02:51,998 --> 00:02:56,920 
view the contents of this RDD
by calling lines.take(5). 
33 
00:03:01,553 --> 00:03:05,010 
The argument 5 says how many
lines to show of the RDD. 
34 
00:03:06,930 --> 00:03:11,112 
Next, we'll transform this RDD
of lines into an RDD of words. 
35 
00:03:11,112 --> 00:03:15,755 
We'll say, words = lines.flatmap, 
36 
00:03:18,940 --> 00:03:20,852 
lambda line: 
37 
00:03:24,123 --> 00:03:28,568 
line.split Double quote
space double quote. 
38 
00:03:31,374 --> 00:03:38,852 
This creates a new RDD called, words,
by running flatMap over the line RDD. 
39 
00:03:38,852 --> 00:03:41,092 
The argument is this lambda expression. 
40 
00:03:43,729 --> 00:03:47,310 
A lambda in Python is a simple way
to declare a one line expression. 
41 
00:03:48,860 --> 00:03:51,890 
In this case,
there's one argument called line and 
42 
00:03:51,890 --> 00:03:55,160 
we called it split method on this line and
we split on spaces. 
43 
00:03:57,080 --> 00:03:59,720 
We can run this and
look at the contents of words. 
44 
00:04:09,720 --> 00:04:12,500 
We can see that each element
now is an individual word. 
45 
00:04:14,769 --> 00:04:17,136 
Next, we'll create tuples of these words. 
46 
00:04:18,907 --> 00:04:23,138 
We'll put them in a new RDD called tuples. 
47 
00:04:23,138 --> 00:04:26,405 
Enter, tuples 
48 
00:04:26,405 --> 00:04:31,433 
= words.map:lambda 
49 
00:04:31,433 --> 00:04:36,465 
word; (word, 1). 
50 
00:04:41,144 --> 00:04:45,080 
This creates the tuples
by transforming words. 
51 
00:04:45,080 --> 00:04:47,370 
This uses map and another lambda function. 
52 
00:04:47,370 --> 00:04:53,400 
In this case, the lambda takes
one argument and returns a tuple. 
53 
00:04:53,400 --> 00:04:55,350 
Where the first value of
the tuple is the word. 
54 
00:04:56,500 --> 00:04:58,320 
The second value, is the number 1. 
55 
00:04:58,320 --> 00:05:04,460 
Not that in this case, we use map,
whereas before, we used flat map. 
56 
00:05:06,490 --> 00:05:09,930 
In this case, we want a tuple for
every word in the words. 
57 
00:05:09,930 --> 00:05:13,690 
So we have a one to one mapping
between inputs and outputs. 
58 
00:05:13,690 --> 00:05:19,064 
Previously, while we were splitting lines
into word, each line had multiple words. 
59 
00:05:21,677 --> 00:05:22,348 
In general, 
60 
00:05:22,348 --> 00:05:26,950 
you want to use map when you have a one to
one mapping between inputs and outputs. 
61 
00:05:26,950 --> 00:05:31,804 
In flatMap you have a one to many or
none mapping between inputs and 
62 
00:05:31,804 --> 00:05:37,607 
outputs Let's run this and look at tuples. 
63 
00:05:45,043 --> 00:05:50,538 
We can see that each word now has
a tuple initialized with the count of 1. 
64 
00:05:50,538 --> 00:05:55,080 
We can now count all the words by
combining or reducing these tuples. 
65 
00:05:55,080 --> 00:05:57,190 
We'll put this in a new RDD called counts. 
66 
00:05:58,660 --> 00:06:03,039 
So we'll say counts equals
tuples.reduce by key. 
67 
00:06:07,602 --> 00:06:11,955 
Lambda a,b:. 
68 
00:06:11,955 --> 00:06:13,125 
a + b. 
69 
00:06:16,313 --> 00:06:20,038 
In this case, the lambda function
takes two arguments, a and be, and 
70 
00:06:20,038 --> 00:06:22,199 
will return the result of adding a and b. 
71 
00:06:24,876 --> 00:06:26,071 
To view the result, 
72 
00:06:33,657 --> 00:06:37,815 
You can see now that the counts for
each words have been created. 
73 
00:06:37,815 --> 00:06:40,839 
We can write this result back to HDFS. 
74 
00:06:40,839 --> 00:06:50,239 
Let's say
counts.coalesce(1).saveAsTextFile, 
75 
00:06:50,239 --> 00:06:53,120 
and then the URL. 
76 
00:07:04,966 --> 00:07:09,675 
The coalesce means we only
want a single output file. 
77 
00:07:09,675 --> 00:07:12,172 
Let's go back to our shell and
view the results. 
78 
00:07:15,582 --> 00:07:18,930 
We'll run hadoop fs -ls
to see the directory. 
79 
00:07:21,809 --> 00:07:24,553 
And run it again to look inside
the wordcount directory. 
80 
00:07:29,294 --> 00:07:32,602 
And once more,
to look inside wordcount/outputDir. 
81 
00:07:38,808 --> 00:07:45,420 
As you recall,
the output from hadoop jobs is part-0000. 
82 
00:07:45,420 --> 00:07:48,837 
This is also true for spark jobs. 
83 
00:07:48,837 --> 00:07:51,484 
Let's copy this file to
the local file system. 
84 
00:07:51,484 --> 00:07:55,899 
We'll run hadoop fs CopyToLocal 
85 
00:07:55,899 --> 00:08:02,212 
wordcount/outputDir/part-00000. 
86 
00:08:09,530 --> 00:08:11,066 
You can view the results with more. 
1 
00:00:01,550 --> 00:00:02,530 
In this hands on activity, 
2 
00:00:02,530 --> 00:00:05,190 
we will be using Spark Streaming
to read weather data. 
3 
00:00:06,900 --> 00:00:09,470 
First, we open
the Spark Streaming Jupyter Notebook. 
4 
00:00:10,610 --> 00:00:13,680 
Next, we will look at sensor format and
measurement types. 
5 
00:00:14,695 --> 00:00:19,110 
We'll then create a Spark DStream of
weather data, read the measurements, and 
6 
00:00:19,110 --> 00:00:20,630 
create a sliding window of the data. 
7 
00:00:20,630 --> 00:00:26,180 
We will define a function to display the
maximum and minimum values in the window. 
8 
00:00:26,180 --> 00:00:28,770 
We start to stream processing
to give their results. 
9 
00:00:31,350 --> 00:00:35,610 
Before we begin this activity, we need
to change the virtual box settings for 
10 
00:00:35,610 --> 00:00:37,100 
our carder virtual machine. 
11 
00:00:39,110 --> 00:00:42,330 
Start streaming needs more
than one thread of execution. 
12 
00:00:42,330 --> 00:00:45,620 
So we need to change the settings to
add more than one virtual processor. 
13 
00:00:47,180 --> 00:00:52,930 
First, shut down your cloudera virtual
machine and go to the virtual box manager. 
14 
00:00:54,960 --> 00:00:58,300 
Select the cloudera virtual box and
click on settings. 
15 
00:01:00,720 --> 00:01:05,040 
Next, click on system, click on Processor. 
16 
00:01:07,520 --> 00:01:14,444 
And change the number of
CPU's to be two or more. 
17 
00:01:14,444 --> 00:01:19,910 
When you're done, click okay,
and start the machine as usual. 
18 
00:01:23,740 --> 00:01:24,760 
Let's begin. 
19 
00:01:24,760 --> 00:01:27,280 
First, click on the browser icon
at the top of the tool bar. 
20 
00:01:29,370 --> 00:01:35,560 
Navigate to the Jupyter Notebook server,
monitoring local host calling 8889. 
21 
00:01:35,560 --> 00:01:39,778 
We'll then go in to downloads. 
22 
00:01:39,778 --> 00:01:43,337 
Big data 3. 
23 
00:01:43,337 --> 00:01:46,186 
Spark-streaming. 
24 
00:01:46,186 --> 00:01:48,616 
Let's then open Spark-Streaming notebook. 
25 
00:01:51,346 --> 00:01:55,070 
This first line, shows the example
data we get from the weather station. 
26 
00:01:56,680 --> 00:01:59,680 
Each line has a time stamp and
a set of measurements. 
27 
00:02:01,860 --> 00:02:05,200 
Each of these abbreviations is
a particular type of measurement, 
28 
00:02:05,200 --> 00:02:06,680 
followed by the actual value. 
29 
00:02:09,140 --> 00:02:11,328 
The next cell shows the key for
these measurements. 
30 
00:02:11,328 --> 00:02:16,190 
For this hands-on, we are interested
in the average wind direction. 
31 
00:02:16,190 --> 00:02:18,059 
Which is abbreviated as DM. 
32 
00:02:20,470 --> 00:02:24,178 
This next cell, defines a function
that parses each line of text and 
33 
00:02:24,178 --> 00:02:26,080 
pulls out the average wind speed. 
34 
00:02:27,610 --> 00:02:29,860 
We define it here, so
we don't have to type it in later. 
35 
00:02:31,360 --> 00:02:32,627 
Let's run this cell. 
36 
00:02:35,308 --> 00:02:38,790 
Next, let's create
a streaming spark context. 
37 
00:02:38,790 --> 00:02:40,870 
First, we'll need to import the module. 
38 
00:02:40,870 --> 00:02:46,590 
We'll enter from pyspark.streaming
import StreamingContext. 
39 
00:02:46,590 --> 00:02:51,130 
We can create a new streaming context. 
40 
00:02:51,130 --> 00:02:52,920 
We'll put in in a variable called ssc. 
41 
00:02:54,400 --> 00:02:59,218 
We'll enter ssc = StreamingContext(sc,1). 
42 
00:02:59,218 --> 00:03:02,710 
The SC is a StreamingContext. 
43 
00:03:02,710 --> 00:03:07,070 
The 1 specifies the batch interval,
1 second in this case. 
44 
00:03:07,070 --> 00:03:07,760 
Let's run this. 
45 
00:03:10,040 --> 00:03:11,580 
Next, we'll create a dstream. 
46 
00:03:13,100 --> 00:03:17,100 
We'll import the streaming weather data,
over a TCP connection. 
47 
00:03:17,100 --> 00:03:19,099 
We'll put this in a dstream called, Lines. 
48 
00:03:21,940 --> 00:03:26,249 
Let's say lines = ssc.socketTextStream, 
49 
00:03:26,249 --> 00:03:31,661 
we'll enter the host name in
port of the weather station, 
50 
00:03:31,661 --> 00:03:35,757 
rtd.hpwren.ucsd.edu for 12028. 
51 
00:03:35,757 --> 00:03:36,784 
Let's run this. 
52 
00:03:40,924 --> 00:03:46,260 
Next, we'll create a new d-stream called
vals that would hold the measurements. 
53 
00:03:46,260 --> 00:03:52,390 
We'll say vals = lines.flatMap parse. 
54 
00:03:52,390 --> 00:03:55,020 
This calls the parse function,
we defined above for 
55 
00:03:55,020 --> 00:03:57,228 
each of the lines coming
from the weather station. 
56 
00:03:57,228 --> 00:04:01,620 
The resulting D-Stream will have just
the average wind direction values. 
57 
00:04:02,850 --> 00:04:03,570 
We'll run this. 
58 
00:04:07,150 --> 00:04:11,340 
Next, we'll create a window that
will aggregate the D-Stream values. 
59 
00:04:13,250 --> 00:04:17,470 
We'll say, window = vals.window(10,5). 
60 
00:04:17,470 --> 00:04:22,850 
The first argument specifies that the
length of the window should be 10 seconds. 
61 
00:04:22,850 --> 00:04:27,515 
The second argument specifies that
the window should move every 5 seconds. 
62 
00:04:27,515 --> 00:04:29,900 
Let's run this. 
63 
00:04:31,740 --> 00:04:34,480 
Next, we'll define a function
that prints the minimum and 
64 
00:04:34,480 --> 00:04:36,240 
maximum values that we see. 
65 
00:04:36,240 --> 00:04:37,710 
We'll start by entering the definition. 
66 
00:04:39,440 --> 00:04:44,230 
Def stats,
this will take an rdd as an argument. 
67 
00:04:47,330 --> 00:04:50,478 
Next, let's print the entire
contents of the rdd. 
68 
00:04:50,478 --> 00:04:54,736 
Print, parenthesis rdd.collect, 
69 
00:04:54,736 --> 00:04:59,932 
this'll print the entire
content of the rdd. 
70 
00:04:59,932 --> 00:05:01,484 
In a real big data application, 
71 
00:05:01,484 --> 00:05:04,020 
this will be impractical due
to the size of the data. 
72 
00:05:05,020 --> 00:05:07,940 
However, for this hands on,
the rdd is small, and so 
73 
00:05:07,940 --> 00:05:10,330 
we can use this to see
the contents of the rdd. 
74 
00:05:12,720 --> 00:05:14,670 
Next, we'll print the min and max. 
75 
00:05:15,940 --> 00:05:17,279 
Before we do that however, 
76 
00:05:17,279 --> 00:05:21,303 
we should check to make sure that
the size of the rdd is greater than zero. 
77 
00:05:21,303 --> 00:05:24,767 
We'll check that rdd.count
is greater than 0. 
78 
00:05:29,285 --> 00:05:34,647 
Finally, we'll print the MinID, MAX. 
79 
00:05:34,647 --> 00:05:39,826 
We'll enter print (“max = {} min = 
80 
00:05:39,826 --> 00:05:44,853 
{}”) Outside of the quote we'll do 
81 
00:05:44,853 --> 00:05:50,653 
.format(rdd.max,rdd.min())). 
82 
00:05:50,653 --> 00:05:54,286 
Let's run this, next, 
83 
00:05:54,286 --> 00:05:59,860 
let's call this function stats. 
84 
00:05:59,860 --> 00:06:01,944 
So all the rdds in our sliding window. 
85 
00:06:01,944 --> 00:06:08,366 
I'll enter window.foreachRDD(stats). 
86 
00:06:08,366 --> 00:06:13,539 
Run this. 
87 
00:06:13,539 --> 00:06:16,110 
We're now ready to start
our streaming processing. 
88 
00:06:16,110 --> 00:06:19,880 
We can do this by entering ssc.start. 
89 
00:06:19,880 --> 00:06:21,567 
We'll run this to start the streaming. 
90 
00:06:29,214 --> 00:06:32,975 
When we want to stop this streaming,
we'll run ssc.stop 
91 
00:06:38,028 --> 00:06:40,200 
Please scroll up and
look at the beginning of the output. 
92 
00:06:43,130 --> 00:06:47,770 
We'll see that it's printing the full
window and the min and max values. 
93 
00:06:50,010 --> 00:06:52,610 
Notice that in the beginning,
the window is not yet filled. 
94 
00:06:52,610 --> 00:06:54,610 
In this case, there's only three entries. 
95 
00:06:55,760 --> 00:06:59,029 
We count to see that the window
is moving by five measurements. 
96 
00:07:00,220 --> 00:07:03,510 
For example, the last five
measurements in the second window, 
97 
00:07:04,540 --> 00:07:06,870 
are the first five measurements
in the third window. 
1 
00:00:00,770 --> 00:00:04,034 
In this hands on activity we
will be using SparkSQL to 
2 
00:00:04,034 --> 00:00:06,119 
query data from an SQL database. 
3 
00:00:07,604 --> 00:00:11,825 
First we will open
the SparkSQL Jupyter Notebook. 
4 
00:00:11,825 --> 00:00:14,080 
We will connect Spark to a Postgres table. 
5 
00:00:15,740 --> 00:00:20,214 
And then view the Spark DataFrame
schema and count the rows. 
6 
00:00:20,214 --> 00:00:22,040 
We will view the contents
of the data frame. 
7 
00:00:23,220 --> 00:00:25,110 
See how to filter rows and columns. 
8 
00:00:26,130 --> 00:00:29,174 
And finally perform aggregate
operation on a column. 
9 
00:00:33,636 --> 00:00:34,714 
Let's begin. 
10 
00:00:34,714 --> 00:00:38,247 
First, click on the browser icon,
the top of the toolbar. 
11 
00:00:38,247 --> 00:00:41,062 
>> [SOUND]
>> Next, 
12 
00:00:41,062 --> 00:00:45,288 
navigate to the Jupyter Notebook server. 
13 
00:00:45,288 --> 00:00:48,275 
It's localhost:8889. 
14 
00:00:51,117 --> 00:00:54,358 
Go to Downloads, 
15 
00:00:54,358 --> 00:00:58,900 
Big Data 3, Spark SQL. 
16 
00:01:00,460 --> 00:01:02,040 
To open the SparkSQL Notebook. 
17 
00:01:02,040 --> 00:01:07,710 
The first three cells have
already been entered for you. 
18 
00:01:09,760 --> 00:01:16,915 
First, we import the SQLContext, run this. 
19 
00:01:16,915 --> 00:01:24,878 
Next, we create an SQLContext
from the SparkContext run this. 
20 
00:01:24,878 --> 00:01:29,676 
And next, we'll create a Spark DataFrame
from a Postgres table. 
21 
00:01:32,980 --> 00:01:34,950 
We used the read attribute format. 
22 
00:01:36,820 --> 00:01:40,020 
The jdbc argument means that we're
using a Java database connection. 
23 
00:01:42,180 --> 00:01:44,000 
The next line sets the URL option. 
24 
00:01:44,000 --> 00:01:49,081 
It says we're using Postgres
database running on the local host. 
25 
00:01:49,081 --> 00:01:54,287 
The database name is Cloudera and
the username is Cloudera. 
26 
00:01:54,287 --> 00:01:56,048 
The second option, DB table, 
27 
00:01:56,048 --> 00:02:00,000 
says we want our data frame
to be the game clicks table. 
28 
00:02:00,000 --> 00:02:01,130 
And finally we call load. 
29 
00:02:03,290 --> 00:02:04,400 
Let's execute this. 
30 
00:02:06,370 --> 00:02:11,153 
You can see the schema of the data
frame by calling df.printschema. 
31 
00:02:14,602 --> 00:02:17,593 
This shows the name of each
column along with the data type. 
32 
00:02:20,499 --> 00:02:24,253 
We can count the rows in this
df frame by calling df.count. 
33 
00:02:31,512 --> 00:02:35,786 
We can look at the first five
rows by calling df.show(5) 
34 
00:02:39,493 --> 00:02:41,362 
This shows all the columns
in the data frame. 
35 
00:02:43,240 --> 00:02:46,260 
We can select specific columns
by using the select method. 
36 
00:02:48,080 --> 00:02:52,275 
Let's select just the User ID and
Team Level columns. 
37 
00:02:52,275 --> 00:02:57,860 
I'll enter df.select("userid",teamlevel. 
38 
00:02:57,860 --> 00:02:58,641 
Parenthesis. 
39 
00:03:01,524 --> 00:03:04,610 
And finally we only want to
see the top five rows. 
40 
00:03:04,610 --> 00:03:06,312 
So we'll do .show(5). 
41 
00:03:10,870 --> 00:03:15,368 
We can also select rows
that have a specific value. 
42 
00:03:15,368 --> 00:03:19,067 
Let's look for the rows where
the team level is greater than one. 
43 
00:03:19,067 --> 00:03:23,092 
We'll enter df.filter. 
44 
00:03:23,092 --> 00:03:27,393 
We'll specify that we want team level
greater than one by entering df, 
45 
00:03:27,393 --> 00:03:30,412 
square bracket, team level,
greater than one. 
46 
00:03:34,336 --> 00:03:38,183 
And again, we only want the user ID and
team level columns. 
47 
00:03:43,078 --> 00:03:45,368 
And finally only the first five rows. 
48 
00:03:50,624 --> 00:03:55,366 
We can use the group by method to
aggregate a particular column. 
49 
00:03:55,366 --> 00:03:59,542 
For example, the ishit column
has a value of zero or one. 
50 
00:03:59,542 --> 00:04:04,611 
And we can use group by to count how
many times each of these values occurs. 
51 
00:04:04,611 --> 00:04:12,068 
Or a df.groupby ishit, and
we'll call count to count the values 
52 
00:04:21,520 --> 00:04:25,552 
We can also perform aggregate statistical
operations on the data in a data frame. 
53 
00:04:27,000 --> 00:04:31,140 
Let's compute the mean and
sum values for ishit. 
54 
00:04:31,140 --> 00:04:36,756 
First we need to import
the statistical functions we'll 
55 
00:04:36,756 --> 00:04:41,910 
run from.pyspark.sql.functions
import star. 
56 
00:04:41,910 --> 00:04:47,891 
Next we'll run df.select
(mean) ishit,sum ishit 
57 
00:04:55,375 --> 00:04:58,580 
We can also join two data
frames on a particular column. 
58 
00:04:59,980 --> 00:05:04,740 
Let's join the existing data frame of the
game clicks table with the adclicks table. 
59 
00:05:05,850 --> 00:05:08,410 
First, we need to create data frame for
the adclicks table. 
60 
00:05:09,810 --> 00:05:10,796 
Let's go back up. 
61 
00:05:15,286 --> 00:05:16,991 
Copy the content of this cell, 
62 
00:05:23,747 --> 00:05:26,795 
Paste it. 
63 
00:05:26,795 --> 00:05:31,356 
We put the adclicks table
the data frame called df2. 
64 
00:05:31,356 --> 00:05:34,462 
And we'll change the db table
option to the adclicks. 
65 
00:05:38,376 --> 00:05:39,388 
Run it. 
66 
00:05:41,687 --> 00:05:44,117 
Let's print the schema of df2. 
67 
00:05:52,283 --> 00:05:56,064 
You can see that it also has
a column called user id. 
68 
00:05:56,064 --> 00:06:00,024 
So let's join the game clicks
data frame with the add 
69 
00:06:00,024 --> 00:06:02,643 
clicks data frame on this column. 
70 
00:06:02,643 --> 00:06:06,229 
We put the result in a new
data frame called merged. 
71 
00:06:06,229 --> 00:06:15,490 
We'll say merge = df.join.df2 "userid". 
72 
00:06:15,490 --> 00:06:15,990 
We'll run it. 
73 
00:06:17,820 --> 00:06:19,163 
Let's look at the schema. 
74 
00:06:19,163 --> 00:06:21,193 
We'll call merge.printschema. 
75 
00:06:25,763 --> 00:06:29,499 
We can see that this merged data frame
has the column for both game clicks and 
76 
00:06:29,499 --> 00:06:30,105 
adclicks. 
77 
00:06:31,780 --> 00:06:35,537 
Finally we'll look at the top five
rows in this merged data frame. 
78 
00:06:35,537 --> 00:06:38,208 
We'll run merge.show. 
1 
00:00:02,020 --> 00:00:05,610 
We have now seen some
simple transformations and 
2 
00:00:05,610 --> 00:00:10,900 
how Spark can create RDDs from
each other using transformations. 
3 
00:00:10,900 --> 00:00:15,390 
We learned that transformations are
evaluated after an action is performed. 
4 
00:00:16,810 --> 00:00:21,800 
So we can simply define actions as RDD
operations that trigger the evaluation of 
5 
00:00:21,800 --> 00:00:27,530 
the transformation pipeline and return
the final result to the driver program or 
6 
00:00:27,530 --> 00:00:29,810 
save the results to a persistent storage. 
7 
00:00:31,660 --> 00:00:36,580 
We can also call them the last
step in a Spark pipeline. 
8 
00:00:36,580 --> 00:00:38,700 
Let's now look at a few action operations. 
9 
00:00:40,870 --> 00:00:46,150 
After this video, you will be able to
explain the steps of a Spark pipeline 
10 
00:00:46,150 --> 00:00:52,010 
ending with a collect action and list
four common action operations in Spark. 
11 
00:00:54,750 --> 00:00:57,740 
A very common action in Spark is collect. 
12 
00:00:59,090 --> 00:01:03,875 
In this example, we can imagine that
initially we are reading from HDFS. 
13 
00:01:05,390 --> 00:01:10,020 
The RDD partitions that go through
the transformation steps in our big data 
14 
00:01:10,020 --> 00:01:14,970 
pipeline are defined as flatMap and
groupbyKey. 
15 
00:01:16,290 --> 00:01:21,625 
When the final step is done,
the collect action is called and 
16 
00:01:21,625 --> 00:01:25,840 
Spark sends all the tasks for
execution to the worker notes. 
17 
00:01:28,372 --> 00:01:32,940 
Collect will send all the resulting
RDDs from the workers and 
18 
00:01:32,940 --> 00:01:37,140 
copy them to the Java virtual
machine on the driver program. 
19 
00:01:37,140 --> 00:01:41,600 
And then, this will be piped
also to our Python shell. 
20 
00:01:43,150 --> 00:01:48,076 
While collect copies all the data,
another action, take, 
21 
00:01:48,076 --> 00:01:51,720 
copies the first n results of the driver. 
22 
00:01:53,690 --> 00:01:57,440 
If the results are too large
to fit in the driver memory, 
23 
00:01:57,440 --> 00:02:01,290 
then there's an opportunity to write
them directly to HDFS instead. 
24 
00:02:03,230 --> 00:02:07,850 
Among many other actions,
reduce is probably the most famous one. 
25 
00:02:08,980 --> 00:02:13,300 
Reduce takes two elements and
returns a result, like sum. 
26 
00:02:13,300 --> 00:02:19,680 
But in this case, we don't have a key,
we just have a large area of some values. 
27 
00:02:19,680 --> 00:02:22,150 
And we are running this function over and 
28 
00:02:22,150 --> 00:02:26,580 
over again to reduce everything
to one single value. 
29 
00:02:26,580 --> 00:02:29,210 
For example,
to the global sum of everything. 
30 
00:02:30,905 --> 00:02:34,945 
Another very useful action Is saveAsText, 
31 
00:02:34,945 --> 00:02:38,585 
to save the results to local disk or
HDFS, and 
32 
00:02:38,585 --> 00:02:44,485 
this is very useful if the output of
the power computation is pretty large. 
1 
00:00:02,200 --> 00:00:06,550 
Hello, I hope you enjoyed your first
programming experience with Spark. 
2 
00:00:07,580 --> 00:00:10,890 
Although the words count
example is simple, 
3 
00:00:10,890 --> 00:00:14,420 
it is useful in starting to
understand how to work with RDDs. 
4 
00:00:16,040 --> 00:00:22,820 
After this video, you'll be able to use
two methods to create RDDs in Spark, 
5 
00:00:22,820 --> 00:00:28,500 
explain what immutable means,
interpret a Spark program as a pipeline 
6 
00:00:28,500 --> 00:00:33,860 
of transformations and actions, and
list the steps to create a Spark program. 
7 
00:00:36,390 --> 00:00:37,970 
So let's remember where we are. 
8 
00:00:39,540 --> 00:00:43,620 
We have a Driver Program that
defines the Spark context. 
9 
00:00:44,700 --> 00:00:48,419 
This is the entry point
to your application. 
10 
00:00:48,419 --> 00:00:53,980 
The driver converts all the data to RDDs,
and 
11 
00:00:53,980 --> 00:00:59,720 
everything from this point on
gets managed using the RDDs. 
12 
00:00:59,720 --> 00:01:03,320 
RDDs can be constructed from files or
any other storage. 
13 
00:01:04,500 --> 00:01:08,650 
They can also be constructed from
data structures for collections and 
14 
00:01:08,650 --> 00:01:10,150 
programs, like lists. 
15 
00:01:11,940 --> 00:01:18,306 
All the transformations and actions on
these RDDs take place either locally, 
16 
00:01:18,306 --> 00:01:22,560 
or on the Worker Nodes
managed by a Cluster Manager. 
17 
00:01:25,190 --> 00:01:29,560 
Each transformation results in
a new updated version of the RDD. 
18 
00:01:29,560 --> 00:01:33,480 
The RDDs at the end get converted and 
19 
00:01:33,480 --> 00:01:37,319 
saved in a persistent storage like HDFS or
your local drive. 
20 
00:01:40,000 --> 00:01:46,480 
As we mentioned before,
RDDs get created in the Driver Program. 
21 
00:01:46,480 --> 00:01:48,680 
The developer of the Driver Program, 
22 
00:01:48,680 --> 00:01:52,820 
who in this case is you,
is responsible for creating them. 
23 
00:01:55,030 --> 00:01:58,650 
You can just read in a file
through your Spark Context, or 
24 
00:01:58,650 --> 00:02:04,300 
as we have in this example,
you can provide an existing collection, 
25 
00:02:04,300 --> 00:02:07,820 
like a list to be turned into
a distributed collection. 
26 
00:02:10,400 --> 00:02:15,130 
You can also create an integer
RDD using parallelize, 
27 
00:02:16,340 --> 00:02:18,690 
and provide a number of partitions for 
28 
00:02:18,690 --> 00:02:23,000 
distribution as we do create
the numbers RDD in this line. 
29 
00:02:25,850 --> 00:02:29,440 
Here, the range function in Python 
30 
00:02:30,460 --> 00:02:33,738 
will give us a list of
numbers starting from 0 to 9. 
31 
00:02:33,738 --> 00:02:40,340 
The parallelize function
will create three partitions 
32 
00:02:40,340 --> 00:02:45,160 
of the RDD to be distributed, based on
the parameter that was provided to it. 
33 
00:02:46,440 --> 00:02:51,040 
Spark will decide how to assign partitions
to our executors and worker nodes. 
34 
00:02:53,110 --> 00:02:58,037 
The distributed RDDs can in the end be
gathered into a single partition on 
35 
00:02:58,037 --> 00:03:01,286 
the driver using
the collect transformation. 
36 
00:03:07,478 --> 00:03:12,225 
Now let's think of a scenario were we
start processing the created RDDs. 
37 
00:03:14,060 --> 00:03:19,252 
There are two types of operations
that help with processing in Spark, 
38 
00:03:19,252 --> 00:03:22,158 
namely Transformations and Actions. 
39 
00:03:24,829 --> 00:03:29,734 
All partitions written in RDD,
go through the same transformation in 
40 
00:03:29,734 --> 00:03:34,820 
the worker node, executors when
a transformation is applied to an RDD. 
41 
00:03:36,420 --> 00:03:40,110 
Spark uses lazy evaluation for
transformations. 
42 
00:03:41,470 --> 00:03:45,310 
That means they will not be
immediately executed, but 
43 
00:03:45,310 --> 00:03:47,610 
instead wait for
an action to be performed. 
44 
00:03:49,440 --> 00:03:53,890 
The transformations get computed
when an action is executed. 
45 
00:03:53,890 --> 00:03:56,810 
For this reason,
a lot of times you will see run 
46 
00:03:56,810 --> 00:04:01,300 
time errors showing up at the action stage
and not at the transformation stages. 
47 
00:04:02,340 --> 00:04:04,840 
It is very similar to Haskell or Erlang, 
48 
00:04:04,840 --> 00:04:07,120 
if any of you are familiar
with these languages. 
49 
00:04:09,730 --> 00:04:12,870 
Let's put some names on
these transformations. 
50 
00:04:12,870 --> 00:04:18,530 
We can have a pipeline by converting a
text file into an RDD with two partitions. 
51 
00:04:19,840 --> 00:04:25,440 
Filter some values out of it, and
maybe apply a map function to it. 
52 
00:04:25,440 --> 00:04:30,350 
In the end, the run,
the collect action on the mapped RDDs 
53 
00:04:30,350 --> 00:04:34,620 
to evaluate the results of the pipeline
and convert the outputs into results. 
54 
00:04:35,680 --> 00:04:41,440 
Here, filter and map are transformations,
and collect is the action. 
55 
00:04:43,340 --> 00:04:47,990 
Although the RDDs are in memory,
and they are not persistent, 
56 
00:04:47,990 --> 00:04:51,240 
we can use the cash function
to make them persistent cash. 
57 
00:04:53,090 --> 00:04:58,684 
For example, in order to reuse the RDD
created from a database query that could 
58 
00:04:58,684 --> 00:05:03,792 
otherwise be costly to re-execute,
we can instead cache these RDDs. 
59 
00:05:06,461 --> 00:05:09,883 
We need to use caution when
using the cache option, 
60 
00:05:09,883 --> 00:05:14,432 
as it can consume too much memory and
generate a bottleneck itself. 
61 
00:05:17,713 --> 00:05:25,070 
As a part of the Word Count example, we
mapped the words RDD to generate tuples. 
62 
00:05:25,070 --> 00:05:29,350 
We then applied reduceByKey
to tuples to generate counts. 
63 
00:05:30,470 --> 00:05:34,465 
In the end, we convert the number
of partitions to one so 
64 
00:05:34,465 --> 00:05:38,120 
that output is one file
when written to this later. 
65 
00:05:38,120 --> 00:05:41,980 
Otherwise, output will be spread
over multiple files on disk. 
66 
00:05:43,572 --> 00:05:49,410 
Finally, saveAsTextFile is an action
that kickstarts the computation and 
67 
00:05:49,410 --> 00:05:50,120 
writes to disk. 
68 
00:05:52,040 --> 00:05:55,700 
To summarize, in a typical Spark program 
69 
00:05:55,700 --> 00:06:00,400 
we create RDDs from external storage or
local collections like lists. 
70 
00:06:01,610 --> 00:06:05,130 
Then we apply transformations
to these RDDs, 
71 
00:06:05,130 --> 00:06:09,820 
like filter, map, and reduceByKey. 
72 
00:06:09,820 --> 00:06:14,800 
These transformations get lazily
evaluated until an action is performed. 
73 
00:06:15,970 --> 00:06:22,330 
Actions are performed both for local and
parallel computation to generate results. 
74 
00:06:22,330 --> 00:06:26,780 
Next, we will talk more about
transformation and actions in Spark. 
1 
00:00:01,210 --> 00:00:05,330 
In the last video,
we talked about the programming model for 
2 
00:00:05,330 --> 00:00:10,930 
Spark where RDD's get generated from
external datasets and gets partitioned. 
3 
00:00:12,450 --> 00:00:18,710 
We said, RDDs are immutable meaning they
can't be changed in place even partially. 
4 
00:00:19,740 --> 00:00:23,690 
They need a transformation
operation applied to them and 
5 
00:00:23,690 --> 00:00:25,650 
get converted into a new RDD. 
6 
00:00:27,650 --> 00:00:32,652 
This is essential for keeping track
of all the processing that has been 
7 
00:00:32,652 --> 00:00:38,094 
applied to our dataset providing the
ability to keep a linear chain of RDDs. 
8 
00:00:38,094 --> 00:00:44,360 
In addition, as a part of a big data
pipeline, we start with an RDD. 
9 
00:00:44,360 --> 00:00:48,842 
And through several transformation steps,
many other RDDs as 
10 
00:00:48,842 --> 00:00:53,840 
intermediate products get executed
until we get to our final result. 
11 
00:00:55,070 --> 00:00:58,910 
We also mention that
an important feature of Spark 
12 
00:00:58,910 --> 00:01:02,060 
is that all these transformation are lazy. 
13 
00:01:03,820 --> 00:01:07,820 
This means they don't execute
immediately when applied to an RDD. 
14 
00:01:09,040 --> 00:01:13,670 
So when we apply a transformation,
nothing happens right away. 
15 
00:01:13,670 --> 00:01:17,850 
We are basically preparing our big
data pipeline to be executed later. 
16 
00:01:18,990 --> 00:01:23,936 
When we are done defining all
the transformations and perform an action, 
17 
00:01:23,936 --> 00:01:28,961 
Spark will take care of finding the best
way to execute this computation and 
18 
00:01:28,961 --> 00:01:32,729 
then start all the necessary
tasks in our worker nodes. 
19 
00:01:32,729 --> 00:01:37,108 
In this video, we will explain some
common transformation in Spark. 
20 
00:01:38,985 --> 00:01:43,537 
After this video, you will be able
to explain the difference between 
21 
00:01:43,537 --> 00:01:47,001 
a narrow transformation and
wide transformation. 
22 
00:01:47,001 --> 00:01:51,748 
Describe map, flatmap,
filter and coalesce as narrow 
23 
00:01:51,748 --> 00:01:56,404 
transformations and
list two wide transformations. 
24 
00:01:59,240 --> 00:02:04,330 
Let's take at look at, probably the
simplest transformation, which is a map. 
25 
00:02:06,280 --> 00:02:10,160 
By now,
you're well versed in home networks. 
26 
00:02:10,160 --> 00:02:16,240 
It applies the function to each
partition or element of an RDD. 
27 
00:02:16,240 --> 00:02:19,690 
This is a one to one transformation. 
28 
00:02:19,690 --> 00:02:24,599 
It is also in the category of element-wise
transformations since it transforms 
29 
00:02:24,599 --> 00:02:26,847 
every element of an RDD separately. 
30 
00:02:30,805 --> 00:02:34,945 
The code example in the blue
box here applies a function 
31 
00:02:34,945 --> 00:02:38,645 
called lower to all
the elements in a text_RDD. 
32 
00:02:39,900 --> 00:02:40,960 
The lower function 
33 
00:02:42,180 --> 00:02:45,810 
turns all the characters in
a line to lower case letters. 
34 
00:02:46,880 --> 00:02:51,650 
So the input is one line
of text with any kind of 
35 
00:02:51,650 --> 00:02:56,360 
capitalization and the outfit is going
to be the same line, all lower case. 
36 
00:02:58,130 --> 00:03:03,160 
In this example, we have two worker
nodes drawn as orange boxes. 
37 
00:03:04,710 --> 00:03:08,328 
The black boxes are partitions
of our dataset. 
38 
00:03:08,328 --> 00:03:11,842 
We work by partition and not by element. 
39 
00:03:11,842 --> 00:03:17,036 
As you would remember this, it is the
difference between Spark and MapReduce. 
40 
00:03:19,488 --> 00:03:24,418 
The partition is just a chunk of our data
with some number of elements in it and 
41 
00:03:24,418 --> 00:03:28,964 
the map function gets applied to all
elements in that partition in each 
42 
00:03:28,964 --> 00:03:30,440 
worker node locally. 
43 
00:03:31,850 --> 00:03:35,748 
Each node applies the map
function to the data or 
44 
00:03:35,748 --> 00:03:39,555 
RDD partition they received independently. 
45 
00:03:39,555 --> 00:03:43,560 
Let's look at a few more in
element-wise transformation category. 
46 
00:03:46,251 --> 00:03:49,190 
FlatMap is very similar to map. 
47 
00:03:50,290 --> 00:03:55,573 
However, instead of returning
an individual element for each map, 
48 
00:03:55,573 --> 00:04:01,323 
it returns an RDD with an aggregate of
all the results for all the elements. 
49 
00:04:01,323 --> 00:04:03,888 
In the example in the blue box, 
50 
00:04:03,888 --> 00:04:08,132 
the split_words fuction
takes a line as an input, 
51 
00:04:08,132 --> 00:04:13,970 
which is one element and it's output
is each word as a single element. 
52 
00:04:13,970 --> 00:04:18,120 
So, it splits a line to words. 
53 
00:04:19,760 --> 00:04:21,570 
The same thing gets done for each line. 
54 
00:04:23,180 --> 00:04:26,130 
When the output for
all the lines is flattened, 
55 
00:04:26,130 --> 00:04:29,520 
we get a simple
one-dimensional list of words. 
56 
00:04:31,220 --> 00:04:36,480 
So, we'll get all the words in
all the lines in just one list. 
57 
00:04:38,070 --> 00:04:43,703 
Depending on the line length the output
partitions might be of different sizes. 
58 
00:04:43,703 --> 00:04:47,463 
Detected here by the height of
each black box being different. 
59 
00:04:49,977 --> 00:04:54,750 
In Spark terms, map and
flatMap are narrow transformations. 
60 
00:04:56,040 --> 00:05:01,390 
Narrow transformation refers to
the processing where the processing 
61 
00:05:01,390 --> 00:05:06,827 
logic depends only on data that is
already residing in the partition and 
62 
00:05:06,827 --> 00:05:09,466 
data shuffling is not necessary. 
63 
00:05:12,080 --> 00:05:16,030 
Another very important
transformation is filter. 
64 
00:05:16,030 --> 00:05:21,957 
Often, we're interested just
in a subset of our data or 
65 
00:05:21,957 --> 00:05:25,118 
we want to get rid of bad data. 
66 
00:05:25,118 --> 00:05:30,105 
Filter transformation takes the function
take executes on each element 
67 
00:05:30,105 --> 00:05:31,575 
of a RDD partition and 
68 
00:05:31,575 --> 00:05:36,740 
returns only the elements that
the transformation element returns true. 
69 
00:05:39,219 --> 00:05:44,281 
The example code in the blue box here,
applies a filter 
70 
00:05:44,281 --> 00:05:50,014 
function that filters out words
that start with the letter a. 
71 
00:05:50,014 --> 00:05:54,130 
The function starts with a,
takes the input word, 
72 
00:05:54,130 --> 00:06:00,080 
then transforms it to lowercase and
then checks if the word starts with a. 
73 
00:06:02,920 --> 00:06:08,640 
So, the output of this operation will be
a list with only words that start with a. 
74 
00:06:10,870 --> 00:06:12,670 
This is another narrow transformation. 
75 
00:06:14,270 --> 00:06:17,930 
So, it only gets executed locally 
76 
00:06:17,930 --> 00:06:22,070 
without the need to shuffle any RDD
partitions across the word kernels. 
77 
00:06:24,320 --> 00:06:30,486 
The output of filter depends on
the input and the filter functions. 
78 
00:06:30,486 --> 00:06:31,663 
In some cases, 
79 
00:06:31,663 --> 00:06:37,003 
even if you started with even RDD
partitions within the worker nodes, 
80 
00:06:37,003 --> 00:06:43,247 
the RDD size can significantly vary across
the workers after a filter operation, 
81 
00:06:43,247 --> 00:06:48,588 
then this happens is a pretty good
idea to join some of those partitions 
82 
00:06:48,588 --> 00:06:53,772 
to increase performance and
even out processing across clusters. 
83 
00:06:53,772 --> 00:07:00,399 
This transformation is called coalesce. 
84 
00:07:00,399 --> 00:07:06,060 
Coalesce simply helps with balancing
the data partition numbers and sizes. 
85 
00:07:07,220 --> 00:07:12,148 
When you have significantly reduced
your initial data after some filters and 
86 
00:07:12,148 --> 00:07:13,817 
other transformations, 
87 
00:07:13,817 --> 00:07:18,311 
having a large number of partitions
might not be very useful anymore. 
88 
00:07:18,311 --> 00:07:19,063 
In this case, 
89 
00:07:19,063 --> 00:07:23,521 
you can use coalesce to reduce the number
of partitions to a more manageable number. 
90 
00:07:26,421 --> 00:07:31,941 
Until now, we talked about narrow
transformations that happen in a worker 
91 
00:07:31,941 --> 00:07:36,947 
node locally without having to
transfer data through the network. 
92 
00:07:36,947 --> 00:07:40,572 
Now, let's start talking
about wide transformations. 
93 
00:07:43,048 --> 00:07:45,693 
Let's remember our Word Count example. 
94 
00:07:45,693 --> 00:07:52,420 
As a part of the Word Count example, we
map the words RDD could generate tuples. 
95 
00:07:52,420 --> 00:07:58,110 
The output of map is a key value pair
list where the key is the word and 
96 
00:07:58,110 --> 00:07:59,540 
the value is always one. 
97 
00:08:01,420 --> 00:08:07,390 
We then apply it reduceByKey
to tuples to generate counts, 
98 
00:08:07,390 --> 00:08:10,360 
which simply sums the values for
each key or word. 
99 
00:08:11,670 --> 00:08:18,555 
Let's imagine for a second that we use
groupByKey instead of reduceByKey. 
100 
00:08:18,555 --> 00:08:23,453 
We will come back to reduceByKey
in just a little bit. 
101 
00:08:23,453 --> 00:08:26,774 
Remember, mapped outputs tuples, 
102 
00:08:26,774 --> 00:08:31,920 
which is a list of key value
pairs in the forms of word one. 
103 
00:08:33,670 --> 00:08:39,146 
At each worker node, we will have
tuples that have the same word as key. 
104 
00:08:39,146 --> 00:08:45,213 
In this example, apple as the key and
1 as the count and 2 worker nodes. 
105 
00:08:47,720 --> 00:08:52,367 
Trying to group together, all the counts
of a word across worker nodes 
106 
00:08:52,367 --> 00:08:55,690 
requires shuffling of
data between these nodes. 
107 
00:08:56,800 --> 00:09:00,466 
Just like we do for the word apple here. 
108 
00:09:00,466 --> 00:09:05,616 
GroupByKey is the transformation
that helps us combine values with 
109 
00:09:05,616 --> 00:09:11,596 
the same key into a list without applying
a special user define function to it. 
110 
00:09:11,596 --> 00:09:17,435 
As you see on the right, the result
of a groupByKey transformation on all 
111 
00:09:17,435 --> 00:09:23,292 
the map outputs by the word apple is
the key ends up in a list with all ones. 
112 
00:09:26,741 --> 00:09:32,109 
If you instead apply the function to
list like summing up all the values, 
113 
00:09:32,109 --> 00:09:35,542 
then we could have had
the word count results. 
114 
00:09:35,542 --> 00:09:36,715 
In this case, 2. 
115 
00:09:36,715 --> 00:09:43,346 
If we instead applied a function to
the list like summing up all the values, 
116 
00:09:43,346 --> 00:09:47,468 
then we would have had
the word count results. 
117 
00:09:47,468 --> 00:09:51,655 
If we need to apply such functions to
a group of values related to a key like 
118 
00:09:51,655 --> 00:09:54,140 
this, we use the reduceByKey operation. 
119 
00:09:56,220 --> 00:10:01,670 
ReduceByKey helps us to combine
the value using a reduce function, 
120 
00:10:01,670 --> 00:10:04,580 
which in the word count
case is a simple summation. 
121 
00:10:06,420 --> 00:10:10,420 
In groupByKey and
reduceByKey transformations, 
122 
00:10:11,650 --> 00:10:15,860 
we observe the behavior that require
shuffling of the data across work nodes, 
123 
00:10:17,350 --> 00:10:20,830 
we call such transformations
wide transformations. 
124 
00:10:22,340 --> 00:10:27,440 
In wide transformation operations,
processing depends on data 
125 
00:10:27,440 --> 00:10:32,940 
residing in multiple partitions
distributed across worker nodes and this 
126 
00:10:32,940 --> 00:10:37,430 
requires data shuffling over the network
to bring related datasets together. 
127 
00:10:39,220 --> 00:10:44,630 
As a summary, we have listed a small
number of transformations in Spark with 
128 
00:10:44,630 --> 00:10:49,240 
some examples and distinguished between
them as narrow and wide transformations. 
129 
00:10:50,520 --> 00:10:55,743 
Although this is a good start,
I advise you to go through the list 
130 
00:10:55,743 --> 00:11:01,760 
provided at the link shown here after
you complete this beginner course. 
131 
00:11:01,760 --> 00:11:06,662 
Read about the rest of the transformations
in Spark before you start programming in 
132 
00:11:06,662 --> 00:11:09,192 
Spark and have fun with transformations. 
1 
00:00:02,170 --> 00:00:05,580 
Lastly, we will introduce
you to Spark GraphX. 
2 
00:00:07,970 --> 00:00:13,021 
After this video,
you will be able to describe GraphX is, 
3 
00:00:13,021 --> 00:00:17,970 
explain how vertices and
edges are stored in GraphX, and 
4 
00:00:17,970 --> 00:00:21,702 
describe how Pregel works at a high level. 
5 
00:00:24,295 --> 00:00:29,875 
GraphX is Apache Spark's Application
Programming Interface for 
6 
00:00:29,875 --> 00:00:33,570 
graphs and graph-parallel computation. 
7 
00:00:33,570 --> 00:00:37,001 
GraphX uses a property graph model. 
8 
00:00:37,001 --> 00:00:40,420 
This means, both nodes. 
9 
00:00:40,420 --> 00:00:44,500 
And edges in a graph can
have attributes and values. 
10 
00:00:46,860 --> 00:00:52,955 
In GraphX, the node properties
are stored in a vertex table and 
11 
00:00:52,955 --> 00:00:57,330 
edge properties are stored
in an edge table. 
12 
00:00:58,930 --> 00:01:04,429 
The connectivity information, that is,
which edge connects which nodes, 
13 
00:01:04,429 --> 00:01:08,501 
is stored separately from the node and
edge properties. 
14 
00:01:11,696 --> 00:01:16,400 
GraphX is built on special RDDs for
vertices and edges. 
15 
00:01:17,890 --> 00:01:23,432 
VertexRDD represents a set of vertices, 
16 
00:01:23,432 --> 00:01:28,823 
all of which have an attribute called A. 
17 
00:01:28,823 --> 00:01:33,865 
The EdgeRDD here extends
this basic edge storing by 
18 
00:01:33,865 --> 00:01:40,326 
the edges in columnar format on
each partition for performance. 
19 
00:01:40,326 --> 00:01:46,740 
Note that VertexID are defined
to be unique by design. 
20 
00:01:47,780 --> 00:01:52,257 
The edge class is an object
with a source vertex and 
21 
00:01:52,257 --> 00:01:56,209 
destination vertex and an edge attribute. 
22 
00:01:58,257 --> 00:02:04,511 
In addition to the vortex and
edge views of the property graph, 
23 
00:02:04,511 --> 00:02:07,584 
GraphX also has triplet view. 
24 
00:02:07,584 --> 00:02:14,350 
The triplet view logically joins
vortex and edge properties. 
25 
00:02:17,003 --> 00:02:21,131 
GraphX has an operator that
can execute operations 
26 
00:02:21,131 --> 00:02:24,880 
from the Pregel library for
graph analytics. 
27 
00:02:26,420 --> 00:02:32,000 
This Pregel operator executes
in a series of super steps 
28 
00:02:32,000 --> 00:02:35,700 
which defines a messaging protocol for
vertices. 
29 
00:02:37,620 --> 00:02:39,824 
We will revisit graph analytics and 
30 
00:02:39,824 --> 00:02:43,946 
using GraphX in more detail in
course five of the specialization. 
31 
00:02:45,934 --> 00:02:51,070 
In summary, Spark can be used for
graph parallel computations. 
32 
00:02:52,300 --> 00:02:58,740 
GraphX uses special RDDs for
storing vertex and edge information. 
33 
00:03:00,170 --> 00:03:04,165 
And the pregel operator works
in a series of super steps. 
1 
00:00:01,580 --> 00:00:04,435 
Now, we will introduce you to Spark MLlib. 
2 
00:00:06,080 --> 00:00:11,020 
After this video, you will be
able to describe what MLlib is, 
3 
00:00:11,020 --> 00:00:15,865 
list the main categories of
techniques available in MLlib, 
4 
00:00:15,865 --> 00:00:20,530 
and explain code segments
containing MLlib algorithms. 
5 
00:00:20,530 --> 00:00:26,750 
MLlib is a scalable machine learning
library that runs on top of Spark Core. 
6 
00:00:26,750 --> 00:00:31,370 
It provides distributed implementations
of commonly used machine learning 
7 
00:00:31,370 --> 00:00:32,750 
algorithms and utilities. 
8 
00:00:34,070 --> 00:00:40,260 
As with Spark Core, MLlib has APIs for
Scala, Java, Python, and R. 
9 
00:00:42,520 --> 00:00:45,120 
MLlib offers many algorithms and 
10 
00:00:45,120 --> 00:00:48,170 
techniques commonly used in
a machine learning process. 
11 
00:00:49,170 --> 00:00:51,710 
The main categories are machine learning, 
12 
00:00:51,710 --> 00:00:55,860 
statistics and some common utility
tools for common techniques. 
13 
00:00:57,120 --> 00:00:58,970 
As the name suggests, 
14 
00:00:58,970 --> 00:01:02,380 
many machine learning algorithms
are available in MLlib. 
15 
00:01:03,760 --> 00:01:08,440 
These are algorithms to build models for
classification, regression, and 
16 
00:01:08,440 --> 00:01:08,990 
clustering. 
17 
00:01:10,140 --> 00:01:14,110 
There are also techniques for
evaluating the resulting models. 
18 
00:01:14,110 --> 00:01:17,740 
For example,
you can compute the values for 
19 
00:01:17,740 --> 00:01:22,560 
a receiver of creating characteristic
that we call an ROC curve. 
20 
00:01:22,560 --> 00:01:25,350 
A common statistical technique for 
21 
00:01:25,350 --> 00:01:28,550 
plotting the performance
of a binary classifier. 
22 
00:01:30,180 --> 00:01:34,040 
Statistical functions
are also provided in MLlib. 
23 
00:01:34,040 --> 00:01:38,655 
Examples are summary statistics,
means, standard deviation, etc. 
24 
00:01:39,850 --> 00:01:43,440 
Correlations and
methods to sample a dataset. 
25 
00:01:45,420 --> 00:01:50,360 
MLlib also has techniques commonly
used in the machine learning process, 
26 
00:01:50,360 --> 00:01:53,040 
such as dimensionality reduction and 
27 
00:01:53,040 --> 00:01:56,630 
feature transformation methods for
preprocessing the data. 
28 
00:01:57,690 --> 00:02:01,550 
In short, Spark MLlib offers 
29 
00:02:01,550 --> 00:02:05,220 
many techniques often used in
a machine learning pipeline. 
30 
00:02:07,280 --> 00:02:13,410 
Let's take a look at an example to
compute summary statistics using MLlib. 
31 
00:02:13,410 --> 00:02:17,920 
Note that we will use the spark pipe
of API similar to the ones used for 
32 
00:02:17,920 --> 00:02:19,520 
our other examples in this course. 
33 
00:02:20,980 --> 00:02:25,220 
Here is the code segment to
compute summary statistics for 
34 
00:02:25,220 --> 00:02:29,180 
a data set consisting
of columns of numbers. 
35 
00:02:29,180 --> 00:02:33,370 
Lines of code are in white, and
the comments are in orange. 
36 
00:02:33,370 --> 00:02:38,759 
The first line imports statistics
functions from the stat module. 
37 
00:02:38,759 --> 00:02:44,710 
The second line creates an RDD
of Vectors with the data. 
38 
00:02:44,710 --> 00:02:48,500 
You can think of each vector
as a column in a data matrix. 
39 
00:02:49,800 --> 00:02:53,670 
The next line denoted with three invokes 
40 
00:02:53,670 --> 00:02:58,210 
the column stats function to compute
summary statistics for each column. 
41 
00:02:59,240 --> 00:03:04,820 
The last three lines show
by four print out the mean, 
42 
00:03:04,820 --> 00:03:08,480 
variance and number of non-zero
entries for each column. 
43 
00:03:10,070 --> 00:03:14,640 
As you can see from this example,
computing the summary statistics for 
44 
00:03:14,640 --> 00:03:17,700 
a data set is very
straightforward using a MLlib. 
45 
00:03:20,410 --> 00:03:22,490 
Here is another example. 
46 
00:03:22,490 --> 00:03:26,600 
Although we will go through the ratio
learning details in our next course, 
47 
00:03:26,600 --> 00:03:30,260 
here we give you a hint of how to
use two ratio learning techniques. 
48 
00:03:30,260 --> 00:03:32,920 
One for Classification,
and one for Clustering. 
49 
00:03:34,270 --> 00:03:40,130 
This code segment, shows the six steps to
build a DecisionTree for classification. 
50 
00:03:40,130 --> 00:03:43,138 
The first line imports
the DecisionTree module, 
51 
00:03:43,138 --> 00:03:46,510 
the second line imports MLUtils module, 
52 
00:03:48,190 --> 00:03:53,600 
the next line fails the DecisionTree
to classify the data for two classes. 
53 
00:03:53,600 --> 00:03:58,570 
Then, the model is printed out and
finally the model is saved in a file. 
54 
00:04:01,580 --> 00:04:05,490 
Here is another MLlib example,
this time for clustering. 
55 
00:04:05,490 --> 00:04:10,480 
This code segment shows the 5-step
code to build a k-means clustering. 
56 
00:04:12,420 --> 00:04:17,520 
The first line imports the k-means module,
the second line 
57 
00:04:17,520 --> 00:04:22,040 
imports an array module from numpy, 
58 
00:04:22,040 --> 00:04:27,048 
the next two lines read in the data and
parses it using space as the limiter, 
59 
00:04:27,048 --> 00:04:31,100 
then the k-means model 
60 
00:04:31,100 --> 00:04:36,270 
is built by dividing the parsedData
into three clusters. 
61 
00:04:36,270 --> 00:04:39,923 
Finally, the cluster centers
are printed out for each. 
62 
00:04:43,259 --> 00:04:47,790 
In summary,
MLlib is Spark's machine learning library. 
63 
00:04:49,240 --> 00:04:50,710 
It provides algorithms and 
64 
00:04:50,710 --> 00:04:53,880 
techniques that are implemented
using distributors processing. 
65 
00:04:54,970 --> 00:04:57,327 
The main categories of algorithms and 
66 
00:04:57,327 --> 00:05:01,968 
techniques available in machine
learning library are machine learning, 
67 
00:05:01,968 --> 00:05:06,324 
statistics and utility functions for
the machine learning process. 
1 
00:00:02,072 --> 00:00:04,356 
Over the next couple of videos, 
2 
00:00:04,356 --> 00:00:09,020 
we will introduce you to the basic
components of the Spark stack. 
3 
00:00:10,660 --> 00:00:13,440 
In this lecture, we start with Spark SQL. 
4 
00:00:15,000 --> 00:00:20,610 
After this video, you will be able to
process structured data using Spark's SQL 
5 
00:00:20,610 --> 00:00:25,140 
module and explain the numerous
benefits of Spark SQL. 
6 
00:00:28,240 --> 00:00:34,150 
Spark SQL is the component of Spark
that enables querying structured and 
7 
00:00:34,150 --> 00:00:37,510 
unstructured data through
a common query language. 
8 
00:00:38,630 --> 00:00:43,720 
It can connect to many data sources and
provides APIs to convert 
9 
00:00:43,720 --> 00:00:48,430 
the query results to RDDs in Python,
Scala, and Java programs. 
10 
00:00:50,670 --> 00:00:53,991 
Spark SQL gives a mechanism for 
11 
00:00:53,991 --> 00:00:58,467 
SQL users to deploy SQL queries on Spark. 
12 
00:01:01,047 --> 00:01:05,806 
Spark SQL enables business
intelligence tools to connect to 
13 
00:01:05,806 --> 00:01:10,854 
Spark using standard connection
protocols like JDBC and ODBC. 
14 
00:01:13,379 --> 00:01:18,346 
Spark SQL also provides APIs to
convert the query data into DataFrames 
15 
00:01:18,346 --> 00:01:20,290 
to hold distributed data. 
16 
00:01:21,410 --> 00:01:26,757 
DataFrames are organized as named
columns and basically look like tables. 
17 
00:01:30,248 --> 00:01:36,320 
The first step to run any SQL Spark
is to create a SQLContext. 
18 
00:01:38,580 --> 00:01:43,360 
Once you have an SQLContext,
you want to leverage it 
19 
00:01:43,360 --> 00:01:48,200 
to create a DataFrame so you can deploy
complex operations on the data set. 
20 
00:01:49,200 --> 00:01:52,480 
DataFrames can be created
from existing RDDs, 
21 
00:01:52,480 --> 00:01:55,270 
Hive tables or many other data sources. 
22 
00:01:57,910 --> 00:02:05,020 
A file can be read and converted into
a DataFrame using a single command. 
23 
00:02:06,610 --> 00:02:14,440 
The show function here will display
the DataFrame in your Spark show. 
24 
00:02:14,440 --> 00:02:18,950 
RDDs can be converted to DataFrames but
require a little more work. 
25 
00:02:19,980 --> 00:02:23,130 
First you will have to
convert each line into a row. 
26 
00:02:24,410 --> 00:02:28,640 
Once your data is in a DataFrame,
you can perform all sorts of 
27 
00:02:28,640 --> 00:02:33,640 
transformation operations on it
as shown here, including show, 
28 
00:02:33,640 --> 00:02:37,590 
printSchema, select, filter and groupBy. 
29 
00:02:39,850 --> 00:02:44,910 
To summarize, Spark SQL lets you
run relational queries on Spark. 
30 
00:02:46,500 --> 00:02:51,110 
It also lets you connect to
a variety of databases, and 
31 
00:02:51,110 --> 00:02:54,750 
deploy business intelligence
tools over Spark. 
32 
00:02:54,750 --> 00:02:58,707 
We will go through some of this
functionality in one of the readings and 
33 
00:02:58,707 --> 00:03:00,795 
in the upcoming hands-on session. 
1 
00:00:01,630 --> 00:00:05,040 
Next we will talk about Spark streaming. 
2 
00:00:06,650 --> 00:00:12,474 
After this video you will be able to
summarize how Spark reads streaming data, 
3 
00:00:12,474 --> 00:00:17,240 
list several sources of streaming
data supported by Spark, and 
4 
00:00:17,240 --> 00:00:20,077 
describe Spark's sliding windows. 
5 
00:00:20,077 --> 00:00:25,778 
Spark streaming provides
scalable processing for 
6 
00:00:25,778 --> 00:00:30,954 
real-time data and
runs on top of Spark Core. 
7 
00:00:32,816 --> 00:00:36,319 
Continuous data streams are converted or 
8 
00:00:36,319 --> 00:00:42,130 
grouped into discrete RDDs which
can then be processed in parallel. 
9 
00:00:43,620 --> 00:00:47,357 
Spark Streaming provides APIs for Scala, 
10 
00:00:47,357 --> 00:00:51,620 
Java, and Python,
like other Spark products. 
11 
00:00:55,370 --> 00:01:00,820 
Spark Streaming can read
data from many different 
12 
00:01:00,820 --> 00:01:06,278 
types of resources,
including Kafka and Flume. 
13 
00:01:06,278 --> 00:01:12,778 
Kafka is a high throughput published
subscribed messaging system, 
14 
00:01:12,778 --> 00:01:17,044 
and Flume collects and
aggregates log data. 
15 
00:01:17,044 --> 00:01:22,125 
Spark Streaming can also read from batch 
16 
00:01:22,125 --> 00:01:26,609 
input data sources, such as HDFS, 
17 
00:01:26,609 --> 00:01:31,720 
S3, and many other non SQL databases. 
18 
00:01:31,720 --> 00:01:39,368 
Additionally, Spark Streaming can read
directly from Twitter, raw TCP sockets, 
19 
00:01:39,368 --> 00:01:45,306 
and many other data sources that
are real-time data providers. 
20 
00:01:48,358 --> 00:01:49,280 
So how does it all work? 
21 
00:01:50,320 --> 00:01:55,910 
Here we show you a flow of
transformations and actions 
22 
00:01:55,910 --> 00:02:01,131 
which you will try in the upcoming reading
and hands-on exercises on Spark Streaming. 
23 
00:02:01,131 --> 00:02:05,102 
Spark streaming reads 
24 
00:02:05,102 --> 00:02:10,760 
streaming data and
converts it into micro batches 
25 
00:02:10,760 --> 00:02:16,350 
which we call DStreams which is short for
discretized stream. 
26 
00:02:19,378 --> 00:02:24,736 
In this example a 10 second
stream gets converted 
27 
00:02:24,736 --> 00:02:29,980 
into five RDDs using a batch
length of 2 seconds. 
28 
00:02:31,800 --> 00:02:36,920 
Similar to other RDDs,
transformations such as map, reduce, 
29 
00:02:36,920 --> 00:02:39,515 
and filter can be applied to DStreams. 
30 
00:02:39,515 --> 00:02:45,720 
DStreams can be aggregated 
31 
00:02:45,720 --> 00:02:52,310 
into Windows allowing you to apply
computations on sliding window of data. 
32 
00:02:53,860 --> 00:02:58,165 
In this example the Window
size is 4 seconds and 
33 
00:02:58,165 --> 00:03:01,510 
the sliding interval is 2 seconds. 
34 
00:03:04,826 --> 00:03:09,594 
In summary, Spark Streaming is
Spark's library to work with 
35 
00:03:09,594 --> 00:03:12,260 
streaming data in near real time. 
36 
00:03:14,375 --> 00:03:19,210 
DStreams can be used just
like any other RDD and 
37 
00:03:19,210 --> 00:03:22,940 
can go through the same
transformation as batch datasets. 
38 
00:03:24,560 --> 00:03:31,907 
DStreams can create a sliding window to
perform calculations on a window of time. 
1 
00:00:02,012 --> 00:00:05,776 
Now that we know what machine learning
is and have seen some examples of it, 
2 
00:00:05,776 --> 00:00:09,010 
let's talk about how we
do machine learning. 
3 
00:00:09,010 --> 00:00:13,015 
In this lecture we will get an overview of
the main categories of machine learning 
4 
00:00:13,015 --> 00:00:13,625 
techniques. 
5 
00:00:15,265 --> 00:00:18,315 
After this video you will be able to 
6 
00:00:18,315 --> 00:00:21,805 
describe the main categories of
machine learning techniques and 
7 
00:00:21,805 --> 00:00:25,469 
summarize how supervised learning
differs from unsupervised learning. 
8 
00:00:27,710 --> 00:00:30,260 
There are different categories of
machine learning techniques for 
9 
00:00:30,260 --> 00:00:32,150 
different types of problems. 
10 
00:00:32,150 --> 00:00:34,240 
The main categories are listed here. 
11 
00:00:34,240 --> 00:00:38,630 
They are classification,
regression, cluster analysis, and 
12 
00:00:38,630 --> 00:00:40,450 
association analysis. 
13 
00:00:40,450 --> 00:00:43,000 
We will cover each one in
detail in the following slides. 
14 
00:00:44,730 --> 00:00:49,830 
In classification, the goal is to
predict the category of the input data. 
15 
00:00:49,830 --> 00:00:54,840 
An example of this is predicting the
weather as being sunny, rainy, windy, or 
16 
00:00:54,840 --> 00:00:56,080 
cloudy. 
17 
00:00:56,080 --> 00:01:00,260 
The input data in this case would be
sensor data specifying the temperature, 
18 
00:01:00,260 --> 00:01:05,840 
relative humidity, atmospheric pressure,
wind speed, wind direction, etc. 
19 
00:01:05,840 --> 00:01:10,502 
The target or what you're trying to
predict would be the different weather 
20 
00:01:10,502 --> 00:01:14,670 
categories, sunny, windy,
rainy, and cloudy. 
21 
00:01:14,670 --> 00:01:19,580 
Another example is to classify
a tumor as either benign or malignant. 
22 
00:01:19,580 --> 00:01:22,460 
In this case,
the classification is referred to as 
23 
00:01:22,460 --> 00:01:26,040 
binary classification since
there are only two categories. 
24 
00:01:26,040 --> 00:01:28,400 
But you can have many categories as well. 
25 
00:01:28,400 --> 00:01:30,170 
As the weather prediction
problem shown here. 
26 
00:01:31,390 --> 00:01:35,080 
Another example is to identify hand
written digits as being in one of ten 
27 
00:01:35,080 --> 00:01:36,950 
categories, zero to nine. 
28 
00:01:38,970 --> 00:01:43,280 
Some more examples of classification
are classifying a tumor from a medical 
29 
00:01:43,280 --> 00:01:45,710 
image as being benign or malignant. 
30 
00:01:45,710 --> 00:01:48,430 
Predicting whether it
will rain the next day. 
31 
00:01:48,430 --> 00:01:53,410 
Determining if a loan application is
high-risk, medium-risk or low-risk. 
32 
00:01:53,410 --> 00:01:55,860 
Identifying the sentiment of a tweet or 
33 
00:01:55,860 --> 00:01:58,990 
review as being positive,
negative, or neutral. 
34 
00:02:00,930 --> 00:02:04,870 
When your model has to predict
a numeric value instead of a category, 
35 
00:02:04,870 --> 00:02:07,380 
then the task becomes
a regression problem. 
36 
00:02:09,040 --> 00:02:12,920 
An example of regression is to
predict the price of a stock. 
37 
00:02:12,920 --> 00:02:16,320 
The stock price is a numeric value,
not a category. 
38 
00:02:16,320 --> 00:02:19,700 
So this is a regression task
instead of a classification task. 
39 
00:02:20,710 --> 00:02:23,480 
If you were to predict whether
the stock price will rise or 
40 
00:02:23,480 --> 00:02:26,620 
fall, then that would be
a classification problem. 
41 
00:02:26,620 --> 00:02:29,340 
But if you're predicting
the actual price of the stock, 
42 
00:02:29,340 --> 00:02:30,620 
then that is a regression problem. 
43 
00:02:31,740 --> 00:02:35,340 
That is the main difference between
classification and regression. 
44 
00:02:35,340 --> 00:02:38,250 
In classification,
you're predicting a category and 
45 
00:02:38,250 --> 00:02:40,520 
in regression,
you're predicting a numeric value. 
46 
00:02:42,630 --> 00:02:47,000 
Some other examples of regression
are estimating the demand of a product 
47 
00:02:47,000 --> 00:02:49,570 
based on time or season of the year. 
48 
00:02:49,570 --> 00:02:52,130 
Predicting a score on a test, 
49 
00:02:52,130 --> 00:02:55,210 
determining the likelihood of
how effective a drug will be for 
50 
00:02:55,210 --> 00:03:00,270 
a particular patient, predicting
the amount of rain for a region. 
51 
00:03:00,270 --> 00:03:01,550 
In cluster analysis, 
52 
00:03:01,550 --> 00:03:06,275 
the goal is to organize similar
items in your data set into groups. 
53 
00:03:06,275 --> 00:03:10,120 
A very common application of
cluster analysis is referred to as 
54 
00:03:10,120 --> 00:03:12,160 
customer segmentation. 
55 
00:03:12,160 --> 00:03:16,227 
This means that you're separating your
customer base into different groups or 
56 
00:03:16,227 --> 00:03:18,054 
segments based on customer types. 
57 
00:03:18,054 --> 00:03:22,833 
For example it would be very beneficial
to segment your customers into seniors, 
58 
00:03:22,833 --> 00:03:25,230 
adults and teenagers. 
59 
00:03:25,230 --> 00:03:27,544 
These groups have different likes and
dislikes and 
60 
00:03:27,544 --> 00:03:29,383 
have different purchasing behaviors. 
61 
00:03:29,383 --> 00:03:33,569 
By segmenting your customers to different
groups you can more effectively provide 
62 
00:03:33,569 --> 00:03:37,652 
marketing adds targeted for
each groups particular interests. 
63 
00:03:37,652 --> 00:03:40,750 
Note that cluster analysis is
also referred to as clustering. 
64 
00:03:42,860 --> 00:03:46,810 
Some other examples of cluster
analysis are, identifying areas of 
65 
00:03:46,810 --> 00:03:51,840 
similar topography, such as desert region,
grassy areas, mountains etc. 
66 
00:03:51,840 --> 00:03:55,610 
Categorizing different types of
tissues from medical images. 
67 
00:03:55,610 --> 00:04:00,080 
Determining different groups of weather
patterns, such as snowy, dry, monsoon. 
68 
00:04:01,160 --> 00:04:04,810 
And discovering hot spots for different
types of crime from police reports. 
69 
00:04:06,960 --> 00:04:10,770 
The goal in association analysis is
to come up with a set of rules to 
70 
00:04:10,770 --> 00:04:14,230 
capture associations between items or
events. 
71 
00:04:14,230 --> 00:04:18,520 
The rules are used to determine when
items or events occur together. 
72 
00:04:18,520 --> 00:04:22,500 
A common application of association
analysis is known as market 
73 
00:04:22,500 --> 00:04:24,300 
basket analysis. 
74 
00:04:24,300 --> 00:04:28,020 
Which is used to understand
customer purchasing behavior. 
75 
00:04:28,020 --> 00:04:32,000 
For example, association analysis can
reveal that banking customers who have 
76 
00:04:32,000 --> 00:04:36,330 
CDs, or Certificates of Deposits,
also tend to be interested in 
77 
00:04:36,330 --> 00:04:39,970 
other investment vehicles such
as money market accounts. 
78 
00:04:39,970 --> 00:04:42,840 
This information can be used for
cross selling. 
79 
00:04:42,840 --> 00:04:47,020 
If you advertise money market accounts to
your customers with CDs they are likely to 
80 
00:04:47,020 --> 00:04:48,390 
open such an account. 
81 
00:04:50,750 --> 00:04:54,700 
According to data mining folklore
a supermarket chain used association 
82 
00:04:54,700 --> 00:04:59,410 
analysis to discover a connection between
two seemingly unrelated products. 
83 
00:04:59,410 --> 00:05:02,130 
They discovered that many
customers who go to the store 
84 
00:05:02,130 --> 00:05:06,150 
late on Sunday night to buy
diapers also tend to buy beer. 
85 
00:05:06,150 --> 00:05:08,470 
This information was then
used to place beer and 
86 
00:05:08,470 --> 00:05:12,620 
diapers close together and
they saw a jump in sales of both items. 
87 
00:05:12,620 --> 00:05:14,630 
This is the famous diaper,
beer connection. 
88 
00:05:16,920 --> 00:05:21,600 
Some other applications of association
analysis are recommending similar 
89 
00:05:21,600 --> 00:05:26,180 
items based on the purchasing behavior or
browsing histories of customers. 
90 
00:05:27,180 --> 00:05:30,720 
Finding items that are often purchased
together, such as garden hose and 
91 
00:05:30,720 --> 00:05:32,080 
potting soil, and 
92 
00:05:32,080 --> 00:05:35,910 
offer sales on these related items at the
same time to drive sales of both items. 
93 
00:05:36,980 --> 00:05:39,970 
Identifying web pages that
are often accessed together so 
94 
00:05:39,970 --> 00:05:44,070 
that you can more efficiently offer up
these related web pages at the same time. 
95 
00:05:45,944 --> 00:05:49,890 
We have now looked at the different
categories of machine learning techniques. 
96 
00:05:49,890 --> 00:05:54,240 
They are classification,
regression, cluster analysis, and 
97 
00:05:54,240 --> 00:05:55,280 
association analysis. 
98 
00:05:55,280 --> 00:05:58,753 
We have also seen some
examples of each category 
99 
00:06:00,941 --> 00:06:04,764 
There is also another categorization
of machine learning techniques, and 
100 
00:06:04,764 --> 00:06:07,520 
that is supervised versus
unsupervised approaches. 
101 
00:06:08,840 --> 00:06:10,900 
In supervised approaches the target, 
102 
00:06:10,900 --> 00:06:13,830 
which is what the model is predicting,
is provided. 
103 
00:06:13,830 --> 00:06:18,750 
This is referred to as having labeled
data because the target is labeled for 
104 
00:06:18,750 --> 00:06:20,457 
every sample that you
have in your data set. 
105 
00:06:21,550 --> 00:06:25,520 
Referring back to our example of
predicting a weather category of sunny, 
106 
00:06:25,520 --> 00:06:27,940 
windy, rainy or cloudy, 
107 
00:06:27,940 --> 00:06:32,550 
every sample in the data set is labeled
as being one of these four categories. 
108 
00:06:32,550 --> 00:06:37,910 
So the data is labeled and predicting the
weather categories is a supervised task. 
109 
00:06:37,910 --> 00:06:41,740 
In general, classification and
regression are supervised approaches. 
110 
00:06:43,680 --> 00:06:46,200 
In unsupervised approaches
on the other hand, 
111 
00:06:46,200 --> 00:06:50,440 
the target that the model is
predicting is unknown or unavailable. 
112 
00:06:50,440 --> 00:06:53,320 
This means that you have unlabeled data. 
113 
00:06:53,320 --> 00:06:57,130 
Going back to our cluster analysis
example of segmenting customers into 
114 
00:06:57,130 --> 00:06:58,400 
different groups. 
115 
00:06:58,400 --> 00:07:02,180 
The samples in your data are not
labeled with the correct group. 
116 
00:07:02,180 --> 00:07:06,250 
Instead, the segmentation is performed
using a clustering technique to group 
117 
00:07:06,250 --> 00:07:09,660 
items based on characteristics
that they have in common. 
118 
00:07:09,660 --> 00:07:11,560 
Thus, the data is unlabeled and 
119 
00:07:11,560 --> 00:07:16,630 
the task of grouping customers into
different segments is an unsupervised one. 
120 
00:07:16,630 --> 00:07:18,606 
In general, cluster analysis and 
121 
00:07:18,606 --> 00:07:21,751 
association analysis
are unsupervised approaches. 
122 
00:07:23,962 --> 00:07:24,534 
In summary, 
123 
00:07:24,534 --> 00:07:29,000 
in this lecture we looked at the different
categories of machine learning techniques. 
124 
00:07:29,000 --> 00:07:35,210 
We discussed classification, regression,
cluster analysis and association analysis. 
125 
00:07:35,210 --> 00:07:37,370 
We also defined what supervised and 
126 
00:07:37,370 --> 00:07:39,590 
unsupervised approaches are,
in machine learning. 
1 
00:00:01,490 --> 00:00:04,360 
In this lecture,
we will review each of the steps in 
2 
00:00:04,360 --> 00:00:06,440 
the machine learning
process in greater detail. 
3 
00:00:07,930 --> 00:00:12,360 
After this video, you will be able
to explain the goals of each step in 
4 
00:00:12,360 --> 00:00:17,989 
the machine learning process, and list key
activities in each step in the process. 
5 
00:00:19,910 --> 00:00:23,840 
This is the machine learning process
that we saw in the last lecture. 
6 
00:00:23,840 --> 00:00:27,560 
In this lecture,
we will cover each step in more detail. 
7 
00:00:27,560 --> 00:00:29,640 
We will describe the goals
of each step and 
8 
00:00:29,640 --> 00:00:31,600 
the key activities performed in each step. 
9 
00:00:33,730 --> 00:00:37,870 
The first step in the data science
process is to acquire the data. 
10 
00:00:37,870 --> 00:00:39,910 
The goal of the step is to identify and 
11 
00:00:39,910 --> 00:00:42,270 
obtain all data related
to the problem at hand. 
12 
00:00:43,980 --> 00:00:47,800 
First, we need to identify all
related data and the sources. 
13 
00:00:47,800 --> 00:00:52,330 
Keep in mind, that data can come from
different sources such as files, 
14 
00:00:52,330 --> 00:00:55,940 
databases, the internet, mobile devices. 
15 
00:00:55,940 --> 00:00:59,290 
So remember to include all data related
to the problem you're addressing. 
16 
00:01:00,520 --> 00:01:03,520 
After you've identified your data and
data sources, 
17 
00:01:03,520 --> 00:01:08,360 
the next step is to collect the data and
integrate data from the different sources. 
18 
00:01:08,360 --> 00:01:12,570 
This may require conversion,
as data can come in different formats. 
19 
00:01:12,570 --> 00:01:15,246 
And it may also require aligning the data, 
20 
00:01:15,246 --> 00:01:19,520 
as data from different sources may have
different time or spacial resolutions. 
21 
00:01:21,660 --> 00:01:26,410 
Once you’ve collected and integrated your
data, you now have a coherent data set for 
22 
00:01:26,410 --> 00:01:27,050 
your analysis. 
23 
00:01:28,920 --> 00:01:33,400 
The next step after acquiring data is
to prepare it to make it suitable for 
24 
00:01:33,400 --> 00:01:34,690 
analysis. 
25 
00:01:34,690 --> 00:01:39,290 
There are two parts to this step,
explore data and preprocess data. 
26 
00:01:39,290 --> 00:01:41,410 
We will discuss data exploration first. 
27 
00:01:43,040 --> 00:01:47,670 
In data exploration, you want to
do some preliminary investigation 
28 
00:01:47,670 --> 00:01:52,680 
in order to gain a better understanding of
the specific characteristics of your data. 
29 
00:01:52,680 --> 00:01:55,779 
This in turn will guide
the rest of the process. 
30 
00:01:55,779 --> 00:01:58,640 
With data exploration,
you'll want to look for 
31 
00:01:58,640 --> 00:02:03,250 
things like correlations,
general trends, outliers, etc. 
32 
00:02:05,080 --> 00:02:07,930 
Correlations provide information
about the relationship 
33 
00:02:07,930 --> 00:02:09,280 
between variables in your data. 
34 
00:02:10,990 --> 00:02:15,710 
Trends in your data will reveal if the
variable is moving in a certain direction, 
35 
00:02:15,710 --> 00:02:18,480 
such as transaction volume
increasing throughout the year. 
36 
00:02:20,120 --> 00:02:23,660 
Outliers indicate potential
problems with the data, or 
37 
00:02:23,660 --> 00:02:27,850 
may indicate an interesting data
point that needs further examination. 
38 
00:02:27,850 --> 00:02:29,680 
Without this data exploration activity, 
39 
00:02:29,680 --> 00:02:32,370 
you will not be able to
use your data effectively. 
40 
00:02:34,350 --> 00:02:38,680 
One way to explore your data is
to calculate summary statistics 
41 
00:02:38,680 --> 00:02:40,290 
to numerically describe the data. 
42 
00:02:42,180 --> 00:02:46,570 
Summary statistics are quantities
that capture various characteristics 
43 
00:02:46,570 --> 00:02:50,390 
of a set of values with a single number,
or a small set of numbers. 
44 
00:02:50,390 --> 00:02:55,400 
Some basic summary statistics that you
should compute for your data set are mean, 
45 
00:02:55,400 --> 00:03:00,360 
median, mode, range and
standard deviation. 
46 
00:03:00,360 --> 00:03:03,660 
Mean and median are measures of
the location of a set of values. 
47 
00:03:03,660 --> 00:03:07,440 
Mode is the value that occurs most
frequently in your data set, and 
48 
00:03:07,440 --> 00:03:11,630 
range and standard deviation
are measures of spread in your data. 
49 
00:03:11,630 --> 00:03:16,060 
Looking at these measures will give you
an idea of the nature of your data. 
50 
00:03:16,060 --> 00:03:19,130 
They can tell you if there's
something wrong with your data. 
51 
00:03:19,130 --> 00:03:23,100 
For example, if the range of
the values for age in your data 
52 
00:03:23,100 --> 00:03:27,020 
includes negative numbers, or
a number much greater than a hundred, 
53 
00:03:27,020 --> 00:03:29,830 
there's something suspicious in
the data that needs to be examined. 
54 
00:03:31,760 --> 00:03:34,230 
Visualization techniques
also provide quick and 
55 
00:03:34,230 --> 00:03:36,570 
effective ways to explore your data. 
56 
00:03:36,570 --> 00:03:39,760 
Some examples are, a histogram, 
57 
00:03:39,760 --> 00:03:43,570 
such as the plot shown here,
shows the distribution of the data and 
58 
00:03:43,570 --> 00:03:47,269 
can show skewness or
unusual dispersion in outliers. 
59 
00:03:48,960 --> 00:03:53,380 
A line plot, like the one in the lower
left, can be used to look at trends in 
60 
00:03:53,380 --> 00:03:56,080 
the data, such as,
the change in the price of a stock. 
61 
00:03:57,750 --> 00:04:00,816 
A heat map can give you an idea
of where the hot spots are. 
62 
00:04:02,485 --> 00:04:07,315 
A scatter plot effectively shows
correlation between two variables. 
63 
00:04:07,315 --> 00:04:10,585 
Overall, there are many types
of plots to visualize data. 
64 
00:04:10,585 --> 00:04:13,525 
They are very useful in helping
you understand the data you have. 
65 
00:04:15,745 --> 00:04:19,155 
The second part of the prepare
step is preprocess. 
66 
00:04:19,155 --> 00:04:23,755 
So, after we've explored the data, we need
to preprocess the data to prepare it for 
67 
00:04:23,755 --> 00:04:24,405 
analysis. 
68 
00:04:26,150 --> 00:04:29,950 
The goal here is to create the data
that will be used for analysis. 
69 
00:04:29,950 --> 00:04:33,280 
The main activities on this
part are to clean the data, 
70 
00:04:33,280 --> 00:04:37,070 
select the appropriate variables to
use and transform the data as needed. 
71 
00:04:38,855 --> 00:04:41,375 
A very important part of
data preparation is to 
72 
00:04:41,375 --> 00:04:43,975 
clean the data to address quality issues. 
73 
00:04:43,975 --> 00:04:45,875 
Real world data is nothing. 
74 
00:04:45,875 --> 00:04:51,275 
There are many examples of quality issues
with data from real applications including 
75 
00:04:51,275 --> 00:04:56,065 
missing values, such as income
in a survey, duplicate data, 
76 
00:04:56,065 --> 00:04:59,615 
such as two different records for the same
customer with different addresses. 
77 
00:05:01,020 --> 00:05:04,670 
Inconsistent or invalid data,
such as a six digit zip code. 
78 
00:05:06,130 --> 00:05:09,110 
Noise in the collection of data
that distorts the true values. 
79 
00:05:10,915 --> 00:05:15,180 
Outliers, such as a number much
larger than 100 for someone's age. 
80 
00:05:15,180 --> 00:05:16,700 
It is essential to detect and 
81 
00:05:16,700 --> 00:05:20,230 
address these issues that can negatively
affect the quality of the data. 
82 
00:05:22,740 --> 00:05:26,620 
Other activities in data preprocessing
can be broadly categorized as 
83 
00:05:26,620 --> 00:05:30,140 
feature selection and
feature transformation. 
84 
00:05:30,140 --> 00:05:33,410 
Feature selection refers to
choosing the set of features to 
85 
00:05:33,410 --> 00:05:36,500 
use that is appropriate for
the application. 
86 
00:05:36,500 --> 00:05:39,820 
Feature selection can
involve removing redundant or 
87 
00:05:39,820 --> 00:05:44,420 
irrelevant features, combining features,
or creating new features. 
88 
00:05:45,610 --> 00:05:47,520 
During the exploring data step, 
89 
00:05:47,520 --> 00:05:50,820 
you may have discovered that two
features are very correlated. 
90 
00:05:50,820 --> 00:05:53,460 
In that case,
one of these features can be removed 
91 
00:05:53,460 --> 00:05:56,630 
without negatively effecting
the analysis results. 
92 
00:05:56,630 --> 00:05:59,110 
For example,
the purchase price of a product and 
93 
00:05:59,110 --> 00:06:02,830 
the amount of sales tax pain
are very likely to be correlated. 
94 
00:06:02,830 --> 00:06:06,860 
Eliminating the sales tax
amount then will be beneficial. 
95 
00:06:06,860 --> 00:06:11,330 
Removing redundant or irrelevant features
will make the subsequent analysis simpler. 
96 
00:06:12,490 --> 00:06:15,870 
You may also want to combine features or
create new ones. 
97 
00:06:15,870 --> 00:06:18,580 
For example,
adding the applicants education level 
98 
00:06:18,580 --> 00:06:21,449 
as a feature to a loan approval
application would make sense. 
99 
00:06:22,450 --> 00:06:25,700 
There are also algorithms to
automatically determine the most relevant 
100 
00:06:25,700 --> 00:06:27,940 
features based on various
mathematical properties. 
101 
00:06:30,280 --> 00:06:34,510 
Feature transformation maps the data
from one format to another. 
102 
00:06:34,510 --> 00:06:36,805 
Various transformation operations exist. 
103 
00:06:36,805 --> 00:06:41,470 
For example, scaling maps
the data values to a specified 
104 
00:06:41,470 --> 00:06:45,630 
range to prevent any one feature from
dominating the analysis results. 
105 
00:06:47,280 --> 00:06:51,900 
Filtering or aggregation can be used to
reduce noise and variability in the data. 
106 
00:06:53,700 --> 00:06:58,160 
Dimensionality reduction maps the data
to a smaller subset of dimensions 
107 
00:06:58,160 --> 00:07:01,020 
to simplify the subsequent analysis. 
108 
00:07:01,020 --> 00:07:05,497 
We will discuss techniques to prepare
data in more detail later in this course. 
109 
00:07:07,000 --> 00:07:10,237 
After preparing the data to
address data quality issues and 
110 
00:07:10,237 --> 00:07:13,020 
preprocess it to get it in
the appropriate format, 
111 
00:07:13,020 --> 00:07:17,680 
the next step in the machine learning
process is to analyze the data. 
112 
00:07:17,680 --> 00:07:20,840 
The goals of the staff are to
build a machine learning model, 
113 
00:07:20,840 --> 00:07:24,960 
to analyze the data and to evaluate
the results that you get from the model. 
114 
00:07:27,140 --> 00:07:32,110 
The analyze steps starts with this
determining the type of problem you have. 
115 
00:07:32,110 --> 00:07:35,990 
You begin by selecting appropriate machine
learning techniques to analyze the data. 
116 
00:07:37,760 --> 00:07:40,350 
Then you construct the model using
the data that you've prepared. 
117 
00:07:41,940 --> 00:07:46,220 
Once the model is built, you will
want to apply it to new data samples 
118 
00:07:46,220 --> 00:07:48,450 
to evaluate how well the model performs. 
119 
00:07:49,800 --> 00:07:53,125 
Thus data analysis involves selecting
the appropriate technique for 
120 
00:07:53,125 --> 00:07:57,375 
your problem, building the model,
then evaluating the results. 
121 
00:07:57,375 --> 00:07:58,855 
As there are a different
types of problems, 
122 
00:07:58,855 --> 00:08:01,855 
there are also different
types of analysis techniques. 
123 
00:08:01,855 --> 00:08:03,097 
We will cover algorithms for 
124 
00:08:03,097 --> 00:08:06,785 
building machine learning models in a much
more detail in week three of this course. 
125 
00:08:08,890 --> 00:08:12,260 
The next step in the machine learning
process is reporting results from your 
126 
00:08:12,260 --> 00:08:12,865 
analysis. 
127 
00:08:12,865 --> 00:08:17,020 
In reporting your results, it is important
to communicate your insights and 
128 
00:08:17,020 --> 00:08:19,170 
make a case for
what actions should follow. 
129 
00:08:21,215 --> 00:08:22,545 
In reporting your results, 
130 
00:08:22,545 --> 00:08:26,945 
you will want to think about what to
present, as well as how to present. 
131 
00:08:26,945 --> 00:08:30,912 
In deciding what to present, you should
consider what the main results are, 
132 
00:08:30,912 --> 00:08:33,537 
what insights were gained
from your analysis, and 
133 
00:08:33,537 --> 00:08:36,725 
what added value do these insights
bring to the application. 
134 
00:08:37,725 --> 00:08:41,517 
Keep in mind that even negative results
are valuable lessons learned, and 
135 
00:08:41,517 --> 00:08:44,730 
suggest further avenues for
additional analysis. 
136 
00:08:44,730 --> 00:08:47,150 
Remember that all findings
must be presented so 
137 
00:08:47,150 --> 00:08:49,840 
that informs decisions can be made for
next steps. 
138 
00:08:50,910 --> 00:08:52,780 
In deciding how to present, 
139 
00:08:52,780 --> 00:08:56,460 
remember that visualization is an
important tool in presenting your results. 
140 
00:08:58,055 --> 00:09:01,285 
Plots and summary statistics
discussing the explore step 
141 
00:09:01,285 --> 00:09:03,625 
can be used effectively here as well. 
142 
00:09:03,625 --> 00:09:07,095 
You should also have tables with
details from your analysis as backup, 
143 
00:09:07,095 --> 00:09:09,695 
if someone wants to take
a deeper dive into the results. 
144 
00:09:10,915 --> 00:09:14,785 
In summary, you want to report your
findings by presenting your results and 
145 
00:09:14,785 --> 00:09:17,815 
the value added with graphs
using visualization tools. 
146 
00:09:20,160 --> 00:09:22,710 
The final step in
the machine loading process 
147 
00:09:22,710 --> 00:09:26,600 
is to determine what action should be
taken based on the insights gained. 
148 
00:09:28,370 --> 00:09:31,930 
What action should be taken based
on the results of your analysis? 
149 
00:09:31,930 --> 00:09:35,110 
Should you market certain products
to a specific customer segment to 
150 
00:09:35,110 --> 00:09:36,630 
increase sales? 
151 
00:09:36,630 --> 00:09:39,730 
What inefficiency is can be
removed from your process? 
152 
00:09:39,730 --> 00:09:42,580 
What incentives would be effective
in attracting new customers? 
153 
00:09:44,450 --> 00:09:47,180 
Once a specific action
has been determined, 
154 
00:09:47,180 --> 00:09:49,090 
the next step is to implement the action. 
155 
00:09:50,100 --> 00:09:55,320 
Things to consider here include, how can
the action be added to your application? 
156 
00:09:55,320 --> 00:09:57,020 
How will end users be affected? 
157 
00:09:58,950 --> 00:10:03,030 
Assessing the impact of the implemented
action is then necessary to evaluate 
158 
00:10:03,030 --> 00:10:04,810 
the benefits gained. 
159 
00:10:04,810 --> 00:10:07,820 
The results of this assessment
determine next steps, 
160 
00:10:07,820 --> 00:10:11,960 
which could suggest additional analysis or
further opportunities, 
161 
00:10:11,960 --> 00:10:15,049 
which would begin another cycle
of the machine learning process. 
162 
00:10:17,100 --> 00:10:20,960 
In summary, we've looked at the steps in
the machine learning process in detail. 
163 
00:10:22,130 --> 00:10:26,470 
The goals or main activities of each
step were discussed in this lecture. 
164 
00:10:26,470 --> 00:10:29,570 
Remember that this is
an iterative process, 
165 
00:10:29,570 --> 00:10:33,450 
in which each step can be repeated
more than once, and findings from 
166 
00:10:33,450 --> 00:10:37,750 
each step may require a previous step
to be repeated with new information 
1 
00:00:01,010 --> 00:00:03,490 
Hello, everyone.
You have probably heard of machine 
2 
00:00:03,490 --> 00:00:07,250 
learning before, enough to grab your
attention and bring you to this class. 
3 
00:00:07,250 --> 00:00:10,250 
However, you might be wondering
what machine learning is. 
4 
00:00:10,250 --> 00:00:11,920 
We will talk about that
in this lecture and 
5 
00:00:11,920 --> 00:00:15,119 
discuss how we can see machine learning
in action in our day-to-day life. 
6 
00:00:17,280 --> 00:00:21,890 
After this video you will be able to
explain what machine learning is. 
7 
00:00:21,890 --> 00:00:25,440 
And list three applications of machine
learning encountered in everyday life. 
8 
00:00:27,690 --> 00:00:32,170 
Since this course is about machine
learning, lets define what that means. 
9 
00:00:32,170 --> 00:00:35,470 
We hear this term a lot these
days used in many contexts. 
10 
00:00:35,470 --> 00:00:39,240 
So it's good to start out with a solid
definition of what machine learning means. 
11 
00:00:40,800 --> 00:00:45,200 
Machine learning is the field of study
that focuses on computer systems that can 
12 
00:00:45,200 --> 00:00:46,850 
learn from data. 
13 
00:00:46,850 --> 00:00:51,360 
That is the system's often called
models can learn to perform a specific 
14 
00:00:51,360 --> 00:00:55,600 
task by analyzing lots of examples for
a particular problem. 
15 
00:00:55,600 --> 00:00:59,550 
For example, a machine learning
model can learn to recognize 
16 
00:00:59,550 --> 00:01:03,600 
an image of a cat by being shown lots and
lots of images of cats. 
17 
00:01:05,010 --> 00:01:07,710 
This notion of learning from data
means that a machine learning 
18 
00:01:07,710 --> 00:01:12,960 
model can learn a specific task
without being explicitly programmed. 
19 
00:01:12,960 --> 00:01:16,400 
In other words, the machine learning
model is not given the step 
20 
00:01:16,400 --> 00:01:19,610 
by step instructions on how to
recognize the image of a cat. 
21 
00:01:20,970 --> 00:01:23,910 
Instead, the model learns
what features are important 
22 
00:01:23,910 --> 00:01:29,030 
in determining whether it picture contains
a cat from the data that has analyzed. 
23 
00:01:29,030 --> 00:01:32,120 
Because the model learns to
perform this task from data 
24 
00:01:32,120 --> 00:01:36,250 
it's good to know that the amount and
quality of data available for 
25 
00:01:36,250 --> 00:01:40,110 
building the model are important factors
in how well the model learns the task. 
26 
00:01:41,520 --> 00:01:44,110 
Because machine learning
models can learn from data 
27 
00:01:44,110 --> 00:01:47,420 
that can be used to discover hidden
patterns and trends in the data. 
28 
00:01:49,160 --> 00:01:53,235 
These patterns and trends lead to
valuable insights into the data. 
29 
00:01:53,235 --> 00:01:57,265 
Thus the use of machine learning allows
for data driven decisions to be made for 
30 
00:01:57,265 --> 00:01:59,095 
a particular problem. 
31 
00:01:59,095 --> 00:02:02,745 
So to summarize, the field of machine
learning focuses on the study and 
32 
00:02:02,745 --> 00:02:06,185 
construction of computer systems
that can learn from data 
33 
00:02:06,185 --> 00:02:08,775 
without being explicitly programmed. 
34 
00:02:08,775 --> 00:02:10,025 
Machine learning algorithms and 
35 
00:02:10,025 --> 00:02:13,355 
techniques are used to build models,
to discover hidden patterns and 
36 
00:02:13,355 --> 00:02:16,980 
trends in the data allowing for
data-driven decisions to be made. 
37 
00:02:19,180 --> 00:02:22,840 
You may have heard that machine learning
is an inter-disciplinary field. 
38 
00:02:22,840 --> 00:02:24,410 
This is very true. 
39 
00:02:24,410 --> 00:02:28,740 
Machine learning combines concepts and
methods from many disciplines, including 
40 
00:02:28,740 --> 00:02:33,830 
math, statistics, computer science,
artificial intelligence, and optimization. 
41 
00:02:34,930 --> 00:02:36,940 
In applying machine learning to a problem, 
42 
00:02:36,940 --> 00:02:41,060 
domain knowledge is essential
to the success of end results. 
43 
00:02:41,060 --> 00:02:44,520 
By domain knowledge we mean
an understanding of the application or 
44 
00:02:44,520 --> 00:02:45,150 
business domain. 
45 
00:02:46,160 --> 00:02:50,070 
Knowledge about the application,
the data related to the application, and 
46 
00:02:50,070 --> 00:02:53,860 
how the outcomes will be used are crucial
to driving the process of building 
47 
00:02:53,860 --> 00:02:55,540 
the machine learning model. 
48 
00:02:55,540 --> 00:02:59,593 
So domain knowledge is also an integral
part of a machine learning solution. 
49 
00:03:01,278 --> 00:03:04,713 
Machine learning has been used in many
different learning application, many of 
50 
00:03:04,713 --> 00:03:08,410 
which you'll probably encounter in your
daily life, perhaps without realizing it. 
51 
00:03:10,430 --> 00:03:14,030 
One application of machine learning that
you likely used this past weekend, or 
52 
00:03:14,030 --> 00:03:17,650 
even just today,
is credit card fraud detection. 
53 
00:03:17,650 --> 00:03:21,340 
Every time you use your credit card,
the current purchase is analyzed 
54 
00:03:21,340 --> 00:03:25,230 
against your history of credit card
transactions to determine if the current 
55 
00:03:25,230 --> 00:03:30,070 
purchase is a legitimate transaction or
a potentially fraudulent one. 
56 
00:03:30,070 --> 00:03:33,840 
If the purchase is very different
from your past purchases, such as for 
57 
00:03:33,840 --> 00:03:37,480 
a big ticket item in a category that
you had never shown an interest in or 
58 
00:03:37,480 --> 00:03:40,630 
when the point of sales location
is from another country, 
59 
00:03:40,630 --> 00:03:42,860 
then it will be flagged
as a suspicious activity. 
60 
00:03:43,920 --> 00:03:46,100 
In that case,
the transaction may be denied. 
61 
00:03:46,100 --> 00:03:48,990 
Or you may get a call from your
credit card company to confirm that 
62 
00:03:48,990 --> 00:03:51,760 
the purchase was indeed made by you. 
63 
00:03:51,760 --> 00:03:54,560 
This is a very common use of machine
learning that is encountered in 
64 
00:03:54,560 --> 00:03:55,160 
everyday life. 
65 
00:03:57,120 --> 00:04:00,690 
Another example application of machine
learning encountered in daily life is 
66 
00:04:00,690 --> 00:04:03,140 
handwritten digit recognition. 
67 
00:04:03,140 --> 00:04:07,890 
When you deposit a hand-written check into
an ATM, a machine learning process is used 
68 
00:04:07,890 --> 00:04:12,660 
to read the numbers written on the check
to determine the amount of the deposit. 
69 
00:04:12,660 --> 00:04:16,420 
Handwritten digits are trickier
to decipher than typed digits 
70 
00:04:16,420 --> 00:04:19,150 
due to the many variations
in people's handwriting. 
71 
00:04:20,490 --> 00:04:24,130 
A machine learning system can sift through
the different variations to find similar 
72 
00:04:24,130 --> 00:04:27,770 
patterns to distinguish a one from a nine,
for example. 
73 
00:04:29,820 --> 00:04:32,930 
Recommendations in what sites is
another example application of 
74 
00:04:32,930 --> 00:04:36,460 
machine learning that most people
have experienced first hand. 
75 
00:04:36,460 --> 00:04:41,160 
After you buy an item on a website you
will often get a list of related items. 
76 
00:04:41,160 --> 00:04:44,100 
Often this will be displayed
as customers who bought this 
77 
00:04:44,100 --> 00:04:47,380 
item also bought these items,
or you may also like. 
78 
00:04:48,630 --> 00:04:52,130 
These related items have been associated
with the item you purchased by 
79 
00:04:52,130 --> 00:04:53,650 
a machine learning model, and 
80 
00:04:53,650 --> 00:04:57,380 
are now being shown to you since
you may also be interested in them. 
81 
00:04:57,380 --> 00:05:00,160 
This is a common application of
machine learning used often in 
82 
00:05:00,160 --> 00:05:01,040 
sales and marketing. 
83 
00:05:01,040 --> 00:05:06,600 
Here are some other examples of where
machine learning has been used. 
84 
00:05:06,600 --> 00:05:09,360 
Like targeted ads on mobile devices. 
85 
00:05:09,360 --> 00:05:12,490 
Sentiment analysis of social media data, 
86 
00:05:12,490 --> 00:05:17,570 
climate monitoring to detect seasonal
patterns, crime pattern detection, and 
87 
00:05:17,570 --> 00:05:20,910 
a healthiness analysis of drugs
among many other applications. 
88 
00:05:22,000 --> 00:05:23,580 
As you can see from this short list, 
89 
00:05:23,580 --> 00:05:27,420 
machine learning has been used in
various applications including science, 
90 
00:05:27,420 --> 00:05:31,910 
medicine, retail, law enforcement,
education and many others. 
91 
00:05:33,850 --> 00:05:38,190 
Let's take a few minutes to discuss the
different terms which refer to this field. 
92 
00:05:38,190 --> 00:05:42,290 
The term we are using for this course is
machine learning, but you may have heard 
93 
00:05:42,290 --> 00:05:47,080 
other terms such as data mining,
predictive analytics and data slangs. 
94 
00:05:47,080 --> 00:05:49,480 
So what is the difference
between these different terms? 
95 
00:05:51,120 --> 00:05:54,740 
As we have discussed, machine learning
has its roots since statistics, 
96 
00:05:54,740 --> 00:05:58,590 
artificial intelligence, and
computer science among other fields. 
97 
00:05:58,590 --> 00:06:01,100 
Machine learning encompasses
the algorithms and 
98 
00:06:01,100 --> 00:06:02,990 
techniques used to learn from data. 
99 
00:06:04,700 --> 00:06:08,560 
The term data mining became popular
around the time that the use 
100 
00:06:08,560 --> 00:06:10,820 
databases became common place. 
101 
00:06:10,820 --> 00:06:14,980 
So data mining was used to refer to
activities related to finding patterns in 
102 
00:06:14,980 --> 00:06:17,780 
databases and data warehouses. 
103 
00:06:17,780 --> 00:06:21,290 
There are some practical data management
aspects to data mining related to 
104 
00:06:21,290 --> 00:06:23,380 
accessing data from databases. 
105 
00:06:23,380 --> 00:06:26,460 
But the process of finding
patterns in data is similar, and 
106 
00:06:26,460 --> 00:06:29,320 
can use the same algorithms and
techniques as machine learning. 
107 
00:06:31,640 --> 00:06:37,020 
Predictive analytics refers to analyzing
data in order to predict future outcomes. 
108 
00:06:37,020 --> 00:06:41,090 
This term is usually used in the business
context to describe activities such as 
109 
00:06:41,090 --> 00:06:45,540 
sales forecasting or predicting
the purchasing behavior of a customer. 
110 
00:06:45,540 --> 00:06:49,060 
But again the techniques used to make
these predictions are the same techniques 
111 
00:06:49,060 --> 00:06:49,790 
from machine learning. 
112 
00:06:52,050 --> 00:06:55,205 
Data science is a new term that is
used to describe processing and 
113 
00:06:55,205 --> 00:06:57,860 
analyzing data to extract meaning. 
114 
00:06:57,860 --> 00:07:00,890 
Again machine learning techniques
can also be used here. 
115 
00:07:00,890 --> 00:07:04,260 
Because the term data science became
popular at the same time that big 
116 
00:07:04,260 --> 00:07:09,670 
data began appearing, data science usually
refers to extracting meaning from big data 
117 
00:07:09,670 --> 00:07:13,380 
and so includes approaches for collecting,
storing and managing big data. 
118 
00:07:14,420 --> 00:07:16,370 
These terms evolved at different times and 
119 
00:07:16,370 --> 00:07:19,050 
may have encompassed
different sets of activities. 
120 
00:07:19,050 --> 00:07:22,115 
But there have always been more
similarities than differences between 
121 
00:07:22,115 --> 00:07:23,610 
them. 
122 
00:07:23,610 --> 00:07:25,720 
Now they are often used
interchangeably and 
123 
00:07:25,720 --> 00:07:28,370 
have come to mean
essentially the same thing. 
124 
00:07:28,370 --> 00:07:33,127 
The process of extracting valuable insight
from data, the core algorithms and 
125 
00:07:33,127 --> 00:07:36,891 
techniques for doing this do not
change with different terms. 
126 
00:07:38,577 --> 00:07:42,544 
To summarize, in this lecture we've
discussed what machine learning is and 
127 
00:07:42,544 --> 00:07:44,210 
how it is being used. 
128 
00:07:44,210 --> 00:07:48,590 
Machine learning models learn from data to
perform a task without being explicitly 
129 
00:07:48,590 --> 00:07:49,730 
programmed. 
130 
00:07:49,730 --> 00:07:52,650 
They are used to discover patterns and
trends in the data. 
131 
00:07:52,650 --> 00:07:57,060 
And a lab for data driven decisions to
be made for the problems being studied. 
132 
00:07:57,060 --> 00:08:01,550 
We also discuss in this lecture examples
of how machine learning is being used. 
133 
00:08:01,550 --> 00:08:04,840 
We see how the example applications
that machine learning can be applied 
134 
00:08:04,840 --> 00:08:06,320 
to many different areas. 
1 
00:00:01,130 --> 00:00:03,610 
What are the steps in
the Machine Learning Process? 
2 
00:00:03,610 --> 00:00:05,180 
We will discuss those in this lecture. 
3 
00:00:06,980 --> 00:00:11,500 
After this video, you will be able
to identify the steps in the machine 
4 
00:00:11,500 --> 00:00:15,390 
learning process and discuss why
the machine learning process is iterative. 
5 
00:00:17,510 --> 00:00:21,400 
This diagram illustrates the steps
in the machine learning process. 
6 
00:00:21,400 --> 00:00:24,800 
In this lecture,
we will cover an overview of these steps. 
7 
00:00:24,800 --> 00:00:27,640 
In the next lecture,
we will cover each step in more detail. 
8 
00:00:29,890 --> 00:00:33,040 
It should be kept in mind that all of
these steps need to be carried out with 
9 
00:00:33,040 --> 00:00:35,130 
a clear purpose in mind. 
10 
00:00:35,130 --> 00:00:39,440 
That is, the problem or opportunity that
is being addressed must be defined with 
11 
00:00:39,440 --> 00:00:42,220 
clearly stated goals and objectives. 
12 
00:00:42,220 --> 00:00:43,150 
For example, 
13 
00:00:43,150 --> 00:00:47,590 
the purpose of a project may be to study
customer purchasing behavior to come up 
14 
00:00:47,590 --> 00:00:52,850 
with a more effective marketing strategy
in order to increase sales revenue. 
15 
00:00:52,850 --> 00:00:56,420 
The purpose behind the project will
drive the machine learning process. 
16 
00:00:58,190 --> 00:01:00,690 
The first step in the machine
learning process is to get 
17 
00:01:00,690 --> 00:01:04,020 
all available data related
to the problem at hand. 
18 
00:01:04,020 --> 00:01:08,450 
Here, we need to identify all data
sources, collect the data, and 
19 
00:01:08,450 --> 00:01:10,800 
finally integrate data from
these multiple sources. 
20 
00:01:12,590 --> 00:01:16,070 
The next step in the machine learning
process is to prepare the data. 
21 
00:01:17,230 --> 00:01:21,920 
This step is further divided into two
parts, explore data and pre-process data. 
22 
00:01:23,740 --> 00:01:27,550 
The first part of data preparation
involves preliminary exploration 
23 
00:01:27,550 --> 00:01:31,300 
of the data to understand the nature
of the data that we have to work with. 
24 
00:01:32,320 --> 00:01:36,780 
Things we want to understand about
the data are its characteristics, format, 
25 
00:01:36,780 --> 00:01:37,300 
and quality. 
26 
00:01:38,600 --> 00:01:42,310 
A good understanding of the data
leads to a more informed analysis and 
27 
00:01:42,310 --> 00:01:43,650 
a more successful outcome. 
28 
00:01:45,580 --> 00:01:49,140 
Once we know more about the data
through exploratory analysis, 
29 
00:01:49,140 --> 00:01:53,240 
the next part is pre-processing
of the data for analysis. 
30 
00:01:53,240 --> 00:01:57,500 
This includes cleaning data,
selecting the variables to use, and 
31 
00:01:57,500 --> 00:02:01,140 
transforming data to make the data more
suitable for analysis in the next step. 
32 
00:02:02,990 --> 00:02:06,705 
The prepared data then would be
passed on to the analysis step. 
33 
00:02:06,705 --> 00:02:10,280 
This step involves selecting
the analytical techniques to use, 
34 
00:02:10,280 --> 00:02:13,350 
building a model using the data,
and assessing the results. 
35 
00:02:14,980 --> 00:02:18,930 
Step four in the machine learning
process it to communicate results. 
36 
00:02:18,930 --> 00:02:23,490 
This includes evaluating the results with
respect to the goal set for the project. 
37 
00:02:23,490 --> 00:02:26,360 
presenting the results in
a easy to understand way and 
38 
00:02:26,360 --> 00:02:28,058 
communicating the results to others. 
39 
00:02:28,058 --> 00:02:32,320 
The last step is to apply the results. 
40 
00:02:32,320 --> 00:02:35,550 
This brings us back to
the purpose of the project. 
41 
00:02:35,550 --> 00:02:40,185 
How can the insights from our analysis
be used to provide effective marketing 
42 
00:02:40,185 --> 00:02:41,907 
to increase sales revenue? 
43 
00:02:41,907 --> 00:02:45,039 
Determining actions from insights gained 
44 
00:02:45,039 --> 00:02:48,524 
from analysis is the main
focus of the act step. 
45 
00:02:48,524 --> 00:02:52,480 
Note that the machine learning
process is a very iterative one. 
46 
00:02:52,480 --> 00:02:55,960 
Findings from one step may require
a previous step to be repeated 
47 
00:02:55,960 --> 00:02:57,650 
with new information. 
48 
00:02:57,650 --> 00:03:02,170 
For example, during the prepare step, we
may find some data quality issues that may 
49 
00:03:02,170 --> 00:03:06,760 
require us to go back to the acquire
step to address some issues with data 
50 
00:03:06,760 --> 00:03:10,550 
collection or to get additional data that
we didn't include in the first go around. 
51 
00:03:11,680 --> 00:03:14,810 
Each step may also require
several iterations. 
52 
00:03:14,810 --> 00:03:19,260 
For example, it is common to try different
analysis techniques in the analyze step 
53 
00:03:19,260 --> 00:03:21,470 
in order to get reasonable
results from the model. 
54 
00:03:22,490 --> 00:03:26,540 
So, it is important to recognize that
this is a highly iterative process, and 
55 
00:03:26,540 --> 00:03:27,320 
not a linear one. 
1 
00:00:00,008 --> 00:00:05,620 
Now that Maya has explained
the basics of machine learning, 
2 
00:00:05,620 --> 00:00:10,660 
let's remember how big data
influences analytical applications. 
3 
00:00:10,660 --> 00:00:15,020 
And how we can take advantage of
the existing big data tools and 
4 
00:00:15,020 --> 00:00:17,490 
techniques in machine learning. 
5 
00:00:17,490 --> 00:00:20,960 
How can machine learning
algorithms be scaled up 
6 
00:00:20,960 --> 00:00:24,010 
to process large volumes of data? 
7 
00:00:24,010 --> 00:00:25,120 
Let's talk about that now. 
8 
00:00:27,100 --> 00:00:30,395 
After this video, you will be able to 
9 
00:00:30,395 --> 00:00:34,665 
explain how machine learning
techniques can scale up to big data. 
10 
00:00:35,685 --> 00:00:40,855 
And discuss the role of distributed
computing platforms like Hadoop and 
11 
00:00:40,855 --> 00:00:43,485 
Spark in applying machine
learning to big data. 
12 
00:00:44,855 --> 00:00:48,605 
With the massive amounts of data
that need to be processed for 
13 
00:00:48,605 --> 00:00:52,775 
applications, such as drug
effectiveness analysis, 
14 
00:00:52,775 --> 00:00:57,200 
climate monitoring, and
website recommendations to name a few. 
15 
00:00:58,420 --> 00:01:03,570 
We need to be able to add scalability
to machine learning techniques. 
16 
00:01:03,570 --> 00:01:05,600 
How do we apply machine learning at scale? 
17 
00:01:06,790 --> 00:01:11,770 
One way, is to scale up by
adding more memory, processors, 
18 
00:01:11,770 --> 00:01:17,400 
and storage to our system so
that it can store and process more data. 
19 
00:01:17,400 --> 00:01:19,110 
This is not the big data approach. 
20 
00:01:20,270 --> 00:01:25,182 
Specialized hardware such as
graphical processing units, or 
21 
00:01:25,182 --> 00:01:29,536 
GPUs for short,
can also be added to speed up the miracle 
22 
00:01:29,536 --> 00:01:33,730 
operations common in machine
learning algorithms. 
23 
00:01:33,730 --> 00:01:37,300 
Although this is a good approach,
this is also not the big data approach. 
24 
00:01:38,650 --> 00:01:44,510 
As we learned in our introductory course,
one problem with this approach 
25 
00:01:44,510 --> 00:01:48,720 
is that larger specialized
hardware can be very costly. 
26 
00:01:50,180 --> 00:01:55,590 
Another problem is that we
eventually will hit a limit. 
27 
00:01:55,590 --> 00:01:58,830 
There's only so
much hardware you can add to a machine. 
28 
00:01:59,990 --> 00:02:03,100 
An alternative approach is to scale out. 
29 
00:02:04,860 --> 00:02:10,260 
This means using many local commodity
distribution systems together. 
30 
00:02:11,310 --> 00:02:16,160 
Data is distributed over these
systems to gain processing speed up. 
31 
00:02:18,030 --> 00:02:25,870 
As shown in this illustration the idea is
to divide the data into smaller subsets. 
32 
00:02:25,870 --> 00:02:31,900 
The same processing is applied to
each subset, or map, and the results 
33 
00:02:31,900 --> 00:02:36,660 
are merged at the end to come up with the
overall results for the original dataset. 
34 
00:02:37,770 --> 00:02:43,700 
Let's consider an example,
where we want to apply the same operation 
35 
00:02:43,700 --> 00:02:48,420 
to all the samples in
a dataset of N samples. 
36 
00:02:48,420 --> 00:02:51,640 
In this case, N is four. 
37 
00:02:51,640 --> 00:02:56,150 
If it takes T time units to perform this 
38 
00:02:56,150 --> 00:03:01,460 
operation on each sample,
then with sequential processing 
39 
00:03:01,460 --> 00:03:06,350 
the time to apply that operation
to all samples, is N times T. 
40 
00:03:07,470 --> 00:03:10,582 
If we have a cluster of four processors, 
41 
00:03:10,582 --> 00:03:14,890 
we can distribute the data
across the four processors. 
42 
00:03:16,770 --> 00:03:23,730 
Each process performs the operation on
the dataset subset of N over four samples. 
43 
00:03:25,230 --> 00:03:30,990 
Processing of the four subsets
of the data is done in parallel. 
44 
00:03:30,990 --> 00:03:34,390 
That is, the subsets
are processed at the same time. 
45 
00:03:36,010 --> 00:03:42,870 
The processing time for the distributed
approach is approximately N over 4 times T 
46 
00:03:44,010 --> 00:03:50,700 
plus any overhead required to merge
the subset results and maybe shuffle them. 
47 
00:03:52,100 --> 00:03:57,050 
This is a speedup of nearly four
times over the sequential approach. 
48 
00:03:59,040 --> 00:04:03,021 
On a distributed computing
platform such as Spark or Hadoop, 
49 
00:04:03,021 --> 00:04:08,610 
scalable machine learning algorithms
use the same scale out approach. 
50 
00:04:08,610 --> 00:04:12,620 
Data is distributed across
different processors, 
51 
00:04:12,620 --> 00:04:17,690 
which operate on the data subsets
in parallel using map, reduce, and 
52 
00:04:17,690 --> 00:04:20,230 
other distributed
parallel transformations. 
53 
00:04:21,480 --> 00:04:22,420 
This allows for 
54 
00:04:22,420 --> 00:04:26,050 
machine learning techniques to be
applied to large volumes of data. 
55 
00:04:27,230 --> 00:04:33,300 
In this course, we will use Spark and
its scalable machine learning library, 
56 
00:04:33,300 --> 00:04:38,830 
MLF, to show you how machine
learning can be applied to big data. 
57 
00:04:38,830 --> 00:04:39,640 
And don't forget, 
58 
00:04:39,640 --> 00:04:44,890 
this is the processing of the machine
learning on where the data resides. 
59 
00:04:44,890 --> 00:04:47,480 
And we call this, the Big Data Approach. 
60 
00:04:48,770 --> 00:04:54,020 
However, you can also imagine
a scenario where you also 
61 
00:04:54,020 --> 00:04:59,070 
update the machine learning
algorithms to scale up. 
62 
00:04:59,070 --> 00:05:03,389 
So you can paralyze the machine
learning algorithms themselves, 
63 
00:05:03,389 --> 00:05:07,644 
and also use processing of big
data together with this approach. 
1 
00:00:00,910 --> 00:00:05,618 
In this video, we will provide you with
a quick summary of the main points 
2 
00:00:05,618 --> 00:00:09,644 
from our first three courses to
recall what you have learned. 
3 
00:00:09,644 --> 00:00:13,520 
If you have just completed our third
course and do not need a refresher, 
4 
00:00:13,520 --> 00:00:15,470 
you might skip to the next lecture. 
5 
00:00:18,140 --> 00:00:23,325 
We started our first course explaining
how a new torrent of big data 
6 
00:00:23,325 --> 00:00:28,974 
combined with cloud computing
capabilities to process data anytime and 
7 
00:00:28,974 --> 00:00:33,803 
anywhere has been at the core of
the launch of the big data era. 
8 
00:00:33,803 --> 00:00:37,995 
Such capabilities enable or
present opportunities for 
9 
00:00:37,995 --> 00:00:43,704 
many dynamic data-driven applications,
including energy management, 
10 
00:00:43,704 --> 00:00:48,620 
smart cities, precision medicine,
and smart manufacturing. 
11 
00:00:49,620 --> 00:00:54,163 
These applications are increasingly
more data-driven, dynamic and 
12 
00:00:54,163 --> 00:00:58,470 
heterogeneous in terms of
their technology needs. 
13 
00:00:58,470 --> 00:01:01,490 
They're also more process-driven and 
14 
00:01:01,490 --> 00:01:06,130 
need to be tackled using
a collaborative approach by a team that 
15 
00:01:06,130 --> 00:01:10,400 
puts value on accountability and
reproducibility of the results. 
16 
00:01:11,840 --> 00:01:17,250 
Overall, by modeling,
managing, integrating diverse 
17 
00:01:17,250 --> 00:01:22,260 
data streams we add value
to our big data and 
18 
00:01:22,260 --> 00:01:26,030 
improve our business even more
before we start analyzing it. 
19 
00:01:27,640 --> 00:01:29,080 
A part of modeling and 
20 
00:01:29,080 --> 00:01:34,740 
managing big data is focusing on
the dimensions of the scalability and 
21 
00:01:34,740 --> 00:01:39,080 
considering the challenges associated with
these dimensions to pick the right tools. 
22 
00:01:40,990 --> 00:01:45,859 
We also talked about characteristics
of big data, referring to some Vs 
23 
00:01:45,859 --> 00:01:51,360 
like volume, variety, velocity,
veracity and valence. 
24 
00:01:52,500 --> 00:01:58,727 
Each week presents a challenging
dimension of big data, 
25 
00:01:58,727 --> 00:02:06,010 
namely size, complexity, speed,
quality and connectedness. 
26 
00:02:06,010 --> 00:02:08,530 
We also added a sixth V, 
27 
00:02:08,530 --> 00:02:12,920 
value, referring to the real reason
we are interested in big data. 
28 
00:02:14,550 --> 00:02:18,660 
To turn it into an advantage in the
context of a problem using data science 
29 
00:02:18,660 --> 00:02:21,890 
techniques, big data needs to be analyzed. 
30 
00:02:23,470 --> 00:02:30,530 
We explained a five steps process for data
science that includes data acquisition, 
31 
00:02:30,530 --> 00:02:34,500 
modeling, management,
integration, and analysis. 
32 
00:02:35,620 --> 00:02:38,370 
The influence of big data pushes for 
33 
00:02:38,370 --> 00:02:42,710 
alternative scalability approaches
at each step of the process. 
34 
00:02:44,820 --> 00:02:50,310 
If we just focus on the scalability
challenges related to the three Vs, 
35 
00:02:50,310 --> 00:02:53,720 
we can say big data has varying volume and 
36 
00:02:53,720 --> 00:02:59,020 
velocity, requiring dynamic and
scalable batch and stream processing. 
37 
00:03:00,110 --> 00:03:05,715 
Big data has variety, requiring management
of data in many different data systems, 
38 
00:03:05,715 --> 00:03:07,790 
and integration of it at scale. 
39 
00:03:09,630 --> 00:03:12,470 
In our introduction to
the big data course, 
40 
00:03:12,470 --> 00:03:16,590 
we talked about the version of a layer
diagram for the tools in the Hadoop 
41 
00:03:16,590 --> 00:03:20,950 
ecosystem, organized vertically
based on the interface. 
42 
00:03:22,520 --> 00:03:27,210 
Low level interfaces for storage and
scheduling on the bottom 
43 
00:03:28,890 --> 00:03:32,610 
and high level languages and
interactivity at the top. 
44 
00:03:33,820 --> 00:03:38,691 
Most of the tools in the Hadoop ecosystem
were initially built to compliment 
45 
00:03:38,691 --> 00:03:43,880 
the capabilities of Hadoop for distributed
file system management using HDFS. 
46 
00:03:45,060 --> 00:03:48,490 
Data processing using
the MapReduce engine, and 
47 
00:03:48,490 --> 00:03:52,810 
resource scheduling, and
negotiation using the YARN engine. 
48 
00:03:54,320 --> 00:03:57,580 
Over time,
a number of new projects were built, 
49 
00:03:57,580 --> 00:04:03,010 
either to add to these complementary
tools or to handle additional types of 
50 
00:04:03,010 --> 00:04:08,640 
big data management and processing not
available in Hadoop, just like Spark. 
51 
00:04:10,260 --> 00:04:15,430 
Arguably, the most important
change to Hadoop over time 
52 
00:04:15,430 --> 00:04:19,960 
was the separation of YARN from
the MapReduce programming model 
53 
00:04:19,960 --> 00:04:22,590 
to solely handle resource
management concerns. 
54 
00:04:24,060 --> 00:04:29,850 
This allowed for Hadoop to be extensible
to different programming models and enable 
55 
00:04:29,850 --> 00:04:34,640 
the development of a number of processing
engines for batch and stream processing. 
56 
00:04:36,140 --> 00:04:40,500 
Another way to look at the vast number of
tools that have been added to the Hadoop 
57 
00:04:40,500 --> 00:04:44,420 
ecosystem is from the point of
view of their functionality 
58 
00:04:44,420 --> 00:04:45,990 
in the big data processing pipeline. 
59 
00:04:47,090 --> 00:04:52,680 
Simply put, these are associated
with three distinct layers for 
60 
00:04:52,680 --> 00:04:57,660 
data management and storage,
for data processing and 
61 
00:04:57,660 --> 00:05:00,780 
for resource coordination and
workflow management. 
62 
00:05:02,360 --> 00:05:08,122 
In our second course, we talked in detail
about the bottom layer in this diagram, 
63 
00:05:08,122 --> 00:05:10,810 
namely data management and storage. 
64 
00:05:12,920 --> 00:05:18,890 
While this layer includes Hadoop's HDFS,
there are a number of other systems 
65 
00:05:18,890 --> 00:05:25,680 
that rely on HDFS as a file system or
implement their own no-SQL storage option. 
66 
00:05:25,680 --> 00:05:30,255 
As big data can have a variety of
structured, semi-structured, and 
67 
00:05:30,255 --> 00:05:35,730 
unstructured formats and
gets analyzed through a variety of tools, 
68 
00:05:35,730 --> 00:05:38,880 
many tools were introduced to
fit this variety of needs. 
69 
00:05:40,210 --> 00:05:43,040 
We call these big data management systems. 
70 
00:05:44,510 --> 00:05:48,690 
We reviewed Redis and Aerospike as 
71 
00:05:48,690 --> 00:05:53,180 
key value stores where each data item
is identified with a unique key. 
72 
00:05:55,670 --> 00:06:00,200 
We also got some practical
experience with Lucene and 
73 
00:06:00,200 --> 00:06:04,842 
Gephi as vector and
graph-stores respectively. 
74 
00:06:06,310 --> 00:06:10,660 
We also talked about Vertica as
a column-store database where 
75 
00:06:10,660 --> 00:06:14,570 
information is stored in
columns rather than rows. 
76 
00:06:16,660 --> 00:06:20,998 
Cassandra and
HBase are also in this category. 
77 
00:06:20,998 --> 00:06:27,749 
Finally, we introduced Solr and
Asterisk DB for managing unstructured and 
78 
00:06:27,749 --> 00:06:32,822 
semi-structured text and
MongoDB as a document store. 
79 
00:06:35,591 --> 00:06:40,444 
The processing layer is where
all these different types 
80 
00:06:40,444 --> 00:06:45,091 
of data get retrieved,
integrated, and analyzed, 
81 
00:06:45,091 --> 00:06:49,343 
which was the primary
focus of our third course. 
82 
00:06:49,343 --> 00:06:52,451 
In the integration and processing layer, 
83 
00:06:52,451 --> 00:06:57,283 
we roughly refer to the tools that
are built on top of HTFS and YARN, 
84 
00:06:57,283 --> 00:07:01,965 
although some of them were with
other storage and file systems. 
85 
00:07:03,675 --> 00:07:10,064 
YARN is a significant enable of many of
these tools making a number of batch and 
86 
00:07:10,064 --> 00:07:16,277 
stream processing engines like Storm,
Spark, Flink and Beam possible. 
87 
00:07:16,277 --> 00:07:20,373 
This layer also includes tools
like Hive and Spark SQL for 
88 
00:07:20,373 --> 00:07:24,894 
bringing a query interface on top
of the storage layer, Pig for 
89 
00:07:24,894 --> 00:07:30,014 
scripting simple big data pipelines
using the MapReduce framework and 
90 
00:07:30,014 --> 00:07:33,512 
a number of specialized
analytical libraries, 
91 
00:07:33,512 --> 00:07:36,775 
formation learning, and graph analytics. 
92 
00:07:36,775 --> 00:07:43,538 
Giraph and GraphX of Spark are examples
of such libraries for graph processing. 
93 
00:07:43,538 --> 00:07:46,448 
Mahout on top of the Hadoop stack and 
94 
00:07:46,448 --> 00:07:50,820 
MLlib of Spark Are two options for
machine learning. 
95 
00:07:51,970 --> 00:07:55,665 
Although we had a basic overview
of graph processing and 
96 
00:07:55,665 --> 00:07:59,991 
machine learning for big data
analytics earlier in our second and 
97 
00:07:59,991 --> 00:08:03,860 
third courses,
we haven't gone into the details there. 
98 
00:08:03,860 --> 00:08:09,576 
In this course, we will use Spark's MLlib
as one of our two main tools, 
99 
00:08:09,576 --> 00:08:15,692 
providing a deeper introduction to
the machine learning library of Spark. 
100 
00:08:15,692 --> 00:08:22,075 
The third and top layer in our diagram is
the coordination and management layer. 
101 
00:08:22,075 --> 00:08:25,888 
This is where integrations,
scheduling, coordination, and 
102 
00:08:25,888 --> 00:08:30,840 
monitoring of applications across many
tools in the bottom two layers take place. 
103 
00:08:31,890 --> 00:08:36,940 
This layer is also where the results of
the big data analysis get communicated 
104 
00:08:36,940 --> 00:08:42,030 
to other programs, websites, visualization
tools and business intelligence tools. 
105 
00:08:43,740 --> 00:08:48,905 
Workflow management systems help to
develop automated solutions that 
106 
00:08:48,905 --> 00:08:53,835 
can manage and coordinate the process
of combining data management and 
107 
00:08:53,835 --> 00:09:00,165 
analytical tasks in a big data pipeline as
a configurable, structured set of steps. 
108 
00:09:01,275 --> 00:09:04,275 
Workflow driven thinking also matches 
109 
00:09:04,275 --> 00:09:08,810 
this basic process of data science
that we overviewed before. 
110 
00:09:08,810 --> 00:09:12,510 
Oozie is an example workflow
scheduler that can interact with 
111 
00:09:12,510 --> 00:09:15,620 
many of the tools in the integration and
processing layer. 
112 
00:09:16,740 --> 00:09:21,364 
Zookeeper is the resource
coordination tool which monitors and 
113 
00:09:21,364 --> 00:09:26,680 
manages and coordinates all these
tools and named after animal. 
114 
00:09:28,510 --> 00:09:32,950 
Now that we've reviewed all three
layers we are ready to come back to 
115 
00:09:32,950 --> 00:09:38,790 
the integration and processing layer, but
now in the context of machine learning. 
116 
00:09:38,790 --> 00:09:42,060 
In which we will use machine
learning techniques to 
117 
00:09:42,060 --> 00:09:46,840 
apply to our five step data science
process and analyze big data. 
118 
00:09:48,570 --> 00:09:54,370 
Just a simple Google search for
big data processing pipelines will bring 
119 
00:09:54,370 --> 00:09:58,950 
a vast number of pipelines with
a large number of technologies 
120 
00:09:58,950 --> 00:10:03,520 
that support scalable data cleaning,
preparation, and analysis. 
121 
00:10:05,080 --> 00:10:09,650 
How do we make sense of all of it to
make sure we use the right tools for 
122 
00:10:09,650 --> 00:10:10,240 
our application? 
123 
00:10:11,320 --> 00:10:14,580 
How do we pick the right
pre-processing and 
124 
00:10:14,580 --> 00:10:17,780 
machine learning techniques to
start doing predictive modeling? 
125 
00:10:19,450 --> 00:10:24,190 
Over the next few weeks Dr.
Mei will walk you through some of the most 
126 
00:10:24,190 --> 00:10:29,515 
fundamental machine learning techniques,
along with introductory hands 
127 
00:10:29,515 --> 00:10:35,102 
on exercises we designed for you to ease
you into the world of machine learning. 
128 
00:10:35,102 --> 00:10:36,418 
Let's get started. 
1 
00:00:01,340 --> 00:00:04,970 
Let's go through an overview of the tools
we will be using in this course. 
2 
00:00:06,210 --> 00:00:10,890 
After this video you will be
able to describe what KNIME is. 
3 
00:00:10,890 --> 00:00:16,450 
Describe what Spark MLlib is, and contrast
KNIME and MLlib as machine learning tools. 
4 
00:00:18,240 --> 00:00:21,940 
The machine learning tools that we will
be using in this course are KNIME and 
5 
00:00:21,940 --> 00:00:23,410 
Spark MLlib. 
6 
00:00:23,410 --> 00:00:25,870 
These are both open source tools. 
7 
00:00:25,870 --> 00:00:28,936 
This lecture will introduce
these tools to you. 
8 
00:00:28,936 --> 00:00:31,780 
You will need to use them for
the hands on activities in this course. 
9 
00:00:33,880 --> 00:00:39,390 
KNIME Analytics is a platform for data
analytics, reporting, and visualization. 
10 
00:00:39,390 --> 00:00:44,650 
The KNIME platform uses a graphical user
interface based approach with drag and 
11 
00:00:44,650 --> 00:00:48,890 
drop features to facilitate
constructing an analytic solution. 
12 
00:00:48,890 --> 00:00:51,630 
The basic components in KNIME
are referred to as nodes. 
13 
00:00:52,690 --> 00:00:55,770 
Each node provides
a specific functionalities, 
14 
00:00:55,770 --> 00:00:57,880 
such as reading in a file, 
15 
00:00:57,880 --> 00:01:01,900 
creating a specific type of machine
running model and generating a plot. 
16 
00:01:03,080 --> 00:01:06,790 
Nodes can be connected to create
machine running workflows or pipelines. 
17 
00:01:08,130 --> 00:01:11,510 
KNIME stands for
Konstanz Information Miner. 
18 
00:01:11,510 --> 00:01:15,610 
The Konstanz is for
the University of Konstanz in Germany. 
19 
00:01:15,610 --> 00:01:18,885 
And note that the K in KNIME is
silent in the pronunciation. 
20 
00:01:21,187 --> 00:01:24,875 
In KNIME you assemble the steps that
need to be performed in a machine 
21 
00:01:24,875 --> 00:01:28,190 
learning process by connecting
nodes to create a workflow. 
22 
00:01:29,200 --> 00:01:33,070 
To create a workflow the user
chooses the appropriate nodes 
23 
00:01:33,070 --> 00:01:37,750 
from the node repository and
assembles them into a workflow. 
24 
00:01:37,750 --> 00:01:41,039 
The workflow can then be
executed in the KNIME work bench. 
25 
00:01:43,200 --> 00:01:47,340 
A node implements a specific
operation in a workflow. 
26 
00:01:47,340 --> 00:01:49,880 
In this screenshot we see two nodes. 
27 
00:01:49,880 --> 00:01:53,380 
The file reader node is used to
read data from a text file or 
28 
00:01:53,380 --> 00:01:55,820 
a URL or a web address. 
29 
00:01:55,820 --> 00:01:59,370 
The decision tree learner node
builds a decision tree model. 
30 
00:02:00,470 --> 00:02:06,226 
Each node can have input and output ports
and can be connected to other nodes. 
31 
00:02:06,226 --> 00:02:09,535 
When a node is executed,
it takes data from its input port, 
32 
00:02:09,535 --> 00:02:15,650 
performs some operations on the data and
writes the results to the output port. 
33 
00:02:15,650 --> 00:02:18,950 
Data is transferred between
nodes that are connected. 
34 
00:02:18,950 --> 00:02:23,130 
A node can be configured by opening
up its configuration dialog. 
35 
00:02:23,130 --> 00:02:25,329 
This is where the parameters for
the node can be set. 
36 
00:02:27,718 --> 00:02:31,878 
The Node Repository is where you will find
all the nodes available in your KNIME 
37 
00:02:31,878 --> 00:02:33,730 
installation. 
38 
00:02:33,730 --> 00:02:36,800 
The nodes are organized by categories. 
39 
00:02:36,800 --> 00:02:41,260 
KNIME provides an array of nodes to
perform operations for data access, 
40 
00:02:41,260 --> 00:02:45,610 
data manipulation, analysis,
visualization, and reporting. 
41 
00:02:47,840 --> 00:02:52,691 
As you can see, KNIME provides
a visual approach to machine learning. 
42 
00:02:52,691 --> 00:02:57,043 
It's GUI-based, drag-and-drop approach
provides an easy way to create and 
43 
00:02:57,043 --> 00:02:59,160 
execute a machine learning workflow. 
44 
00:03:00,320 --> 00:03:04,530 
The open source version of KNIME however
is limited in how large of a dataset 
45 
00:03:04,530 --> 00:03:05,230 
it can handle. 
46 
00:03:06,290 --> 00:03:10,100 
There are commercial extensions to
KNIME to manage large datasets and 
47 
00:03:10,100 --> 00:03:14,390 
offer other extra functionalities, but
these extensions are not open source. 
48 
00:03:16,150 --> 00:03:18,456 
Now let's talk about Spark MLlib. 
49 
00:03:18,456 --> 00:03:21,407 
You've worked with Spark before if
you took the third course in this 
50 
00:03:21,407 --> 00:03:22,710 
specialization on big data. 
51 
00:03:23,980 --> 00:03:26,210 
Spark is a distributed computing platform. 
52 
00:03:27,210 --> 00:03:30,900 
MLlib is a scalable machine learning
library that runs on top of Spark. 
53 
00:03:32,070 --> 00:03:36,770 
It provides distributed implementations of
commonly used machine learning algorithms 
54 
00:03:36,770 --> 00:03:37,469 
and utilities. 
55 
00:03:38,470 --> 00:03:41,502 
The ML and MLlib of course stands for
machine learning. 
56 
00:03:43,801 --> 00:03:49,120 
To implement machine learning operations
in Spark MLlib, you need to write code. 
57 
00:03:49,120 --> 00:03:52,740 
So MLlib is not a GUI-based approach. 
58 
00:03:52,740 --> 00:03:56,360 
This segment of code reads and
parses data from a file, 
59 
00:03:56,360 --> 00:04:00,937 
then builds a decision
tree classification model. 
60 
00:04:00,937 --> 00:04:05,480 
MLlib, as with the base Spark core,
provides an application programming 
61 
00:04:05,480 --> 00:04:11,570 
interface, or API, for
Java, Python, Scala and R. 
62 
00:04:11,570 --> 00:04:15,174 
This means that you can write code in
these programming languages to execute 
63 
00:04:15,174 --> 00:04:17,098 
the base operations provided in Spark. 
64 
00:04:19,082 --> 00:04:22,640 
Spark MLlib runs on
a distributed platform. 
65 
00:04:22,640 --> 00:04:24,550 
It provides machine
learning algorithms and 
66 
00:04:24,550 --> 00:04:28,560 
techniques that are implemented
using distributed processing. 
67 
00:04:28,560 --> 00:04:32,470 
So MLlib is used for processing and
analyzing large datasets. 
68 
00:04:33,500 --> 00:04:37,610 
And as we have discussed, writing code is
required to implement operations in MLlib. 
69 
00:04:39,680 --> 00:04:44,576 
In summary, KNIME is a GUI-based machine
learning tool, while Spark MLlib provides 
70 
00:04:44,576 --> 00:04:50,040 
a programming-based scalable platform for
processing very large datasets. 
71 
00:04:50,040 --> 00:04:54,270 
You will be using both Spark MLlib and
KNIME throughout this course. 
72 
00:04:54,270 --> 00:04:58,420 
We have readings and hands-on exercises to
help you get familiar with these popular 
73 
00:04:58,420 --> 00:05:01,040 
open source tools for machine learning. 
74 
00:05:01,040 --> 00:05:04,360 
I think you will find it very informative
and fun to work with these tools. 
1 
00:00:01,335 --> 00:00:05,239 
Welcome to course four of
the big data specialization. 
2 
00:00:05,239 --> 00:00:10,926 
I'm Ilkay Altintas, for the new learners
I'm the Chief Data Science Officer 
3 
00:00:10,926 --> 00:00:17,059 
at the San Diego Supercomputer Center at
the University of California, San Diego. 
4 
00:00:17,059 --> 00:00:21,317 
I feel honored to teach you
the basics of big data modeling, 
5 
00:00:21,317 --> 00:00:25,170 
management, and
analysis in this specialization. 
6 
00:00:25,170 --> 00:00:29,544 
And to work with Dr.
Mai Nguyen on this class. 
7 
00:00:29,544 --> 00:00:33,270 
>> And I'm Mai Nguyen, while most of
you might be familiar with Ilkay, 
8 
00:00:33,270 --> 00:00:34,915 
I'm a new face. 
9 
00:00:34,915 --> 00:00:39,010 
I'm excited to be here to teach you
what I love doing, machine learning. 
10 
00:00:39,010 --> 00:00:42,330 
I received my PhD in computer science
with a focus on machine learning 
11 
00:00:42,330 --> 00:00:45,240 
from the University of
California in San Diego. 
12 
00:00:45,240 --> 00:00:47,390 
Since then,
I have worked as a data scientist, 
13 
00:00:47,390 --> 00:00:50,550 
and instructor of machine
learning in various venues. 
14 
00:00:50,550 --> 00:00:53,580 
I am the Lead for Data Analytics at SDFC. 
15 
00:00:53,580 --> 00:00:56,770 
In this role, I work on data
science projects doing research 
16 
00:00:56,770 --> 00:00:59,480 
on scalability on machine
learning methods to big data. 
17 
00:01:00,870 --> 00:01:05,970 
>> We are really happy to have you in this
course to develop your understanding and 
18 
00:01:05,970 --> 00:01:08,060 
skills in machine learning. 
19 
00:01:08,060 --> 00:01:11,060 
>> And
give you an introductory level experience 
20 
00:01:11,060 --> 00:01:13,500 
with application of machine
learning to big data. 
21 
00:01:14,650 --> 00:01:19,410 
By now you might have just finished
our first three courses and 
22 
00:01:19,410 --> 00:01:26,250 
learned the basics of big data modeling,
management, integration, and processing. 
23 
00:01:26,250 --> 00:01:28,990 
If you haven't, it's not required. 
24 
00:01:28,990 --> 00:01:32,320 
But for those with less background
in big data management and 
25 
00:01:32,320 --> 00:01:34,650 
systems, you might find it valuable. 
26 
00:01:35,950 --> 00:01:38,590 
>> We understand that you may
not even have heard anything on 
27 
00:01:38,590 --> 00:01:40,090 
machine learning yet. 
28 
00:01:40,090 --> 00:01:44,230 
That's why we will start by
discussing what machine learning is. 
29 
00:01:44,230 --> 00:01:48,330 
Describing some sample applications and
presenting the typical process 
30 
00:01:48,330 --> 00:01:52,500 
of a machine learning project to give
you a sense of what machine learning is. 
31 
00:01:52,500 --> 00:01:55,210 
Then we will delve into some
commonly used machine learning 
32 
00:01:55,210 --> 00:01:57,310 
techniques like classification and
clustering. 
33 
00:01:58,480 --> 00:02:03,630 
>> We are also going to show you how
to explore your data, prepare it for 
34 
00:02:03,630 --> 00:02:08,900 
analysis, and evaluate the results you
get with your machine learning model. 
35 
00:02:10,020 --> 00:02:14,440 
These are all necessary steps for
a successful machine learning solution. 
36 
00:02:16,060 --> 00:02:18,980 
>> As you know, for
many data science applications 
37 
00:02:18,980 --> 00:02:23,210 
one has to use many different tools and
methods to analyze data. 
38 
00:02:23,210 --> 00:02:26,000 
In fact, keeping up with the rapid
development of new tools is 
39 
00:02:26,000 --> 00:02:29,190 
one of the challenges of
today's big data environment. 
40 
00:02:29,190 --> 00:02:32,870 
In this course, we will introduce you to
two different types of machine learning 
41 
00:02:32,870 --> 00:02:35,470 
tools namely Nime and Spark MNL. 
42 
00:02:36,520 --> 00:02:41,320 
Nime is a graphical user interface based
tool that requires no programming. 
43 
00:02:41,320 --> 00:02:44,370 
And as representative of
a set of tools used in visual 
44 
00:02:44,370 --> 00:02:46,810 
workflow approach to machine learning. 
45 
00:02:46,810 --> 00:02:49,784 
You will have hand on practice with
Nime as you go through the exercises in 
46 
00:02:49,784 --> 00:02:50,380 
this course. 
47 
00:02:51,740 --> 00:02:56,260 
We are also excited to show you
examples of data processing 
48 
00:02:56,260 --> 00:03:00,229 
using Sparks Machine Learning library and
OwlWeb. 
49 
00:03:00,229 --> 00:03:02,770 
Our goal here is to provide you 
50 
00:03:02,770 --> 00:03:07,840 
with simple hands-on exercises that
require introductory level programming, 
51 
00:03:07,840 --> 00:03:11,930 
to inspire you on how big data machine
learning tools can be operated. 
52 
00:03:13,230 --> 00:03:15,390 
We wish you a fun time learning and 
53 
00:03:15,390 --> 00:03:20,420 
hope to hear from you in the discussion
forums and learner stories as always. 
54 
00:03:21,530 --> 00:03:24,590 
>> We have suggested time estimates
each week for the course. 
55 
00:03:24,590 --> 00:03:27,990 
But feel free to take the course
at a faster or slower pace. 
56 
00:03:27,990 --> 00:03:30,920 
And don't forget to connect to other
learners through the forums to 
57 
00:03:30,920 --> 00:03:33,000 
enhance your learning experience. 
58 
00:03:33,000 --> 00:03:33,800 
>> Happy learning. 
59 
00:03:33,800 --> 00:03:34,711 
>> Happy learning. 
1 
00:00:00,790 --> 00:00:03,940 
In the last lecture we
discussed data quality issues. 
2 
00:00:03,940 --> 00:00:08,340 
We will now discuss some common techniques
for addressing those quality issues. 
3 
00:00:08,340 --> 00:00:13,600 
After this video, you will be able
to define what imputation means, 
4 
00:00:13,600 --> 00:00:16,570 
illustrate three ways to
handle missing values, and 
5 
00:00:16,570 --> 00:00:21,270 
describe the role of domain knowledge
in addressing data quality issues. 
6 
00:00:21,270 --> 00:00:25,150 
As we discussed in the last lecture,
real world data is messy. 
7 
00:00:25,150 --> 00:00:28,776 
Some data quality issues that you can
find in your data are missing values, 
8 
00:00:28,776 --> 00:00:34,420 
duplicate data, invalid data,
noise and outliers. 
9 
00:00:34,420 --> 00:00:37,840 
You will need to clean your data if you
want to perform any meaningful analysis 
10 
00:00:37,840 --> 00:00:38,390 
on that data. 
11 
00:00:39,960 --> 00:00:42,880 
Recall that missing data occurs
when you don't have a value for 
12 
00:00:42,880 --> 00:00:45,380 
certain variables in some samples. 
13 
00:00:45,380 --> 00:00:49,050 
A simple way to handle missing data is
to simply drop any samples with missing 
14 
00:00:49,050 --> 00:00:50,220 
values or NAs. 
15 
00:00:51,460 --> 00:00:54,320 
All machine learning tools provide
a mechanism or command for 
16 
00:00:54,320 --> 00:00:57,200 
filtering out rows with
any missing values. 
17 
00:00:57,200 --> 00:01:00,360 
The advantage of this approach
is that it is very simple. 
18 
00:01:00,360 --> 00:01:04,770 
The caveat is that you are removing
data when you filter out examples. 
19 
00:01:04,770 --> 00:01:08,510 
If the number of samples dropped is large,
then you end up losing a lot of your data. 
20 
00:01:09,830 --> 00:01:12,970 
An alternative to dropping
samples with missing data is to 
21 
00:01:12,970 --> 00:01:14,810 
impute the missing values. 
22 
00:01:14,810 --> 00:01:18,720 
Imputing means to replace the missing
values with some reasonable values. 
23 
00:01:19,900 --> 00:01:24,000 
The advantage of this approach is that
you're making use of all your data. 
24 
00:01:24,000 --> 00:01:27,310 
Oc course, imputing is more complicated
than simply dropping samples. 
25 
00:01:28,650 --> 00:01:31,000 
There are several ways to
impute missing values. 
26 
00:01:31,000 --> 00:01:34,640 
One strategy is to replace
the missing values with the mean or 
27 
00:01:34,640 --> 00:01:37,100 
median value of the variable. 
28 
00:01:37,100 --> 00:01:42,490 
For example, a missing value for years of
employment can be replaced by the mean or 
29 
00:01:42,490 --> 00:01:47,120 
median value for years of employment for
all current employees. 
30 
00:01:47,120 --> 00:01:50,290 
Another approach is to use
the most frequent value 
31 
00:01:50,290 --> 00:01:52,070 
in place of the missing value. 
32 
00:01:52,070 --> 00:01:56,230 
For example, the most frequently
recorded age of customers 
33 
00:01:56,230 --> 00:02:01,110 
associated with the specific item can
be used if that value is missing. 
34 
00:02:01,110 --> 00:02:05,160 
Alternatively, a sensible value can
be derived as a replacement for 
35 
00:02:05,160 --> 00:02:06,510 
a missing value. 
36 
00:02:06,510 --> 00:02:09,980 
For example, a missing value for
income can be set to zero for 
37 
00:02:09,980 --> 00:02:12,340 
customers less then 18 years old, or 
38 
00:02:12,340 --> 00:02:17,120 
it can be replaced with an average
value based on occupation and location. 
39 
00:02:17,120 --> 00:02:20,110 
Note that this approach requires
knowledge about the application and 
40 
00:02:20,110 --> 00:02:24,130 
the variable with missing values in
order to make reasonable choices 
41 
00:02:24,130 --> 00:02:27,140 
about what valuables would be sensible
to replace the missing values. 
42 
00:02:28,660 --> 00:02:33,390 
In the case of duplicate data one
approach is to delete the older record. 
43 
00:02:33,390 --> 00:02:36,500 
Another approach is to
merge duplicate records. 
44 
00:02:36,500 --> 00:02:41,330 
This often requires a way to determine
how to resolve conflicting values. 
45 
00:02:41,330 --> 00:02:45,720 
For example, in the case of multiple
addresses for the same customer, 
46 
00:02:45,720 --> 00:02:50,360 
some logic for determining similarities
between addresses might be necessary. 
47 
00:02:50,360 --> 00:02:53,130 
For example,
St period is the same as Street. 
48 
00:02:54,740 --> 00:02:59,180 
To address invalid data, consulting
another data source may be necessary. 
49 
00:02:59,180 --> 00:03:02,910 
For example,
an invalid zip code can be corrected 
50 
00:03:02,910 --> 00:03:06,810 
by looking up the correct zip
code based on city and state. 
51 
00:03:06,810 --> 00:03:11,200 
A best estimate for a reasonable value
can also be used as a replacement. 
52 
00:03:11,200 --> 00:03:14,460 
For example, for
a missing age value for an employee, 
53 
00:03:14,460 --> 00:03:18,980 
a reasonable value can be estimated based
on the employee's length of employment. 
54 
00:03:20,850 --> 00:03:23,750 
Noise that distorts the data
values can be addressed by 
55 
00:03:23,750 --> 00:03:26,010 
filtering out the source of the noise. 
56 
00:03:26,010 --> 00:03:30,390 
For example, filtering out the frequency
of a constant background noise 
57 
00:03:30,390 --> 00:03:33,680 
will remove that noise
component from a recording. 
58 
00:03:33,680 --> 00:03:36,670 
This filtering must be
done with care however, 
59 
00:03:36,670 --> 00:03:40,290 
as it can also remove some components
of the true data in the process. 
60 
00:03:41,760 --> 00:03:45,005 
Outliers can be detected through
the use of summary statistics and 
61 
00:03:45,005 --> 00:03:46,760 
plots of the data. 
62 
00:03:46,760 --> 00:03:49,940 
Outliers can significantly skew
the distribution of your data and 
63 
00:03:49,940 --> 00:03:52,430 
thus the results of your analysis. 
64 
00:03:52,430 --> 00:03:55,240 
In cases where outliers are not
the focus of your analysis, 
65 
00:03:55,240 --> 00:03:59,100 
you will want to remove these
outlier samples from your data set. 
66 
00:03:59,100 --> 00:04:01,710 
For example,
when a thermostat malfunctions and 
67 
00:04:01,710 --> 00:04:05,070 
causes values to fluctuate wildly,
or to be much higher or 
68 
00:04:05,070 --> 00:04:08,760 
lower than normal,
these samples should be filtered out. 
69 
00:04:08,760 --> 00:04:13,140 
In some applications, however, outliers
are exactly what you're looking for. 
70 
00:04:13,140 --> 00:04:16,100 
So when you detect outliers,
you don't want to throw them out. 
71 
00:04:16,100 --> 00:04:19,110 
Instead, you want to
examine them more closely. 
72 
00:04:19,110 --> 00:04:23,610 
A classic example of this is in fraud
detection, where outliers represent 
73 
00:04:23,610 --> 00:04:27,410 
potential fraudulent use and
those samples should be analyzed closely. 
74 
00:04:28,700 --> 00:04:31,750 
In order to address data
quality issues effectively 
75 
00:04:31,750 --> 00:04:34,640 
knowledge about
the application is crucial. 
76 
00:04:34,640 --> 00:04:37,360 
Things such as how the data was collected, 
77 
00:04:37,360 --> 00:04:42,980 
the user population, the intended use
of the application etc, are important. 
78 
00:04:42,980 --> 00:04:47,060 
This domain knowledge is essential
to making informed decisions on how 
79 
00:04:47,060 --> 00:04:50,936 
to best impute missing values,
how to handle duplicate records and 
80 
00:04:50,936 --> 00:04:54,688 
invalid data and what to do about
noise and outliers in your data. 
1 
00:00:00,163 --> 00:00:04,662 
This activity we'll be exploring
weather data in Spark. 
2 
00:00:04,662 --> 00:00:09,936 
First, we will load weather data from
a CSV file into a Spark DataFrame. 
3 
00:00:09,936 --> 00:00:15,259 
Next, we will examine the columns and
schema of the DataFrame. 
4 
00:00:15,259 --> 00:00:20,957 
We will then view the summary statistics
and drop rows with missing values. 
5 
00:00:20,957 --> 00:00:23,762 
Finally, we will compute
the correlation between two columns. 
6 
00:00:26,918 --> 00:00:28,698 
Let's begin. 
7 
00:00:28,698 --> 00:00:32,088 
First, we'll create a new
Jupyter Python notebook. 
8 
00:00:32,088 --> 00:00:36,741 
You do this by clicking on New and
choosing Python 3. 
9 
00:00:40,522 --> 00:00:44,058 
Next, we'll import SQL context. 
10 
00:00:44,058 --> 00:00:48,007 
This is done by entering from 
11 
00:00:48,007 --> 00:00:52,960 
pyspark.sql import SQLContext. 
12 
00:00:52,960 --> 00:00:55,320 
Next, we'll create an instance
of the SQLContext. 
13 
00:00:56,320 --> 00:01:03,510 
We'll enter sqlContext = SQLContext(sc). 
14 
00:01:03,510 --> 00:01:06,490 
Now let's read our weather
data into a DataFrame. 
15 
00:01:06,490 --> 00:01:12,177 
We'll call the DataFrame df and
we'll read it using the sqlContext. 
16 
00:01:12,177 --> 00:01:17,027 
We'll enter sqlContext.read.load. 
17 
00:01:17,027 --> 00:01:20,808 
The first argument is the URL of the file. 
18 
00:01:20,808 --> 00:01:28,681 
That's
file:///home/cloudera/Downloads/big-data-- 
19 
00:01:28,681 --> 00:01:34,313 
4/daily_weather.csv. 
20 
00:01:34,313 --> 00:01:39,269 
The second argument specifies
the format of how to read the file. 
21 
00:01:39,269 --> 00:01:44,246 
In this case, we're going to use the
Spark CSV package from Databricks to load 
22 
00:01:44,246 --> 00:01:46,748 
the CSV directly into the DataFrame. 
23 
00:01:46,748 --> 00:01:51,202 
We need to use this because
the Cloudera image only has Spark 1. 
24 
00:01:51,202 --> 00:01:54,628 
In Spark 2 and later,
this package is included so 
25 
00:01:54,628 --> 00:01:57,166 
we don't have to use this argument. 
26 
00:01:57,166 --> 00:02:01,159 
We'll enter ,format= and 
27 
00:02:01,159 --> 00:02:09,510 
the name of the package is
com.databricks.spark.csv. 
28 
00:02:09,510 --> 00:02:13,745 
The next argument specifies that the first
line in the CSV file is the header, 
29 
00:02:13,745 --> 00:02:19,060 
header='true'. 
30 
00:02:19,060 --> 00:02:24,503 
The last argument tells Spark
to try to infer the schema 
31 
00:02:24,503 --> 00:02:29,366 
from the CSV header, inferSchema='true'. 
32 
00:02:29,366 --> 00:02:31,020 
Run this. 
33 
00:02:31,020 --> 00:02:33,240 
Now let's look at our DataFrame. 
34 
00:02:33,240 --> 00:02:38,873 
We can run a df.columns to see
the names of all the columns. 
35 
00:02:38,873 --> 00:02:42,633 
We can also run df.printSchema to
see the schema of the DataFrame. 
36 
00:02:46,360 --> 00:02:49,983 
Next, let's look at the summary
statistics for the data. 
37 
00:02:49,983 --> 00:02:52,645 
We can do this using the describe method. 
38 
00:02:52,645 --> 00:02:58,459 
We'll run df.describe().show(). 
39 
00:03:02,491 --> 00:03:06,739 
This shows summary statistics for
all the columns in the DataFrame. 
40 
00:03:06,739 --> 00:03:10,020 
There's a lot of information here,
so lets just choose one column. 
41 
00:03:10,020 --> 00:03:13,490 
Let's look at air pressure at 9 AM. 
42 
00:03:13,490 --> 00:03:18,867 
We can see the summary statistics for
air pressure 9 AM by running 
43 
00:03:18,867 --> 00:03:23,868 
df.describe('air_pressure_9am').show(). 
44 
00:03:28,531 --> 00:03:34,860 
There are five statistics in this output,
the count, the number of rows, 
45 
00:03:34,860 --> 00:03:40,300 
the mean, the standard deviation,
and the min and max values. 
46 
00:03:40,300 --> 00:03:46,872 
We can see the total number of columns in
the DataFrame by running len(df.columns). 
47 
00:03:50,056 --> 00:03:56,214 
We can also see the total number of rows
in the DataFrame by writing df.count. 
48 
00:03:56,214 --> 00:04:00,502 
This says that there are 1,095
rows in the DataFrame. 
49 
00:04:00,502 --> 00:04:05,393 
However, the summary statistics for
air_pressure_9am, 
50 
00:04:05,393 --> 00:04:07,889 
we see the count is 1,092. 
51 
00:04:09,800 --> 00:04:12,882 
Summary statistics do not
include rows of missing values. 
52 
00:04:12,882 --> 00:04:17,587 
This means that there are three rows in
the air_pressure_9am column that have 
53 
00:04:17,587 --> 00:04:18,630 
missing values. 
54 
00:04:18,630 --> 00:04:21,066 
We can drop these missing values. 
55 
00:04:21,066 --> 00:04:24,670 
Let's create a new DataFrame where
we've dropped these missing values. 
56 
00:04:26,370 --> 00:04:29,572 
We'll call the new DataFrame df2. 
57 
00:04:29,572 --> 00:04:34,773 
To drop the missing values, we'll enter 
58 
00:04:34,773 --> 00:04:42,656 
df.na.drop(subset=['air_pressure_9am']). 
59 
00:04:42,656 --> 00:04:47,779 
We can then count the total number of
rows in the new DataFrame, df2.count. 
60 
00:04:50,551 --> 00:04:55,208 
We see this value agrees with
our earlier value of 1092. 
61 
00:04:55,208 --> 00:05:00,772 
Next, let's compute the correlation
between two columns in the DataFrame. 
62 
00:05:00,772 --> 00:05:05,485 
We'll compute the correlation between
rain accumulation and rain duration. 
63 
00:05:05,485 --> 00:05:11,144 
To do this, we'll enter df2.stat.corr and 
64 
00:05:11,144 --> 00:05:15,245 
then the names of the two columns, 
65 
00:05:15,245 --> 00:05:22,182 
rain_accumulation_9am and
rain_duration_9am. 
1 
00:00:01,100 --> 00:00:05,060 
Visualizing your data is a very
effective way to explore your data. 
2 
00:00:05,060 --> 00:00:08,780 
We'll look at different ways to
visualize your data in this lecture. 
3 
00:00:08,780 --> 00:00:09,950 
After this video, 
4 
00:00:09,950 --> 00:00:15,150 
you will be able to discuss how plots
can be useful in exploring data, 
5 
00:00:15,150 --> 00:00:19,669 
describe how you would use a scatter plot,
and summarize what a boxplot shows. 
6 
00:00:20,840 --> 00:00:24,390 
Visualizing data,
that is looking at data graphically, 
7 
00:00:24,390 --> 00:00:26,800 
is a great way to explore your data set. 
8 
00:00:26,800 --> 00:00:30,505 
Data visualization is a nice complement
to using summary statistics for 
9 
00:00:30,505 --> 00:00:31,422 
exploring data. 
10 
00:00:31,422 --> 00:00:35,358 
We will cover several ways to
visualize your data in this lecture. 
11 
00:00:35,358 --> 00:00:40,080 
There are several types of plots that
you can use to visualize your data. 
12 
00:00:40,080 --> 00:00:47,350 
We will go over histogram, line plot,
scatter plot, bar plot, and box plot. 
13 
00:00:47,350 --> 00:00:50,550 
These are the most commonly used plots,
but there are many others as well. 
14 
00:00:51,750 --> 00:00:55,930 
A histogram is used to display
the distribution of a variable. 
15 
00:00:55,930 --> 00:01:00,250 
The range of values for the variable
is divided into the number of bins, and 
16 
00:01:00,250 --> 00:01:03,610 
the number of values that fall
into each bin is counted. 
17 
00:01:03,610 --> 00:01:05,660 
Which determines the height of each bin. 
18 
00:01:07,410 --> 00:01:11,600 
A histogram can reveal many things
about a variable in your data, for 
19 
00:01:11,600 --> 00:01:16,200 
example, you can usually determine
the central tendency of a variable, 
20 
00:01:16,200 --> 00:01:19,060 
that is where the majority
of the values lie. 
21 
00:01:19,060 --> 00:01:21,989 
You can also see the most frequent
value of values for that variable. 
22 
00:01:23,280 --> 00:01:27,940 
A histogram also shows whether the values
for that variable are skewed and 
23 
00:01:27,940 --> 00:01:30,920 
whether the skewness is to
the left towards smaller values or 
24 
00:01:30,920 --> 00:01:32,810 
to the right towards larger values. 
25 
00:01:33,860 --> 00:01:37,340 
You can also pick outliers in
the histogram as shown on the bottom plot. 
26 
00:01:38,820 --> 00:01:42,460 
A line plot shows how data
values change over time. 
27 
00:01:42,460 --> 00:01:46,910 
The values of a variable or
variables are shown on the Y axis and 
28 
00:01:46,910 --> 00:01:49,780 
the X axis shows the motion of time. 
29 
00:01:49,780 --> 00:01:52,810 
The resulting line displays
the data values over time. 
30 
00:01:54,070 --> 00:01:57,780 
A line plot can show
patterns in your variables. 
31 
00:01:57,780 --> 00:02:02,840 
For example, a cyclical pattern
can be detected as in this plot, 
32 
00:02:02,840 --> 00:02:06,720 
where the values start high,
then decrease and go back up again. 
33 
00:02:07,870 --> 00:02:12,060 
Trends can also be detected as
shown in the upper-right plot 
34 
00:02:12,060 --> 00:02:16,550 
where the values fluctuate but
show a general upward trend over time. 
35 
00:02:17,630 --> 00:02:22,030 
It is also easy to compare how multiple
variables change over time on a single 
36 
00:02:22,030 --> 00:02:24,930 
line plot as displayed in
the center bottom plot. 
37 
00:02:26,500 --> 00:02:31,360 
A scatter plot is a great way to visualize
the relationship between two variables. 
38 
00:02:31,360 --> 00:02:33,830 
One variable is on the x axis. 
39 
00:02:33,830 --> 00:02:37,530 
The other variable is on
the y axis Each sample is 
40 
00:02:37,530 --> 00:02:42,010 
a product using the values of the 2
variables aspects and Y coordinates. 
41 
00:02:42,010 --> 00:02:46,590 
The resulting plot shows how one variable
changes as the other is changed. 
42 
00:02:47,870 --> 00:02:52,220 
A scatter plot can be used to display
the correlation between 2 variables. 
43 
00:02:52,220 --> 00:02:56,090 
For example, 2 variables such as
the high temperature of the day, and 
44 
00:02:56,090 --> 00:02:57,240 
the low temperature of the day, 
45 
00:02:57,240 --> 00:02:59,920 
can have a positive correlation
as shown in this plot. 
46 
00:03:01,080 --> 00:03:05,200 
A positive correlation means that as
the value of one variable increases, 
47 
00:03:05,200 --> 00:03:10,205 
the value of the other variable
also increases by a similar amount. 
48 
00:03:10,205 --> 00:03:13,745 
The upper right scatter plot shows
a negative correlation between two 
49 
00:03:13,745 --> 00:03:15,015 
variables. 
50 
00:03:15,015 --> 00:03:18,285 
This means that as the value
of one variable increases, 
51 
00:03:18,285 --> 00:03:22,730 
there is a corresponding decrease in
the other variable, two variables 
52 
00:03:22,730 --> 00:03:27,440 
can also have a non-linear correlation
as shown in the lower left plot. 
53 
00:03:27,440 --> 00:03:30,040 
This means that a change
in one variable will not 
54 
00:03:30,040 --> 00:03:33,550 
always correspond to the same
change in the other variable. 
55 
00:03:33,550 --> 00:03:37,010 
This is indicated by the curve in
the scatter plot as opposed to something 
56 
00:03:37,010 --> 00:03:39,640 
closer to a straight line for
linear correlation. 
57 
00:03:40,750 --> 00:03:43,890 
There can also be no correlation
between two variables. 
58 
00:03:43,890 --> 00:03:46,910 
In this case, you will see something
like randomly placed dots as 
59 
00:03:46,910 --> 00:03:49,730 
displayed in the lower right plot,
indicating no 
60 
00:03:49,730 --> 00:03:53,250 
relationship between how the two variables
change with respect to each other. 
61 
00:03:54,580 --> 00:03:58,980 
A bar plot is used to show
the distribution of categorical variables. 
62 
00:03:58,980 --> 00:04:02,980 
Recall that a histogram is also used to
look at the distribution of the values 
63 
00:04:02,980 --> 00:04:04,450 
of the variable. 
64 
00:04:04,450 --> 00:04:07,860 
The difference is that in general,
a histogram is used for 
65 
00:04:07,860 --> 00:04:12,990 
numeric variables whereas a bar plot
is used for categorical variables. 
66 
00:04:12,990 --> 00:04:17,850 
In a bar chart, the different categories
of a categorical variable is shown along 
67 
00:04:17,850 --> 00:04:23,870 
the x-axis, and the count of instances for
each category is displayed on the y-axis. 
68 
00:04:23,870 --> 00:04:27,670 
This is an effective way to
compare the different categories. 
69 
00:04:27,670 --> 00:04:30,700 
For example, the most frequent
category can be easily determined. 
70 
00:04:32,090 --> 00:04:36,790 
A bar plot is also a great way to
compare two categorical variables. 
71 
00:04:36,790 --> 00:04:40,630 
For example, this plot compares
two categorical variables. 
72 
00:04:40,630 --> 00:04:45,270 
One in blue and the other in orange,
each with three different categories. 
73 
00:04:45,270 --> 00:04:47,638 
Here you can see that for
the first category, 
74 
00:04:47,638 --> 00:04:49,757 
the blue variable has the higher count, 
75 
00:04:49,757 --> 00:04:53,949 
while the orange variable has a higher
count for the second and third category. 
76 
00:04:53,949 --> 00:04:57,642 
This type of Bar Plot is
called a Grouped Bar Chart. 
77 
00:04:57,642 --> 00:05:00,480 
And the different variables
of products side by side. 
78 
00:05:01,830 --> 00:05:05,580 
A different kind of comparison can be
performed using a Stacked Bar chart 
79 
00:05:05,580 --> 00:05:08,040 
as seen in a lower right quad. 
80 
00:05:08,040 --> 00:05:11,460 
Here, the accounts for the two variables
are stacked on top of each other for 
81 
00:05:11,460 --> 00:05:12,160 
each category. 
82 
00:05:13,290 --> 00:05:16,700 
With this bar chart, you can
determine that the combined count for 
83 
00:05:16,700 --> 00:05:21,830 
the first category is about equal to
the combine count for the second category, 
84 
00:05:21,830 --> 00:05:25,000 
while the compliant count for
the third category is much larger. 
85 
00:05:26,140 --> 00:05:31,140 
A box plot is another plot that shows
the distribution of a numeric variable, 
86 
00:05:31,140 --> 00:05:34,510 
it shows the distribution in a different
format than the histogram, however. 
87 
00:05:35,870 --> 00:05:38,990 
This is how a box plot displays
the distribution of values for 
88 
00:05:38,990 --> 00:05:43,530 
a variable, the gray portion
in the figure is the box part. 
89 
00:05:43,530 --> 00:05:48,110 
The lower and upper boundaries of
the box represent the 25th and 
90 
00:05:48,110 --> 00:05:50,830 
75th percentiles respectively. 
91 
00:05:50,830 --> 00:05:54,480 
This means that the box represents
the middle 50% of the data, 
92 
00:05:54,480 --> 00:05:59,430 
the median is the 50th percentile,
meaning that 50% of 
93 
00:05:59,430 --> 00:06:04,560 
the data is greater than its value and
50% of the data is less than this value. 
94 
00:06:05,600 --> 00:06:10,540 
The top and bottom lines are the Whiskers
and represent the 10th and 
95 
00:06:10,540 --> 00:06:12,890 
90th percentiles respectively. 
96 
00:06:12,890 --> 00:06:17,210 
So, 80% of the data are in the region
indicated by the upper extreme and 
97 
00:06:17,210 --> 00:06:18,710 
lower extreme. 
98 
00:06:18,710 --> 00:06:21,790 
Any data values outside of
this region are outliers and 
99 
00:06:21,790 --> 00:06:24,080 
are indicated as single
point on the box plot. 
100 
00:06:25,410 --> 00:06:28,600 
Note that there are different
variations of the box plot, 
101 
00:06:28,600 --> 00:06:32,520 
with the whiskers representing
different types of extreme values. 
102 
00:06:32,520 --> 00:06:36,700 
Box plots provide a compact way to
show how variables are distributed, so 
103 
00:06:36,700 --> 00:06:39,660 
they are often used to compare variables. 
104 
00:06:39,660 --> 00:06:40,920 
The box plot on the left for 
105 
00:06:40,920 --> 00:06:44,540 
example compares the base salary for
two different roles. 
106 
00:06:44,540 --> 00:06:48,720 
This plot can quickly provide information
regarding the median value, the range and 
107 
00:06:48,720 --> 00:06:51,950 
the spread of the two different variables. 
108 
00:06:51,950 --> 00:06:54,200 
We can quickly see that
the median salary for 
109 
00:06:54,200 --> 00:06:57,160 
the marketing role is higher
than the research role. 
110 
00:06:58,170 --> 00:07:01,210 
We can also see that the variation or
spread of the values for 
111 
00:07:01,210 --> 00:07:06,240 
marketing is greater than for research,
due to the larger area of the purple box. 
112 
00:07:07,870 --> 00:07:11,380 
A box plot can also show you if
the distribution of the data values is 
113 
00:07:11,380 --> 00:07:15,740 
symmetrical, positively skewed or
negatively skewed. 
114 
00:07:15,740 --> 00:07:19,480 
Here we see that a box plot can
also be displayed on its side. 
115 
00:07:19,480 --> 00:07:22,890 
A symmetric distribution is
indicated if the line in the box 
116 
00:07:22,890 --> 00:07:27,090 
which specifies the median,
is in the center of the box. 
117 
00:07:27,090 --> 00:07:30,430 
A negative skew is indicated
when the median is to the right 
118 
00:07:30,430 --> 00:07:32,030 
of the center of the box. 
119 
00:07:32,030 --> 00:07:35,470 
This means that there are more
values that are less than the median 
120 
00:07:35,470 --> 00:07:37,010 
than there are values
greater than the median. 
121 
00:07:38,140 --> 00:07:42,310 
Similarly, a positive skew is indicated
when the median is to the left 
122 
00:07:42,310 --> 00:07:43,270 
of the center of the box. 
123 
00:07:44,455 --> 00:07:47,370 
To summarize,
data visualization provides a quick and 
124 
00:07:47,370 --> 00:07:49,910 
intuitive way to examine your data. 
125 
00:07:49,910 --> 00:07:52,680 
Data visualization should be
used in conjunction with summary 
126 
00:07:52,680 --> 00:07:56,420 
statistics that we discussed in
the last lecture to explore data. 
127 
00:07:56,420 --> 00:08:00,210 
The different types of plots that we have
covered in this lecture will also be very 
128 
00:08:00,210 --> 00:08:03,960 
helpful in communicating your results
throughout your machine learning project. 
1 
00:00:00,950 --> 00:00:04,820 
Let's look at how we can use summary
statistics to explore data in more detail. 
2 
00:00:06,320 --> 00:00:11,740 
After this video you will be able to
define what a summary statistic is, 
3 
00:00:11,740 --> 00:00:14,610 
list three common summary statistics and 
4 
00:00:14,610 --> 00:00:17,930 
explain how summary statistics
are useful in exploring data. 
5 
00:00:19,830 --> 00:00:24,096 
Summary statistics are quantities
that describe a set of data values. 
6 
00:00:24,096 --> 00:00:29,750 
Summary statistics provide a simple and
quick way to summarize a dataset. 
7 
00:00:29,750 --> 00:00:33,270 
We will discuss three main
categories of summary statistics. 
8 
00:00:33,270 --> 00:00:38,650 
Measures of location or centrality,
measures of spread, and measures of shape. 
9 
00:00:40,680 --> 00:00:44,970 
Measures of location are summary
statistics that describe the central or 
10 
00:00:44,970 --> 00:00:47,670 
typical value in your dataset. 
11 
00:00:47,670 --> 00:00:51,700 
These statistics give a sense of
the middle or center of the dataset. 
12 
00:00:52,730 --> 00:00:56,870 
Examples of these are mean,
median and mode. 
13 
00:00:56,870 --> 00:01:00,720 
The mean is just the average
of the values in a dataset. 
14 
00:01:00,720 --> 00:01:05,198 
The median is the value in the middle if
you sorted the values in your dataset. 
15 
00:01:05,198 --> 00:01:10,300 
In a sorted list, half of the values
will be less than the median and 
16 
00:01:10,300 --> 00:01:11,930 
half will be greater than the median. 
17 
00:01:13,200 --> 00:01:15,700 
If the number of data values is even, 
18 
00:01:15,700 --> 00:01:18,609 
then the median is the mean
of the two middle values. 
19 
00:01:19,620 --> 00:01:23,550 
The mode is a value that is repeated
more often than any other value. 
20 
00:01:25,660 --> 00:01:29,290 
In this example we have
a dataset with ten values. 
21 
00:01:29,290 --> 00:01:30,667 
For this dataset, 
22 
00:01:30,667 --> 00:01:36,010 
the mean is 51.1 which is the number
of all the values divided by 10. 
23 
00:01:36,010 --> 00:01:43,380 
The median is 46, if you sort these
numbers, the middle numbers are 42 and 50. 
24 
00:01:43,380 --> 00:01:47,151 
The average of these two numbers is 46. 
25 
00:01:47,151 --> 00:01:51,054 
There are two modes for
the this dataset, 42 and 78, 
26 
00:01:51,054 --> 00:01:55,705 
since each occurs twice,
more than any other value in the dataset. 
27 
00:01:57,480 --> 00:02:02,187 
Measures of spread describe how
dispersed or varied your dataset is. 
28 
00:02:02,187 --> 00:02:06,764 
Common measures of spread are minimum,
maximum, range, 
29 
00:02:06,764 --> 00:02:10,450 
standard deviation and variance. 
30 
00:02:10,450 --> 00:02:13,440 
Minimum and
maximum are of course the smallest and 
31 
00:02:13,440 --> 00:02:16,740 
largest values in your
dataset respectively. 
32 
00:02:16,740 --> 00:02:21,070 
The range is simply the difference
between the maximum and minimum and 
33 
00:02:21,070 --> 00:02:24,020 
tells you how spread out your data is. 
34 
00:02:24,020 --> 00:02:27,785 
Standard deviation describes the amount
of variation in your dataset. 
35 
00:02:28,920 --> 00:02:32,620 
A low standard deviation value means
that the samples in your dataset 
36 
00:02:32,620 --> 00:02:34,630 
tend to be close to the mean. 
37 
00:02:34,630 --> 00:02:39,050 
And a high standard deviation value means
that the data samples are spread out. 
38 
00:02:39,050 --> 00:02:42,480 
Variance is closely related
to standard deviation. 
39 
00:02:42,480 --> 00:02:46,530 
In fact the variance is the square
of the standard deviation. 
40 
00:02:46,530 --> 00:02:51,900 
So it also indicates how spread out
the data samples are from the mean. 
41 
00:02:51,900 --> 00:02:56,780 
For the same dataset, the range is 66
which is a difference between the largest 
42 
00:02:56,780 --> 00:03:00,964 
number which is 87 and
the smallest number which is 21. 
43 
00:03:00,964 --> 00:03:04,620 
The variance is 548.767, 
44 
00:03:04,620 --> 00:03:09,770 
you can calculate this using
a calculator or a spreadsheet. 
45 
00:03:09,770 --> 00:03:15,030 
And the standard deviation is 23.426
which is the square root of the variance. 
46 
00:03:16,680 --> 00:03:21,470 
Measures of shape describe the shape
of the distribution of a set of values. 
47 
00:03:21,470 --> 00:03:24,740 
Common members of shape are skewness and
kurtosis. 
48 
00:03:25,790 --> 00:03:30,150 
Skewness indicates whether the data
values are asymmetrically distributed. 
49 
00:03:31,150 --> 00:03:35,020 
A skewness value of around zero
indicates that the data distribution 
50 
00:03:35,020 --> 00:03:38,760 
is approximately normal, as shown in
the middle figure in the top diagram. 
51 
00:03:39,830 --> 00:03:44,280 
A negative skewness value indicates that
the distribution is skewed to the left, 
52 
00:03:44,280 --> 00:03:47,590 
as indicated in the left
figure in the top diagram. 
53 
00:03:47,590 --> 00:03:50,200 
A positive skewness
value on the other hand 
54 
00:03:50,200 --> 00:03:53,230 
indicates that the data distribution
is skewed to the right. 
55 
00:03:54,340 --> 00:03:58,430 
Kurtosis measures the tailedness
of the data distribution or 
56 
00:03:58,430 --> 00:04:02,170 
how heavy or
fat the tails of the distribution are. 
57 
00:04:02,170 --> 00:04:07,040 
A high kurtosis value describes a
distribution with longer and fatter tails 
58 
00:04:07,040 --> 00:04:12,075 
and a higher and sharper central peak,
indicating the presence of outliers. 
59 
00:04:12,075 --> 00:04:14,692 
A low kurtosis value on the other hand, 
60 
00:04:14,692 --> 00:04:19,376 
describes a distribution with shorter and
lighter tails and lower and 
61 
00:04:19,376 --> 00:04:23,280 
broader central peak,
suggesting the lack of outliers. 
62 
00:04:25,140 --> 00:04:31,740 
In our age example, the skewness is about
0.3 indicating a slight positive skew. 
63 
00:04:31,740 --> 00:04:37,210 
And the kurtosis is -1.2 indicating
a distribution with a low and 
64 
00:04:37,210 --> 00:04:39,940 
broad central peak and
shorter and lighter tails. 
65 
00:04:41,080 --> 00:04:46,210 
Measures of dependence determine if any
relationship exists between variables. 
66 
00:04:46,210 --> 00:04:49,560 
Pairwise correlation is a commonly
used measure of dependence. 
67 
00:04:50,850 --> 00:04:55,480 
This is a table that shows pairwise
correlation for a set of variables. 
68 
00:04:55,480 --> 00:04:58,430 
Note that correlation applies
only to numerical variables. 
69 
00:04:59,470 --> 00:05:04,517 
Correlations is between zero and one,
with zero indicating no correlation, 
70 
00:05:04,517 --> 00:05:07,430 
and one indicating a one
to one correlation. 
71 
00:05:07,430 --> 00:05:10,268 
So a correlation of
0.89 is very strong and 
72 
00:05:10,268 --> 00:05:15,240 
this is expected since a person's height
and weight should be very correlated. 
73 
00:05:16,780 --> 00:05:21,480 
The summary statistics we just covered
are useful for numerical variables. 
74 
00:05:21,480 --> 00:05:25,900 
For categorical variables, we want to look
at statistics that describe the number of 
75 
00:05:25,900 --> 00:05:29,290 
categories and
the frequency of each category. 
76 
00:05:29,290 --> 00:05:31,400 
This is done using a contingency table. 
77 
00:05:32,940 --> 00:05:35,860 
Here's an example that shows
a distribution of people's pets and 
78 
00:05:35,860 --> 00:05:37,140 
their colors. 
79 
00:05:37,140 --> 00:05:42,150 
We can see the most common pet is
a dog and least common's a fish. 
80 
00:05:42,150 --> 00:05:45,948 
Similarly, black is the most common
color and orange the least common. 
81 
00:05:45,948 --> 00:05:51,240 
The contingency table also shows
the distribution between the categories. 
82 
00:05:51,240 --> 00:05:56,750 
For example, only fish are orange
while most of the brown pets are dogs. 
83 
00:05:56,750 --> 00:05:59,810 
In addition to looking at
the traditional summary statistics for 
84 
00:05:59,810 --> 00:06:05,100 
numerical variables, and
category count for categorical variables. 
85 
00:06:05,100 --> 00:06:06,370 
For machine learning problems, 
86 
00:06:06,370 --> 00:06:10,450 
we also want to examine some additional
statistics to quickly validate the data. 
87 
00:06:11,760 --> 00:06:15,050 
One of the first things to
check is the number of rows and 
88 
00:06:15,050 --> 00:06:17,660 
the number of columns in your dataset. 
89 
00:06:17,660 --> 00:06:21,080 
Does the number of rows match
the expected number of samples? 
90 
00:06:21,080 --> 00:06:25,110 
Does the number of columns match
the expected number of variables? 
91 
00:06:25,110 --> 00:06:27,000 
These should be very quick and
easy checks. 
92 
00:06:28,490 --> 00:06:32,770 
Another easy data validation check is
to look at the values in the first and 
93 
00:06:32,770 --> 00:06:36,170 
last few samples in your dataset
to see if they're reasonable. 
94 
00:06:37,500 --> 00:06:41,780 
For example, do the temperature values
looks to be in the right units of measure. 
95 
00:06:43,180 --> 00:06:45,514 
Do the values for rainfall look correct or 
96 
00:06:45,514 --> 00:06:48,053 
are there some values
that look out of place? 
97 
00:06:48,053 --> 00:06:51,447 
Are the data types for
your variables correct, for example, 
98 
00:06:51,447 --> 00:06:55,130 
is the date field captured as dates or
timestamp. 
99 
00:06:55,130 --> 00:06:57,926 
Or is it capture as a string or
numerical value? 
100 
00:06:57,926 --> 00:07:01,770 
These will have consequences in how
these fields should be processed. 
101 
00:07:03,210 --> 00:07:06,490 
Another important step is to check for
missing values. 
102 
00:07:06,490 --> 00:07:10,280 
You need to determine the number
of samples with missing values. 
103 
00:07:10,280 --> 00:07:13,060 
You also need to determine
if there are any variables 
104 
00:07:13,060 --> 00:07:14,964 
with a high percentage of missing values. 
105 
00:07:16,155 --> 00:07:19,585 
Handling missing values is a very
important step in data preparation 
106 
00:07:19,585 --> 00:07:21,260 
which we will cover in the next module. 
107 
00:07:21,260 --> 00:07:25,875 
Having this information will be very
helpful in determining how missing values 
108 
00:07:25,875 --> 00:07:27,915 
should be handled in data preparation. 
109 
00:07:29,270 --> 00:07:33,190 
We covered several types of summary
statistics useful for exploring data and 
110 
00:07:33,190 --> 00:07:34,590 
machine learning. 
111 
00:07:34,590 --> 00:07:39,020 
The statistics provide useful information
about your dataset and should be 
112 
00:07:39,020 --> 00:07:42,480 
thoroughly examine if you want to get
a better understanding of your data. 
1 
00:00:01,066 --> 00:00:05,080 
Now that we are familiar with some
commonly use terms to describe data, 
2 
00:00:05,080 --> 00:00:08,010 
let's look at what data exploration is and
why it's important. 
3 
00:00:09,180 --> 00:00:13,610 
After this video,
you will be able to explain why data 
4 
00:00:13,610 --> 00:00:18,760 
exploration is necessary, articulate
the objectives of data exploration, 
5 
00:00:18,760 --> 00:00:20,920 
list the categories of techniques for
exploring data. 
6 
00:00:22,520 --> 00:00:25,160 
Data exploration means
doing some preliminary 
7 
00:00:25,160 --> 00:00:27,300 
investigation of your data set. 
8 
00:00:27,300 --> 00:00:31,860 
The goal is to gain a better understanding
of the data that you have to work with. 
9 
00:00:31,860 --> 00:00:36,120 
If you understand the characteristics of
your data, you can make optimal use of it 
10 
00:00:36,120 --> 00:00:39,320 
in whatever subsequent processing and
analysis you do with the data. 
11 
00:00:40,690 --> 00:00:45,730 
Note that data exploration is also
called exploratory data analysis, or 
12 
00:00:45,730 --> 00:00:46,470 
EDA for short. 
13 
00:00:47,810 --> 00:00:49,780 
How do you go about exploring data? 
14 
00:00:49,780 --> 00:00:53,150 
There are two main categories of
techniques to explore your data, 
15 
00:00:53,150 --> 00:00:57,130 
one based on summary statistics and
the other based on visualization methods. 
16 
00:00:58,570 --> 00:01:02,580 
Summary statistics provide important
information that summarizes a set 
17 
00:01:02,580 --> 00:01:04,120 
of data values. 
18 
00:01:04,120 --> 00:01:06,050 
There are many such statistics. 
19 
00:01:06,050 --> 00:01:08,470 
Many of them you have
probably heard of before, 
20 
00:01:08,470 --> 00:01:14,560 
such as mean, median,
and standard deviation. 
21 
00:01:14,560 --> 00:01:18,602 
These are some very commonly
used summary statistics. 
22 
00:01:18,602 --> 00:01:22,780 
A summary statistic provides a single
quantity that summarizes some aspects of 
23 
00:01:22,780 --> 00:01:24,090 
the dataset. 
24 
00:01:24,090 --> 00:01:28,200 
For example, the mean, is a single
value that describes the average value 
25 
00:01:28,200 --> 00:01:31,630 
of the dataset,
no matter how large that dataset is. 
26 
00:01:31,630 --> 00:01:36,140 
You can think of the mean as an indicator
of where your dataset is centrally located 
27 
00:01:36,140 --> 00:01:39,720 
on a number line, thus summary
statistics provide a simple and 
28 
00:01:39,720 --> 00:01:41,285 
quick way to summarize a dataset. 
29 
00:01:42,710 --> 00:01:47,000 
Data visualization techniques allow
you to look at your data, graphically. 
30 
00:01:47,000 --> 00:01:50,540 
There are several types of plots that
you can use to visualize your data. 
31 
00:01:50,540 --> 00:01:55,790 
Some examples are histogram,
line plot, and scatter plot. 
32 
00:01:57,290 --> 00:02:01,590 
Each type of plot serves a different
purpose, we will cover the use of plots to 
33 
00:02:01,590 --> 00:02:03,710 
visualize your data in
an upcoming lecture. 
34 
00:02:05,190 --> 00:02:08,110 
What should you look for
when exploring your data? 
35 
00:02:08,110 --> 00:02:10,910 
You use statistics and
visual methods to summarize and 
36 
00:02:10,910 --> 00:02:14,240 
describe your dataset, and
some of the things you'll want to look for 
37 
00:02:14,240 --> 00:02:17,430 
are correlations,
general trends and outliers. 
38 
00:02:18,900 --> 00:02:21,940 
Correlations provide information
about the relation took between 
39 
00:02:21,940 --> 00:02:23,760 
variables in your data. 
40 
00:02:23,760 --> 00:02:25,220 
By looking at correlations, 
41 
00:02:25,220 --> 00:02:29,200 
you may be able to determine that
two variables are very correlated. 
42 
00:02:29,200 --> 00:02:33,500 
This means they provide the same or
similar information about your data. 
43 
00:02:33,500 --> 00:02:37,440 
Since this contain redundant information,
this suggest that you may want to 
44 
00:02:37,440 --> 00:02:40,220 
remove one of the variables
to make the analysis simpler. 
45 
00:02:41,780 --> 00:02:45,690 
Trends in your data will reveal
characteristics in your data. 
46 
00:02:45,690 --> 00:02:49,940 
For example, you can see where
the majority of the data values lie, 
47 
00:02:49,940 --> 00:02:51,220 
whether your data is skilled or 
48 
00:02:51,220 --> 00:02:55,585 
not, what the most frequent value or
values are in a date set, etc. 
49 
00:02:56,960 --> 00:03:00,660 
Looking at trends in your data can also
reveal that a variable is moving in 
50 
00:03:00,660 --> 00:03:05,380 
a certain direction, such as sales revenue
increasing or decreasing over the years. 
51 
00:03:06,820 --> 00:03:08,954 
Calculating the minimum, the maximum and 
52 
00:03:08,954 --> 00:03:12,910 
range of the data values are basic
steps in exploring your data. 
53 
00:03:12,910 --> 00:03:16,450 
Determining outliers is
a also very important. 
54 
00:03:16,450 --> 00:03:19,340 
Outliers indicate potential
problems with the data and 
55 
00:03:19,340 --> 00:03:21,560 
may need to be eliminated
in some applications. 
56 
00:03:22,660 --> 00:03:23,830 
In other applications, 
57 
00:03:23,830 --> 00:03:28,560 
outliers represent interesting data points
that should be looked at more closely. 
58 
00:03:28,560 --> 00:03:31,680 
In either case, outliers usually
require further examination. 
59 
00:03:32,850 --> 00:03:36,600 
In summary, what you get by exploring
your data is a better understanding of 
60 
00:03:36,600 --> 00:03:40,826 
the complexity of the data so
you can work with it more effectively. 
61 
00:03:40,826 --> 00:03:44,560 
Better understanding in turn will
guide the rest of the process and 
62 
00:03:44,560 --> 00:03:47,200 
lead to more informed analysis. 
63 
00:03:47,200 --> 00:03:48,450 
Summary statistics and 
64 
00:03:48,450 --> 00:03:52,190 
visualization techniques
are essential in exploring your data. 
65 
00:03:52,190 --> 00:03:55,330 
This should be used together
to examined a dataset. 
66 
00:03:55,330 --> 00:03:56,523 
In the next two lectures, 
67 
00:03:56,523 --> 00:03:59,895 
we will look at a specific methods that
you can apply to explore your data. 
1 
00:00:01,040 --> 00:00:04,230 
In this lesson you will learn how
to prepare data for analysis. 
2 
00:00:05,500 --> 00:00:09,950 
After this video, you will be able
to articulate the importance of data 
3 
00:00:09,950 --> 00:00:14,480 
preparation, define the objectives
of data preparation, and 
4 
00:00:14,480 --> 00:00:16,310 
list some activities in preparing data. 
5 
00:00:18,170 --> 00:00:22,040 
The raw data that you get directly from
your sources is rarely in the format 
6 
00:00:22,040 --> 00:00:24,790 
that you can use to perform analysis on. 
7 
00:00:24,790 --> 00:00:29,070 
The goal of data preparation is to create
the data that will be used for analysis. 
8 
00:00:30,600 --> 00:00:36,240 
This means cleaning the data, and putting
the data in the right format for analysis. 
9 
00:00:36,240 --> 00:00:40,470 
The latter generally involves selecting
the appropriate features to use and 
10 
00:00:40,470 --> 00:00:42,070 
transforming the data as needed. 
11 
00:00:43,520 --> 00:00:46,950 
The data that you've acquired
will likely have many problems. 
12 
00:00:46,950 --> 00:00:50,940 
An important part of data preparation is
to clean the data that you have to work 
13 
00:00:50,940 --> 00:00:57,030 
with to address what are referred
to as data quality issues. 
14 
00:00:57,030 --> 00:01:01,049 
There are many types of
data quality issues, 
15 
00:01:01,049 --> 00:01:05,176 
including missing values, duplicate data, 
16 
00:01:05,176 --> 00:01:10,180 
inconsistent or invalid data,
noise, and outliers. 
17 
00:01:10,180 --> 00:01:14,310 
The problems listed here can negatively
affect the quality of the data and 
18 
00:01:14,310 --> 00:01:17,430 
compromise the analysis process and
results. 
19 
00:01:17,430 --> 00:01:21,139 
So it is very important to detect and
address these data quality issues. 
20 
00:01:22,370 --> 00:01:27,260 
Some techniques to address data quality
issues include removing data records 
21 
00:01:27,260 --> 00:01:31,250 
with missing values,
merging duplicate records, 
22 
00:01:31,250 --> 00:01:36,430 
generating a best, or at most reasonable,
estimate for invalid values. 
23 
00:01:36,430 --> 00:01:40,859 
We will discuss these techniques in
more detail in the next lecture. 
24 
00:01:40,859 --> 00:01:45,135 
Since the goal of the step of data
preparation is cleaning the data, 
25 
00:01:45,135 --> 00:01:48,520 
it is referred to as Data Cleaning or
Data Cleansing. 
26 
00:01:50,030 --> 00:01:53,360 
After the data has been cleaned,
another goal of data preparation is to 
27 
00:01:53,360 --> 00:01:56,720 
get the data into the format needed for
the analysis. 
28 
00:01:56,720 --> 00:02:01,709 
This step is referred to by many names,
Data Munching, Data Wrangling and 
29 
00:02:01,709 --> 00:02:03,220 
Data Preprocessing. 
30 
00:02:04,930 --> 00:02:08,910 
The two broad categories of data
wrangling are feature selection and 
31 
00:02:08,910 --> 00:02:09,940 
feature transformation. 
32 
00:02:11,040 --> 00:02:13,770 
Feature selection involves
deciding on which features 
33 
00:02:13,770 --> 00:02:16,150 
to use from the existing
ones available in your data. 
34 
00:02:17,160 --> 00:02:20,860 
Features can be removed,
added or combined. 
35 
00:02:20,860 --> 00:02:23,770 
Feature transformation involves
changing the format of the data 
36 
00:02:23,770 --> 00:02:29,610 
in some way to reduce noise or variability
or make the data easier to analyze. 
37 
00:02:29,610 --> 00:02:32,450 
Two common feature transformations
are scaling the data so 
38 
00:02:32,450 --> 00:02:37,260 
that all features have the same value
range and reducing the dimensionality, 
39 
00:02:37,260 --> 00:02:40,490 
which is effectively the number
of features of the data. 
40 
00:02:40,490 --> 00:02:42,750 
We will discuss these techniques
later in this lesson. 
41 
00:02:44,440 --> 00:02:48,390 
Data preparation is a very important
part of the machine learning process. 
42 
00:02:48,390 --> 00:02:51,840 
It can be a tedious process,
but it is a crucial step. 
43 
00:02:51,840 --> 00:02:54,750 
If you do not spend the time and
effort to create good data for 
44 
00:02:54,750 --> 00:02:57,230 
the analysis,
you will not get good results, 
45 
00:02:57,230 --> 00:03:01,200 
no matter how sophisticated the analysis
technique you are using is. 
46 
00:03:01,200 --> 00:03:04,530 
Always remember, garbage in, garbage out. 
47 
00:03:04,530 --> 00:03:08,189 
So take the time to prepare your data
if you want good analysis results. 
1 
00:00:01,220 --> 00:00:05,360 
A very important part of data preparation
is to assess the quality of your data. 
2 
00:00:05,360 --> 00:00:09,120 
We will look at some common data
quality issues in this lecture. 
3 
00:00:09,120 --> 00:00:14,660 
After this video, you will be able to
describe three data quality issues, 
4 
00:00:14,660 --> 00:00:17,680 
name three reasons for
poor data quality and 
5 
00:00:17,680 --> 00:00:21,060 
explain why data quality
issues need to be addressed. 
6 
00:00:21,060 --> 00:00:23,820 
Real world data is often very messy, so 
7 
00:00:23,820 --> 00:00:27,150 
it's a given fact that you will need
to clean your data by identifying and 
8 
00:00:27,150 --> 00:00:30,660 
addressing many issues that
affect the quality of your data. 
9 
00:00:30,660 --> 00:00:33,130 
Let's take a closer look at what
these data quality issues are. 
10 
00:00:34,330 --> 00:00:37,770 
A very common data quality
issue is missing data. 
11 
00:00:37,770 --> 00:00:41,410 
Recall that a sample in your dataset
typically contains several variables or 
12 
00:00:41,410 --> 00:00:45,170 
features like name, age and income. 
13 
00:00:45,170 --> 00:00:48,960 
For some samples, some of these
variables may not have a value. 
14 
00:00:48,960 --> 00:00:53,420 
These are referred to as
missing values in the data. 
15 
00:00:53,420 --> 00:00:57,650 
Missing values are also referred
to as N/A for not available. 
16 
00:00:57,650 --> 00:01:00,810 
So you will see N/A and
missing values used interchangeably. 
17 
00:01:01,910 --> 00:01:05,462 
You may have missing values in your data
if you have an optional field in your 
18 
00:01:05,462 --> 00:01:06,500 
data set. 
19 
00:01:06,500 --> 00:01:11,471 
For example, the field age is often
an optional field on a survey. 
20 
00:01:11,471 --> 00:01:15,590 
Also many people may choose not
to provide a response for income. 
21 
00:01:15,590 --> 00:01:20,470 
And so you will end up with missing values
for the variable income in your data set. 
22 
00:01:20,470 --> 00:01:24,390 
In some cases, a variable may
not be applicable to all cases. 
23 
00:01:24,390 --> 00:01:29,120 
For example, income may not be
applicable to people who are retired or 
24 
00:01:29,120 --> 00:01:31,430 
unemployed or to children. 
25 
00:01:31,430 --> 00:01:34,350 
So you will not have an entry for
income in all of your samples. 
26 
00:01:35,490 --> 00:01:39,930 
You can also have missing values, due to a
data collecting device that malfunctions, 
27 
00:01:39,930 --> 00:01:43,240 
a network problem that affects
how the data was transmitted, or 
28 
00:01:43,240 --> 00:01:47,120 
something else that goes wrong during
the data collection process itself, or 
29 
00:01:47,120 --> 00:01:49,930 
the process of transmitting the data or
storing the data. 
30 
00:01:51,360 --> 00:01:55,680 
Duplicate data occurs when your data set
has data objects that are duplicates or 
31 
00:01:55,680 --> 00:01:57,030 
new duplicates of one another. 
32 
00:01:58,280 --> 00:02:01,090 
An example of this is when there
are two different records for 
33 
00:02:01,090 --> 00:02:03,470 
the same customer with
different addresses. 
34 
00:02:04,860 --> 00:02:06,590 
This can come about, for example, 
35 
00:02:06,590 --> 00:02:10,510 
if a customer's address has changed,
but the second address was simply 
36 
00:02:10,510 --> 00:02:15,160 
added to this customer's records instead
of used to update the first address. 
37 
00:02:15,160 --> 00:02:18,220 
Duplicate data can occur when
merging data from multiple sources. 
38 
00:02:19,470 --> 00:02:20,260 
Invalid or 
39 
00:02:20,260 --> 00:02:24,720 
inconsistent data occurs when you have
an impossible value for a variable. 
40 
00:02:24,720 --> 00:02:29,810 
Some common examples are when you have
a six digit zip code, the letters AB for 
41 
00:02:29,810 --> 00:02:33,570 
state abbreviations, or
a negative numbers for age. 
42 
00:02:33,570 --> 00:02:36,460 
These invalid data values can
come about when there is a data 
43 
00:02:36,460 --> 00:02:38,860 
entry error during data collection. 
44 
00:02:38,860 --> 00:02:41,460 
For example, if you allow people
to type in their zip code and 
45 
00:02:41,460 --> 00:02:45,650 
someone accidentally includes an extra
digit to their five digit zip code, 
46 
00:02:45,650 --> 00:02:47,915 
then you will end up with
an invalid six digit zipcode. 
47 
00:02:49,240 --> 00:02:52,710 
Noise refers to anything
that can distort your data. 
48 
00:02:52,710 --> 00:02:55,780 
Noise can be introduced during
the data collection process or 
49 
00:02:55,780 --> 00:02:57,910 
data transmission process. 
50 
00:02:57,910 --> 00:03:01,590 
An example is buzzing in the background
when an audio message is recorded 
51 
00:03:01,590 --> 00:03:04,920 
due to background noise or
a faulty microphone. 
52 
00:03:04,920 --> 00:03:08,470 
Another example is an overly bright
image due to an incorrect light 
53 
00:03:08,470 --> 00:03:09,290 
exposure setting. 
54 
00:03:10,690 --> 00:03:14,150 
An Outlier is a data sample with
values that are considerably different 
55 
00:03:14,150 --> 00:03:16,550 
than the rest of the other
data samples in a data set. 
56 
00:03:17,670 --> 00:03:21,110 
An example scenario, that can create
Outliers is when there's a sense of 
57 
00:03:21,110 --> 00:03:26,650 
failure that causes values being recorded
to be much higher or lower than normal. 
58 
00:03:26,650 --> 00:03:30,130 
In this case, you want to remove
the Outliers from your data. 
59 
00:03:30,130 --> 00:03:33,580 
In other applications however,
such as fraud detection, 
60 
00:03:33,580 --> 00:03:38,080 
outliers are the important samples
that should be examined more closely. 
61 
00:03:38,080 --> 00:03:41,460 
So depending on the application,
outliers may need to be removed or 
62 
00:03:41,460 --> 00:03:43,270 
be kept for further analysis. 
63 
00:03:44,400 --> 00:03:47,000 
If you simply ignore these
data quality issues, 
64 
00:03:47,000 --> 00:03:51,300 
any analysis that is performed
will produce misleading results. 
65 
00:03:51,300 --> 00:03:54,220 
In addition, some implementations
of analysis techniques 
66 
00:03:54,220 --> 00:03:57,030 
cannot handle some of these
problems such as missing values. 
67 
00:03:58,160 --> 00:04:01,231 
So, problems that we've discussed
in this lecture need to be 
68 
00:04:01,231 --> 00:04:04,312 
addressed before any meaningful
analysis can be performed. 
69 
00:04:04,312 --> 00:04:05,813 
We will discuss some techniques for 
70 
00:04:05,813 --> 00:04:07,970 
handling data quality
issues in the next lecture. 
1 
00:00:00,790 --> 00:00:03,560 
If you have been in a conversation
on machine learning, 
2 
00:00:03,560 --> 00:00:07,680 
you have probably heard terms like
feature, sample, and variable. 
3 
00:00:07,680 --> 00:00:09,950 
We will be defining some of
those terms in this lecture. 
4 
00:00:11,930 --> 00:00:16,580 
After this video, you will be able
to describe what a feature is, and 
5 
00:00:16,580 --> 00:00:18,740 
how it relates to a sample. 
6 
00:00:18,740 --> 00:00:21,480 
Name some alternative terms for feature. 
7 
00:00:21,480 --> 00:00:25,100 
Summarize how a categorical feature
differs from a numerical feature. 
8 
00:00:28,170 --> 00:00:30,300 
Before we delve into the methods for
processing and 
9 
00:00:30,300 --> 00:00:34,690 
analyzing data, let's first start with
defining some term used to describe data, 
10 
00:00:34,690 --> 00:00:36,610 
starting with sample and variable. 
11 
00:00:38,100 --> 00:00:42,240 
A sample is an instance or
example of an entity in your data. 
12 
00:00:42,240 --> 00:00:45,150 
This is typically a row in your dataset. 
13 
00:00:45,150 --> 00:00:48,380 
This figure shows part of a dataset
of values related to weather. 
14 
00:00:49,670 --> 00:00:54,950 
Each row is a sample representing
weather data for particular day. 
15 
00:00:54,950 --> 00:00:59,370 
The table in the figure shows four samples
of weather data, each for different day. 
16 
00:01:00,960 --> 00:01:05,410 
In this table, each sample has
five values associated with it. 
17 
00:01:05,410 --> 00:01:09,430 
These values are different
information pieces about the sample 
18 
00:01:09,430 --> 00:01:13,200 
such as the sample ID, sample date, 
19 
00:01:13,200 --> 00:01:17,660 
minimum temperature, maximum temperature,
and rainfall on that day. 
20 
00:01:18,820 --> 00:01:21,630 
We call these different values
variables of the sample. 
21 
00:01:24,020 --> 00:01:26,920 
There are many names for
sample and variable. 
22 
00:01:26,920 --> 00:01:31,600 
Some other terms for sample that you
might hear in a machine learning 
23 
00:01:31,600 --> 00:01:37,470 
context include record, example,
row, instance and observation. 
24 
00:01:37,470 --> 00:01:40,600 
It is helpful to realize that all of
these terms mean the same thing in 
25 
00:01:40,600 --> 00:01:41,830 
machine learning. 
26 
00:01:41,830 --> 00:01:46,360 
That is, they all refer to a specific
example of an entity in your dataset. 
27 
00:01:47,970 --> 00:01:53,400 
There are also many names for the term
variable, such as feature, column, 
28 
00:01:53,400 --> 00:01:55,450 
dimension, attribute, and field. 
29 
00:01:55,450 --> 00:01:59,330 
All of these terms refer to
specific characteristics for 
30 
00:01:59,330 --> 00:02:00,660 
each sample in your dataset. 
31 
00:02:02,680 --> 00:02:05,540 
An important point to emphasize
about variable is that, 
32 
00:02:05,540 --> 00:02:08,380 
they are additional
values with a data type. 
33 
00:02:08,380 --> 00:02:11,550 
Each variable has a data
type associated with it. 
34 
00:02:11,550 --> 00:02:14,630 
The most common data types are numeric and
categorical. 
35 
00:02:15,770 --> 00:02:19,080 
There are other data types as
well such as string and date but 
36 
00:02:19,080 --> 00:02:23,260 
we will focus on two of the more common
data types, numeric and categorical. 
37 
00:02:25,380 --> 00:02:30,720 
As the name implies, numeric variables
are variables that take on number values. 
38 
00:02:30,720 --> 00:02:35,320 
Numeric variables can be measured, and
their values can be sorted in some way. 
39 
00:02:35,320 --> 00:02:39,120 
Note that a numeric variable can
take on just integer values or 
40 
00:02:39,120 --> 00:02:40,950 
be continuous valued. 
41 
00:02:40,950 --> 00:02:44,600 
It can also have just positive numbers,
negative numbers or both. 
42 
00:02:45,990 --> 00:02:48,750 
Let's go over some examples
of various numeric variables. 
43 
00:02:49,830 --> 00:02:54,570 
A person's height is a positive,
continuous valued number. 
44 
00:02:54,570 --> 00:03:00,220 
The score in an exam is a positive number
that range between zero and a 100%. 
45 
00:03:00,220 --> 00:03:04,280 
The number of transactions per
hour is a positive integer, 
46 
00:03:04,280 --> 00:03:08,030 
whereas the change in a stock price
can be either positive or negative. 
47 
00:03:10,170 --> 00:03:13,650 
A variable with labels,
names, or categories for 
48 
00:03:13,650 --> 00:03:17,070 
values instead of numbers
are called categorical variables. 
49 
00:03:18,120 --> 00:03:22,250 
For example a variable that describes
the color of an item, such as the color of 
50 
00:03:22,250 --> 00:03:28,948 
a car, can have values such as red,
silver, blue, white and black. 
51 
00:03:28,948 --> 00:03:32,140 
These are non-numeric values
that describes some quality or 
52 
00:03:32,140 --> 00:03:33,650 
characteristic of an entity. 
53 
00:03:34,830 --> 00:03:39,670 
These values can be thought of as names or
labels that can be sorted into categories. 
54 
00:03:39,670 --> 00:03:44,450 
Therefore, categorical variables are also
referred to as qualitative variables, or 
55 
00:03:44,450 --> 00:03:45,650 
nominal variables. 
56 
00:03:47,360 --> 00:03:52,340 
Some examples of categorical
variables are gender, marital status, 
57 
00:03:52,340 --> 00:03:55,640 
type of customer, for example,
teenager, adult, senior. 
58 
00:03:56,650 --> 00:04:01,310 
Product categories, for example,
electronics, kitchen, bathroom and 
59 
00:04:01,310 --> 00:04:02,200 
color of an item. 
60 
00:04:04,110 --> 00:04:08,790 
To summarize, a sample is an instance or
example of an entity in your data. 
61 
00:04:08,790 --> 00:04:13,190 
A variable captures a specific
characteristic of each entity. 
62 
00:04:13,190 --> 00:04:16,800 
So a sample has many
variables to describe it. 
63 
00:04:16,800 --> 00:04:19,786 
Data from real applications
are often multidimensional, 
64 
00:04:19,786 --> 00:04:24,240 
meaning that there are many dimensions or
variables describing each sample. 
65 
00:04:24,240 --> 00:04:27,430 
Each variable has a data
type associated with it, 
66 
00:04:27,430 --> 00:04:31,280 
the most common data types are numeric and
categorical. 
67 
00:04:31,280 --> 00:04:34,633 
Note that there are many terms to
describe these data related concepts. 
1 
00:00:00,790 --> 00:00:05,070 
The data used in machine learning
processes often have many variables. 
2 
00:00:05,070 --> 00:00:08,413 
This is what we call
highly dimensional data. 
3 
00:00:08,413 --> 00:00:12,815 
Most of these dimensions may or may not
matter in the context of our application 
4 
00:00:12,815 --> 00:00:14,760 
with the questions we are asking. 
5 
00:00:14,760 --> 00:00:19,288 
Reducing such high dimensions to
a more manageable set of related and 
6 
00:00:19,288 --> 00:00:24,222 
useful variables improves the performance
and accuracy of our analysis. 
7 
00:00:24,222 --> 00:00:30,365 
After this video, you will be able to
explain what dimensionality reduction is, 
8 
00:00:30,365 --> 00:00:34,639 
discuss the benefits of
dimensionality reduction, and 
9 
00:00:34,639 --> 00:00:37,678 
describe how PCA transforms your data. 
10 
00:00:37,678 --> 00:00:42,465 
The number of features or variables
you have in your data set determines 
11 
00:00:42,465 --> 00:00:46,391 
the number of dimensions or
dimensionality of your data. 
12 
00:00:46,391 --> 00:00:50,857 
If your dataset has two features,
than it is two dimensional data. 
13 
00:00:50,857 --> 00:00:55,454 
If it has three features than
it has three features and so on. 
14 
00:00:55,454 --> 00:01:00,090 
You want to use as many features as
possible to capture the characteristics of 
15 
00:01:00,090 --> 00:01:01,016 
your data, but 
16 
00:01:01,016 --> 00:01:05,105 
you also don't want the dimension
audio of your data to be too high. 
17 
00:01:05,105 --> 00:01:10,844 
As the dimensionality increases, the
problem spaces you're looking at increases 
18 
00:01:10,844 --> 00:01:16,197 
requiring substantially more instances
to adequately sample of that space. 
19 
00:01:16,197 --> 00:01:18,805 
So as the dimensionality increases, 
20 
00:01:18,805 --> 00:01:22,640 
the space that you are looking
at grows exponentially. 
21 
00:01:22,640 --> 00:01:26,460 
As the space grows data
becomes increasingly sparse. 
22 
00:01:26,460 --> 00:01:30,782 
In this diagram we see how
the problem space grows as 
23 
00:01:30,782 --> 00:01:35,011 
the dimensionality
increases from 1 to 2 to 3. 
24 
00:01:35,011 --> 00:01:39,915 
In the left plot, we have a one
dimensional space partitioned into four 
25 
00:01:39,915 --> 00:01:43,150 
regions each with size of 5 units. 
26 
00:01:43,150 --> 00:01:48,249 
The middle plot shows a two
dimensional space with 5x5 regions. 
27 
00:01:48,249 --> 00:01:52,429 
The number of regions has
now going from 4 to 16. 
28 
00:01:52,429 --> 00:01:58,911 
In the third plot, the problem space is
three dimensional with 5x5x5 regions. 
29 
00:01:58,911 --> 00:02:03,171 
The number of regions
increased even more to 64. 
30 
00:02:03,171 --> 00:02:08,364 
We see that as the number of dimensions
increases, the number of regions 
31 
00:02:08,364 --> 00:02:13,403 
increases exponentially and
the data becomes increasingly sparse. 
32 
00:02:13,403 --> 00:02:19,230 
With a small dataset relative to the
problem space, analysis results degrade. 
33 
00:02:19,230 --> 00:02:23,927 
In addition, certain calculations used in
analysis become much more difficult to 
34 
00:02:23,927 --> 00:02:26,018 
define and calculate effectively. 
35 
00:02:26,018 --> 00:02:29,998 
For example, distances between samples
are harder to compare since all samples 
36 
00:02:29,998 --> 00:02:32,330 
are far away from each other. 
37 
00:02:32,330 --> 00:02:36,784 
All of these challenges represent
the difficulty of dealing with high 
38 
00:02:36,784 --> 00:02:41,172 
dimensional data and as referred
to as the curse of dimensionality. 
39 
00:02:41,172 --> 00:02:43,632 
To avoid the curse of dimensionality, 
40 
00:02:43,632 --> 00:02:46,918 
you want to reduce
the dimensionality of your data. 
41 
00:02:46,918 --> 00:02:51,095 
This means finding a smaller subset of
features that can effectively capture 
42 
00:02:51,095 --> 00:02:53,030 
the characteristics of your data. 
43 
00:02:54,050 --> 00:02:58,350 
Recall from the lecture on feature
selection part of data preparation is to 
44 
00:02:58,350 --> 00:02:59,996 
select the features to use. 
45 
00:02:59,996 --> 00:03:05,081 
For example, you can a feature that is
very correlated with another feature. 
46 
00:03:05,081 --> 00:03:08,021 
Using feature selection
techniques to select assessitive 
47 
00:03:08,021 --> 00:03:10,850 
features is one approach to
dimensionality reduction. 
48 
00:03:12,220 --> 00:03:16,160 
Another approach to dimensionality
reduction is to mathematically determine 
49 
00:03:16,160 --> 00:03:20,230 
the most important dimension to keep and
ignore the rest. 
50 
00:03:20,230 --> 00:03:23,260 
The idea is to find a smallest
subset of dimensions that 
51 
00:03:23,260 --> 00:03:25,670 
capture the most variation in your data. 
52 
00:03:26,850 --> 00:03:31,734 
This reduces the dimensions of the data
while eliminating the relevant features 
53 
00:03:31,734 --> 00:03:34,263 
making the subsequent analysis simple. 
54 
00:03:34,263 --> 00:03:39,188 
A technique commonly use to find
the subset of most important dimensions is 
55 
00:03:39,188 --> 00:03:43,028 
called principal component analysis,
or PCA for short. 
56 
00:03:43,028 --> 00:03:47,997 
The goal of PCA is to map the data from
the original high dimensional space 
57 
00:03:47,997 --> 00:03:52,722 
to a lower dimensional space that
captures as much of the variation in 
58 
00:03:52,722 --> 00:03:54,282 
the data as possible. 
59 
00:03:54,282 --> 00:03:55,444 
In other words, 
60 
00:03:55,444 --> 00:04:00,760 
PCA aims to find the most useful subset
of dimensions to summarize the data. 
61 
00:04:02,380 --> 00:04:04,330 
This plot illustrates the idea behind PCA. 
62 
00:04:05,560 --> 00:04:10,200 
Here, we have data samples in a two
dimensional space that is defined 
63 
00:04:10,200 --> 00:04:12,044 
by the x axis and the y axis. 
64 
00:04:12,044 --> 00:04:17,168 
You can see that most of the variation in
the data lies along the red diagonal line. 
65 
00:04:17,168 --> 00:04:21,527 
This means that the dat samples are best
differentiated along this dimension 
66 
00:04:21,527 --> 00:04:26,730 
because they're spread out,
not clumped together along this dimension. 
67 
00:04:26,730 --> 00:04:31,577 
This dimension indicated by the red
line is the first principle component 
68 
00:04:31,577 --> 00:04:33,538 
labelled as PC1 in the part. 
69 
00:04:33,538 --> 00:04:39,130 
It captures the large amount of variance
along a single dimension in data. 
70 
00:04:39,130 --> 00:04:44,305 
PC1, indicated by the red line does
not correspond to either axis. 
71 
00:04:44,305 --> 00:04:48,450 
The next principle component is determined
by looking in the direction that is 
72 
00:04:48,450 --> 00:04:52,972 
orthogonal, in other words perpendicular,
to the first principle component which 
73 
00:04:52,972 --> 00:04:56,800 
captures the next largest
amount of variance in the data. 
74 
00:04:56,800 --> 00:05:00,042 
This is the second
principal component PC2 and 
75 
00:05:00,042 --> 00:05:03,212 
it's indicated by
the green line in the plot. 
76 
00:05:03,212 --> 00:05:08,198 
This process can be repeated to find as
many principal components as desired. 
77 
00:05:08,198 --> 00:05:12,726 
Note that the principal components do
not align with either the x-axis or 
78 
00:05:12,726 --> 00:05:13,610 
the y-axis. 
79 
00:05:13,610 --> 00:05:17,720 
And that they are orthogonal, in other
words, perpendicular to each other. 
80 
00:05:17,720 --> 00:05:19,489 
This is what PCA does. 
81 
00:05:19,489 --> 00:05:22,693 
It finds the underlined dimensions,
the principal 
82 
00:05:22,693 --> 00:05:27,223 
components that capture as much of
the variation in the data as possible. 
83 
00:05:27,223 --> 00:05:31,853 
These principal components form a new
coordinates system to transform 
84 
00:05:31,853 --> 00:05:36,567 
the data to, instead of the conventional
dimensions like X, Y, and Z. 
85 
00:05:36,567 --> 00:05:40,320 
So how does PCA help with
dimensionality reduction? 
86 
00:05:40,320 --> 00:05:43,817 
Let's look again in this plot with
the first principle component. 
87 
00:05:43,817 --> 00:05:47,846 
Since the first principle component
captures most of the variations in 
88 
00:05:47,846 --> 00:05:52,342 
the data, the original data sample can
be mapped to this dimension indicated by 
89 
00:05:52,342 --> 00:05:55,045 
the red line with minimum
loss of information. 
90 
00:05:55,045 --> 00:05:58,645 
In this case then,
we map a two-dimensional dataset to 
91 
00:05:58,645 --> 00:06:03,487 
a one-dimensional space while keeping
as much information as possible. 
92 
00:06:03,487 --> 00:06:06,990 
Here are some main points about
principal components analysis. 
93 
00:06:06,990 --> 00:06:10,147 
PCA finds a new coordinate system for
your data, 
94 
00:06:10,147 --> 00:06:14,151 
such that the first coordinate
defined by the first principal 
95 
00:06:14,151 --> 00:06:17,855 
component Captures the greatest
variance in your data. 
96 
00:06:17,855 --> 00:06:22,341 
The second coordinate defined by
the second principal component captures 
97 
00:06:22,341 --> 00:06:25,271 
the second greatest variance in a data,
etc.. 
98 
00:06:25,271 --> 00:06:30,098 
The first few principle components that
capture most of the variance in a data 
99 
00:06:30,098 --> 00:06:34,120 
can be used to define
a lower-dimensional space for your data. 
100 
00:06:34,120 --> 00:06:38,513 
PCA can be a very useful technique for
dimensionality reduction, 
101 
00:06:38,513 --> 00:06:42,284 
especially when working
with high-dimensional data. 
102 
00:06:42,284 --> 00:06:46,032 
While PCA is a useful technique for
reducing the dimensionality of your 
103 
00:06:46,032 --> 00:06:48,718 
data which can help with
the downstream analysis, 
104 
00:06:48,718 --> 00:06:53,715 
it can also make the resulting analysis
models more difficult to interpret. 
105 
00:06:53,715 --> 00:06:58,965 
The original features in your data set
have specific meanings such as income, 
106 
00:06:58,965 --> 00:07:00,865 
age and occupation. 
107 
00:07:00,865 --> 00:07:05,792 
By mapping the data to a new coordinate
system defined by principal components, 
108 
00:07:05,792 --> 00:07:10,364 
the dimensions in your transformed
data no longer have natural meanings. 
109 
00:07:10,364 --> 00:07:14,136 
This should be kept in mind when using
PCA for dimensionality reduction. 
1 
00:00:01,740 --> 00:00:05,720 
In this activity, we will use KNIME to
perform data exploration of weather data. 
2 
00:00:07,890 --> 00:00:10,000 
First, we will create
a new KNIME workflow. 
3 
00:00:11,140 --> 00:00:12,560 
We'll then input the weather data. 
4 
00:00:12,560 --> 00:00:17,740 
Next, we will create
a histogram of air temperature. 
5 
00:00:17,740 --> 00:00:21,250 
Create a scatter plot
between two variables. 
6 
00:00:21,250 --> 00:00:25,930 
Create a bar chart to show the
distribution of a categorical variable. 
7 
00:00:25,930 --> 00:00:29,700 
And finally, create a box plot to
compare two different distributions. 
8 
00:00:31,493 --> 00:00:34,350 
Let's begin. 
9 
00:00:34,350 --> 00:00:36,660 
First, we will create
a new workflow in KNIME. 
10 
00:00:38,100 --> 00:00:40,000 
To do this we go to the File menu. 
11 
00:00:41,277 --> 00:00:48,890 
Select new, select new KNIME workflow and
click on next. 
12 
00:00:50,410 --> 00:00:52,720 
Then we type the name of
the workflow we want to create. 
13 
00:00:53,730 --> 00:00:55,380 
We'll call this one plots. 
14 
00:00:56,850 --> 00:00:57,710 
Click on finish. 
15 
00:00:59,969 --> 00:01:04,690 
Next we want to load
weather data into KNIME. 
16 
00:01:04,690 --> 00:01:06,710 
To do this we'll use a file reader node. 
17 
00:01:07,720 --> 00:01:11,890 
To add this node,
go to the bottom left node repository. 
18 
00:01:11,890 --> 00:01:14,220 
And in the box type in file reader. 
19 
00:01:15,298 --> 00:01:19,670 
Drag File Reader to the canvas. 
20 
00:01:19,670 --> 00:01:23,890 
Next, double click on File Reader to
configure it with the weather data. 
21 
00:01:24,930 --> 00:01:25,900 
Click on the Browse button. 
22 
00:01:27,879 --> 00:01:31,360 
We'll choose the weather file. 
23 
00:01:31,360 --> 00:01:34,084 
That's daily_weather.csv. 
24 
00:01:35,794 --> 00:01:38,875 
We can see preview of
the data at the bottom half 
25 
00:01:38,875 --> 00:01:41,410 
of the file reader configure dialog. 
26 
00:01:41,410 --> 00:01:45,270 
You can see values for
each column in the CSV file. 
27 
00:01:45,270 --> 00:01:48,440 
Click OK to close the dialog. 
28 
00:01:49,520 --> 00:01:53,510 
Next, we want to create
a histogram of air temperature. 
29 
00:01:53,510 --> 00:01:56,410 
We'll add the histogram
node to the workflow. 
30 
00:01:56,410 --> 00:02:00,230 
Again, we go to the node repository and
type in histogram. 
31 
00:02:01,640 --> 00:02:04,170 
We'll drag and
drop histogram to the workflow and 
32 
00:02:06,640 --> 00:02:08,700 
we'll connect File Reader to histogram. 
33 
00:02:09,700 --> 00:02:13,327 
We'll click and
hold on output to File Reader and 
34 
00:02:13,327 --> 00:02:16,610 
drag to the input of histogram and
release. 
35 
00:02:18,530 --> 00:02:22,520 
Before we configure the histogram node,
we need to run the file reader node. 
36 
00:02:22,520 --> 00:02:26,662 
We can do that by selecting
the file reader and 
37 
00:02:26,662 --> 00:02:30,913 
either clicking on the green
arrow at the top or 
38 
00:02:30,913 --> 00:02:34,952 
right clicking and choosing execute here. 
39 
00:02:37,466 --> 00:02:42,508 
Once we've done that,
double-click on histogram. 
40 
00:02:42,508 --> 00:02:48,176 
We'll select error_temp_9AM
as the biding column and 
41 
00:02:48,176 --> 00:02:54,520 
also add error_temp_9AM to
the aggregation column. 
42 
00:02:54,520 --> 00:02:57,570 
We'll leave the default
number of bins as ten. 
43 
00:02:57,570 --> 00:02:59,020 
Click on OK. 
44 
00:03:01,110 --> 00:03:02,070 
Now we'll run the workflow. 
45 
00:03:02,070 --> 00:03:07,060 
You can click on the two arrow green
button at the top to run all the nodes. 
46 
00:03:08,339 --> 00:03:11,420 
Now let's view the histogram. 
47 
00:03:11,420 --> 00:03:13,690 
Right click on the histogram node. 
48 
00:03:13,690 --> 00:03:17,453 
And choose View, Histogram View. 
49 
00:03:17,453 --> 00:03:21,530 
The x-axis shows the value for
each bin in the histogram. 
50 
00:03:21,530 --> 00:03:24,850 
And the y-axis is the count, or frequency. 
51 
00:03:26,080 --> 00:03:30,000 
We can see the most frequent
values are between 60 and 73. 
52 
00:03:30,000 --> 00:03:35,119 
On the right, we also see that
there's some with missing values. 
53 
00:03:36,710 --> 00:03:38,250 
Next close the window. 
54 
00:03:39,580 --> 00:03:43,550 
Now let's create a scatter plot to show
the relationship between two variables. 
55 
00:03:44,650 --> 00:03:47,430 
First, we will add the scatter
plot node to the work flow. 
56 
00:03:50,128 --> 00:03:54,870 
We'll connect the the output of
File Reader to the input of Scatter Plot. 
57 
00:03:56,244 --> 00:04:01,700 
Execute the workflow, right-click on 
58 
00:04:01,700 --> 00:04:07,650 
Scatter Plot and choose View Scatter Plot. 
59 
00:04:07,650 --> 00:04:14,560 
We'll click on the Column Selection and
choose air_temp_9am for the X Column. 
60 
00:04:14,560 --> 00:04:17,990 
And for the Y Column,
choose relative humidity 9 AM. 
61 
00:04:17,990 --> 00:04:20,630 
In this plot, 
62 
00:04:20,630 --> 00:04:24,650 
we can see a negative correlation
between temperature and humidity. 
63 
00:04:24,650 --> 00:04:27,650 
As the temperature increases,
the humidity goes down. 
64 
00:04:29,520 --> 00:04:30,480 
We'll close this window. 
65 
00:04:32,490 --> 00:04:37,890 
Next, we'll create a bar chart to show the
distribution of a categorical variable. 
66 
00:04:37,890 --> 00:04:40,120 
We'll visualize the wind
direction at 9 AM. 
67 
00:04:40,120 --> 00:04:45,290 
We will begin by creating the categorical
variable by using the numeric binner node. 
68 
00:04:46,310 --> 00:04:48,400 
Let's add numeric binner to the work flow. 
69 
00:04:51,011 --> 00:04:56,510 
We'll connect the output of file
reader to the input of numeric binner. 
70 
00:04:57,675 --> 00:05:01,528 
Double-click on Numeric Binner
to configure it. 
71 
00:05:01,528 --> 00:05:05,799 
Select max_wind_direction_9am and 
72 
00:05:05,799 --> 00:05:09,950 
click on add five times to add five bins. 
73 
00:05:11,220 --> 00:05:13,050 
Now let's give a name for
each of these bins. 
74 
00:05:14,510 --> 00:05:17,180 
Select a bin and now choose a name. 
75 
00:05:17,180 --> 00:05:19,920 
The first will be for the North direction. 
76 
00:05:19,920 --> 00:05:22,938 
So we'll call that 1-N. 
77 
00:05:22,938 --> 00:05:29,214 
Next, 2-E for East, 3-S for 
78 
00:05:29,214 --> 00:05:34,444 
South, 4-W for West and 
79 
00:05:34,444 --> 00:05:38,630 
finally 1-N for north again. 
80 
00:05:39,650 --> 00:05:43,340 
Now we need to specify the values for
each of these bins. 
81 
00:05:43,340 --> 00:05:48,010 
We'll select the first one and
set the endpoint to be 45 degrees. 
82 
00:05:48,010 --> 00:05:53,820 
Next, choose east and
set that maximum to be 135. 
83 
00:05:55,330 --> 00:05:59,360 
Select the third one and
choose the max to be 225. 
84 
00:05:59,360 --> 00:06:05,110 
Finally, select the fourth one and
select the max to be 315. 
85 
00:06:05,110 --> 00:06:08,510 
Next, click on a pin new column. 
86 
00:06:08,510 --> 00:06:14,368 
Set the name for categorical variable to
be categorical underscore max underscore. 
87 
00:06:14,368 --> 00:06:18,531 
Wind_direction_9AM. 
88 
00:06:20,419 --> 00:06:23,740 
Click OK. 
89 
00:06:23,740 --> 00:06:25,880 
Now add a histogram note to the work flow. 
90 
00:06:30,111 --> 00:06:34,455 
Connect the output of
Numeric Binner to histogram. 
91 
00:06:34,455 --> 00:06:38,630 
Double-click on Histogram to configure it. 
92 
00:06:38,630 --> 00:06:44,908 
Make sure they binning column is
categorical_max_wind_direction_9AM. 
93 
00:06:44,908 --> 00:06:47,160 
And also there are no aggregation columns. 
94 
00:06:48,800 --> 00:06:52,140 
Click OK and execute the work flow. 
95 
00:06:54,066 --> 00:06:55,840 
Let's view the chart. 
96 
00:06:56,880 --> 00:07:01,170 
Right click on Histogram and
choose View, Histogram View. 
97 
00:07:03,470 --> 00:07:07,720 
This tells us that most of the wind
comes from the east and the south. 
98 
00:07:09,070 --> 00:07:10,620 
And not many measurements from the north. 
99 
00:07:12,220 --> 00:07:13,650 
Next, close this. 
100 
00:07:15,490 --> 00:07:19,252 
Now lets create a box plot to
compare two different distributions. 
101 
00:07:19,252 --> 00:07:22,380 
We w'll examine the distribution,
air pressure for 
102 
00:07:22,380 --> 00:07:26,610 
low humidity days versus normal or
high humidity days. 
103 
00:07:26,610 --> 00:07:30,810 
To do this, we'll first need to creat
a categorical variable for humidity. 
104 
00:07:30,810 --> 00:07:33,800 
Lets add another numeric
binner actor to the workflow. 
105 
00:07:36,599 --> 00:07:40,190 
Connect the output,
a file reader to this actor. 
106 
00:07:42,090 --> 00:07:44,500 
Double click to configure. 
107 
00:07:44,500 --> 00:07:46,830 
Select relative humidity, 9 AM. 
108 
00:07:46,830 --> 00:07:49,830 
And click on add twice to add two bins. 
109 
00:07:51,820 --> 00:07:56,350 
Select the first bin and
we'll call that humidity_low. 
110 
00:07:56,350 --> 00:08:03,750 
Select the second bin and
we'll call that humidity_not_low. 
111 
00:08:03,750 --> 00:08:06,859 
Select the first bin, and 
112 
00:08:06,859 --> 00:08:11,390 
we'll set the maximum value to 25. 
113 
00:08:11,390 --> 00:08:12,900 
Check append new column. 
114 
00:08:14,290 --> 00:08:21,130 
And set the name to low_humidity_day,
click on OK. 
115 
00:08:21,130 --> 00:08:24,110 
Next, add the conditional box
plot node to the workflow. 
116 
00:08:28,604 --> 00:08:33,310 
Right click on conditional
box plot to configure it. 
117 
00:08:33,310 --> 00:08:37,891 
Make sure the nominal column
is low_humidity_day and 
118 
00:08:37,891 --> 00:08:41,100 
the numeric column is air_pressure_9am. 
119 
00:08:41,100 --> 00:08:42,010 
Click OK. 
120 
00:08:42,010 --> 00:08:46,260 
Run the workflow. 
121 
00:08:48,760 --> 00:08:53,410 
Right click on conditional box plot and
choose few additional box plot. 
122 
00:08:55,210 --> 00:08:59,870 
And the x axis, we see the two values or
a categorical variable, humidity low and 
123 
00:08:59,870 --> 00:09:01,650 
humidity not low. 
124 
00:09:01,650 --> 00:09:04,000 
The y axis shows the air pressure values. 
125 
00:09:05,370 --> 00:09:09,300 
I can see that on average,
lower humidity means higher air pressure. 
126 
00:09:11,792 --> 00:09:13,750 
Close this. 
127 
00:09:13,750 --> 00:09:17,330 
Finally, save the workflow when
we're done by clicking on the disk 
128 
00:09:17,330 --> 00:09:18,630 
at the top left of the toolbar. 
1 
00:00:01,190 --> 00:00:04,590 
In addition to cleaning your data
to address data quality issues, 
2 
00:00:04,590 --> 00:00:09,470 
data preparation also includes
selecting features to use for analysis. 
3 
00:00:09,470 --> 00:00:14,980 
After this video, you will be able to
explain what feature selection involves, 
4 
00:00:14,980 --> 00:00:18,880 
discuss the goal of feature selection,
and list three approaches for 
5 
00:00:18,880 --> 00:00:20,940 
selecting features. 
6 
00:00:20,940 --> 00:00:23,720 
Feature selection refers to
choosing the set of features to 
7 
00:00:23,720 --> 00:00:27,000 
use that is appropriate for
the subsequent analysis. 
8 
00:00:27,000 --> 00:00:30,930 
The goal of feature selection is to come
up with the smallest set of features 
9 
00:00:30,930 --> 00:00:35,030 
that best captures the characteristics
of the problem being addressed. 
10 
00:00:35,030 --> 00:00:39,030 
The smaller the number of features used,
the simpler the analysis will be. 
11 
00:00:39,030 --> 00:00:39,980 
But of course, 
12 
00:00:39,980 --> 00:00:44,430 
the set of features used must include
all features relevant to the problem. 
13 
00:00:44,430 --> 00:00:47,100 
So, there must be a balance
between expressiveness and 
14 
00:00:47,100 --> 00:00:48,830 
compactness of the feature set. 
15 
00:00:48,830 --> 00:00:53,560 
There are several methods to
consider in selecting features. 
16 
00:00:53,560 --> 00:00:55,010 
New features can be added, 
17 
00:00:55,010 --> 00:01:00,430 
some features can be removed, features can
be re-coded or features can be combined. 
18 
00:01:00,430 --> 00:01:04,060 
All these operations affect the final
set of features that will be used for 
19 
00:01:04,060 --> 00:01:05,270 
analysis. 
20 
00:01:05,270 --> 00:01:07,790 
Of course some features
can be kept as is as well. 
21 
00:01:09,210 --> 00:01:12,070 
New features can be derived
from existing features. 
22 
00:01:12,070 --> 00:01:17,200 
For example, a new feature to specify
whether a student is in state or 
23 
00:01:17,200 --> 00:01:21,700 
out of state can be added based on
the student's state of residence. 
24 
00:01:21,700 --> 00:01:24,490 
For an application such
as college admissions, 
25 
00:01:24,490 --> 00:01:28,300 
this new feature represents an important
aspect of an application and so 
26 
00:01:28,300 --> 00:01:30,450 
would be very helpful
as a separate feature. 
27 
00:01:31,480 --> 00:01:35,510 
Another example is adding a feature
to indicate the color of a vehicle, 
28 
00:01:35,510 --> 00:01:38,870 
which can play an important role
in an auto insurance application. 
29 
00:01:39,940 --> 00:01:43,000 
Features can also be removed,
candidates for 
30 
00:01:43,000 --> 00:01:45,950 
removal are features
that are very correlated. 
31 
00:01:45,950 --> 00:01:48,630 
During data exploration,
you may have discovered that 
32 
00:01:48,630 --> 00:01:53,820 
two features are very correlated,
that is they change in very similar ways. 
33 
00:01:53,820 --> 00:01:55,990 
For example,
the purchase price of a product and 
34 
00:01:55,990 --> 00:02:00,480 
the amount of sales tax paid
are likely to be very correlated. 
35 
00:02:00,480 --> 00:02:04,250 
The higher the purchase price
the higher the sales tax. 
36 
00:02:04,250 --> 00:02:07,110 
In this case, you might want
to drop one of these features, 
37 
00:02:07,110 --> 00:02:10,340 
since these features have
essentially duplicate information. 
38 
00:02:10,340 --> 00:02:13,280 
And keeping both features makes
the feature set larger and 
39 
00:02:13,280 --> 00:02:16,770 
the analysis unnecessarily more complex. 
40 
00:02:16,770 --> 00:02:19,810 
Features with a high percentage of
missing values may also be good 
41 
00:02:19,810 --> 00:02:21,640 
candidates for removal. 
42 
00:02:21,640 --> 00:02:22,740 
The validity and 
43 
00:02:22,740 --> 00:02:26,360 
usefulness of features with a lot
of missing values are in question. 
44 
00:02:26,360 --> 00:02:29,810 
So removing may not result
in any loss of information. 
45 
00:02:29,810 --> 00:02:32,350 
Again these features would have
been discovered during the data 
46 
00:02:32,350 --> 00:02:33,090 
exploration step. 
47 
00:02:34,240 --> 00:02:37,880 
Irrelevant features should also
be removed from the data set. 
48 
00:02:37,880 --> 00:02:41,550 
Irrelevant features are those that
contain no information that is useful for 
49 
00:02:41,550 --> 00:02:43,320 
the analysis task. 
50 
00:02:43,320 --> 00:02:46,778 
An example of this is employee
ID in predicting income. 
51 
00:02:46,778 --> 00:02:51,770 
Other fields used simply for
identification such as row number, 
52 
00:02:51,770 --> 00:02:54,850 
person's ID, etc.,
are good candidates for removal. 
53 
00:02:55,940 --> 00:02:59,990 
Features can also be combined if the new
feature presents important information 
54 
00:02:59,990 --> 00:03:03,570 
that is not represented by looking at
the original features individually. 
55 
00:03:05,160 --> 00:03:09,640 
For example, BMI, which is body
mass index is an indicator of 
56 
00:03:09,640 --> 00:03:13,290 
whether a person is underweight,
average weight, or overweight. 
57 
00:03:14,290 --> 00:03:17,890 
This is an important feature to have for
a weight loss application. 
58 
00:03:17,890 --> 00:03:22,270 
It represents information about how much
a person weighs relative to their height 
59 
00:03:22,270 --> 00:03:26,230 
that is not available by looking at just
the person's height or weight alone. 
60 
00:03:27,840 --> 00:03:31,480 
A feature can be re-coded as
appropriate for the application. 
61 
00:03:31,480 --> 00:03:35,550 
A common example of this is when you
want to turn a continuous feature in to 
62 
00:03:35,550 --> 00:03:37,420 
a categorical one. 
63 
00:03:37,420 --> 00:03:42,324 
For example for a marketing application
you might want to re-code customer's age 
64 
00:03:42,324 --> 00:03:48,910 
into customer categories such as teenager,
young adult, adult and senior citizen. 
65 
00:03:48,910 --> 00:03:55,310 
So you would map ages 13 to 19 to
teenager, ages 20 to 25 to young adult, 
66 
00:03:55,310 --> 00:04:00,370 
26 to 55 as adult and over 55 as senior. 
67 
00:04:01,590 --> 00:04:05,640 
For some applications you may want
to make use of finery features. 
68 
00:04:05,640 --> 00:04:09,550 
As an example, you might want a feature to
capture whether a customer tends to buy 
69 
00:04:09,550 --> 00:04:11,810 
expensive items or not. 
70 
00:04:11,810 --> 00:04:16,100 
In this case, you would want a feature
that maps to one for a customer with 
71 
00:04:16,100 --> 00:04:20,484 
an average purchase price over a certain
amount and maps to zero otherwise. 
72 
00:04:20,484 --> 00:04:24,960 
Re-coding features can also
result in breaking one feature 
73 
00:04:24,960 --> 00:04:26,920 
into multiple features. 
74 
00:04:26,920 --> 00:04:30,730 
A common example of this is to
separate an address feature 
75 
00:04:30,730 --> 00:04:36,580 
into its constituent parts street address,
city, state and zip code. 
76 
00:04:36,580 --> 00:04:39,280 
This way you can more easily
group records by state, for 
77 
00:04:39,280 --> 00:04:42,130 
example, to provide
a state by state analysis. 
78 
00:04:43,550 --> 00:04:48,280 
Future selection aims to select
the smallest set of features to best 
79 
00:04:48,280 --> 00:04:51,100 
capture the characteristics of
the data for your application. 
80 
00:04:52,390 --> 00:04:56,310 
Know from the examples represented
that domain knowledge once again place 
81 
00:04:56,310 --> 00:05:00,400 
a key role in choosing
the appropriate features to use. 
82 
00:05:00,400 --> 00:05:04,934 
Good understanding of the application is
essential in deciding which features to 
83 
00:05:04,934 --> 00:05:06,190 
add, drop or modify. 
84 
00:05:07,350 --> 00:05:11,256 
It should also be noted that feature
selection can be referred to as feature 
85 
00:05:11,256 --> 00:05:15,596 
engineering, since what you're doing here
is to engineer the best feature set for 
86 
00:05:15,596 --> 00:05:16,661 
your application. 
1 
00:00:01,850 --> 00:00:04,330 
Let's now discuss what
feature transformation is. 
2 
00:00:05,640 --> 00:00:09,340 
After this video you will be able
to articulate the purpose of 
3 
00:00:09,340 --> 00:00:14,170 
feature transformation, list three
feature transformation operations, and 
4 
00:00:14,170 --> 00:00:16,180 
discuss when scaling is important. 
5 
00:00:17,670 --> 00:00:19,390 
In addition to feature selection, 
6 
00:00:19,390 --> 00:00:23,690 
data pre-processing can also
include feature transformation. 
7 
00:00:23,690 --> 00:00:26,730 
Feature transformation involves
mapping a set of values for 
8 
00:00:26,730 --> 00:00:30,860 
the feature to a new set of values to
make the representation of the data more 
9 
00:00:30,860 --> 00:00:34,800 
suitable or easier to process for
the downstream analysis. 
10 
00:00:36,760 --> 00:00:40,010 
A common feature transformation
operation is scaling. 
11 
00:00:40,010 --> 00:00:42,200 
This involves changing
the range of values for 
12 
00:00:42,200 --> 00:00:46,570 
a feature of features to
another specified range. 
13 
00:00:46,570 --> 00:00:49,840 
This is done to avoid allowing
features with large values to 
14 
00:00:49,840 --> 00:00:51,280 
dominate the analysis results. 
15 
00:00:52,300 --> 00:00:56,030 
For example, if your dataset has
both width and height as features, 
16 
00:00:56,030 --> 00:00:59,910 
the magnitude of the weight values,
which are in pounds, will be much 
17 
00:00:59,910 --> 00:01:03,910 
larger than the magnitude of the height
values which are in feet and inches. 
18 
00:01:03,910 --> 00:01:06,950 
So scaling both features
to a common value range 
19 
00:01:06,950 --> 00:01:09,910 
will make the contributions from
both weight and height equal. 
20 
00:01:11,350 --> 00:01:16,230 
One way to perform scaling is to map all
values of a feature to a specific range 
21 
00:01:16,230 --> 00:01:18,690 
such as between zero and one. 
22 
00:01:18,690 --> 00:01:21,342 
For example,
let's say you have a feature for 
23 
00:01:21,342 --> 00:01:24,750 
income that ranges from 30,000 to 100,000. 
24 
00:01:24,750 --> 00:01:29,420 
And you have another feature for years
of employment that ranges from 0 to 50. 
25 
00:01:29,420 --> 00:01:32,500 
These features have very different scales. 
26 
00:01:32,500 --> 00:01:35,840 
If you want both features to have equal
weighting when you compare the data 
27 
00:01:35,840 --> 00:01:41,170 
samples, then you can scale the range
of both features to be between 0 and 1. 
28 
00:01:41,170 --> 00:01:44,870 
That way the income feature which is
on much largest scale than the years 
29 
00:01:44,870 --> 00:01:48,940 
of employment feature will not
dominate the compares result. 
30 
00:01:48,940 --> 00:01:53,350 
Alternatively, scaling can be perform
by transforming the features such that 
31 
00:01:53,350 --> 00:01:57,420 
the results have zero mean,
and unit standard deviation. 
32 
00:01:57,420 --> 00:02:00,730 
The steps to perform the scaling
is to first calculate the mean and 
33 
00:02:00,730 --> 00:02:03,930 
standard deviation values for
the feature to be scaled. 
34 
00:02:03,930 --> 00:02:06,560 
Then for each value, for this feature, 
35 
00:02:06,560 --> 00:02:11,510 
subtract the mean value from that value,
and divide by the standard deviation. 
36 
00:02:11,510 --> 00:02:15,290 
The transformed feature will end
up with a mean value of zero, and 
37 
00:02:15,290 --> 00:02:16,680 
standard deviation of one. 
38 
00:02:17,720 --> 00:02:20,420 
This effectively removes
the units of the features and 
39 
00:02:20,420 --> 00:02:25,370 
converts each future value to number
of standard deviations from the mean. 
40 
00:02:26,540 --> 00:02:30,720 
This scaling method is used when
the min and max values are known. 
41 
00:02:30,720 --> 00:02:34,760 
This is also useful when there are
outliers which will skew the calculation 
42 
00:02:34,760 --> 00:02:38,380 
for the range as the max value is
determined by the furthest outlier. 
43 
00:02:39,400 --> 00:02:43,980 
This scaling operation is often
referred to as zero-normalization or 
44 
00:02:43,980 --> 00:02:45,030 
as standardization. 
45 
00:02:46,390 --> 00:02:49,110 
Filtering is another feature
transformation operation. 
46 
00:02:50,250 --> 00:02:55,450 
This is commonly applied to time series
data, such as speech or audio signals. 
47 
00:02:55,450 --> 00:02:58,520 
A low pass filter can be
used to filter out noise. 
48 
00:02:58,520 --> 00:03:02,910 
Which usually manifests as the high
frequency component in the signal. 
49 
00:03:02,910 --> 00:03:07,090 
A low pass filter removes components
above a certain frequency 
50 
00:03:07,090 --> 00:03:09,410 
allowing the rest to
pass through unaltered. 
51 
00:03:10,750 --> 00:03:14,310 
Filtering can also be used
to remove noise in images. 
52 
00:03:14,310 --> 00:03:19,530 
Noise in an image is random variation
in intensity or color in the pixels. 
53 
00:03:19,530 --> 00:03:23,590 
For example, a noise can cause
an image to appear grainy. 
54 
00:03:23,590 --> 00:03:28,180 
A mean or median filter can be used to
replace the pixel value with the mean or 
55 
00:03:28,180 --> 00:03:30,240 
median value of its neighboring pixels. 
56 
00:03:31,270 --> 00:03:33,730 
This has the effect of
smoothing out the image, 
57 
00:03:33,730 --> 00:03:36,390 
removing the noise that causes
the graininess in the image. 
58 
00:03:37,830 --> 00:03:41,911 
Aggregation combines values for
a feature in order to summarize data or 
59 
00:03:41,911 --> 00:03:43,210 
reduce variability. 
60 
00:03:44,340 --> 00:03:48,850 
Aggregation combines values for a feature
in order to summarize the data or 
61 
00:03:48,850 --> 00:03:50,970 
to reduce variability. 
62 
00:03:50,970 --> 00:03:55,660 
Aggregation is done by summing or
averaging data values at a higher level. 
63 
00:03:55,660 --> 00:03:59,720 
For example, hourly values can be
aggregated to the daily level. 
64 
00:03:59,720 --> 00:04:04,255 
Or values within a city region can
be aggregated to a state level. 
65 
00:04:04,255 --> 00:04:07,010 
These plots show
the results of aggregation. 
66 
00:04:07,010 --> 00:04:11,060 
The left plot shows the wind
speed values in miles per hour 
67 
00:04:11,060 --> 00:04:14,730 
averaged every 10 minutes
over a period of seven days. 
68 
00:04:14,730 --> 00:04:19,110 
Notice that there's a lot of variability,
that is the values fluctuate a lot. 
69 
00:04:20,130 --> 00:04:23,730 
The right plot shows wind speed
values averaged every hour so 
70 
00:04:23,730 --> 00:04:26,920 
every 60 minutes instead
of every 10 minutes. 
71 
00:04:26,920 --> 00:04:31,340 
Notice that the line is much smoother
here because the values are aggregated at 
72 
00:04:31,340 --> 00:04:32,750 
a higher time scale level. 
73 
00:04:33,750 --> 00:04:36,900 
So aggregation can have
the effect of removing noise 
74 
00:04:36,900 --> 00:04:40,160 
to provide a clear representation
of the structure of your data. 
75 
00:04:41,240 --> 00:04:44,260 
Another example is tracking stock prices. 
76 
00:04:44,260 --> 00:04:47,620 
Hourly deviations of a stock
may be difficult to track but, 
77 
00:04:47,620 --> 00:04:52,900 
aggregated daily changes may better reveal
any upward or downward trend of the stock. 
78 
00:04:54,280 --> 00:04:58,330 
In summary, feature transformation
involves mapping the set of values for 
79 
00:04:58,330 --> 00:05:02,600 
a feature to a new set of values to
make the representation of the data 
80 
00:05:02,600 --> 00:05:06,180 
more suitable for the downstream analysis. 
81 
00:05:06,180 --> 00:05:10,090 
Feature transformation should be used with
cautions since they change the nature of 
82 
00:05:10,090 --> 00:05:16,030 
the data and unintentionally remove some
important characteristics of the data. 
83 
00:05:16,030 --> 00:05:19,757 
So it's important to look at the effects
of the transformation you're applying 
84 
00:05:19,757 --> 00:05:22,897 
to your data to make certain that
it has the intended consequences. 
1 
00:00:00,000 --> 00:00:04,871 
In this activity we will be using
KNIME to handle missing data. 
2 
00:00:04,871 --> 00:00:09,045 
First, we will create new workflow and
import our weather data. 
3 
00:00:09,045 --> 00:00:15,571 
Next, we will remove missing values for
specific measurement in the data. 
4 
00:00:15,571 --> 00:00:18,813 
We will then impute missing
values with the mean. 
5 
00:00:18,813 --> 00:00:21,554 
And finally,
remove all the rows with missing values. 
6 
00:00:24,655 --> 00:00:28,145 
Let's begin, first,
let's create a new workflow in KNIME. 
7 
00:00:34,723 --> 00:00:36,558 
We'll name it, Handling Missing Values. 
8 
00:00:42,252 --> 00:00:47,453 
Next, let's import the weather
data using the File Reader node. 
9 
00:00:47,453 --> 00:00:49,421 
We'll add the File Reader
node to the canvas. 
10 
00:00:58,553 --> 00:01:02,060 
We'll configure File Reader
to read daily_weather.csv. 
11 
00:01:08,409 --> 00:01:13,280 
Next, we'll connect the Histogram
node to File Reader. 
12 
00:01:20,553 --> 00:01:24,909 
Before we configure the Histogram node,
we need to run the File Reader node. 
13 
00:01:24,909 --> 00:01:28,331 
We can do this by clicking on File Reader,
and 
14 
00:01:28,331 --> 00:01:31,677 
clicking on the green
arrow in the toolbar. 
15 
00:01:35,224 --> 00:01:38,259 
Next, double-click on
Histogram to configure it. 
16 
00:01:38,259 --> 00:01:43,561 
We'll set both the binning column and
aggregation columns to air_temp_9am. 
17 
00:01:52,003 --> 00:01:54,259 
Next, we'll add a missing value node. 
18 
00:02:02,931 --> 00:02:06,257 
We'll connect this to
the file reader node. 
19 
00:02:06,257 --> 00:02:09,992 
Double click on missing
value to configuring it. 
20 
00:02:09,992 --> 00:02:13,945 
Go to column settings, 
21 
00:02:13,945 --> 00:02:21,466 
select air_temp_9am, and click on Add. 
22 
00:02:21,466 --> 00:02:23,584 
We'll then choose remove row. 
23 
00:02:26,982 --> 00:02:32,298 
This will remove the measurements of
air_temp_9am that had missing values. 
24 
00:02:32,298 --> 00:02:32,948 
Click OK. 
25 
00:02:35,838 --> 00:02:41,106 
Next, we'll add another histogram
node to the output of missing value. 
26 
00:02:41,106 --> 00:02:45,162 
We can do this easily by copying and
pasting the existing histogram node. 
27 
00:02:54,404 --> 00:02:57,144 
Now let's run the workflow by
clicking on the double green arrows. 
28 
00:03:00,966 --> 00:03:04,491 
Let's view the output in
the histogram nodes before and 
29 
00:03:04,491 --> 00:03:06,592 
after we remove missing values. 
30 
00:03:06,592 --> 00:03:10,158 
In this histogram view,
we can see that there are missing values. 
31 
00:03:10,158 --> 00:03:12,985 
If we go and
look at the second histogram node, 
32 
00:03:12,985 --> 00:03:15,752 
the one after we removed
the missing values. 
33 
00:03:18,766 --> 00:03:20,958 
We can see that there are no
missing values in this chart. 
34 
00:03:23,678 --> 00:03:28,748 
Next, instead of removing missing values,
let's impute the values to the mean. 
35 
00:03:28,748 --> 00:03:32,042 
We could do this by configuring
the missing value node. 
36 
00:03:34,865 --> 00:03:39,454 
And instead of remove row,
change that to mean. 
37 
00:03:39,454 --> 00:03:47,050 
Click OK, and rerun the work flow. 
38 
00:03:47,050 --> 00:03:50,906 
We could see the difference by comparing
the graphs and the two histogram nodes. 
39 
00:03:56,182 --> 00:03:57,700 
If we go to visualization settings. 
40 
00:04:00,403 --> 00:04:04,412 
And change labels to all elements, we'll
see the number of elements in each band. 
41 
00:04:11,379 --> 00:04:14,929 
In the histogram,
before we removed the missing values, 
42 
00:04:14,929 --> 00:04:18,486 
we can see the fifth column
has 216 measurements in it. 
43 
00:04:18,486 --> 00:04:25,952 
Where as the fifth column after we imputed
the missing values has 221 values in it. 
44 
00:04:25,952 --> 00:04:26,680 
Let's close these. 
45 
00:04:31,608 --> 00:04:36,756 
Next, let's remove all the rows that
have missing values in the data. 
46 
00:04:36,756 --> 00:04:39,073 
We can do this by double
clicking on missing value. 
47 
00:04:42,578 --> 00:04:44,770 
Removing air_temp_ 9am. 
48 
00:04:46,918 --> 00:04:48,116 
Clicking on the default tab. 
49 
00:04:50,728 --> 00:04:53,892 
Changing this to remove row. 
50 
00:04:53,892 --> 00:04:59,224 
I'll click OK, now we rerun the workflow. 
51 
00:04:59,224 --> 00:05:03,470 
Again, we'll look at the histograms before
and after we remove the missing values. 
52 
00:05:15,128 --> 00:05:20,170 
Again, we'll go to visualization settings
and change the labels to all elements so 
53 
00:05:20,170 --> 00:05:22,878 
we can see the number of
elements in each bin. 
54 
00:05:27,213 --> 00:05:30,490 
We can see the number of elements in
each bin in the different histograms 
55 
00:05:30,490 --> 00:05:31,149 
has changed. 
1 
00:00:00,910 --> 00:00:05,280 
In this activity we will see how
to handle missing values in Spark. 
2 
00:00:05,280 --> 00:00:08,320 
First, we will load weather
data into a Spark DataFrame. 
3 
00:00:09,560 --> 00:00:14,092 
We'll then examine the summary
statistics for air temperature, remove 
4 
00:00:14,092 --> 00:00:19,072 
the rows with missing values, and finally
impute missing values with the mean. 
5 
00:00:22,037 --> 00:00:23,002 
Let's begin. 
6 
00:00:23,002 --> 00:00:27,900 
First, we'll open the notebook
called handling missing values. 
7 
00:00:28,920 --> 00:00:31,910 
The first cell creates an SQLContext and 
8 
00:00:31,910 --> 00:00:34,720 
then loads the weather data
csv into a data frame. 
9 
00:00:36,170 --> 00:00:41,060 
Let's execute this,
we can view the summary statistics for 
10 
00:00:41,060 --> 00:00:47,570 
the DataFrame by running
df.describe().show(). 
11 
00:00:47,570 --> 00:00:50,238 
Let's just look at the summary
statistics for the air temperature. 
12 
00:00:50,238 --> 00:00:51,913 
We'll run 
13 
00:00:51,913 --> 00:01:00,852 
df.describe(['air_score_9am']) .show. 
14 
00:01:03,730 --> 00:01:06,590 
This says that there are 1090 rows. 
15 
00:01:07,790 --> 00:01:11,490 
This does not include the rows of
missing values for the air temperature. 
16 
00:01:12,630 --> 00:01:17,381 
We can count the total number of rows
in the DataFrame by running df.count.() 
17 
00:01:19,027 --> 00:01:23,026 
Since there are 1095 total
rows in the DataFrame, but 
18 
00:01:23,026 --> 00:01:25,801 
only 1090 in the air_temp column, 
19 
00:01:25,801 --> 00:01:30,630 
that means there are five rows in
air_temp that have missing values. 
20 
00:01:32,240 --> 00:01:36,029 
Next, let's remove all the rows in
the DataFrame that have missing values. 
21 
00:01:37,060 --> 00:01:40,220 
We'll put these in a new data
frame called removeAllDF. 
22 
00:01:41,700 --> 00:01:46,476 
To drop the missing values
we'll run df.na.drop. 
23 
00:01:48,971 --> 00:01:50,984 
Let's look at the summary statistics for 
24 
00:01:50,984 --> 00:01:54,160 
air_temp 9AM with
the missing values dropped. 
25 
00:01:54,160 --> 00:02:00,384 
We'll run removeAllDF .describe([
air_temp_9am]) show.() 
26 
00:02:03,051 --> 00:02:06,146 
We can see that the mean and
standard deviation values 
27 
00:02:06,146 --> 00:02:10,840 
are close to the original values before
we removed the rows with missing values. 
28 
00:02:12,650 --> 00:02:16,784 
Additionally, the count of
the number of rows is 1064. 
29 
00:02:18,380 --> 00:02:22,768 
We can verify that this is the total
number of rows in the new DataFrame by 
30 
00:02:22,768 --> 00:02:24,758 
running removeAllDF.count. 
31 
00:02:29,389 --> 00:02:32,075 
Next, let's impute the missing
values with the mean, 
32 
00:02:32,075 --> 00:02:34,314 
instead of removing them
from the DataFrame. 
33 
00:02:35,920 --> 00:02:39,712 
First, we'll need to load
the average function from pyspark. 
34 
00:02:41,060 --> 00:02:50,030 
We'll do this by running from
pyspark.sql.functions import avg. 
35 
00:02:50,030 --> 00:02:54,689 
Next, we'll create a copy of the DataFrame
in which we will input the missing values. 
36 
00:02:55,910 --> 00:02:58,590 
We'll call the new DataFrame imputeDF. 
37 
00:03:02,000 --> 00:03:06,349 
To impute the missing values we'll
iterate through each column of 
38 
00:03:06,349 --> 00:03:11,317 
the original DataFrame, first computing
the mean value for that column and 
39 
00:03:11,317 --> 00:03:15,913 
then replacing the missing values
in that column with the mean value. 
40 
00:03:15,913 --> 00:03:20,145 
To move through the columns
in the data frame, 
41 
00:03:20,145 --> 00:03:24,704 
we'll enter for
x in imputeDF.columns: next, 
42 
00:03:24,704 --> 00:03:29,061 
we'll compute the mean value for
that column. 
43 
00:03:32,105 --> 00:03:36,296 
We'll use the data frame in which
we removed all the missing values, 
44 
00:03:36,296 --> 00:03:39,630 
we'll call the agg function
to compute an aggregate. 
45 
00:03:39,630 --> 00:03:42,900 
And the argument that we give it is avg. 
46 
00:03:42,900 --> 00:03:48,830 
The argument avg is x, which is the column
we are trying to compute the average of. 
47 
00:03:50,730 --> 00:03:54,010 
The agg function returns to DataFrame and 
48 
00:03:54,010 --> 00:03:57,220 
we want to get the first
row of that data frame. 
49 
00:03:57,220 --> 00:04:02,280 
We can do this by calling .first and
then you get the first value in 
50 
00:04:02,280 --> 00:04:07,420 
this row or say [0]. 
51 
00:04:07,420 --> 00:04:10,410 
Next let's print the column
name in mean value. 
52 
00:04:11,890 --> 00:04:16,561 
print(x, meanValue) 
53 
00:04:19,522 --> 00:04:23,202 
Now let's update our new DataFrame, 
54 
00:04:23,202 --> 00:04:28,156 
replacing the missing
values with the mean value. 
55 
00:04:28,156 --> 00:04:37,242 
imputeDF = imputeDF.na.fill(meanValue,
[x]). 
56 
00:04:38,460 --> 00:04:39,135 
Let's run this. 
57 
00:04:44,273 --> 00:04:46,344 
We can see that the mean value for 
58 
00:04:46,344 --> 00:04:51,260 
air_temp 9am matches the mean value
computed in the summary statistics of 
59 
00:04:51,260 --> 00:04:54,820 
the data frame where the missing
values were removed. 
60 
00:04:56,710 --> 00:05:00,390 
Finally, let's print the imputed
data summary statistics. 
61 
00:05:00,390 --> 00:05:04,450 
First we'll show the summary
statistics for the original DataFrame. 
62 
00:05:04,450 --> 00:05:07,318 
And then the summary statistics for
the imputed DataFrame. 
63 
00:05:07,318 --> 00:05:14,813 
We'll enter df.describe([
air_temp_ 9am]) .show () and 
64 
00:05:14,813 --> 00:05:22,062 
imputeDF.describe(['air_temp_9am']).sho-
w(). 
65 
00:05:22,062 --> 00:05:24,997 
Run this. 
66 
00:05:24,997 --> 00:05:29,869 
We can see that the number of rows
in the imputed DataFrame is larger 
67 
00:05:29,869 --> 00:05:33,612 
than the number of rows in
the original DataFrame. 
68 
00:05:34,875 --> 00:05:38,500 
There were five rows in the original
DataFrame with missing values. 
69 
00:05:38,500 --> 00:05:40,210 
And these have now been
replaced with the mean. 
1 
00:00:00,760 --> 00:00:04,340 
Let's talk about what it means to
build a classification model and 
2 
00:00:04,340 --> 00:00:06,990 
how building a model differs
from applying a model. 
3 
00:00:08,080 --> 00:00:09,290 
After this video, 
4 
00:00:09,290 --> 00:00:14,170 
you will be able to discuss what
building a classification model means. 
5 
00:00:14,170 --> 00:00:17,511 
Explain the difference between
building and applying a model. 
6 
00:00:17,511 --> 00:00:20,910 
And summarize why the parameters
of a model needs to be adjusted. 
7 
00:00:22,170 --> 00:00:25,150 
A machine learning model
is a mathematical model. 
8 
00:00:25,150 --> 00:00:29,560 
In the general sense, this means that
the model has parameters and uses 
9 
00:00:29,560 --> 00:00:33,630 
equations to determine the relationship
between its inputs and outputs. 
10 
00:00:34,640 --> 00:00:39,730 
The parameters are used by the model to
modify the inputs to generate the outputs. 
11 
00:00:39,730 --> 00:00:42,930 
The model adjusts its parameters
in order to correct or 
12 
00:00:42,930 --> 00:00:45,770 
refine this input, output relationship. 
13 
00:00:47,030 --> 00:00:49,630 
Here's an example of a simple model. 
14 
00:00:49,630 --> 00:00:52,150 
This mathematical model represents a line. 
15 
00:00:52,150 --> 00:00:58,554 
y is the output, x is the input,
m determines the slope of the line and 
16 
00:00:58,554 --> 00:01:05,080 
b determines the y-intercept or
where the line crosses the y-axis. 
17 
00:01:05,080 --> 00:01:07,720 
m and b are the model's parameters. 
18 
00:01:07,720 --> 00:01:09,722 
Given a specific value for 
19 
00:01:09,722 --> 00:01:14,368 
x, the model uses as parameters
along with x to determine y. 
20 
00:01:14,368 --> 00:01:19,168 
By adjusting the values for
the parameters m and b, 
21 
00:01:19,168 --> 00:01:24,760 
the model can adjust how the input
x matched to the output y. 
22 
00:01:26,010 --> 00:01:29,262 
Here we see how the output y changes for 
23 
00:01:29,262 --> 00:01:33,844 
the same value of input x,
when parameter b changes. 
24 
00:01:33,844 --> 00:01:39,480 
Recall that b is the y-intercept, or
where the line crosses the y-axis. 
25 
00:01:40,520 --> 00:01:48,048 
The value of b is +1 for
the red line and -1 for the blue line. 
26 
00:01:48,048 --> 00:01:51,996 
For the input x=1, the value of y is 3 for 
27 
00:01:51,996 --> 00:01:56,175 
the red line,
as indicated by the red arrow. 
28 
00:01:56,175 --> 00:02:02,024 
For the blue line, when the parameter
b changes from +1 to -1, 
29 
00:02:02,024 --> 00:02:07,670 
for x=1, the value of y is 1,
as indicated by the blue arrow. 
30 
00:02:08,700 --> 00:02:12,710 
So we see that with just a simple
change in one model parameter, 
31 
00:02:12,710 --> 00:02:15,074 
the input to output mapping changes. 
32 
00:02:16,585 --> 00:02:19,495 
A machine learning model
works in a similar way. 
33 
00:02:19,495 --> 00:02:22,285 
It maps input values to output values. 
34 
00:02:22,285 --> 00:02:25,640 
And it adjusts the parameters
in order to correct or 
35 
00:02:25,640 --> 00:02:28,145 
refine this input-output mapping. 
36 
00:02:29,215 --> 00:02:32,665 
The parameters of a machine
learning model are adjusted or 
37 
00:02:32,665 --> 00:02:36,185 
estimated from the data
using a learning algorithm. 
38 
00:02:37,300 --> 00:02:41,240 
This, in essence,
is what is involved in building a model. 
39 
00:02:41,240 --> 00:02:46,869 
This process is referred to by many terms,
such as model building, 
40 
00:02:46,869 --> 00:02:51,326 
model creation,
model training and model fitting. 
41 
00:02:51,326 --> 00:02:52,803 
In building a model, 
42 
00:02:52,803 --> 00:02:57,736 
we want to adjust the parameters in
order to reduce the model's error. 
43 
00:02:57,736 --> 00:03:02,424 
In the case of supervised tasks,
such as classification, this means getting 
44 
00:03:02,424 --> 00:03:07,340 
the model's outputs to match the targets
or desired outputs as much as possible. 
45 
00:03:08,580 --> 00:03:12,600 
Since the classification task is
to predict the correct category or 
46 
00:03:12,600 --> 00:03:15,650 
class, given the input variables, 
47 
00:03:15,650 --> 00:03:20,150 
you can think of the classification
problem visually as carving out the input 
48 
00:03:20,150 --> 00:03:24,910 
space as regions corresponding
to the different class labels. 
49 
00:03:24,910 --> 00:03:29,160 
In this diagram for example,
the classification model needs to form 
50 
00:03:29,160 --> 00:03:34,350 
the boundaries to define the regions
separating red triangles 
51 
00:03:34,350 --> 00:03:38,580 
from blue diamonds, from green circles,
from yellow squares. 
52 
00:03:39,670 --> 00:03:45,180 
In this example, if a sample falls within
the region in the upper right corner, 
53 
00:03:45,180 --> 00:03:47,160 
it will be classified as a blue diamond. 
54 
00:03:48,280 --> 00:03:53,215 
Classification decisions are based on
these regions, and the regions are defined 
55 
00:03:53,215 --> 00:03:57,316 
by the boundaries, as indicated by
the dashed lines in the diagram. 
56 
00:03:57,316 --> 00:04:00,349 
So these boundaries are referred
to as decision boundaries. 
57 
00:04:01,520 --> 00:04:06,720 
Building a classification then means using
the data to adjust the model's parameters 
58 
00:04:06,720 --> 00:04:10,680 
in order to form decision boundaries
to separate the target classes. 
59 
00:04:11,915 --> 00:04:15,770 
Note that the term classifier is often
used to mean classification model. 
60 
00:04:17,480 --> 00:04:20,650 
In general,
building a classification model, 
61 
00:04:20,650 --> 00:04:24,190 
as well as other machine learning models,
involves two phases. 
62 
00:04:25,870 --> 00:04:30,515 
The first is the training phase,
in which the model is constructed and 
63 
00:04:30,515 --> 00:04:34,190 
its parameters adjusted using as
what referred to as training data. 
64 
00:04:35,350 --> 00:04:39,020 
Training data is the data set
used to train or create a model. 
65 
00:04:40,500 --> 00:04:42,750 
The second is the testing phase. 
66 
00:04:42,750 --> 00:04:45,984 
This is where the learned
model is applied to new data. 
67 
00:04:45,984 --> 00:04:48,250 
That is,
data not used in training the model. 
68 
00:04:49,790 --> 00:04:51,570 
Here's another way to
look at the two phases. 
69 
00:04:53,010 --> 00:04:57,130 
In a training phase, the learning
algorithm uses the training data 
70 
00:04:57,130 --> 00:05:00,070 
to adjust the model's
parameters to minimize errors. 
71 
00:05:01,090 --> 00:05:03,700 
At the end of the training phase,
you get the trained model. 
72 
00:05:05,370 --> 00:05:10,270 
In the testing phase,
the trained model is applied to test data. 
73 
00:05:10,270 --> 00:05:16,220 
Test data is separate from training data
and is previously unseen by the model. 
74 
00:05:16,220 --> 00:05:19,810 
The model is then evaluated on
how it performs on the test data. 
75 
00:05:20,930 --> 00:05:24,607 
The goal in building a classifier
model is to have the model 
76 
00:05:24,607 --> 00:05:27,696 
perform well on training,
as well as test data. 
77 
00:05:27,696 --> 00:05:30,611 
We will discuss in more detail
the use of training and 
78 
00:05:30,611 --> 00:05:35,470 
test data sets in the next module,
when we discuss model evaluation. 
79 
00:05:35,470 --> 00:05:40,330 
To adjust a model's parameters,
we need to apply a learning algorithm. 
80 
00:05:40,330 --> 00:05:44,460 
We will discuss the specific algorithms
to build a classification model in 
81 
00:05:44,460 --> 00:05:45,510 
the next few lectures. 
1 
00:00:01,070 --> 00:00:04,650 
In this video, we will outline
some commonly used algorithms for 
2 
00:00:04,650 --> 00:00:07,260 
building a classification model. 
3 
00:00:07,260 --> 00:00:11,540 
After this video,
you will be able to describe the goal of 
4 
00:00:11,540 --> 00:00:16,260 
a classification algorithm and name some
common algorithms for classification. 
5 
00:00:17,630 --> 00:00:22,140 
Recall that a classification task is
to predict the category from the input 
6 
00:00:22,140 --> 00:00:23,580 
variables. 
7 
00:00:23,580 --> 00:00:29,570 
A classification model processes the input
data it receives and provides an output. 
8 
00:00:29,570 --> 00:00:33,860 
Since classification is a supervised task,
a target or 
9 
00:00:33,860 --> 00:00:37,710 
desired output is provided for
each sample. 
10 
00:00:37,710 --> 00:00:42,640 
The goal is to get the model outputs to
match the targets as much as possible. 
11 
00:00:44,040 --> 00:00:47,350 
A classification model
adjusts its parameters 
12 
00:00:47,350 --> 00:00:50,610 
to get its outputs to match the targets. 
13 
00:00:50,610 --> 00:00:54,500 
To adjust a model's parameters,
a learning algorithm is applied. 
14 
00:00:55,580 --> 00:00:59,080 
This occurs in a training phase
when the model is constructed. 
15 
00:01:00,570 --> 00:01:03,950 
There are many algorithms to
build a classification model. 
16 
00:01:03,950 --> 00:01:08,514 
In this course,
we will cover the algorithms listed here, 
17 
00:01:08,514 --> 00:01:13,371 
kNN or k Nearest Neighbors,
decision tree, and naive Bayes. 
18 
00:01:13,371 --> 00:01:17,870 
kNN stands for k Nearest Neighbors. 
19 
00:01:17,870 --> 00:01:22,490 
This technique relies on the notion that
samples with similar characteristics, 
20 
00:01:22,490 --> 00:01:28,440 
that is samples with similar values for
input, likely belong to the same class. 
21 
00:01:28,440 --> 00:01:31,150 
So classification of a sample is dependent 
22 
00:01:31,150 --> 00:01:33,650 
on the target values of
the neighboring points. 
23 
00:01:35,410 --> 00:01:39,560 
Another classification technique
is referred to as decision tree. 
24 
00:01:39,560 --> 00:01:44,270 
A decision tree is a classification
model that uses a treelike structure 
25 
00:01:44,270 --> 00:01:47,398 
to represent multiple decision paths. 
26 
00:01:47,398 --> 00:01:52,010 
Traversing each path leads to a different
way to classify an input sample. 
27 
00:01:53,450 --> 00:01:57,770 
A naive Bayes model uses a probabilistic
approach to classification. 
28 
00:01:58,870 --> 00:02:02,920 
Baye's Theorem is used to capture the
relationship between the input data and 
29 
00:02:02,920 --> 00:02:03,830 
the output class. 
30 
00:02:04,880 --> 00:02:09,130 
Simply put, the Baye's Theorem
compares the probability of an event 
31 
00:02:09,130 --> 00:02:11,660 
in the presence of another event. 
32 
00:02:11,660 --> 00:02:16,150 
We see here the probability
of A if B is present. 
33 
00:02:16,150 --> 00:02:20,770 
For example, probability of having
a fire if the weather is hot. 
34 
00:02:20,770 --> 00:02:24,100 
You can imagine event B depending
on more than one variable. 
35 
00:02:24,100 --> 00:02:26,416 
For example, weather is hot and windy. 
36 
00:02:26,416 --> 00:02:34,060 
We will cover kNN, decision tree and naive
Bayes in detail in the next few lectures. 
37 
00:02:34,060 --> 00:02:38,150 
There are many other classification
techniques, but we will focus on these 
38 
00:02:38,150 --> 00:02:41,970 
since they are fundamental algorithms
that are commonly used and 
39 
00:02:41,970 --> 00:02:44,940 
form the basis of other algorithms for
classification. 
1 
00:00:01,720 --> 00:00:05,366 
In this activity we will perform
classification in Spark is in 
2 
00:00:05,366 --> 00:00:06,355 
decision tree. 
3 
00:00:06,355 --> 00:00:10,889 
First, we will load weather
data into DataFrame and 
4 
00:00:10,889 --> 00:00:13,578 
drop unused and missing data. 
5 
00:00:13,578 --> 00:00:16,686 
We'll then,
create a categorical variable for 
6 
00:00:16,686 --> 00:00:21,100 
low humidity days and aggregate
features used to make predictions. 
7 
00:00:21,100 --> 00:00:25,110 
We will then split our data
into training and test sets. 
8 
00:00:26,400 --> 00:00:28,330 
And then create and
train the decision tree. 
9 
00:00:29,730 --> 00:00:34,414 
Finally, we will save our
predictions to a CSV file. 
10 
00:00:34,414 --> 00:00:38,635 
Let's begin, first, we'll open
the notebook called classification. 
11 
00:00:38,635 --> 00:00:46,344 
The first cell contains the classes
we need to load to run this exercise. 
12 
00:00:46,344 --> 00:00:47,122 
Let's run this. 
13 
00:00:50,152 --> 00:00:55,540 
Next we create SQL context and load
the weather data CSV into a data frame. 
14 
00:00:55,540 --> 00:00:58,570 
The second cell also prints all
the columns in this data frame. 
15 
00:00:59,770 --> 00:01:00,270 
Run this. 
16 
00:01:05,731 --> 00:01:08,923 
The third cell defines the columns
in the weather data we will use for 
17 
00:01:08,923 --> 00:01:10,440 
the decision tree classifier. 
18 
00:01:11,460 --> 00:01:11,960 
Let's run it. 
19 
00:01:14,570 --> 00:01:16,710 
We will now use the column name number. 
20 
00:01:16,710 --> 00:01:23,030 
So, let's drop that from the data frame,
df = df.drop Number. 
21 
00:01:24,960 --> 00:01:31,422 
Now, let's revolve the rows with
missing data, df = df.na.drop. 
22 
00:01:33,486 --> 00:01:40,129 
Now, let's print the number of rows and
columns in our resulting data frame, 
23 
00:01:40,129 --> 00:01:43,664 
df.count(), len(df.columns). 
24 
00:01:47,994 --> 00:01:52,800 
Next, let's create a categorical variable
to denote if the humidity is low. 
25 
00:01:54,070 --> 00:01:59,793 
We'll enter binarizer = Binarizer (). 
26 
00:01:59,793 --> 00:02:03,663 
The first argument specifies
a threshold value for the variable. 
27 
00:02:03,663 --> 00:02:09,598 
We want the categorical variable to be 1,
if the humidity is greater than 25%. 
28 
00:02:09,598 --> 00:02:15,555 
So we'll enter a threshold=24.9999. 
29 
00:02:15,555 --> 00:02:20,820 
The next argument specifies the column to
use to create the categorical variable. 
30 
00:02:20,820 --> 00:02:26,409 
We'll input,
inputCol = relative_humidity_3pm. 
31 
00:02:26,409 --> 00:02:33,610 
The final argument specifies the new
column name, outputCol = label. 
32 
00:02:33,610 --> 00:02:36,206 
Now, let's create a new data frame
with this categorical variable. 
33 
00:02:36,206 --> 00:02:42,123 
binarizeredDF = binarizer.transform df. 
34 
00:02:42,123 --> 00:02:43,702 
Let's run this. 
35 
00:02:45,787 --> 00:02:48,774 
Let's look at the first four
rows in this new data frame. 
36 
00:02:48,774 --> 00:02:54,519 
We'll run binarizedDF.select
('relative_humidity_3pm', 
37 
00:02:54,519 --> 00:02:56,918 
'label').show(4). 
38 
00:03:00,238 --> 00:03:06,630 
The relative humidity in the first row
is greater than 25% and the label is 1. 
39 
00:03:06,630 --> 00:03:10,720 
The relative humidity in the second,
third, and fourth rows are less than 25%, 
40 
00:03:10,720 --> 00:03:14,060 
and the label is 0. 
41 
00:03:14,060 --> 00:03:20,014 
Next, let's aggregate the features we will
use to make predictions into a single col, 
42 
00:03:20,014 --> 00:03:22,517 
assembler = VectorAssember( ). 
43 
00:03:22,517 --> 00:03:26,248 
The first argument is a list of
the columns to be aggregated. 
44 
00:03:26,248 --> 00:03:31,946 
inputCols=featureColumns, and
the second argument is the name 
45 
00:03:31,946 --> 00:03:38,876 
of the new column containing the
aggregated features, outputCol=features. 
46 
00:03:38,876 --> 00:03:42,836 
We can create the new
data frame by running 
47 
00:03:42,836 --> 00:03:47,829 
assembled=assembler.transform binarizedDF. 
48 
00:03:47,829 --> 00:03:49,141 
Let's run this. 
49 
00:03:51,707 --> 00:03:55,410 
Next we will split our data
set into two parts, one for 
50 
00:03:55,410 --> 00:03:57,972 
training data and one for test data. 
51 
00:03:57,972 --> 00:04:03,198 
You can do this by entering (training 
52 
00:04:03,198 --> 00:04:11,693 
Data,
testData)=assembled.randomSplit([0.8, 
53 
00:04:11,693 --> 00:04:16,602 
0.2], seed=13234). 
54 
00:04:16,602 --> 00:04:21,049 
We can see the size of the two
sets by running count, 
55 
00:04:21,049 --> 00:04:25,818 
trainingData.count(), testData.count (). 
56 
00:04:28,419 --> 00:04:30,996 
Next let's create and
train the decision tree. 
57 
00:04:30,996 --> 00:04:35,490 
We'll enter dt = DecisionTreeClassifier. 
58 
00:04:36,510 --> 00:04:41,763 
The first argument is the column we're
trying to predict, labelCol='label'. 
59 
00:04:41,763 --> 00:04:46,036 
The second argument is the name
of the column containing your 
60 
00:04:46,036 --> 00:04:49,989 
aggregated features,
featuresCol='features'. 
61 
00:04:51,310 --> 00:04:53,880 
The third argument is
the stopping criteria for 
62 
00:04:53,880 --> 00:04:58,700 
tree induction based on the maximum
depth of the tree, maxDepth=5. 
63 
00:04:58,700 --> 00:05:04,135 
The fourth argument is the stopping
criteria for tree induction based 
64 
00:05:04,135 --> 00:05:09,954 
on the minimum number of samples
in a node, minInstancesPerNode=20. 
65 
00:05:09,954 --> 00:05:14,942 
And finally, the last argument
specifies the impurity measure 
66 
00:05:14,942 --> 00:05:20,127 
used to split the nodes,
impurity="gini", let's run this. 
67 
00:05:22,507 --> 00:05:25,665 
Next, we can create a model by
training the decision tree. 
68 
00:05:25,665 --> 00:05:28,463 
We'll do this by executing
it in a pipeline. 
69 
00:05:28,463 --> 00:05:36,020 
We'll enter pipeline=Pipeline
(stages=[dt]). 
70 
00:05:36,020 --> 00:05:40,719 
We'll create them all by putting
a training data, model = 
71 
00:05:40,719 --> 00:05:45,740 
pipeline.fit trainingData. 
72 
00:05:45,740 --> 00:05:52,695 
Let's run this Now, we can make
predictions using our test data. 
73 
00:05:52,695 --> 00:05:58,520 
We'll enter predictions =
model.transform(testData). 
74 
00:05:58,520 --> 00:06:02,824 
You can look at the first 10 rows
of the prediction by running, 
75 
00:06:02,824 --> 00:06:07,619 
predictions.select('prediction', label')
show(10). 
76 
00:06:10,232 --> 00:06:13,611 
You can see in the first ten rows,
the prediction matches the label. 
77 
00:06:13,611 --> 00:06:16,595 
Now let's save our
predictions to a CSV file. 
78 
00:06:16,595 --> 00:06:20,190 
In the next Spark hands-on activity
we will evaluate the accuracy. 
79 
00:06:20,190 --> 00:06:25,861 
You can save it by running
predictions.select('prediction', 
80 
00:06:25,861 --> 00:06:30,753 
'label').write.save(path='file File:/// 
81 
00:06:30,753 --> 00:06:36,631 
home/cloudera/downloads/big-data-4/predic-
tions.csv. 
82 
00:06:36,631 --> 00:06:40,365 
We'll specify the format to use Spark csv. 
83 
00:06:40,365 --> 00:06:46,101 
Format='com.databricks.spark.csv. 
84 
00:06:46,101 --> 00:06:49,735 
Finally, we'll enter header='true'. 
85 
00:06:49,735 --> 00:06:52,823 
Run this to save
the predictions to a .csv file. 
1 
00:00:01,640 --> 00:00:04,850 
In this activity, we will be
performing classification in KNIME, 
2 
00:00:05,900 --> 00:00:09,830 
first we will create a new workflow and
import our weather data. 
3 
00:00:09,830 --> 00:00:14,990 
Next, we will remove the missing data and
then create a categorical value for 
4 
00:00:14,990 --> 00:00:16,260 
the humidity measurements. 
5 
00:00:17,540 --> 00:00:21,490 
We will then examine summary
statistics of the data before and 
6 
00:00:21,490 --> 00:00:26,710 
after the missing data was removed, and
finally build a decision tree workflow. 
7 
00:00:30,203 --> 00:00:31,570 
Let's begin. 
8 
00:00:31,570 --> 00:00:33,450 
First, let's create a New Workflow. 
9 
00:00:37,330 --> 00:00:38,990 
We'll call it Classification. 
10 
00:00:41,650 --> 00:00:46,010 
Next, we'll import the daily weather data,
using the File Reader node. 
11 
00:00:49,495 --> 00:00:56,429 
Configure the file reader node,
to use daily_weather.csv. 
12 
00:00:56,429 --> 00:01:01,250 
Next, we'll add a missing value node, 
13 
00:01:01,250 --> 00:01:04,020 
to remove the missing values
in the daily weather data. 
14 
00:01:09,872 --> 00:01:13,480 
We'll configure missing value to
remove all the missing values. 
15 
00:01:21,112 --> 00:01:26,072 
Next we'll use a numeric binner node
to create a categorical variable for 
16 
00:01:26,072 --> 00:01:27,595 
the humidity at 3pm. 
17 
00:01:29,090 --> 00:01:32,530 
We'll add the numeric binner node,
we'll connect it to missing value. 
18 
00:01:32,530 --> 00:01:38,170 
We'll select the relative
humidity 3pm column. 
19 
00:01:39,320 --> 00:01:45,144 
We'll create two bins,
the first bin we'll call humidity_low. 
20 
00:01:49,696 --> 00:01:51,450 
And that value will go up to 25. 
21 
00:01:51,450 --> 00:01:56,885 
The second bin, we'll 
22 
00:01:56,885 --> 00:02:03,411 
call humidity_not_low. 
23 
00:02:03,411 --> 00:02:06,274 
We'll check the Append new column and 
24 
00:02:06,274 --> 00:02:10,045 
we'll call the new
column low_humidity_day. 
25 
00:02:19,286 --> 00:02:23,435 
Next, we'll examine some summary
statistics, both before, and 
26 
00:02:23,435 --> 00:02:26,940 
after we've removed the missing values. 
27 
00:02:26,940 --> 00:02:29,649 
We'll add the statistics
node to the canvas. 
28 
00:02:34,914 --> 00:02:38,130 
We'll connect the first one to
the output of the File Reader actor. 
29 
00:02:39,740 --> 00:02:42,590 
We'll add another one to
the output of Numeric Binner. 
30 
00:02:47,550 --> 00:02:51,540 
We'll change the name of the first
one to be BEFORE filtering. 
31 
00:02:55,630 --> 00:02:58,660 
And change the name of the second
one to AFTER filtering. 
32 
00:03:02,440 --> 00:03:03,650 
Lets configure the first one. 
33 
00:03:05,650 --> 00:03:10,114 
We'll change the maximum number of
possible values per column to 1500. 
34 
00:03:12,400 --> 00:03:14,090 
We'll also add all the columns. 
35 
00:03:18,130 --> 00:03:20,670 
Let's configure the second statistics now. 
36 
00:03:20,670 --> 00:03:24,210 
Again, we'll change the maximum number of
possible values per column to 1500, and 
37 
00:03:24,210 --> 00:03:25,950 
we'll add all the columns. 
38 
00:03:29,165 --> 00:03:30,360 
Now let's run our workflow. 
39 
00:03:34,983 --> 00:03:38,680 
Let's view both Statistic
nodes to compare the outputs. 
40 
00:03:42,971 --> 00:03:47,240 
In the BEFORE filtering statistics,
we can see that there are missing values. 
41 
00:03:49,770 --> 00:03:54,519 
However, there are no missing values
in the AFTER filtering statistics. 
42 
00:03:58,890 --> 00:04:02,050 
We can also compare the summary
statistics, for different measurements, 
43 
00:04:02,050 --> 00:04:04,450 
to see that they are similar values. 
44 
00:04:04,450 --> 00:04:07,810 
For example,
let's take a look at air_temp_9am. 
45 
00:04:18,505 --> 00:04:21,065 
We could see that most of
the statistics are the same. 
46 
00:04:22,435 --> 00:04:24,325 
As well as the distribution of values. 
47 
00:04:26,365 --> 00:04:30,210 
In the AFTER filtering statistics,
click on the Nominal tab, 
48 
00:04:30,210 --> 00:04:35,610 
and we'll look at the low humidity
day categorical variable we created. 
49 
00:04:43,759 --> 00:04:47,719 
We can see that the samples are equally
distributed between the two values. 
50 
00:04:51,140 --> 00:04:53,290 
Let's close these Statistics views. 
51 
00:04:54,740 --> 00:04:56,720 
Next, let's add a Column Filter node. 
52 
00:05:04,100 --> 00:05:07,441 
We'll connect this to
the output of Numeric Binner. 
53 
00:05:07,441 --> 00:05:09,390 
Double-click to configure the node. 
54 
00:05:09,390 --> 00:05:15,141 
And we will exclude
relative_humidity_9am and 
55 
00:05:15,141 --> 00:05:20,770 
relative_humidity_3pm columns, click OK. 
56 
00:05:20,770 --> 00:05:24,329 
Next, we'll add a Color Manager
node to the canvas. 
57 
00:05:28,940 --> 00:05:33,850 
Will connect this to the output of Column
Filter, double-click to configure it. 
58 
00:05:35,350 --> 00:05:42,450 
We will make sure the humidly_low is red,
and humidity_not_low is blue. 
59 
00:05:42,450 --> 00:05:44,317 
Click OK to close. 
60 
00:05:44,317 --> 00:05:49,710 
Next, we'll add the Partitioning
node to the campus. 
61 
00:05:54,380 --> 00:05:57,800 
We'll connect this to
the output of Color Manager. 
62 
00:05:57,800 --> 00:05:58,990 
Double-click to configure. 
63 
00:05:58,990 --> 00:06:03,060 
We want to split the data
into two partitions. 
64 
00:06:03,060 --> 00:06:05,920 
The first partition should
have 80% of the data. 
65 
00:06:05,920 --> 00:06:07,890 
The second partition should have 20%. 
66 
00:06:07,890 --> 00:06:12,560 
To do this click on Relative[%] and
change this to 80. 
67 
00:06:12,560 --> 00:06:16,870 
We'll make sure Draw randomly is selected. 
68 
00:06:19,030 --> 00:06:25,390 
And check Use random seed and
change the seed to 12345. 
69 
00:06:25,390 --> 00:06:28,700 
Normally we would not
specific the random seed. 
70 
00:06:28,700 --> 00:06:33,040 
However, since we want repeatable results,
we use a specific seed value here. 
71 
00:06:34,260 --> 00:06:35,520 
Click OK to continue. 
72 
00:06:36,830 --> 00:06:39,770 
Next, we'll add a Decision Tree Learner
to the work flow. 
73 
00:06:42,790 --> 00:06:45,950 
We'll connect this to the top
output of the Partitioning node. 
74 
00:06:47,248 --> 00:06:50,580 
Double-click to configure, and 
75 
00:06:50,580 --> 00:06:55,090 
change the Min number of
records per node to 20. 
76 
00:06:55,090 --> 00:06:56,480 
Click OK. 
77 
00:06:58,330 --> 00:07:03,410 
Next, we'll add a Decision Tree Predictor
node to the canvas, and we will connect 
78 
00:07:03,410 --> 00:07:08,310 
the bottom output of Partitioning to
the bottom input of the predictor. 
79 
00:07:09,660 --> 00:07:13,760 
Next we'll connect the output
of Decision Tree Learner 
80 
00:07:13,760 --> 00:07:16,420 
to the top input of
Decision Tree Predictor. 
81 
00:07:17,530 --> 00:07:19,190 
Now let's execute our workflow. 
82 
00:07:20,250 --> 00:07:23,000 
We can view the resulting
classification rules 
83 
00:07:23,000 --> 00:07:25,190 
by right clicking on
Decision Tree Predictor. 
84 
00:07:27,130 --> 00:07:32,040 
And choosing either View: Decision Tree
View or View: Decision Tree View (simple). 
85 
00:07:33,800 --> 00:07:34,610 
Let's do the first one. 
86 
00:07:35,650 --> 00:07:39,100 
This shows the first two
splits in our decision tree. 
87 
00:07:39,100 --> 00:07:41,300 
We can expand each branch
by clicking on the plus. 
88 
00:07:41,300 --> 00:07:43,343 
Let's close this. 
89 
00:07:43,343 --> 00:07:47,910 
Now let's look at the simplified view. 
90 
00:07:49,980 --> 00:07:53,160 
Again, we can expand the branches to
see the splits in the Decision Tree. 
91 
00:07:54,910 --> 00:07:59,250 
Close this,
finally let's save this workflow. 
92 
00:08:00,310 --> 00:08:05,150 
We'll analyze the results of the Decision
Tree model in the next KNIME Hands On. 
93 
00:08:05,150 --> 00:08:07,388 
Save the workflow by
clicking on the disk icon. 
1 
00:00:01,160 --> 00:00:05,410 
Welcome back, we already discussing
Classification models and techniques for 
2 
00:00:05,410 --> 00:00:07,030 
next few lectures. 
3 
00:00:07,030 --> 00:00:09,450 
Let's first define what
the classification task is, 
4 
00:00:10,670 --> 00:00:15,720 
after this video you will be able
to define what classification is. 
5 
00:00:15,720 --> 00:00:20,040 
Explain whether classification is
supervised or unsupervised and 
6 
00:00:20,040 --> 00:00:24,660 
describe how binomial classification
differs from multinomial classification. 
7 
00:00:26,250 --> 00:00:29,780 
Classification is one type of
machine learning problems. 
8 
00:00:29,780 --> 00:00:33,340 
In the classification problem, the input
data is presented to the machine learning 
9 
00:00:33,340 --> 00:00:39,620 
model and the task is to predict the
target corresponding to the input data. 
10 
00:00:39,620 --> 00:00:44,530 
The target is a categorical variable,
so the classification task is 
11 
00:00:44,530 --> 00:00:49,850 
to predict the category or
label of the target given the input data. 
12 
00:00:49,850 --> 00:00:53,870 
For example, the classification
problem illustrated in this image 
13 
00:00:53,870 --> 00:00:56,660 
is to predict the type of weather. 
14 
00:00:56,660 --> 00:01:01,540 
The target that the model has to predict
is the weather and the possible values for 
15 
00:01:01,540 --> 00:01:06,490 
weather in this case is Sunny,
Windy, Rainy, or Cloudy. 
16 
00:01:07,560 --> 00:01:12,127 
The input data can consist of measurements
like temperature, relative humidity, 
17 
00:01:12,127 --> 00:01:16,380 
atmospheric pressure,
wind speed, wind direction, etc. 
18 
00:01:17,570 --> 00:01:22,640 
So, given specific values for temperature,
relative humidity, atmospheric pressure, 
19 
00:01:22,640 --> 00:01:27,570 
etc., the task for the model is to
predict if the weather will be sunny. 
20 
00:01:27,570 --> 00:01:30,370 
Windy, rainy, or cloudy for the day, 
21 
00:01:32,230 --> 00:01:36,530 
this is what the data set might look like
for the weather classification problem. 
22 
00:01:36,530 --> 00:01:41,540 
Each row is a sample with input
variables temperature, humidity, and 
23 
00:01:41,540 --> 00:01:43,980 
pressure and target variable weather. 
24 
00:01:45,010 --> 00:01:48,580 
Each row has specific values for
the input variables and 
25 
00:01:48,580 --> 00:01:50,970 
a corresponding value for
the target variable. 
26 
00:01:52,000 --> 00:01:56,580 
The classification task is to predict
the value of the target variable 
27 
00:01:56,580 --> 00:01:58,389 
from the values of the input variables. 
28 
00:02:00,090 --> 00:02:03,950 
Since a target is provided,
we have labeled data and so 
29 
00:02:03,950 --> 00:02:07,470 
classification is a supervised task. 
30 
00:02:07,470 --> 00:02:10,680 
Recall that in a supervised task,
the target or 
31 
00:02:10,680 --> 00:02:14,170 
desired output for each sample is given. 
32 
00:02:14,170 --> 00:02:18,440 
Note that the target variable goes
by many names such as target, 
33 
00:02:18,440 --> 00:02:23,170 
label, output, class variable,
category, and class. 
34 
00:02:25,040 --> 00:02:30,050 
A classification problem can be binary or
multi-class with binary 
35 
00:02:30,050 --> 00:02:36,520 
classification the target variable has two
possible values, for example yes and no. 
36 
00:02:36,520 --> 00:02:40,500 
With multi-class classification
the target variable has more than 
37 
00:02:40,500 --> 00:02:42,320 
two possible values. 
38 
00:02:42,320 --> 00:02:46,425 
For example, the target can be short,
medium and tall. 
39 
00:02:46,425 --> 00:02:51,805 
Multi-class classification is
also referred to multinomial or 
40 
00:02:51,805 --> 00:02:53,200 
multi-label classification. 
41 
00:02:54,230 --> 00:02:58,600 
Remember though that the target is always
a categorical variable in classification. 
42 
00:03:00,270 --> 00:03:04,270 
Some examples of binary
classification are predicting 
43 
00:03:04,270 --> 00:03:09,380 
whether it will rain tomorrow or not,
here there are two possible outcomes, 
44 
00:03:09,380 --> 00:03:12,450 
yes it will rain tomorrow or
no, it will not rain tomorrow. 
45 
00:03:13,890 --> 00:03:18,360 
Identifying whether a credit card
transaction is legitimate or fraudulent, 
46 
00:03:18,360 --> 00:03:22,660 
again, there are only two possible values
for the target, legitimate or fraudulent. 
47 
00:03:23,910 --> 00:03:27,780 
Some examples of multi-class
classification include 
48 
00:03:27,780 --> 00:03:31,610 
predicting what type of product
that a customer will buy. 
49 
00:03:31,610 --> 00:03:34,540 
The possible values for
the target variables would be product 
50 
00:03:34,540 --> 00:03:38,960 
categories such as kitchen,
electronics, clothes, etc. 
51 
00:03:38,960 --> 00:03:42,580 
There is more than one
category of products, so 
52 
00:03:42,580 --> 00:03:45,210 
this is a multi-class
classification problem. 
53 
00:03:46,430 --> 00:03:51,530 
Another example is categorizing a tweet
as having a positive, negative, or 
54 
00:03:51,530 --> 00:03:55,090 
neutral sentiment, again,
the number of possible values for 
55 
00:03:55,090 --> 00:03:56,830 
the target is more than two here. 
56 
00:03:56,830 --> 00:04:01,250 
So, this is also a multi-class
classification task 
57 
00:04:01,250 --> 00:04:03,330 
to summarize in classification, 
58 
00:04:03,330 --> 00:04:07,770 
the model has to predict the category
corresponding to the input data. 
59 
00:04:07,770 --> 00:04:12,870 
Since the target is provided for each
sample, classification is a supervised 
60 
00:04:12,870 --> 00:04:17,519 
task, the target variable is always
categorical in classification. 
1 
00:00:01,120 --> 00:00:02,080 
In this lecture, 
2 
00:00:02,080 --> 00:00:07,490 
we will look at the decision tree model,
a popular method used for classification. 
3 
00:00:07,490 --> 00:00:12,500 
After this video, you will be able to
explain how a decision tree is used for 
4 
00:00:12,500 --> 00:00:14,120 
classification. 
5 
00:00:14,120 --> 00:00:18,650 
Describe the process of constructing
a decision tree for classification. 
6 
00:00:18,650 --> 00:00:22,520 
And interpret how a decision tree comes
up with a classification decision. 
7 
00:00:23,530 --> 00:00:27,520 
The idea behind decision trees for
classification is to split the data 
8 
00:00:27,520 --> 00:00:33,040 
into subsets where each subset
belongs to only one class. 
9 
00:00:33,040 --> 00:00:37,900 
This is accomplished by dividing
the input space into pure regions, 
10 
00:00:37,900 --> 00:00:41,210 
that is regions with samples
from only one class. 
11 
00:00:42,420 --> 00:00:46,740 
With real data completely pure
subsets may not be possible. 
12 
00:00:46,740 --> 00:00:52,006 
So the goal is to divide the data into
subsets that are as pure as possible. 
13 
00:00:52,006 --> 00:00:57,320 
That is each subset contains as many
samples as possible from a single class. 
14 
00:00:58,450 --> 00:01:03,050 
Graphically this is equivalent to dividing
the input space into regions that are as 
15 
00:01:03,050 --> 00:01:04,940 
pure as possible. 
16 
00:01:04,940 --> 00:01:09,760 
Boundaries separating these regions
are called decision boundaries. 
17 
00:01:09,760 --> 00:01:13,450 
And the decision tree model
makes classification decisions 
18 
00:01:13,450 --> 00:01:15,600 
based on these decision boundaries. 
19 
00:01:16,760 --> 00:01:23,040 
A decision tree is a hierarchical
structure with nodes and directed edges. 
20 
00:01:23,040 --> 00:01:25,560 
The node at the top is
called the root node. 
21 
00:01:26,600 --> 00:01:28,870 
The nodes at the bottom
are called the leaf nodes. 
22 
00:01:30,000 --> 00:01:35,810 
Nodes that are neither the root node or
the leaf nodes are called internal nodes. 
23 
00:01:35,810 --> 00:01:38,530 
The root and
internal nodes have test conditions, 
24 
00:01:39,580 --> 00:01:42,740 
each leaf node has a class
label associated with it. 
25 
00:01:43,770 --> 00:01:48,250 
A classification decision is made
by traversing the decision tree 
26 
00:01:48,250 --> 00:01:49,280 
starting with the root node. 
27 
00:01:50,620 --> 00:01:54,470 
At each node the answer to the test
condition determines which 
28 
00:01:54,470 --> 00:01:56,740 
branch to traverse to. 
29 
00:01:56,740 --> 00:02:00,330 
When a leaf node is reached
the category at the leaf node 
30 
00:02:00,330 --> 00:02:02,595 
determines the classification decision. 
31 
00:02:03,740 --> 00:02:09,590 
The depth of a node is the number of
edges from the root node to that node. 
32 
00:02:09,590 --> 00:02:11,130 
The depth of the root node is 0. 
33 
00:02:11,130 --> 00:02:15,620 
The depth of a decision
tree is the number of edges 
34 
00:02:15,620 --> 00:02:19,700 
in the longest path from
the root node to the leaf node. 
35 
00:02:19,700 --> 00:02:23,280 
The size of a decision tree is
the number of nodes in the tree. 
36 
00:02:24,460 --> 00:02:27,160 
This is an example of a decision tree. 
37 
00:02:27,160 --> 00:02:32,290 
It can be used to classify an animal
as a mammal or not a mammal. 
38 
00:02:32,290 --> 00:02:36,530 
According to this decision tree,
if an animal is warm-blooded, 
39 
00:02:36,530 --> 00:02:41,570 
gives live birth, and
is a vertebrate, then it is a mammal. 
40 
00:02:41,570 --> 00:02:45,000 
If an animal does not have all
of these three characteristics, 
41 
00:02:45,000 --> 00:02:46,350 
then it is not a mammal. 
42 
00:02:47,520 --> 00:02:51,790 
A decision tree is built by starting
with all samples at a single node, 
43 
00:02:51,790 --> 00:02:53,400 
the root node. 
44 
00:02:53,400 --> 00:02:58,170 
Additional nodes are added when
the data is split into subsets. 
45 
00:02:58,170 --> 00:03:02,449 
At a high level, constructing a decision
tree consists of the following steps. 
46 
00:03:03,860 --> 00:03:05,650 
Start with all samples and a node, 
47 
00:03:07,180 --> 00:03:11,850 
partition the samples into subsets
based in the input variables. 
48 
00:03:11,850 --> 00:03:16,250 
The goal is to create subsets of
records that are purest, that is 
49 
00:03:16,250 --> 00:03:20,990 
each subset contains as many samples as
possible, belonging to just one class. 
50 
00:03:22,260 --> 00:03:26,557 
Another way to say this is that
the subsets should be as homogeneous or 
51 
00:03:26,557 --> 00:03:27,900 
as pure as possible. 
52 
00:03:29,120 --> 00:03:33,230 
Repeatedly partition data into
successively purer subsets 
53 
00:03:33,230 --> 00:03:35,460 
until some stopping
criterion is satisfied. 
54 
00:03:36,990 --> 00:03:37,930 
An algorithm for 
55 
00:03:37,930 --> 00:03:42,720 
constructing a decision tree model is
referred to as an induction algorithm. 
56 
00:03:42,720 --> 00:03:46,410 
So you may hear the term tree induction
used to describe the process of 
57 
00:03:46,410 --> 00:03:47,690 
building a decision tree. 
58 
00:03:49,330 --> 00:03:52,150 
Note that at each split
the induction algorithm 
59 
00:03:52,150 --> 00:03:56,820 
only considers the best way to split
that particular portion of the data. 
60 
00:03:56,820 --> 00:03:59,660 
This is referred to as a greedy approach. 
61 
00:03:59,660 --> 00:04:03,800 
Greedy algorithms solve a subset
of the problem at a time, and 
62 
00:04:03,800 --> 00:04:08,510 
as a necessary approach when solving
the entire problem is not feasible. 
63 
00:04:08,510 --> 00:04:11,020 
This is the case with decision trees. 
64 
00:04:11,020 --> 00:04:15,090 
It is not feasible to determine
the best tree given a data set, so 
65 
00:04:15,090 --> 00:04:18,460 
the tree has to be built
in piecemeal fashion 
66 
00:04:18,460 --> 00:04:21,978 
by determining the best way to split
the current node at each step. 
67 
00:04:21,978 --> 00:04:26,485 
And combining these decisions together
to form the final decision tree, 
68 
00:04:26,485 --> 00:04:31,600 
in constructing a decision tree
how is the data partitioned? 
69 
00:04:31,600 --> 00:04:34,520 
How does a decision tree
determine the best way to split 
70 
00:04:34,520 --> 00:04:36,920 
the set of samples at a node? 
71 
00:04:36,920 --> 00:04:41,530 
Again the goal is to partition data
at a node into subsets that are as 
72 
00:04:41,530 --> 00:04:42,700 
pure as possible. 
73 
00:04:43,760 --> 00:04:45,110 
In this example, 
74 
00:04:45,110 --> 00:04:49,640 
the partition shown on the right
results in more homogeneous subsets. 
75 
00:04:49,640 --> 00:04:54,620 
Since these subsets contain more
samples belonging to a single class 
76 
00:04:54,620 --> 00:04:57,950 
than the resulting subsets
shown on the left. 
77 
00:04:57,950 --> 00:05:01,800 
So the partition on the right
results in purer subsets and 
78 
00:05:01,800 --> 00:05:03,180 
is the preferred partition. 
79 
00:05:04,440 --> 00:05:08,650 
Therefore, we need a way to
measure the purity of a split 
80 
00:05:08,650 --> 00:05:12,910 
in order to compare different
ways to partition a set of data. 
81 
00:05:12,910 --> 00:05:17,980 
It turns out that it works out better
mathematically if we measure the impurity 
82 
00:05:17,980 --> 00:05:19,820 
rather than the purity of a split. 
83 
00:05:20,830 --> 00:05:23,370 
So the impurity measure of a node 
84 
00:05:23,370 --> 00:05:26,730 
specifies how mixed
the resulting subsets are. 
85 
00:05:26,730 --> 00:05:30,580 
Since we want the resulting subsets
to have homogeneous class labels, 
86 
00:05:30,580 --> 00:05:36,230 
not mixed class labels, we want the split
that minimizes the impurity measure. 
87 
00:05:37,720 --> 00:05:39,610 
A common impurity measure used for 
88 
00:05:39,610 --> 00:05:41,900 
determining the best
split is the Gini Index. 
89 
00:05:42,900 --> 00:05:47,440 
The lower the Gini Index the higher
the purity of the split. 
90 
00:05:47,440 --> 00:05:52,470 
So the decision tree will select
the split that minimizes the Gini Index. 
91 
00:05:52,470 --> 00:05:56,710 
Besides the Gini Index, other
impurity measures include entropy, or 
92 
00:05:56,710 --> 00:05:59,571 
information gain, and
misclassification rate. 
93 
00:06:01,140 --> 00:06:04,750 
The other factor in determining
the best way to partition a node 
94 
00:06:04,750 --> 00:06:07,500 
is which variable to split on. 
95 
00:06:07,500 --> 00:06:12,910 
The decision tree will test all variables
to determine the best way to split a node 
96 
00:06:12,910 --> 00:06:16,860 
using a purity measure such as
the Gini index to compare the various 
97 
00:06:16,860 --> 00:06:17,709 
possibilities. 
98 
00:06:18,910 --> 00:06:23,880 
Recall that the tree induction algorithm
repeatedly splits nodes to get more and 
99 
00:06:23,880 --> 00:06:25,890 
more homogeneous subsets. 
100 
00:06:25,890 --> 00:06:27,970 
So when does this process stop? 
101 
00:06:27,970 --> 00:06:30,150 
When does the algorithm
stop growing the tree? 
102 
00:06:31,150 --> 00:06:35,670 
There's several criteria that can be used
to determine when a node should no longer 
103 
00:06:35,670 --> 00:06:36,935 
be split into subsets. 
104 
00:06:38,830 --> 00:06:41,730 
The induction algorithm can
stop expanding a node when 
105 
00:06:41,730 --> 00:06:45,500 
all samples in the node
have the same class label. 
106 
00:06:45,500 --> 00:06:49,370 
This means that this set of data
is as pure as possible, and 
107 
00:06:49,370 --> 00:06:53,030 
further splitting will not result in
any better partition of the data. 
108 
00:06:54,230 --> 00:06:58,890 
Since getting completely pure subsets
is difficult to achieve with real data, 
109 
00:06:58,890 --> 00:07:01,620 
this stopping criterion can be modified. 
110 
00:07:01,620 --> 00:07:05,961 
To when a certain percentage of
the samples in the node, say 90% for 
111 
00:07:05,961 --> 00:07:08,400 
example, have the same class labels. 
112 
00:07:09,750 --> 00:07:13,710 
The algorithm can stop expanding a node
when the number of samples in the node 
113 
00:07:13,710 --> 00:07:15,940 
falls below a certain minimum value. 
114 
00:07:16,940 --> 00:07:19,920 
A this point the number of
samples is too small to make 
115 
00:07:19,920 --> 00:07:23,119 
much difference in the classification
results with the further splitting. 
116 
00:07:24,570 --> 00:07:28,860 
The induction algorithm can stop expanding
a node when the improvement in impurity 
117 
00:07:28,860 --> 00:07:33,540 
measure is too small to make much of
a difference in classification results. 
118 
00:07:35,060 --> 00:07:39,980 
The algorithm can stop expanding a node
when the maximum tree depth is reached. 
119 
00:07:39,980 --> 00:07:43,240 
This is to control the complexity
of the resulting tree. 
120 
00:07:45,060 --> 00:07:48,590 
There can be other criteria that can be
used to determine when tree induction 
121 
00:07:48,590 --> 00:07:49,090 
should stop. 
122 
00:07:50,720 --> 00:07:55,140 
Let's take a look at an example to
illustrate the tree induction process. 
123 
00:07:55,140 --> 00:07:59,720 
Let's say that we want to classify loan
applicants as being likely to repay a loan 
124 
00:07:59,720 --> 00:08:03,090 
or not likely to repay a loan
based on their income and 
125 
00:08:03,090 --> 00:08:04,750 
amount of debt they already have. 
126 
00:08:06,090 --> 00:08:07,360 
Building a decision tree for 
127 
00:08:07,360 --> 00:08:10,490 
this classification problem
could proceed as follows. 
128 
00:08:10,490 --> 00:08:14,230 
Consider the input space of this
problem as shown in the left figure. 
129 
00:08:14,230 --> 00:08:18,990 
One way to split this
data set into homogeneous 
130 
00:08:18,990 --> 00:08:23,380 
subsets is to consider the decision
boundary where income equals t1. 
131 
00:08:24,800 --> 00:08:28,410 
To the right of this decision
boundary are mostly red samples and 
132 
00:08:28,410 --> 00:08:30,140 
to the left are mostly blue samples. 
133 
00:08:31,340 --> 00:08:33,950 
The subsets are not
completely homogeneous but 
134 
00:08:33,950 --> 00:08:38,100 
that is the best way to split the original
data set based on the variable income. 
135 
00:08:40,120 --> 00:08:44,380 
This decision boundary is represented
in the decision tree by the condition 
136 
00:08:44,380 --> 00:08:48,326 
Income is greater than
t1 at the root node. 
137 
00:08:48,326 --> 00:08:51,894 
This is the condition used to
split the original data set. 
138 
00:08:51,894 --> 00:08:55,350 
Samples with income greater
than the threshold value 
139 
00:08:55,350 --> 00:08:59,800 
of t1 are placed in the right subset and
samples with income less than or 
140 
00:08:59,800 --> 00:09:04,950 
equal to t1 are placed in the left subset,
just as shown in the right diagram. 
141 
00:09:06,480 --> 00:09:08,748 
The right subset is now labeled as red, 
142 
00:09:08,748 --> 00:09:12,203 
meaning that the loan applicant
is likely to be paid alone. 
143 
00:09:13,688 --> 00:09:18,980 
The second step then is to determine how
to split the region outlined in red. 
144 
00:09:18,980 --> 00:09:24,000 
As shown in the left diagram, in input
space, the best way to split this data 
145 
00:09:24,000 --> 00:09:27,940 
is specified by the second decision
boundary with debt equals t2. 
146 
00:09:29,520 --> 00:09:32,580 
This is represented in
the decision tree on the right 
147 
00:09:32,580 --> 00:09:36,760 
with the addition of the node with
condition debt greater than t2. 
148 
00:09:38,050 --> 00:09:41,930 
Samples with the value of debt greater
than t2 are shown in the region 
149 
00:09:41,930 --> 00:09:44,010 
above the decision boundary. 
150 
00:09:44,010 --> 00:09:49,370 
This region contains all blue samples and
so the corresponding node is labeled blue. 
151 
00:09:49,370 --> 00:09:52,700 
Meaning that the loan applicant
is not likely to repay the loan. 
152 
00:09:54,170 --> 00:09:58,180 
The third and final split looks at how
to split the region outlined in red 
153 
00:09:58,180 --> 00:09:59,010 
in the left diagram. 
154 
00:10:00,210 --> 00:10:04,930 
The best split is specified by
the boundary with income equals T3. 
155 
00:10:04,930 --> 00:10:08,190 
This splits the red region
into two pure subsets. 
156 
00:10:09,250 --> 00:10:13,330 
This split is represented in the decision
tree by adding a node with condition, 
157 
00:10:13,330 --> 00:10:15,629 
Income is greater than t3. 
158 
00:10:15,629 --> 00:10:19,210 
The left resulting node is labeled blue,
and 
159 
00:10:19,210 --> 00:10:21,980 
the right resulting node is labeled red, 
160 
00:10:21,980 --> 00:10:26,200 
corresponding to the resulting subsets
within the red border in the left diagram. 
161 
00:10:27,780 --> 00:10:30,810 
We end with the final
decision tree on the right, 
162 
00:10:30,810 --> 00:10:35,400 
which implements the decision boundaries
shown as dash lines in the left diagram. 
163 
00:10:36,630 --> 00:10:40,280 
These decision boundaries
partition the data set as shown. 
164 
00:10:40,280 --> 00:10:45,200 
The label for each region is determined by
the label of the majority of the samples. 
165 
00:10:45,200 --> 00:10:49,090 
These labels are reflected in
the leaf nodes of the decision tree. 
166 
00:10:49,090 --> 00:10:49,910 
Shown on the right. 
167 
00:10:51,500 --> 00:10:55,850 
You may have noticed that the decision
boundaries of a decision tree are parallel 
168 
00:10:55,850 --> 00:11:02,110 
to the axes formed by the variables,
this is referred to as being rectilinear. 
169 
00:11:02,110 --> 00:11:06,410 
The boundaries are rectilinear
because each split considers only 
170 
00:11:06,410 --> 00:11:08,370 
a single variable. 
171 
00:11:08,370 --> 00:11:12,185 
There are variance of the tree induction
algorithm that consider more than one 
172 
00:11:12,185 --> 00:11:14,560 
attribute when splitting a note. 
173 
00:11:14,560 --> 00:11:19,550 
However, each split has to consider all
combinations of combined variables and 
174 
00:11:19,550 --> 00:11:24,000 
so such induction algorithms are much
more computationally expensive. 
175 
00:11:25,310 --> 00:11:28,650 
There are a few important things to
note about the decision tree classifier. 
176 
00:11:30,160 --> 00:11:33,970 
The resulting tree is often simple
to understand and interpret. 
177 
00:11:33,970 --> 00:11:38,500 
This is one of the biggest advantages
of decision trees for classification. 
178 
00:11:38,500 --> 00:11:41,550 
It is often possible to look
at the resulting tree to see 
179 
00:11:41,550 --> 00:11:45,570 
which variables are important to
the classification problem and 
180 
00:11:45,570 --> 00:11:48,840 
understand how
the classification is performed. 
181 
00:11:48,840 --> 00:11:52,590 
For this reason, many people will start
out with the decision tree classifier 
182 
00:11:52,590 --> 00:11:55,690 
to get a feel for
the classification problem. 
183 
00:11:55,690 --> 00:11:58,730 
Even if they end up using more
sophisticated models later on. 
184 
00:12:00,730 --> 00:12:03,480 
The tree induction algorithm
as described in this lesson 
185 
00:12:03,480 --> 00:12:06,990 
is relatively computationally inexpensive. 
186 
00:12:06,990 --> 00:12:10,940 
So training a decision tree for
classification can be relatively fast. 
187 
00:12:12,985 --> 00:12:17,495 
The greedy approach used by tree induction
algorithm determines the best way to split 
188 
00:12:17,495 --> 00:12:19,905 
the portion of the data at a node but 
189 
00:12:19,905 --> 00:12:23,823 
does not guarantee the best solution
overall for the entire data set. 
190 
00:12:26,000 --> 00:12:28,490 
Decision boundaries are rectilinear. 
191 
00:12:28,490 --> 00:12:32,550 
This can limit the expressiveness
of the resulting model which means 
192 
00:12:32,550 --> 00:12:36,480 
that it may not be able to solve
complicated classification problems that 
193 
00:12:36,480 --> 00:12:39,400 
require more complex decision
boundaries to be formed. 
194 
00:12:41,060 --> 00:12:46,280 
In summary, the decision tree classifier
uses a tree like structure to specify 
195 
00:12:46,280 --> 00:12:51,600 
a series of conditions that are tested to
determine the class label for a sample. 
196 
00:12:52,740 --> 00:12:57,230 
The decision tree is constructed by
repeatedly splitting a data partition 
197 
00:12:57,230 --> 00:13:00,860 
into successively more
homogeneous subsets. 
198 
00:13:00,860 --> 00:13:03,680 
The resulting tree can
often be easy to interpret. 
1 
00:00:00,681 --> 00:00:05,059 
We will start with a very simple
classification technique called k-Nearest 
2 
00:00:05,059 --> 00:00:05,807 
Neighbors. 
3 
00:00:05,807 --> 00:00:10,451 
After this video, you will be able
to describe how kNN is used for 
4 
00:00:10,451 --> 00:00:12,710 
classification. 
5 
00:00:12,710 --> 00:00:17,995 
Discuss the assumption behind kNN and
explain what the k stands for in kNN. 
6 
00:00:17,995 --> 00:00:21,995 
kNN stands for k-Nearest Neighbors. 
7 
00:00:21,995 --> 00:00:25,924 
This is one of the simplest techniques
to build a classification model. 
8 
00:00:25,924 --> 00:00:30,623 
The basic idea is to classify
a sample based on its neighbors. 
9 
00:00:30,623 --> 00:00:35,774 
So when you get a new sample as shown by
the green circle in the figure, the class 
10 
00:00:35,774 --> 00:00:40,944 
label for that sample is determined by
looking at the labels of its neighbors. 
11 
00:00:40,944 --> 00:00:44,030 
KNN relies on the notion
of the so-called duck test. 
12 
00:00:44,030 --> 00:00:48,724 
That is if it looks like a duck,
swims like a duck and quacks like a duck, 
13 
00:00:48,724 --> 00:00:51,430 
then it probably is a duck. 
14 
00:00:51,430 --> 00:00:53,260 
In the classification context, 
15 
00:00:53,260 --> 00:00:59,320 
this means that samples with similar input
values likely belong to the same class. 
16 
00:00:59,320 --> 00:01:03,640 
So, samples with similar input values
should be labeled with the same 
17 
00:01:03,640 --> 00:01:05,390 
target label. 
18 
00:01:05,390 --> 00:01:08,820 
This means that classification
of a sample is dependent 
19 
00:01:08,820 --> 00:01:12,250 
on the target labels of
the neighboring points. 
20 
00:01:12,250 --> 00:01:15,140 
In more detail then,
this is how kNN works. 
21 
00:01:15,140 --> 00:01:17,084 
Given a new sample, look for 
22 
00:01:17,084 --> 00:01:21,710 
the samples in the training data
that are closest to the new sample. 
23 
00:01:21,710 --> 00:01:23,740 
These are the neighbors. 
24 
00:01:23,740 --> 00:01:28,550 
Use the labels of this neighboring points
to determine the label for the new sample. 
25 
00:01:29,590 --> 00:01:32,410 
This figure illustrate how kNN works. 
26 
00:01:32,410 --> 00:01:36,240 
The problem here is to determine if
a sample should be classified as 
27 
00:01:36,240 --> 00:01:37,940 
a blue square or red triangle. 
28 
00:01:38,990 --> 00:01:40,980 
The green circle is the new sample. 
29 
00:01:40,980 --> 00:01:45,780 
To determine a class label for this new
sample, look at its closest neighbors. 
30 
00:01:45,780 --> 00:01:49,230 
These neighbors are the samples
within the dashed circle. 
31 
00:01:50,270 --> 00:01:53,180 
Two blue squares and one red triangle. 
32 
00:01:53,180 --> 00:01:56,590 
The class labels of the neighboring
samples determine the label for 
33 
00:01:56,590 --> 00:01:57,290 
the new sample. 
34 
00:01:58,690 --> 00:02:03,480 
The value of k determines the number
of nearest neighbor to consider. 
35 
00:02:03,480 --> 00:02:08,160 
So if k equals 1,
then only the closest neighbor is examined 
36 
00:02:08,160 --> 00:02:12,030 
to determine the class of the new
sample as shown in the left figure. 
37 
00:02:13,170 --> 00:02:14,197 
If k equals 2, 
38 
00:02:14,197 --> 00:02:19,182 
then the 2 nearest neighbors are
considered as seen in the middle figure. 
39 
00:02:19,182 --> 00:02:23,815 
If k equal 3, then the 3 nearest
neighbors are considered as 
40 
00:02:23,815 --> 00:02:26,101 
in the right figure and so on. 
41 
00:02:26,101 --> 00:02:30,311 
If k equal 1 and only 1 neighbor is used,
then the label for 
42 
00:02:30,311 --> 00:02:34,030 
the new sample is simpler
the label of the neighbor. 
43 
00:02:34,030 --> 00:02:37,090 
This is shown in the left figure. 
44 
00:02:37,090 --> 00:02:39,610 
The label of the new sample is then A, 
45 
00:02:39,610 --> 00:02:42,510 
since that is the label of
its one nearest neighbor. 
46 
00:02:43,630 --> 00:02:48,270 
When multiple neighbors are considered,
then a voting scheme is used. 
47 
00:02:48,270 --> 00:02:52,290 
Majority of vote is commonly used,
so the label associated with 
48 
00:02:52,290 --> 00:02:57,670 
the majority of the neighbors is
used as the label of the new sample. 
49 
00:02:57,670 --> 00:03:00,730 
This is what we see in the right figure. 
50 
00:03:00,730 --> 00:03:04,090 
With k equals 3,
3 nearest neighbors are considered. 
51 
00:03:05,398 --> 00:03:08,830 
With two neighbors labeled as A and
one as B, 
52 
00:03:08,830 --> 00:03:12,560 
the majority of vote determines that
the new sample should be labeled as A. 
53 
00:03:13,680 --> 00:03:18,168 
In case of a tie which could be
possible if the value of k is even, 
54 
00:03:18,168 --> 00:03:21,154 
then some tight breaking rule is needed. 
55 
00:03:21,154 --> 00:03:24,853 
For example, the label of
the closer neighbor is used or 
56 
00:03:24,853 --> 00:03:28,250 
the label is chosen randomly
among the neighbors. 
57 
00:03:28,250 --> 00:03:30,480 
This is seen in the middle figure. 
58 
00:03:31,550 --> 00:03:35,970 
With two nearest neighbors and each with
a different class label, the label for 
59 
00:03:35,970 --> 00:03:38,640 
the new sample is randomly
chosen here to be B. 
60 
00:03:40,000 --> 00:03:44,230 
With kNN, some measure of similarity
is needed to determine how 
61 
00:03:44,230 --> 00:03:47,030 
close two samples are together. 
62 
00:03:47,030 --> 00:03:50,690 
This is necessary to determine which
samples are the nearest neighbors. 
63 
00:03:51,690 --> 00:03:55,650 
Distance measures such as
distance are commonly used. 
64 
00:03:55,650 --> 00:03:59,480 
Other distance measures that can be used,
include Manhattan and hemming distance. 
65 
00:04:00,640 --> 00:04:05,671 
To summarize, kNN is a very
simple classification technique. 
66 
00:04:05,671 --> 00:04:08,970 
Note that there is no
separate training phase. 
67 
00:04:08,970 --> 00:04:13,490 
There is no separate part where a model is
constructed and its parameter is adjusted. 
68 
00:04:13,490 --> 00:04:16,970 
This is unlike most other
classification algorithms. 
69 
00:04:18,510 --> 00:04:22,230 
KNN can generate complex
decision boundaries allowing for 
70 
00:04:22,230 --> 00:04:24,980 
complex classification
decisions to be made. 
71 
00:04:26,400 --> 00:04:28,410 
It can be susceptible to noise,however, 
72 
00:04:28,410 --> 00:04:32,910 
because classification decisions
are made using only information about 
73 
00:04:32,910 --> 00:04:35,735 
a few neighboring points
instead of the entire dataset. 
74 
00:04:37,570 --> 00:04:41,540 
KNN can be slow, however, since
the distance between a new sample and 
75 
00:04:41,540 --> 00:04:45,630 
all sample points in the data must
be calculated in order to determine 
76 
00:04:45,630 --> 00:04:49,796 
the k-Nearest Neighbors. 
1 
00:00:00,920 --> 00:00:05,020 
In this lecture, we will discuss
the Naive Bayes classifier. 
2 
00:00:05,020 --> 00:00:09,680 
After this video, you will be able
to discuss how a Naive Bayes model 
3 
00:00:09,680 --> 00:00:14,550 
works fro classification,
define the components of Bayes' Rule and 
4 
00:00:14,550 --> 00:00:17,349 
explain what the naive
means in Naive Bayes. 
5 
00:00:18,570 --> 00:00:24,500 
A Naive Bayes classification model uses a
probabilistic approach to classification. 
6 
00:00:24,500 --> 00:00:28,190 
What this means is that the relationships
between the input features and 
7 
00:00:28,190 --> 00:00:31,980 
the class is expressed as probabilities. 
8 
00:00:31,980 --> 00:00:36,390 
So given the input features for
a sample, the probability for 
9 
00:00:36,390 --> 00:00:38,880 
each class is estimated. 
10 
00:00:38,880 --> 00:00:43,980 
The class with the highest probability
then, determines the label for the sample. 
11 
00:00:45,800 --> 00:00:50,250 
In addition to using a probabilistic
framework for classification, 
12 
00:00:50,250 --> 00:00:55,060 
the Naive Bayes classifier also uses
what is known as Bayes' theorem. 
13 
00:00:55,060 --> 00:00:58,930 
The application of Bayes' theorem makes
estimating the probabilities easier. 
14 
00:01:00,100 --> 00:01:04,850 
In addition, Naive Bayes assumes that
the input features are statistically 
15 
00:01:04,850 --> 00:01:07,400 
independent of one another. 
16 
00:01:07,400 --> 00:01:10,070 
This means that, for a given class, 
17 
00:01:10,070 --> 00:01:15,060 
the value of one feature does not
affect the value of any other feature. 
18 
00:01:15,060 --> 00:01:19,060 
This independence assumption is
an oversimplified one that does not always 
19 
00:01:19,060 --> 00:01:23,330 
hold true, and so
is considered a naive assumption. 
20 
00:01:23,330 --> 00:01:25,510 
The naive independence assumption and 
21 
00:01:25,510 --> 00:01:28,830 
the use of Bayes theorem gives this
classification model its name. 
22 
00:01:29,910 --> 00:01:31,320 
We will cover Bayes theorem and 
23 
00:01:31,320 --> 00:01:34,120 
the independence assumption in
more detail in this lecture. 
24 
00:01:35,580 --> 00:01:38,200 
Before we look at naive
Bayes in more detail, 
25 
00:01:38,200 --> 00:01:40,630 
let's first start with some
background on probability. 
26 
00:01:41,800 --> 00:01:45,970 
Probability is the measure
of how likely an event is, 
27 
00:01:45,970 --> 00:01:51,060 
the probability of an event A occurring
is denoted P and in parenthesis A. 
28 
00:01:52,330 --> 00:01:57,710 
It is calculated by dividing
the number of ways event A can occur, 
29 
00:01:57,710 --> 00:01:59,769 
by the total number of possible outcomes. 
30 
00:02:01,370 --> 00:02:07,170 
For example, what is the probability
of rolling a die and getting six? 
31 
00:02:07,170 --> 00:02:10,890 
When you roll a die you can get
a number from one to six, so 
32 
00:02:10,890 --> 00:02:13,060 
the number of possible outcomes is six. 
33 
00:02:14,330 --> 00:02:17,310 
The number of ways fro getting six is one, 
34 
00:02:17,310 --> 00:02:21,850 
since the way you can get six is if
the die shows six when it stops rolling. 
35 
00:02:22,850 --> 00:02:27,800 
That means that the probability of getting
the number six when you roll a die 
36 
00:02:27,800 --> 00:02:30,080 
is one over six. 
37 
00:02:30,080 --> 00:02:34,790 
This is denoted p of six and
that's equal to one over six and 
38 
00:02:34,790 --> 00:02:38,670 
is read as probability
of six is one over six. 
39 
00:02:40,440 --> 00:02:45,020 
There's also a joint probability,
the joint probability specifies 
40 
00:02:45,020 --> 00:02:49,930 
the probability of event A and
event B occurring together. 
41 
00:02:51,090 --> 00:02:56,620 
In this diagram, the probability of event
A occurring is shown as the blue circle 
42 
00:02:57,650 --> 00:03:01,710 
and the probability of event B
occurring is shown as the green circle. 
43 
00:03:02,770 --> 00:03:07,630 
Then the joint probability,
that is the probability of A and 
44 
00:03:07,630 --> 00:03:12,340 
B occurring together is shown as
the overlap of these two circles. 
45 
00:03:13,470 --> 00:03:18,635 
The joint probability of A and
B is denoted, 
46 
00:03:18,635 --> 00:03:21,610 
P(A,B) for 
47 
00:03:21,610 --> 00:03:26,860 
an example of joint probability,
let's consider rolling 2 dice together. 
48 
00:03:26,860 --> 00:03:31,090 
What is the probability in getting
2 sixes or a six from each die. 
49 
00:03:32,250 --> 00:03:37,020 
If the two events are independent, then
the joint probability is simply the result 
50 
00:03:37,020 --> 00:03:40,880 
of multiplying the probabilities
of the individual events together. 
51 
00:03:42,000 --> 00:03:45,690 
In this case then, we have
the probability of rolling a six for 
52 
00:03:45,690 --> 00:03:51,040 
each die is one over six so
the joint probability is one over 36, 
53 
00:03:51,040 --> 00:03:56,480 
this leads us to conditional probability. 
54 
00:03:56,480 --> 00:04:01,250 
The conditional probability is
the probability of event A occurring 
55 
00:04:01,250 --> 00:04:04,090 
Given that event B has already occurred. 
56 
00:04:05,150 --> 00:04:11,570 
Another way to say this is that
event A is conditioned on event B. 
57 
00:04:11,570 --> 00:04:16,170 
The conditional probability is
the noted P and in parentheses A, 
58 
00:04:16,170 --> 00:04:21,390 
vertical line B and is read as,
probability of A Given B. 
59 
00:04:22,730 --> 00:04:27,510 
This diagram gives a graphical
definition of conditional probability. 
60 
00:04:27,510 --> 00:04:33,030 
As before, the blue circle is
the probability of event A occurring, 
61 
00:04:33,030 --> 00:04:37,260 
the green circle is the probability
of event B occurring. 
62 
00:04:37,260 --> 00:04:40,610 
The overlap is a joint
probability of A and B. 
63 
00:04:41,730 --> 00:04:46,500 
The conditional probability,
P(A given B) then is 
64 
00:04:46,500 --> 00:04:51,870 
calculated as the join probability
divided by the probability of B. 
65 
00:04:53,040 --> 00:04:56,980 
The conditional probability is
an important concept in classification 
66 
00:04:56,980 --> 00:04:58,800 
as we will see later. 
67 
00:04:58,800 --> 00:05:03,020 
It provides the means to specify
the probability of a class label, 
68 
00:05:03,020 --> 00:05:04,240 
given the input values. 
69 
00:05:05,960 --> 00:05:12,390 
The relationship between conditional
probabilities P of B given A and 
70 
00:05:12,390 --> 00:05:17,295 
P of A given B can be expressed
through Bayes' Theorem. 
71 
00:05:17,295 --> 00:05:21,811 
This theorem is named after a reverend
named Thomas Bayes who lived in the 1700s. 
72 
00:05:21,811 --> 00:05:26,691 
It is a way to look at how the probability
of a hypothesis is affected by 
73 
00:05:26,691 --> 00:05:29,170 
new evidence gathered from data. 
74 
00:05:29,170 --> 00:05:34,925 
Bayes' theorem expresses the relationship
between probability of B 
75 
00:05:34,925 --> 00:05:40,285 
given A and probability of A given
B as shown in this equation. 
76 
00:05:40,285 --> 00:05:46,033 
Bayes' Theorem is also known
as Bayes' Rule or Bayes' Law. 
77 
00:05:46,033 --> 00:05:49,750 
Now that we have reviewed some
background on probability 
78 
00:05:49,750 --> 00:05:52,610 
let's see how all this relates
to the classification problem. 
79 
00:05:53,840 --> 00:05:58,249 
With the probabilistic framework the
classification task is defined as follows. 
80 
00:05:59,530 --> 00:06:03,930 
Capital X is the set of values for
the input features in the sample, 
81 
00:06:05,090 --> 00:06:09,780 
given a sample with features X,
predict the corresponding class C. 
82 
00:06:10,860 --> 00:06:15,930 
Another way to state this is,
what is the class label associated with 
83 
00:06:15,930 --> 00:06:21,550 
the feature vector X or how should
the feature vector x be classified? 
84 
00:06:22,910 --> 00:06:27,470 
To find the class label C we need to
calculate the conditional probability of 
85 
00:06:27,470 --> 00:06:31,540 
class C, given X for all classes and 
86 
00:06:31,540 --> 00:06:33,990 
select a class with
the highest probability. 
87 
00:06:35,450 --> 00:06:37,010 
So for classification, 
88 
00:06:37,010 --> 00:06:43,460 
we want to find the value of C that
maximizes the probability of C given X. 
89 
00:06:43,460 --> 00:06:47,140 
The problem is that it is difficult
to estimate this probability, 
90 
00:06:47,140 --> 00:06:52,530 
because we would need to enumerate every
possible combination of feature values and 
91 
00:06:52,530 --> 00:06:54,460 
to know the conditional probability. 
92 
00:06:54,460 --> 00:06:59,570 
For each class given every
possible feature combination and 
93 
00:06:59,570 --> 00:07:02,330 
here is where Bayes'
theorem comes into play. 
94 
00:07:02,330 --> 00:07:06,710 
The classification problem can be
reformulated using Bayes' theorem 
95 
00:07:06,710 --> 00:07:08,876 
to simplify the classification problem. 
96 
00:07:08,876 --> 00:07:16,060 
Specifically, using Bayes' theorem, the
probability of c given x, can be expressed 
97 
00:07:16,060 --> 00:07:20,600 
using other probability quantities,
which can be estimated from the data. 
98 
00:07:22,030 --> 00:07:26,010 
Here's Bayes' theorem again, but
some additional terms defined. 
99 
00:07:26,010 --> 00:07:31,870 
Probability of C I X is referred to
as the posterior probability since 
100 
00:07:31,870 --> 00:07:37,930 
it is the probability of the class label
being C after observing input features X. 
101 
00:07:39,400 --> 00:07:43,850 
Probability of X given
C is the probability of 
102 
00:07:43,850 --> 00:07:48,450 
observing input features X given
that c is the class label. 
103 
00:07:49,540 --> 00:07:54,480 
This is the class conditional probability
since it is conditioned on the class. 
104 
00:07:55,790 --> 00:08:00,440 
Probability of c is the probability
of the class label being C, 
105 
00:08:01,470 --> 00:08:06,560 
this is the probability of each class
prior to observing any input data. 
106 
00:08:06,560 --> 00:08:09,350 
And so
is referred to as the prior probability. 
107 
00:08:10,810 --> 00:08:14,240 
The probability of X is the probability 
108 
00:08:14,240 --> 00:08:18,479 
of observing input features X
regardless of what the class label is. 
109 
00:08:20,070 --> 00:08:24,617 
So for classification we want
to calculate the posterior 
110 
00:08:24,617 --> 00:08:27,939 
probability P(C | X) for each class C. 
111 
00:08:27,939 --> 00:08:34,343 
From Bayes' theorem P(C | X) is related to 
112 
00:08:34,343 --> 00:08:40,250 
the P(X | C) P(C And probability of X. 
113 
00:08:41,790 --> 00:08:45,680 
Probability of X does not depend
on the class C, therefore, 
114 
00:08:45,680 --> 00:08:49,040 
it is a constant value, given the input X. 
115 
00:08:49,040 --> 00:08:52,940 
Since it the same value for
all classes, the probability 
116 
00:08:52,940 --> 00:08:57,610 
of X can be removed from the calculation
of probability of C, given X. 
117 
00:08:58,610 --> 00:09:03,910 
What's left then are probability of
X given C and the probability of C. 
118 
00:09:05,630 --> 00:09:09,826 
So estimating the probability
of C given X boils down to 
119 
00:09:09,826 --> 00:09:14,583 
estimating the probability of X
given C and probability of C. 
120 
00:09:14,583 --> 00:09:18,397 
The nice thing is that
probability of X given C and 
121 
00:09:18,397 --> 00:09:22,220 
probability of C can be
estimated from the data. 
122 
00:09:23,390 --> 00:09:28,200 
So now we have a way to calculate probably
of C given X which is what we need for 
123 
00:09:28,200 --> 00:09:28,940 
classification. 
124 
00:09:30,150 --> 00:09:31,720 
To the estimate the probability of C 
125 
00:09:32,780 --> 00:09:37,450 
which is the probability of the class
of C before observing any input data. 
126 
00:09:37,450 --> 00:09:41,690 
We simply calculate the fraction
of samples with that class label 
127 
00:09:41,690 --> 00:09:42,910 
C in the training data. 
128 
00:09:44,170 --> 00:09:48,650 
For this example, there are four samples
labeled as green circles out of 10 
129 
00:09:48,650 --> 00:09:55,740 
samples, so probability of green
circle is 4 out of 10, or 0.4. 
130 
00:09:55,740 --> 00:10:01,650 
Similarly, the fraction of samples labeled
as red triangles is 6 out of 10, or 0.6. 
131 
00:10:01,650 --> 00:10:06,500 
So estimating the prior
probabilities is a simple count 
132 
00:10:06,500 --> 00:10:09,660 
of number of samples with each class label 
133 
00:10:09,660 --> 00:10:12,970 
divided by the total number of
samples in the training data center. 
134 
00:10:14,410 --> 00:10:19,000 
In estimating probability of X
given C which is the probability 
135 
00:10:19,000 --> 00:10:23,520 
of observing feature factor
X given that the class is C, 
136 
00:10:23,520 --> 00:10:26,760 
we can use the independent
assumption to simplify the problem. 
137 
00:10:27,790 --> 00:10:31,820 
The Independence Assumption of
the Naive Bayes classifier assumes 
138 
00:10:31,820 --> 00:10:36,620 
that each feature X sub I
in the featured vector X 
139 
00:10:36,620 --> 00:10:41,410 
is conditionally independent of every
other feature, given the class C. 
140 
00:10:42,520 --> 00:10:47,600 
This means that we only need to
estimate the probability of X 
141 
00:10:47,600 --> 00:10:51,360 
sub i given C, instead of having to 
142 
00:10:51,360 --> 00:10:56,030 
estimate the probability of
the entire feature X given C. 
143 
00:10:56,030 --> 00:10:58,070 
For every combination of values for 
144 
00:10:58,070 --> 00:11:03,900 
the features in X then we would simply
multiply these individual probabilities 
145 
00:11:03,900 --> 00:11:08,630 
together to get the probability
of the entire feature vector X. 
146 
00:11:08,630 --> 00:11:14,660 
Given the class C to estimate
the probability of X sub I, 
147 
00:11:14,660 --> 00:11:19,210 
given C, we count up the number
of times a particular input 
148 
00:11:19,210 --> 00:11:23,820 
value is observed for
the class c in the training data. 
149 
00:11:23,820 --> 00:11:28,880 
For example, the number of times that we
see the value of yes for the future home 
150 
00:11:28,880 --> 00:11:35,380 
owner, when the class label is no it's
three as indicated by the green arrows. 
151 
00:11:35,380 --> 00:11:40,980 
This is divided by the number of samples
with no as the class label which is seven. 
152 
00:11:40,980 --> 00:11:43,160 
This fraction, three out of seven, 
153 
00:11:43,160 --> 00:11:47,910 
is the probability that home owner
is Yes given that the class is No. 
154 
00:11:50,090 --> 00:11:53,420 
Similarly, the samples with
the value of Single for 
155 
00:11:53,420 --> 00:11:58,710 
the feature Marital Status when it crosses
Yes are indicated by the red arrows. 
156 
00:11:58,710 --> 00:12:02,050 
And the probability that
Marital Status is Single, 
157 
00:12:02,050 --> 00:12:07,520 
given that the class label
is Yes is 2/3 or 0.67. 
158 
00:12:09,050 --> 00:12:14,360 
Some things to know about the Naive Bayes
classification model are it is a fast and 
159 
00:12:14,360 --> 00:12:15,840 
simple algorithm. 
160 
00:12:15,840 --> 00:12:19,940 
The algorithm boils down to calculating
counts for probabilities and 
161 
00:12:19,940 --> 00:12:24,930 
performing some multiplication, so
it is very simple to implement. 
162 
00:12:24,930 --> 00:12:28,000 
And the probabilities that are needed
can be calculated with a single 
163 
00:12:28,000 --> 00:12:30,180 
scan of the data set and
stored in a table. 
164 
00:12:31,610 --> 00:12:35,610 
Either two processing of the data
is not necessary as with many other 
165 
00:12:35,610 --> 00:12:36,500 
machine learning algorithms. 
166 
00:12:37,540 --> 00:12:42,150 
So model building and
testing of both task, it scales well. 
167 
00:12:42,150 --> 00:12:44,970 
Due today independent assumption,
the probability for 
168 
00:12:44,970 --> 00:12:48,000 
each feature can be
independently estimated. 
169 
00:12:48,000 --> 00:12:52,120 
These means that featured probability
is can be calculated in parallel, 
170 
00:12:52,120 --> 00:12:56,270 
this also means that the data set size
does not have to grow exponentially 
171 
00:12:56,270 --> 00:12:57,630 
with a number of features. 
172 
00:12:58,780 --> 00:13:02,130 
This avoids the many problems associated
with the curse of dimensionality, 
173 
00:13:03,480 --> 00:13:06,480 
this also means that you do not need
a lot of data to build the model. 
174 
00:13:07,700 --> 00:13:11,410 
The number of parameters scales
linearly with the number of features. 
175 
00:13:12,740 --> 00:13:16,700 
The Independence assumption does
not hold true in many cases. 
176 
00:13:16,700 --> 00:13:20,850 
In practice however, the Naive Bayes
classifier still tends to perform very 
177 
00:13:20,850 --> 00:13:24,070 
well this is because even
though Naive Bayes may 
178 
00:13:24,070 --> 00:13:28,400 
not provide good estimates of
the correct class probabilities. 
179 
00:13:28,400 --> 00:13:32,870 
As long as the correct class is
more probable than any other class, 
180 
00:13:32,870 --> 00:13:35,420 
the correct classification
results will be reached. 
181 
00:13:36,920 --> 00:13:40,940 
The independence assumption also prevents
the naive base classifier to model 
182 
00:13:40,940 --> 00:13:45,810 
interactions between features which
limits its classification power. 
183 
00:13:45,810 --> 00:13:50,040 
The increased risk of smoking in a history
of cancer would not be captured, 
184 
00:13:50,040 --> 00:13:50,560 
for example. 
185 
00:13:52,510 --> 00:13:56,230 
The Naive Bays classifier has been
applied to many real world problems 
186 
00:13:56,230 --> 00:14:00,620 
including spam filtering, document
classification, and sentiment analysis. 
187 
00:14:01,650 --> 00:14:06,530 
To summarize, the Naive Bayes classifier
uses a probabilistic framework for 
188 
00:14:06,530 --> 00:14:08,120 
classification. 
189 
00:14:08,120 --> 00:14:11,916 
It applies Bayes Theorem and
the Feature Independence Assumption, 
190 
00:14:11,916 --> 00:14:16,570 
to simplify the problem of estimating
probabilities for the classification task. 
1 
00:00:01,130 --> 00:00:04,720 
In addition to the evaluation
matrix covered n the last lecture. 
2 
00:00:04,720 --> 00:00:08,170 
The performance of a classification
model can also be evaluated 
3 
00:00:08,170 --> 00:00:10,430 
using a Confusion Matrix. 
4 
00:00:10,430 --> 00:00:12,990 
We will introduce the Confusion Matrix,
in this lecture. 
5 
00:00:14,180 --> 00:00:16,990 
After this video you will be able to, 
6 
00:00:16,990 --> 00:00:21,410 
describe how a confusion matrix can
be used to evaluate a classifier. 
7 
00:00:21,410 --> 00:00:24,240 
Interpret the confusion matrix of a model. 
8 
00:00:24,240 --> 00:00:27,160 
And relate accuracy to values
in a confusion matrix. 
9 
00:00:28,430 --> 00:00:34,080 
Let's use our example again of predicting
whether a given animal is a mammal or not. 
10 
00:00:34,080 --> 00:00:38,280 
Recall that this is a binary
classification task, with the Class Label 
11 
00:00:38,280 --> 00:00:42,849 
being either Yes, indicating mammal or
No indicating non-mammal. 
12 
00:00:44,360 --> 00:00:47,190 
Now let's review the different Types
of Errors that you can get with 
13 
00:00:47,190 --> 00:00:48,810 
Classification. 
14 
00:00:48,810 --> 00:00:50,650 
If the True Label is Yes and 
15 
00:00:50,650 --> 00:00:55,760 
the Predicted Label is Yes, then this
is a True Positive, abbreviated as TP. 
16 
00:00:56,850 --> 00:01:00,450 
This is a case where the label is
correctly predicted as positive. 
17 
00:01:01,500 --> 00:01:03,430 
If the True Label is No and 
18 
00:01:03,430 --> 00:01:08,280 
the Predicted Label is No, then this
is a True Negative, abbreviated as TN. 
19 
00:01:09,350 --> 00:01:12,710 
This is the case where the label is
correctly predicted as negative. 
20 
00:01:13,900 --> 00:01:15,970 
If the True Label is No and 
21 
00:01:15,970 --> 00:01:20,960 
the Predicted Label is Yes, then this
is a False Positive, abbreviated as FP. 
22 
00:01:22,240 --> 00:01:26,910 
This is the case where the label is
incorrectly predicted as positive 
23 
00:01:26,910 --> 00:01:27,830 
when it should be negative. 
24 
00:01:29,110 --> 00:01:32,850 
If the True Label is Yes and
the Predicted Label is No, 
25 
00:01:32,850 --> 00:01:36,180 
then this is a False Negative
abbreviated as FN. 
26 
00:01:37,300 --> 00:01:41,350 
This is the case where the label is
incorrectly predicted as negative, 
27 
00:01:41,350 --> 00:01:42,390 
when it should be positive. 
28 
00:01:44,390 --> 00:01:47,910 
A Confusion Matrix can be used to
summarize the different types of 
29 
00:01:47,910 --> 00:01:49,050 
classification errors. 
30 
00:01:50,100 --> 00:01:54,710 
The True Positive cell corresponds to the
samples that are correctly predicted as 
31 
00:01:54,710 --> 00:01:56,180 
positive by the model. 
32 
00:01:57,290 --> 00:02:01,460 
The True Negative cell corresponds to
the samples that are correctly predicted 
33 
00:02:01,460 --> 00:02:02,030 
as negative. 
34 
00:02:03,070 --> 00:02:07,240 
The False Positive cell corresponds to
samples that are incorrectly predicted 
35 
00:02:07,240 --> 00:02:07,940 
as positive. 
36 
00:02:08,950 --> 00:02:13,310 
The False Negative cell corresponds to
samples that are incorrectly predicted 
37 
00:02:13,310 --> 00:02:13,810 
as negative. 
38 
00:02:15,110 --> 00:02:19,280 
Each cell has the count, or percentage
of samples, with each type of errors. 
39 
00:02:20,940 --> 00:02:25,520 
Let's look at an example to see how
a Confusion Matrix is filled in. 
40 
00:02:25,520 --> 00:02:29,750 
The table on the lists True Labels
along with the models prediction for 
41 
00:02:29,750 --> 00:02:31,910 
a data set of ten samples. 
42 
00:02:31,910 --> 00:02:34,610 
We'll summarize this results
in a Confusion Matrix. 
43 
00:02:36,100 --> 00:02:39,140 
First let's figure out
the number of true positives. 
44 
00:02:39,140 --> 00:02:42,860 
We call that a true positive occurrence
when the output is correctly predicted 
45 
00:02:42,860 --> 00:02:44,320 
as positive. 
46 
00:02:44,320 --> 00:02:49,140 
In other words, the true label is Yes and
the model's prediction is also Yes. 
47 
00:02:49,140 --> 00:02:54,680 
In this example, there are three true
positives as indicated by the red arrows. 
48 
00:02:54,680 --> 00:02:58,390 
We enter three and the true positive
cell in the Confusion Matrix. 
49 
00:02:59,660 --> 00:03:02,310 
Now let's look at the true negatives. 
50 
00:03:02,310 --> 00:03:06,950 
A true negative occurs when the output
is correctly predicted as negative. 
51 
00:03:06,950 --> 00:03:12,150 
In other words, the true label is No and
the models prediction is also No. 
52 
00:03:12,150 --> 00:03:13,160 
In this example, 
53 
00:03:13,160 --> 00:03:17,360 
there are four true negatives as
indicated by the green arrows. 
54 
00:03:17,360 --> 00:03:21,050 
We enter four in the true negative
cell in the Confusion Matrix. 
55 
00:03:22,800 --> 00:03:24,720 
What about false negatives? 
56 
00:03:24,720 --> 00:03:29,490 
A false negative occurs when the output
is incorrectly predicted as negative, 
57 
00:03:29,490 --> 00:03:31,390 
when it should be positive. 
58 
00:03:31,390 --> 00:03:34,890 
That is the true label is Yes,
and the model's prediction is No. 
59 
00:03:35,890 --> 00:03:36,690 
In this example, 
60 
00:03:36,690 --> 00:03:40,990 
there are two false negatives as
indicated by the purple arrows. 
61 
00:03:40,990 --> 00:03:44,530 
We enter two in the false negative
cell in the Confusion Matrix. 
62 
00:03:45,970 --> 00:03:49,030 
Finally we need to look
at false positives. 
63 
00:03:49,030 --> 00:03:50,730 
A false positive occurs, 
64 
00:03:50,730 --> 00:03:55,760 
when the input is incorrectly predicted
as positive, when it should be negative. 
65 
00:03:55,760 --> 00:04:00,380 
That is the true label is No and
models prediction is Yes. 
66 
00:04:00,380 --> 00:04:04,020 
In this example there is one false
positive as indicated by the yellow arrow. 
67 
00:04:05,300 --> 00:04:08,870 
We enter one in the false positive
cell in the Confusion Matrix. 
68 
00:04:10,210 --> 00:04:13,460 
This is our complete Confusion Matrix for
this example. 
69 
00:04:13,460 --> 00:04:16,630 
We see that the sum of the numbers
in the cells add up to ten, 
70 
00:04:16,630 --> 00:04:18,820 
which is the number of
samples in our dataset. 
71 
00:04:20,650 --> 00:04:23,320 
Note that the diagonal values,
true positives and 
72 
00:04:23,320 --> 00:04:27,150 
true negatives are samples
with Correct Predictions. 
73 
00:04:27,150 --> 00:04:30,010 
In our example, these values sum up to 7. 
74 
00:04:30,010 --> 00:04:34,340 
Meaning that 7 out of 10 samples were
correctly predicted by the model. 
75 
00:04:35,370 --> 00:04:39,910 
The higher the sum of the diagonal values,
the better the performance of the model. 
76 
00:04:41,430 --> 00:04:46,050 
The off diagonal values capture
the misclassified samples. 
77 
00:04:46,050 --> 00:04:49,040 
Where the model's predictions
do not match the true labels. 
78 
00:04:50,070 --> 00:04:55,242 
In this example these values indicate
that there were three misclassifications. 
79 
00:04:55,242 --> 00:04:56,410 
Smaller values for 
80 
00:04:56,410 --> 00:05:01,130 
the off diagonal cells in the confusion
matrix indicate better model performance. 
81 
00:05:03,320 --> 00:05:06,230 
Note that the diagonal values,
true positives and 
82 
00:05:06,230 --> 00:05:10,080 
true negatives are samples
with correct predictions. 
83 
00:05:10,080 --> 00:05:13,970 
In our example,
these values sum up to 7 meaning 
84 
00:05:13,970 --> 00:05:18,030 
that 7 out of 10 samples were
correctly predicted by the model. 
85 
00:05:18,030 --> 00:05:22,350 
The higher the sum of the diagonal values,
the better the performance of the model. 
86 
00:05:23,460 --> 00:05:27,910 
You may have noticed that the diagonal
values are related to Accuracy Rate. 
87 
00:05:27,910 --> 00:05:31,390 
Recall that accuracy is defined
as the sum of two positives and 
88 
00:05:31,390 --> 00:05:34,610 
true negatives, divided by all samples. 
89 
00:05:34,610 --> 00:05:36,352 
The sum of two positives and 
90 
00:05:36,352 --> 00:05:40,839 
two negatives is the sum of the diagonal
values in a confusion matrix. 
91 
00:05:40,839 --> 00:05:44,516 
The sum of the diagonal
values in a confusion matrix, 
92 
00:05:44,516 --> 00:05:49,187 
divided by the total number of
samples gives you the Accuracy Rate. 
93 
00:05:49,187 --> 00:05:54,200 
Similarly, the off diagonal values
are related to the Error Rate. 
94 
00:05:54,200 --> 00:05:58,320 
Recall that the Error Rate is
the opposite of the Accuracy Rate. 
95 
00:05:58,320 --> 00:06:02,222 
The sum of the off diagonal
values in a confusion matrix, 
96 
00:06:02,222 --> 00:06:06,783 
divided by the total number of samples,
gives you the Error Rate. 
97 
00:06:06,783 --> 00:06:10,399 
Looking at the values in the Confusion
Matrix can help you understand 
98 
00:06:10,399 --> 00:06:13,529 
the kind of Misclassifications
that your model is making. 
99 
00:06:13,529 --> 00:06:16,937 
A high value for
this cell indicated by the yellow arrow, 
100 
00:06:16,937 --> 00:06:22,250 
means that classifying the Positive
class is problematic for the model. 
101 
00:06:22,250 --> 00:06:26,460 
A high value for the cell indicated by
the orange arrow on the other hand, 
102 
00:06:26,460 --> 00:06:30,260 
means that classifying the Negative
class is problematic for the model. 
103 
00:06:31,340 --> 00:06:34,230 
In summary,
the Confusion Matrix is a table 
104 
00:06:34,230 --> 00:06:37,980 
used to summarize the different
types of errors for a classifier. 
105 
00:06:37,980 --> 00:06:41,990 
The values in a Confusion Matrix can
be used to evaluate the performance of 
106 
00:06:41,990 --> 00:06:47,470 
a classifier and are related to evaluation
metrics, such as accuracy and error rates. 
107 
00:06:47,470 --> 00:06:51,020 
They also indicate what other types of
misclassifications the model is making. 
108 
00:06:52,040 --> 00:06:56,334 
Note that in some implementations
of a Confusion Matrix, the true and 
109 
00:06:56,334 --> 00:06:58,460 
predictive labels are switched. 
110 
00:06:58,460 --> 00:07:03,025 
Be sure to review the documentation for
the software you're using to generate 
111 
00:07:03,025 --> 00:07:06,565 
a Confusion Matrix to understand
what each cell specifies. 
1 
00:00:01,780 --> 00:00:05,020 
In this activity, we will be
evaluating the decision tree model 
2 
00:00:05,020 --> 00:00:08,788 
we created in the KNIME
classification hands-on. 
3 
00:00:08,788 --> 00:00:12,630 
First, we will create a confusion
matrix to determine the accuracy 
4 
00:00:12,630 --> 00:00:13,690 
of the decision tree model. 
5 
00:00:15,030 --> 00:00:16,920 
Next, we will use highlighting and 
6 
00:00:16,920 --> 00:00:19,820 
a scatter plot to analyze
the classification errors. 
7 
00:00:22,606 --> 00:00:24,969 
Let's begin. 
8 
00:00:24,969 --> 00:00:29,281 
First, let's open the Classification
workflow that we built in the previous 
9 
00:00:29,281 --> 00:00:30,033 
hands-on. 
10 
00:00:30,033 --> 00:00:34,252 
In the top-left of KNIME
is a KNIME Explorer. 
11 
00:00:34,252 --> 00:00:36,780 
Double-click on the Classification,
under LOCAL. 
12 
00:00:39,914 --> 00:00:43,802 
Next, we'll create a confusion
matrix to analyze the accuracy 
13 
00:00:43,802 --> 00:00:45,530 
of our decision tree model. 
14 
00:00:47,040 --> 00:00:50,380 
To do this,
we'll add the Scorer node to the canvas. 
15 
00:00:57,249 --> 00:01:02,340 
Connect the output of Decision Tree
Predictor, to the input of Scorer. 
16 
00:01:03,340 --> 00:01:04,480 
Double-click on Scorer. 
17 
00:01:06,500 --> 00:01:08,635 
We'll use the default values, so click OK. 
18 
00:01:11,580 --> 00:01:13,280 
Next, run the workflow. 
19 
00:01:14,420 --> 00:01:15,810 
Now, let's view the confusion matrix. 
20 
00:01:20,998 --> 00:01:23,570 
I can see the accuracy as 80.282%. 
21 
00:01:23,570 --> 00:01:30,651 
At the top, I can see that it
accurately predicted 76 values for 
22 
00:01:30,651 --> 00:01:36,205 
humidity_low and 95 for humidity_not_low. 
23 
00:01:36,205 --> 00:01:42,035 
It inaccurately predicted 24 values for
humidty_low and 
24 
00:01:42,035 --> 00:01:48,242 
18 for humidty_not_low,
for an error of 19.718%. 
25 
00:01:48,242 --> 00:01:52,225 
Close this. 
26 
00:01:52,225 --> 00:01:55,875 
Next, we use an interactive table to
look at the values that were incorrectly 
27 
00:01:55,875 --> 00:01:56,500 
predicted. 
28 
00:01:57,730 --> 00:01:59,990 
We'll add the Interactive Table
to the canvas. 
29 
00:02:08,767 --> 00:02:13,872 
We'll connect this to the output
of Decision Tree Predictor, 
30 
00:02:13,872 --> 00:02:16,870 
run the workflow and view the table. 
31 
00:02:23,262 --> 00:02:27,810 
The right two columns have the real value
for low_humidity and the prediction. 
32 
00:02:28,965 --> 00:02:32,150 
For some of these rows, we can tell
that the prediction was not correct. 
33 
00:02:33,700 --> 00:02:39,119 
For example, in row ten, And in row 17. 
34 
00:02:43,800 --> 00:02:47,707 
Let's leave the table view open,
and go back to the workflow. 
35 
00:02:47,707 --> 00:02:51,415 
Next, we'll add the Scatter Plot
nodes to the workflow. 
36 
00:03:00,185 --> 00:03:02,790 
Connect this to the output
of Decision Tree Predictor. 
37 
00:03:06,169 --> 00:03:09,900 
Execute the workflow, and
view the scatter plot. 
38 
00:03:13,413 --> 00:03:17,840 
We'll select row 17, and
choose, Hilite Selected. 
39 
00:03:19,720 --> 00:03:20,780 
Go back to the scatter plot, 
40 
00:03:20,780 --> 00:03:25,310 
and we see this particular value
is highlighted in the plot. 
41 
00:03:26,930 --> 00:03:28,960 
We could choose another
row from the table, and 
42 
00:03:28,960 --> 00:03:31,570 
highlight it again to see
its place in the plot. 
43 
00:03:32,730 --> 00:03:33,730 
Let's choose row ten. 
44 
00:03:36,669 --> 00:03:40,083 
We can do this for
values that were incorrectly predicted, 
45 
00:03:40,083 --> 00:03:42,620 
to find any patterns for further analysis. 
1 
00:00:00,990 --> 00:00:04,970 
In this activity we will use Spark
to evaluate our decision tree. 
2 
00:00:04,970 --> 00:00:07,820 
First, we load
the classification predictions, 
3 
00:00:07,820 --> 00:00:10,170 
created during the last
Spark hands on activity. 
4 
00:00:11,180 --> 00:00:14,320 
We then compute the accuracy
of these predictions. 
5 
00:00:14,320 --> 00:00:16,000 
And then, generate a confusion matrix. 
6 
00:00:18,050 --> 00:00:19,480 
Let's begin. 
7 
00:00:19,480 --> 00:00:22,065 
First, let's open the model
evaluation notebook. 
8 
00:00:24,630 --> 00:00:28,320 
Next, let's execute the first
cell to load the classes. 
9 
00:00:29,460 --> 00:00:32,740 
Then, execute the second cell
to load the predictions we saved 
10 
00:00:32,740 --> 00:00:33,950 
during the previous hands on. 
11 
00:00:35,760 --> 00:00:38,580 
We could then complete
the accuracy of these predictions 
12 
00:00:38,580 --> 00:00:41,539 
by using a multi-class
classification evaluator. 
13 
00:00:42,590 --> 00:00:50,088 
Let's enter evaluator =
MulticlassClassificationEvaluator(labelCo- 
14 
00:00:50,088 --> 00:00:55,124 
l="label", predictionCol="prediction") and 
15 
00:00:55,124 --> 00:01:00,505 
finally metricName="precision" and
execute this. 
16 
00:01:04,253 --> 00:01:08,605 
We can then compute the accuracy by
calling evaluate on the evaluator. 
17 
00:01:08,605 --> 00:01:13,384 
We'll enter
evaluator.evaluate[predictions]. 
18 
00:01:15,877 --> 00:01:18,924 
This says that the accuracy is about 81%. 
19 
00:01:19,940 --> 00:01:24,095 
Next, let's use multi-class metrics
to compute a confusion matrix. 
20 
00:01:24,095 --> 00:01:28,485 
Multi-class metrics takes
an RDD of numbers, and 
21 
00:01:28,485 --> 00:01:31,759 
our data is currently in a data frame. 
22 
00:01:31,759 --> 00:01:36,457 
We can access the RDD of the underlying
data frame by using the RDD 
23 
00:01:36,457 --> 00:01:38,549 
attribute of predictions. 
24 
00:01:38,549 --> 00:01:44,184 
If we look at predictions.rdd.take(2)
we see the RDD 
25 
00:01:44,184 --> 00:01:50,520 
is the RDD of rows, or
each row has a prediction and label. 
26 
00:01:50,520 --> 00:01:54,660 
However multi-class metrics
wants an RDD of numbers. 
27 
00:01:54,660 --> 00:01:56,046 
We could do this using a map. 
28 
00:01:56,046 --> 00:02:01,860 
We'll enter predictions.rdd.map and
we'll use a key word, 
29 
00:02:01,860 --> 00:02:06,010 
tuple, and we'll look at the first
two elements in this RDD. 
30 
00:02:06,010 --> 00:02:06,510 
Run this. 
31 
00:02:09,137 --> 00:02:11,740 
We can see now this RDD is just numbers. 
32 
00:02:12,810 --> 00:02:17,058 
Now we'll create a new instance of
a multiclass metrics using this RDD. 
33 
00:02:17,058 --> 00:02:24,275 
Metrics =
MulticlassMetrics(predrictions.rdd.map(tu- 
34 
00:02:24,275 --> 00:02:25,162 
ple)). 
35 
00:02:28,149 --> 00:02:32,667 
We can then display
the confusion matrix by running 
36 
00:02:32,667 --> 00:02:38,780 
metrics.confusionMatrix().toArray().trans-
pose(). 
37 
00:02:38,780 --> 00:02:45,674 
We can see these results are similar
to the confusion matrix in nine. 
1 
00:00:01,000 --> 00:00:05,150 
Generalization and overfitting are very
important concepts in machine learning. 
2 
00:00:05,150 --> 00:00:07,390 
We will cover them in
the next three lectures. 
3 
00:00:08,690 --> 00:00:13,660 
After this video you will be able
to define what overfitting is, 
4 
00:00:13,660 --> 00:00:17,690 
describe how overfitting is
related to generalization, and 
5 
00:00:17,690 --> 00:00:19,799 
explain why overfitting should be avoided. 
6 
00:00:21,140 --> 00:00:25,860 
Before we look at generalization and
overfitting, let's first define some terms 
7 
00:00:25,860 --> 00:00:28,710 
that we will need to know to
discuss errors in classification. 
8 
00:00:30,140 --> 00:00:35,400 
Recall that a machine learning model
maps the input it receives to an output. 
9 
00:00:35,400 --> 00:00:39,760 
For a classification model, the model's
output is the predicted class label for 
10 
00:00:39,760 --> 00:00:43,580 
the input variables and
the true class label is the target. 
11 
00:00:45,210 --> 00:00:48,640 
Then if the classifier predicts
the correct classes label for 
12 
00:00:48,640 --> 00:00:51,610 
a sample, that is a success. 
13 
00:00:51,610 --> 00:00:55,370 
If the predicted class label is
different from the true class label, 
14 
00:00:55,370 --> 00:00:56,530 
then that is an error. 
15 
00:00:57,880 --> 00:01:02,950 
The error rate, then, is the percentage
of errors made over the entire data set. 
16 
00:01:02,950 --> 00:01:07,330 
That is, it is the number of errors
divided by the total number of samples 
17 
00:01:07,330 --> 00:01:07,950 
in a data set. 
18 
00:01:08,980 --> 00:01:13,260 
Error rate is also known as
misclassification rate, or simply error. 
19 
00:01:14,690 --> 00:01:19,160 
In our lesson on classification we discuss
that there is a training phase in which 
20 
00:01:19,160 --> 00:01:24,910 
the model is built, and a testing phase in
which the model is applied to new data. 
21 
00:01:24,910 --> 00:01:30,070 
The model is built using training data and
evaluated on test data. 
22 
00:01:30,070 --> 00:01:33,760 
The training and
test data are two different data sets. 
23 
00:01:33,760 --> 00:01:37,810 
The goal in building a machine learning
model is to have the model perform well 
24 
00:01:37,810 --> 00:01:41,450 
on training, as well as test data. 
25 
00:01:41,450 --> 00:01:46,290 
Error rate, or simply error, on the
training data is refered to as training 
26 
00:01:46,290 --> 00:01:50,960 
error, and the error on test data
is referred to as test error. 
27 
00:01:52,030 --> 00:01:56,120 
The error on the test data is
an indication of how well the classifier 
28 
00:01:56,120 --> 00:01:57,560 
will perform on new data. 
29 
00:01:58,690 --> 00:02:01,210 
This is known as generalization. 
30 
00:02:01,210 --> 00:02:05,180 
Generalization refers to how well
your model performs on new data, 
31 
00:02:05,180 --> 00:02:07,240 
that is data not used to train the model. 
32 
00:02:08,260 --> 00:02:11,530 
You want your model to
generalize well to new data. 
33 
00:02:11,530 --> 00:02:15,200 
If your model generalizes well, then
it will perform well on data sets that 
34 
00:02:15,200 --> 00:02:18,210 
are similar in structure
to the training data, but 
35 
00:02:18,210 --> 00:02:21,330 
doesn't contain exactly the same
samples as in the training set. 
36 
00:02:22,700 --> 00:02:27,280 
Since the test error indicates how well
your model generalizes to new data, 
37 
00:02:27,280 --> 00:02:30,330 
note that the test error is also
called generalization error. 
38 
00:02:31,780 --> 00:02:35,280 
A related concept to
generalization is overfitting. 
39 
00:02:35,280 --> 00:02:37,810 
If your model has very
low training error but 
40 
00:02:37,810 --> 00:02:41,290 
high generalization error,
then it is overfitting. 
41 
00:02:41,290 --> 00:02:45,680 
This means that the model has learned to
model the noise in the training data, 
42 
00:02:45,680 --> 00:02:48,250 
instead of learning the underlying
structure of the data. 
43 
00:02:49,530 --> 00:02:52,180 
These plots illustrate what
happens when a model overfits. 
44 
00:02:53,190 --> 00:02:55,470 
Training samples are shown as points, and 
45 
00:02:55,470 --> 00:03:00,340 
the input to output mapping that the model
has learned is indicated as a curve. 
46 
00:03:00,340 --> 00:03:04,260 
The plot on the left shows that the model
has learned the underlying structure of 
47 
00:03:04,260 --> 00:03:09,530 
the data, as the curve follows
the trend of the sample point as well. 
48 
00:03:09,530 --> 00:03:10,580 
The plot on the right, 
49 
00:03:10,580 --> 00:03:14,640 
however, shows that the model has learned
to model the noise in a data set. 
50 
00:03:15,640 --> 00:03:18,640 
The model tries to capture
every sample point, 
51 
00:03:18,640 --> 00:03:21,210 
instead of the general trend
of the samples together. 
52 
00:03:22,220 --> 00:03:23,220 
The training error and 
53 
00:03:23,220 --> 00:03:26,440 
the generalization error are plotted
together, during model training. 
54 
00:03:28,400 --> 00:03:32,000 
What is the connection between
overfitting and generalization? 
55 
00:03:32,000 --> 00:03:35,820 
A model that overfits will not
generalize well to new data. 
56 
00:03:35,820 --> 00:03:39,525 
So the model will do well on just
the data it was trained on, but 
57 
00:03:39,525 --> 00:03:42,410 
given a new data set,
it will perform poorly. 
58 
00:03:43,420 --> 00:03:46,650 
A classifier that performs well
on just the training data set 
59 
00:03:46,650 --> 00:03:48,270 
will not be very useful. 
60 
00:03:48,270 --> 00:03:52,370 
So it is essential that the goal of good
generalization performance is kept in mind 
61 
00:03:52,370 --> 00:03:53,170 
when building a model. 
62 
00:03:54,890 --> 00:03:57,380 
A problem related to
overfitting is underfitting. 
63 
00:03:58,890 --> 00:04:03,540 
Overfitting occurs when the model is
fitting to the noise in the training data. 
64 
00:04:03,540 --> 00:04:06,770 
This results in low training error and
high test error. 
65 
00:04:08,330 --> 00:04:09,830 
Underfitting on the other hand, 
66 
00:04:09,830 --> 00:04:13,940 
occurs when the model has not
learned the structure of the data. 
67 
00:04:13,940 --> 00:04:16,930 
This results in high training error and
high test error. 
68 
00:04:18,590 --> 00:04:20,164 
Both are undesirable, 
69 
00:04:20,164 --> 00:04:24,658 
since both mean that the model will
not generalize well to new data. 
70 
00:04:24,658 --> 00:04:28,903 
Overfitting generally occurs when
a model is too complex, that is, 
71 
00:04:28,903 --> 00:04:33,306 
it has too many parameters relative
to the number of training samples. 
72 
00:04:33,306 --> 00:04:37,911 
So to avoid overfitting, the model needs
to be kept as simple as possible, and yet 
73 
00:04:37,911 --> 00:04:42,400 
still solve the input/output mapping for
the given data set. 
74 
00:04:42,400 --> 00:04:43,880 
We will discuss methods for 
75 
00:04:43,880 --> 00:04:46,470 
avoiding overfitting in
the next couple of lectures. 
76 
00:04:47,940 --> 00:04:53,220 
In summary, overfitting is when your model
has learned the noise in the training data 
77 
00:04:53,220 --> 00:04:55,870 
instead of the underlying
structure of the data. 
78 
00:04:55,870 --> 00:05:00,353 
You want to avoid overfitting so that your
model will generalize well to new data. 
1 
00:00:00,970 --> 00:00:03,490 
How do you evaluate
your model performance? 
2 
00:00:03,490 --> 00:00:06,870 
In this lecture, we will look at
different metrics that can be used to 
3 
00:00:06,870 --> 00:00:09,360 
evaluate the performance of
your classification model. 
4 
00:00:10,470 --> 00:00:13,280 
After this video, you will be able to 
5 
00:00:13,280 --> 00:00:17,480 
discuss how performance metrics
can be used to evaluate models. 
6 
00:00:17,480 --> 00:00:22,270 
Name three model evaluation metrics, and
explain why accuracy may be misleading. 
7 
00:00:24,110 --> 00:00:28,090 
For the classification task, an error
occurs when the model's prediction of 
8 
00:00:28,090 --> 00:00:32,370 
the class label is different
from the true class label. 
9 
00:00:32,370 --> 00:00:36,330 
We can also define the different types
of errors in classification depending on 
10 
00:00:36,330 --> 00:00:37,920 
the predicted and true labels. 
11 
00:00:38,930 --> 00:00:43,535 
Let's take the case with the task is
to predict whether a given animal is 
12 
00:00:43,535 --> 00:00:44,627 
a mammal or not. 
13 
00:00:44,627 --> 00:00:50,596 
This is a binary classification task
with the class label being either yes, 
14 
00:00:50,596 --> 00:00:54,830 
indicating mammal, or
no indicating non-mammal. 
15 
00:00:54,830 --> 00:00:57,450 
Then the different types
of errors are as follows. 
16 
00:00:58,600 --> 00:01:03,693 
If the true label is yes and
the predicted label is yes, 
17 
00:01:03,693 --> 00:01:08,367 
then this is a true positive,
abbreviated as TP. 
18 
00:01:08,367 --> 00:01:12,380 
This is the case where the label is
correctly predicted as positive. 
19 
00:01:14,270 --> 00:01:18,500 
If the true label is no and
the predicted label is no, 
20 
00:01:18,500 --> 00:01:22,640 
then this is a true negative,
abbreviated as TN. 
21 
00:01:24,080 --> 00:01:27,760 
This is the case where the label is
correctly predicted as negative. 
22 
00:01:29,910 --> 00:01:34,120 
If the true label is no and
the predicted label is yes, 
23 
00:01:34,120 --> 00:01:39,380 
then this is a false positive,
abbreviated as FP. 
24 
00:01:39,380 --> 00:01:43,800 
This is the case with the label is
incorrectly predicted as positive, 
25 
00:01:43,800 --> 00:01:44,660 
when it should be negative. 
26 
00:01:46,770 --> 00:01:48,720 
If the true label is yes and 
27 
00:01:48,720 --> 00:01:53,830 
the predicted label is no, then this
is a false negative abbreviated as FN. 
28 
00:01:54,950 --> 00:01:59,380 
This is the case where the label is
incorrectly predicted as negative, 
29 
00:01:59,380 --> 00:02:00,630 
when it should be positive. 
30 
00:02:01,990 --> 00:02:06,400 
These definitions can take a while to sink
in, so feel free to hit the pause button 
31 
00:02:06,400 --> 00:02:09,080 
and replay button several times
here to review this part. 
32 
00:02:11,110 --> 00:02:14,940 
These four different types of errors
are used in calculating many evaluation 
33 
00:02:14,940 --> 00:02:17,110 
metrics for classifiers. 
34 
00:02:17,110 --> 00:02:21,270 
The most commonly used evaluation
metric is the accuracy rate, or 
35 
00:02:21,270 --> 00:02:23,410 
accuracy for short. 
36 
00:02:23,410 --> 00:02:26,880 
For classication,
accuracy is calculated as the number of 
37 
00:02:26,880 --> 00:02:30,160 
correct predictions divided by
the total number of predictions. 
38 
00:02:31,300 --> 00:02:34,770 
Note that the number of correct
predictions is the sum of the true 
39 
00:02:34,770 --> 00:02:39,660 
positives, and the true negatives, since
the true and predicted labels match for 
40 
00:02:39,660 --> 00:02:41,160 
those cases. 
41 
00:02:41,160 --> 00:02:44,640 
The accuracy rate is an intuitive
way to measure the performance 
42 
00:02:44,640 --> 00:02:45,780 
of a classification model. 
43 
00:02:47,410 --> 00:02:51,540 
Model performance can also be
expressed in terms of error rate. 
44 
00:02:51,540 --> 00:02:54,140 
Error rate is the opposite
of accuracy rate. 
45 
00:02:55,420 --> 00:03:00,290 
Let's look at an example to see how
accuracy and error rates are calculated. 
46 
00:03:00,290 --> 00:03:04,660 
The table on the left, lists the true
label along with the model's prediction 
47 
00:03:04,660 --> 00:03:06,760 
for a data set of ten samples. 
48 
00:03:08,130 --> 00:03:11,345 
First, let’s figure out
the number of true positives. 
49 
00:03:11,345 --> 00:03:15,930 
Recall that a true positive occurs when
the output is correctly predicted as 
50 
00:03:15,930 --> 00:03:17,250 
positive. 
51 
00:03:17,250 --> 00:03:21,820 
In other words the true label is yes,
and the model's prediction is yes. 
52 
00:03:21,820 --> 00:03:26,298 
In this example there are three true
positives as indicated by the red arrows. 
53 
00:03:26,298 --> 00:03:30,690 
So, TP=3, remember that value
as we'll need it later. 
54 
00:03:32,870 --> 00:03:36,000 
Now, let's figure out
the number of true negatives. 
55 
00:03:36,000 --> 00:03:40,860 
A true negative occurs when the output
is correctly predicted as negative. 
56 
00:03:40,860 --> 00:03:44,940 
In other words, the true label is no and
the model's prediction is no. 
57 
00:03:44,940 --> 00:03:48,719 
In this example there are four true
negatives as indicated by the green 
58 
00:03:48,719 --> 00:03:49,239 
arrows. 
59 
00:03:49,239 --> 00:03:53,160 
So TN = 4,
we'll need to remember this value as well. 
60 
00:03:55,330 --> 00:04:00,490 
Now we use the values for TP and
TN to calculate the accuracy rate. 
61 
00:04:00,490 --> 00:04:06,660 
Using the equation for accuracy rate,
we plug in three for TP and four for TN. 
62 
00:04:06,660 --> 00:04:10,270 
We get seven correct predictions for
the numerator. 
63 
00:04:10,270 --> 00:04:15,480 
The denominator is simply the total number
of samples in our data set, which is ten. 
64 
00:04:15,480 --> 00:04:18,201 
So the accuracy rate for 
65 
00:04:18,201 --> 00:04:24,063 
example is 7 out of 10 which is 0.7 or
70%. 
66 
00:04:24,063 --> 00:04:27,890 
The error rate is the exact
opposite of the accuracy rate. 
67 
00:04:27,890 --> 00:04:33,120 
To calculate the error rate, we simply
subtract the accuracy rate from 1. 
68 
00:04:33,120 --> 00:04:38,555 
For our example that is
1- 0.7 which is 0.3. 
69 
00:04:38,555 --> 00:04:43,771 
So the error rate for
this example is 0.3 or 30%. 
70 
00:04:43,771 --> 00:04:47,440 
There's a limitation with accuracy and 
71 
00:04:47,440 --> 00:04:50,370 
error rates when you have
a class imbalance problem. 
72 
00:04:51,740 --> 00:04:55,060 
This is when there are very few
samples of the class of interest, and 
73 
00:04:55,060 --> 00:04:56,710 
the majority are negative examples. 
74 
00:04:57,870 --> 00:05:02,540 
An example of this is identifying
if a tumor is cancerous or not. 
75 
00:05:02,540 --> 00:05:06,820 
What is of interest is identifying
samples with cancerous tumors, but 
76 
00:05:06,820 --> 00:05:11,200 
these positive cases where the tumor
is cancerous are very rare. 
77 
00:05:11,200 --> 00:05:15,110 
So, you end up with a very small
fraction of positive samples, and 
78 
00:05:15,110 --> 00:05:17,150 
most of the samples are negative. 
79 
00:05:17,150 --> 00:05:19,310 
Thus the name, class imbalance problem. 
80 
00:05:20,880 --> 00:05:24,260 
What could be the problem with using
accuracy for a class imbalance problem? 
81 
00:05:25,590 --> 00:05:30,130 
Consider the situation where only 3%
of the cases are cancerous tumors. 
82 
00:05:31,400 --> 00:05:35,480 
If the classification model
always predicts non-cancer, 
83 
00:05:35,480 --> 00:05:39,090 
it will have an accuracy rate of 97%, 
84 
00:05:39,090 --> 00:05:43,910 
since 97% of the samples will
have non-cancerous tumors. 
85 
00:05:43,910 --> 00:05:49,370 
But note that in this case, the model
fails to detect any cancer cases at all. 
86 
00:05:49,370 --> 00:05:52,600 
So the accuracy rate is
very misleading here. 
87 
00:05:52,600 --> 00:05:55,430 
You may think that your model is
performing very well with such 
88 
00:05:55,430 --> 00:05:56,740 
a high accuracy rate. 
89 
00:05:56,740 --> 00:06:02,190 
But in fact it cannot identify any of
the cases in the class of interest. 
90 
00:06:02,190 --> 00:06:05,860 
In these cases we need evaluation
metrics that can capture how 
91 
00:06:05,860 --> 00:06:09,680 
well the model classifies positive,
versus negative classes. 
92 
00:06:11,290 --> 00:06:14,942 
A pair of evaluations metrics that
are commonly used when there is a class 
93 
00:06:14,942 --> 00:06:18,220 
imbalance are precision and recall. 
94 
00:06:18,220 --> 00:06:22,950 
Precision is defined as the number of
true positives divided by the sum of 
95 
00:06:22,950 --> 00:06:25,640 
true positives and false positives. 
96 
00:06:25,640 --> 00:06:29,670 
In other words, it is the number of true
positives divided by the total number 
97 
00:06:29,670 --> 00:06:32,730 
of samples predicted as being positive. 
98 
00:06:34,060 --> 00:06:38,430 
Recall is defined as the number of
true positives divided by the sum of 
99 
00:06:38,430 --> 00:06:40,860 
true positives and false negatives. 
100 
00:06:40,860 --> 00:06:45,440 
It is the number of true positives
divided by the total number of samples, 
101 
00:06:45,440 --> 00:06:47,379 
actually belonging to the true class. 
102 
00:06:48,895 --> 00:06:52,050 
Here's an illustration that
shows precision and recall. 
103 
00:06:52,050 --> 00:06:57,320 
The selected elements indicated by the
green half circle are the true positives. 
104 
00:06:57,320 --> 00:07:01,240 
That is samples predicted as positive and
are actually positive. 
105 
00:07:02,670 --> 00:07:05,830 
The relevant elements indicated
by the green half circle and 
106 
00:07:05,830 --> 00:07:11,140 
the green half rectangle, are the true
positives, plus the false negatives. 
107 
00:07:11,140 --> 00:07:15,520 
That is samples that are actually
positive, but some are correctly predicted 
108 
00:07:15,520 --> 00:07:18,990 
as positive, and some are incorrectly
predicted as negative. 
109 
00:07:19,990 --> 00:07:24,140 
Recall then is the number of samples
correctly predicted as positive, 
110 
00:07:24,140 --> 00:07:27,250 
divided by all samples that
are actually positive. 
111 
00:07:29,140 --> 00:07:32,200 
The entire circle indicated
by the green half circle and 
112 
00:07:32,200 --> 00:07:37,000 
the pink half circle, are the true
positives plus the false positives. 
113 
00:07:37,000 --> 00:07:39,710 
That is samples that were
predicted as positive 
114 
00:07:39,710 --> 00:07:42,950 
although some were actually positive and
some were actually negative. 
115 
00:07:44,175 --> 00:07:48,530 
Then precision is the number of samples
correctly predicted as positive, 
116 
00:07:48,530 --> 00:07:52,670 
divided by the number of all
samples predicted as positive. 
117 
00:07:54,230 --> 00:07:57,790 
Precision is considered a measure
of exactness because it calculates 
118 
00:07:57,790 --> 00:08:00,710 
the percentage of samples
predicted as positive, 
119 
00:08:00,710 --> 00:08:02,420 
which are actually in a positive class. 
120 
00:08:03,460 --> 00:08:07,270 
Recall is considered a measure of
completeness, because it calculates 
121 
00:08:07,270 --> 00:08:10,940 
the percentage of positive samples
that the model correctly identified. 
122 
00:08:12,720 --> 00:08:15,950 
There is a trade off between precision and
recall. 
123 
00:08:15,950 --> 00:08:20,762 
A perfect precision score of one for
a class C means that every sample 
124 
00:08:20,762 --> 00:08:25,506 
predicted as belonging to class C,
does indeed belong to class C. 
125 
00:08:25,506 --> 00:08:29,674 
But this says nothing about the number of
samples from class C that were predicted 
126 
00:08:29,674 --> 00:08:30,480 
incorrectly. 
127 
00:08:31,550 --> 00:08:33,790 
A perfect recall score of one for 
128 
00:08:33,790 --> 00:08:38,710 
a class C, means that every sample
from class C was correctly labeled. 
129 
00:08:38,710 --> 00:08:41,870 
But this doesn't say anything
about how many other samples were 
130 
00:08:41,870 --> 00:08:44,850 
incorrectly labeled as
belonging to class C. 
131 
00:08:44,850 --> 00:08:46,640 
So they are used together. 
132 
00:08:46,640 --> 00:08:51,070 
For example, precision values can be
compared for a fixed value of recall or 
133 
00:08:51,070 --> 00:08:52,360 
vice versa. 
134 
00:08:52,360 --> 00:08:56,140 
The goal for classification is to
maximize both precision and recall. 
135 
00:08:57,870 --> 00:09:03,060 
Precision and recall can be combined into
a single metric called the F-measure. 
136 
00:09:03,060 --> 00:09:06,830 
The equation for that is 2 times
the product of precision and 
137 
00:09:06,830 --> 00:09:09,630 
recall divided by their sum. 
138 
00:09:09,630 --> 00:09:12,350 
There are different
versions of the F-measure. 
139 
00:09:12,350 --> 00:09:13,910 
The equation on this side is for 
140 
00:09:13,910 --> 00:09:18,860 
the F1 measure which is the most
commonly used variant of the F measure. 
141 
00:09:18,860 --> 00:09:22,990 
With the F1 measure, precision and
recall are equally weighted. 
142 
00:09:22,990 --> 00:09:26,245 
The F2 measure weights recall
higher than precision. 
143 
00:09:26,245 --> 00:09:31,410 
And the F0.5 measure weights
precision higher than recall. 
144 
00:09:32,410 --> 00:09:35,560 
The value for
the F1 measure ranges from zero to one, 
145 
00:09:35,560 --> 00:09:38,720 
with higher values giving better
classification performance. 
146 
00:09:40,160 --> 00:09:42,580 
In summary, there are several metrics for 
147 
00:09:42,580 --> 00:09:45,990 
evaluating the performance
of a classification model. 
148 
00:09:45,990 --> 00:09:49,170 
They are defined in terms of
the types of errors you can get in 
149 
00:09:49,170 --> 00:09:50,390 
a classification problem. 
150 
00:09:51,450 --> 00:09:56,348 
We covered some of the most commonly
used evaluation metrics in this lecture, 
151 
00:09:56,348 --> 00:10:00,960 
namely accuracy and error rates,
precision and recall and F1 measure. 
1 
00:00:00,810 --> 00:00:05,170 
In this lecture, we will discuss how
overfitting occurs with decision trees and 
2 
00:00:05,170 --> 00:00:06,210 
how it can be avoided. 
3 
00:00:07,300 --> 00:00:08,160 
After this video, 
4 
00:00:08,160 --> 00:00:14,120 
you will be able to discuss overfitting
in the context of decision tree models. 
5 
00:00:14,120 --> 00:00:17,496 
Explain how overfitting is addressed
in decision tree induction. 
6 
00:00:17,496 --> 00:00:21,370 
Define pre-pruning and post-pruning. 
7 
00:00:21,370 --> 00:00:25,260 
In our lecture on decision trees, we
discussed that during the construction of 
8 
00:00:25,260 --> 00:00:30,830 
a decision tree, also referred to as
tree induction, the tree repeatedly 
9 
00:00:30,830 --> 00:00:35,000 
splits the data in a node in order to
get successively paired subsets of data. 
10 
00:00:36,320 --> 00:00:40,810 
Note that a decision tree classifier can
potentially expand its nodes until it can 
11 
00:00:40,810 --> 00:00:44,230 
perfectly classify samples
in the training data. 
12 
00:00:44,230 --> 00:00:48,280 
But if the tree grows nodes to fit
the noise in the training data, 
13 
00:00:48,280 --> 00:00:51,370 
then it will not classify
a new sample well. 
14 
00:00:51,370 --> 00:00:55,450 
This is because the tree has partitioned
the input space according to the noise in 
15 
00:00:55,450 --> 00:00:59,410 
the data instead of to
the true structure of a data. 
16 
00:00:59,410 --> 00:01:00,825 
In other words, it has overfit. 
17 
00:01:02,710 --> 00:01:05,580 
How can overfitting be
avoided in decision trees? 
18 
00:01:05,580 --> 00:01:07,220 
There are two ways. 
19 
00:01:07,220 --> 00:01:10,510 
One is to stop growing the tree
before the tree is fully grown 
20 
00:01:10,510 --> 00:01:12,590 
to perfectly fit the training data. 
21 
00:01:12,590 --> 00:01:15,890 
This is referred to as pre-pruning. 
22 
00:01:15,890 --> 00:01:19,770 
The other way to avoid overfitting in
decision trees is to grow the tree to its 
23 
00:01:19,770 --> 00:01:24,860 
maximum size and then prune the tree
back by removing parts of the tree. 
24 
00:01:24,860 --> 00:01:28,020 
This is referred to as post-pruning. 
25 
00:01:28,020 --> 00:01:32,300 
In general, overfitting occurs
because the model is too complex. 
26 
00:01:32,300 --> 00:01:34,060 
For a decision tree model, 
27 
00:01:34,060 --> 00:01:38,250 
model complexity is determined by
the number of nodes in the tree. 
28 
00:01:38,250 --> 00:01:43,300 
Addressing overfitting in decision trees
means controlling the number of nodes. 
29 
00:01:43,300 --> 00:01:46,920 
Both methods of pruning control
the growth of the tree and consequently, 
30 
00:01:46,920 --> 00:01:48,880 
the complexity of the resulting model. 
31 
00:01:50,380 --> 00:01:55,270 
With pre-pruning, the idea is to stop tree
induction before a fully grown tree is 
32 
00:01:55,270 --> 00:01:58,540 
built that perfectly
fits the training data. 
33 
00:01:58,540 --> 00:02:03,320 
To do this, restrictive stopping
conditions for growing nodes must be used. 
34 
00:02:03,320 --> 00:02:08,121 
For example, a nose stops expanding if
the number of samples in the node is less 
35 
00:02:08,121 --> 00:02:10,940 
than some minimum threshold. 
36 
00:02:10,940 --> 00:02:14,954 
Another example is to stop expanding
a note if the improvement in the impurity 
37 
00:02:14,954 --> 00:02:17,190 
measure falls below a certain threshold. 
38 
00:02:18,480 --> 00:02:22,440 
In post-pruning,
the tree is grown to its maximum size, 
39 
00:02:22,440 --> 00:02:27,280 
then the tree is pruned by removing
nodes using a bottom up approach. 
40 
00:02:27,280 --> 00:02:31,250 
That is, the tree is trimmed
starting with the leaf nodes. 
41 
00:02:31,250 --> 00:02:34,900 
The pruning is done by replacing
a subtree with a leaf node if 
42 
00:02:34,900 --> 00:02:38,140 
this improves the generalization error,
or if there is no 
43 
00:02:38,140 --> 00:02:41,580 
change to the generalization
error with this replacement. 
44 
00:02:41,580 --> 00:02:45,590 
In other words, if removing a subtree
does not have a negative effect 
45 
00:02:45,590 --> 00:02:49,240 
on the generalization error,
then the nodes in that subtree only 
46 
00:02:49,240 --> 00:02:52,730 
add to the complexity of the tree,
and not to its overall performance. 
47 
00:02:52,730 --> 00:02:54,058 
So those nodes should be removed. 
48 
00:02:54,058 --> 00:02:59,290 
In practice,
post-pruning tends to give better results. 
49 
00:02:59,290 --> 00:03:03,250 
This is because pruning decisions are
based on information from the full tree. 
50 
00:03:03,250 --> 00:03:08,510 
Pre-pruning, on the other hand, may stop
the tree growing process prematurely. 
51 
00:03:08,510 --> 00:03:12,951 
However, post-pruning is more
computationally expensive since the tree 
52 
00:03:12,951 --> 00:03:15,040 
has to be expanded to its full size. 
53 
00:03:16,490 --> 00:03:21,720 
In summary, to address overfitting in
decision trees, tree pruning is used. 
54 
00:03:21,720 --> 00:03:26,030 
There are two pruning methods,
pre-pruning and post-pruning. 
55 
00:03:26,030 --> 00:03:28,880 
Both methods control
the complexity of the tree model. 
1 
00:00:00,980 --> 00:00:04,400 
In this lecture, we will discuss
what a validation set is and 
2 
00:00:04,400 --> 00:00:07,610 
how it relates to overfitting and
model performance evaluation. 
3 
00:00:09,150 --> 00:00:13,300 
After this video, you will be able to
describe how validation sets can be 
4 
00:00:13,300 --> 00:00:15,450 
used to avoid overfitting. 
5 
00:00:15,450 --> 00:00:19,940 
Articulate how training,
validation, and test sets are used. 
6 
00:00:19,940 --> 00:00:22,539 
And list three ways that
validation can be performed. 
7 
00:00:23,920 --> 00:00:25,600 
In our lesson on classification, 
8 
00:00:25,600 --> 00:00:29,570 
we discussed that there is a training
phase in which the model is built, and 
9 
00:00:29,570 --> 00:00:33,690 
a testing phase in which
the model is applied to new data. 
10 
00:00:33,690 --> 00:00:38,580 
The model is built using training data and
evaluated on test data. 
11 
00:00:38,580 --> 00:00:42,290 
The training and
test data are two different datasets. 
12 
00:00:42,290 --> 00:00:46,490 
The goal in building a machine learning
model is to have the model perform well 
13 
00:00:46,490 --> 00:00:51,110 
on the training set, as well as generalize
well on new data in the test set. 
14 
00:00:52,750 --> 00:00:57,690 
Recall that a model that overfits
does not generalize well to new data. 
15 
00:00:57,690 --> 00:01:03,630 
Recall also that overfitting generally
occurs when a model is too complex. 
16 
00:01:03,630 --> 00:01:07,110 
So to have a model with good
generalization performance, 
17 
00:01:07,110 --> 00:01:11,550 
model training has to stop before
the model gets too complex. 
18 
00:01:11,550 --> 00:01:13,170 
How do you determine
when this should occur? 
19 
00:01:14,730 --> 00:01:19,184 
A validation set can be used to guide the
training process to avoid overfitting and 
20 
00:01:19,184 --> 00:01:22,450 
deliver good generalization performance. 
21 
00:01:22,450 --> 00:01:26,550 
We have discussed having a training
set and a separate test set. 
22 
00:01:26,550 --> 00:01:29,000 
The training set is used
to build a model and 
23 
00:01:29,000 --> 00:01:31,900 
the test set is used to see how
the model performs a new data. 
24 
00:01:32,960 --> 00:01:37,450 
Now we want to further divide up
the training data into a training set and 
25 
00:01:37,450 --> 00:01:39,200 
a validation set. 
26 
00:01:39,200 --> 00:01:42,680 
The training set is used to
train the model as before and 
27 
00:01:42,680 --> 00:01:46,780 
the validation set is used to determine
when to stop training the model 
28 
00:01:46,780 --> 00:01:50,800 
to avoid overfitting, in order to get
the best generalization performance. 
29 
00:01:52,060 --> 00:01:55,010 
The idea is to look at the errors
on both training set and 
30 
00:01:55,010 --> 00:01:58,500 
validation set during model
training as shown here. 
31 
00:01:58,500 --> 00:02:02,670 
The orange solid line on the plot
is the training error and 
32 
00:02:02,670 --> 00:02:05,960 
the green line is the validation error. 
33 
00:02:05,960 --> 00:02:09,760 
We see that as model building
progresses along the x-axis, 
34 
00:02:09,760 --> 00:02:11,890 
the number of nodes increases. 
35 
00:02:11,890 --> 00:02:15,280 
That is the complexity
of the model increases. 
36 
00:02:15,280 --> 00:02:21,200 
We can see that as the model complexity
increases, the training error decreases. 
37 
00:02:21,200 --> 00:02:25,140 
On the other hand, the validation
error initially decreases but 
38 
00:02:25,140 --> 00:02:26,370 
then starts to increase. 
39 
00:02:27,900 --> 00:02:32,960 
When the validation error increases, this
indicates that the model is overfitting, 
40 
00:02:32,960 --> 00:02:35,800 
resulting in decreased
generalization performance. 
41 
00:02:37,640 --> 00:02:40,950 
This can be used to determine
when to stop training. 
42 
00:02:40,950 --> 00:02:45,390 
Where validation error starts to increase
is when you get the best generalization 
43 
00:02:45,390 --> 00:02:48,340 
performance, so
training should stop there. 
44 
00:02:48,340 --> 00:02:52,050 
This method of using a validation set
to determine when to stop training 
45 
00:02:52,050 --> 00:02:54,610 
is referred to as model selection 
46 
00:02:54,610 --> 00:02:58,610 
since you're selecting one from
many of varying complexities. 
47 
00:02:58,610 --> 00:03:02,831 
Note that this was illustrated for
a decision tree classifier, but 
48 
00:03:02,831 --> 00:03:07,135 
the same method can be applied to
any type of machine learning model. 
49 
00:03:07,135 --> 00:03:12,510 
There are several ways to create and use
the validation set to avoid overfitting. 
50 
00:03:12,510 --> 00:03:16,482 
The different methods are holdout method, 
51 
00:03:16,482 --> 00:03:21,313 
random subsampling,
k-fold cross-validation, 
52 
00:03:21,313 --> 00:03:25,300 
and leave-one-out cross-validation. 
53 
00:03:25,300 --> 00:03:29,050 
The first way to use a validation
set is the holdout method. 
54 
00:03:29,050 --> 00:03:31,900 
This describes the scenario
that we have been discussing, 
55 
00:03:31,900 --> 00:03:36,270 
where part of the training data
is reserved as a validation set. 
56 
00:03:36,270 --> 00:03:38,300 
The validation set is
then the holdout set. 
57 
00:03:39,300 --> 00:03:42,950 
Errors on the training set and the holdout
set are calculated at each step 
58 
00:03:42,950 --> 00:03:47,080 
during model training and
plotted together as we've seen before. 
59 
00:03:47,080 --> 00:03:50,950 
And the lowest error on the holdout
set is when training should stop. 
60 
00:03:50,950 --> 00:03:54,240 
This is the just the process that
we have described here before. 
61 
00:03:54,240 --> 00:03:56,670 
There's some limitations to
the holdout method however. 
62 
00:03:57,710 --> 00:04:01,970 
First, since some samples are reserved for
the holdout validation set, 
63 
00:04:01,970 --> 00:04:05,580 
the training set now has less data
than it originally started out with. 
64 
00:04:06,770 --> 00:04:11,810 
Secondly, if the training and holdout sets
do not have the same data distributions, 
65 
00:04:11,810 --> 00:04:14,140 
then the results will be misleading. 
66 
00:04:14,140 --> 00:04:18,750 
For example, if the training data has
many more samples of one class and 
67 
00:04:18,750 --> 00:04:22,540 
the holdout dataset has many
more samples of another class. 
68 
00:04:22,540 --> 00:04:27,100 
The next method for using
a validation set is repeated holdout. 
69 
00:04:27,100 --> 00:04:28,260 
As the name implies, 
70 
00:04:28,260 --> 00:04:32,220 
this is essentially repeating
the holdout method several times. 
71 
00:04:32,220 --> 00:04:37,170 
With each iteration, samples are randomly
selected from the original training data 
72 
00:04:37,170 --> 00:04:40,040 
to create the holdout validation set. 
73 
00:04:40,040 --> 00:04:43,740 
This is repeated several times with
different training and validation sets. 
74 
00:04:44,810 --> 00:04:48,010 
Then the iterates on the holdout set for
the different iterations 
75 
00:04:48,010 --> 00:04:53,150 
are averaged together to get the overall
iterate for model selection. 
76 
00:04:53,150 --> 00:04:57,420 
A potential problem with repeated holdout
is that you could end up with some samples 
77 
00:04:57,420 --> 00:04:59,570 
being used more than others for training. 
78 
00:05:00,640 --> 00:05:05,470 
Since a sample can be used for either
testing or training any number of times, 
79 
00:05:05,470 --> 00:05:10,050 
some samples may be put in the training
set more times than other samples. 
80 
00:05:10,050 --> 00:05:14,250 
So you might end up with some
samples being overrepresented while 
81 
00:05:14,250 --> 00:05:17,430 
other samples are underrepresented
in training or testing. 
82 
00:05:19,520 --> 00:05:24,154 
A way to improve on the repeated
holdout method is use cross-validation. 
83 
00:05:24,154 --> 00:05:26,628 
Cross-validation works as follows. 
84 
00:05:26,628 --> 00:05:31,440 
Segment the data into k number
of disjoint partitions. 
85 
00:05:31,440 --> 00:05:36,420 
During each iteration, one partition
is used as the validation set. 
86 
00:05:36,420 --> 00:05:38,690 
Repeat the process k times. 
87 
00:05:38,690 --> 00:05:42,230 
Each time using a different partition for
validation. 
88 
00:05:42,230 --> 00:05:45,880 
So each partition is used for
validation exactly once. 
89 
00:05:45,880 --> 00:05:48,230 
This is illustrated in this figure. 
90 
00:05:48,230 --> 00:05:52,520 
In the fist iteration, the first
partition, specified in green, is used for 
91 
00:05:52,520 --> 00:05:53,960 
validation. 
92 
00:05:53,960 --> 00:05:56,950 
In the second iteration,
the second partition is used for 
93 
00:05:56,950 --> 00:05:58,160 
validation and so on. 
94 
00:05:59,300 --> 00:06:03,920 
The overall validation error is calculated
by averaging the validation errors for 
95 
00:06:03,920 --> 00:06:04,980 
all k iterations. 
96 
00:06:06,040 --> 00:06:10,490 
The model with the smallest average
validation error then is selected. 
97 
00:06:10,490 --> 00:06:15,530 
The process we just described is
referred to as k-fold cross-validation. 
98 
00:06:15,530 --> 00:06:19,390 
This is a very commonly used approach
to model selection in practice. 
99 
00:06:20,440 --> 00:06:25,405 
This approach gives you a more structured
way to divide available data up between 
100 
00:06:25,405 --> 00:06:30,371 
training and validation datasets and
provides a way to overcome the variability 
101 
00:06:30,371 --> 00:06:35,134 
in performance that you can get when
using a single partitioning of the data. 
102 
00:06:35,134 --> 00:06:39,401 
Leave-one-out cross-validation
is a special case of k-fold 
103 
00:06:39,401 --> 00:06:44,220 
cross-validation where k equals N,
where N is the size of your dataset. 
104 
00:06:45,260 --> 00:06:49,860 
Here, for each iteration the validation
set has exactly one sample. 
105 
00:06:49,860 --> 00:06:53,240 
So the model is trained to
using N minus one samples and 
106 
00:06:53,240 --> 00:06:55,920 
is validated on the remaining sample. 
107 
00:06:55,920 --> 00:07:01,600 
The rest of the process works the same
way as regular k-fold cross-validation. 
108 
00:07:01,600 --> 00:07:06,391 
Note that cross-validation is often
abbreviated CV and leave-one-out 
109 
00:07:06,391 --> 00:07:11,340 
cross-validation is in abbreviated
L-O-O-C-V and pronounced LOOCV. 
110 
00:07:13,550 --> 00:07:17,990 
We have described several ways to use
a validation set to address overfitting. 
111 
00:07:17,990 --> 00:07:21,750 
Error on the validation set is used
to determine when to stop training so 
112 
00:07:21,750 --> 00:07:23,350 
that the model does not overfit. 
113 
00:07:24,400 --> 00:07:28,440 
Note that the validation error that comes
out of this process can also be used 
114 
00:07:28,440 --> 00:07:31,720 
to estimate generalization
performance of the model. 
115 
00:07:31,720 --> 00:07:32,780 
In other words, 
116 
00:07:32,780 --> 00:07:37,420 
the error on the validation set provides
an estimate of the error on the test set. 
117 
00:07:38,890 --> 00:07:40,570 
With the addition of the validation set, 
118 
00:07:40,570 --> 00:07:44,460 
you really need three distinct
datasets when you build a model. 
119 
00:07:44,460 --> 00:07:45,660 
Let's review these datasets. 
120 
00:07:47,080 --> 00:07:51,560 
The training dataset is used to train the
model, that is to adjust the parameters of 
121 
00:07:51,560 --> 00:07:53,990 
the model to learn the input
to output mapping. 
122 
00:07:55,640 --> 00:07:59,710 
The validation dataset is used to
determine when training should stop 
123 
00:07:59,710 --> 00:08:01,370 
in order to avoid overfitting. 
124 
00:08:03,110 --> 00:08:07,490 
The test data set is used to evaluate
the performance of the model on new data. 
125 
00:08:09,640 --> 00:08:14,320 
Note that the test data set should never,
ever be used in any way to create or 
126 
00:08:14,320 --> 00:08:15,940 
tune the model. 
127 
00:08:15,940 --> 00:08:17,070 
It should not be used, for 
128 
00:08:17,070 --> 00:08:21,010 
example, in a cross-validation process
to determine when to stop training. 
129 
00:08:22,020 --> 00:08:26,680 
The test dataset must always remain
independent from model training and 
130 
00:08:26,680 --> 00:08:31,980 
remain untouched until the very end
when all training has been completed. 
131 
00:08:31,980 --> 00:08:36,467 
Note that in sampling the original dataset
to create the training, validation, and 
132 
00:08:36,467 --> 00:08:40,909 
test sets, all datasets must contain the
same distribution of the target classes. 
133 
00:08:40,909 --> 00:08:46,362 
For example, if in the original dataset,
70% of the samples belong to one class and 
134 
00:08:46,362 --> 00:08:51,297 
30% to the other class, then this same
distribution should approximately 
135 
00:08:51,297 --> 00:08:55,427 
be present in each of the training,
validation, and test sets. 
136 
00:08:55,427 --> 00:08:58,713 
Otherwise, analysis results
will be misleading. 
137 
00:08:58,713 --> 00:09:01,667 
To summarize, we have discuss the need for 
138 
00:09:01,667 --> 00:09:04,900 
three different datasets
in building model. 
139 
00:09:04,900 --> 00:09:09,588 
A training set to train the model,
a validation set to determine when to stop 
140 
00:09:09,588 --> 00:09:12,920 
training, and a test to evaluate
performance on new data. 
141 
00:09:14,210 --> 00:09:17,800 
We learned how a validation set can
be used to avoid overfitting and 
142 
00:09:17,800 --> 00:09:22,600 
in the process, provide an estimate
of generalization performance. 
143 
00:09:22,600 --> 00:09:24,982 
And we covered different
ways to create and 
144 
00:09:24,982 --> 00:09:28,236 
use a validation set such
as k-fold cross-validation. 
1 
00:00:00,930 --> 00:00:04,150 
Now that we have seen what
association analysis is, 
2 
00:00:04,150 --> 00:00:07,460 
let's go over the association
analysis process in more detail. 
3 
00:00:08,930 --> 00:00:13,970 
After this video, you will be able
to define the terms support and 
4 
00:00:13,970 --> 00:00:18,790 
confidence, describe the steps
in association analysis, and 
5 
00:00:18,790 --> 00:00:22,330 
explain how association rules
are formed from item sets. 
6 
00:00:23,540 --> 00:00:26,240 
Let's review the set in
association analysis. 
7 
00:00:26,240 --> 00:00:31,920 
They are create item the sets,
then identify the frequent item sets. 
8 
00:00:31,920 --> 00:00:33,230 
Finally, generate the rules. 
9 
00:00:34,240 --> 00:00:37,010 
We will continue with
this example dataset, 
10 
00:00:37,010 --> 00:00:39,530 
there are five transactions
in the dataset. 
11 
00:00:39,530 --> 00:00:42,650 
Each with a set of items
purchased together. 
12 
00:00:42,650 --> 00:00:46,770 
The goal is to come up with rules
describing associations between items. 
13 
00:00:48,280 --> 00:00:50,390 
The first step is to create item sets. 
14 
00:00:51,450 --> 00:00:54,780 
Item sets have different sizes
which need to be created. 
15 
00:00:54,780 --> 00:00:56,220 
We will color code the items so 
16 
00:00:56,220 --> 00:00:59,350 
that each one is easier to pick
out from the transactions table. 
17 
00:01:00,880 --> 00:01:05,250 
We start out with just 1-item sets,
that is, sets with just one item. 
18 
00:01:06,260 --> 00:01:09,530 
The left table is
the dataset of transactions. 
19 
00:01:09,530 --> 00:01:13,920 
The right table contains the 1-item sets
that can be created from this dataset. 
20 
00:01:15,170 --> 00:01:19,940 
As each item set is created, we also need
to keep track of the frequency at which 
21 
00:01:19,940 --> 00:01:21,910 
these item set occurs in the dataset. 
22 
00:01:23,275 --> 00:01:27,130 
This is referred to support for
the item set and 
23 
00:01:27,130 --> 00:01:32,580 
is calculated by dividing the number of
times the item set occurs in the dataset 
24 
00:01:32,580 --> 00:01:35,250 
by the total number of transactions. 
25 
00:01:35,250 --> 00:01:39,050 
This is what is in the Support
column in the right table. 
26 
00:01:39,050 --> 00:01:42,586 
For example, eggs the last
item in the right table occurs 
27 
00:01:42,586 --> 00:01:46,502 
just occurs just once in the dataset,
in the transaction two. 
28 
00:01:46,502 --> 00:01:51,838 
So if Support is 1/5 or
one fifth,the item set with 
29 
00:01:51,838 --> 00:01:59,330 
diaper occurs in all transactions,
so if Support is 5/5 or 1. 
30 
00:01:59,330 --> 00:02:03,410 
The Support for each item set will be
used to identify frequent item sets 
31 
00:02:03,410 --> 00:02:07,930 
in the next step, specifically,
the Support issues to prune, or 
32 
00:02:07,930 --> 00:02:10,990 
remove, item sets that
do not occur frequently. 
33 
00:02:12,605 --> 00:02:15,825 
The support of each item set
will be used to identify 
34 
00:02:15,825 --> 00:02:18,155 
frequent item sets in the next step. 
35 
00:02:18,155 --> 00:02:20,905 
Specifically, the support
is used to prune or 
36 
00:02:20,905 --> 00:02:24,705 
remove item sets that do
not occur frequently. 
37 
00:02:24,705 --> 00:02:28,525 
For example, the minimum support
threshold is set to 3/5. 
38 
00:02:28,525 --> 00:02:33,271 
So looking at the 1-item sets table
We can remove any item set with 
39 
00:02:33,271 --> 00:02:35,656 
the support of less than 3/5. 
40 
00:02:35,656 --> 00:02:40,348 
These item sets are highlighted in pink,
they will be removed before the sets for 
41 
00:02:40,348 --> 00:02:42,360 
two items are created. 
42 
00:02:42,360 --> 00:02:48,300 
The final one item sets are then the item
sets with bread, milk, beer and diaper. 
43 
00:02:49,770 --> 00:02:53,640 
We only consider items that were in
the one item sets, that were not pruned. 
44 
00:02:54,740 --> 00:02:56,990 
The two item sets are shown
in the right table. 
45 
00:02:58,000 --> 00:03:00,370 
We, again,
need to keep track of the support for 
46 
00:03:00,370 --> 00:03:03,100 
these item sets,
just as we did with the one item sets. 
47 
00:03:04,180 --> 00:03:06,730 
For example, for
the last item set, with beer, and 
48 
00:03:06,730 --> 00:03:10,280 
diaper, we see, by looking at
the left table, that beer and 
49 
00:03:10,280 --> 00:03:16,000 
diaper occur together three times In
transactions two, three and four. 
50 
00:03:16,000 --> 00:03:18,700 
So with support is 3/5. 
51 
00:03:18,700 --> 00:03:21,870 
Again, we need to prune
item sets with low support. 
52 
00:03:21,870 --> 00:03:25,450 
The ones highlighted in pink
in the two item sets table. 
53 
00:03:25,450 --> 00:03:29,800 
Those would be the item set with bread and
beer and the item set with milk and beer. 
54 
00:03:30,830 --> 00:03:33,120 
The remaining two items that end. 
55 
00:03:33,120 --> 00:03:36,150 
One item such or
then use to create the three item sets. 
56 
00:03:37,560 --> 00:03:39,460 
Let's look now at
creating three item sets. 
57 
00:03:40,750 --> 00:03:45,190 
The only three item sets that has a
support value greater than minimum support 
58 
00:03:45,190 --> 00:03:47,320 
is the one shown in the right table. 
59 
00:03:47,320 --> 00:03:50,240 
Namely the items start with bread,
milk and diaper. 
60 
00:03:51,940 --> 00:03:57,130 
The second step in association analysis
is to identify the frequent item sets. 
61 
00:03:57,130 --> 00:03:59,860 
But note that the process
that we just described for 
62 
00:03:59,860 --> 00:04:03,530 
creating item sets already
identifies frequent item sets. 
63 
00:04:04,530 --> 00:04:07,880 
A frequent item set is one whose
support is greater than or 
64 
00:04:07,880 --> 00:04:10,590 
equal to the minimum support. 
65 
00:04:10,590 --> 00:04:15,580 
So by keeping track of the support of
each item set as it is being created and 
66 
00:04:15,580 --> 00:04:18,020 
removing item sets with low support, 
67 
00:04:18,020 --> 00:04:20,570 
we are already identifying
frequent item sets. 
68 
00:04:21,650 --> 00:04:26,740 
For our example, the frequent one,
two and three item sets are shown here. 
69 
00:04:28,230 --> 00:04:31,940 
Now that we identified the frequent
item sets, the last step is to 
70 
00:04:31,940 --> 00:04:35,650 
generate the rules to capture
associations that we see in the data. 
71 
00:04:37,050 --> 00:04:41,627 
Let's first define some terms we'll
need to discuss association rules. 
72 
00:04:41,627 --> 00:04:45,690 
The format of an association
rule is shown at the top. 
73 
00:04:45,690 --> 00:04:51,910 
It's written as X arrow Y and
is read as if X, then Y. 
74 
00:04:51,910 --> 00:04:55,190 
The X part is called the antecedent and 
75 
00:04:55,190 --> 00:04:57,770 
the Y part is called
the consequent of the rule. 
76 
00:04:58,870 --> 00:05:00,520 
X and Y are item sets. 
77 
00:05:01,560 --> 00:05:04,920 
An important term in rule
generation is the rule confidence. 
78 
00:05:06,120 --> 00:05:08,680 
This is to find as a support for X and 
79 
00:05:08,680 --> 00:05:14,080 
Y together divided by the support for
X only. 
80 
00:05:14,080 --> 00:05:16,680 
So rule confidence
calculates the frequency of 
81 
00:05:16,680 --> 00:05:18,710 
instances to which the rule applies. 
82 
00:05:20,210 --> 00:05:24,830 
Recall that the support for
X is the frequency of item set X and 
83 
00:05:24,830 --> 00:05:28,470 
is defined as the number of
transactions containing items in X 
84 
00:05:28,470 --> 00:05:31,595 
divided by the total
number of transactions. 
85 
00:05:31,595 --> 00:05:34,875 
The rule confidence measures
how frequently items in 
86 
00:05:34,875 --> 00:05:38,585 
Y appear in the transaction
that contain X. 
87 
00:05:38,585 --> 00:05:43,365 
In other words, the confidence measures
the reliability of the rule by determining 
88 
00:05:43,365 --> 00:05:48,635 
how often, if X and
Y is found to be true in the data. 
89 
00:05:48,635 --> 00:05:50,835 
How is rule confidence
used in rule generation? 
90 
00:05:52,230 --> 00:05:57,160 
Association rules are generated from the
frequent item sets created from the data. 
91 
00:05:57,160 --> 00:06:01,070 
Each item in an item set can be
used as a part of the antecedent or 
92 
00:06:01,070 --> 00:06:03,200 
consequent of the rule. 
93 
00:06:03,200 --> 00:06:06,970 
And you can have many ways to combine
items to form the antecedent and 
94 
00:06:06,970 --> 00:06:07,490 
consequent. 
95 
00:06:08,810 --> 00:06:12,210 
So if we just simply generate
rules from each frequent item set, 
96 
00:06:12,210 --> 00:06:14,390 
we would end up with lots and
lots of rules. 
97 
00:06:15,390 --> 00:06:20,680 
Each item set with k items can
generate 2 to the k-2 rules. 
98 
00:06:20,680 --> 00:06:22,330 
That's a lot of rules. 
99 
00:06:22,330 --> 00:06:25,240 
And the majority of those rules
would not be found in the data. 
100 
00:06:26,330 --> 00:06:28,070 
This is where rule confidence comes in. 
101 
00:06:29,380 --> 00:06:33,140 
We can use rule confidence to
constrain the number of rules to keep. 
102 
00:06:34,190 --> 00:06:38,140 
Specifically, a minimum
confidence threshold is set and 
103 
00:06:38,140 --> 00:06:40,810 
only rules with confidence greater than or 
104 
00:06:40,810 --> 00:06:45,410 
equal to the minimum confidence are
significant and only those will be kept. 
105 
00:06:46,850 --> 00:06:49,320 
Let's look at how this works
with our example dataset. 
106 
00:06:50,520 --> 00:06:55,150 
We call that only one three item set
was created from the transactions. 
107 
00:06:55,150 --> 00:06:58,380 
That three items that contains
items bread, milk and 
108 
00:06:58,380 --> 00:06:59,630 
diaper as shown at the top. 
109 
00:07:00,730 --> 00:07:05,000 
With these three item set let's see
how we can generate rules from it and 
110 
00:07:05,000 --> 00:07:07,670 
determine which rules to keep and
which one to prune. 
111 
00:07:09,150 --> 00:07:12,507 
Let's set the minimum confidence to 0.95. 
112 
00:07:12,507 --> 00:07:15,350 
And here again is the definition for
confidence. 
113 
00:07:16,890 --> 00:07:21,250 
For candidate rule if bread and
milk then diaper, 
114 
00:07:21,250 --> 00:07:24,940 
we can calculate it's confidence
as follows the support for 
115 
00:07:24,940 --> 00:07:29,790 
both antecedent and consequent is
the number of times we see bread, milk and 
116 
00:07:29,790 --> 00:07:35,570 
diaper together in the data, divided
by the total number of transactions. 
117 
00:07:35,570 --> 00:07:40,240 
Items bread, milk and diaper appear
together in transaction 1, 4 and 
118 
00:07:40,240 --> 00:07:43,720 
5 so the support is 3/5. 
119 
00:07:43,720 --> 00:07:47,570 
The support for just the antecedent is
the number of times we see bread and 
120 
00:07:47,570 --> 00:07:50,690 
milk together divided by
the total number of transactions. 
121 
00:07:51,770 --> 00:07:56,778 
Items bread and milk appear together
also in transactions 1, 4, and 5. 
122 
00:07:56,778 --> 00:07:59,880 
So the support is 3/5. 
123 
00:07:59,880 --> 00:08:04,119 
The confidence of this rule is then 1,
or 100%. 
124 
00:08:04,119 --> 00:08:07,743 
This means that the rule is correct 100%. 
125 
00:08:07,743 --> 00:08:11,540 
Every time bread and milk are bought
together, diaper is bought as well. 
126 
00:08:12,780 --> 00:08:14,780 
For candidate rule if bread and 
127 
00:08:14,780 --> 00:08:19,170 
diaper than milk,
we calculate its confidence the same way. 
128 
00:08:19,170 --> 00:08:23,780 
The support for bread, diaper and
milk is 3/5 as before. 
129 
00:08:23,780 --> 00:08:29,920 
Items bread and diaper are paired
together in transactions 1, 2, 4 and 5. 
130 
00:08:29,920 --> 00:08:34,230 
So the support for
the items set with bread and milk is 4/5. 
131 
00:08:34,230 --> 00:08:39,324 
Then the confidence with
this rule is 0.75 or 75%. 
132 
00:08:39,324 --> 00:08:42,845 
Since the minimum confidence is 0.95 or
95%, 
133 
00:08:42,845 --> 00:08:47,710 
the first rule is kept and the second
rule is removed from consideration. 
134 
00:08:49,430 --> 00:08:52,670 
There are several algorithms for
association analysis. 
135 
00:08:52,670 --> 00:08:56,820 
Each uses a different set of methods
to make frequent items set creation and 
136 
00:08:56,820 --> 00:08:58,970 
rule generation efficient. 
137 
00:08:58,970 --> 00:09:03,080 
The more popular algorithms are Apriori,
FP Growth and Eclat. 
138 
00:09:04,560 --> 00:09:09,410 
As a summary, we just looked at the steps
in association analysis in more detail. 
139 
00:09:09,410 --> 00:09:14,055 
We saw how items sets can be created from
a dataset, how frequent items sets can be 
140 
00:09:14,055 --> 00:09:18,770 
identified, and how association rules can
be created from frequent item sets and 
141 
00:09:18,770 --> 00:09:20,633 
pruned using rule confidence. 
1 
00:00:00,950 --> 00:00:04,990 
We've looked at classification,
regression and cluster analysis. 
2 
00:00:04,990 --> 00:00:09,280 
Let's now discuss association
analysis as a machine learning task. 
3 
00:00:09,280 --> 00:00:15,100 
After this video, you will be able to
explain what association analysis entails, 
4 
00:00:15,100 --> 00:00:20,259 
list some applications of association
analysis, define what an item set is. 
5 
00:00:21,490 --> 00:00:25,600 
In association analysis, the goal is to
come up with a set of rules to capture 
6 
00:00:25,600 --> 00:00:29,070 
associations between items or events. 
7 
00:00:29,070 --> 00:00:33,870 
The rules are used to determine when
items or events occur together. 
8 
00:00:33,870 --> 00:00:38,110 
You may remember seeing these images
earlier in the course, where we introduced 
9 
00:00:38,110 --> 00:00:41,730 
the different categories of machine
learning tasks and techniques. 
10 
00:00:41,730 --> 00:00:45,580 
But do you remember what
the association is between these items? 
11 
00:00:45,580 --> 00:00:48,810 
Well, let's recap that story,
in case you don't remember. 
12 
00:00:48,810 --> 00:00:50,380 
The story goes like this. 
13 
00:00:50,380 --> 00:00:54,100 
A supermarket chain used
association analysis to discover 
14 
00:00:54,100 --> 00:00:58,050 
a connection between two
seemingly unrelated products. 
15 
00:00:58,050 --> 00:01:01,200 
They discovered that many
customers who go to the store 
16 
00:01:01,200 --> 00:01:05,860 
late on Sunday night to buy
diapers also tend to buy beer. 
17 
00:01:05,860 --> 00:01:09,634 
This information was then used to
place beer and diapers close together. 
18 
00:01:09,634 --> 00:01:12,780 
And they saw a jump in
sales of both items. 
19 
00:01:12,780 --> 00:01:15,620 
This illustrates that you
can uncover unexpected and 
20 
00:01:15,620 --> 00:01:19,040 
useful relationships with
association analysis. 
21 
00:01:19,040 --> 00:01:23,530 
This diaper and beer story has become
part of the data mining folklore. 
22 
00:01:23,530 --> 00:01:27,650 
It's unclear how much of it true, but it
has become the prime example of what you 
23 
00:01:27,650 --> 00:01:31,600 
can discover with association analysis and
machine learning in general. 
24 
00:01:32,990 --> 00:01:37,511 
A common application of association
analysis is referred to as 
25 
00:01:37,511 --> 00:01:39,398 
market basket analysis. 
26 
00:01:39,398 --> 00:01:43,550 
This is used to understand
the purchasing behavior of customers. 
27 
00:01:43,550 --> 00:01:47,190 
The idea is that you're looking into
the shopping basket of customers 
28 
00:01:47,190 --> 00:01:48,650 
when they are at the market, and 
29 
00:01:48,650 --> 00:01:53,150 
analyzing that data to understand
what items are purchased together. 
30 
00:01:53,150 --> 00:01:57,090 
This information can be used to
place related items together, or 
31 
00:01:57,090 --> 00:02:00,290 
to have sales on items that
are often purchased together. 
32 
00:02:01,310 --> 00:02:05,210 
Another application of association
analysis is to recommend items that 
33 
00:02:05,210 --> 00:02:10,620 
a customer may be interested in, based
on their purchasing or browsing history. 
34 
00:02:10,620 --> 00:02:13,720 
This is very commonly used
on companies' websites 
35 
00:02:13,720 --> 00:02:15,379 
to get customers to buy more items. 
36 
00:02:16,470 --> 00:02:18,610 
There are medical applications as well. 
37 
00:02:18,610 --> 00:02:23,430 
Analysis of patients and treatments may
reveal associations to identify effective 
38 
00:02:23,430 --> 00:02:27,080 
treatments for
patients with certain medical histories. 
39 
00:02:27,080 --> 00:02:30,710 
this diagram illustrates how
association analysis works. 
40 
00:02:30,710 --> 00:02:34,090 
The data set is a collection
of transactions. 
41 
00:02:34,090 --> 00:02:36,519 
Each transaction contains one or
more items. 
42 
00:02:37,530 --> 00:02:40,490 
This is referred to as the item set. 
43 
00:02:40,490 --> 00:02:42,070 
From the given items sets, 
44 
00:02:42,070 --> 00:02:46,250 
generate association rules that capture
which item tend occur together. 
45 
00:02:47,390 --> 00:02:51,520 
In our example, the data set
consists of five transactions. 
46 
00:02:51,520 --> 00:02:56,518 
In the first transaction,
the items are diaper, bread and milk. 
47 
00:02:56,518 --> 00:03:01,900 
The second transaction has items bread,
diaper, beer, and eggs, and so on. 
48 
00:03:03,258 --> 00:03:07,290 
Rules that could be generated from
this data set are shown at the bottom. 
49 
00:03:07,290 --> 00:03:10,090 
For example,
the first rule states that if bread and 
50 
00:03:10,090 --> 00:03:13,910 
milk are bought together,
then diaper is also bought. 
51 
00:03:13,910 --> 00:03:17,870 
The second rule state that if milk is
bought, then bread is also bought. 
52 
00:03:19,420 --> 00:03:22,690 
The association analysis process
consist of the following steps. 
53 
00:03:24,060 --> 00:03:27,140 
The first step is to create item sets. 
54 
00:03:27,140 --> 00:03:31,690 
Item sets are generated for sets with one
item, two items, three items and so on. 
55 
00:03:33,200 --> 00:03:36,300 
Then frequent item sets are identified. 
56 
00:03:36,300 --> 00:03:40,380 
Frequent item sets are those that occur
at least a minimum number of times. 
57 
00:03:41,590 --> 00:03:45,580 
From the frequent item sets,
association rules are generated. 
58 
00:03:45,580 --> 00:03:48,310 
We will take a more detailed look
at these steps in the next lecture. 
59 
00:03:49,660 --> 00:03:52,800 
Some things to note about
association analysis. 
60 
00:03:52,800 --> 00:03:57,480 
Like cluster analysis, each transaction
does not have a label to specify which 
61 
00:03:57,480 --> 00:03:59,760 
item set or rule it belongs to. 
62 
00:03:59,760 --> 00:04:03,340 
So, association analysis
is an unsupervised task. 
63 
00:04:04,540 --> 00:04:07,510 
You may end up with many rules
at the end of the analysis. 
64 
00:04:07,510 --> 00:04:10,540 
But, whether those rules are interesting,
useful, 
65 
00:04:10,540 --> 00:04:15,370 
or applicable, requires interpretation
using domain knowledge of the application. 
66 
00:04:16,540 --> 00:04:17,720 
In addition, 
67 
00:04:17,720 --> 00:04:22,200 
the association analysis process will
not tell you how to apply the rules. 
68 
00:04:22,200 --> 00:04:25,999 
This also requires knowledge
of the application. 
69 
00:04:25,999 --> 00:04:30,245 
So as with cluster analysis,
interpretation and analysis are required 
70 
00:04:30,245 --> 00:04:34,780 
to make sense of the resulting rules
that you get from association analysis. 
71 
00:04:35,970 --> 00:04:37,020 
In summary, 
72 
00:04:37,020 --> 00:04:42,240 
association analysis finds rules to
capture associations between items. 
73 
00:04:42,240 --> 00:04:46,670 
The association rules have intuitive
appeal because they are in the form of 
74 
00:04:46,670 --> 00:04:50,420 
if this, then that,
which is easy to understand. 
75 
00:04:50,420 --> 00:04:54,150 
The results of association
analysis require analysis and 
76 
00:04:54,150 --> 00:04:58,430 
interpretation using domain knowledge
to determine the usefulness and 
77 
00:04:58,430 --> 00:05:00,020 
applicability of the resulting rules. 
78 
00:05:01,310 --> 00:05:04,756 
Next we will cover the steps in
the association analysis process in 
79 
00:05:04,756 --> 00:05:05,501 
more detail. 
1 
00:00:00,990 --> 00:00:05,240 
In this activity, we will use
Spark to perform cluster analysis. 
2 
00:00:05,240 --> 00:00:07,320 
First, we will load
the minute weather data. 
3 
00:00:08,600 --> 00:00:12,479 
Next, we will remove the unused and
missing data and 
4 
00:00:12,479 --> 00:00:15,752 
then scale the data so
that the mean is zero. 
5 
00:00:15,752 --> 00:00:18,931 
We will then create an elbow plot,
a subset of the data, 
6 
00:00:18,931 --> 00:00:22,430 
to determine the optimal
number of clusters. 
7 
00:00:22,430 --> 00:00:26,280 
And then cluster the full
data set using k-means. 
8 
00:00:26,280 --> 00:00:30,390 
Finally, we will generate parallel plots
to analyze the individual clusters. 
9 
00:00:32,000 --> 00:00:32,530 
Let's begin. 
10 
00:00:33,600 --> 00:00:35,560 
First, let's open the clustering notebook. 
11 
00:00:38,650 --> 00:00:41,259 
Execute the first cell
to load the libraries. 
12 
00:00:43,090 --> 00:00:45,220 
Execute the second cell
to load the data set. 
13 
00:00:47,630 --> 00:00:51,870 
This data set contains weather station
measurements that were taken every minute. 
14 
00:00:51,870 --> 00:00:53,980 
So there's a lot of measurements. 
15 
00:00:53,980 --> 00:01:00,705 
We can count how many rows there are in
the data frame by running df.count. 
16 
00:01:01,757 --> 00:01:05,860 
This says that there are over 1.5
million rows in the data frame. 
17 
00:01:05,860 --> 00:01:10,090 
Clustering this much data on a single
Cloudera VM can take a lot of time. 
18 
00:01:10,090 --> 00:01:12,680 
So let's only work with
one-tenth of the data. 
19 
00:01:12,680 --> 00:01:15,340 
Let's subset the data
into the new data frame. 
20 
00:01:15,340 --> 00:01:20,124 
We'll enter filteredDF 
21 
00:01:20,124 --> 00:01:25,820 
= df.filter((df.rowID 
22 
00:01:25,820 --> 00:01:29,924 
% 10)) == 0. 
23 
00:01:29,924 --> 00:01:38,182 
We then count the rows of the new data
frame by calling filteredDF.count. 
24 
00:01:40,997 --> 00:01:45,130 
The new data frame has one-tenth as
many rows as the original data set. 
25 
00:01:46,520 --> 00:01:48,888 
Let's compute the summary
statistics using describe. 
26 
00:01:48,888 --> 00:01:54,613 
filteredDF.describe, and
to display it nicely, 
27 
00:01:54,613 --> 00:01:59,303 
we'll enter toPandas().transpose. 
28 
00:01:59,303 --> 00:01:59,803 
Run this. 
29 
00:02:07,604 --> 00:02:10,895 
These weather measurements were taken
during the period of a long drought. 
30 
00:02:12,090 --> 00:02:15,680 
As we can see from the mean values
of the two rain measurements, 
31 
00:02:15,680 --> 00:02:19,820 
rain accumulation, the rain duration,
the measurements are close to zero. 
32 
00:02:21,600 --> 00:02:24,504 
Let's count how many rain
values are equal to 0. 
33 
00:02:24,504 --> 00:02:32,198 
FilterDF.filter(filterDF.rain_accumulation
== 
34 
00:02:32,198 --> 00:02:35,431 
0.0).count(). 
35 
00:02:38,436 --> 00:02:40,890 
Now let's do rain duration. 
36 
00:02:40,890 --> 00:02:47,994 
filteredDF.filter(filteredDF.rain_duration
== 
37 
00:02:47,994 --> 00:02:51,563 
0.0).count(). 
38 
00:02:51,563 --> 00:02:57,053 
We can see from these counts that most
in these two measurements are zero. 
39 
00:02:57,053 --> 00:03:00,527 
So let's remove them from our data frame. 
40 
00:03:00,527 --> 00:03:06,719 
workingDF =
filterDF.drop('rain_accumulation').drop('- 
41 
00:03:06,719 --> 00:03:08,710 
rain_duration'). 
42 
00:03:08,710 --> 00:03:11,710 
And we'll also drop the column
called hpwren_timestamp, 
43 
00:03:11,710 --> 00:03:13,502 
since we will not use it. 
44 
00:03:13,502 --> 00:03:16,099 
So .drop('hpwren_timestamp'). 
45 
00:03:20,422 --> 00:03:25,608 
Next, let's drop rows with missing values,
and count how many rows were dropped. 
46 
00:03:25,608 --> 00:03:30,085 
before = workingDF.count() workingDF = 
47 
00:03:30,085 --> 00:03:36,055 
workingDF.na.drop() after
= workingDF.count(), 
48 
00:03:36,055 --> 00:03:42,147 
and finally, we'll print the difference,
before- after. 
49 
00:03:46,410 --> 00:03:49,000 
So only 46 rows had missing values and
were dropped. 
50 
00:03:50,320 --> 00:03:54,190 
Next, let's scale the data so that each
feature will have a value of 0 for 
51 
00:03:54,190 --> 00:03:58,010 
the mean and a value of 1 for
the standard deviation. 
52 
00:03:58,010 --> 00:04:01,500 
First, we need to combine the columns
into a single vector column. 
53 
00:04:01,500 --> 00:04:06,366 
We can look at the existing columns
by entering workingDF.columns. 
54 
00:04:06,366 --> 00:04:10,510 
We do not want to include rowID
since it is a row number. 
55 
00:04:10,510 --> 00:04:13,430 
Additionally, the minimum wind
measurements have a high correlation 
56 
00:04:13,430 --> 00:04:16,510 
to the average wind measurements,
so we will not include this either. 
57 
00:04:17,560 --> 00:04:20,230 
Let's create an array of
the columns we want to combine and 
58 
00:04:20,230 --> 00:04:23,798 
then use vector assembler to
create the vector column. 
59 
00:04:23,798 --> 00:04:28,760 
featuresUsed = [. 
60 
00:04:28,760 --> 00:04:31,439 
Now, let's copy and
paste the columns we want to use. 
61 
00:04:50,974 --> 00:04:57,085 
Assembler = VectorAssembler(inputCols
= featuresUsed, 
62 
00:04:57,085 --> 00:05:01,358 
outputCol = "features_unscaled"). 
63 
00:05:01,358 --> 00:05:07,534 
And finally, assembled =
assembler.transform(workingDF). 
64 
00:05:07,534 --> 00:05:09,111 
Run this. 
65 
00:05:09,111 --> 00:05:15,021 
Now we'll use standard
scaler to scale the data. 
66 
00:05:15,021 --> 00:05:21,255 
Scaler = standard scaler
inputCol=”features _unscaler”, 
67 
00:05:21,255 --> 00:05:25,706 
outputCol=“features”, withStd = true, 
68 
00:05:25,706 --> 00:05:31,828 
withMean = True,
scalerModel = scaler.fit(assembled), 
69 
00:05:31,828 --> 00:05:37,297 
scaledData =
scalerModel.transform[assembled]. 
70 
00:05:43,150 --> 00:05:46,642 
Next we will create an elbow plot
to determine the value k, for 
71 
00:05:46,642 --> 00:05:48,400 
the number of clusters. 
72 
00:05:48,400 --> 00:05:52,440 
To create the elbow plot,
we will calculate the within cluster, 
73 
00:05:52,440 --> 00:05:56,120 
sum of squared error, or WSSE,
for different values of k. 
74 
00:05:56,120 --> 00:06:00,800 
So since some valves running
K-means many times, let's do it for 
75 
00:06:00,800 --> 00:06:03,330 
a smaller subset of the data
since it'll be faster. 
76 
00:06:05,350 --> 00:06:08,280 
First, let's choose the data to work with. 
77 
00:06:08,280 --> 00:06:10,286 
Let's subset the data. 
78 
00:06:10,286 --> 00:06:15,347 
So scaledData =
scaledData.select("features"), ("rowID"). 
79 
00:06:15,347 --> 00:06:22,407 
elbowset =
scaledData.filter((scaledData.rowID 
80 
00:06:22,407 --> 00:06:28,173 
%3) == 0).select("features"). 
81 
00:06:28,173 --> 00:06:32,907 
And finally,
we'll call the persist method on elbowset. 
82 
00:06:32,907 --> 00:06:36,736 
Persist will keep it in memory,
and make the calculations faster. 
83 
00:06:36,736 --> 00:06:40,925 
Run this. 
84 
00:06:40,925 --> 00:06:46,072 
Now, let's compute the WSSE values for
different values of k. 
85 
00:06:46,072 --> 00:06:49,080 
We'll do for k, 2 to 30. 
86 
00:06:49,080 --> 00:06:53,693 
Clusters equals range(2,31). 
87 
00:06:53,693 --> 00:07:02,271 
wsseList = utils.elbow(elbowset,
clusters). 
88 
00:07:02,271 --> 00:07:09,800 
Run this, This will print out
the value of wsse for each value of k. 
89 
00:07:09,800 --> 00:07:13,410 
As you can tell, this'll take some
time to run, let's skip to the end. 
90 
00:07:16,570 --> 00:07:19,158 
Now let's display the plot of our data. 
91 
00:07:19,158 --> 00:07:29,159 
utils.elbow_plot(wsseList, clusters). 
92 
00:07:29,159 --> 00:07:36,614 
The x axis is k, the number of clusters,
and the y axis is the WSSE value. 
93 
00:07:36,614 --> 00:07:40,190 
You can see that the graph flattens
out between 10 and 15 for k. 
94 
00:07:41,320 --> 00:07:45,620 
So let's choose k equals 12 as
the midpoint for our number of clusters. 
95 
00:07:47,788 --> 00:07:52,040 
We'll now cluster the data into
12 clusters using k-means. 
96 
00:07:52,040 --> 00:07:54,285 
First, let's select the data
we want to cluster. 
97 
00:07:54,285 --> 00:08:01,726 
scaledDataFeat =
scaledData.select("features"), 
98 
00:08:01,726 --> 00:08:05,142 
scaledDataFeat.persist. 
99 
00:08:07,496 --> 00:08:10,394 
Now I'll perform
the clustering using kmeans. 
100 
00:08:10,394 --> 00:08:15,688 
kmeans = KMeans(k=12, seed=1). 
101 
00:08:15,688 --> 00:08:21,752 
model = KMeans.fit(scaledDataFeat),
and finally, 
102 
00:08:21,752 --> 00:08:27,582 
transformed =
model.transform(scaleDataFeat). 
103 
00:08:27,582 --> 00:08:29,198 
Run this. 
104 
00:08:29,198 --> 00:08:33,702 
We can now see the centre
measurement of each 
105 
00:08:33,702 --> 00:08:38,459 
cluster by calling model.clusterCenters. 
106 
00:08:42,924 --> 00:08:47,080 
It is difficult to compare the cluster
centers just by looking at these numbers. 
107 
00:08:47,080 --> 00:08:52,319 
So let's use parallel
plots to visualize them. 
108 
00:08:52,319 --> 00:08:58,095 
P = utils.pd_centers(featuresUsed, 
109 
00:08:58,095 --> 00:09:02,000 
model.clusterCenters). 
110 
00:09:03,810 --> 00:09:04,850 
Let's show the clusters for 
111 
00:09:04,850 --> 00:09:08,390 
dry days where the weather samples
have low relative humidity. 
112 
00:09:09,670 --> 00:09:19,670 
utils.parallel_plot(P[P['relative_humidi-
ty'] 
113 
00:09:20,714 --> 00:09:24,674 
< -0.5], P). 
114 
00:09:28,719 --> 00:09:31,920 
The x axis of this chart shows
the different measurement types. 
115 
00:09:32,990 --> 00:09:36,730 
The y values show the standard deviations,
with zero being the mean. 
116 
00:09:38,330 --> 00:09:42,330 
Each line is a different cluster, and
there are five clusters in this graph. 
117 
00:09:44,010 --> 00:09:47,130 
We can see that they all have
a low relative humidity. 
118 
00:09:47,130 --> 00:09:52,060 
Notice that cluster four, the red one,
has a high average wind speed and 
119 
00:09:52,060 --> 00:09:53,430 
a high maximum wind speed. 
120 
00:09:55,400 --> 00:09:58,850 
Additionally, it has a low
average wind direction. 
121 
00:09:58,850 --> 00:10:02,380 
Which means it was coming from
the north and northeast directions. 
122 
00:10:02,380 --> 00:10:05,480 
So this cluster probably
represents Santa Ana conditions. 
123 
00:10:07,770 --> 00:10:10,615 
Next, let's show the plot for warm days, 
124 
00:10:10,615 --> 00:10:13,867 
the weather samples with
high air temperature. 
125 
00:10:13,867 --> 00:10:21,877 
utils.parallel_plot(P[P['air_temp'] > 
126 
00:10:21,877 --> 00:10:24,490 
0.5], P). 
127 
00:10:30,292 --> 00:10:34,283 
Other clusters in this plot have air
temperature greater than 0.5 standard 
128 
00:10:34,283 --> 00:10:35,920 
deviations away from the mean. 
129 
00:10:36,930 --> 00:10:40,000 
However, they have different values for
the other features. 
130 
00:10:40,000 --> 00:10:42,530 
Now let's show the clusters for cool days. 
131 
00:10:42,530 --> 00:10:46,430 
Weather samples with high relative
humidity and low air temperatures. 
132 
00:10:47,650 --> 00:10:57,650 
utils.parallel_plot(P[(P['relative_humidi-
ty'] 
133 
00:10:59,966 --> 00:11:07,129 
> 0.5) & (P['air_temp'] 
134 
00:11:07,129 --> 00:11:11,387 
< 0.5)], P). 
135 
00:11:11,387 --> 00:11:16,222 
All the clusters in this plot have
relative humidity greater than 0.5 
136 
00:11:16,222 --> 00:11:21,608 
standard deviations and air temp
less than 0.5 standard deviations. 
137 
00:11:21,608 --> 00:11:24,680 
These clusters represent cool
temperature with high humidity and 
138 
00:11:24,680 --> 00:11:27,220 
possibly rainy weather patterns. 
139 
00:11:27,220 --> 00:11:29,980 
So far we have seen all
the clusters except two. 
140 
00:11:29,980 --> 00:11:34,012 
Since it is not falling to any other
categories let's plot this cluster, 
141 
00:11:34,012 --> 00:11:43,795 
utils.parallel_plot(P.iloc[[2]], 
142 
00:11:43,795 --> 00:11:44,527 
P). 
143 
00:11:47,556 --> 00:11:49,892 
Cluster two captures
days with mild weather. 
1 
00:00:01,000 --> 00:00:04,770 
In addition to classification and
regression, machine learning tasks and 
2 
00:00:04,770 --> 00:00:09,720 
techniques can also fall into another
category known as cluster analysis. 
3 
00:00:09,720 --> 00:00:15,460 
After this video, you will be able to
articulate the goal of cluster analysis, 
4 
00:00:15,460 --> 00:00:20,310 
discuss whether cluster analysis
is supervised or unsupervised, and 
5 
00:00:20,310 --> 00:00:22,530 
list some ways that cluster
results can be applied. 
6 
00:00:24,110 --> 00:00:25,150 
In cluster analysis, 
7 
00:00:25,150 --> 00:00:30,870 
the goal is to organize similar items in
your data set into groups or clusters. 
8 
00:00:30,870 --> 00:00:36,550 
By segmenting your data into clusters, you
can analyze each cluster more carefully. 
9 
00:00:36,550 --> 00:00:39,790 
Note that cluster analysis is
also referred to as clustering. 
10 
00:00:41,480 --> 00:00:46,220 
A very common application of cluster
analysis that we have discussed before is 
11 
00:00:46,220 --> 00:00:50,720 
to divide your customer base into segments
based on their purchasing histories. 
12 
00:00:50,720 --> 00:00:54,180 
For example, you can segment customers
into those who have purchased science 
13 
00:00:54,180 --> 00:00:58,960 
fiction books and videos, versus those
who tend to buy nonfiction books, 
14 
00:00:58,960 --> 00:01:02,240 
versus those who have bought
many children's books. 
15 
00:01:02,240 --> 00:01:06,860 
This way, you can provide more targeted
suggestions to each different group. 
16 
00:01:06,860 --> 00:01:11,620 
Some other examples of cluster analysis
are characterizing different weather 
17 
00:01:11,620 --> 00:01:15,620 
patterns for a region,
grouping the latest news articles into 
18 
00:01:15,620 --> 00:01:19,450 
topics to identify the trending
topics of the day, and 
19 
00:01:19,450 --> 00:01:23,650 
discovering hot spots for different
types of crime from police reports 
20 
00:01:23,650 --> 00:01:26,690 
in order to provide sufficient
police presence for problem areas. 
21 
00:01:28,550 --> 00:01:32,100 
Cluster analysis divides all
the samples in a data set into groups. 
22 
00:01:33,340 --> 00:01:36,620 
In this diagram,
we see that the red, green, and 
23 
00:01:36,620 --> 00:01:38,550 
purple data points are clustered together. 
24 
00:01:39,570 --> 00:01:43,630 
Which group a sample is placed in is
based on some measure of similarity. 
25 
00:01:45,060 --> 00:01:50,070 
The goal of cluster analysis is to segment
data so that differences between samples 
26 
00:01:50,070 --> 00:01:55,240 
in the same cluster are minimized, as
shown by the yellow arrow, and differences 
27 
00:01:55,240 --> 00:02:00,280 
between samples of different clusters are
maximized, as shown by the orange arrow. 
28 
00:02:00,280 --> 00:02:03,190 
Visually, you can think of
this as getting samples in 
29 
00:02:03,190 --> 00:02:06,750 
each cluster to be as close
together as possible, and 
30 
00:02:06,750 --> 00:02:10,540 
the samples from different clusters
to be as far apart as possible. 
31 
00:02:11,810 --> 00:02:14,380 
Cluster analysis requires
some sort of metric to 
32 
00:02:14,380 --> 00:02:17,425 
measure similarity between two samples. 
33 
00:02:17,425 --> 00:02:22,490 
Some common similarity measures are
Euclidean distance, which is the distance 
34 
00:02:22,490 --> 00:02:26,325 
along a straight line between two points,
A and B, as shown in this plot. 
35 
00:02:27,390 --> 00:02:31,920 
Manhattan distance, which is
calculated on a strictly horizontal and 
36 
00:02:31,920 --> 00:02:35,100 
vertical path, as shown in the right plot. 
37 
00:02:35,100 --> 00:02:39,932 
To go from point A to point B, you can
only step along either the x-axis or 
38 
00:02:39,932 --> 00:02:43,490 
the y-axis in a two-dimensional case. 
39 
00:02:43,490 --> 00:02:47,390 
So the path to calculate the Manhattan
distance consists of segments 
40 
00:02:47,390 --> 00:02:52,240 
along the axes instead of along a diagonal
path, as with Euclidean distance. 
41 
00:02:53,580 --> 00:02:58,010 
Cosine similarity measures the cosine
of the angle between points A and 
42 
00:02:58,010 --> 00:03:00,460 
B, as shown in the bottom plot. 
43 
00:03:00,460 --> 00:03:05,400 
Since distance measures such as Euclidean
distance are often used to measure 
44 
00:03:05,400 --> 00:03:09,048 
similarity between samples
in clustering algorithms, 
45 
00:03:09,048 --> 00:03:13,152 
note that it may be necessary to
normalize the input variables so 
46 
00:03:13,152 --> 00:03:16,889 
that no one value dominates
the similarity calculation. 
47 
00:03:16,889 --> 00:03:18,071 
We discussed scaling and 
48 
00:03:18,071 --> 00:03:22,250 
normalizing variables in the lecture
on feature transformation. 
49 
00:03:22,250 --> 00:03:25,500 
Normalizing is one method
to scale variables. 
50 
00:03:25,500 --> 00:03:30,560 
Essentially, scaling the input variables
puts the variables on the same scale so 
51 
00:03:30,560 --> 00:03:33,840 
that all variables have equal
weighting in the calculation 
52 
00:03:33,840 --> 00:03:35,889 
to determine similarity between samples. 
53 
00:03:36,930 --> 00:03:41,110 
Scaling is necessary when you have
variables that have very different scales, 
54 
00:03:41,110 --> 00:03:43,040 
such as weight and height. 
55 
00:03:43,040 --> 00:03:47,250 
The magnitude of the height values,
which are in feet and inches, 
56 
00:03:47,250 --> 00:03:51,910 
will be much smaller than the magnitude of
the weight values, which are in pounds. 
57 
00:03:51,910 --> 00:03:55,500 
So scaling both variables to
a common value range will 
58 
00:03:55,500 --> 00:03:58,130 
make the contributions from
both weight and height equal. 
59 
00:03:59,410 --> 00:04:01,710 
Here are some things to note
about cluster analysis. 
60 
00:04:02,720 --> 00:04:06,220 
First, unlike classification or
regression, in general, 
61 
00:04:06,220 --> 00:04:09,500 
cluster analysis is an unsupervised task. 
62 
00:04:09,500 --> 00:04:13,590 
This means that there is no target
label for any sample in the data set. 
63 
00:04:15,020 --> 00:04:18,660 
In general,
there is no correct clustering results. 
64 
00:04:18,660 --> 00:04:21,260 
The best set of clusters
is highly dependent 
65 
00:04:21,260 --> 00:04:24,230 
on how the resulting
clusters will be used. 
66 
00:04:24,230 --> 00:04:28,070 
There are numerical measures to
compare two different clusters, but 
67 
00:04:28,070 --> 00:04:31,000 
since there are no labels to
determine whether a sample has 
68 
00:04:31,000 --> 00:04:34,190 
been correctly clustered,
there is no ground truth 
69 
00:04:34,190 --> 00:04:38,520 
to determine if a set of clustering
results are truly correct or incorrect. 
70 
00:04:40,120 --> 00:04:42,190 
Clusters don't come with labels. 
71 
00:04:42,190 --> 00:04:45,410 
You may end up with five different
clusters at the end of a cluster 
72 
00:04:45,410 --> 00:04:49,630 
analysis process, but you don't
know what each cluster represents. 
73 
00:04:49,630 --> 00:04:52,410 
Only by analyzing
the samples in each cluster 
74 
00:04:52,410 --> 00:04:56,450 
can you come out with reasonable
labels for your clusters. 
75 
00:04:56,450 --> 00:05:00,150 
Given all this, it is important to
keep in mind that interpretation and 
76 
00:05:00,150 --> 00:05:05,400 
analysis of the clusters
are required to make sense of and 
77 
00:05:05,400 --> 00:05:07,869 
make use of the results
of cluster analysis. 
78 
00:05:09,330 --> 00:05:13,040 
There are several ways that the results
of cluster analysis can be used. 
79 
00:05:13,040 --> 00:05:16,570 
The most obvious is data segmentation and
the benefits that come from that. 
80 
00:05:17,570 --> 00:05:21,230 
If you segment your customer base
into different types of readers, 
81 
00:05:21,230 --> 00:05:25,280 
the resulting insights can be used
to provide more effective marketing 
82 
00:05:25,280 --> 00:05:28,540 
to the different customer groups
based on their preferences. 
83 
00:05:28,540 --> 00:05:32,840 
For example, analyzing each segment
separately can provide valuable insights 
84 
00:05:32,840 --> 00:05:37,570 
into each group's likes, dislikes and
purchasing behavior, just like we see 
85 
00:05:37,570 --> 00:05:40,730 
science fiction, non-fiction and
children's books preferences here. 
86 
00:05:42,450 --> 00:05:45,910 
Clusters can also be used to
classify new data samples. 
87 
00:05:45,910 --> 00:05:48,030 
When a new sample is received, 
88 
00:05:48,030 --> 00:05:52,520 
like the orange sample here, compute
the similarity measure between it and 
89 
00:05:52,520 --> 00:05:57,160 
the centers of all clusters, and assign
a new sample to the closest cluster. 
90 
00:05:57,160 --> 00:05:58,850 
The label of that cluster, 
91 
00:05:58,850 --> 00:06:04,020 
manually determined through analysis,
is then used to classify the new sample. 
92 
00:06:04,020 --> 00:06:08,770 
In our book buyers' preferences example,
a new customer can be classified as being 
93 
00:06:08,770 --> 00:06:12,950 
either a science fiction, non-fiction or
children's books customer 
94 
00:06:12,950 --> 00:06:16,000 
depending on which cluster the new
customer is most similar to. 
95 
00:06:17,530 --> 00:06:19,880 
Once cluster labels have been determined, 
96 
00:06:19,880 --> 00:06:24,660 
samples in each cluster can be used as
labeled data for a classification task. 
97 
00:06:24,660 --> 00:06:28,090 
The samples would be the input
to the classification model. 
98 
00:06:28,090 --> 00:06:31,750 
And the cluster label would be
the target class for each sample. 
99 
00:06:31,750 --> 00:06:35,840 
This process can be used to provide much
needed labeled data for classification. 
100 
00:06:37,240 --> 00:06:42,260 
Yet another use of cluster results
is as a basis for anomaly detection. 
101 
00:06:42,260 --> 00:06:44,470 
If a sample is very far away, or 
102 
00:06:44,470 --> 00:06:49,490 
very different from any of the cluster
centers, like the yellow sample here, 
103 
00:06:49,490 --> 00:06:53,720 
then that sample is a cluster outlier and
can be flagged as an anomaly. 
104 
00:06:54,740 --> 00:06:58,060 
However, these anomalies
require further analysis. 
105 
00:06:58,060 --> 00:07:02,130 
Depending on the application, these
anomalies can be considered noise, and 
106 
00:07:02,130 --> 00:07:04,470 
should be removed from the data set. 
107 
00:07:04,470 --> 00:07:08,580 
An example of this would be a sample
with a value of 150 for age. 
108 
00:07:09,840 --> 00:07:14,610 
For other cases, these anomalous cases
should be studied more carefully. 
109 
00:07:14,610 --> 00:07:18,290 
Examples of this are in a credit
card fraud detection, or, 
110 
00:07:18,290 --> 00:07:21,280 
a network intrusion detection application. 
111 
00:07:21,280 --> 00:07:25,810 
In these applications, examples outside
of the norm are the interesting cases 
112 
00:07:25,810 --> 00:07:29,840 
that should be looked at to determine
if they represent potential problems. 
113 
00:07:30,880 --> 00:07:36,440 
To summarize, cluster analysis is used to
organize similar data items into groups or 
114 
00:07:36,440 --> 00:07:37,990 
clusters. 
115 
00:07:37,990 --> 00:07:41,830 
Analyzing the resulting clusters
often leads to useful insights 
116 
00:07:41,830 --> 00:07:44,400 
about the characteristics of each group, 
117 
00:07:44,400 --> 00:07:47,240 
as well as the underlying
structure of the entire data set. 
118 
00:07:48,530 --> 00:07:50,400 
Clusters require analysis and 
119 
00:07:50,400 --> 00:07:53,140 
interpretation to make
sense of the results, 
120 
00:07:53,140 --> 00:07:58,580 
since there are no labels associated with
samples or clusters in a clustering task. 
121 
00:07:58,580 --> 00:08:02,109 
In the next lecture, we will discuss
a specific algorithm for cluster analysis. 
1 
00:00:00,050 --> 00:00:04,100 
k-means clustering is a simple yet
effective algorithm for 
2 
00:00:04,100 --> 00:00:06,820 
cluster analysis that is
commonly used in practice. 
3 
00:00:07,920 --> 00:00:08,720 
After this video, 
4 
00:00:08,720 --> 00:00:13,410 
you will be able to describe
the steps in the k-means algorithm, 
5 
00:00:13,410 --> 00:00:18,229 
explain what the k stands for in k-means
and define what a cluster centroid is. 
6 
00:00:19,300 --> 00:00:24,640 
Recall a cluster analysis divides samples
in a data set into groups or clusters. 
7 
00:00:24,640 --> 00:00:29,590 
The idea is to group similar items in
the same cluster, where similar is defined 
8 
00:00:29,590 --> 00:00:33,340 
by some metric that measures
similarity between data samples. 
9 
00:00:33,340 --> 00:00:36,760 
So the goal of cluster analysis
is to divide data sample 
10 
00:00:36,760 --> 00:00:41,260 
such that sample within a cluster
are as close together as possible. 
11 
00:00:41,260 --> 00:00:46,387 
And samples from different clusters
are as far apart as possible. 
12 
00:00:46,387 --> 00:00:50,440 
k-means is a classic algorithm used for
cluster analysis. 
13 
00:00:50,440 --> 00:00:51,810 
The algorithm is very simple. 
14 
00:00:53,040 --> 00:00:56,015 
The first step is to select
k initial centroids. 
15 
00:00:57,200 --> 00:01:01,540 
A centroid is simply the center of
a cluster, as you see in the diagram here. 
16 
00:01:03,180 --> 00:01:07,950 
Next, assign each sample in
a dataset to the closest centroid. 
17 
00:01:07,950 --> 00:01:10,920 
This means you calculate
the distance between the sample and 
18 
00:01:10,920 --> 00:01:16,790 
each cluster center and assign a sample
to the cluster with the closest centroid. 
19 
00:01:16,790 --> 00:01:18,140 
Then you calculate the mean, 
20 
00:01:18,140 --> 00:01:21,630 
or average, of each cluster
to determine a new centroid. 
21 
00:01:23,165 --> 00:01:27,330 
These two steps are then repeated until
some stopping criterion are reached. 
22 
00:01:28,450 --> 00:01:32,346 
Here's an illustration
of how k-Means works. 
23 
00:01:32,346 --> 00:01:34,460 
(a) shows the original data
set with some samples. 
24 
00:01:36,070 --> 00:01:38,990 
And (b) illustrates centroids
initially selected. 
25 
00:01:40,385 --> 00:01:42,820 
(c) shows the first iteration. 
26 
00:01:42,820 --> 00:01:46,352 
Here, samples are assigned
to the closest centroid. 
27 
00:01:46,352 --> 00:01:49,050 
In (d), the centroids are recalculated. 
28 
00:01:50,625 --> 00:01:52,560 
(e) shows the second iteration. 
29 
00:01:52,560 --> 00:01:55,540 
Samples are assigned to
the closer centroid. 
30 
00:01:55,540 --> 00:01:59,040 
Note that some samples changed
their cluster assignments. 
31 
00:01:59,040 --> 00:02:02,480 
And in (f),
the centroids are recalculated again. 
32 
00:02:02,480 --> 00:02:03,630 
Cluster assignments and 
33 
00:02:03,630 --> 00:02:08,730 
centroid calculations are repeated until
some stopping criteria is reached. 
34 
00:02:08,730 --> 00:02:10,940 
And you get your final clusters,
as shown in (f). 
35 
00:02:12,700 --> 00:02:14,770 
How are the initial centroids selected? 
36 
00:02:14,770 --> 00:02:18,050 
The issue is that the final
cluster results are sensitive 
37 
00:02:18,050 --> 00:02:19,500 
to initial centroids. 
38 
00:02:19,500 --> 00:02:22,990 
This means that cluster results with
one set of initial centroids can be 
39 
00:02:22,990 --> 00:02:27,270 
very different from results with
another set of initial centroids. 
40 
00:02:27,270 --> 00:02:30,145 
There are many approaches to
selecting the initial centroids for 
41 
00:02:30,145 --> 00:02:33,660 
k-means varying in levels
of sophistication. 
42 
00:02:33,660 --> 00:02:38,620 
The easiest and most widely used approach
is to apply k-means several times 
43 
00:02:38,620 --> 00:02:43,510 
with different initial centroids
randomly chosen to cluster you dataset. 
44 
00:02:43,510 --> 00:02:47,020 
And then select the centroids that
give the best clustering results. 
45 
00:02:48,130 --> 00:02:50,970 
To evaluate the cluster results,
an error measure, 
46 
00:02:50,970 --> 00:02:55,870 
known as the within cluster sum
of squared error, can be used. 
47 
00:02:55,870 --> 00:02:58,450 
The error associated with a sample 
48 
00:02:58,450 --> 00:03:03,720 
within a cluster is the distance between
the sample and the cluster centroid. 
49 
00:03:03,720 --> 00:03:07,840 
The squared error of the sample then,
is the squared of that distance. 
50 
00:03:09,020 --> 00:03:12,160 
We sum up all the squared errors for
all samples for 
51 
00:03:12,160 --> 00:03:15,430 
a cluster to the get the squared error for
that cluster. 
52 
00:03:16,590 --> 00:03:21,130 
We then do the same thing for all
clusters to get the final calculation for 
53 
00:03:21,130 --> 00:03:24,590 
the within-cluster sum
of squared error for 
54 
00:03:24,590 --> 00:03:27,490 
all clusters in the results
of a cluster analysis run. 
55 
00:03:28,810 --> 00:03:33,920 
Given two clustering results, the one with
the smaller within-cluster sum of squared 
56 
00:03:33,920 --> 00:03:39,400 
error, or WSSE for short,
provides the better solution numerically. 
57 
00:03:39,400 --> 00:03:42,770 
However, as we've discussed before,
there is no ground truth 
58 
00:03:42,770 --> 00:03:47,120 
to mathematically determine which set of
clusters is more correct than the other. 
59 
00:03:48,160 --> 00:03:52,561 
In addition, note that increasing
the number of clusters, 
60 
00:03:52,561 --> 00:03:56,977 
that is, increasing the value for
k, always reduces WSSE. 
61 
00:03:56,977 --> 00:04:00,510 
So WSSE should be used with caution. 
62 
00:04:00,510 --> 00:04:04,770 
It only makes sense to use WSSE
to compare two sets of clusters 
63 
00:04:04,770 --> 00:04:08,880 
with the same value for k and
generate it from the same dataset. 
64 
00:04:10,060 --> 00:04:13,410 
Also the set of clusters with
the smallest WSSE may not 
65 
00:04:13,410 --> 00:04:16,730 
always be the best solution for
the application at hand. 
66 
00:04:16,730 --> 00:04:18,150 
Again, interpretation and 
67 
00:04:18,150 --> 00:04:22,430 
domain knowledge about what the cluster
should represent and how they will be used 
68 
00:04:22,430 --> 00:04:26,710 
are crucial in determining
which cluster results are best. 
69 
00:04:26,710 --> 00:04:30,710 
Now that there are several metrics that are
used to evaluate cluster results as well. 
70 
00:04:31,950 --> 00:04:36,223 
Choosing the optimal value for k is
always a big question in using k-means. 
71 
00:04:36,223 --> 00:04:39,185 
There are several methods to
determine the value for k. 
72 
00:04:39,185 --> 00:04:41,140 
We will discuss a few here. 
73 
00:04:42,310 --> 00:04:45,880 
Visualization techniques can be used
to determine the dataset to see if 
74 
00:04:45,880 --> 00:04:48,720 
there are natural
groupings of the samples. 
75 
00:04:48,720 --> 00:04:49,700 
Scatter plots and 
76 
00:04:49,700 --> 00:04:53,450 
the use of dimensionality reduction
are useful here, to visualize the data. 
77 
00:04:54,850 --> 00:04:57,540 
A good value for
k is application-dependent. 
78 
00:04:57,540 --> 00:05:02,260 
So domain knowledge of the application can
drive the selection for the value of k. 
79 
00:05:02,260 --> 00:05:03,030 
For example, 
80 
00:05:03,030 --> 00:05:06,940 
if you want to cluster the types of
products customers are purchasing, 
81 
00:05:06,940 --> 00:05:11,850 
a natural choice for k might be the number
of product categories that you offer. 
82 
00:05:11,850 --> 00:05:16,510 
Or k might be selected to represent the
geographical locations of respondents to 
83 
00:05:16,510 --> 00:05:17,670 
a survey. 
84 
00:05:17,670 --> 00:05:19,150 
In which case, a good value for 
85 
00:05:19,150 --> 00:05:22,655 
k would be the number of regions
your interested in analyzing. 
86 
00:05:24,210 --> 00:05:28,330 
There are also data-driven method for
determining the value of k. 
87 
00:05:28,330 --> 00:05:30,140 
These methods calculate symmetric for 
88 
00:05:30,140 --> 00:05:34,570 
different values of k to determine
the best selections of k. 
89 
00:05:34,570 --> 00:05:36,540 
One such method is the elbow method. 
90 
00:05:37,790 --> 00:05:41,890 
The elbow method for determining
the value of k is shown on this plot. 
91 
00:05:41,890 --> 00:05:44,140 
As we saw in the previous slide, 
92 
00:05:44,140 --> 00:05:48,690 
WSSE, or within-cluster sum of
squared error, measures how much 
93 
00:05:48,690 --> 00:05:53,750 
data samples deviate from their respective
centroids in a set of clustering results. 
94 
00:05:53,750 --> 00:05:58,810 
If we plot WSSE for different values for
k, we can see how this 
95 
00:05:58,810 --> 00:06:04,170 
error measure changes as a value
of k changes as seen in the plot. 
96 
00:06:04,170 --> 00:06:10,010 
The bend in this error curve indicates
a drop in gain by adding more clusters. 
97 
00:06:10,010 --> 00:06:14,270 
So this elbow in the curve provides
a suggestion for a good value of k. 
98 
00:06:15,320 --> 00:06:19,540 
Note that the elbow can not always be
unambiguously determined, especially for 
99 
00:06:19,540 --> 00:06:21,170 
complex data. 
100 
00:06:21,170 --> 00:06:24,630 
And in many cases, the error curve
will not have a clear suggestion for 
101 
00:06:24,630 --> 00:06:27,270 
one value, but for multiple values. 
102 
00:06:27,270 --> 00:06:30,810 
This can be used as a guideline for
the range of values to try for k. 
103 
00:06:32,490 --> 00:06:34,650 
We've discussed choosing
the initial centroids and 
104 
00:06:34,650 --> 00:06:38,370 
looked at ways to select a value for
k, the number of clusters. 
105 
00:06:38,370 --> 00:06:40,440 
Let's now look at when to stop. 
106 
00:06:40,440 --> 00:06:42,763 
How do you know when to stop
iterating when using k-means? 
107 
00:06:42,763 --> 00:06:48,890 
One obviously stopping criterion is when
there are no changes to the centroids. 
108 
00:06:48,890 --> 00:06:52,370 
This means that no samples would
change cluster assignments. 
109 
00:06:52,370 --> 00:06:56,140 
And recalculating the centroids
will not result in any changes. 
110 
00:06:56,140 --> 00:06:59,410 
So additional iterations will
not bring about any more changes 
111 
00:06:59,410 --> 00:07:00,320 
to the cluster results. 
112 
00:07:01,510 --> 00:07:06,350 
The stopping criterion can be relaxed to
the second stopping criterion listed here. 
113 
00:07:06,350 --> 00:07:09,390 
Which is when the number of
sample changing clusters is below 
114 
00:07:09,390 --> 00:07:13,180 
a certain threshold, say 1% for example. 
115 
00:07:13,180 --> 00:07:16,750 
At this point, the clusters
are changing by only a few samples, 
116 
00:07:16,750 --> 00:07:20,820 
resulting in only minimal changes
to the final cluster results. 
117 
00:07:20,820 --> 00:07:22,350 
So the algorithm can be stopped here. 
118 
00:07:23,880 --> 00:07:27,370 
At the end of k-means we have a set
of clusters, each with a centroid. 
119 
00:07:28,370 --> 00:07:32,860 
Each centroid is the mean of
the samples assigned to that cluster. 
120 
00:07:32,860 --> 00:07:37,500 
You can think of the centroid as
a representative sample for that cluster. 
121 
00:07:37,500 --> 00:07:39,860 
So to interpret the cluster
analysis results, 
122 
00:07:39,860 --> 00:07:42,580 
we can examine the cluster centroids. 
123 
00:07:42,580 --> 00:07:46,720 
Comparing the values of the variables
between the centroids will reveal 
124 
00:07:46,720 --> 00:07:49,480 
how different or alike clusters are and 
125 
00:07:49,480 --> 00:07:52,800 
provide insights into what
each cluster represents. 
126 
00:07:52,800 --> 00:07:57,259 
For example, if the value for age is
different for different customer clusters, 
127 
00:07:57,259 --> 00:08:00,361 
this indicates that the clusters
are encoding different 
128 
00:08:00,361 --> 00:08:03,213 
customer segments by age,
among other variables. 
129 
00:08:03,213 --> 00:08:08,390 
In summary, k-means is a classic algorithm
for performing cluster analysis. 
130 
00:08:08,390 --> 00:08:11,570 
It is an algorithm that is simple
to understand and implement, and 
131 
00:08:11,570 --> 00:08:13,330 
is also efficient. 
132 
00:08:13,330 --> 00:08:17,158 
The value of k, the number of clusters,
must be specified. 
133 
00:08:17,158 --> 00:08:20,070 
And final clusters are sensitive
to initial centroids. 
1 
00:00:01,260 --> 00:00:04,590 
Linear regression is a very common
algorithm to build regression models. 
2 
00:00:06,000 --> 00:00:11,150 
After this video you will be able to
describe how linear regression works, 
3 
00:00:11,150 --> 00:00:16,120 
discuss how least squares is used in
linear regression, define simple and 
4 
00:00:16,120 --> 00:00:17,260 
multiple linear regression. 
5 
00:00:19,070 --> 00:00:24,440 
A linear regression model captures the
relationship between a numerical output 
6 
00:00:24,440 --> 00:00:26,440 
and the input variables. 
7 
00:00:26,440 --> 00:00:30,520 
The relationship is modeled as
a linear relationship hence the linear 
8 
00:00:30,520 --> 00:00:31,410 
in linear regression. 
9 
00:00:32,620 --> 00:00:37,250 
To see how linear regression works, let's
take a look at an example from the Iris 
10 
00:00:37,250 --> 00:00:41,792 
flower dataset, which is a commonly
used dataset for machine learning. 
11 
00:00:41,792 --> 00:00:46,060 
This dataset has samples of
different species of iris flowers 
12 
00:00:46,060 --> 00:00:49,630 
along with measurements such as
petal width and petal length. 
13 
00:00:49,630 --> 00:00:54,600 
Here we have a plot with petal width
measurements in centimeters on the x axis, 
14 
00:00:54,600 --> 00:00:57,610 
and petal length
measurements on the y axis. 
15 
00:00:57,610 --> 00:01:01,589 
Let's say that we want to predict
petal length based on petal width. 
16 
00:01:02,680 --> 00:01:06,240 
Then the regression task is this,
given a measurement for 
17 
00:01:06,240 --> 00:01:08,350 
petal width predict the petal length. 
18 
00:01:09,620 --> 00:01:13,650 
We can build a linear regression model to
capture this linear relationship between 
19 
00:01:13,650 --> 00:01:17,330 
the input petal width and
the output petal length. 
20 
00:01:17,330 --> 00:01:21,510 
The linear relationship for this samples
is shown as the red line on the plot. 
21 
00:01:22,910 --> 00:01:26,880 
From this example we see that linear
regression works by finding the best 
22 
00:01:26,880 --> 00:01:29,740 
fitting straight line,
through the samples. 
23 
00:01:29,740 --> 00:01:31,300 
This is called the regression line. 
24 
00:01:32,450 --> 00:01:35,360 
In the simple case with
just one input variable, 
25 
00:01:35,360 --> 00:01:38,000 
the regression line is simply a line. 
26 
00:01:38,000 --> 00:01:43,290 
The equation for a line is y = mx + b, 
27 
00:01:43,290 --> 00:01:47,530 
where m determines
the slope of the line and 
28 
00:01:47,530 --> 00:01:51,480 
b is the y intercept or
where the line crosses the y axis. 
29 
00:01:52,790 --> 00:01:54,960 
M and b are the parameters of the model. 
30 
00:01:56,130 --> 00:01:59,850 
Training a linear regression model
means adjusting these parameters to 
31 
00:01:59,850 --> 00:02:02,770 
fit the regression line to the samples. 
32 
00:02:02,770 --> 00:02:05,890 
The regression line can be
determined using what's referred to 
33 
00:02:05,890 --> 00:02:07,380 
as the least squares method. 
34 
00:02:08,540 --> 00:02:12,100 
This plot illustrates how
the least squares method works. 
35 
00:02:12,100 --> 00:02:13,980 
The yellow dots are the data samples. 
36 
00:02:15,320 --> 00:02:17,690 
The red line is the regression line, 
37 
00:02:17,690 --> 00:02:20,150 
the straight line that
goes through the samples. 
38 
00:02:20,150 --> 00:02:24,210 
This line represents the model's
prediction of the output given the input. 
39 
00:02:25,380 --> 00:02:30,010 
Each green line indicates the distance
of each sample from the regression line. 
40 
00:02:30,010 --> 00:02:33,630 
So the green line represents
the error between the prediction, 
41 
00:02:33,630 --> 00:02:37,870 
which is the value of the red regression
line and the actual value of the sample. 
42 
00:02:38,990 --> 00:02:43,080 
The square of this distance is
referred to as the residual 
43 
00:02:43,080 --> 00:02:45,390 
associated with that sample. 
44 
00:02:45,390 --> 00:02:47,970 
The least squares method
finds the regression line 
45 
00:02:47,970 --> 00:02:52,430 
that makes the sum of
the residuals as small as possible. 
46 
00:02:52,430 --> 00:02:55,920 
In other words, we want to find
the line that minimizes the sum 
47 
00:02:55,920 --> 00:02:57,800 
of the squared errors of prediction. 
48 
00:02:59,040 --> 00:03:03,160 
The goal of linear regression then is
to find the best fitting straight line 
49 
00:03:03,160 --> 00:03:05,490 
through the samples using
the least squares method. 
50 
00:03:06,820 --> 00:03:10,550 
Once the regression model is built,
we can use it to make predictions. 
51 
00:03:10,550 --> 00:03:14,200 
For example,
given a measurement of 1.5 centimeters for 
52 
00:03:14,200 --> 00:03:18,920 
petal width, the model will predict
a value of 4.5 centimeters for 
53 
00:03:18,920 --> 00:03:22,250 
petal length base on the regression
line that it has constructed. 
54 
00:03:23,450 --> 00:03:27,808 
In linear regression, if there is only
one input variable then the task is 
55 
00:03:27,808 --> 00:03:30,350 
referred to as simple linear regression. 
56 
00:03:31,490 --> 00:03:34,210 
In cases with more than
one input variables, 
57 
00:03:34,210 --> 00:03:37,850 
then it is referred to as
multiple linear regression. 
58 
00:03:37,850 --> 00:03:41,640 
To summarize, linear regression
captures the linear relationship 
59 
00:03:41,640 --> 00:03:45,328 
between a numerical output and
the input variables. 
60 
00:03:45,328 --> 00:03:49,240 
The least squares method can be used
to build a linear regression model 
61 
00:03:49,240 --> 00:03:51,780 
by finding the best fitting
line through the samples. 
1 
00:00:00,880 --> 00:00:04,540 
We studied the machine learning process,
applied techniques to explore and 
2 
00:00:04,540 --> 00:00:08,910 
prepare data, discussed the different
categories of machine learning tasks, 
3 
00:00:08,910 --> 00:00:11,200 
looked at metrics and
methods for evaluating a model, 
4 
00:00:11,200 --> 00:00:16,020 
learned how to use scalable machine
learning algorithms for big data problems, 
5 
00:00:16,020 --> 00:00:19,469 
and worked with two widely used tools to
construct the machine learning models. 
6 
00:00:20,650 --> 00:00:24,040 
I hope that the lectures, along with
the hands-on activities, have given you 
7 
00:00:24,040 --> 00:00:27,680 
a sound and practical introduction to
machine learning tools and techniques. 
8 
00:00:27,680 --> 00:00:32,010 
I also hope that the course piqued
your interest in the exiting and 
9 
00:00:32,010 --> 00:00:34,960 
rapidly developing field of
machine learning for big data. 
10 
00:00:36,110 --> 00:00:38,630 
Keep in mind that the best
way to learn machine learning 
11 
00:00:38,630 --> 00:00:40,600 
is to do machine learning. 
12 
00:00:40,600 --> 00:00:45,470 
So I encourage you to go out and find
a problem or data set that interest you. 
13 
00:00:45,470 --> 00:00:49,800 
Apply the techniques you've learned in
this course to it and start analyzing. 
14 
00:00:49,800 --> 00:00:53,110 
Thank you for your time and effort on
this course, happy machine learning. 
1 
00:00:01,260 --> 00:00:04,660 
If you recall, we have previously
discussed that the main categories of 
2 
00:00:04,660 --> 00:00:09,170 
machine learning tasks are classification,
regression, 
3 
00:00:09,170 --> 00:00:12,560 
cluster analysis, and
association analysis. 
4 
00:00:12,560 --> 00:00:15,100 
We have discussed
classification in detail. 
5 
00:00:15,100 --> 00:00:18,190 
Now let's look at the other categories,
starting with regression. 
6 
00:00:19,840 --> 00:00:24,880 
After this video you will be able
to define what regression is, 
7 
00:00:24,880 --> 00:00:28,870 
explain the difference between
regression and classification, and 
8 
00:00:28,870 --> 00:00:30,630 
name some applications of regression. 
9 
00:00:32,140 --> 00:00:36,410 
Before we talk about regression,
let's review classification. 
10 
00:00:36,410 --> 00:00:40,630 
In a classification problem the input
data is presented to the machine learning 
11 
00:00:40,630 --> 00:00:46,450 
model, and the task is to predict the
target corresponding to the input data. 
12 
00:00:46,450 --> 00:00:49,960 
The target is a categorical variable. 
13 
00:00:49,960 --> 00:00:54,132 
So the classification task is to
predict the category or label of 
14 
00:00:54,132 --> 00:00:57,450 
the target, given the input data. 
15 
00:00:57,450 --> 00:01:01,870 
The classification example shown
here is one we have seen before. 
16 
00:01:01,870 --> 00:01:06,950 
The input variables are measurements
such as temperature, relative humidity, 
17 
00:01:06,950 --> 00:01:10,790 
atmospheric pressure,
wind speed, wind direction, etc. 
18 
00:01:10,790 --> 00:01:11,500 
The task for 
19 
00:01:11,500 --> 00:01:16,590 
the model is to predict the weather
category associated with the input data. 
20 
00:01:16,590 --> 00:01:17,730 
The possible values for 
21 
00:01:17,730 --> 00:01:22,180 
the weather category is sunny,
windy, rainy, or cloudy. 
22 
00:01:22,180 --> 00:01:26,265 
Since we're predicting the category,
this is a classification task. 
23 
00:01:26,265 --> 00:01:31,080 
With that context in mind,
let's now discuss regression. 
24 
00:01:31,080 --> 00:01:35,220 
When the model has to predict
a numeric value instead of a category, 
25 
00:01:35,220 --> 00:01:38,410 
then the task becomes
a regression problem. 
26 
00:01:38,410 --> 00:01:42,170 
An example of regression is to
predict the price of a stock. 
27 
00:01:42,170 --> 00:01:45,415 
The stock price is a numeric value,
not a category. 
28 
00:01:45,415 --> 00:01:49,455 
So this is a regression task
instead of a classification task. 
29 
00:01:49,455 --> 00:01:53,755 
Note that if you were to predict not
the actual price of the stock, but whether 
30 
00:01:53,755 --> 00:01:58,505 
the stock price will go up or go down,
then that would be a classification task. 
31 
00:01:58,505 --> 00:02:02,085 
That is the main difference between
classification and regression. 
32 
00:02:02,085 --> 00:02:05,120 
In classification,
you're predicting a category, and 
33 
00:02:05,120 --> 00:02:07,270 
in regression you're
predicting a numeric value. 
34 
00:02:08,720 --> 00:02:11,810 
Here are some examples where
regression can be used. 
35 
00:02:11,810 --> 00:02:16,610 
Forecast the high temperature for the next
day, estimate the average housing price 
36 
00:02:16,610 --> 00:02:21,640 
for a particular region, determine the
demand for a new product, a new book for 
37 
00:02:21,640 --> 00:02:26,980 
example, based on similar existing
products, predict the power usage for 
38 
00:02:26,980 --> 00:02:28,160 
a particular power grid. 
39 
00:02:29,890 --> 00:02:31,780 
This is what the data
set might look like for 
40 
00:02:31,780 --> 00:02:35,190 
the regression task of predicting
tomorrow's high temperature. 
41 
00:02:35,190 --> 00:02:37,340 
The input variables could
be the high temperature for 
42 
00:02:37,340 --> 00:02:40,940 
today, the low temperature for
today, and the month. 
43 
00:02:40,940 --> 00:02:44,270 
And the target is the high temperature for
tomorrow. 
44 
00:02:44,270 --> 00:02:48,220 
The model has to predict this
target value for each sample. 
45 
00:02:48,220 --> 00:02:52,250 
Recall that in a supervised
task the target is provided. 
46 
00:02:52,250 --> 00:02:57,070 
Well, for an unsupervised task the target
is not available or not known. 
47 
00:02:57,070 --> 00:03:00,540 
Since the target label is provided for
each sample here, 
48 
00:03:00,540 --> 00:03:04,370 
the regression task is a supervised one,
similar to classification. 
49 
00:03:05,900 --> 00:03:10,580 
As with classification building a
regression model also involve two phases. 
50 
00:03:10,580 --> 00:03:13,330 
A training phase in which
the model is built, and 
51 
00:03:13,330 --> 00:03:16,410 
a testing phase in which
the model is applied to new data. 
52 
00:03:17,590 --> 00:03:21,550 
The model is built using training data and
evaluated on test data. 
53 
00:03:22,690 --> 00:03:27,011 
Similar to classification, the goal in
building a regression model is also to 
54 
00:03:27,011 --> 00:03:31,340 
have a model perform well on training
data, as well as generalize to new data. 
55 
00:03:32,850 --> 00:03:36,190 
The use of three different datasets
that we have previously discussed 
56 
00:03:36,190 --> 00:03:38,470 
also apply to regression. 
57 
00:03:38,470 --> 00:03:41,650 
Recall that the three
datasets are used as follows. 
58 
00:03:41,650 --> 00:03:46,130 
The training dataset is used to train the
model, that is to adjust the parameters of 
59 
00:03:46,130 --> 00:03:48,408 
the model to learn the input
to output mapping. 
60 
00:03:48,408 --> 00:03:53,380 
The validation dataset is used to
determine when training should stop 
61 
00:03:53,380 --> 00:03:55,700 
in order to avoid over fitting. 
62 
00:03:55,700 --> 00:04:01,320 
And the test dataset is used to evaluate
the performance of the model on new data. 
63 
00:04:01,320 --> 00:04:05,900 
In summary, in regression the model
needs to predict the numeric value 
64 
00:04:05,900 --> 00:04:07,970 
corresponding to the input data. 
65 
00:04:07,970 --> 00:04:13,160 
Since a target is provided for each
sample, regression is a supervised task. 
66 
00:04:13,160 --> 00:04:16,990 
The target is always a numerical
variable in regression. 
67 
00:04:16,990 --> 00:04:18,042 
In the next lecture, 
68 
00:04:18,042 --> 00:04:21,323 
we will discuss a specific algorithm
to build a regression model. 
1 
00:00:01,060 --> 00:00:02,330 
Hello. 
2 
00:00:02,330 --> 00:00:05,750 
Welcome to the Graph Analytics module
in the Big Data specialization. 
3 
00:00:07,180 --> 00:00:11,250 
I'm Amarnath Gupta, a research scientist
at the San Diego Supercomputer Center. 
4 
00:00:12,730 --> 00:00:14,080 
What do I do research on? 
5 
00:00:15,150 --> 00:00:20,380 
Well, a number of different areas,
all generally related to data engineering. 
6 
00:00:21,570 --> 00:00:25,740 
But the area I'm recently very
excited about has to do with graphs. 
7 
00:00:27,160 --> 00:00:30,650 
Now, graphs or
networks has many people call them, 
8 
00:00:31,650 --> 00:00:37,270 
are about studying relationships and
relationship patterns on objects. 
9 
00:00:39,570 --> 00:00:45,420 
I became interested in graphs
when I was a graduate student and 
10 
00:00:45,420 --> 00:00:51,930 
the professor in our artificial
intelligence class showed us how a part of 
11 
00:00:51,930 --> 00:00:58,240 
human knowledge can be represented as
a kind of graphs called semantic networks. 
12 
00:01:00,560 --> 00:01:05,030 
She showed us how even
very simple things like 
13 
00:01:05,030 --> 00:01:09,840 
relationships somewhere in a family can
be represented and viewed as graphs. 
14 
00:01:10,960 --> 00:01:13,121 
Represent your knowledge huh? 
15 
00:01:13,121 --> 00:01:15,030 
Now, that got me really excited. 
16 
00:01:16,160 --> 00:01:22,042 
Someday, I thought this will be a really
interesting topic to research on. 
17 
00:01:23,730 --> 00:01:29,660 
What I didn't realize then is that
in just a few years graphs and 
18 
00:01:29,660 --> 00:01:35,787 
the need to model and analyze them would
become so dominant in both academia and 
19 
00:01:35,787 --> 00:01:40,600 
industry that graphs will
be found everywhere today. 
20 
00:01:41,980 --> 00:01:46,520 
Now we look at Facebook,
LinkedIn, Twitter, and many, 
21 
00:01:46,520 --> 00:01:51,930 
many more companies that
are thriving in the market with data 
22 
00:01:51,930 --> 00:01:56,450 
that are represented, modeled,
and processed as graphs. 
23 
00:01:58,150 --> 00:02:01,210 
Now, even the entire World Wide Web,
if you think about it, 
24 
00:02:01,210 --> 00:02:03,140 
is a giant graph that people analyze. 
25 
00:02:04,320 --> 00:02:06,710 
And that brings us to this course. 
26 
00:02:06,710 --> 00:02:11,400 
Now, in this course, I'll introduce you
to the wonderful world of Graph Analytics 
27 
00:02:11,400 --> 00:02:14,700 
specifically, I'd like
to show how different 
28 
00:02:14,700 --> 00:02:19,070 
kinds of real world data science problems
can be viewed and modeled as graphs. 
29 
00:02:20,110 --> 00:02:24,856 
And how the process of solving
them can apply analytical 
30 
00:02:24,856 --> 00:02:29,710 
techniques that used graph based methods,
that is Algorithms. 
31 
00:02:32,180 --> 00:02:33,870 
This course would have four models. 
32 
00:02:35,000 --> 00:02:40,820 
In model one, we'll introduce graphs and
different applications that use graphs. 
33 
00:02:43,280 --> 00:02:48,880 
In model two,
we'll cover a number of common techniques, 
34 
00:02:48,880 --> 00:02:53,000 
mathematical and algorithm techniques,
that are used in Graph Analytics. 
35 
00:02:54,190 --> 00:02:57,264 
In module three,
we'll look at a graph database. 
36 
00:02:57,264 --> 00:03:00,288 
And through some sort
of a hands on guidance, 
37 
00:03:00,288 --> 00:03:04,840 
we'll show you how to store and
query graph data with the database. 
38 
00:03:05,870 --> 00:03:10,800 
In module four, we'll cover some
strategies of handling very large 
39 
00:03:10,800 --> 00:03:15,855 
graphs and discuss how existing
tools that are currently used by 
40 
00:03:15,855 --> 00:03:21,625 
the community are actually prints. 
41 
00:03:21,625 --> 00:03:23,585 
Thank you for joining this course and 
42 
00:03:23,585 --> 00:03:27,885 
I sincerely hope that you'll find
it both exciting and useful. 
43 
00:03:29,265 --> 00:03:29,884 
Happy learning. 
1 
00:00:00,660 --> 00:00:04,950 
As we saw in these four examples
what graphs are used for 
2 
00:00:04,950 --> 00:00:07,920 
are kind of different but they all show 
3 
00:00:07,920 --> 00:00:11,275 
different viewpoints from which you
can use graphs for your analysis. 
4 
00:00:11,275 --> 00:00:16,380 
Since this course focuses
on graph analytics, 
5 
00:00:16,380 --> 00:00:19,150 
I'd like to briefly recap
what the term means. 
6 
00:00:20,430 --> 00:00:25,790 
Analytics is the ability to
discover meaningful patterns and 
7 
00:00:25,790 --> 00:00:29,972 
interesting insights into data using
mathematical properties of data. 
8 
00:00:31,150 --> 00:00:35,260 
It covers the process of computing
with mathematical properties, and 
9 
00:00:35,260 --> 00:00:38,460 
accessing the data itself efficiently. 
10 
00:00:38,460 --> 00:00:40,970 
Further, it involves
the ability to represent and 
11 
00:00:40,970 --> 00:00:44,910 
work with domain knowledge as we
saw in use case two with biology. 
12 
00:00:44,910 --> 00:00:49,220 
Finally, analytics often involves
statistical modeling techniques for 
13 
00:00:49,220 --> 00:00:52,180 
drawing inferences and
making predictions on data. 
14 
00:00:53,510 --> 00:00:58,110 
With analytics, we should be able
to achieve the goals shown here. 
15 
00:00:59,900 --> 00:01:00,977 
Take a minute to read. 
16 
00:01:10,544 --> 00:01:15,180 
Therefore, graph analytics is
a special case of analytics where 
17 
00:01:15,180 --> 00:01:18,985 
the underlying data can be
modeled as a set of graphs. 
1 
00:00:00,890 --> 00:00:05,257 
So in this video,
we'll going to talk about graph analytics 
2 
00:00:05,257 --> 00:00:09,820 
within the context of this
big data specialization. 
3 
00:00:12,080 --> 00:00:16,330 
So in the previous courses you know about
the three important V's of big data. 
4 
00:00:17,410 --> 00:00:21,580 
So the three well-known V's are volume,
velocity, and variety. 
5 
00:00:22,940 --> 00:00:27,440 
We will also talk about a lesser-known V,
which is called valence. 
6 
00:00:29,170 --> 00:00:33,976 
Okay, what we want to talk about is, 
7 
00:00:33,976 --> 00:00:39,881 
what impact these things
have on graph data. 
8 
00:00:39,881 --> 00:00:44,693 
So for volume, let's take a dataset like 
9 
00:00:44,693 --> 00:00:49,238 
the load network of the United States. 
10 
00:00:49,238 --> 00:00:51,770 
Well, that's a pretty large graph. 
11 
00:00:51,770 --> 00:00:55,870 
So when we say volume,
I mean that the size of the graph 
12 
00:00:55,870 --> 00:00:59,790 
is much larger than what you might have 
13 
00:01:00,960 --> 00:01:05,400 
in the memory of a reasonable computer or
real computing infrastructure. 
14 
00:01:07,130 --> 00:01:12,462 
Now, we will see what impact the size 
15 
00:01:12,462 --> 00:01:18,490 
of the graph has on analytic operations. 
16 
00:01:18,490 --> 00:01:21,497 
What we mean by velocity
when it comes to graphs? 
17 
00:01:21,497 --> 00:01:22,620 
Well, think of Facebook again. 
18 
00:01:23,860 --> 00:01:25,680 
So these little graphs are updates. 
19 
00:01:26,700 --> 00:01:34,190 
So you write a post, then like somebody
else's post, and make a comment. 
20 
00:01:34,190 --> 00:01:35,774 
That's a bunch of updates. 
21 
00:01:35,774 --> 00:01:37,810 
That comes and adds to your graph. 
22 
00:01:39,110 --> 00:01:43,440 
Well, then ten minutes later,
you do something similar, and 
23 
00:01:43,440 --> 00:01:45,000 
that also comes and adds to the graph. 
24 
00:01:45,000 --> 00:01:48,060 
Then your friend does the same thing,
it adds to your graph. 
25 
00:01:48,060 --> 00:01:53,530 
So as time goes by,
you are sending more edges to your graph. 
26 
00:01:55,060 --> 00:01:59,240 
And the speed at which
you are doing this for 
27 
00:01:59,240 --> 00:02:02,870 
at least like Facebook can be really,
really high. 
28 
00:02:02,870 --> 00:02:05,590 
So the rate of update in
Facebook is really high. 
29 
00:02:06,910 --> 00:02:13,997 
This is what is called
streaming edges into graphs. 
30 
00:02:13,997 --> 00:02:19,314 
And there can be multiple streams for
various reasons. 
31 
00:02:19,314 --> 00:02:20,515 
What do we mean by variety? 
32 
00:02:20,515 --> 00:02:28,790 
For graphs, it means that the graph is
collecting data from various places. 
33 
00:02:28,790 --> 00:02:33,002 
And all these different places are giving
different kinds of information to 
34 
00:02:33,002 --> 00:02:33,671 
the graph. 
35 
00:02:33,671 --> 00:02:36,905 
So in the end,
the graph has more non-uniform and 
36 
00:02:36,905 --> 00:02:41,217 
complex information potentially
coming from multiple sources. 
37 
00:02:41,217 --> 00:02:44,210 
That's what we mean by variety
when we refer to graphs. 
38 
00:02:45,690 --> 00:02:48,930 
That picture there, by the way, is
different kinds of protein interactions. 
39 
00:02:52,600 --> 00:02:55,850 
The next one,
the less-known one is valence. 
40 
00:02:57,510 --> 00:03:02,578 
Now, if you remember your chemistry,
this comes from valence electrons, 
41 
00:03:02,578 --> 00:03:06,470 
which are electrons in an atom
which are used for bonding. 
42 
00:03:06,470 --> 00:03:09,790 
The other electrons
are called core electrons. 
43 
00:03:09,790 --> 00:03:14,274 
So the idea is if we increase
the valence of the graphs, 
44 
00:03:14,274 --> 00:03:18,183 
you increase the connectiveness
of the graph. 
45 
00:03:18,183 --> 00:03:19,901 
How, we will see. 
46 
00:03:22,920 --> 00:03:27,894 
Now, graph size clearly impacts analytics. 
47 
00:03:27,894 --> 00:03:35,041 
Why, a, it takes more space, but more
importantly, it increases the algorithmic 
48 
00:03:35,041 --> 00:03:40,190 
complexity of any operation that
you want it to on the graph. 
49 
00:03:40,190 --> 00:03:45,344 
Now, we'll see an example of that,
but what happens as 
50 
00:03:45,344 --> 00:03:50,948 
a result is that the data-to-analysis
time becomes high. 
51 
00:03:50,948 --> 00:03:54,240 
So I put in some data, and
I wanted to do this analysis. 
52 
00:03:54,240 --> 00:03:59,120 
But there is so much data, that my
analysis takes way longer than it should. 
53 
00:04:00,620 --> 00:04:05,343 
Let's give a simple example,
an example we have seen before. 
54 
00:04:05,343 --> 00:04:10,373 
Remember, we had this little graph
from our biological example where we 
55 
00:04:10,373 --> 00:04:16,488 
were asking, find a simple path between
Alzheimer's Disease and Colorectal Cancer. 
56 
00:04:16,488 --> 00:04:18,925 
And in this case, the result is obvious. 
57 
00:04:22,524 --> 00:04:25,589 
Now, let's pause and ask. 
58 
00:04:25,589 --> 00:04:29,248 
There are two nodes that I mentioned,
in this case, 
59 
00:04:29,248 --> 00:04:33,170 
my Colorectal Cancer and
Alzheimer's Disease nodes. 
60 
00:04:34,800 --> 00:04:39,940 
And we are asking,
is there a simple path connecting them? 
61 
00:04:42,290 --> 00:04:44,470 
This is called a decision problem. 
62 
00:04:44,470 --> 00:04:51,988 
I give you a data, and I'm asking does
such a simple path exist or not exist? 
63 
00:04:51,988 --> 00:04:55,124 
But this is actually a very
hard decision problem. 
64 
00:04:55,124 --> 00:04:59,964 
And the computer scientists will tell you
that this is a very complicated problem 
65 
00:04:59,964 --> 00:05:02,321 
because it has a very high complexity. 
66 
00:05:02,321 --> 00:05:04,340 
Let's ask another question. 
67 
00:05:04,340 --> 00:05:07,070 
Well, how many simple paths,
now I want to count. 
68 
00:05:07,070 --> 00:05:09,430 
How many simple paths exist
between these two nodes? 
69 
00:05:10,810 --> 00:05:13,580 
Indeed, it is another
hard computing problem. 
70 
00:05:15,370 --> 00:05:19,750 
And if you really want to know,
the size of the result, 
71 
00:05:19,750 --> 00:05:24,780 
in the worst case is exponential
in the number of nodes. 
72 
00:05:24,780 --> 00:05:27,930 
So if we increase the number of nodes and
edges, if we increase the size of 
73 
00:05:27,930 --> 00:05:35,490 
the graph such a seemingly simple question
can take a very, very, very long time. 
74 
00:05:37,180 --> 00:05:40,890 
So that it's almost practically
impossible to compute it for 
75 
00:05:40,890 --> 00:05:44,910 
a really large graph if we have
no other information supporting. 
76 
00:05:44,910 --> 00:05:47,008 
That's the worst case. 
77 
00:05:47,008 --> 00:05:52,441 
But when we say algorithmic complexity
increases, that's what we mean. 
78 
00:05:55,692 --> 00:06:00,894 
Let's talk about velocity, and
I said our favorite example is Facebook. 
79 
00:06:00,894 --> 00:06:05,096 
So we are adding a bunch of updates, which
means we are adding a bunch of edges. 
80 
00:06:05,096 --> 00:06:11,490 
We are streaming the edges into the data,
and we want to compute a metric. 
81 
00:06:11,490 --> 00:06:15,791 
We want to see what is
the shortest distance 
82 
00:06:15,791 --> 00:06:20,697 
between person a and
person b or item a and item b. 
83 
00:06:20,697 --> 00:06:25,155 
Or I want to know that
Facebook has communities. 
84 
00:06:25,155 --> 00:06:26,690 
Twitter has communities like we saw. 
85 
00:06:28,220 --> 00:06:32,540 
And how many people out there,
in these communities, and 
86 
00:06:32,540 --> 00:06:35,600 
how many such communities are there,
like a Facebook group? 
87 
00:06:37,560 --> 00:06:42,180 
Now, if you want to compute this metric,
and you get this 
88 
00:06:42,180 --> 00:06:46,449 
edges very fast, it is very difficult
to know when you have the answer. 
89 
00:06:47,480 --> 00:06:55,450 
Because you are going to get an increasing
number of edges in the system, 
90 
00:06:56,450 --> 00:07:01,660 
and you keep computing this metric that
you want to find the answer for, and 
91 
00:07:01,660 --> 00:07:07,040 
it will turn out that your continuous
stream does not fit in memory. 
92 
00:07:07,040 --> 00:07:11,820 
Because your memory is limited
compared to the amount of 
93 
00:07:13,930 --> 00:07:17,680 
edges, or edge updates you
are streaming into the system. 
94 
00:07:19,010 --> 00:07:23,221 
So that's what's happened when you
have high velocity information. 
95 
00:07:23,221 --> 00:07:28,584 
Very soon, your memory runs out,
and you want to compute 
96 
00:07:28,584 --> 00:07:33,290 
your answer right now from
the data that you have. 
97 
00:07:36,110 --> 00:07:39,640 
Okay, let's look at variety,
also known as heterogeneity. 
98 
00:07:41,460 --> 00:07:44,881 
There are two aspects of heterogeneity. 
99 
00:07:44,881 --> 00:07:50,803 
One, we have already mentioned, graph data
is often created through integration, 
100 
00:07:50,803 --> 00:07:53,449 
like we saw in the case of the biology. 
101 
00:07:56,446 --> 00:08:02,530 
And therefore, the variety comes because
the nature of data is very different. 
102 
00:08:04,510 --> 00:08:07,800 
Also, they may not be all
the same kind of data. 
103 
00:08:08,890 --> 00:08:13,340 
For example, the data may come
from a relational database, 
104 
00:08:14,440 --> 00:08:16,580 
it may come from an XML database. 
105 
00:08:16,580 --> 00:08:18,900 
It may come from another graph. 
106 
00:08:18,900 --> 00:08:20,130 
It may come from a document. 
107 
00:08:21,470 --> 00:08:26,910 
It may even come from complex
things like social networks, 
108 
00:08:26,910 --> 00:08:31,890 
like citation networks between papers or
patents, between interaction networks, 
109 
00:08:33,070 --> 00:08:36,300 
between web entities,
which are connected through links. 
110 
00:08:37,780 --> 00:08:43,360 
And from human knowledge that has been
represented as graphs through ontologists. 
111 
00:08:43,360 --> 00:08:49,476 
So all of these graphs, the nodes and
the edges do not mean the same thing. 
112 
00:08:49,476 --> 00:08:54,083 
And somehow in there,
you need to capture what it means to have 
113 
00:08:54,083 --> 00:08:58,971 
an edge because that will determine
what you can do with the edge. 
114 
00:08:58,971 --> 00:09:03,490 
A simple example, in an ontology, 
115 
00:09:03,490 --> 00:09:10,280 
is something that says a is a b,
and b is a c, so a is a c. 
116 
00:09:11,730 --> 00:09:18,430 
The a is a c is an inference that you do,
given the other two relationships. 
117 
00:09:18,430 --> 00:09:21,133 
What would be an example? 
118 
00:09:21,133 --> 00:09:27,570 
My pet is a dog, and the dog is a mammal,
therefore, my pet is a mammal. 
119 
00:09:28,770 --> 00:09:32,320 
You want to do inferences for
some edges likes is a. 
120 
00:09:33,480 --> 00:09:36,520 
Now, you need to know this. 
121 
00:09:36,520 --> 00:09:40,140 
You do not do this with the biology
example where you are looking at genes and 
122 
00:09:40,140 --> 00:09:45,447 
proteins because that operation does
not make sense when you have genes and 
123 
00:09:45,447 --> 00:09:46,230 
proteins. 
124 
00:09:46,230 --> 00:09:50,720 
So therefore, every graph may
have a different semantics. 
125 
00:09:50,720 --> 00:09:53,880 
And what happens with variety is
the number of sub-semantics and 
126 
00:09:53,880 --> 00:09:56,670 
the number of valid
operations that you can do. 
127 
00:09:56,670 --> 00:09:58,730 
That changes, and
that becomes more complex. 
128 
00:10:00,395 --> 00:10:04,720 
Now, valence I said,
is about connectedness. 
129 
00:10:04,720 --> 00:10:07,760 
It is also about
interdependency among data. 
130 
00:10:07,760 --> 00:10:12,844 
So if I have a higher valence which means,
I have more data elements that 
131 
00:10:12,844 --> 00:10:18,040 
are more strongly related, and
these relationships can be exploited. 
132 
00:10:20,820 --> 00:10:25,437 
In most cases,
the part where valence becomes important, 
133 
00:10:25,437 --> 00:10:31,806 
is that it increases over time, which
means, parts of the graph becomes denser, 
134 
00:10:31,806 --> 00:10:36,264 
and the average distance
between node pairs decreases. 
135 
00:10:36,264 --> 00:10:39,210 
Let me show you, here is my Gmail. 
136 
00:10:41,080 --> 00:10:49,140 
And I have plotted my Gmail graphs
from 2006 to about two months back. 
137 
00:10:50,950 --> 00:10:54,810 
When I first started using it,
I had these users, 
138 
00:10:54,810 --> 00:10:59,140 
a very few users, and
they are not really related. 
139 
00:11:00,170 --> 00:11:02,220 
Now with time, more and 
140 
00:11:02,220 --> 00:11:06,200 
more people started communicating
with me through Gmail. 
141 
00:11:07,290 --> 00:11:11,618 
And more and more of these people were
also talking amongst themselves and 
142 
00:11:11,618 --> 00:11:13,240 
copying me and responding to me. 
143 
00:11:14,410 --> 00:11:20,130 
By the end, you would see that
you can find dense groups 
144 
00:11:20,130 --> 00:11:24,290 
within my Gmail because
the information and 
145 
00:11:24,290 --> 00:11:31,020 
the connectedness between people have
evolved and become more dense over time. 
146 
00:11:31,020 --> 00:11:36,620 
This is the phenomenon of valence, and
this is very important to study because 
147 
00:11:36,620 --> 00:11:43,120 
you want to study things like, what parts
of the graph have become more dense? 
148 
00:11:43,120 --> 00:11:44,801 
And why have they become more dense? 
149 
00:11:44,801 --> 00:11:46,490 
Maybe something was going on. 
150 
00:11:46,490 --> 00:11:49,010 
Maybe there was an event that
brought these people together, 
151 
00:11:49,010 --> 00:11:52,550 
and you want to analyze that and
find that out from your graph analytics. 
152 
00:11:54,090 --> 00:11:59,067 
That's why you want to understand
the effect of valence. 
153 
00:11:59,067 --> 00:12:04,361 
You also want to understand what do I do
if the graph becomes very, very dense 
154 
00:12:04,361 --> 00:12:09,840 
in a place, so that finding a path through
that dense space becomes very hard. 
155 
00:12:11,830 --> 00:12:17,532 
You will see in a later video that when
this happens, the computer system, 
156 
00:12:17,532 --> 00:12:23,597 
that is trying to process these graphs in
a parallel and distributed way has to do 
157 
00:12:23,597 --> 00:12:29,507 
something special to handle these
increasing density in parts of the graph. 
1 
00:00:01,190 --> 00:00:06,640 
So before we go into graph analytics,
the first question we want to ask is, 
2 
00:00:06,640 --> 00:00:07,168 
what is a graph? 
3 
00:00:07,168 --> 00:00:10,332 
Let's see. 
4 
00:00:10,332 --> 00:00:15,360 
This is a plot of the sales of
some items against time and 
5 
00:00:15,360 --> 00:00:20,190 
it gives you a nice visual representation
of the data, but is it a graph? 
6 
00:00:21,470 --> 00:00:23,760 
While you think about it,
let's try another one. 
7 
00:00:25,790 --> 00:00:29,170 
This is another common visual
representation of data. 
8 
00:00:29,170 --> 00:00:31,450 
If you go to Google and type pie graph, 
9 
00:00:31,450 --> 00:00:34,750 
you will see many results
that look like this. 
10 
00:00:34,750 --> 00:00:35,580 
But is this a graph? 
11 
00:00:36,750 --> 00:00:38,950 
In our research, this is not a graph. 
12 
00:00:40,260 --> 00:00:41,700 
We call it a chart. 
13 
00:00:43,420 --> 00:00:46,190 
Yes, you guessed it right this time. 
14 
00:00:46,190 --> 00:00:48,070 
This too is a chart, and not a graph. 
15 
00:00:49,760 --> 00:00:52,460 
So why do people call them graphs, then? 
16 
00:00:53,630 --> 00:00:56,460 
In a sense, they're abbreviating. 
17 
00:00:56,460 --> 00:00:59,519 
A chart depicts what's called
a graph of a function. 
18 
00:01:00,580 --> 00:01:01,150 
Let me explain. 
19 
00:01:02,300 --> 00:01:03,970 
Look at the two column table on the right. 
20 
00:01:05,000 --> 00:01:08,170 
The first column has information
about product category 
21 
00:01:08,170 --> 00:01:10,940 
with values like furniture,
office supplies, and technology. 
22 
00:01:12,120 --> 00:01:17,170 
The second column represents another set
of values called total containing 1724, 
23 
00:01:17,170 --> 00:01:21,570 
4610, and 2065. 
24 
00:01:21,570 --> 00:01:25,320 
Now we can define our mapping option. 
25 
00:01:25,320 --> 00:01:29,180 
Which means a correspondence
from Product Category to Total. 
26 
00:01:30,190 --> 00:01:34,070 
So we map Furniture to the value 1,724, 
27 
00:01:34,070 --> 00:01:37,440 
Office Supplies to the value 4,610,
and so forth. 
28 
00:01:37,440 --> 00:01:39,480 
If we visually portray this, 
29 
00:01:39,480 --> 00:01:42,820 
we can represent it like a bar chart or
a pie chart. 
30 
00:01:42,820 --> 00:01:47,560 
This is why both the previous diagram and
this diagram are charts and not graphs. 
31 
00:01:50,190 --> 00:01:54,990 
Graph analytics has its basis in a brand
of mathematics called graph theory. 
32 
00:01:54,990 --> 00:01:55,830 
What's more interesting, 
33 
00:01:55,830 --> 00:02:00,640 
however, is that graph theory was
born out of a very practical problem. 
34 
00:02:02,040 --> 00:02:06,900 
The problem started in a very old city
in Prussia which is now in Russia 
35 
00:02:07,960 --> 00:02:08,995 
called Konigsberg. 
36 
00:02:10,890 --> 00:02:15,528 
Even if we look at in in Google Maps
it would sort of look like this. 
37 
00:02:15,528 --> 00:02:20,135 
One of the interesting features of
Konigsberg is that it has two islands and 
38 
00:02:20,135 --> 00:02:23,074 
these islands are connected
by seven bridges. 
39 
00:02:23,074 --> 00:02:29,624 
Back in 1736, the city wanted to create
a walkway, and the criteria was, 
40 
00:02:29,624 --> 00:02:35,083 
this walkway would traverse all
seven bridges such that somebody 
41 
00:02:35,083 --> 00:02:41,556 
wanting to go from one part of the city
to another can cross a bridge only once. 
42 
00:02:41,556 --> 00:02:46,120 
Now this is sort of an urban
planning problem right? 
43 
00:02:46,120 --> 00:02:49,520 
Well in fact, it required
a mathematician to solve the problem. 
44 
00:02:50,850 --> 00:02:54,440 
The mathematician named Euler shown
on the left looked at this and 
45 
00:02:54,440 --> 00:02:58,260 
figured out that you really
cannot create such a walk. 
46 
00:02:58,260 --> 00:02:59,510 
Why? 
47 
00:02:59,510 --> 00:03:03,030 
He said, it cannot be done, because
there are an odd number of bridges, and 
48 
00:03:03,030 --> 00:03:04,870 
proved it mathematically. 
49 
00:03:04,870 --> 00:03:08,120 
From this problem, the whole field
called graph theory emerged. 
50 
00:03:09,670 --> 00:03:14,600 
On the right, you can see Edsger Dijkstra,
a well-known computer scientist who has 
51 
00:03:14,600 --> 00:03:19,330 
developed graph algorithms, one of which
we will study later in the course. 
52 
00:03:19,330 --> 00:03:24,120 
His work has had far further impact on
both the theoretical computer science and 
53 
00:03:24,120 --> 00:03:25,040 
practical applications. 
54 
00:03:26,760 --> 00:03:29,130 
What's the difference between
the mathematics view and 
55 
00:03:29,130 --> 00:03:30,140 
the computer science view? 
56 
00:03:31,190 --> 00:03:33,170 
Let's try to define
the mathematical view of graphs. 
57 
00:03:34,490 --> 00:03:36,970 
We start with a set of vertices. 
58 
00:03:36,970 --> 00:03:41,595 
Here we have a set of six nodes or
vertices. 
59 
00:03:41,595 --> 00:03:44,060 
Now, I'll add another set. 
60 
00:03:44,060 --> 00:03:45,810 
I'll call this the set of edges. 
61 
00:03:47,020 --> 00:03:49,720 
In our diagram there are only four edges,
but 
62 
00:03:49,720 --> 00:03:51,560 
there's something special
about these edges. 
63 
00:03:52,990 --> 00:03:57,720 
Each edge is just not
an ordinary atomic object. 
64 
00:03:57,720 --> 00:04:03,390 
An edge like e1, actually, is one term
from v and then a second term from v. 
65 
00:04:04,510 --> 00:04:08,260 
So this edge, e1, goes from v1 to v5. 
66 
00:04:09,650 --> 00:04:12,230 
Pictorially, we can draw and
arrow from v1 to v5. 
67 
00:04:13,800 --> 00:04:17,270 
Now if I had said v5 to v1,
the arrow would be reversed. 
68 
00:04:18,370 --> 00:04:19,870 
So what do we have so far? 
69 
00:04:19,870 --> 00:04:23,070 
We have a set of vertices and
a set of edges. 
70 
00:04:23,070 --> 00:04:25,550 
That is the mathematical
definition of a graph. 
71 
00:04:27,900 --> 00:04:30,050 
What about the computer
scientist's definition? 
72 
00:04:31,410 --> 00:04:35,760 
Of course a computer scientist needs to
adhere to the mathematical definition, but 
73 
00:04:35,760 --> 00:04:39,120 
they want to represent and
manipulate the same information. 
74 
00:04:39,120 --> 00:04:41,380 
So they need a data structure. 
75 
00:04:41,380 --> 00:04:44,220 
In other words,
something they can operate on. 
76 
00:04:44,220 --> 00:04:45,720 
So what kind of operations would they do? 
77 
00:04:46,808 --> 00:04:47,650 
Well, with a graph, 
78 
00:04:47,650 --> 00:04:52,340 
they can say add edge, or add a new
vertex, find the nearest neighbors of 
79 
00:04:52,340 --> 00:04:56,100 
the vertex where the term neighbor refers
to the nodes connected to the vertex. 
80 
00:04:58,320 --> 00:05:02,590 
We said a computer scientist needs to
represent graphs using data structures. 
81 
00:05:02,590 --> 00:05:04,400 
Here is one that you should recognize. 
82 
00:05:04,400 --> 00:05:07,360 
It's a matrix,
called the adjacency matrix of a graph. 
83 
00:05:08,390 --> 00:05:11,560 
Both the rows and
columns of this matrix represent notes. 
84 
00:05:13,080 --> 00:05:18,718 
If I go from v1 one along the row,
I see that there's one at v3, 
85 
00:05:18,718 --> 00:05:22,597 
which means there is
an edge from v1 to v3. 
86 
00:05:22,597 --> 00:05:27,543 
Similarly there is another from v1 to v5. 
87 
00:05:27,543 --> 00:05:29,563 
Let's look at some operation. 
88 
00:05:29,563 --> 00:05:33,580 
Let's say I first want to
add an edge from v3 to v2. 
89 
00:05:33,580 --> 00:05:35,298 
What should I do? 
90 
00:05:35,298 --> 00:05:38,831 
I'll start from the row v3, 
91 
00:05:38,831 --> 00:05:44,493 
go up to the column v2,
and add a 1 in that set. 
92 
00:05:44,493 --> 00:05:48,030 
So I've added an edge,
which is an operation on the matrix. 
93 
00:05:49,890 --> 00:05:50,700 
Another operation. 
94 
00:05:52,020 --> 00:05:53,540 
I want to get the neighbors a v3. 
95 
00:05:54,600 --> 00:05:56,180 
This will be a little more complicated. 
96 
00:05:58,020 --> 00:06:01,090 
I look at the row v3 and
paint that row and 
97 
00:06:01,090 --> 00:06:03,930 
I'll also look at the column v3 and
paint that column. 
98 
00:06:05,370 --> 00:06:11,430 
If we go down the row of v3,
we'll get v2, so v2 is neighbor. 
99 
00:06:11,430 --> 00:06:16,150 
And if we go down the column of v3,
we'll get v1 and v6. 
100 
00:06:17,420 --> 00:06:20,930 
So v1 and v6 have alternators. 
101 
00:06:20,930 --> 00:06:21,540 
What's the difference? 
102 
00:06:24,250 --> 00:06:30,070 
Since the matrix has the From along
the rows and the Tos along the columns, 
103 
00:06:30,070 --> 00:06:33,950 
following the row v3 gives us
the edges outgoing from v3. 
104 
00:06:34,960 --> 00:06:38,910 
And following the column v3 gives
us the edges coming into v3. 
105 
00:06:41,590 --> 00:06:44,340 
Is this the only
representation of the graph? 
106 
00:06:44,340 --> 00:06:45,830 
No it's not. 
107 
00:06:45,830 --> 00:06:46,390 
Here is another one, 
108 
00:06:47,490 --> 00:06:51,300 
in this representation there
are two kinds of data objects. 
109 
00:06:51,300 --> 00:06:55,800 
No data, which are the blue rectangles and
edge data which are the triangles. 
110 
00:06:57,080 --> 00:07:01,000 
To get a sense of this representation,
look at the note v1 and 
111 
00:07:01,000 --> 00:07:03,490 
follow the top yellow line. 
112 
00:07:03,490 --> 00:07:05,490 
It'll reach v3. 
113 
00:07:05,490 --> 00:07:10,330 
So this link structure directly captures
the graph diagram we created before. 
114 
00:07:10,330 --> 00:07:15,150 
Now, start from v1 again and
follow the blue link and 
115 
00:07:15,150 --> 00:07:18,450 
it will reach e1 and then v3. 
116 
00:07:18,450 --> 00:07:22,630 
So that tells you that e1 is
an edge object between v1 and v3. 
117 
00:07:24,530 --> 00:07:30,020 
Next, let's stand on the edge triangle e1,
and follow the red 
118 
00:07:30,020 --> 00:07:35,450 
dashed line to get to e2, which is
the next edge from the same starting node. 
119 
00:07:36,570 --> 00:07:38,790 
Is this possibly too complex? 
120 
00:07:38,790 --> 00:07:40,200 
Yes it is. 
121 
00:07:40,200 --> 00:07:45,390 
However, as we'll see down the road, many
graph database systems are using this kind 
122 
00:07:45,390 --> 00:07:50,050 
of data structure internally, so that the
database operations become more efficient. 
1 
00:00:00,860 --> 00:00:03,220 
In the first example
of graph analytics for 
2 
00:00:03,220 --> 00:00:06,340 
big data, we'll consider
the social media called Twitter. 
3 
00:00:07,480 --> 00:00:09,320 
And graphs representing tweets. 
4 
00:00:11,160 --> 00:00:14,600 
What types of objects and
relationships does a tweet have? 
5 
00:00:14,600 --> 00:00:19,240 
Well, in fact, tweets consist
of many of the same elements and 
6 
00:00:19,240 --> 00:00:20,890 
relationships as the Facebook posts. 
7 
00:00:22,910 --> 00:00:26,380 
Tweets have users, they contain text, 
8 
00:00:26,380 --> 00:00:31,780 
tweets can point to one another or
to URL's, they have hashtags, they can 
9 
00:00:31,780 --> 00:00:36,720 
include reference to various other types
of media, but what about relationships? 
10 
00:00:38,120 --> 00:00:42,120 
Well much like Facebook, relationships and 
11 
00:00:42,120 --> 00:00:46,660 
tweets are more reflective of
what the users do with tweets. 
12 
00:00:46,660 --> 00:00:50,360 
For example, people create tweets,
they respond to tweets, 
13 
00:00:50,360 --> 00:00:52,030 
they mention other users, and so forth. 
14 
00:00:53,300 --> 00:00:55,850 
Let's look at these
relationships in detail. 
15 
00:00:57,290 --> 00:01:02,600 
In this graph, all of the light
blue notes are tweets, and 
16 
00:01:02,600 --> 00:01:05,090 
all of the purple notes are users. 
17 
00:01:05,090 --> 00:01:07,930 
When a specific user
creates a specific tweet, 
18 
00:01:07,930 --> 00:01:10,240 
an entity is created in
the graph as a result. 
19 
00:01:11,280 --> 00:01:13,580 
We'll come back to this graph
in the next few slides. 
20 
00:01:14,580 --> 00:01:17,980 
So what we want to do with it? 
21 
00:01:17,980 --> 00:01:19,820 
Well to be very specific, 
22 
00:01:19,820 --> 00:01:23,480 
this graph was created from tweets
of a bunch of online gamers. 
23 
00:01:24,700 --> 00:01:28,220 
In this case, we were monitoring
the tweets of all people 
24 
00:01:28,220 --> 00:01:30,112 
who played particular online video game. 
25 
00:01:31,205 --> 00:01:34,415 
Typically, gamers are very
excited about their video games. 
26 
00:01:34,415 --> 00:01:37,295 
They're exited about their characters
of the video games and they want to 
27 
00:01:37,295 --> 00:01:41,695 
discuss the video games they're excited
about, when new versions are released. 
28 
00:01:43,325 --> 00:01:46,195 
So what are the data
science questions here? 
29 
00:01:46,195 --> 00:01:50,420 
People who look at these things, such as
behavioral psychologists, want to know. 
30 
00:01:50,420 --> 00:01:56,420 
If you are a war game user or
if you play any other violent game, where 
31 
00:01:56,420 --> 00:02:01,830 
there is a lot of fighting online, do
these people also show violent behavior? 
32 
00:02:01,830 --> 00:02:03,110 
Maybe they do, maybe they don't. 
33 
00:02:04,290 --> 00:02:08,430 
They also want to know whether as a result
of looking at somebody's tweets and 
34 
00:02:08,430 --> 00:02:13,610 
following them over time they can tell if
the person is addicted to the game or not. 
35 
00:02:13,610 --> 00:02:15,190 
So why use graphs for that? 
36 
00:02:16,960 --> 00:02:19,850 
One of the things you could
do is what we showed here, 
37 
00:02:20,860 --> 00:02:25,210 
from the graphs you can extract
certain elements like conversations. 
38 
00:02:27,530 --> 00:02:32,320 
If you look at it, you will see all of
them have somebody posting something. 
39 
00:02:32,320 --> 00:02:37,100 
Somebody responding to the post, and the
first person responding to the response. 
40 
00:02:37,100 --> 00:02:40,390 
They are retweeting, and
they are responding, and so forth. 
41 
00:02:40,390 --> 00:02:44,478 
You would see a conversation chain
going on which can be short or long, 
42 
00:02:44,478 --> 00:02:47,940 
like you see, and
other people would join in. 
43 
00:02:47,940 --> 00:02:50,140 
We don't know if it's violent or not, but 
44 
00:02:50,140 --> 00:02:52,909 
at least there is a lively
conversation going on about something. 
45 
00:02:54,730 --> 00:02:59,870 
Using graphs, we could find some
meaningful parts of a graph 
46 
00:02:59,870 --> 00:03:03,600 
that we can further analyze using
techniques like text analytics. 
47 
00:03:05,130 --> 00:03:07,870 
Another kind of thing people
would like to identify, 
48 
00:03:07,870 --> 00:03:11,170 
is which players are interacting
with which other players? 
49 
00:03:11,170 --> 00:03:12,580 
Who are these people? 
50 
00:03:12,580 --> 00:03:14,550 
Do they form a close community? 
51 
00:03:14,550 --> 00:03:16,470 
Can anybody join in? 
52 
00:03:16,470 --> 00:03:19,060 
Are there groups, what are the groups? 
53 
00:03:19,060 --> 00:03:21,870 
If you find a group,
who are the most influential users? 
54 
00:03:23,710 --> 00:03:27,740 
Who are the people that everybody
refers to, listens to and so on? 
55 
00:03:27,740 --> 00:03:31,989 
Graph analytics is used to answer all
these questions about a conversation 
56 
00:03:31,989 --> 00:03:35,467 
that's going on live in
social media stream. 
57 
00:03:35,467 --> 00:03:37,681 
Let's look at our next example. 
1 
00:00:01,160 --> 00:00:03,230 
Our second use case is about biology. 
2 
00:00:04,610 --> 00:00:06,790 
Interactions arise naturally in biology. 
3 
00:00:08,410 --> 00:00:13,120 
Genes produce proteins, proteins regulate
the functions of other proteins, 
4 
00:00:14,400 --> 00:00:16,970 
cells transmit signals to other cells. 
5 
00:00:18,215 --> 00:00:21,610 
These functional genes lead to
pathological conditions that 
6 
00:00:21,610 --> 00:00:24,980 
present themselves as
observable phenotypes. 
7 
00:00:24,980 --> 00:00:28,520 
Some of these interactions that are
observed and measured by experiment lists. 
8 
00:00:30,200 --> 00:00:33,770 
Another class of graph relations
represent human knowledge. 
9 
00:00:33,770 --> 00:00:38,390 
For example, an edge may denote
an anatomical association, such as 
10 
00:00:38,390 --> 00:00:42,986 
the nucleuses inside a cell, part of
the cerebral cortex is part of the brain. 
11 
00:00:42,986 --> 00:00:47,820 
Similarly, one can encode different 
12 
00:00:47,820 --> 00:00:51,950 
classifications of entities like
both humans and dogs and mammals. 
13 
00:00:53,470 --> 00:00:58,980 
Researchers also find these
relationships from literature 
14 
00:00:58,980 --> 00:01:02,890 
or by computational techniques
like bioinformatics algorithms. 
15 
00:01:03,900 --> 00:01:08,120 
There are many bioinformatics algorithms
that find statistical correlations between 
16 
00:01:08,120 --> 00:01:08,880 
genes and proteins. 
17 
00:01:09,880 --> 00:01:15,070 
Many biological networks are created by
creating edges between entities because 
18 
00:01:15,070 --> 00:01:19,140 
they are strongly correlated based on
measurements like gene expression level. 
19 
00:01:21,110 --> 00:01:25,280 
Yet, a third category of
computed relationships 
20 
00:01:25,280 --> 00:01:29,090 
associates terms by mining
scientific literature. 
21 
00:01:29,090 --> 00:01:33,320 
If two entities are co-mentioned
in scientific articles very often, 
22 
00:01:33,320 --> 00:01:37,100 
then there is a likelihood that
these two entities are related. 
23 
00:01:37,100 --> 00:01:41,910 
In this case, the edge between them does
not depict a specific relationship, but 
24 
00:01:41,910 --> 00:01:43,670 
the fact they are associated. 
25 
00:01:45,150 --> 00:01:51,360 
All these interactions can be assembled
into networks of graphs where the nodes 
26 
00:01:51,360 --> 00:01:56,800 
are biological entities and
edges represent different categories of 
27 
00:01:56,800 --> 00:02:02,012 
molecular interactions,
associations between diseases and. 
28 
00:02:02,012 --> 00:02:05,400 
Now there are two issues I
would like you to understand. 
29 
00:02:05,400 --> 00:02:11,150 
First, graphs like the one
we saw are assembled. 
30 
00:02:11,150 --> 00:02:17,160 
That is, integrated from many data sources
produced by many independent science 
31 
00:02:17,160 --> 00:02:23,600 
groups who have different research goals,
use different scientific techniques, 
32 
00:02:23,600 --> 00:02:28,120 
but the underlying biological entities
are common across many groups. 
33 
00:02:29,180 --> 00:02:33,690 
It is the commonality that helps us
stitch together these logic graphs. 
34 
00:02:33,690 --> 00:02:39,950 
Second, as more results get linked and
integrated, the size of the networks grow, 
35 
00:02:39,950 --> 00:02:43,370 
leading to big data problems and
the need for big data technology. 
36 
00:02:44,700 --> 00:02:47,050 
Now let's ask the same question. 
37 
00:02:47,050 --> 00:02:47,660 
Why graphs? 
38 
00:02:49,180 --> 00:02:54,060 
In bio medicine people are always
trying to discover new science, for 
39 
00:02:54,060 --> 00:02:57,810 
instance, they want to discover
unknown relationships. 
40 
00:02:57,810 --> 00:03:03,600 
For example, they can take two diseases,
very different diseases, colorectal 
41 
00:03:03,600 --> 00:03:09,048 
cancer and Alzheimers disease, and
they might want to ask are they related? 
42 
00:03:09,048 --> 00:03:11,210 
If so, can you connect the dots and 
43 
00:03:11,210 --> 00:03:15,270 
find out what the intervening
network between them is? 
44 
00:03:15,270 --> 00:03:19,310 
It turns out, as you can see,
there are several genes that directly or 
45 
00:03:19,310 --> 00:03:21,080 
indirectly connect these two diseases. 
46 
00:03:22,180 --> 00:03:26,726 
Thus, we can use path finding techniques
like this to discover previously 
47 
00:03:26,726 --> 00:03:28,939 
unknown connections in a network. 
48 
00:03:30,280 --> 00:03:31,380 
In addition, 
49 
00:03:31,380 --> 00:03:36,570 
researchers sometimes think to explore the
networks to help their discovery process. 
50 
00:03:36,570 --> 00:03:40,510 
I have collaborated with a group that
is using graph exploration techniques 
51 
00:03:40,510 --> 00:03:44,570 
to figure out how phenotypes of
undiagnosed diseases all fit together. 
1 
00:00:01,610 --> 00:00:05,600 
Our third use case is about
the human information network. 
2 
00:00:05,600 --> 00:00:07,870 
That means a personal information network. 
3 
00:00:08,950 --> 00:00:11,680 
The graphic you see there
is my LinkedIn network. 
4 
00:00:13,030 --> 00:00:15,060 
Everybody who is on my LinkedIn is a node. 
5 
00:00:16,070 --> 00:00:19,500 
And if they know each other,
there's an edge between them. 
6 
00:00:19,500 --> 00:00:22,880 
You can possibly see some groups or
clusters there. 
7 
00:00:22,880 --> 00:00:26,240 
One analytical question
we'll explore in module two 
8 
00:00:26,240 --> 00:00:28,440 
is how to discover these
groups automatically. 
9 
00:00:30,120 --> 00:00:34,310 
Now of course with LinkedIn we only
have professional information. 
10 
00:00:34,310 --> 00:00:37,130 
But it is prudent to ask whether 
11 
00:00:37,130 --> 00:00:42,340 
one can integrate the professional network
with other forms of personal information. 
12 
00:00:42,340 --> 00:00:47,440 
This would be other social media data like
my friendship network from Facebook or 
13 
00:00:47,440 --> 00:00:53,230 
Google, or it could be information
like my Outlook email network. 
14 
00:00:53,230 --> 00:00:58,060 
One can add even more information and
put that interpersonal relationships. 
15 
00:00:58,060 --> 00:01:03,490 
For example, Professor Norman who is
on my contact list is my director. 
16 
00:01:03,490 --> 00:01:05,860 
And if we add my calendar to this, 
17 
00:01:05,860 --> 00:01:11,980 
one can find events that I'll be attending
and people that I will be meeting. 
18 
00:01:11,980 --> 00:01:14,431 
Taking it even further, for 
19 
00:01:14,431 --> 00:01:19,949 
special applications you might
actually include financial and 
20 
00:01:19,949 --> 00:01:24,956 
business transactions, or
performance in activities, 
21 
00:01:24,956 --> 00:01:29,671 
or fitness information, or
the location from my GPS. 
22 
00:01:31,276 --> 00:01:36,360 
Now, if you add all of these questions, we
should ask what we do with these things? 
23 
00:01:37,420 --> 00:01:37,920 
Let's see. 
24 
00:01:39,920 --> 00:01:42,910 
One use of this kind of
information is matching. 
25 
00:01:44,180 --> 00:01:48,960 
For example, matching people with jobs
is not just a matter of screening 
26 
00:01:48,960 --> 00:01:52,840 
series based on job descriptions or
evaluating their job experience. 
27 
00:01:53,880 --> 00:01:57,890 
To recruit for high level positions,
like the board of directors of a company, 
28 
00:01:59,020 --> 00:02:03,210 
you need to evaluate the network of the
candidate to determine their relationships 
29 
00:02:03,210 --> 00:02:05,850 
with important organizations,
groups, and individuals. 
30 
00:02:07,020 --> 00:02:09,900 
For some tasks like choosing a surgeon, 
31 
00:02:09,900 --> 00:02:12,230 
you might also want inspect
their social media ratings. 
32 
00:02:14,130 --> 00:02:16,830 
In other classic applications,
we might want to look for 
33 
00:02:16,830 --> 00:02:19,630 
people who can influence a human network. 
34 
00:02:19,630 --> 00:02:22,960 
Suppose, we are in an election
campaign team, and 
35 
00:02:22,960 --> 00:02:27,620 
need our message to get to as many
people as possible in a city. 
36 
00:02:27,620 --> 00:02:32,800 
We can not go door to door ourselves,
we need to go to some people, and 
37 
00:02:32,800 --> 00:02:35,360 
these people will reach
others on our behalf. 
38 
00:02:36,360 --> 00:02:41,150 
Graph analytic techniques may help
identify the fewest number of people 
39 
00:02:41,150 --> 00:02:45,610 
who will have maximal reach into most
of the potential voters in the city. 
40 
00:02:47,310 --> 00:02:50,250 
A third category of application
is threat detection. 
41 
00:02:52,200 --> 00:02:56,076 
This network is created
by groups who study news. 
42 
00:02:56,076 --> 00:02:59,390 
For example,
they collect information about all 
43 
00:02:59,390 --> 00:03:03,670 
the militant groups in different countries
and all the reported acts of terrorism. 
44 
00:03:05,260 --> 00:03:07,460 
This creates a network as you can see. 
45 
00:03:08,590 --> 00:03:10,266 
We show only a part of the network. 
46 
00:03:10,266 --> 00:03:14,203 
In this graph view,
the green notes are groups and 
47 
00:03:14,203 --> 00:03:17,406 
the pink notes are deemed individuals. 
48 
00:03:18,730 --> 00:03:20,920 
It should be clear that some
of these individuals and 
49 
00:03:20,920 --> 00:03:24,050 
groups are more closely
associated than others. 
50 
00:03:25,100 --> 00:03:29,057 
Discovering and tracking these groups,
their current activities, 
51 
00:03:29,057 --> 00:03:33,685 
together with an analysis of details of
the events they're associated with might 
52 
00:03:33,685 --> 00:03:36,904 
help analysts to get deeper
insight into the networks and 
53 
00:03:36,904 --> 00:03:39,604 
perhaps anticipate their
future activities. 
1 
00:00:01,880 --> 00:00:05,350 
Our fourth and
last use case has to do with smart cities. 
2 
00:00:06,380 --> 00:00:10,820 
Now, a city is a geographically
bounded space, and 
3 
00:00:10,820 --> 00:00:15,630 
contains many different networks
operating within the same spatial domain. 
4 
00:00:15,630 --> 00:00:17,140 
What kind of networks? 
5 
00:00:17,140 --> 00:00:17,640 
Let's see. 
6 
00:00:18,820 --> 00:00:20,820 
It has transportation networks. 
7 
00:00:22,320 --> 00:00:27,870 
Water and sewage networks,
power transmission networks, 
8 
00:00:27,870 --> 00:00:28,990 
your IP broadband networks. 
9 
00:00:30,960 --> 00:00:33,670 
Some of these networks have
multiple subtypes, for 
10 
00:00:33,670 --> 00:00:38,520 
example transportation networks include
the bus networks, the subway network, 
11 
00:00:38,520 --> 00:00:41,700 
the surface street network, and
the railway network, and so on. 
12 
00:00:42,760 --> 00:00:47,380 
These networks form a physical
infrastructure and therefore 
13 
00:00:47,380 --> 00:00:51,500 
can be represented as graphs where
each node has a geographic coordinate. 
14 
00:00:52,750 --> 00:00:57,400 
But some of these networks can also be
thought of as a commodity flow network. 
15 
00:00:58,550 --> 00:01:01,560 
People flow through
transportation networks. 
16 
00:01:01,560 --> 00:01:04,330 
Sewage material flows through
a sewage network and so on. 
17 
00:01:06,090 --> 00:01:07,710 
For many of these networks, 
18 
00:01:07,710 --> 00:01:12,100 
a city planner would like to make
sure that they cover the entire city, 
19 
00:01:12,100 --> 00:01:16,920 
that commute time is optimized,
traffic congestions are well planned. 
20 
00:01:16,920 --> 00:01:21,380 
To accomplish this, they would need to
create what's called a network model. 
21 
00:01:21,380 --> 00:01:26,210 
For example, urban planners develop
city traffic model servicing. 
22 
00:01:26,210 --> 00:01:29,320 
A traffic model will use both
the geographic layout and 
23 
00:01:29,320 --> 00:01:32,080 
connectivity of the network along
with the flow parameters like 
24 
00:01:32,080 --> 00:01:34,610 
the number of commuters getting
on board at any station. 
25 
00:01:36,110 --> 00:01:40,170 
Well, if we're planning
to create a smart hub, 
26 
00:01:40,170 --> 00:01:43,900 
we need to make sure that all the right
things happen at the same place. 
27 
00:01:44,960 --> 00:01:48,700 
People who come out of a metro
station find nearby businesses. 
28 
00:01:48,700 --> 00:01:50,630 
They find nearby facilities. 
29 
00:01:50,630 --> 00:01:53,060 
Those facilities should
have broadband network for 
30 
00:01:53,060 --> 00:01:55,433 
people who are going to go
on their mobile phones. 
31 
00:01:55,433 --> 00:01:59,550 
The same places need to have
a water supply network, and 
32 
00:01:59,550 --> 00:02:02,560 
you have to plan it in such
a way that all these networks 
33 
00:02:02,560 --> 00:02:05,750 
who exist within a certain
distance of each other. 
34 
00:02:05,750 --> 00:02:07,910 
And all the facilities can
be planned accordingly. 
35 
00:02:09,140 --> 00:02:11,120 
Beyond normal operations, 
36 
00:02:11,120 --> 00:02:15,010 
we also need to model what would
happen if the network gets disrupted. 
37 
00:02:15,010 --> 00:02:17,970 
What are the kinds of congestion or
traffic that might disturb the network? 
38 
00:02:18,970 --> 00:02:24,000 
Therefore these graphs are no longer just
structures, but they represent things like 
39 
00:02:24,000 --> 00:02:28,140 
congestions, things like people's behavior
and materials behavior over the network. 
40 
00:02:29,400 --> 00:02:32,030 
One should also compute
energy use patterns for 
41 
00:02:32,030 --> 00:02:36,240 
the busy parts of the network in order to
figure out how the network structure or 
42 
00:02:36,240 --> 00:02:40,370 
the network flow can be altered to
enable energy optimal operations. 
43 
00:02:41,790 --> 00:02:44,130 
As we saw in these four examples, 
44 
00:02:44,130 --> 00:02:47,700 
what graphs are used for
are kind of different but 
45 
00:02:47,700 --> 00:02:52,380 
they all show different view points from
which you can use graphs for analysis. 
46 
00:02:53,430 --> 00:02:57,510 
So this course focuses on Graph Analytics. 
47 
00:02:57,510 --> 00:03:00,220 
I would like to briefly
recap what the term means. 
48 
00:03:01,620 --> 00:03:06,900 
Analytics is the ability to
discover meaningful patterns and 
49 
00:03:06,900 --> 00:03:12,280 
interesting insights into data using
mathematical properties of data. 
50 
00:03:12,280 --> 00:03:16,410 
It covers a process of computing
with mathematical properties and 
51 
00:03:16,410 --> 00:03:19,590 
accessing the data itself efficiently. 
52 
00:03:19,590 --> 00:03:22,110 
Further, it involves
the ability to represent and 
53 
00:03:22,110 --> 00:03:26,060 
work with domain knowledge as we
saw in use case two with biology. 
54 
00:03:26,060 --> 00:03:30,350 
Finally, analytics often involves
statistical modeling techniques for 
55 
00:03:30,350 --> 00:03:33,310 
drawing inferences and
making predictions on data. 
56 
00:03:34,630 --> 00:03:39,195 
With analytics, we should be able
to achieve the goals shown here. 
57 
00:03:55,099 --> 00:03:58,375 
Therefore, Graph Analytics
is a special piece of 
58 
00:03:58,375 --> 00:04:02,980 
analytics where the underlying data
can be modeled as a set of graphs. 
1 
00:00:01,460 --> 00:00:04,750 
In the previous video,
we learned what a graph is. 
2 
00:00:05,770 --> 00:00:08,550 
We gained insight into both
the mathematician's view and 
3 
00:00:08,550 --> 00:00:09,700 
the computer scientist's view. 
4 
00:00:10,890 --> 00:00:14,310 
Now we want to look at the big
picture of graph analytics. 
5 
00:00:15,390 --> 00:00:18,210 
Why are we doing what we are doing, and 
6 
00:00:18,210 --> 00:00:21,090 
why do I need a graph
representation on graph analytics? 
7 
00:00:22,760 --> 00:00:27,730 
Let's start with a graph network which you
may already be familiar with, Facebook. 
8 
00:00:28,780 --> 00:00:30,770 
Here is my Facebook page. 
9 
00:00:30,770 --> 00:00:33,820 
Let's look at it and
consider some of the elements it contains. 
10 
00:00:36,470 --> 00:00:38,340 
There is a primary user, and that's me. 
11 
00:00:40,100 --> 00:00:40,860 
There are my friends. 
12 
00:00:42,490 --> 00:00:47,310 
And typically there are some posts,
some of which may have media in them, 
13 
00:00:47,310 --> 00:00:48,800 
such as video. 
14 
00:00:48,800 --> 00:00:50,380 
What else does it have? 
15 
00:00:50,380 --> 00:00:53,619 
Well, let's consider what's inside a post. 
16 
00:00:53,619 --> 00:00:56,281 
The post contains text. 
17 
00:00:56,281 --> 00:00:58,434 
It has tags. 
18 
00:00:58,434 --> 00:01:02,700 
I tag some people,
some people comment on my post. 
19 
00:01:03,770 --> 00:01:05,240 
If somebody's commenting, 
20 
00:01:05,240 --> 00:01:10,330 
then that means there must be a commenter
who is also a user in the Facebook. 
21 
00:01:11,640 --> 00:01:15,340 
Other people like my post or 
22 
00:01:15,340 --> 00:01:20,550 
they like a comment and
they also respond to some of the comments. 
23 
00:01:21,820 --> 00:01:24,940 
Many posts have locations
associated with them as well. 
24 
00:01:26,780 --> 00:01:29,380 
When we consider all of these together,
do you see the graph? 
25 
00:01:29,380 --> 00:01:30,910 
Well if not, we'll show you. 
26 
00:01:32,300 --> 00:01:36,300 
Here, we see very much the same types of
information but now they're in a graph. 
27 
00:01:37,320 --> 00:01:40,300 
First, notice me on the far left. 
28 
00:01:40,300 --> 00:01:44,878 
If you look carefully at this graph you'll
also notice there are relations labeled 
29 
00:01:44,878 --> 00:01:49,830 
Friend-of and Created_post and parts
of the post and everything that you saw 
30 
00:01:49,830 --> 00:01:55,400 
before that are now organized in terms of
objects and relationship speaking objects. 
31 
00:01:56,580 --> 00:02:01,250 
The objects here on the post, comments on
the post, replies to the comments and so 
32 
00:02:01,250 --> 00:02:05,880 
forth, and the relationships also
include tagged in or refers to. 
33 
00:02:05,880 --> 00:02:08,330 
So this is sort of the big picture, right? 
34 
00:02:10,590 --> 00:02:15,380 
Next we will look at 4 specific
Use Cases in four different disciplines. 
35 
00:02:16,480 --> 00:02:18,110 
The first one will be on Social Media. 
36 
00:02:20,010 --> 00:02:24,020 
The second one will be on biology,
where we'll look at genes and diseases. 
37 
00:02:25,700 --> 00:02:30,440 
The third one will be Human Information
Network, which means personal information. 
38 
00:02:31,840 --> 00:02:33,750 
And the fourth one will
be on smart cities. 
39 
00:02:34,950 --> 00:02:36,790 
Let's go through these
examples one by one. 
1 
00:00:01,020 --> 00:00:03,990 
We will go over the logic of Dijkstra's
algorithm without writing code. 
2 
00:00:05,460 --> 00:00:06,940 
If you are an advanced student and 
3 
00:00:06,940 --> 00:00:09,350 
know the algorithm,
you can skip to the next lecture. 
4 
00:00:11,010 --> 00:00:14,840 
The basic plan is to start from node I and 
5 
00:00:14,840 --> 00:00:17,210 
progressively traverse
a sequence of notes. 
6 
00:00:18,580 --> 00:00:22,450 
When the method attempts to choose
the next node to traverse to, 
7 
00:00:22,450 --> 00:00:27,850 
it chooses a node for which the total rate
of the path to that node is the lowest. 
8 
00:00:29,120 --> 00:00:32,130 
In the beginning,
the algorithm is on the start node I. 
9 
00:00:33,530 --> 00:00:36,311 
The distance from I to I is 0. 
10 
00:00:36,311 --> 00:00:38,677 
And the distance to all
other nodes is infinity, 
11 
00:00:38,677 --> 00:00:40,820 
because the system doesn't know them yet. 
12 
00:00:42,560 --> 00:00:45,921 
A second table, called a priority queue, 
13 
00:00:45,921 --> 00:00:51,255 
is currently exactly the same as
the distance row of the first array. 
14 
00:00:52,330 --> 00:00:55,350 
The system starts with processing node I,
the source node. 
15 
00:00:56,350 --> 00:01:01,050 
That means it finds the nodes
that can reach from I, F and J. 
16 
00:01:02,270 --> 00:01:07,121 
Note that the respective total weights,
that is 5 for F, 
17 
00:01:07,121 --> 00:01:11,882 
and 15 for J,
to get to these nodes in the distance row. 
18 
00:01:11,882 --> 00:01:13,520 
Then it marks I as done. 
19 
00:01:14,710 --> 00:01:18,330 
We have made the node I agree,
because the node is processed. 
20 
00:01:20,080 --> 00:01:26,640 
Next it looks at row d to find
the least distance, which is 5. 
21 
00:01:28,560 --> 00:01:30,470 
The corresponding vertex is F. 
22 
00:01:31,870 --> 00:01:36,170 
So next, the method traverses to F. 
23 
00:01:36,170 --> 00:01:41,086 
Now the algorithm on node F and
has determined that out of its possible 
24 
00:01:41,086 --> 00:01:46,040 
destinations, E, G, and J,
J is the least expensive. 
25 
00:01:46,040 --> 00:01:52,081 
The total path, that is the weight
of the path to J, is 10. 
26 
00:01:52,081 --> 00:01:56,320 
5 from I to F, plus 5 from F to J. 
27 
00:01:56,320 --> 00:02:00,440 
This diagram shows that the priority
is now shorter because it has 
28 
00:02:00,440 --> 00:02:03,510 
popped out the already processed load, I. 
29 
00:02:03,510 --> 00:02:08,260 
At step three, we are processing node
J but face the following situation. 
30 
00:02:09,450 --> 00:02:12,454 
We can go back to F from J, but 
31 
00:02:12,454 --> 00:02:17,227 
that will cost 10 plus 15, that is 25. 
32 
00:02:17,227 --> 00:02:22,470 
25 is worse than the cost of the current
path to F, which is 5 directly from I. 
33 
00:02:23,680 --> 00:02:28,220 
Thus we do not go from F to J, and
do not update the distance shown. 
34 
00:02:28,220 --> 00:02:34,775 
The other option is to go from J to G,
which incurs a cost of 10 plus 5, 15. 
35 
00:02:34,775 --> 00:02:39,559 
Hm, this does not improve the current
cost to reach G through F, 
36 
00:02:39,559 --> 00:02:42,710 
which is now at 15 already. 
37 
00:02:42,710 --> 00:02:46,720 
Therefore, we do not
update the distance for G. 
38 
00:02:46,720 --> 00:02:50,880 
So at this point,
we see that while J is processed, 
39 
00:02:50,880 --> 00:02:54,230 
it had no impact on
the traversal process. 
40 
00:02:54,230 --> 00:02:58,916 
We consider the distance row again, and
find that the next node to expand is G, 
41 
00:02:58,916 --> 00:03:00,630 
which is reached through F. 
42 
00:03:01,630 --> 00:03:04,531 
Continuing as before, G's processed. 
43 
00:03:04,531 --> 00:03:09,191 
It opens up the possibility of
diverging to C, at a cost of 35. 
44 
00:03:09,191 --> 00:03:12,430 
Or to D, at the cost of 25. 
45 
00:03:12,430 --> 00:03:17,254 
But wait, we have an issue,
there are two competing nodes, 
46 
00:03:17,254 --> 00:03:23,760 
E coming form F or D coming from G,
that are both expansion candidates. 
47 
00:03:23,760 --> 00:03:24,705 
At this point, 
48 
00:03:24,705 --> 00:03:29,516 
the algorithm can make a random choice
because there is no other information. 
49 
00:03:29,516 --> 00:03:32,640 
Let's say we make an arbitrary choice,
we expand to E next. 
50 
00:03:33,650 --> 00:03:37,240 
In an optional video, we will see 
51 
00:03:37,240 --> 00:03:42,140 
how we can use the additional information
to make a more informed decision. 
52 
00:03:42,140 --> 00:03:43,930 
After expanding to E, 
53 
00:03:43,930 --> 00:03:47,700 
we can find that we have already
reached the node B, so we are done. 
54 
00:03:49,000 --> 00:03:54,080 
The other choice, that is to go to D from
G, costs less than the path through B, but 
55 
00:03:54,080 --> 00:03:56,690 
it doesn't matter now,
because the destination is reached. 
56 
00:03:57,960 --> 00:04:03,390 
Just for the sake of completeness, if we
did let the algorithm continue to operate, 
57 
00:04:03,390 --> 00:04:06,440 
it will terminate when all
reachable nodes are reached. 
58 
00:04:07,480 --> 00:04:11,310 
We say all reachable nodes,
become some nodes like H, 
59 
00:04:12,850 --> 00:04:15,220 
are not reachable because
it has no incoming edge. 
60 
00:04:16,220 --> 00:04:20,060 
Such a node, as we said before,
is called the root node of the graph. 
61 
00:04:20,060 --> 00:04:23,400 
In general,
a graph can have more than one root node. 
62 
00:04:25,040 --> 00:04:29,824 
Now that we have reached our destination,
we need to construct the shortest path. 
63 
00:04:29,824 --> 00:04:35,630 
We start by taking the destination B and
find its predecessor, E. 
64 
00:04:37,350 --> 00:04:43,069 
Then we find the node E, and
check its predecessor, which is F. 
65 
00:04:43,069 --> 00:04:46,347 
Finally, we find the predecessor
of F to obtain I, 
66 
00:04:46,347 --> 00:04:48,645 
which is a source node for the task. 
67 
00:04:49,880 --> 00:04:52,610 
So these nodes can then be
stretched together in reverse, 
68 
00:04:52,610 --> 00:04:57,570 
thus building I to F, to E, to B,
which is highlighted in the film. 
69 
00:04:58,590 --> 00:05:03,380 
So, how well does this algorithm work for
big graphs? 
70 
00:05:03,380 --> 00:05:06,020 
Actually, not very well. 
71 
00:05:06,020 --> 00:05:08,940 
We often measure
the performance of an algorithm 
72 
00:05:08,940 --> 00:05:10,710 
in terms of the size of the data. 
1 
00:00:00,350 --> 00:00:05,040 
So, we saw that the Dijkstra Algorithm
has a very high worst case complexity. 
2 
00:00:06,370 --> 00:00:09,770 
Despite the high complexity of
the algorithm, there are several 
3 
00:00:09,770 --> 00:00:13,590 
practical improvements that will
enhance the performance of the method. 
4 
00:00:13,590 --> 00:00:17,310 
One of them is
Bi-directional Dijkstra Algorithm. 
5 
00:00:17,310 --> 00:00:21,340 
The idea is very simple,
we will go forward from the source now and 
6 
00:00:21,340 --> 00:00:26,960 
backward from the target node and stop
when the two expanding frontiers meet. 
7 
00:00:26,960 --> 00:00:29,840 
We will briefly illustrate
the process without going deep into 
8 
00:00:29,840 --> 00:00:31,690 
the details of every step. 
9 
00:00:31,690 --> 00:00:34,890 
So the technique starts just
like the regular method. 
10 
00:00:34,890 --> 00:00:38,810 
The control moves from
I to F at a cost of 5. 
11 
00:00:38,810 --> 00:00:40,500 
But then, it switches and 
12 
00:00:40,500 --> 00:00:45,240 
starts from the target, and
moves backward along the edges. 
13 
00:00:45,240 --> 00:00:47,950 
So from A, it comes to D or C. 
14 
00:00:48,960 --> 00:00:53,460 
We'll chose D,
because an AD is a least weight part. 
15 
00:00:53,460 --> 00:00:59,730 
Now the forward step is performed again,
and we traverse from F to G. 
16 
00:00:59,730 --> 00:01:03,060 
We are skipping the expansion
to J because as we saw before, 
17 
00:01:03,060 --> 00:01:05,040 
it does not contribute to the path. 
18 
00:01:05,040 --> 00:01:08,510 
So the total rate of the IFG path is 15. 
19 
00:01:08,510 --> 00:01:13,420 
The backward process
then reaches G through D. 
20 
00:01:13,420 --> 00:01:16,000 
The cost of the IFG is 15. 
21 
00:01:16,000 --> 00:01:18,930 
And the cost of ADG is also 15. 
22 
00:01:18,930 --> 00:01:22,650 
We stop because a common node is reached. 
23 
00:01:22,650 --> 00:01:25,570 
We need to ensure that the weight
of the forward path and 
24 
00:01:25,570 --> 00:01:30,810 
that of the reverse path are added, and
the total combined rate is minimal. 
25 
00:01:30,810 --> 00:01:34,701 
At this point,
we can concatenate the partial paths 
26 
00:01:34,701 --> 00:01:38,777 
to construct the full shortest path,
which is IFGDA. 
27 
00:01:38,777 --> 00:01:43,773 
Now one point to remember is that
the length of the smallest weight path 
28 
00:01:43,773 --> 00:01:46,744 
can be longer than the shortest hop path. 
29 
00:01:46,744 --> 00:01:52,444 
Here the FCA path has 2 hops but
a weight of 20. 
30 
00:01:52,444 --> 00:01:57,366 
But the weight of the 3 hop path,
FGDA, is 15. 
31 
00:01:57,366 --> 00:01:59,060 
So just remember that. 
1 
00:00:00,580 --> 00:00:03,750 
The final type of analytics
we'll discuss in this module 
2 
00:00:03,750 --> 00:00:05,425 
is called Centrality Analysis. 
3 
00:00:06,850 --> 00:00:12,010 
We call something central if it's
in the middle of a larger entity. 
4 
00:00:12,010 --> 00:00:16,370 
We also call something central
if it's important and vital. 
5 
00:00:17,680 --> 00:00:23,770 
For graphs, we'll identify important
notes by looking at how central they are. 
6 
00:00:23,770 --> 00:00:24,270 
In the graph. 
7 
00:00:27,280 --> 00:00:31,673 
Look at the the six node graph,
you don't have to know a lot 
8 
00:00:31,673 --> 00:00:36,345 
to figure out that the orange
node is pretty important, Why? 
9 
00:00:36,345 --> 00:00:39,290 
Well, one reason could be it's
the most vulnerable node. 
10 
00:00:39,290 --> 00:00:41,240 
If you remove it the graph
will fall apart. 
11 
00:00:41,240 --> 00:00:44,210 
because this is clearly one way to
look at the centrality of the node. 
12 
00:00:45,630 --> 00:00:46,650 
But it's not the only way. 
13 
00:00:48,270 --> 00:00:50,440 
Another way of looking at the orange node, 
14 
00:00:50,440 --> 00:00:53,880 
is that it can reach all other
nodes quicker, than any other node. 
15 
00:00:55,390 --> 00:00:58,570 
So this is the idea behind influences. 
16 
00:00:58,570 --> 00:01:02,480 
People in a social network who
are connected enough to reach out and 
17 
00:01:02,480 --> 00:01:06,810 
possibly influence a lot more
people than others will be able to. 
18 
00:01:08,340 --> 00:01:10,820 
If the little graph represents
a transportation network. 
19 
00:01:11,820 --> 00:01:15,890 
And you were asked to build a restaurant
somewhere around this network. 
20 
00:01:15,890 --> 00:01:18,780 
You'll possibly choose an area
near the central node. 
21 
00:01:18,780 --> 00:01:23,250 
Because more traffic will flow
through this node than other nodes. 
22 
00:01:23,250 --> 00:01:25,830 
And some of them will get
out at the station and 
23 
00:01:25,830 --> 00:01:27,010 
bring business to your restaurant. 
24 
00:01:28,250 --> 00:01:30,310 
We can give many more examples. 
25 
00:01:30,310 --> 00:01:34,810 
In biological networks,
house-keeping genes are genes needed for 
26 
00:01:34,810 --> 00:01:38,050 
the maintenance of basic
cellular function, and 
27 
00:01:38,050 --> 00:01:42,060 
are expressed in all cells of
an organism under normal and 
28 
00:01:42,060 --> 00:01:46,780 
abnormal conditions, these genes
are central because they're vital and 
29 
00:01:46,780 --> 00:01:49,300 
they're connected to many other
nodes in the biological network. 
30 
00:01:50,440 --> 00:01:54,330 
So much so that they need to
be taken out of the network so 
31 
00:01:54,330 --> 00:01:56,930 
that the rest of
the network can be studied. 
32 
00:01:56,930 --> 00:02:01,390 
Out of all these examples,
we'll focus on a kind of problem 
33 
00:02:01,390 --> 00:02:04,480 
researchers have called
the key player problem. 
34 
00:02:05,740 --> 00:02:07,380 
The problem comes in two flavors. 
35 
00:02:08,460 --> 00:02:11,177 
Now take a little time to
read the two examples. 
36 
00:02:23,255 --> 00:02:25,970 
The first is sort of a negative piece. 
37 
00:02:27,250 --> 00:02:31,660 
We have a network and we are trying
to find a small subset of people 
38 
00:02:31,660 --> 00:02:36,250 
who's removal will maximally
disrupt the network. 
39 
00:02:37,260 --> 00:02:40,610 
The key operative word here is maximally. 
40 
00:02:40,610 --> 00:02:46,360 
So if there is a node which removal
breaks the network into two parts, 
41 
00:02:46,360 --> 00:02:50,300 
it is not what we want if
there's another two nodes 
42 
00:02:50,300 --> 00:02:53,440 
whose removal will break
the network into ten parts. 
43 
00:02:53,440 --> 00:02:58,270 
The second case, however,
is a lot more conventional. 
44 
00:02:58,270 --> 00:03:03,000 
The goal is to find a small set of nodes
with maximal combined reachability 
45 
00:03:03,000 --> 00:03:04,250 
to other nodes. 
46 
00:03:04,250 --> 00:03:09,000 
That means, taken together, these nodes
should reach almost all other nodes. 
47 
00:03:11,010 --> 00:03:14,180 
You should know that
people use two terms to 
48 
00:03:14,180 --> 00:03:16,940 
characterize a general concept
of network centrality. 
49 
00:03:18,340 --> 00:03:20,920 
The first one, centrality,
is about a node. 
50 
00:03:22,290 --> 00:03:26,450 
It measures how central the node
is with respect to the network. 
51 
00:03:27,450 --> 00:03:31,070 
So the orange node in the left
graph has high centrality, and 
52 
00:03:31,070 --> 00:03:32,550 
the blue nodes have low centrality. 
53 
00:03:34,140 --> 00:03:39,670 
The second term, centralization,
is about the network. 
54 
00:03:39,670 --> 00:03:41,620 
Now look at the graph on the right, 
55 
00:03:41,620 --> 00:03:46,830 
where the dark orange node is still very
central for the light orange node, and 
56 
00:03:46,830 --> 00:03:50,770 
between the other nodes, and
therefore has reasonable centrality. 
57 
00:03:52,050 --> 00:03:55,170 
As more nodes start
having higher centrality, 
58 
00:03:55,170 --> 00:03:58,090 
the centralization of the network drops 
59 
00:03:58,090 --> 00:04:02,330 
because there is less variation in
the centrality values of the network. 
60 
00:04:03,940 --> 00:04:09,332 
So for each type of centrality that we
discuss next we can compute the network 
61 
00:04:09,332 --> 00:04:14,812 
centralization by considering the sum
of the difference between the maximum 
62 
00:04:14,812 --> 00:04:20,393 
centrality and the centrality of the node
divided by the maximum centrality. 
63 
00:04:20,393 --> 00:04:23,310 
There are thirty different
measures of centrality. 
64 
00:04:24,640 --> 00:04:27,190 
We will consider only a few of them and 
65 
00:04:27,190 --> 00:04:30,645 
explore the conceptual
principles supported here. 
66 
00:04:30,645 --> 00:04:34,080 
The first and the most intuitive
measure is degree centrality. 
67 
00:04:34,080 --> 00:04:39,180 
Quick measures, the degree of a node
divided by the possible edges it 
68 
00:04:39,180 --> 00:04:44,100 
could have if it connected to each of
the other N- 1 nodes in the graph. 
69 
00:04:45,110 --> 00:04:45,960 
Now, we have seen this before. 
70 
00:04:47,010 --> 00:04:50,850 
This measure gives us a sense
of the hubness of the node. 
71 
00:04:50,850 --> 00:04:55,630 
The higher the number is,
the more hub-like the node is. 
72 
00:04:55,630 --> 00:04:58,640 
Thinking of our second key player problem. 
73 
00:04:58,640 --> 00:05:02,880 
One way to approach this is to
find a hub-like measure for 
74 
00:05:02,880 --> 00:05:05,660 
a group with multiple nodes. 
75 
00:05:05,660 --> 00:05:08,690 
Instead of measuring the degree
centrality of individual nodes, 
76 
00:05:10,310 --> 00:05:15,520 
the measure simply counts the number of
edges coming into the group as a whole 
77 
00:05:15,520 --> 00:05:17,430 
compared to the number of outsiders. 
78 
00:05:18,620 --> 00:05:25,460 
In our example two red nodes here
together connect to all blue nodes. 
79 
00:05:26,830 --> 00:05:31,230 
Although there is just one
node neighboring both of them. 
80 
00:05:31,230 --> 00:05:34,950 
Closeness centrality takes a different
approach to the centrality problem. 
81 
00:05:36,670 --> 00:05:41,222 
It acts the shortest distance
of a node to all other nodes and 
82 
00:05:41,222 --> 00:05:43,232 
divides it by a minus one. 
83 
00:05:43,232 --> 00:05:48,575 
So in our graph, a node like I,
which is on the periphery of the graph, 
84 
00:05:48,575 --> 00:05:52,199 
will be quite far from
all nodes in general, and 
85 
00:05:52,199 --> 00:05:56,930 
therefore will have a very high
closeness centrality value. 
86 
00:05:56,930 --> 00:06:03,070 
On the other hand, nodes like F, C and
H are much closer to all other nodes. 
87 
00:06:04,340 --> 00:06:07,580 
Know that we define this measure
in terms of shortest paths. 
88 
00:06:09,810 --> 00:06:15,060 
So if a moving object, like information,
flows to the shortest path 
89 
00:06:15,060 --> 00:06:20,810 
in the network, F is more likely to
receive them earlier than other nodes. 
90 
00:06:20,810 --> 00:06:23,710 
And therefore can process into
other nodes more quickly. 
91 
00:06:25,190 --> 00:06:26,590 
Therefore. 
92 
00:06:26,590 --> 00:06:31,300 
If we look to inject a new piece of
information into the network with the idea 
93 
00:06:31,300 --> 00:06:36,486 
that it should read every other node
quickly, I should possibly inject it at F. 
94 
00:06:36,486 --> 00:06:43,940 
Recall however, that node information
flow is two shortest routes. 
95 
00:06:43,940 --> 00:06:49,370 
An example is gossip, that tends to
travel through centrality nodes. 
96 
00:06:49,370 --> 00:06:53,430 
Closeness centrality does not work well
for these types of information nodes. 
97 
00:06:54,490 --> 00:06:58,410 
Another very popular centrality measure
is called betweenness centrality. 
98 
00:06:59,640 --> 00:07:06,430 
For any node it measures the fraction of
shortest paths flowing through that node. 
99 
00:07:06,430 --> 00:07:08,730 
Compared to the number of
shortest paths in the graph. 
100 
00:07:10,590 --> 00:07:16,065 
Since B is at one end,
its between a centrality is 0. 
101 
00:07:16,065 --> 00:07:17,935 
But let's look at A. 
102 
00:07:17,935 --> 00:07:20,010 
A is in the path from B to E. 
103 
00:07:21,320 --> 00:07:21,880 
So is D. 
104 
00:07:23,060 --> 00:07:27,440 
Therefore, A's score for
the B to E path is 0.5. 
105 
00:07:27,440 --> 00:07:35,280 
Similarly, its score for the C to E path
is also 0.5, making the total score 1. 
106 
00:07:35,280 --> 00:07:40,623 
E is in the path from A to D,
for there is one more 
107 
00:07:40,623 --> 00:07:46,651 
path from A to do through C,
so E's score is 0.5. 
108 
00:07:46,651 --> 00:07:49,840 
Now can you verify why C's score is 3.5? 
109 
00:07:49,840 --> 00:07:53,275 
Betweenness centrality
is typically used for 
110 
00:07:53,275 --> 00:07:57,440 
problem where a commodity is
flowing through the network. 
111 
00:07:57,440 --> 00:07:59,840 
As in the case of closeness centrality. 
112 
00:08:01,080 --> 00:08:05,573 
Any quantity that does not flow in
shortest path channels like infection or 
113 
00:08:05,573 --> 00:08:09,872 
rumor on the internet doesn't work
well with betweeness centrality. 
1 
00:00:01,070 --> 00:00:04,570 
This lesson on graph analytics,
is about identifying and 
2 
00:00:04,570 --> 00:00:07,290 
tracking groups of interacting
entities in a network. 
3 
00:00:08,900 --> 00:00:10,740 
We call these groups communities. 
4 
00:00:12,550 --> 00:00:16,060 
Let's try to provide a more concrete
definition of communities in a network. 
5 
00:00:17,410 --> 00:00:19,800 
We multiply the definition
by this diagram, 
6 
00:00:19,800 --> 00:00:22,820 
showing a research study
on the Santa Fe Institute. 
7 
00:00:22,820 --> 00:00:27,680 
A theoretical research institute
located in Santa Fe, New Mexico, US. 
8 
00:00:27,680 --> 00:00:31,850 
They performed multidisciplinary
studies on fundamental principles 
9 
00:00:31,850 --> 00:00:34,220 
of complex adaptive systems. 
10 
00:00:34,220 --> 00:00:37,110 
The nodes in the graph are researchers. 
11 
00:00:37,110 --> 00:00:39,640 
And an edge exists
between two researchers, 
12 
00:00:39,640 --> 00:00:40,830 
if they collaborate with each other. 
13 
00:00:42,070 --> 00:00:47,020 
As you can see, there are distinct
groups among researchers. 
14 
00:00:47,020 --> 00:00:49,300 
A mathematically college researcher, 
15 
00:00:49,300 --> 00:00:53,090 
does not collaborate with researchers
who work on structure of the RNA. 
16 
00:00:54,370 --> 00:00:58,450 
So the graph,
is essentially a set of separate groups, 
17 
00:00:58,450 --> 00:01:02,570 
thinly connected through a handful
of cross-disciplinary researchers, 
18 
00:01:02,570 --> 00:01:04,000 
who collaborate across groups. 
19 
00:01:05,290 --> 00:01:08,320 
These groups are then the communities
in this collaboration graph. 
20 
00:01:09,590 --> 00:01:10,280 
What does this tell us? 
21 
00:01:11,650 --> 00:01:16,260 
It tells us that communities are highly
interacting clusters in a graph. 
22 
00:01:16,260 --> 00:01:20,690 
That is, they form pockets of
denser subgraphs that are more 
23 
00:01:20,690 --> 00:01:24,400 
connected to each other,
than to members of the other clusters. 
24 
00:01:25,540 --> 00:01:30,020 
Communities of humans, or otherwise,
are interesting things to study, 
25 
00:01:30,020 --> 00:01:33,740 
because it gives us an insight
into the interaction patterns. 
26 
00:01:33,740 --> 00:01:35,280 
And how they change with time. 
27 
00:01:36,340 --> 00:01:39,280 
Here, are some analytics
questions about communities. 
28 
00:01:40,310 --> 00:01:42,740 
We have divided them
into three categories. 
29 
00:01:44,080 --> 00:01:46,990 
Analytics questions that do not
depend on time, are called static. 
30 
00:01:48,030 --> 00:01:52,290 
Here, we ask questions about
the composition of the community, 
31 
00:01:52,290 --> 00:01:54,970 
how tight-knit members are connected,
and so forth. 
32 
00:01:56,030 --> 00:02:02,470 
In the second category, we involve the
formation, and evolution of the community. 
33 
00:02:03,740 --> 00:02:06,720 
Communities can form temporal,
for example, 
34 
00:02:06,720 --> 00:02:09,720 
around an event like a school shooting. 
35 
00:02:09,720 --> 00:02:12,620 
Or some communities,
despite the comings and 
36 
00:02:12,620 --> 00:02:16,750 
goings of members,
sustain themselves well. 
37 
00:02:16,750 --> 00:02:22,040 
A Facebook group, a political party,
fans of a music band, 
38 
00:02:22,040 --> 00:02:25,515 
are likely to continue over time,
and hence are non-transient. 
39 
00:02:27,070 --> 00:02:30,350 
One can also be interested in
the history of formation of a community, 
40 
00:02:30,350 --> 00:02:31,100 
like a criminal network. 
41 
00:02:32,230 --> 00:02:34,410 
The third category is about predictions. 
42 
00:02:35,780 --> 00:02:39,740 
Analysts would like to predict
how a community would grow. 
43 
00:02:39,740 --> 00:02:41,828 
Whether it's composition
of members might change. 
44 
00:02:41,828 --> 00:02:46,300 
Or whether there are emerging
power shifts within the community. 
45 
00:02:47,920 --> 00:02:51,490 
Now before we ask these questions,
however, 
46 
00:02:51,490 --> 00:02:54,620 
we need to first identify
communities in a large network. 
47 
00:02:55,900 --> 00:03:00,790 
To find communities, we need to
formalize the idea that there are more 
48 
00:03:00,790 --> 00:03:05,520 
connections within the community, and
fewer connections between two communities. 
49 
00:03:06,770 --> 00:03:11,180 
One way to achieve this, is to think
of dividing the degree of a node, 
50 
00:03:11,180 --> 00:03:15,430 
into an internal and
an external component. 
51 
00:03:15,430 --> 00:03:20,310 
The internal component is the count
of edges within community. 
52 
00:03:20,310 --> 00:03:23,680 
And the external degree, is the count
of edges outside the community. 
53 
00:03:25,480 --> 00:03:26,110 
An example, 
54 
00:03:26,110 --> 00:03:30,450 
will be to consider that my community
is where I live, that is San Diego. 
55 
00:03:30,450 --> 00:03:35,280 
And then count the number of my friends
within San Diego versus outside San Diego. 
56 
00:03:36,660 --> 00:03:41,560 
In the figure, the highlighted node
has four internal connections and 
57 
00:03:41,560 --> 00:03:42,640 
one external connection. 
58 
00:03:44,730 --> 00:03:48,310 
The next step, would be to think
of the internal degree, and 
59 
00:03:48,310 --> 00:03:50,660 
the external degree of an entire cluster. 
60 
00:03:51,890 --> 00:03:56,080 
We can sum up the internal degrees
of all nodes in a cluster. 
61 
00:03:56,080 --> 00:03:59,480 
And call it the internal
degree of the whole cluster. 
62 
00:03:59,480 --> 00:04:02,089 
And similarly,
sum the external degrees of the nodes, 
63 
00:04:02,089 --> 00:04:04,320 
to compute the external
degree of the cluster. 
64 
00:04:05,460 --> 00:04:08,360 
Now, we can define intra-cluster 
65 
00:04:08,360 --> 00:04:12,810 
density to be the ratio of the number
of internal edges of the cluster, 
66 
00:04:12,810 --> 00:04:16,250 
divided by the number of possible
connections inside the box. 
67 
00:04:17,670 --> 00:04:20,350 
The denominator is N cues 2. 
68 
00:04:20,350 --> 00:04:24,190 
Which is the number of pairwise
combination of nodes within the cluster. 
69 
00:04:24,190 --> 00:04:25,490 
We call this delta int. 
70 
00:04:26,780 --> 00:04:30,750 
Similarly, inter-cluster density,
delta x is the number 
71 
00:04:30,750 --> 00:04:35,040 
of inter-cluster edges divided by
the possible pairings between nC, 
72 
00:04:36,120 --> 00:04:40,070 
a node in the cluster, to n- nC,
the number of nodes outside the cluster. 
73 
00:04:41,970 --> 00:04:47,500 
There are two kinds of methods used for
finding communities in the network. 
74 
00:04:47,500 --> 00:04:52,320 
One of them focuses on local properties,
that is, 
75 
00:04:52,320 --> 00:04:56,560 
properties for which one only
looks at a node and its neighbor. 
76 
00:04:57,720 --> 00:05:01,110 
For the most ideal community
in a network is a subgraph, 
77 
00:05:01,110 --> 00:05:04,099 
where every node is connected to
every other node in the subgraph. 
78 
00:05:05,230 --> 00:05:06,840 
Such a structure is called a clique. 
79 
00:05:08,570 --> 00:05:11,740 
To find a perfect community
structure as a clique, 
80 
00:05:11,740 --> 00:05:16,560 
one can try to find the largest
clique within a graph, return cell. 
81 
00:05:17,580 --> 00:05:19,670 
That is a computationally
challenging problem. 
82 
00:05:21,260 --> 00:05:25,780 
It's much simpler to find cliques
if we know the value of k. 
83 
00:05:26,780 --> 00:05:30,312 
That means the number of
members in the clique. 
84 
00:05:30,312 --> 00:05:33,000 
We're going to show a simple
version of this in model three. 
85 
00:05:34,710 --> 00:05:38,230 
The more general problem has been
solved by complex algorithms, 
86 
00:05:38,230 --> 00:05:39,990 
that are beyond the scope of this course. 
87 
00:05:41,950 --> 00:05:47,620 
In the real world, perfect cliques larger
than three or four are harder to find. 
88 
00:05:47,620 --> 00:05:49,450 
So we need to relax the definition of it. 
89 
00:05:50,750 --> 00:05:53,680 
Now, there are two types of relaxations, 
90 
00:05:53,680 --> 00:05:57,250 
those based on distance,
and those based on density. 
91 
00:05:58,950 --> 00:06:04,970 
Two distance based definitions
are n-clique and n-plan. 
92 
00:06:04,970 --> 00:06:08,660 
We'll illustrate this over
a friendship graph shown here. 
93 
00:06:08,660 --> 00:06:12,050 
n-clique, is a subgraph, 
94 
00:06:12,050 --> 00:06:16,990 
such that the distance between each
node pair in that subgraph is n or less. 
95 
00:06:16,990 --> 00:06:22,790 
By this definition, Holly, Paul,
and Gary form a two clique. 
96 
00:06:24,100 --> 00:06:27,090 
That's a little awkward, isn't it? 
97 
00:06:27,090 --> 00:06:28,120 
Yeah. 
98 
00:06:28,120 --> 00:06:30,550 
They are within two
distance of each other. 
99 
00:06:30,550 --> 00:06:33,600 
But the two clique does not
include intermediate nodes that 
100 
00:06:33,600 --> 00:06:34,480 
connect the member nodes. 
101 
00:06:35,760 --> 00:06:39,690 
So, Mike, is not in the two clique. 
102 
00:06:40,890 --> 00:06:44,570 
This situation is corrected in n-clan. 
103 
00:06:44,570 --> 00:06:49,280 
Where, to belong to the n-clan,
the shortest part between any members, 
104 
00:06:49,280 --> 00:06:51,438 
without involving outsiders, is n or less. 
105 
00:06:51,438 --> 00:06:56,250 
Now, Holly, Mike, 
106 
00:06:56,250 --> 00:07:01,030 
Bill, Don, Harry, and
Gary form a two-clan. 
107 
00:07:01,030 --> 00:07:05,660 
Clearly, this group is more cohesive
than the two-clique we saw before. 
108 
00:07:07,760 --> 00:07:11,490 
n-clique and n-clan
are distance based measures for 
109 
00:07:11,490 --> 00:07:13,455 
finding cohesive groups of communities. 
110 
00:07:13,455 --> 00:07:19,270 
K-core is a density based method for
finding communities. 
111 
00:07:19,270 --> 00:07:22,117 
Let's look at the dark, orange subgraph. 
112 
00:07:22,117 --> 00:07:27,410 
Every node is connected to at least
three other nodes within the subgraph. 
113 
00:07:27,410 --> 00:07:28,867 
They form a 3-core. 
114 
00:07:28,867 --> 00:07:33,110 
Let's include the medium light
orange nodes in the subgraph now. 
115 
00:07:34,720 --> 00:07:39,380 
Each node is connected to at least two
other members within the subgraph, 
116 
00:07:39,380 --> 00:07:40,495 
they form a 2-core. 
117 
00:07:42,400 --> 00:07:49,894 
Relaxing further, we can add the light
orange nodes and the graph as a 1-core. 
1 
00:00:00,910 --> 00:00:03,480 
We have already defined
the degree of the node 
2 
00:00:03,480 --> 00:00:05,965 
as the number of edges connected to it. 
3 
00:00:05,965 --> 00:00:08,815 
Thus specifying if a node is
more connected than another. 
4 
00:00:10,210 --> 00:00:13,760 
Looking closer we can separate out 
5 
00:00:13,760 --> 00:00:18,450 
that two components denotes
degrees into in degree and out. 
6 
00:00:18,450 --> 00:00:23,150 
Which are the counts of the incident and
the outgoing edges of a node respectively. 
7 
00:00:24,570 --> 00:00:27,970 
In the example graph G has an indegree and 
8 
00:00:27,970 --> 00:00:33,860 
outdegree of three making
the total degree equal to six. 
9 
00:00:33,860 --> 00:00:38,680 
We first construct this degree table for
each node. 
10 
00:00:38,680 --> 00:00:42,460 
It's a simple procedure where we count
the number of nodes with degree. 
11 
00:00:42,460 --> 00:00:43,960 
0, 1, 2, and so forth. 
12 
00:00:43,960 --> 00:00:51,100 
The degree versus count table is
a degree histogram of the graph. 
13 
00:00:51,100 --> 00:00:56,380 
We can compare two graphs by computing
the vector distance between them. 
14 
00:00:56,380 --> 00:00:59,625 
One simplistic measure is
just the Euclidean distance. 
15 
00:00:59,625 --> 00:01:04,331 
For our case, the degree
histogram based on comparisons of 
16 
00:01:04,331 --> 00:01:08,390 
the histograms find the graphs
to be very similar. 
17 
00:01:08,390 --> 00:01:12,036 
The more sophisticated
methods are available but 
18 
00:01:12,036 --> 00:01:14,822 
are outside the scope of this course. 
19 
00:01:26,919 --> 00:01:30,280 
We can also compute histograms
of just the in degree or 
20 
00:01:30,280 --> 00:01:32,380 
just the out degree of the graph. 
21 
00:01:33,495 --> 00:01:39,210 
But perhaps more interesting is the joint
two dimensional histogram of the graph, 
22 
00:01:39,210 --> 00:01:43,970 
the colorful histogram of the graph
can be interpreted here as follows. 
23 
00:01:43,970 --> 00:01:49,330 
The graph has a maximum in degree of
three and a maximum out degree of three. 
24 
00:01:49,330 --> 00:01:52,644 
This creates a two-dimensional
histogram with four times four 
25 
00:01:52,644 --> 00:01:54,740 
equal to 16 different joined values. 
26 
00:01:56,020 --> 00:01:59,700 
The actual value for any combination
is computed from the graph and 
27 
00:01:59,700 --> 00:02:00,740 
color coded in the ratings. 
28 
00:02:01,900 --> 00:02:05,890 
For example, there is no node with
in degree 0 and out degree 0. 
29 
00:02:05,890 --> 00:02:09,930 
So the lower-left square of the graph
has value zero and color-coded blue. 
30 
00:02:11,540 --> 00:02:15,580 
On the other hand, there are two nodes
with in-degree three and out-degree three. 
31 
00:02:15,580 --> 00:02:20,510 
Thus, the top-right corner has the value
twp, which is 20% of the nodes. 
32 
00:02:21,810 --> 00:02:25,310 
Color coded as light green. 
33 
00:02:25,310 --> 00:02:27,630 
The 2D histogram provides
an interesting insight. 
34 
00:02:29,160 --> 00:02:32,940 
The nodes with more incident
edges than outgoing edges 
35 
00:02:34,080 --> 00:02:37,740 
represent entities that take
in more than they put out. 
36 
00:02:38,880 --> 00:02:44,090 
In a social networking setting,
they represent members who are listeners. 
37 
00:02:45,450 --> 00:02:48,910 
They receive a lot of posts,
but send much fewer posts. 
38 
00:02:50,360 --> 00:02:53,510 
On the opposite side of the spectrum, 
39 
00:02:53,510 --> 00:02:59,242 
there are talkers whose out-degrees
exceed their in-degrees. 
40 
00:03:00,710 --> 00:03:06,173 
The entities that are in between,
we have both large values of in-degree and 
41 
00:03:06,173 --> 00:03:07,340 
out-degree. 
42 
00:03:07,340 --> 00:03:09,460 
These are communicators. 
43 
00:03:09,460 --> 00:03:13,180 
In this graph, there seems to
be more talkers than listeners. 
44 
00:03:13,180 --> 00:03:14,850 
Not surprising though. 
45 
00:03:14,850 --> 00:03:18,236 
My social media friends
show similar statistics. 
1 
00:00:00,008 --> 00:00:03,640 
In this video we will explore 
2 
00:00:03,640 --> 00:00:08,040 
another fundamental property
of graphs called Connectivity. 
3 
00:00:08,040 --> 00:00:12,350 
We list two important kinds
of graph analytic questions 
4 
00:00:12,350 --> 00:00:13,610 
that are based on connectivity. 
5 
00:00:14,770 --> 00:00:18,380 
In the first case we are asking
about the robustness of the network. 
6 
00:00:19,440 --> 00:00:22,520 
Suppose we have a computer
network with many servers and 
7 
00:00:22,520 --> 00:00:24,330 
a hacker is trying to
bring the system down, 
8 
00:00:25,670 --> 00:00:30,190 
you set a small set of servers that the
hacker can target to disrupt the network. 
9 
00:00:31,690 --> 00:00:35,750 
What we mean by disrupt is to disconnect
a part of the network from the rest. 
10 
00:00:37,262 --> 00:00:41,110 
A similar problem may occur in
the power distribution network. 
11 
00:00:41,110 --> 00:00:46,100 
In that case, an attacker may be able to
attack one or two central components so 
12 
00:00:46,100 --> 00:00:48,700 
that large portions of
the network loses power. 
13 
00:00:50,250 --> 00:00:54,117 
I have a geneticist colleague at
Tech Graduate Institute who studied 
14 
00:00:54,117 --> 00:00:56,460 
the robustness of biological networks. 
15 
00:00:57,850 --> 00:01:02,990 
He told me that many biological
networks have built in redundancy, so 
16 
00:01:02,990 --> 00:01:09,030 
that even if you disrupt one important
gene, there are other genes in the network 
17 
00:01:09,030 --> 00:01:12,580 
which will support the biological
function, possibly through other routes. 
18 
00:01:13,760 --> 00:01:17,940 
Therefore a network is robust,
if removing one or 
19 
00:01:17,940 --> 00:01:21,610 
more edges or
nodes still keeps it connected. 
20 
00:01:23,380 --> 00:01:27,720 
The second category is about network
comparison in terms of their 
21 
00:01:27,720 --> 00:01:28,630 
overall connectivity. 
22 
00:01:30,090 --> 00:01:33,060 
The two graphs shown here are very
different in their structure. 
23 
00:01:34,730 --> 00:01:37,920 
What are some parameters by
which we can compare them? 
24 
00:01:37,920 --> 00:01:43,540 
That to talk about connectivity, we must
first define the concept of connectivity. 
25 
00:01:43,540 --> 00:01:44,200 
Very simple. 
26 
00:01:45,330 --> 00:01:50,120 
A graph is connected if we can
reach any node from any other node. 
27 
00:01:52,150 --> 00:01:57,243 
Let's look at the crypto graph in
the picture, clearly we cannot 
28 
00:01:57,243 --> 00:02:02,714 
reach from all nodes to all nodes here
because this graph is not connected. 
29 
00:02:02,714 --> 00:02:08,850 
However, we can identify four parts of
the graph that are themselves connected. 
30 
00:02:09,920 --> 00:02:14,300 
These islands of connected
parts are called components or 
31 
00:02:14,300 --> 00:02:15,790 
connected components of a graph. 
32 
00:02:17,198 --> 00:02:21,420 
For directed graphs, we need to be
a little more specific about connectivity. 
33 
00:02:22,450 --> 00:02:27,200 
We say that the directed graph is strongly
connected, if we follow the direction of 
34 
00:02:27,200 --> 00:02:31,690 
the edges and still reach every
node from every other node. 
35 
00:02:33,170 --> 00:02:38,640 
A weaker form of connectivity is if we do
not care about the direction of the arrows 
36 
00:02:38,640 --> 00:02:41,790 
and can reach every node
from every other node. 
37 
00:02:43,010 --> 00:02:47,430 
Another way of saying it is that
the undirected version of the graph 
38 
00:02:47,430 --> 00:02:51,760 
is connected,
this is called Weak Connected. 
39 
00:02:51,760 --> 00:02:55,300 
Look at the graph that we have seen so
many times in this course. 
40 
00:02:55,300 --> 00:02:57,994 
Is it strongly connected or
weakly connected? 
41 
00:03:05,749 --> 00:03:10,070 
This leads to some big graph challenge. 
42 
00:03:10,070 --> 00:03:11,750 
Given an arbitrarily large graph, 
43 
00:03:12,960 --> 00:03:16,050 
can I find the connected components
of the graph efficiently? 
44 
00:03:17,230 --> 00:03:20,280 
Second given an arbitrarily large graph, 
45 
00:03:20,280 --> 00:03:22,970 
can we find its sub graphs
that are strongly connected? 
46 
00:03:24,080 --> 00:03:26,649 
We'll touch upon these
questions in Module Four. 
1 
00:00:00,840 --> 00:00:02,630 
So there are two ways to break a graph. 
2 
00:00:04,320 --> 00:00:08,720 
Identifying the smallest node set which
if removed will disconnect the graph. 
3 
00:00:10,000 --> 00:00:16,617 
Here, if we remove F, D and 
4 
00:00:16,617 --> 00:00:21,140 
H or H, F, G,
we have disconnected the graph. 
5 
00:00:22,220 --> 00:00:27,640 
So the separating set is either H,
F and G, or H, F and D. 
6 
00:00:28,870 --> 00:00:30,475 
So the connectivity is three. 
7 
00:00:30,475 --> 00:00:36,726 
Similarly, removing C-F, D-G, D-F and 
8 
00:00:36,726 --> 00:00:43,590 
H-E edges will also disconnect the graph. 
9 
00:00:43,590 --> 00:00:45,730 
Therefore, the edge connectivity is four. 
10 
00:00:46,800 --> 00:00:49,900 
So this is how network
robustness is defined. 
11 
00:00:50,950 --> 00:00:53,790 
Now, let's ask, is this network robust? 
12 
00:00:54,790 --> 00:00:58,530 
Suppose the attacker removed node F. 
13 
00:00:58,530 --> 00:00:59,220 
Why remove F? 
14 
00:01:00,410 --> 00:01:04,430 
Because F is the most connected node. 
15 
00:01:04,430 --> 00:01:06,620 
That is, F has the highest degree. 
16 
00:01:07,780 --> 00:01:11,220 
If we remove F, five paths are disrupted. 
17 
00:01:11,220 --> 00:01:15,550 
For example,
there is no way to go from C to G, or 
18 
00:01:15,550 --> 00:01:19,190 
from I to E,
because these paths went through F. 
19 
00:01:20,340 --> 00:01:21,270 
Let's try this exercise. 
20 
00:01:22,350 --> 00:01:25,800 
Suppose the attacker
has already removed F. 
21 
00:01:25,800 --> 00:01:29,610 
And would like to cause more
damage to the rest of the network. 
22 
00:01:29,610 --> 00:01:31,340 
Which node should be targeted next? 
23 
00:01:32,490 --> 00:01:37,330 
The answer is C,
because C is the next most connected node. 
24 
00:01:38,600 --> 00:01:42,637 
Higher degree nodes make
the network more vulnerable. 
1 
00:00:00,752 --> 00:00:05,460 
So far, the centrality of a node
is defined using the degree and 
2 
00:00:05,460 --> 00:00:08,250 
the shortest distance to other nodes. 
3 
00:00:08,250 --> 00:00:09,830 
Now, we introduce a different idea. 
4 
00:00:11,050 --> 00:00:14,480 
We would like to say that
if you are important and 
5 
00:00:14,480 --> 00:00:17,350 
I'm connected to you,
then I must be somewhat important too. 
6 
00:00:18,540 --> 00:00:21,510 
In other words, my centrality 
7 
00:00:21,510 --> 00:00:26,030 
is proportional to the combined
centrality values of my neighbors. 
8 
00:00:26,030 --> 00:00:30,720 
Now, if we write that down mathematically
it would look like the top formula. 
9 
00:00:30,720 --> 00:00:34,690 
The centrality of (Vi) is
the sum of its neighbors. 
10 
00:00:34,690 --> 00:00:37,140 
Now, we can write that as an equation 
11 
00:00:37,140 --> 00:00:39,300 
where Lambda is
a proportionality constant. 
12 
00:00:40,710 --> 00:00:45,040 
The resulting equation looks exactly
like the Eigenvector equation we have 
13 
00:00:45,040 --> 00:00:46,210 
seen before. 
14 
00:00:46,210 --> 00:00:49,790 
Now again, you really do not
have to understand how it works. 
15 
00:00:49,790 --> 00:00:53,010 
It's fine to know that if
we solve that equation, 
16 
00:00:53,010 --> 00:00:55,390 
we will get the Eigen values lambda. 
17 
00:00:56,690 --> 00:00:59,130 
Now, let's take the largest lambda and 
18 
00:00:59,130 --> 00:01:04,130 
find the corresponding Eigenvector, which
will give you the centrality of each note. 
19 
00:01:05,680 --> 00:01:08,390 
Notice the difference between
the degree centrality and 
20 
00:01:08,390 --> 00:01:10,790 
the Eigenvector centrality
in the same graph. 
21 
00:01:12,290 --> 00:01:15,600 
The yellow node in the middle
has a low degree centrality 
22 
00:01:15,600 --> 00:01:17,540 
compared to its neighbors. 
23 
00:01:17,540 --> 00:01:23,470 
However, with the Eigenvector centrality,
the node becomes comparably more important 
24 
00:01:23,470 --> 00:01:27,880 
because the neighbor centrality status
boost some of the centrality of this node. 
25 
00:01:29,260 --> 00:01:32,790 
In contrast,
consider the second highlighted note. 
26 
00:01:32,790 --> 00:01:37,350 
It had the same on normalize degree
centrality as the previous node. 
27 
00:01:38,520 --> 00:01:42,610 
But because the neighbors
are low centrality nodes, 
28 
00:01:42,610 --> 00:01:45,440 
the eigenvector centrality goes down. 
29 
00:01:45,440 --> 00:01:51,730 
So, this is the intended consequence
of the eigenvector centrality measure. 
30 
00:01:51,730 --> 00:01:57,060 
Speaking of consequences,
Eigenvector centrality essentially says 
31 
00:01:57,060 --> 00:01:59,810 
if you know the right people,
your importance will go up. 
32 
00:02:01,090 --> 00:02:02,760 
Well, that's kind of risky proposition. 
33 
00:02:03,880 --> 00:02:09,090 
Here is me in a social network,
and let's say I'm connected to 
34 
00:02:09,090 --> 00:02:13,250 
this somewhat dubious character, and
I think it doesn't really matter. 
35 
00:02:14,430 --> 00:02:19,490 
What I don't know is that my connection
has it's own set of connections, and 
36 
00:02:19,490 --> 00:02:20,160 
look at who they are. 
37 
00:02:21,480 --> 00:02:25,740 
So, on the one hand, these shady
characters that are now connected to 
38 
00:02:25,740 --> 00:02:30,020 
indirectly does raise my
eigenvector centrality, but 
39 
00:02:30,020 --> 00:02:34,000 
it also has quite a damaging
effect on my reputation. 
40 
00:02:34,000 --> 00:02:37,230 
My EV centrality almost makes
me look like a suspect. 
41 
00:02:38,230 --> 00:02:38,950 
Now, think about that. 
42 
00:02:40,320 --> 00:02:44,090 
Now Brin and Page, the founders of Google 
43 
00:02:44,090 --> 00:02:47,260 
had an interesting way to think
about the eigenvector centrality. 
44 
00:02:48,660 --> 00:02:50,750 
They thought about a server. 
45 
00:02:50,750 --> 00:02:53,410 
Well no, not that kind of server. 
46 
00:02:53,410 --> 00:02:54,770 
This kind of server. 
47 
00:02:54,770 --> 00:02:56,860 
The kind that surfs the web. 
48 
00:02:56,860 --> 00:02:59,855 
But, this is a special web
surfer called a random surfer. 
49 
00:03:01,050 --> 00:03:06,050 
And here is what he does, he picks
a web page and looks at the links. 
50 
00:03:08,040 --> 00:03:13,320 
Then he chooses a random link, goes to
that page and does the same thing again. 
51 
00:03:15,000 --> 00:03:19,600 
Except sometimes when you kind of get
bored and goes to totally new page. 
52 
00:03:21,220 --> 00:03:23,510 
How often does he do this random jump? 
53 
00:03:24,720 --> 00:03:29,250 
Let's say, there's always sort
of a 15% chance that he will. 
54 
00:03:29,250 --> 00:03:32,490 
Or more generally,
with the probability of 1 minus alpha. 
55 
00:03:34,040 --> 00:03:37,210 
So, Page and Brin's idea was to figure out 
56 
00:03:37,210 --> 00:03:41,780 
that this surfer will visit a page with
a high chance, if the page is central. 
57 
00:03:43,060 --> 00:03:44,390 
They came up with a measure for 
58 
00:03:44,390 --> 00:03:49,010 
this stationary probability of a page
being visited by the random surfer. 
59 
00:03:50,160 --> 00:03:52,820 
They did not call it centrality,
they called it pagerank. 
60 
00:03:54,330 --> 00:03:57,114 
Let's see a YouTube video to
understand how pagerank behaves. 
61 
00:05:15,194 --> 00:05:19,427 
Okay, now we are talking about
the World Wide Web which is a huge graph. 
62 
00:05:19,427 --> 00:05:22,565 
How do we of such a graph? 
63 
00:05:23,575 --> 00:05:28,550 
The answer is iteratively using
a method called power iteration. 
64 
00:05:29,710 --> 00:05:33,440 
This method can be used because we
are looking for the largest eigenvalue. 
65 
00:05:33,440 --> 00:05:34,070 
Let me show you. 
66 
00:05:35,390 --> 00:05:36,360 
Let's take a small graph. 
67 
00:05:37,990 --> 00:05:41,730 
Let's initialize the still unknown
page rank as zero for all nodes. 
68 
00:05:43,020 --> 00:05:48,930 
Now, page rank A is a 0.15 chance,
that I was at A already. 
69 
00:05:48,930 --> 00:05:53,740 
And 0.85 chance that I come to A from B,
or I come to A from C. 
70 
00:05:55,270 --> 00:05:58,370 
However, at this point,
the page rank of B and C are at zero. 
71 
00:05:59,700 --> 00:06:03,134 
So, in the first iteration,
page rank of A is 0.15. 
72 
00:06:04,670 --> 00:06:10,210 
Now for B, I can only come to B from A,
but we cannot claim all of pagerank of A, 
73 
00:06:10,210 --> 00:06:14,901 
because there is always a half chance
that the surfer will come to B 
74 
00:06:14,901 --> 00:06:18,255 
from A because he could
also get to here from C. 
75 
00:06:18,255 --> 00:06:24,042 
This, plus the 0.15 chance that
the surfer is already at B, 
76 
00:06:24,042 --> 00:06:26,733 
makes Bs pagerank 0.21. 
77 
00:06:26,733 --> 00:06:30,221 
Now after doing a few rounds of this
computation, between 50 and 100 iteration, 
78 
00:06:30,221 --> 00:06:30,760 
let's say. 
79 
00:06:31,820 --> 00:06:32,830 
The values will converge. 
80 
00:06:33,830 --> 00:06:37,570 
Now, this computation has been
demonstrated to perform well 
81 
00:06:37,570 --> 00:06:38,740 
in the MapReduce framework. 
82 
00:06:40,020 --> 00:06:44,100 
In module four, we'll talk about
another way of computing this metric. 
1 
00:00:03,940 --> 00:00:06,930 
Welcome back to the second
module of the course. 
2 
00:00:08,000 --> 00:00:11,480 
In this module, we will cover
a number of basic principles and 
3 
00:00:11,480 --> 00:00:12,780 
techniques of graph analytics. 
4 
00:00:14,340 --> 00:00:18,400 
As we mentioned in the last module,
the goal of graph analytics 
5 
00:00:18,400 --> 00:00:23,200 
is to utilize the mathematical
properties of data and provide 
6 
00:00:23,200 --> 00:00:27,710 
efficient algorithmic solutions for large
and complex graph structure problems. 
7 
00:00:28,970 --> 00:00:33,870 
As I said, in this module, we'll learn a
number of basic graph analytic techniques. 
8 
00:00:35,080 --> 00:00:38,580 
After this module,
you'll be able to identify the right 
9 
00:00:38,580 --> 00:00:42,160 
class of techniques to apply for
a graph analytics problem. 
10 
00:00:42,160 --> 00:00:46,630 
To be more specific,
in this module we will consider 
11 
00:00:46,630 --> 00:00:50,380 
the mathematical and
algorithmic aspects, and not so 
12 
00:00:50,380 --> 00:00:53,050 
much the computing frameworks
to implement these methods. 
13 
00:00:54,590 --> 00:00:59,210 
In Modules 3 and 4, we will look at
two different kinds of computing 
14 
00:00:59,210 --> 00:01:03,190 
platforms that are used for implementing
the techniques discussed in this module. 
15 
00:01:04,330 --> 00:01:06,110 
Here is the lesson plan for the module. 
16 
00:01:07,320 --> 00:01:10,800 
First, we'll discuss a few basic terms and 
17 
00:01:10,800 --> 00:01:13,850 
their definition which we'll use for
the rest of the course. 
18 
00:01:16,030 --> 00:01:18,980 
Of course, they are not the only terms and
concepts we will learn. 
19 
00:01:20,200 --> 00:01:23,520 
As we go through each technique,
we will add more terms and 
20 
00:01:23,520 --> 00:01:25,026 
definitions in our vocabulary. 
21 
00:01:25,026 --> 00:01:29,838 
Now following these definitions we
will consider four categories of 
22 
00:01:29,838 --> 00:01:32,060 
graph analytic procedures. 
23 
00:01:32,060 --> 00:01:37,300 
The first, called Path Analytics,
is centered around the Analytic Techniques 
24 
00:01:37,300 --> 00:01:40,250 
where the primary objective involves
traversing to the nodes and 
25 
00:01:40,250 --> 00:01:41,170 
edges of the ground. 
26 
00:01:42,820 --> 00:01:45,260 
The second analytic technique inquires and 
27 
00:01:45,260 --> 00:01:48,010 
explores the connectivity
pattern of the gaps. 
28 
00:01:48,010 --> 00:01:52,160 
Where the term connectivity pattern
refers to the structure and 
29 
00:01:52,160 --> 00:01:54,510 
organizations of the edges of the graph. 
30 
00:01:56,070 --> 00:01:59,230 
The third analytics category
involves the discovery and 
31 
00:01:59,230 --> 00:02:03,820 
behavior of communities which are closely
interacting entities in a network. 
32 
00:02:04,970 --> 00:02:10,210 
The fourth category, termed Centrality
Analytics, detects and characterizes 
33 
00:02:10,210 --> 00:02:14,670 
significant nodes of a network with
respect to a specific analysis problem. 
34 
00:02:15,750 --> 00:02:19,570 
Of course there are many more types of
graph analytic techniques that we'll cover 
35 
00:02:19,570 --> 00:02:20,710 
in the course. 
36 
00:02:20,710 --> 00:02:24,000 
We'll provide some additional reading
material for those who are interested. 
37 
00:02:24,000 --> 00:02:27,590 
But we start by recapitulating
our definition of graphs 
38 
00:02:27,590 --> 00:02:32,225 
as a collection of vertices and edges,
which represent ordered pairs of nodes. 
39 
00:02:33,580 --> 00:02:36,880 
While this mathematical
definition is indeed correct, 
40 
00:02:36,880 --> 00:02:41,430 
in practice it needs to be extended to
give you other information elements. 
41 
00:02:43,040 --> 00:02:44,540 
Let us consider a single Tweet. 
42 
00:02:46,090 --> 00:02:51,040 
As we have mentioned previously,
a Tweet is a complex information output 
43 
00:02:51,040 --> 00:02:53,970 
because it is a graph in itself
with several nodes and edges. 
44 
00:02:55,120 --> 00:02:57,870 
But over and about the structure, a Tweet 
45 
00:02:57,870 --> 00:03:01,880 
actually contains much more information
and code inside the nodes and edges. 
46 
00:03:04,320 --> 00:03:07,560 
First, there several kinds
of nodes in a Tweet. 
47 
00:03:08,680 --> 00:03:12,810 
For example it has a Tweet node,
a User node, a Media node, 
48 
00:03:12,810 --> 00:03:15,335 
a URL node, a hashtag node and so forth. 
49 
00:03:16,750 --> 00:03:22,115 
This assignment of kinds or labels to
nodes is often called node typing. 
50 
00:03:23,710 --> 00:03:27,352 
Every graph application will
have its own set of types. 
51 
00:03:27,352 --> 00:03:32,350 
And it'll assign one or more types to
a node but it is not mandatory for 
52 
00:03:32,350 --> 00:03:33,840 
an application to use node types. 
53 
00:03:36,140 --> 00:03:41,280 
Mathematically we can extend our original
definition with two more elements. 
54 
00:03:41,280 --> 00:03:46,760 
The set of node types and the mapping
function that assigns types to nodes. 
55 
00:03:46,760 --> 00:03:49,360 
That means it associates
a type to every node. 
56 
00:03:50,430 --> 00:03:56,950 
However, not all nodes need to have
a type but in many applications they do. 
57 
00:03:59,370 --> 00:04:03,250 
In addition to types,
a node also has attributes and values. 
58 
00:04:04,680 --> 00:04:05,910 
In our Tweet example, 
59 
00:04:07,090 --> 00:04:11,740 
text is the name of an attribute that
refers to the textual body of the Tweet 
60 
00:04:11,740 --> 00:04:16,910 
whose value is a character string
written by the author of the Tweet. 
61 
00:04:17,980 --> 00:04:20,620 
For a specific kind of data like a Tweet, 
62 
00:04:20,620 --> 00:04:23,770 
one has a fixed set of attributes
as decided by Twitter. 
63 
00:04:24,970 --> 00:04:27,800 
This collection of attributes
is called a node schema. 
64 
00:04:29,080 --> 00:04:30,480 
For a general graph, 
65 
00:04:30,480 --> 00:04:33,700 
a node schema may have an arbitrary
number of attribute value pairs. 
66 
00:04:34,810 --> 00:04:39,330 
We will revisit this in module 3
when we discuss graphing the models. 
67 
00:04:40,690 --> 00:04:45,290 
Similarly, at edge of a graph, we have
an edge type, also called an edge label. 
68 
00:04:46,390 --> 00:04:47,508 
Also just like a node, 
69 
00:04:47,508 --> 00:04:52,026 
an edge may have an edge schema
consisting of attribute value pierce. 
70 
00:04:52,026 --> 00:04:57,590 
Here, Interaction Type is
an attribute in our biological 
71 
00:04:57,590 --> 00:05:01,260 
network that describes the modality of
interaction between a pair of genes. 
72 
00:05:03,550 --> 00:05:06,100 
For the specific edge we have highlighted, 
73 
00:05:06,100 --> 00:05:09,470 
the genes interact through
biochemical activity. 
74 
00:05:09,470 --> 00:05:12,040 
Because they are party to
some biochemical process. 
75 
00:05:14,830 --> 00:05:18,130 
Clearly, there are different kinds
of interaction between these 
76 
00:05:18,130 --> 00:05:18,910 
genes or proteins. 
77 
00:05:19,930 --> 00:05:24,684 
That means an attribute called
interaction type can have a set of 
78 
00:05:24,684 --> 00:05:28,570 
possible values like physical,
genetic and so on. 
79 
00:05:30,410 --> 00:05:35,055 
This set of possible values is
called the domain of the attribute. 
80 
00:05:37,850 --> 00:05:40,670 
Putting these elements back
into our mathematical model, 
81 
00:05:40,670 --> 00:05:45,040 
we get a more concrete specification of
what a real live graph would contain. 
82 
00:05:46,340 --> 00:05:50,450 
We have already discussed edge types
as well as node and edge properties. 
83 
00:05:52,450 --> 00:05:54,090 
Take a minute to look through this again. 
84 
00:05:55,910 --> 00:06:01,880 
Whenever you consider an application that
needs graph analytics, the first task 
85 
00:06:01,880 --> 00:06:05,800 
should be to determine the informational
model of the graph your application needs. 
86 
00:06:06,850 --> 00:06:10,630 
It's always a good exercise to
document the information model, 
87 
00:06:10,630 --> 00:06:13,280 
in terms of the elements
described on the slide. 
88 
00:06:15,365 --> 00:06:17,890 
Let's see a little more on
the topic of edge properties. 
89 
00:06:19,468 --> 00:06:23,650 
Many application encode different
kids of numeric knowledge 
90 
00:06:23,650 --> 00:06:26,370 
into edges of a graph in
the form of edge points. 
91 
00:06:27,850 --> 00:06:31,050 
If we do not put weights
in an adjacency metrics, 
92 
00:06:31,050 --> 00:06:36,290 
an edge is just represented by
placing a one in the appropriate cell. 
93 
00:06:36,290 --> 00:06:38,800 
However, if we do use a weight, 
94 
00:06:38,800 --> 00:06:43,070 
the weight value can be placed in
the adjacency matrix to facilitate 
95 
00:06:43,070 --> 00:06:45,939 
down stream computation as
we'll show in the next lesson. 
96 
00:06:47,330 --> 00:06:48,130 
What do the weights mean? 
97 
00:06:49,440 --> 00:06:51,920 
That depends on the application. 
98 
00:06:51,920 --> 00:06:52,720 
Let's see some examples. 
99 
00:06:54,530 --> 00:07:00,440 
The most obvious example is a road map
where the nodes are road intersections and 
100 
00:07:00,440 --> 00:07:02,960 
the edges represent
stretches of the street or 
101 
00:07:02,960 --> 00:07:04,380 
highway between these intersections. 
102 
00:07:05,520 --> 00:07:09,200 
The edge weight can represent the distance
of a particular segment of the road. 
103 
00:07:12,070 --> 00:07:16,500 
In a personal communication network,
for example an email network, 
104 
00:07:16,500 --> 00:07:22,520 
we can count the average number of emails
per week sent from John to Jill and 
105 
00:07:22,520 --> 00:07:25,700 
use it as a proxy for
the strength of their connection. 
106 
00:07:25,700 --> 00:07:30,049 
So more emails means
a stronger connection. 
107 
00:07:30,049 --> 00:07:34,276 
In a biological network, one often has
to assess whether an interaction that 
108 
00:07:34,276 --> 00:07:38,699 
can occur is actually likely to occur
given the concentration of the reactants, 
109 
00:07:38,699 --> 00:07:43,330 
the chemical environment at the site
of the reaction and so forth. 
110 
00:07:43,330 --> 00:07:47,520 
This is represented as a weight that
designates the likelihood of interaction. 
111 
00:07:48,930 --> 00:07:51,610 
Finally, consider a knowledge network 
112 
00:07:51,610 --> 00:07:55,830 
where nodes represent entities
like people, places and events. 
113 
00:07:55,830 --> 00:08:00,920 
And edges represent relationships
like a person visited a place or 
114 
00:08:00,920 --> 00:08:04,460 
movie actor Tom is dating
movie actress Kim. 
115 
00:08:04,460 --> 00:08:07,780 
Now this kind of information may
be important for some news media. 
116 
00:08:07,780 --> 00:08:12,716 
However, if the information does
not come from an authentic source, 
117 
00:08:12,716 --> 00:08:17,200 
itself, it is more prudent to
put a certainty value on it. 
118 
00:08:17,200 --> 00:08:20,150 
This certainty value may be
treated as a weight on the edge. 
119 
00:08:21,530 --> 00:08:24,280 
Moving on from the information
model of the graph, 
120 
00:08:24,280 --> 00:08:28,200 
the structure of the graph often
contains valuable insights to a graph. 
121 
00:08:29,270 --> 00:08:32,060 
Many of the graph analytic techniques
we will discuss in this section 
122 
00:08:33,130 --> 00:08:36,338 
will consider these structural
properties of graphs. 
123 
00:08:36,338 --> 00:08:43,130 
One such structure is a loop,
which is an edge from a node to itself. 
124 
00:08:43,130 --> 00:08:44,570 
In the example here, 
125 
00:08:44,570 --> 00:08:48,410 
you can see that a protein can interact
with another protein of the same kind. 
126 
00:08:49,440 --> 00:08:51,420 
Many other examples abound. 
127 
00:08:51,420 --> 00:08:53,997 
People send emails to themselves. 
128 
00:08:53,997 --> 00:08:56,630 
A road segment circles back
to the same intersection. 
129 
00:08:57,800 --> 00:09:00,814 
A website has a link to its own URL. 
130 
00:09:00,814 --> 00:09:05,578 
The existence of loops and the nodes that
have such loops can be very informative 
131 
00:09:05,578 --> 00:09:09,790 
in some applications and
can be problematic for other applications. 
132 
00:09:11,160 --> 00:09:14,910 
Another structure property of a graph
is the occurrence of multiple 
133 
00:09:14,910 --> 00:09:17,230 
edges between the same node pair. 
134 
00:09:17,230 --> 00:09:21,170 
The graphs with this feature
are called multi-graphs. 
135 
00:09:21,170 --> 00:09:25,330 
In this example, the two map kinase
genes have five edges between them. 
136 
00:09:27,030 --> 00:09:28,396 
Why multiple edges? 
137 
00:09:28,396 --> 00:09:35,130 
It's because each edge has
a different information content. 
138 
00:09:35,130 --> 00:09:35,760 
In this case, 
139 
00:09:35,760 --> 00:09:40,100 
these two genes can have five different
types of interactions between them. 
140 
00:09:40,100 --> 00:09:44,540 
Where each interaction has a different
value for the attribute interaction type. 
141 
00:09:45,830 --> 00:09:48,500 
We see this all the time
in human networks too. 
142 
00:09:48,500 --> 00:09:53,080 
A person can be my spouse,
a co-performer in music and 
143 
00:09:53,080 --> 00:09:55,050 
my financial adviser,
all at the same time. 
144 
00:09:56,140 --> 00:10:00,100 
Many analytics algorithms
are not natively designed for 
145 
00:10:00,100 --> 00:10:05,040 
multigraphs, and often need a little
customization to handle them. 
146 
00:10:05,040 --> 00:10:09,130 
We will mention some of these
customizations as we go forward and walk 
147 
00:10:09,130 --> 00:10:13,791 
through the different kinds of analytics
applications preformed on graphs. 
1 
00:00:00,600 --> 00:00:02,180 
So we talked about local properties. 
2 
00:00:03,300 --> 00:00:04,160 
In this lecture, 
3 
00:00:04,160 --> 00:00:07,860 
we'll cover a global property based
method of [INAUDIBLE] finding. 
4 
00:00:09,610 --> 00:00:13,000 
The specific property we focus on,
is called modularity. 
5 
00:00:14,230 --> 00:00:16,990 
It tries to estimate
the quality of clusters of 
6 
00:00:16,990 --> 00:00:17,855 
communities in the [INAUDIBLE]. 
7 
00:00:19,270 --> 00:00:20,680 
The intuition is as follows. 
8 
00:00:21,890 --> 00:00:24,255 
If we consider the edges in a group and 
9 
00:00:24,255 --> 00:00:28,528 
try to see whether it's different
from what you'd see if the edges 
10 
00:00:28,528 --> 00:00:32,588 
were assigned randomly with
some probability distribution. 
11 
00:00:32,588 --> 00:00:36,730 
If there is a community, there will be
more edges than would happen at random. 
12 
00:00:37,850 --> 00:00:42,692 
If there is no community in some part
of the graph, the number of edges in 
13 
00:00:42,692 --> 00:00:47,637 
that part will either be close to
the random case or even lower than that. 
14 
00:00:47,637 --> 00:00:53,136 
The modularity measure thus estimates
the quality of the clusters in 
15 
00:00:53,136 --> 00:01:00,070 
the graph by evaluating this difference of
the actual minus the random edge fraction. 
16 
00:01:01,260 --> 00:01:03,930 
So this is the mathematical
formulation of what I just described. 
17 
00:01:05,290 --> 00:01:09,045 
The adjacency matrix, A,
gives us the actual edges. 
18 
00:01:10,740 --> 00:01:15,547 
The Pij provides a probability
of a random edge. 
19 
00:01:15,547 --> 00:01:19,500 
The m in the denominator gives
us the fractional edges. 
20 
00:01:21,110 --> 00:01:25,654 
And the delta function's
task is to evaluate if i and 
21 
00:01:25,654 --> 00:01:28,451 
j should be in the same cluster. 
22 
00:01:28,451 --> 00:01:33,502 
If they are,
the contribution will be added to Q, 
23 
00:01:33,502 --> 00:01:38,449 
the quality metric,
which is multi-layered. 
24 
00:01:39,590 --> 00:01:43,599 
Well, we have not defined what
the probability model looks like. 
25 
00:01:43,599 --> 00:01:46,790 
There are many ways to figure
out what Pij should look like. 
26 
00:01:48,360 --> 00:01:53,170 
One simple model says, that the chance
that there is an edge between nodes i and 
27 
00:01:53,170 --> 00:02:00,240 
j, is proportional to the degree of
node i times the degree of node j. 
28 
00:02:00,240 --> 00:02:02,510 
That means if nodes i and 
29 
00:02:02,510 --> 00:02:06,890 
j are already well connected, there is
a high chance that they share an edge. 
30 
00:02:07,940 --> 00:02:10,520 
So if you're a mathematical person,
you might be thinking, okay, 
31 
00:02:11,690 --> 00:02:16,500 
let's find clusters in the graph so
that Q is maximum and then we're done. 
32 
00:02:17,960 --> 00:02:23,094 
Well sadly, maximizing Q is very hard. 
33 
00:02:23,094 --> 00:02:26,050 
So we need to find
an approximate solution. 
34 
00:02:28,490 --> 00:02:31,180 
So we will illustrate a very 
35 
00:02:31,180 --> 00:02:35,660 
popular method of finding this
modularity based community detection. 
36 
00:02:37,290 --> 00:02:40,500 
Well, the best way to describe
this is through a YouTube video. 
37 
00:02:41,560 --> 00:02:42,060 
That's the URL. 
38 
00:02:43,600 --> 00:02:47,758 
Now, the next slide and
the following slide, 
39 
00:02:47,758 --> 00:02:53,195 
describes it, but
I think it's better to explain the method 
40 
00:02:53,195 --> 00:02:57,686 
through screenshots off
the video as it happens. 
41 
00:02:59,748 --> 00:03:04,273 
We will show you some
snapshots of this video. 
42 
00:03:04,273 --> 00:03:07,630 
There are 309 nodes in this graph. 
43 
00:03:08,670 --> 00:03:11,720 
Initially, they all have different colors. 
44 
00:03:11,720 --> 00:03:13,580 
That is,
they belong to different communities. 
45 
00:03:15,270 --> 00:03:22,328 
This screenshot shows the graph at
iteration 144 of the algorithm. 
46 
00:03:22,328 --> 00:03:26,586 
The number of communities now is 286. 
47 
00:03:26,586 --> 00:03:33,050 
The chart on the right side plots time on
the x-axis and modularity on the y-axis. 
48 
00:03:33,050 --> 00:03:35,670 
As you see,
the modularity's on the rights. 
49 
00:03:36,750 --> 00:03:39,730 
Roughly, at each iteration, 
50 
00:03:40,800 --> 00:03:45,910 
the system is trying to change the color
of a node to that of its neighbors. 
51 
00:03:45,910 --> 00:03:50,593 
But it actually changes the color
only if the modularity value of 
52 
00:03:50,593 --> 00:03:54,765 
the whole graph changes as
a result of that color change. 
53 
00:03:54,765 --> 00:04:00,107 
After a few more iterations,
the number of communities has become 241. 
54 
00:04:00,107 --> 00:04:05,990 
The three arrows show some parts of the
graph where the nodes have changed colors. 
55 
00:04:05,990 --> 00:04:09,578 
Modularity is on the rise. 
56 
00:04:09,578 --> 00:04:15,115 
After 1,437 iterations, the modularity
of the graph is still going up. 
57 
00:04:15,115 --> 00:04:17,610 
Now, there are 113 communities. 
58 
00:04:19,060 --> 00:04:23,220 
The errors show some new areas where
the neighboring nodes have the same color. 
59 
00:04:24,580 --> 00:04:29,690 
At 1,842 iterations,
the modulary gains slows down. 
60 
00:04:29,690 --> 00:04:34,992 
Meanwhile, the number of
communities have reduced to 75. 
61 
00:04:34,992 --> 00:04:40,540 
At 4,179 iterations, the modularity
growth has started becoming flat. 
62 
00:04:41,980 --> 00:04:47,657 
But in the meantime, the number of colors,
that is communities, has reduced to 48. 
63 
00:04:47,657 --> 00:04:52,556 
At around 5,196 iterations, the algorithm
decides that there is not enough 
64 
00:04:52,556 --> 00:04:55,041 
reduction in the number of communities, 
65 
00:04:55,041 --> 00:04:59,240 
which is reduced to only 45 in
the last 1,000 iterations or so. 
66 
00:05:00,730 --> 00:05:05,440 
Now it collapses each community that
is clustered to a single node, and 
67 
00:05:05,440 --> 00:05:08,180 
creates a cluster to cluster edges. 
68 
00:05:09,250 --> 00:05:14,140 
The orange box,
an arrow shows this contraction. 
69 
00:05:14,140 --> 00:05:18,140 
Now compared to the previous slide,
this collapsing or 
70 
00:05:18,140 --> 00:05:22,130 
contraction of the plastic creates
a skeleton of the original draft. 
71 
00:05:23,400 --> 00:05:26,470 
Now the algorithm starts again
with this reduced graph. 
72 
00:05:28,480 --> 00:05:31,509 
You will find many graph
analysis software, 
73 
00:05:31,509 --> 00:05:35,658 
where you can run the Louvain
method of community detection. 
74 
00:05:37,747 --> 00:05:39,837 
[INAUDIBLE]. 
75 
00:05:39,837 --> 00:05:43,749 
I took my Linked-in network,
which has me at the center, and 
76 
00:05:43,749 --> 00:05:45,640 
all my connections as nodes. 
77 
00:05:47,630 --> 00:05:50,483 
If two of my contacts are also
connected in Linked-in, 
78 
00:05:50,483 --> 00:05:52,069 
there is an edge between them. 
79 
00:05:53,110 --> 00:05:57,350 
This kind of mean-centric network
is often called an ego network. 
80 
00:05:58,806 --> 00:06:04,700 
When I [INAUDIBLE] the community direction
algorithm here, I found six communities, 
81 
00:06:04,700 --> 00:06:08,420 
with one set of parameters and
seven communities with another. 
82 
00:06:09,500 --> 00:06:12,390 
I could clearly see my
connections in San Diego. 
83 
00:06:12,390 --> 00:06:16,190 
My connections in my professional network,
my friends in India, and so forth. 
84 
00:06:17,380 --> 00:06:22,090 
There's always one false community,
which stands for 
85 
00:06:22,090 --> 00:06:25,850 
others, nodes that do not clearly
belong to any specific group. 
86 
00:06:27,730 --> 00:06:32,720 
Perhaps more interesting and
important than the static 
87 
00:06:32,720 --> 00:06:37,490 
analysis of communities is to track
communities over a length of time. 
88 
00:06:37,490 --> 00:06:39,300 
And determine how they evolve and why. 
89 
00:06:41,140 --> 00:06:45,400 
There are six large
categories of evolution steps 
90 
00:06:45,400 --> 00:06:46,690 
that can happen within a community. 
91 
00:06:47,810 --> 00:06:50,340 
A community like a new
Facebook group can be born. 
92 
00:06:51,850 --> 00:06:55,090 
A community, like a group of
people who gathered for an event, 
93 
00:06:55,090 --> 00:06:59,570 
would dissolve because the event and
the mutual interest around it has ended. 
94 
00:07:01,180 --> 00:07:04,986 
A community can grow because the members
rally around a common cause. 
95 
00:07:04,986 --> 00:07:10,318 
Typically, new cross-community
edges start getting 
96 
00:07:10,318 --> 00:07:14,977 
formed before the communities
actually merge. 
97 
00:07:14,977 --> 00:07:17,930 
The communities can
shrink like my book club. 
98 
00:07:17,930 --> 00:07:19,190 
Where do you see this in real life? 
99 
00:07:20,430 --> 00:07:21,510 
Well, how about company mergers? 
100 
00:07:23,520 --> 00:07:26,390 
Surely enough, the [INAUDIBLE] results
will happen when a community splits. 
101 
00:07:27,510 --> 00:07:29,690 
Going along with the previous example, 
102 
00:07:29,690 --> 00:07:34,170 
a closely working group in a company may
at some point create their own product and 
103 
00:07:34,170 --> 00:07:39,230 
form a new company with very
little ties to the old company. 
104 
00:07:39,230 --> 00:07:44,081 
One standard symptom of a group splitting
is that the nodes in the subgroup 
105 
00:07:44,081 --> 00:07:48,399 
show an increase in the number of
edges just amongst themselves. 
1 
00:00:01,110 --> 00:00:04,130 
So far we have seen two versions
of the Dijkstra Algorithm. 
2 
00:00:05,200 --> 00:00:09,680 
Both these versions assume that the edge
weights provided by the network 
3 
00:00:09,680 --> 00:00:10,850 
must be used as is. 
4 
00:00:11,890 --> 00:00:14,430 
Now that can lead to some
interesting problems. 
5 
00:00:14,430 --> 00:00:16,570 
We saw one such problem before. 
6 
00:00:16,570 --> 00:00:19,810 
Remember we were trying to decide
whether we should go from G to D or 
7 
00:00:19,810 --> 00:00:24,060 
from F to E because both options
had the same total weight? 
8 
00:00:24,060 --> 00:00:26,670 
Now had we chosen to go from G to D, 
9 
00:00:26,670 --> 00:00:30,070 
it would take us a few extra steps
to arrive at correct solution. 
10 
00:00:31,190 --> 00:00:35,110 
One way of handling this problem
is to use additional knowledge. 
11 
00:00:35,110 --> 00:00:39,690 
So intuitively, we want to say that
we know that we want to go to B. 
12 
00:00:39,690 --> 00:00:42,800 
So traversing through
D is not a good idea. 
13 
00:00:42,800 --> 00:00:45,530 
Because it will take us away from B. 
14 
00:00:45,530 --> 00:00:50,700 
In other words we use the knowledge
of the destination of B's location 
15 
00:00:50,700 --> 00:00:52,650 
to steer the direction of search. 
16 
00:00:53,930 --> 00:00:57,400 
This variant is called
Goal-Directed Dijkstra Algorithm because 
17 
00:00:57,400 --> 00:01:02,440 
it is using the information about the
target known at any point in the search. 
18 
00:01:02,440 --> 00:01:06,919 
The trick to use this information is to
change the edge weights as we diverse. 
19 
00:01:08,000 --> 00:01:09,350 
How do we change the weight? 
20 
00:01:09,350 --> 00:01:12,040 
We use a formula, where the new weight 
21 
00:01:12,040 --> 00:01:16,560 
is the original weight together with
a function called the potential function. 
22 
00:01:16,560 --> 00:01:18,340 
Now we'll show this in our example. 
23 
00:01:18,340 --> 00:01:19,950 
Since our graph is a proxy for 
24 
00:01:19,950 --> 00:01:24,190 
a road network, we can assume that we
know the coordinates of every node. 
25 
00:01:24,190 --> 00:01:27,810 
Therefore, we can compute
the distance between any two nodes. 
26 
00:01:27,810 --> 00:01:30,630 
In practice, we will choose a few nodes so 
27 
00:01:30,630 --> 00:01:35,210 
that we can compute the distance of every
other node from these chosen nodes. 
28 
00:01:35,210 --> 00:01:37,450 
These chosen nodes are called landmarks. 
29 
00:01:37,450 --> 00:01:41,280 
Let's assume B, which is our target,
is a landmark node, and 
30 
00:01:41,280 --> 00:01:44,430 
let's rewind to the state where
we are trying to choose between 
31 
00:01:44,430 --> 00:01:47,780 
the GD expansion or the FE expansion. 
32 
00:01:47,780 --> 00:01:53,172 
So we calculate the distance of F,
G and E from B. 
33 
00:01:53,172 --> 00:01:56,730 
BF is 20, BG is 80, and BE is 15. 
34 
00:01:56,730 --> 00:01:59,312 
Now we'll apply the formula like this. 
35 
00:01:59,312 --> 00:02:03,900 
For the FG case,
we subtract the BF distance 
36 
00:02:03,900 --> 00:02:07,950 
from the weight and
add the BD distance to the weight. 
37 
00:02:07,950 --> 00:02:12,180 
This gives us 70 because G is far from B. 
38 
00:02:12,180 --> 00:02:16,210 
Similarly for the FE case,
we subtract the BF distance and 
39 
00:02:16,210 --> 00:02:20,000 
add the BE distance to the weight and
it gives us 15. 
40 
00:02:20,000 --> 00:02:23,960 
Now, with these modified weights
we choose the FE expansion. 
41 
00:02:23,960 --> 00:02:29,060 
In practice this significantly improves
the actual performance of the algorithm. 
42 
00:02:29,060 --> 00:02:33,201 
So, this technique is used by many online
mapping services when they give you 
43 
00:02:33,201 --> 00:02:33,973 
directions. 
1 
00:00:00,530 --> 00:00:04,710 
So now let's bring back the constraints
that we have ignored so far. 
2 
00:00:05,820 --> 00:00:10,230 
There are two constraints here,
parts of the graph to include and 
3 
00:00:10,230 --> 00:00:12,280 
parts of the graph to exclude. 
4 
00:00:12,280 --> 00:00:16,840 
In this example we have to go to B, but 
5 
00:00:16,840 --> 00:00:21,020 
we should go to J first, and
then travel from B to J. 
6 
00:00:22,380 --> 00:00:25,850 
Also, we cannot use any
of the paths through E. 
7 
00:00:26,960 --> 00:00:28,650 
This really means two things. 
8 
00:00:29,930 --> 00:00:35,610 
First, we split the problem into two
independent shortest path problems 
9 
00:00:35,610 --> 00:00:38,670 
that we can solve in parallel if needed. 
10 
00:00:38,670 --> 00:00:43,440 
Second, when we go from J
to B We need to extract 
11 
00:00:44,630 --> 00:00:48,240 
the useful subgraph that
we need to consider. 
12 
00:00:48,240 --> 00:00:51,960 
For a large network and
a complex exclusion condition, 
13 
00:00:53,010 --> 00:00:56,160 
we will essentially operate
over a smaller graph, 
14 
00:00:56,160 --> 00:00:58,640 
thereby reducing the effective
size of the problem. 
15 
00:01:00,130 --> 00:01:04,770 
As we'll see in module three,
this kind of subgraph extraction operation 
16 
00:01:04,770 --> 00:01:09,460 
can be done effectively and
efficiently with a graph database system. 
17 
00:01:09,460 --> 00:01:12,738 
This concludes our short
tour of path analytics. 
1 
00:00:00,690 --> 00:00:05,620 
So, in this optional module, we'll look
at the two key player problems again. 
2 
00:00:07,160 --> 00:00:11,178 
The goal of the first problem is
to identify a small set of nodes, 
3 
00:00:11,178 --> 00:00:15,120 
whose removal will create
maximum disruption. 
4 
00:00:16,710 --> 00:00:21,540 
Now, in this case, a traditional
centrality algorithm may not work, 
5 
00:00:21,540 --> 00:00:24,310 
because the optimization goal
is to break up the network. 
6 
00:00:26,040 --> 00:00:28,810 
So we need a quantitive
measure of the breakage. 
7 
00:00:30,090 --> 00:00:32,980 
If dij is the distance between nodes i and 
8 
00:00:32,980 --> 00:00:37,920 
j, then 1 over dij is
the closeness of these two nodes. 
9 
00:00:39,360 --> 00:00:43,980 
If we add the closeness of all nodes and
normalize it by the number of node pairs, 
10 
00:00:43,980 --> 00:00:47,150 
we'll get a measure of
cohesiveness as a fraction. 
11 
00:00:48,420 --> 00:00:54,070 
So, 1 minus this value is
a measure of fragmentation. 
12 
00:00:55,300 --> 00:00:58,880 
Our goal is to maximize
this fragmentation metric. 
13 
00:01:00,660 --> 00:01:06,599 
In the model terrorist network shown here,
removing the red nodes A, 
14 
00:01:06,599 --> 00:01:11,530 
B, and C will break up the network
into seven components, 
15 
00:01:11,530 --> 00:01:14,669 
with F reaching a value of 0.59. 
16 
00:01:14,669 --> 00:01:20,026 
The second key player problem, is trying
to find a group of S influencers, 
17 
00:01:20,026 --> 00:01:24,120 
which can reach a maximum
number of nodes within K steps. 
18 
00:01:25,920 --> 00:01:29,850 
The number of unique nodes reachable from
a starting node is called the reach, 
19 
00:01:29,850 --> 00:01:30,510 
of the starting node. 
20 
00:01:32,240 --> 00:01:37,420 
For this we need to adapt the concept
of reach to limit it to k steps. 
21 
00:01:38,820 --> 00:01:43,570 
We also need to adapt it to measure
the distance of an arbitrary node 
22 
00:01:43,570 --> 00:01:45,891 
from a group of nodes
between our influences. 
23 
00:01:47,530 --> 00:01:53,120 
The distance from one node to a group
of nodes can be defined as a maximum, 
24 
00:01:53,120 --> 00:01:58,100 
or average, or minimum distance,
of the node from the members of the group. 
25 
00:01:59,220 --> 00:02:01,530 
Often, the minimum
distance is a good choice. 
26 
00:02:03,560 --> 00:02:05,400 
So the distance, we could reach, 
27 
00:02:05,400 --> 00:02:10,280 
can then be through of, as the proportion
of all nodes reached by the group, 
28 
00:02:10,280 --> 00:02:12,810 
where the nodes are weighted
by the distance from the set. 
29 
00:02:14,090 --> 00:02:17,630 
And only nodes at distance
1 are given full rate. 
30 
00:02:18,920 --> 00:02:22,270 
Hence, a distance we could reach
that use a maximum value of 1, 
31 
00:02:22,270 --> 00:02:27,730 
where every outside node is adjacent to at
least one member of the set of influences. 
32 
00:02:28,970 --> 00:02:33,340 
In the network shown,
just three nodes, A, C, and D, 
33 
00:02:33,340 --> 00:02:38,340 
are sufficient to reach every other
member within just four steps. 
34 
00:02:40,120 --> 00:02:44,150 
Now this concludes this module, where we
looked at several analytic techniques and 
35 
00:02:44,150 --> 00:02:48,090 
measures to extract different
kinds of insights from a network. 
1 
00:00:04,060 --> 00:00:08,180 
We saw how a community
in a graph can evolve. 
2 
00:00:09,540 --> 00:00:11,940 
To track the nature of evolution, 
3 
00:00:11,940 --> 00:00:15,180 
we need to measure how
the community changes over time. 
4 
00:00:16,430 --> 00:00:18,800 
So here are three cases. 
5 
00:00:18,800 --> 00:00:25,670 
One, two and three of a community
changing between two observation points. 
6 
00:00:25,670 --> 00:00:31,060 
The goal is to figure out whether these
are normal fluctuations in the network or 
7 
00:00:31,060 --> 00:00:32,610 
are more drastic changes occurring. 
8 
00:00:34,040 --> 00:00:34,990 
Look at them for a second. 
9 
00:00:36,400 --> 00:00:41,138 
Just visually, the first case
seems to show just minor changes. 
10 
00:00:41,138 --> 00:00:47,770 
Whereas case two shows a merger,
and case three shows a split. 
11 
00:00:50,280 --> 00:00:54,660 
Now, to come up with a quantitative
measure of change over time, we need to 
12 
00:00:54,660 --> 00:00:59,210 
take two observations from two consecutive
time points and fuse the graph. 
13 
00:01:00,400 --> 00:01:03,910 
If you do it for case one,
you'll find one new node, 
14 
00:01:05,000 --> 00:01:07,870 
one living node, and
the rest will come on over time. 
15 
00:01:09,320 --> 00:01:14,060 
For case two, you will see
that two previous communities, 
16 
00:01:14,060 --> 00:01:19,150 
colored differently, are internally
connected the same way as before. 
17 
00:01:19,150 --> 00:01:23,170 
But some members of the two communities
have created new crosslinks. 
18 
00:01:24,470 --> 00:01:27,520 
Now can you tell me what
you observe in case three? 
19 
00:01:27,520 --> 00:01:31,060 
Well I see one join node, color purple. 
20 
00:01:32,130 --> 00:01:36,470 
Apart from it, there are just two
edges connecting the two groups. 
21 
00:01:37,830 --> 00:01:42,820 
Now with these observations,
we can now compute the autocorrelation 
22 
00:01:42,820 --> 00:01:46,930 
between the graphs across time t and
t plus 1. 
23 
00:01:46,930 --> 00:01:51,770 
This is just a measure of
the number of common nodes 
24 
00:01:51,770 --> 00:01:54,090 
divided by the number of
nodes in the combined graph. 
25 
00:01:55,560 --> 00:01:59,010 
If the community does not change at all,
this number is 1. 
26 
00:01:59,010 --> 00:02:02,860 
If a community has only a few connection,
the number is lower. 
27 
00:02:04,400 --> 00:02:09,360 
After computing autocorrelation
over every pair of time steps, 
28 
00:02:09,360 --> 00:02:11,570 
we can then compute stationarity, 
29 
00:02:12,770 --> 00:02:17,040 
which measures the overall change in
the autocorrelation over a period of time. 
30 
00:02:18,250 --> 00:02:21,300 
So if we measure over 100 time steps, 
31 
00:02:21,300 --> 00:02:27,990 
we will add the 99 correlation values
from the steps and then divide it by 99. 
32 
00:02:27,990 --> 00:02:32,000 
This will tell us what fraction
of members remain unchanged 
33 
00:02:32,000 --> 00:02:34,110 
on an average over these 100 time steps. 
34 
00:02:35,590 --> 00:02:40,380 
Therefore, the 1 minus zeta tells
us the average ratio of members 
35 
00:02:40,380 --> 00:02:41,940 
that are changed in a time step. 
36 
00:02:44,642 --> 00:02:46,060 
Let's take three cases. 
37 
00:02:47,310 --> 00:02:51,010 
In the first plot,
the size of the graph is small and 
38 
00:02:51,010 --> 00:02:52,890 
nothing much is happening here. 
39 
00:02:52,890 --> 00:02:57,180 
A note occasionally joins or leaves,
keeping the stationarity pretty flat. 
40 
00:02:59,420 --> 00:03:05,320 
In the second case, the graph is small,
but there are a lot of changes, especially 
41 
00:03:05,320 --> 00:03:10,710 
at time step seven a whole bunch of purple
nodes have joined and then they went away. 
42 
00:03:11,750 --> 00:03:14,320 
The size of the graph
clearly reflects this 
43 
00:03:14,320 --> 00:03:18,440 
with a purple spike that you see
on the size versus the time graph. 
44 
00:03:21,123 --> 00:03:24,078 
This spike on the time series
by the way is called a burst. 
45 
00:03:24,078 --> 00:03:30,640 
The third plot shows a large graph
with many nodes joining and leaving. 
46 
00:03:31,690 --> 00:03:36,683 
The stationarity of this graph will be
quite low given the abrupt changes we 
47 
00:03:36,683 --> 00:03:38,039 
observe over time. 
1 
00:00:01,320 --> 00:00:05,140 
In this video,
we'll discuss what paths are and 
2 
00:00:05,140 --> 00:00:08,420 
how to find your way as it travels
along the nodes and edges of the graph. 
3 
00:00:10,080 --> 00:00:11,020 
Let's start with an example. 
4 
00:00:12,480 --> 00:00:13,260 
For this example, 
5 
00:00:13,260 --> 00:00:17,740 
we'll consider an edge weighted
graph with no loops as shown here. 
6 
00:00:17,740 --> 00:00:21,180 
Term edge weighted means
the edges have weights. 
7 
00:00:22,520 --> 00:00:26,320 
The weight of the edge I to J is 15. 
8 
00:00:26,320 --> 00:00:30,770 
We can think of this graph as a small road
network, where the nodes are cities, and 
9 
00:00:30,770 --> 00:00:33,190 
the edge weights are highway
distances between them. 
10 
00:00:34,630 --> 00:00:38,470 
To walk, or traverse, through this
graph we'll define what a walk is. 
11 
00:00:39,940 --> 00:00:43,040 
A walk is an arbitrary
sequence of nodes and 
12 
00:00:43,040 --> 00:00:46,860 
edges that starts from some node and
ends on some node. 
13 
00:00:46,860 --> 00:00:52,158 
Here we can go from H to F 
14 
00:00:52,158 --> 00:00:58,935 
to G to C to 
15 
00:00:58,935 --> 00:01:05,750 
F to E to B. 
16 
00:01:05,750 --> 00:01:09,880 
Notice that in this walk,
we went through the node F twice. 
17 
00:01:11,230 --> 00:01:16,280 
In many applications, we do not want
to consider arbitrary walks but 
18 
00:01:16,280 --> 00:01:19,640 
consider a walk where
we do not repeat nodes 
19 
00:01:19,640 --> 00:01:21,510 
unless we need to come back
to the starting point. 
20 
00:01:22,710 --> 00:01:25,640 
Such a constrained walk is called a path. 
21 
00:01:25,640 --> 00:01:28,920 
The green arrows indicate
a path from J to B. 
22 
00:01:30,330 --> 00:01:34,580 
We said on the last slide that a path
can start from a node and end on it. 
23 
00:01:34,580 --> 00:01:39,074 
Such a path is called a cycle when
the path has a three or more nodes. 
24 
00:01:39,074 --> 00:01:43,694 
The J, G, C, 
25 
00:01:43,694 --> 00:01:48,260 
F, J, is a four node cycle,
sometimes called a four cycle. 
26 
00:01:49,620 --> 00:01:52,010 
C, F, E is a three cycle. 
27 
00:01:53,650 --> 00:01:59,030 
However, although there is an edge from
F to J and another from J back to F, 
28 
00:01:59,030 --> 00:02:03,590 
this two node path is not
a cycle by our definition. 
29 
00:02:04,980 --> 00:02:08,680 
A network with no cycles
is called acyclic. 
30 
00:02:10,440 --> 00:02:13,110 
A graph that is both directed and 
31 
00:02:13,110 --> 00:02:18,600 
acyclic is called a directed
acyclic graph, in short, a deck. 
32 
00:02:19,780 --> 00:02:26,470 
A trail is a concept similar to a path,
it is a walk with no repeating edges. 
33 
00:02:27,500 --> 00:02:33,211 
In the graph shown in the walk,
H, F, G, C, F, E, 
34 
00:02:33,211 --> 00:02:38,500 
C, F, J, is not a trail. 
35 
00:02:38,500 --> 00:02:41,470 
Because C, F Is traversed twice. 
36 
00:02:43,130 --> 00:02:46,050 
Remember the seven bridges of
the Konigsberg problem that we described 
37 
00:02:46,050 --> 00:02:47,580 
in module one? 
38 
00:02:47,580 --> 00:02:48,840 
In that problem, 
39 
00:02:48,840 --> 00:02:52,260 
we had the constraint that one
cannot cross the same bridge twice. 
40 
00:02:54,200 --> 00:02:57,090 
Often we see such constraints
in path planning problems. 
41 
00:02:58,510 --> 00:03:02,190 
A common notion in directive
graphs is called reachability. 
42 
00:03:03,710 --> 00:03:08,280 
If there is a path from node U to node V,
V is reachable from U. 
43 
00:03:09,460 --> 00:03:10,770 
Reachability is not symmetric. 
44 
00:03:11,860 --> 00:03:15,800 
Even if V is reachable from U,
U may or may not be reachable from V. 
45 
00:03:16,960 --> 00:03:21,220 
As we can see in this graph,
I is not reachable from A. 
46 
00:03:23,030 --> 00:03:25,300 
Look at A more carefully. 
47 
00:03:25,300 --> 00:03:27,560 
There is no outgoing edge from A at all. 
48 
00:03:28,570 --> 00:03:31,590 
So no node in this graph
is reachable from A. 
49 
00:03:32,770 --> 00:03:35,790 
A is a terminal, a leaf node of the graph. 
50 
00:03:37,300 --> 00:03:39,480 
Where do we see this in real life? 
51 
00:03:39,480 --> 00:03:43,040 
Think of road network with one way streets
and road blocks through the construction. 
52 
00:03:44,040 --> 00:03:46,610 
This can make some areas
unreachable by cars. 
53 
00:03:48,370 --> 00:03:51,810 
In biology,
there are gene regulation networks, 
54 
00:03:51,810 --> 00:03:53,930 
in which the nodes represent genes and 
55 
00:03:53,930 --> 00:03:58,479 
edges represent the fact that gene
A regulates gene B and not visa verse. 
56 
00:03:59,530 --> 00:04:00,940 
This makes it a directed graph. 
57 
00:04:02,090 --> 00:04:06,930 
We can easily think of a case where
A regulates B, and B regulates C, but 
58 
00:04:06,930 --> 00:04:09,270 
C does not regulate B or A. 
59 
00:04:09,270 --> 00:04:12,800 
That's making A unreachable from C. 
60 
00:04:12,800 --> 00:04:15,050 
The final concept you will
explore in this graph, 
61 
00:04:15,050 --> 00:04:19,220 
in this lecture,
is that of a graph diameter. 
62 
00:04:19,220 --> 00:04:24,950 
The diameter of a graph measures the
maximum number of steps, also called hops, 
63 
00:04:24,950 --> 00:04:29,320 
want us to traverse to go to
the most distant node in the graph. 
64 
00:04:30,580 --> 00:04:33,630 
That means,
if you go from an arbitrary node 
65 
00:04:33,630 --> 00:04:39,160 
to any other arbitrary node in the graph,
following only the shortest paths roots, 
66 
00:04:39,160 --> 00:04:41,790 
what is the maximum number
of steps you have to take? 
67 
00:04:43,000 --> 00:04:44,520 
Let's see how this is computed. 
68 
00:04:45,670 --> 00:04:50,350 
For this task, we'll create a matrix
called the shortest hop distance matrix. 
69 
00:04:51,790 --> 00:04:57,060 
Just like the adjacency matrix, the rows
and columns here represent graph nodes. 
70 
00:04:58,070 --> 00:05:01,930 
And each cell contains the distance
from the I eighth node, 
71 
00:05:01,930 --> 00:05:03,780 
to the G eighth node
via the shortest path. 
72 
00:05:05,000 --> 00:05:09,700 
The distance from any
node to itself is zero. 
73 
00:05:09,700 --> 00:05:12,870 
So all diagonal elements
of the matrix are zero. 
74 
00:05:14,570 --> 00:05:18,440 
If a node I cannot reach
node J through any path, 
75 
00:05:18,440 --> 00:05:21,200 
the distance is marked as infinity. 
76 
00:05:21,200 --> 00:05:25,600 
Thus, the distance from
B to C is infinity. 
77 
00:05:25,600 --> 00:05:30,203 
You can go from E to F in two steps,
E to C to F. 
78 
00:05:30,203 --> 00:05:36,670 
Therefore, the wait at the C cell is two. 
79 
00:05:36,670 --> 00:05:41,830 
If you fill this matrix, you will
notice that the largest reachable value 
80 
00:05:41,830 --> 00:05:44,610 
is 4 in doing the infinite distance. 
81 
00:05:45,840 --> 00:05:50,463 
Thus, the diameter of this graph is
4 where It represents the part from I 
82 
00:05:50,463 --> 00:05:55,020 
to A, through J, G and D. 
83 
00:05:55,020 --> 00:05:59,000 
There are a few noteworthy
items in this Distance Matrix. 
84 
00:05:59,000 --> 00:06:05,310 
The row of A has a value of 0 with itself
and infinity for every other node. 
85 
00:06:05,310 --> 00:06:08,230 
This happens because A is a leaf node. 
86 
00:06:09,740 --> 00:06:13,040 
Similarly, except for
the 0 distance with itself, 
87 
00:06:13,040 --> 00:06:17,810 
the H column has infinity for
all of the nodes. 
88 
00:06:17,810 --> 00:06:21,984 
This happens because H
has no incoming edges and 
89 
00:06:21,984 --> 00:06:25,226 
therefore no other node can reach H. 
1 
00:00:02,450 --> 00:00:06,090 
So we have seen how to compute
degree histograms of a graph. 
2 
00:00:07,760 --> 00:00:11,100 
While degree histograms are useful
to characterize a graph, 
3 
00:00:11,100 --> 00:00:13,110 
it is usually a means to an end. 
4 
00:00:14,130 --> 00:00:18,400 
It's a known practice in statistics to
compute a mathematical expression for 
5 
00:00:18,400 --> 00:00:22,390 
a statistical distribution
Using histograms. 
6 
00:00:23,910 --> 00:00:28,600 
For graphs, we often look for a function
to describe the degree distribution. 
7 
00:00:29,620 --> 00:00:33,510 
But this is expressed as
a distribution of the probability 
8 
00:00:33,510 --> 00:00:38,090 
that a random vertex will
have exactly k neighbors. 
9 
00:00:38,090 --> 00:00:40,669 
Now, this problem has been
investigated by many. 
10 
00:00:41,780 --> 00:00:45,300 
One popular, well known model,
is a Power Law. 
11 
00:00:46,770 --> 00:00:52,910 
A graph follows a Power Law if,
the best probability is given by k, 
12 
00:00:52,910 --> 00:00:55,380 
erased to a negative
exponent called alpha. 
13 
00:00:56,890 --> 00:01:01,660 
The value of alpha, for many practical
networks, is between two and three. 
14 
00:01:03,240 --> 00:01:04,310 
More recently, 
15 
00:01:04,310 --> 00:01:07,750 
people have suggested other distributions
that look like the power law graph. 
16 
00:01:08,880 --> 00:01:12,611 
One of them is a log-normal graph. 
17 
00:01:12,611 --> 00:01:16,148 
Here, the logarithm of k has
a Gaussian distribution. 
18 
00:01:16,148 --> 00:01:20,630 
So this log-normal distribution
seems to have the better fit to 
19 
00:01:20,630 --> 00:01:23,120 
natural graphs that are observed. 
20 
00:01:25,200 --> 00:01:27,280 
So, why are power law graphs important? 
21 
00:01:28,930 --> 00:01:31,290 
Interestingly, many very, 
22 
00:01:31,290 --> 00:01:34,810 
very different real life graphs in
the world seem to follow the power law. 
23 
00:01:36,030 --> 00:01:41,860 
If a graph does follow the power law, it
would have one large connected component 
24 
00:01:41,860 --> 00:01:49,030 
with a very high proportion of notes
connected to it In a power law graph, 
25 
00:01:50,060 --> 00:01:54,036 
most nodes have a low degree and
some nodes will be disconnected. 
26 
00:01:54,036 --> 00:01:59,260 
The low degree nodes belong to
a very dense sub graphs and 
27 
00:01:59,260 --> 00:02:01,600 
those sub graphs are connected
to each other through hubs. 
28 
00:02:03,150 --> 00:02:06,700 
In the center of the graph
has a high density, 
29 
00:02:06,700 --> 00:02:09,690 
power law graphs can be
difficult to compute with. 
30 
00:02:09,690 --> 00:02:13,907 
For example, the shortest path
algorithm operating in the dense part 
31 
00:02:13,907 --> 00:02:18,695 
will possibly be very inefficient, not
because of the size of the network, but 
32 
00:02:18,695 --> 00:02:22,928 
because there are too many paths to
explore inside the denser pieces. 
33 
00:02:27,090 --> 00:02:31,574 
The more interesting reason why
people study power-law graphs 
34 
00:02:31,574 --> 00:02:35,570 
is because power-law graphs
are supposed to be robust. 
35 
00:02:36,840 --> 00:02:42,329 
So in nature, all biological networks
show power-law graphs because 
36 
00:02:42,329 --> 00:02:47,552 
It gives you a high rate of redundancy
against failure and attacks. 
1 
00:00:00,760 --> 00:00:05,528 
The most primitive path analytics
question one can ask is to find the best 
2 
00:00:05,528 --> 00:00:07,570 
path from node one to node two. 
3 
00:00:08,980 --> 00:00:11,220 
What does best mean? 
4 
00:00:11,220 --> 00:00:13,621 
Well, that depends on the actual
needs of the application. 
5 
00:00:13,621 --> 00:00:16,975 
But in general, to specify the best path, 
6 
00:00:16,975 --> 00:00:21,274 
we need to define when one
path is better than another. 
7 
00:00:21,274 --> 00:00:28,010 
This is usually expressed in
terms of an optimization problem. 
8 
00:00:28,010 --> 00:00:32,790 
Where we need to minimize and maximize
our subfunction subject to constraints. 
9 
00:00:32,790 --> 00:00:34,420 
What kind of constraints? 
10 
00:00:34,420 --> 00:00:39,280 
Two common criteria for graphs are,
inclusion and exclusion conditions. 
11 
00:00:40,310 --> 00:00:46,860 
Inclusion criteria may specify which
nodes we have to include in the path. 
12 
00:00:46,860 --> 00:00:50,050 
And exclusion criteria
specifies which nodes and 
13 
00:00:50,050 --> 00:00:52,440 
edges should be excluded from the path. 
14 
00:00:52,440 --> 00:00:57,390 
In addition,
one specify a preference criteria 
15 
00:00:57,390 --> 00:01:01,030 
that act as a softer or
less strict constraint. 
16 
00:01:01,030 --> 00:01:04,880 
For example, we would like to
minimize highways on my trip. 
17 
00:01:04,880 --> 00:01:05,900 
Or avoid condition. 
18 
00:01:06,980 --> 00:01:11,179 
These are soft because although
the users would like to have them 
19 
00:01:11,179 --> 00:01:15,779 
enforced completely, it is all right
if they are not fully enforced. 
20 
00:01:15,779 --> 00:01:20,362 
A good practical use case occurs when I'm
trying to drive to work in the morning. 
21 
00:01:20,362 --> 00:01:25,284 
Ideally, I would like to take a path
having the shortest distance from my home. 
22 
00:01:25,284 --> 00:01:30,620 
For example, node I, to my workplace,
which is node B in the graph. 
23 
00:01:31,850 --> 00:01:35,680 
But I have to drop off my son at school. 
24 
00:01:35,680 --> 00:01:39,210 
So my path must include his school,
the J here. 
25 
00:01:41,130 --> 00:01:45,150 
However, I would like to avoid roads
around the new construction that's 
26 
00:01:45,150 --> 00:01:49,580 
happening about five miles from my
workplace, like the node E in the graph. 
27 
00:01:51,140 --> 00:01:55,260 
Because there is usually a huge traffic
delay around that construction site. 
28 
00:01:56,620 --> 00:01:59,210 
I could also add
a preference criteria like 
29 
00:01:59,210 --> 00:02:01,290 
I don't prefer to drive on the highway. 
30 
00:02:01,290 --> 00:02:03,982 
But for this discussion,
we'll skip the preference idea. 
31 
00:02:03,982 --> 00:02:06,059 
Too complicated? 
32 
00:02:06,059 --> 00:02:10,900 
Okay, let's start with a simpler problem. 
33 
00:02:10,900 --> 00:02:15,895 
To start with, let's drop the constraints
and look at the problem with just 
34 
00:02:15,895 --> 00:02:20,280 
the optimization part of the problem,
having a single variable. 
35 
00:02:22,150 --> 00:02:27,115 
In our case, that variable is the sum
of edge weights from the source, 
36 
00:02:27,115 --> 00:02:31,086 
that is the starting node I,
to the target, which is B. 
37 
00:02:33,095 --> 00:02:38,000 
This problem is handled by all mapping and
road direction software. 
38 
00:02:38,000 --> 00:02:41,820 
Here is a Google map screenshot,
in which I am trying to go from my home 
39 
00:02:41,820 --> 00:02:45,450 
in North San Diego,
to a collision center in a nearby city. 
40 
00:02:47,030 --> 00:02:50,760 
Google Maps shows three different routes,
and 
41 
00:02:50,760 --> 00:02:53,040 
highlights one as a preferred solution. 
42 
00:02:54,876 --> 00:03:00,170 
You should readily see that the real
shortest path of 26.6 miles 
43 
00:03:00,170 --> 00:03:04,000 
will take the longest time at the time of
the day when I was looking at the map. 
44 
00:03:05,080 --> 00:03:10,980 
So this means the weights here are not
raw distances but estimated travel time. 
45 
00:03:10,980 --> 00:03:13,574 
You should also notice the blue, red, and 
46 
00:03:13,574 --> 00:03:17,482 
orange segments in the preferred
path are presented by Google. 
47 
00:03:17,482 --> 00:03:22,731 
The orange and red street segments
clearly represent congestion areas and 
48 
00:03:22,731 --> 00:03:26,570 
therefore have higher weight
than the blue segments. 
49 
00:03:27,840 --> 00:03:32,070 
Therefore, the weights of the street
segments are not really static but 
50 
00:03:32,070 --> 00:03:35,370 
change with many other factors,
like weather or the time of the day. 
51 
00:03:36,510 --> 00:03:41,360 
This is why the least weight path problem
is an important problem to solve for 
52 
00:03:41,360 --> 00:03:42,680 
the benefit of the commuter. 
53 
00:03:43,710 --> 00:03:46,960 
A widely applied algorithm
that is applied for 
54 
00:03:46,960 --> 00:03:49,990 
shortest path problems is
called Dijkstra's algorithm. 
55 
00:03:51,360 --> 00:03:56,280 
Originally, Dijkstra considered a variant
of the problem where the task is 
56 
00:03:56,280 --> 00:04:00,580 
to find the shortest path from a single
source node to all other nodes. 
57 
00:04:02,230 --> 00:04:04,110 
We'll go through the algorithm here. 
58 
00:04:04,110 --> 00:04:08,290 
However, there are many good online
resources including tutorials and 
59 
00:04:08,290 --> 00:04:10,230 
YouTube videos describing the algorithm. 
60 
00:04:11,570 --> 00:04:15,284 
For our discussion, we'll confine
ourselves to the case where both 
61 
00:04:15,284 --> 00:04:18,175 
the source and
the target nodes are known in advance. 
1 
00:00:00,840 --> 00:00:06,150 
As you probably already know, Cypher is
the scripting language used in Neo4j and 
2 
00:00:06,150 --> 00:00:10,120 
it's what we have been using
already in the previous lectures. 
3 
00:00:10,120 --> 00:00:15,160 
In this lecture, we're going to go through
a series of basic queries using Cypher 
4 
00:00:15,160 --> 00:00:18,900 
with the focus on the data sets
that we've already been using. 
5 
00:00:18,900 --> 00:00:22,787 
Here's a listing of the basic queries
we will go through step by step. 
6 
00:00:31,424 --> 00:00:32,341 
So let's get started. 
7 
00:00:36,764 --> 00:00:42,167 
To keep things simple we will be using a
text file containing the basic queries and 
8 
00:00:42,167 --> 00:00:47,930 
we'll make this file available for
download as a reading in the module. 
9 
00:00:47,930 --> 00:00:51,130 
I will briefly review the queries and 
10 
00:00:51,130 --> 00:00:56,255 
then we'll toggle to the browser to view
the results of the queries in Neo4j. 
11 
00:00:57,680 --> 00:01:03,120 
Neo4j has a very nice feature which
allows us to retain individual queries 
12 
00:01:03,120 --> 00:01:09,620 
in multiple panels as we're going through
the process of exploring each query. 
13 
00:01:09,620 --> 00:01:13,470 
So for the first few queries we'll be
using the simple road network data set 
14 
00:01:13,470 --> 00:01:16,250 
that we used in previous demonstrations. 
15 
00:01:16,250 --> 00:01:18,800 
And here we've already loaded
the data set into Neo4j. 
16 
00:01:19,910 --> 00:01:21,185 
So let's look at our first query. 
17 
00:01:21,185 --> 00:01:23,530 
Our first query is a very a simple one, 
18 
00:01:23,530 --> 00:01:27,280 
in which we're counting the number
of nodes in the network. 
19 
00:01:27,280 --> 00:01:31,770 
So the first line of code simply matches
all of the nodes with the label MyNode and 
20 
00:01:31,770 --> 00:01:34,200 
it returns a count of those nodes. 
21 
00:01:34,200 --> 00:01:36,120 
So let's look at the results. 
22 
00:01:36,120 --> 00:01:40,100 
The results are very simple,
the value of eleven. 
23 
00:01:40,100 --> 00:01:43,520 
And we can visually confirm this
by inspecting the graph itself and 
24 
00:01:43,520 --> 00:01:46,140 
see it has 11 nodes and
we can see that up here. 
25 
00:01:49,510 --> 00:01:54,540 
The next query is almost as simple,
we want to count the number of edges. 
26 
00:01:54,540 --> 00:01:59,290 
Now, first thing to keep in mind
is in order to count edges, 
27 
00:01:59,290 --> 00:02:03,970 
we also need to declare nodes that
are associated with those edges. 
28 
00:02:03,970 --> 00:02:07,070 
So this first line of codes
includes a declaration 
29 
00:02:07,070 --> 00:02:09,840 
of the nodes associated with the edges. 
30 
00:02:09,840 --> 00:02:12,231 
Here we're identifying
with thevariable r and 
31 
00:02:12,231 --> 00:02:14,570 
then we're returning
a count of those edges. 
32 
00:02:15,580 --> 00:02:20,850 
So let's take a look in Neo4j and the
results of this query are a value of 14. 
33 
00:02:20,850 --> 00:02:24,938 
And once again we can confirm this
visually by looking at our network and 
34 
00:02:24,938 --> 00:02:26,720 
counting the number of edges. 
35 
00:02:29,749 --> 00:02:35,290 
The next query involves finding all
of the leaf nodes in the network. 
36 
00:02:35,290 --> 00:02:37,535 
Now as you may remember
from a previous lecture, 
37 
00:02:37,535 --> 00:02:42,810 
leaf nodes are defined as those
nodes which have no outgoing edges. 
38 
00:02:44,400 --> 00:02:48,300 
Notice that we're returning the node
represented by the variable M, 
39 
00:02:48,300 --> 00:02:50,950 
which is the target node in this query. 
40 
00:02:50,950 --> 00:02:55,750 
So here we're matching all nodes
associated with edges having the label two 
41 
00:02:56,780 --> 00:02:59,000 
and we want to place
a constraint on those nodes, 
42 
00:02:59,000 --> 00:03:02,180 
such that they have no outgoing edges. 
43 
00:03:02,180 --> 00:03:03,930 
And then we return all those nodes. 
44 
00:03:04,950 --> 00:03:07,170 
So let's take a look at the results. 
45 
00:03:07,170 --> 00:03:10,380 
We're going to see a single node returned,
the node with the label p. 
46 
00:03:12,020 --> 00:03:17,490 
And if we inspect our graph, we can
see over here on the right the node p, 
47 
00:03:17,490 --> 00:03:20,630 
and sure enough it has no outgoing edges,
only one incoming edge. 
48 
00:03:24,290 --> 00:03:26,170 
The next query is also similar, 
49 
00:03:26,170 --> 00:03:29,390 
we're looking for
root nodes instead of leaf notes. 
50 
00:03:29,390 --> 00:03:30,650 
And as you may also remember, 
51 
00:03:30,650 --> 00:03:35,180 
root nodes are defined as a node
which has no incoming edges. 
52 
00:03:36,230 --> 00:03:40,550 
You may notice that this segment of
the first line of code is sort of a mirror 
53 
00:03:40,550 --> 00:03:46,040 
image of the same segment in the first
line of code from the previous query. 
54 
00:03:46,040 --> 00:03:50,540 
We also want to place the constraint
on the nodes that we want to return 
55 
00:03:50,540 --> 00:03:54,150 
by specifying that they can
have no incoming edges. 
56 
00:03:54,150 --> 00:03:58,340 
And then, we return all of those nodes
When we look at our results in Neo4j. 
57 
00:03:58,340 --> 00:04:00,420 
Once again, 
58 
00:04:00,420 --> 00:04:04,270 
we see we only get a single node
return it's the node with the label H. 
59 
00:04:05,500 --> 00:04:09,640 
And if we inspect our graph,
we see over here the node H and 
60 
00:04:09,640 --> 00:04:11,480 
sure enough it has no incoming edges. 
61 
00:04:11,480 --> 00:04:15,382 
Only a single outgoing edge which
means it's a root node of the network. 
62 
00:04:18,388 --> 00:04:23,740 
The next query can be described
as a pattern matching query. 
63 
00:04:23,740 --> 00:04:26,990 
We're looking for a pattern that
we're describing as triangles. 
64 
00:04:26,990 --> 00:04:29,880 
This is a pattern that's
a bit more complex than 
65 
00:04:29,880 --> 00:04:33,540 
the patterns we've been looking for
in our previous queries. 
66 
00:04:33,540 --> 00:04:38,750 
A triangle can also be described as a
three cycle, consisting of three nodes and 
67 
00:04:38,750 --> 00:04:43,010 
three edges where the beginning and
end node are the same. 
68 
00:04:43,010 --> 00:04:47,420 
So here we're matching a node A which
goes through an edge to node B 
69 
00:04:47,420 --> 00:04:51,480 
which goes through a second
edge to a second node C and 
70 
00:04:51,480 --> 00:04:54,960 
through a third edge back
to the original node A. 
71 
00:04:54,960 --> 00:04:57,500 
And then we return all of those notes. 
72 
00:04:57,500 --> 00:04:59,600 
Let's look at our results. 
73 
00:04:59,600 --> 00:05:03,390 
And here we see we have five
distinct nodes returned. 
74 
00:05:03,390 --> 00:05:06,720 
And we have two triangles or
two three cycles. 
75 
00:05:06,720 --> 00:05:10,804 
From D to E to G and from D to C to B. 
76 
00:05:13,868 --> 00:05:17,788 
Finally the last query we're going
to execute with this data set, 
77 
00:05:17,788 --> 00:05:21,290 
is going to explore the neighborhood
of a particular node. 
78 
00:05:21,290 --> 00:05:23,220 
In this case, the node with the label d. 
79 
00:05:23,220 --> 00:05:27,290 
And we're going to be looking for
what we're calling second neighbors of D. 
80 
00:05:27,290 --> 00:05:31,480 
This means nodes that
are two nodes away from d. 
81 
00:05:31,480 --> 00:05:36,230 
So the first line of code matches
all nodes that are two nodes away 
82 
00:05:36,230 --> 00:05:38,130 
from a specific node. 
83 
00:05:38,130 --> 00:05:43,150 
And then the second line of codes
specifies the actual node that we 
84 
00:05:43,150 --> 00:05:47,240 
we want to consider by constraining
its name to have the label d, 
85 
00:05:47,240 --> 00:05:50,240 
and then we return those nodes. 
86 
00:05:50,240 --> 00:05:53,040 
And here we're using the command distinct 
87 
00:05:53,040 --> 00:05:56,440 
because we want to make sure that we
don't return any duplicate nodes. 
88 
00:05:56,440 --> 00:05:58,650 
All of our nodes must be unique. 
89 
00:05:59,760 --> 00:06:00,930 
So let's look at the results. 
90 
00:06:02,130 --> 00:06:07,040 
And here we have a network that
consists of nine nodes and 11 edges and 
91 
00:06:07,040 --> 00:06:13,710 
we can see that each node is
two nodes away from the node D. 
92 
00:06:13,710 --> 00:06:17,510 
Some nodes appear to be only one
node away from the node D but 
93 
00:06:17,510 --> 00:06:21,890 
we can get to those nodes indirectly
through another node, which means that 
94 
00:06:21,890 --> 00:06:24,660 
they're not only a first neighbor but
they're also a second neighbor. 
95 
00:06:26,390 --> 00:06:28,930 
So we encourage you to play
around with these queries, 
96 
00:06:28,930 --> 00:06:32,320 
and make minor changes with them,
and see what the results might be. 
1 
00:00:01,190 --> 00:00:06,030 
For our next three queries,
we're going to switch to the terrorist 
2 
00:00:06,030 --> 00:00:08,820 
data set that we had used in
our previous demonstration. 
3 
00:00:10,740 --> 00:00:15,480 
Our first query involves finding
the types of a particular node. 
4 
00:00:15,480 --> 00:00:19,740 
So, as you may recall in the terrorist
data set there were node types 
5 
00:00:19,740 --> 00:00:22,010 
corresponding to a country. 
6 
00:00:22,010 --> 00:00:25,240 
So, in this case we
want to match all nodes, 
7 
00:00:25,240 --> 00:00:28,420 
where the name is equal to Afghanistan. 
8 
00:00:28,420 --> 00:00:33,450 
And we will return the labels for
that particular match. 
9 
00:00:33,450 --> 00:00:35,990 
And the results for
this query are relatively simple. 
10 
00:00:35,990 --> 00:00:41,450 
The label for the node named Afghanistan
is country, as you might expect. 
11 
00:00:44,685 --> 00:00:48,650 
Next, we're going to do something
similar but with an edge. 
12 
00:00:48,650 --> 00:00:51,740 
In this case, we want to find the label 
13 
00:00:51,740 --> 00:00:56,690 
of a edge associated with
a node named Afghanistan. 
14 
00:00:56,690 --> 00:01:01,230 
And also, if you can recall from that
particular network, the label for 
15 
00:01:01,230 --> 00:01:08,038 
an edge associated with a country named
Afghanistan would be an IS_FROM label. 
16 
00:01:11,218 --> 00:01:16,330 
And the final query, we would like to
demonstrate with this terrorist data set, 
17 
00:01:16,330 --> 00:01:19,500 
involves finding all of
the properties of a node. 
18 
00:01:20,590 --> 00:01:24,520 
Now, based on how we defined
our data import script, 
19 
00:01:25,540 --> 00:01:29,370 
we defined all of our
nodes to be of type Actor. 
20 
00:01:29,370 --> 00:01:33,430 
So in this query, we're going to
search for all nodes of type Actor and 
21 
00:01:33,430 --> 00:01:38,700 
then, we're going to return all of the
properties associated with those nodes. 
22 
00:01:38,700 --> 00:01:39,990 
But, since there are thousands of nodes, 
23 
00:01:39,990 --> 00:01:42,630 
we're going to limit
the results to the first 20. 
24 
00:01:42,630 --> 00:01:45,920 
So, let's go ahead and submit our query. 
25 
00:01:46,970 --> 00:01:48,980 
And here are the results. 
26 
00:01:48,980 --> 00:01:54,750 
Each node represents a terrorist and
if we look at the rows data 
27 
00:01:54,750 --> 00:02:00,030 
behind the scenes we can see the various
properties associated with each node. 
28 
00:02:00,030 --> 00:02:01,920 
Each node has a name. 
29 
00:02:01,920 --> 00:02:05,330 
Each node has aliases,
one or more aliases. 
30 
00:02:05,330 --> 00:02:07,960 
And they have a type property. 
31 
00:02:07,960 --> 00:02:11,090 
Now, depending on how we defined our
import script in the first place, 
32 
00:02:11,090 --> 00:02:14,820 
these nodes could have
different types of properties. 
33 
00:02:14,820 --> 00:02:17,950 
In this case, we only defined them
to have three different properties. 
34 
00:02:22,091 --> 00:02:26,460 
For our next two queries,
we're going to use a different data set, 
35 
00:02:26,460 --> 00:02:31,149 
it's a biological data set consisting
of genetics data representing 
36 
00:02:31,149 --> 00:02:33,960 
interactions between genes. 
37 
00:02:33,960 --> 00:02:36,690 
We will be providing the data for
you to download. 
38 
00:02:36,690 --> 00:02:39,280 
In fact, there are two separate data sets. 
39 
00:02:39,280 --> 00:02:44,150 
One is a complete data set
consisting of 250,000 rows, 
40 
00:02:44,150 --> 00:02:48,425 
and the other data set consists
of the first 50,000 rows, 
41 
00:02:48,425 --> 00:02:51,780 
and that's what we'll be using for
this demonstration. 
42 
00:02:51,780 --> 00:02:55,060 
So, let's take a look at
a small sample of the data. 
43 
00:02:55,060 --> 00:03:00,390 
Here you see a graph network
in which each node is a gene. 
44 
00:03:00,390 --> 00:03:03,540 
And each edge represents
the association between genes. 
45 
00:03:05,200 --> 00:03:10,248 
To load this data set, this is the smaller 
46 
00:03:10,248 --> 00:03:15,152 
data set, required 1446 seconds, 
47 
00:03:15,152 --> 00:03:19,768 
and it consists of 9656 labels and 
48 
00:03:19,768 --> 00:03:23,388 
46621 relationships. 
49 
00:03:23,388 --> 00:03:28,133 
So, the first query involves finding
loops in the data which represent genes 
50 
00:03:28,133 --> 00:03:30,980 
that have associations
with there own types. 
51 
00:03:32,200 --> 00:03:35,257 
So, it's a very simple query
in which the source node and 
52 
00:03:35,257 --> 00:03:39,335 
the target node are the same and we'll
be returning those along the edges and 
53 
00:03:39,335 --> 00:03:41,644 
we'll limit our results to the first ten. 
54 
00:03:41,644 --> 00:03:43,920 
So, here are the results of our query. 
55 
00:03:43,920 --> 00:03:45,840 
We can see a few different loops. 
56 
00:03:46,870 --> 00:03:52,600 
If we look at the rows, our query returned
not only the node but the edge type. 
57 
00:03:53,610 --> 00:03:57,234 
So, on the left we see the particular
gene and on the right, 
58 
00:03:57,234 --> 00:03:59,201 
we see the type of association. 
59 
00:03:59,201 --> 00:04:01,616 
And there are a range of different types. 
60 
00:04:05,749 --> 00:04:08,560 
So, this data set also
contains multigraphs. 
61 
00:04:08,560 --> 00:04:12,520 
If you recall from a previous lecture,
the definition of a multigraph 
62 
00:04:12,520 --> 00:04:17,370 
is any two nodes which have two or
more edges between them. 
63 
00:04:17,370 --> 00:04:22,370 
So in this case, we'll be matching
two separate node edge relationships. 
64 
00:04:22,370 --> 00:04:27,190 
And we'll apply a constraint in which
the edges must be different for 
65 
00:04:27,190 --> 00:04:28,940 
the same pairs of nodes. 
66 
00:04:28,940 --> 00:04:32,490 
And then we will return those nodes and
those edges. 
67 
00:04:32,490 --> 00:04:34,610 
We'll limit our results to the first ten. 
68 
00:04:34,610 --> 00:04:36,120 
And here's the results. 
69 
00:04:36,120 --> 00:04:41,377 
Here we see a set of four genes, and
there are two pairs that have three 
70 
00:04:41,377 --> 00:04:46,738 
edges between them and another pair
that has four edges between them. 
71 
00:04:50,360 --> 00:04:54,189 
Our final query addresses something that
is not necessarily been fully covered 
72 
00:04:54,189 --> 00:04:57,329 
in previous lectures, but
it is useful enough to address here and 
73 
00:04:57,329 --> 00:05:00,835 
you'll get an understanding of it by
going through this query example. 
74 
00:05:02,285 --> 00:05:06,855 
We're going to essentially
extract a subset of nodes and 
75 
00:05:06,855 --> 00:05:10,205 
edges from the graph that
we've been working with. 
76 
00:05:10,205 --> 00:05:12,445 
And this is called an induced subgraph. 
77 
00:05:13,470 --> 00:05:18,330 
In which, if we provide a set of nodes,
we want to return 
78 
00:05:18,330 --> 00:05:22,970 
the network that consists only of those
nodes and their associated edges. 
79 
00:05:22,970 --> 00:05:28,000 
So, my first line of code is very
familiar, we're matching nodes and 
80 
00:05:28,000 --> 00:05:30,910 
edges, it's a very basic node and
edge match. 
81 
00:05:32,570 --> 00:05:36,660 
The second line of code is where
the constraints are explicitly stated. 
82 
00:05:36,660 --> 00:05:41,540 
A resulting network must consist
of edges in which the source node 
83 
00:05:41,540 --> 00:05:46,520 
must be constrained to this
subset of node labels. 
84 
00:05:46,520 --> 00:05:51,180 
And the target node must also be
constrained to be a part of the subset of 
85 
00:05:51,180 --> 00:05:53,290 
nodes with just these labels. 
86 
00:05:53,290 --> 00:05:55,410 
And then we return the nodes and
the edges. 
87 
00:05:56,890 --> 00:05:59,010 
So, let's see the resulting graph. 
88 
00:05:59,010 --> 00:06:00,050 
And here it is. 
89 
00:06:00,050 --> 00:06:01,040 
So, as we would expect, 
90 
00:06:01,040 --> 00:06:05,650 
we're seeing the five nodes that were
defined in our query constraints. 
91 
00:06:05,650 --> 00:06:09,850 
And the edges corresponding to them,
retain the network structure. 
92 
00:06:09,850 --> 00:06:13,902 
So, this concludes our review of
some of the basic queries you can do 
93 
00:06:13,902 --> 00:06:16,974 
with Neo4j using the cipher
scripting language. 
1 
00:00:01,130 --> 00:00:04,808 
Next, we will talk about
connectivity analytics with Cypher. 
2 
00:00:04,808 --> 00:00:06,713 
If you remember in module two, 
3 
00:00:06,713 --> 00:00:11,810 
we talked about connectivity analytics
in terms of network robustness. 
4 
00:00:11,810 --> 00:00:15,590 
In other words, a measure of how resistant
a graph network is to being disconnected. 
5 
00:00:16,820 --> 00:00:19,720 
Specifically, we used
two kinds of methods. 
6 
00:00:19,720 --> 00:00:24,790 
One computed the eigenvalues, and the
second computed the degree distribution. 
7 
00:00:24,790 --> 00:00:29,150 
For these examples, we're going to use
the second one, degree distributions. 
8 
00:00:29,150 --> 00:00:32,710 
And we will use the same graph
network we've used previously. 
9 
00:00:32,710 --> 00:00:35,280 
A simple graph representing
a road network. 
10 
00:00:36,280 --> 00:00:40,534 
And here's a listing of the query examples
we're going to be applying to our network. 
11 
00:00:57,639 --> 00:01:03,080 
My first query example finds all
of the outdegrees of all nodes. 
12 
00:01:03,080 --> 00:01:07,050 
Now, if you'll notice, this query consists
of two parts, because there's a specific 
13 
00:01:07,050 --> 00:01:12,940 
type of node, a leaf node, which does not
conform to this particular constraint. 
14 
00:01:12,940 --> 00:01:16,620 
So our first match statement finds
all nodes with outgoing edges, 
15 
00:01:16,620 --> 00:01:19,510 
as you can see here,
this is a directed edge. 
16 
00:01:19,510 --> 00:01:21,930 
And then,
we returns the names of the nodes and 
17 
00:01:21,930 --> 00:01:24,870 
the count as the variable outdegree. 
18 
00:01:24,870 --> 00:01:27,610 
And for convenience,
we order by outdegree. 
19 
00:01:27,610 --> 00:01:32,820 
And we need to combine that with
a specific query dealing with leaf nodes. 
20 
00:01:32,820 --> 00:01:36,820 
We're familiar with how to
do that from past examples. 
21 
00:01:36,820 --> 00:01:41,220 
And so, we'll match all leaf nodes and
return the name and 
22 
00:01:41,220 --> 00:01:43,950 
the value zero for its outdegree. 
23 
00:01:43,950 --> 00:01:48,460 
So when we submit this query we
get this listing right here. 
24 
00:01:48,460 --> 00:01:52,792 
The node P has 0 for its outdegree and
all of the other nodes are as we might 
25 
00:01:52,792 --> 00:01:56,286 
expect and they're ordered
by their value of outdegree. 
26 
00:02:00,118 --> 00:02:02,695 
Our next query finds
the indegree of all nodes, 
27 
00:02:02,695 --> 00:02:05,850 
which is very similar to
our previous example. 
28 
00:02:05,850 --> 00:02:07,360 
But, in this case, as you might expect, 
29 
00:02:07,360 --> 00:02:12,240 
we're going to take into account
root nodes instead of leaf nodes. 
30 
00:02:12,240 --> 00:02:15,910 
And so, our match involves incoming edges. 
31 
00:02:15,910 --> 00:02:19,540 
Indegree is a measure of all nodes
connected to a specific node with 
32 
00:02:19,540 --> 00:02:21,130 
incoming edges. 
33 
00:02:21,130 --> 00:02:23,730 
And we return similar results and 
34 
00:02:23,730 --> 00:02:29,030 
we union that with the specific query
commands to find all of the root nodes, 
35 
00:02:29,030 --> 00:02:33,090 
and then we return those names and
0 as the value of indegree. 
36 
00:02:33,090 --> 00:02:37,820 
So when we submit this query,
here's our results as we might expect. 
37 
00:02:37,820 --> 00:02:40,690 
In this case, H is our only root node. 
38 
00:02:40,690 --> 00:02:43,264 
So it has a value of 0 for indegree, and 
39 
00:02:43,264 --> 00:02:46,072 
all the other nodes
are as we might expect. 
40 
00:02:49,438 --> 00:02:53,040 
And our third query example
finds the degree of all nodes, 
41 
00:02:53,040 --> 00:02:56,220 
which is a combination of outdegree and
indegree. 
42 
00:02:56,220 --> 00:03:02,010 
So in this case we're not including any
specific direction in our match statement. 
43 
00:03:02,010 --> 00:03:06,820 
And we're returning the name and
the count for all of our edges. 
44 
00:03:06,820 --> 00:03:09,540 
But we're using the distinct statement, 
45 
00:03:09,540 --> 00:03:13,090 
otherwise we would be
counting some nodes twice. 
46 
00:03:13,090 --> 00:03:16,820 
And then, for convenience,
we order this by the value of degree. 
47 
00:03:16,820 --> 00:03:21,628 
And when we submit this query,
we get the results as shown here. 
48 
00:03:21,628 --> 00:03:26,071 
We have 1 column with the name and
the other column with the degree and 
49 
00:03:26,071 --> 00:03:31,048 
the values are as we would expect, we have
a leaf node P with the degree of 1 and 
50 
00:03:31,048 --> 00:03:33,128 
a root node H with a degree of 1. 
51 
00:03:37,296 --> 00:03:43,262 
Our next query example generates a degree
histogram of the graph since we're able to 
52 
00:03:43,262 --> 00:03:49,670 
calculate the degree of each node, we can
sort those into actual values of degree. 
53 
00:03:49,670 --> 00:03:53,606 
So if we look at the distribution
of degree among our nodes, 
54 
00:03:53,606 --> 00:03:58,801 
we see there's 2 nodes with the degree 1,
there's 3 nodes with degree 2, 
55 
00:03:58,801 --> 00:04:04,280 
there's 4 nodes with degree 3, and
there's 2 nodes with degree 4. 
56 
00:04:04,280 --> 00:04:07,181 
So we're going to group those
in the form of a histogram. 
57 
00:04:07,181 --> 00:04:11,310 
So when we submit this query,
we get this table. 
58 
00:04:11,310 --> 00:04:16,002 
The first column list the degree
value in ascending order and 
59 
00:04:16,002 --> 00:04:22,500 
the second column list the counts of
the nodes that have that degree value. 
60 
00:04:22,500 --> 00:04:27,451 
So for those of you who are familiar with
SQL you might recognize this as similar 
61 
00:04:27,451 --> 00:04:30,989 
to the group by command,
it performs a similar function 
62 
00:04:34,471 --> 00:04:38,980 
Our next query example saves the degree
of the node as a new node property. 
63 
00:04:38,980 --> 00:04:44,000 
This provides an added convenience so that
we don't have to calculate the degree of 
64 
00:04:44,000 --> 00:04:47,330 
a node every time we're
performing some sort of analysis. 
65 
00:04:47,330 --> 00:04:50,640 
So we match all nodes with edges, and 
66 
00:04:50,640 --> 00:04:54,950 
there's no direction in this
particular edge definition. 
67 
00:04:54,950 --> 00:04:59,090 
And then, we return distinct
counts of each node's degree, and 
68 
00:04:59,090 --> 00:05:05,460 
then we create a new property, called deg,
and assign the value of degree to it. 
69 
00:05:05,460 --> 00:05:09,090 
Then, we return the names and
the degree values, and so 
70 
00:05:09,090 --> 00:05:13,040 
when we submit this query,
we see this distribution right here, 
71 
00:05:13,040 --> 00:05:17,690 
with the names in the left column, and
the values of degree in the right column. 
72 
00:05:19,070 --> 00:05:23,740 
And we can verify that if we issue a
command to return all of the properties of 
73 
00:05:23,740 --> 00:05:25,250 
the specific node. 
74 
00:05:25,250 --> 00:05:28,970 
So in this case I issued a command
to match the node named D and 
75 
00:05:28,970 --> 00:05:30,640 
return all of its properties. 
76 
00:05:30,640 --> 00:05:34,272 
And sure enough we see that it has
a property name and a property degree. 
77 
00:05:37,569 --> 00:05:39,476 
Before we go to the last two examples, 
78 
00:05:39,476 --> 00:05:43,800 
there's a philosophical issue that we
need to remember with all databases. 
79 
00:05:43,800 --> 00:05:47,780 
Every database will allow you some
analytical computation and the remainder 
80 
00:05:47,780 --> 00:05:51,610 
of the analytical computations must
be done outside of the database. 
81 
00:05:51,610 --> 00:05:54,830 
However, it is always a judicious
idea to get the database to 
82 
00:05:54,830 --> 00:05:58,683 
achieve an intermediate result formatted
in a way that you would need for 
83 
00:05:58,683 --> 00:05:59,962 
the next computation. 
84 
00:05:59,962 --> 00:06:04,560 
And then, you use that intermediate result
as the input to the next computation. 
85 
00:06:04,560 --> 00:06:07,290 
We've seen that a number of
computations in graph analytics 
86 
00:06:07,290 --> 00:06:10,040 
start with the adjacency matrix. 
87 
00:06:10,040 --> 00:06:14,370 
So we should be able to force Cypher
to produce an adjacency matrix. 
88 
00:06:14,370 --> 00:06:15,480 
And this is what we're doing here. 
89 
00:06:16,980 --> 00:06:22,010 
So think of a Matrix as a three column
table, in which, here's one column, 
90 
00:06:22,010 --> 00:06:25,220 
here's another column, and the third
column will be the values that we 
91 
00:06:25,220 --> 00:06:30,140 
are calculating when we determine whether
two nodes have an edge between them. 
92 
00:06:30,140 --> 00:06:33,910 
And we're introducing a new
construct in Cypher called case. 
93 
00:06:33,910 --> 00:06:38,320 
This allows us to evaluate conditions and
return one result, or 
94 
00:06:38,320 --> 00:06:41,000 
a different result
depending on the condition. 
95 
00:06:41,000 --> 00:06:46,200 
Here, we're specifying that when
there is an edge between nodes n and 
96 
00:06:46,200 --> 00:06:50,890 
m, then we return a value of 1,
otherwise return a value of 0. 
97 
00:06:50,890 --> 00:06:53,068 
And we'll output those results as a value. 
98 
00:06:53,068 --> 00:06:57,748 
And so, when we submit this query,
we get our three column table in which 
99 
00:06:57,748 --> 00:07:01,340 
the first column is
the name of our first node. 
100 
00:07:01,340 --> 00:07:05,077 
The second column is the name of our
second node and the value is either a 1 or 
101 
00:07:05,077 --> 00:07:09,250 
a 1 depending on whether the nodes
have an edge between them. 
102 
00:07:09,250 --> 00:07:12,839 
So in this case we see node A and
C have an edge, 
103 
00:07:12,839 --> 00:07:16,530 
A and L have an edge and
so on as we would expect. 
104 
00:07:20,050 --> 00:07:25,150 
So if we can calculate the adjacency
matrix then we can calculate any matrix. 
105 
00:07:25,150 --> 00:07:29,280 
You might remember from our
module two lecture where we 
106 
00:07:29,280 --> 00:07:32,760 
learned about this complex structure
called the Normalized Laplacian Matrix. 
107 
00:07:33,970 --> 00:07:35,437 
So let's go ahead and calculate that. 
108 
00:07:35,437 --> 00:07:41,090 
We'll perform something very similar
to what we did in the previous example. 
109 
00:07:41,090 --> 00:07:44,320 
We'll match all nodes for
the first column, and all nodes for 
110 
00:07:44,320 --> 00:07:46,080 
the second column. 
111 
00:07:46,080 --> 00:07:51,390 
We'll return the names of those nodes and
then we'll use the case structure again 
112 
00:07:51,390 --> 00:07:55,280 
to compare the names of each node and
determine whether we have the same node. 
113 
00:07:56,590 --> 00:08:01,160 
If we do have the same node then
that is a diagonal of the matrix and 
114 
00:08:01,160 --> 00:08:01,945 
should get a value of 1. 
115 
00:08:03,030 --> 00:08:06,264 
If they are different nodes and
contain an edge between them, 
116 
00:08:06,264 --> 00:08:09,948 
then we calculate the normalized
Laplacian with this equation here. 
117 
00:08:09,948 --> 00:08:14,638 
And you'll also want to notice that here
we're using the actual degree property 
118 
00:08:14,638 --> 00:08:18,210 
that we assigned to the nodes
in a previous example. 
119 
00:08:18,210 --> 00:08:22,080 
This is an example of how that
can become a convenient option. 
120 
00:08:22,080 --> 00:08:25,790 
So when the calculation is performed,
the value would be returned. 
121 
00:08:25,790 --> 00:08:30,030 
If there's no edge between the 2 nodes,
then the value of 0 will be returned. 
122 
00:08:30,030 --> 00:08:32,194 
And these values will end
up in the value column. 
123 
00:08:32,194 --> 00:08:36,780 
So when we submit this query,
here's the table that get returned. 
124 
00:08:36,780 --> 00:08:39,662 
This is the first column
with the source node. 
125 
00:08:39,662 --> 00:08:43,370 
The second column with the target node and
the values. 
126 
00:08:43,370 --> 00:08:47,070 
So in the first row, the first node
is P and the second node is P, so 
127 
00:08:47,070 --> 00:08:50,600 
it's identical, which means it's
on the diagonal of the matrix. 
128 
00:08:50,600 --> 00:08:53,630 
Likewise, for A in this row down here. 
129 
00:08:53,630 --> 00:08:58,040 
And then the first value of the Laplacian
is calculated between nodes A and 
130 
00:08:58,040 --> 00:08:59,180 
C, and so on. 
131 
00:09:01,310 --> 00:09:06,202 
So that concludes our examples of how
to perform connectivity analytics 
132 
00:09:06,202 --> 00:09:07,815 
in Neo4j with Cypher. 
1 
00:00:00,430 --> 00:00:04,690 
Hello everyone and welcome to a series of
lessons on graph analytics with Neo4j. 
2 
00:00:05,730 --> 00:00:10,520 
In this first tutorial we are going to go
briefly through the process of downloading 
3 
00:00:10,520 --> 00:00:15,420 
and installing Neo4j, and
then we'll go ahead and run Neo4j and 
4 
00:00:16,560 --> 00:00:20,670 
provide a brief review of
the Neo4j graphical interface. 
5 
00:00:20,670 --> 00:00:21,713 
So, let's get started. 
6 
00:00:25,417 --> 00:00:30,480 
To download Neo4j,
you'll want to browse to neo4j.com, and 
7 
00:00:30,480 --> 00:00:36,660 
once you're at that URL,
you should see a webpage similar to this. 
8 
00:00:36,660 --> 00:00:40,030 
Neo4j typically will detect which
operating system you're running, but 
9 
00:00:40,030 --> 00:00:43,340 
if you want to download
other versions of Neo4j, 
10 
00:00:43,340 --> 00:00:46,990 
click this link in the upper
right-hand corner to download Neo4j. 
11 
00:00:46,990 --> 00:00:51,030 
We're going to use the community edition,
so we'll click this link right here. 
12 
00:00:51,030 --> 00:00:52,950 
But if you'd like to
explore other releases, 
13 
00:00:52,950 --> 00:00:54,360 
you can click the link down here. 
14 
00:00:55,550 --> 00:00:58,760 
When we click
the Download Community Edition link, 
15 
00:00:58,760 --> 00:01:02,260 
Neo4j should automatically
begin the download. 
16 
00:01:02,260 --> 00:01:07,220 
There are versions of Neo4j for
Windows, Mac, and Linux. 
17 
00:01:07,220 --> 00:01:10,300 
Neo4j is developed with the Java
programming language, so 
18 
00:01:10,300 --> 00:01:15,240 
you'll want to be sure you have the
minimum system requirements to run Neo4j. 
19 
00:01:15,240 --> 00:01:18,220 
We'll provide a link to those system
requirements in the resources of 
20 
00:01:18,220 --> 00:01:18,750 
this video. 
21 
00:01:19,820 --> 00:01:24,000 
This example is with
the Mac OS X operating system. 
22 
00:01:24,000 --> 00:01:27,829 
And we've downloaded the .dmg file and
we can click to open that. 
23 
00:01:31,126 --> 00:01:35,037 
And as is the case with the Mac
applications to install is very 
24 
00:01:35,037 --> 00:01:36,890 
straightforward. 
25 
00:01:36,890 --> 00:01:40,590 
We'll just click and drag this icon
into the Applications folder and 
26 
00:01:40,590 --> 00:01:43,210 
that should automatically install. 
27 
00:01:43,210 --> 00:01:44,820 
Once the application is installed, 
28 
00:01:44,820 --> 00:01:48,260 
we should see it listed in
the Applications folder. 
29 
00:01:48,260 --> 00:01:50,149 
So let's go ahead and run Neo4j. 
30 
00:01:54,491 --> 00:01:58,078 
To run Neo4j, you'll just double-click
on the application's icon. 
31 
00:01:58,078 --> 00:02:02,770 
Neo4j is browser-based, so
it launches a web server, and 
32 
00:02:02,770 --> 00:02:07,050 
it launches a URL specific to Neo4j. 
33 
00:02:07,050 --> 00:02:10,520 
So to activate that server
we'll click the Start button. 
34 
00:02:10,520 --> 00:02:14,940 
If it's your first time using Neo4j
then you probably won't have any graph 
35 
00:02:14,940 --> 00:02:19,890 
databases available to open,
so Neo4j makes it easy for 
36 
00:02:19,890 --> 00:02:23,650 
you by providing a default
graph database of movie data. 
37 
00:02:24,660 --> 00:02:29,534 
To run Neo4j, you click this link and
that should launch your default browser. 
38 
00:02:33,598 --> 00:02:37,510 
Installation on Windows should be
just as straightforward as OS X. 
39 
00:02:37,510 --> 00:02:41,380 
You'll click the same
Download Neo4j button and 
40 
00:02:41,380 --> 00:02:45,900 
when the download page opens,
you'll click Download Community Edition. 
41 
00:02:45,900 --> 00:02:51,390 
Neo4j should recognize you're running
Windows and should download an executable. 
42 
00:02:51,390 --> 00:02:56,340 
Once that downloads we can run it and
we'll step through the same steps 
43 
00:02:56,340 --> 00:02:59,150 
which are typical for
a Windows operating system. 
44 
00:02:59,150 --> 00:03:02,491 
Now I already have Neo4j
installed in my Windows system so 
45 
00:03:02,491 --> 00:03:05,161 
I will be updating my
existing installation. 
46 
00:03:09,538 --> 00:03:12,210 
And Neo4j downloads a Java jar file. 
47 
00:03:13,340 --> 00:03:17,600 
Once the application's installed
it will open the dialog box and 
48 
00:03:17,600 --> 00:03:19,730 
we can start up our web server. 
49 
00:03:19,730 --> 00:03:23,200 
And the same URL is available and
when we click that, our default 
50 
00:03:23,200 --> 00:03:27,427 
browser should be launched and we should
see the same interface we see In OS X. 
51 
00:03:28,665 --> 00:03:32,810 
You should be able to subsequently
run neo4j by accessing the executable 
52 
00:03:32,810 --> 00:03:34,890 
in the Windows Start menu. 
53 
00:03:34,890 --> 00:03:37,279 
So let's take a look at
the neo4j interface. 
54 
00:03:40,797 --> 00:03:45,150 
As I mentioned previously,
neo4j is a browser-based application. 
55 
00:03:45,150 --> 00:03:49,135 
So once the application launches it
should open your default browser and 
56 
00:03:49,135 --> 00:03:52,110 
in my case, I'm using the Chrome browser. 
57 
00:03:52,110 --> 00:03:56,250 
And you should see a URL like
this in the address bar. 
58 
00:03:56,250 --> 00:04:00,740 
Most browsers provide a full-screen
viewing mode, so I'm going to go ahead and 
59 
00:04:00,740 --> 00:04:04,560 
activate that, in this case,
what Chrome calls presentation mode. 
60 
00:04:04,560 --> 00:04:08,020 
The interface for
neo4j consists of three main areas. 
61 
00:04:08,020 --> 00:04:12,600 
There's the command line along the top
where you can issue commands and you can 
62 
00:04:12,600 --> 00:04:17,030 
paste content into the command line and
execute multiple lines at one time. 
63 
00:04:18,760 --> 00:04:23,330 
You can specify these commands
to be one of your favorites. 
64 
00:04:23,330 --> 00:04:25,600 
You can clear the command box. 
65 
00:04:25,600 --> 00:04:28,738 
And you can run, or execute,
the commands with the Play button. 
66 
00:04:28,738 --> 00:04:33,640 
On the left-hand side is an expandable and
collapsible panel that provides 
67 
00:04:33,640 --> 00:04:36,830 
supplementary information about the
database you're currently working with. 
68 
00:04:37,900 --> 00:04:40,550 
There's an option to list your favorites,
and 
69 
00:04:40,550 --> 00:04:45,020 
Neo4j provides an initial
default listing of favorites. 
70 
00:04:45,020 --> 00:04:47,890 
And there's supplementary information and
links to references and 
71 
00:04:47,890 --> 00:04:50,920 
examples and tutorials and so on. 
72 
00:04:50,920 --> 00:04:55,990 
You can also modify the configuration
of your Neo4j installation for 
73 
00:04:55,990 --> 00:04:57,570 
your particular user. 
74 
00:04:57,570 --> 00:05:00,383 
And there's additional information
about the application. 
75 
00:05:00,383 --> 00:05:04,631 
The third area is the main panel in
the middle where the results of all of 
76 
00:05:04,631 --> 00:05:07,760 
the commands that you
execute are displayed. 
77 
00:05:07,760 --> 00:05:11,220 
In fact, this panel itself has been
generated by issuing a command, 
78 
00:05:11,220 --> 00:05:14,770 
which is displayed in
the very top of the panel. 
79 
00:05:14,770 --> 00:05:18,180 
The command is ;play start. 
80 
00:05:18,180 --> 00:05:23,160 
In fact, if I type that command into my
command line I should be able to generate 
81 
00:05:23,160 --> 00:05:26,088 
a duplicate of that panel,
and sure enough, there it is. 
82 
00:05:26,088 --> 00:05:32,310 
The upper right-hand corner of these
panels I can pin the panel to my webpage, 
83 
00:05:32,310 --> 00:05:34,590 
and I can do this with as
many panels as I want and 
84 
00:05:34,590 --> 00:05:37,820 
the window will simply allow
me to scroll up and down. 
85 
00:05:37,820 --> 00:05:40,520 
I can view this particular
panel full screen. 
86 
00:05:40,520 --> 00:05:43,430 
Or it can cancel or close it,
which is what I'm going to do. 
87 
00:05:43,430 --> 00:05:49,530 
Now you can see neo4j makes it very
easy to learn all about graph analytics. 
88 
00:05:49,530 --> 00:05:53,460 
So let's go ahead and get started
with learning how to create and 
89 
00:05:53,460 --> 00:05:55,220 
modify our first graph network. 
1 
00:00:02,654 --> 00:00:07,877 
Next we're going to get started with Neo4j
by creating our first graph network. 
2 
00:00:08,960 --> 00:00:11,940 
To do this,
first we will review a graphical or 
3 
00:00:11,940 --> 00:00:15,010 
a diagrammatic representation
of a graph network. 
4 
00:00:15,010 --> 00:00:19,350 
Then, we'll introduce you to an equivalent
text representation of that network. 
5 
00:00:19,350 --> 00:00:24,060 
Then, we will build on those text
representations in the form of pseudocode, 
6 
00:00:24,060 --> 00:00:28,240 
with the ultimate goal being
to develop an actual script 
7 
00:00:28,240 --> 00:00:30,010 
to create our network in Neo4j. 
8 
00:00:31,160 --> 00:00:35,423 
Then, we'll go ahead and run the script to
create the network, and we'll explore it, 
9 
00:00:35,423 --> 00:00:37,333 
and confirm it structure and content. 
10 
00:00:41,776 --> 00:00:47,800 
You're looking at a simple graph network,
consisting of five nodes and five edges. 
11 
00:00:47,800 --> 00:00:51,050 
Each node represents a person,
an individual. 
12 
00:00:51,050 --> 00:00:54,450 
And, the edges represent
relationships between those people. 
13 
00:00:54,450 --> 00:00:58,840 
Each node has a number associate with it,
N1 through N5. 
14 
00:00:58,840 --> 00:01:04,732 
And, each edge has a corresponding number
associated with it, E1 through E5. 
15 
00:01:04,732 --> 00:01:09,286 
Edges are relationships such
as Harry is known by Tom, 
16 
00:01:09,286 --> 00:01:11,964 
or Julian is coworker of Harry. 
17 
00:01:11,964 --> 00:01:16,460 
We could have more or less edges, but we
wanted to keep things relatively simple, 
18 
00:01:16,460 --> 00:01:20,050 
while still maintaining
a reasonable degree of complexity. 
19 
00:01:20,050 --> 00:01:23,000 
What we want is a script that we can 
20 
00:01:23,000 --> 00:01:28,270 
process with Neo4j in order to
create an actual graph network. 
21 
00:01:28,270 --> 00:01:31,273 
So, let's look at a text
representation of this network. 
22 
00:01:35,494 --> 00:01:40,879 
So, here we list our five nodes and
five edges, and we're going to begin 
23 
00:01:40,879 --> 00:01:46,940 
the process of extending the text
descriptions of our graph network. 
24 
00:01:46,940 --> 00:01:50,210 
I'm going to scroll down briefly,
and just show you the end result, so 
25 
00:01:50,210 --> 00:01:54,070 
you have a better idea of what
the final goal is going to be. 
26 
00:01:54,070 --> 00:01:58,854 
This is the actual code that we're going
to submit to Neo4j in order to create 
27 
00:01:58,854 --> 00:01:59,745 
our network. 
28 
00:01:59,745 --> 00:02:02,425 
But, we're going to back
up a little bit and 
29 
00:02:02,425 --> 00:02:05,615 
look at some simplified
versions of this syntax, so 
30 
00:02:05,615 --> 00:02:10,627 
we can better understand how simple the
graph network relationships really are. 
31 
00:02:13,374 --> 00:02:18,930 
Here, we list the five nodes and
five edges, as you saw just a moment ago. 
32 
00:02:18,930 --> 00:02:23,340 
And down below,
we have a simple notation structure, 
33 
00:02:23,340 --> 00:02:26,490 
which attempts to describe
the five edge relationships. 
34 
00:02:26,490 --> 00:02:30,600 
The first line represents the edge E1. 
35 
00:02:30,600 --> 00:02:34,820 
We can see that the nodes N1 and
N2 are included, 
36 
00:02:34,820 --> 00:02:40,410 
because Harry is known by Tom, and
Tom is node N1 and Harry is node N2. 
37 
00:02:40,410 --> 00:02:44,950 
The same goes for the second line,
the relationship between Julian and Harry. 
38 
00:02:44,950 --> 00:02:47,770 
Julian is co-worker of Harry, and so on. 
39 
00:02:48,890 --> 00:02:53,830 
So, let's take this a step further
by defining our graph network, 
40 
00:02:53,830 --> 00:02:57,300 
so that each node is
a particular type of node. 
41 
00:02:57,300 --> 00:03:03,410 
In this case, we're going to define a node
type as what we're calling a ToyNode. 
42 
00:03:03,410 --> 00:03:08,040 
As we introduce each node and
its relationship with other nodes, 
43 
00:03:08,040 --> 00:03:11,248 
we'll define the node
to be of type ToyNode. 
44 
00:03:11,248 --> 00:03:16,181 
So, on this first line,
N1 goes through e1 to N2. 
45 
00:03:16,181 --> 00:03:18,822 
And, both of those are introduced for
the first time, so 
46 
00:03:18,822 --> 00:03:21,370 
we'll define them as type ToyNode. 
47 
00:03:21,370 --> 00:03:25,380 
But, on the next line, since we
already introduced N2 as type ToyNode, 
48 
00:03:25,380 --> 00:03:26,970 
we don't need to repeat that statement. 
49 
00:03:28,000 --> 00:03:33,200 
And, so we continue in the same manner
with the remaining edge relationships. 
50 
00:03:33,200 --> 00:03:38,860 
Taking this even further, we'll apply
a similar kind of constraint to our edges. 
51 
00:03:38,860 --> 00:03:44,880 
In this case, we'll define our network
such that each edge is a particular type, 
52 
00:03:44,880 --> 00:03:47,120 
which we're calling ToyRelation. 
53 
00:03:47,120 --> 00:03:50,510 
Next, we're going to add
properties to our nodes and edges. 
54 
00:03:50,510 --> 00:03:54,070 
Our nodes can have properties
such as name or job, 
55 
00:03:54,070 --> 00:03:59,850 
so in this case, our first node,
N1, will have the name Tom. 
56 
00:03:59,850 --> 00:04:04,700 
And, the appropriate syntax for this
includes curly braces surrounding the key 
57 
00:04:04,700 --> 00:04:09,570 
value pairs, a colon separating
the key value pairs, and 
58 
00:04:09,570 --> 00:04:13,500 
the values defined within single quotes. 
59 
00:04:13,500 --> 00:04:17,738 
Likewise, each edge may have
a specific type of relationship, 
60 
00:04:17,738 --> 00:04:20,722 
including co-worker, wife, and friend. 
61 
00:04:22,534 --> 00:04:26,499 
So, finally this brings us to the actual
code we're going to use to create our 
62 
00:04:26,499 --> 00:04:28,130 
graph network. 
63 
00:04:28,130 --> 00:04:31,057 
So, let's go ahead and copy this,
and paste it into Neo4j, so 
64 
00:04:31,057 --> 00:04:32,587 
we can take a look at our network. 
65 
00:04:35,295 --> 00:04:38,410 
Here we are, running Neo4j in our browser. 
66 
00:04:38,410 --> 00:04:43,830 
So, I'm going to paste the code that
I just copied from my text file 
67 
00:04:43,830 --> 00:04:46,360 
into the command line
in the Neo4j interface. 
68 
00:04:47,450 --> 00:04:50,970 
And, I'll click the Play button
to execute those commands, and 
69 
00:04:50,970 --> 00:04:55,250 
we'll see the results returned
in this newly displayed panel. 
70 
00:04:55,250 --> 00:05:00,320 
We can see that we have 5 labels added,
5 nodes were created, 13 properties were 
71 
00:05:00,320 --> 00:05:06,680 
set, 5 relationships were created, and the
entire process required 31 milliseconds. 
72 
00:05:09,000 --> 00:05:14,550 
However, we still can't actually view our
graph unless we issue yet another command. 
73 
00:05:14,550 --> 00:05:19,640 
So, let's shuffle to our text file,
and take a look at that command. 
74 
00:05:19,640 --> 00:05:23,910 
What this command does,
is it tries to identify a match 
75 
00:05:23,910 --> 00:05:29,600 
in which a particular node has
a relationship with any other node. 
76 
00:05:29,600 --> 00:05:33,180 
And, then we'll return those nodes and
relationships. 
77 
00:05:33,180 --> 00:05:36,020 
So, we'll go ahead and copy this, and 
78 
00:05:36,020 --> 00:05:40,280 
we'll paste it in to our command line,
and we'll execute. 
79 
00:05:40,280 --> 00:05:41,970 
And, there's our graph. 
80 
00:05:41,970 --> 00:05:47,360 
When we mouse over the nodes, we can see
information displayed on the bottom. 
81 
00:05:47,360 --> 00:05:51,770 
And, when we select those nodes, that
information is displayed permanently along 
82 
00:05:51,770 --> 00:05:55,810 
the bottom,
likewise with edge relationships. 
83 
00:05:55,810 --> 00:06:01,536 
So, we would expect to see things such
as Michelle is the wife of Harry, 
84 
00:06:01,536 --> 00:06:06,110 
Julian is a co-worker of Harry, and so on. 
85 
00:06:06,110 --> 00:06:09,310 
So, it looks like our network
has been created successfully. 
86 
00:06:09,310 --> 00:06:12,980 
We can also display
information in a tabular format 
87 
00:06:12,980 --> 00:06:15,190 
by clicking the Rows icon on the left. 
88 
00:06:15,190 --> 00:06:19,030 
And, we'll see that this information
is not constrained by the directed 
89 
00:06:19,030 --> 00:06:20,320 
nature of our graph. 
90 
00:06:20,320 --> 00:06:23,585 
For example, we see that Harry
has a relationship with Tom, 
91 
00:06:23,585 --> 00:06:25,910 
in which Harry knows Tom. 
92 
00:06:25,910 --> 00:06:29,386 
But likewise, down here,
Tom has a relationship with Harry, 
93 
00:06:29,386 --> 00:06:30,840 
in which Tom knows Harry. 
94 
00:06:32,400 --> 00:06:35,620 
Next, we're going to learn
how to add to this graph and 
95 
00:06:35,620 --> 00:06:38,777 
modify some of the properties
of the nodes and edges. 
1 
00:00:01,100 --> 00:00:05,230 
In this segment, we're going to
learn how to import data into Neo4j. 
2 
00:00:06,780 --> 00:00:11,440 
We will begin by using a fairly simple
spreadsheet consisting of only a few 
3 
00:00:11,440 --> 00:00:15,550 
rows and three columns in a format
that's fairly typical for 
4 
00:00:15,550 --> 00:00:17,630 
importing into a graph database. 
5 
00:00:17,630 --> 00:00:23,180 
We will review the Neo4j CYPHER
script used to perform this import. 
6 
00:00:23,180 --> 00:00:26,340 
And then we'll run the script and
validate the resulting graph. 
7 
00:00:26,340 --> 00:00:28,790 
Then we'll demonstrate
a similar process but 
8 
00:00:28,790 --> 00:00:32,060 
with a more challenging dataset,
consisting of terrorist network data. 
9 
00:00:33,210 --> 00:00:35,060 
We will review this dataset and 
10 
00:00:35,060 --> 00:00:38,900 
the script commands necessary for
performing the import. 
11 
00:00:38,900 --> 00:00:41,100 
And we'll run the script and
explore the resulting graph. 
12 
00:00:42,180 --> 00:00:46,953 
And finally, we will review a third
dataset that you will use yourself to 
13 
00:00:46,953 --> 00:00:49,780 
perform similar data inport operations. 
14 
00:00:54,323 --> 00:00:57,440 
First, let's take a look
at our sample dataset. 
15 
00:00:57,440 --> 00:01:01,890 
This spreadsheet consists of just
a few rows and three columns. 
16 
00:01:01,890 --> 00:01:03,480 
Each column has a heading. 
17 
00:01:03,480 --> 00:01:08,034 
The first column heading is Source,
the second column heading is Target, and 
18 
00:01:08,034 --> 00:01:10,324 
the third column heading is distance. 
19 
00:01:10,324 --> 00:01:14,762 
You can imagine that this might represent
data from a simple road network in which 
20 
00:01:14,762 --> 00:01:17,570 
the Source and
Target values represent towns, and 
21 
00:01:17,570 --> 00:01:21,963 
the distance values represent the actual
distance in miles between the towns. 
22 
00:01:25,333 --> 00:01:28,492 
So let's take a look at
the Neo4j CYPHER script and 
23 
00:01:28,492 --> 00:01:32,110 
see what we will need to do to
import our spreadsheet data. 
24 
00:01:32,110 --> 00:01:36,010 
The first line of code
performs the actual import. 
25 
00:01:36,010 --> 00:01:40,340 
The other three lines of code provide
constraints to the formatting 
26 
00:01:40,340 --> 00:01:41,670 
of that data. 
27 
00:01:41,670 --> 00:01:45,350 
Since our data file is
a comma separated values or 
28 
00:01:45,350 --> 00:01:49,170 
CSV file,
we will need to specify that in our code. 
29 
00:01:49,170 --> 00:01:51,530 
Our file also contains headers. 
30 
00:01:51,530 --> 00:01:53,718 
And if you're using
a Windows operating system, 
31 
00:01:53,718 --> 00:01:55,758 
your file path will look
something like this. 
32 
00:01:55,758 --> 00:02:00,609 
Since we're running Neo4j in a browser,
the file path needs 
33 
00:02:00,609 --> 00:02:05,365 
to conform to the HTTP address
conventions of the word file, 
34 
00:02:05,365 --> 00:02:10,030 
followed by a colon,
followed by three forward slashes and 
35 
00:02:10,030 --> 00:02:16,307 
then the hard disk letter where the file
is located, plus the path to that file. 
36 
00:02:16,307 --> 00:02:20,046 
If you're using Mac OSX,
your command will be similar, but 
37 
00:02:20,046 --> 00:02:24,490 
the file path will probably
look something more like this. 
38 
00:02:24,490 --> 00:02:28,280 
The next three lines specify which
nodes will be the source nodes and 
39 
00:02:28,280 --> 00:02:30,810 
which nodes will be the target nodes. 
40 
00:02:30,810 --> 00:02:36,020 
And the properties will attach to them as
well as defining the relationships and 
41 
00:02:36,020 --> 00:02:38,860 
the properties we will
attach to the relationships. 
42 
00:02:38,860 --> 00:02:43,800 
As we read each line of data n,
we're going to use keyword line, 
43 
00:02:43,800 --> 00:02:47,330 
to specify the individual line
we're currently working on. 
44 
00:02:47,330 --> 00:02:51,120 
We use that here at the end
of our load command and so 
45 
00:02:51,120 --> 00:02:54,800 
we'll have to continue to use it
in the subsequent merge commands. 
46 
00:02:56,000 --> 00:02:58,990 
Our source node variable
is going to be n and 
47 
00:02:58,990 --> 00:03:03,010 
we'll use the MyNode type,
which we've made up ourselves. 
48 
00:03:03,010 --> 00:03:07,150 
We're going to add a Name property
to each of our source nodes. 
49 
00:03:07,150 --> 00:03:12,880 
And we're going to attach the value in
the Source column to that particular node. 
50 
00:03:12,880 --> 00:03:17,350 
Likewise for our target nodes,
we'll use a variable m. 
51 
00:03:17,350 --> 00:03:19,930 
We'll define them as the same type,
MyNode. 
52 
00:03:21,330 --> 00:03:25,820 
We will give them a Name and we will
extract that name from the Target column 
53 
00:03:25,820 --> 00:03:28,300 
on the particular line we're working with. 
54 
00:03:28,300 --> 00:03:31,590 
Finally, we need to define
our edge relationships. 
55 
00:03:31,590 --> 00:03:34,870 
We're going to give each
edge a label of the word TO, 
56 
00:03:34,870 --> 00:03:40,360 
and we're going to add a property called
dist, which represents the distance. 
57 
00:03:40,360 --> 00:03:43,150 
And we'll attach the values
in the distance column 
58 
00:03:43,150 --> 00:03:45,950 
from the particular line
we're currently working on. 
59 
00:03:45,950 --> 00:03:52,050 
So let's go ahead and copy this code and
paste it into Neo4j, and see what happens. 
60 
00:03:52,050 --> 00:03:54,868 
Now, I'm working on a Mac OSX machine, so 
61 
00:03:54,868 --> 00:03:59,663 
I'm going to copy this code down here and
perform the import operation. 
62 
00:04:02,380 --> 00:04:07,986 
So I've pasted the line
of code into Neo4j and 
63 
00:04:07,986 --> 00:04:10,740 
I'll click Execute. 
64 
00:04:10,740 --> 00:04:12,190 
And it takes a few moments to run. 
65 
00:04:12,190 --> 00:04:16,410 
There's not too much data so it shouldn't
take more than just a few seconds. 
66 
00:04:16,410 --> 00:04:20,794 
And now we get the results in which
11 labels, 11 nodes have been added, 
67 
00:04:20,794 --> 00:04:25,057 
25 properties have been set and
14 relationships have been created. 
68 
00:04:25,057 --> 00:04:27,320 
So let's take a look
at this graph network. 
69 
00:04:27,320 --> 00:04:31,540 
We can see that the nodes are listed
all as MyNode and there's our graph. 
70 
00:04:32,590 --> 00:04:36,345 
The edge relationships should all
have different distance values and 
71 
00:04:36,345 --> 00:04:38,833 
each node should be named
a different letter. 
72 
00:04:38,833 --> 00:04:41,887 
Okay, so
let's try a more difficult dataset. 
73 
00:04:44,849 --> 00:04:48,135 
Here is the spreadsheet
containing terrorist data. 
74 
00:04:48,135 --> 00:04:53,175 
The spreadsheet consists of seven
columns with headings such as Country, 
75 
00:04:53,175 --> 00:04:57,897 
ActorName, ActorType, AffiliationTo,
AffiliationStartDate, 
76 
00:04:57,897 --> 00:05:04,300 
AffiliationEndDate, and any aliases
associated with that particular terrorist. 
77 
00:05:04,300 --> 00:05:09,880 
This dataset consists of
over 100,000 rows of data. 
78 
00:05:09,880 --> 00:05:14,350 
Since that could take a very long
time to load into Neo4j, we will be 
79 
00:05:14,350 --> 00:05:19,980 
working with a subset of this dataset
consisting of the first 1,000 rows. 
80 
00:05:19,980 --> 00:05:23,246 
That much data will still
include three countries, 
81 
00:05:23,246 --> 00:05:26,296 
which should be plenty of data for
our purposes. 
82 
00:05:29,953 --> 00:05:34,911 
So here is the script we're going to use
to import the subset of terrorist data, 
83 
00:05:34,911 --> 00:05:39,420 
which shares similarities with
the script we used previously. 
84 
00:05:39,420 --> 00:05:42,030 
But since there are more
columns in our dataset, 
85 
00:05:42,030 --> 00:05:46,810 
we're going to include some additional
properties into our graph network. 
86 
00:05:46,810 --> 00:05:50,160 
The first line of code is very similar 
87 
00:05:50,160 --> 00:05:53,610 
to the load command we've
used in previous datasets. 
88 
00:05:53,610 --> 00:06:00,720 
The second line of code will use
a variable c and a label Country for 
89 
00:06:00,720 --> 00:06:04,155 
the particular nodes representing
the individual countries in the dataset. 
90 
00:06:05,300 --> 00:06:09,280 
In this particular case, we're going to
use the keyword row instead of line to 
91 
00:06:09,280 --> 00:06:11,990 
read our data n, but
it really doesn't matter. 
92 
00:06:11,990 --> 00:06:16,397 
We could use either word as long we
are consistent from command to command. 
93 
00:06:16,397 --> 00:06:21,136 
So we're using the term row and
associating the value in the Country 
94 
00:06:21,136 --> 00:06:25,891 
column with the node that we're
working on in that particular row. 
95 
00:06:25,891 --> 00:06:30,375 
We will do something similar with
nodes that are intended to represent 
96 
00:06:30,375 --> 00:06:33,570 
the actors or
the actual individual terrorists. 
97 
00:06:33,570 --> 00:06:39,670 
We're going to use a variable a and
we will associate a property 
98 
00:06:39,670 --> 00:06:45,140 
called Name, and associate
the ActorName with that property. 
99 
00:06:45,140 --> 00:06:49,410 
We'll also associate
a property named Aliases, and 
100 
00:06:49,410 --> 00:06:53,250 
associate the value in the Aliases
column with that property. 
101 
00:06:54,590 --> 00:06:59,580 
And finally,
we will define a property called Type and 
102 
00:06:59,580 --> 00:07:03,580 
associate the values in the ActorType
column with that property. 
103 
00:07:04,650 --> 00:07:08,110 
We're going to create nodes
representing organizations as well and 
104 
00:07:08,110 --> 00:07:13,700 
we will use the variable o, and
the label Organization for those nodes. 
105 
00:07:13,700 --> 00:07:18,800 
We will attach a single property
to these nodes called Name and 
106 
00:07:18,800 --> 00:07:23,968 
we'll assign the values in
the AffiliationTo column to that property. 
107 
00:07:23,968 --> 00:07:27,862 
Then we're going to define
relationships between the Actors and 
108 
00:07:27,862 --> 00:07:30,700 
the Organizations they're affiliated with. 
109 
00:07:32,030 --> 00:07:35,835 
The relationship label is
going to be AFFILIATED_TO and 
110 
00:07:35,835 --> 00:07:39,350 
we'll define a property called Start. 
111 
00:07:39,350 --> 00:07:43,525 
And we'll assign the values in the
AffiliationStartDate with that property. 
112 
00:07:43,525 --> 00:07:48,210 
Likewise, we will define
a property called End and 
113 
00:07:48,210 --> 00:07:52,800 
assign the values in
the AffiliationEndDate with that property. 
114 
00:07:52,800 --> 00:07:57,500 
And finally, we're going to create
relationships between the countries and 
115 
00:07:57,500 --> 00:07:59,160 
the actors. 
116 
00:07:59,160 --> 00:08:04,160 
In this case, we will define relationships
with the label IS_FROM that will 
117 
00:08:04,160 --> 00:08:09,890 
describe the fact that a particular
actor is from a particular country. 
118 
00:08:09,890 --> 00:08:13,711 
So if all this makes sense,
let's go ahead and copy this script and 
119 
00:08:13,711 --> 00:08:16,250 
paste it into Neo4j, and see the results. 
120 
00:08:19,270 --> 00:08:22,926 
So here we are in Neo4j, and
I'm going to go ahead and 
121 
00:08:22,926 --> 00:08:27,619 
paste that script into the command line,
and we will execute it. 
122 
00:08:29,951 --> 00:08:37,021 
We loaded 1,000 rows of data, which
consists of 658 labels and 658 nodes. 
123 
00:08:37,021 --> 00:08:42,401 
3,464 properties and
1,403 relationships and 
124 
00:08:42,401 --> 00:08:45,809 
it took about two and a half seconds. 
125 
00:08:45,809 --> 00:08:50,376 
So let's look at a small
subset of this network. 
126 
00:08:55,795 --> 00:09:01,090 
Here, we see the equivalent of
the first 25 rows of the dataset. 
127 
00:09:01,090 --> 00:09:05,120 
There's only enough data
such that 1 Country, 
128 
00:09:05,120 --> 00:09:09,067 
8 Actors and 15 Organizations are visible. 
129 
00:09:09,067 --> 00:09:12,874 
Let's go ahead and
change this command to 250. 
130 
00:09:12,874 --> 00:09:16,550 
By clicking on the line of
code in the top of our panel, 
131 
00:09:16,550 --> 00:09:20,653 
it automatically gets pasted
into the command line above. 
132 
00:09:20,653 --> 00:09:26,085 
So all we need to do is add a zero on the
end of our command and execute that again. 
133 
00:09:26,085 --> 00:09:30,752 
Now we have a much larger,
more complex graph, 
134 
00:09:30,752 --> 00:09:34,262 
but we still only have one country. 
135 
00:09:34,262 --> 00:09:36,918 
In order to see more than one country, 
136 
00:09:36,918 --> 00:09:43,010 
we'll need to render the entire 1,000
rows of our terrorist data subset. 
137 
00:09:43,010 --> 00:09:46,710 
But in so doing, we will have a difficult
time viewing the entire graph. 
138 
00:09:47,720 --> 00:09:51,950 
The community edition of Neo4j
is limited in its ability to 
139 
00:09:51,950 --> 00:09:53,810 
navigate a graph network. 
140 
00:09:53,810 --> 00:09:56,440 
But I'm going to show you
a little trick by editing 
141 
00:09:56,440 --> 00:10:00,290 
the HTML behind the scenes to
scale the view of our graph. 
142 
00:10:01,770 --> 00:10:04,862 
So let's go ahead and
render all 1,000 rows. 
143 
00:10:10,923 --> 00:10:15,473 
So most recent versions of the major
browsers provide the ability to go behind 
144 
00:10:15,473 --> 00:10:18,080 
the scenes and edit the HTML. 
145 
00:10:18,080 --> 00:10:23,024 
So the trick is to find a place on
the viewing panel that does not have 
146 
00:10:23,024 --> 00:10:24,096 
any objects. 
147 
00:10:24,096 --> 00:10:27,971 
So that when you right-click and
inspect the element, 
148 
00:10:27,971 --> 00:10:30,864 
you'll be inspecting the viewing area. 
149 
00:10:30,864 --> 00:10:33,257 
Neo4j uses SVG graphics, or 
150 
00:10:33,257 --> 00:10:38,360 
Scalable Vector Graphics to
render its graph networks. 
151 
00:10:38,360 --> 00:10:42,373 
And SVG uses a g element,
which can be seen here on the right. 
152 
00:10:42,373 --> 00:10:48,452 
So in order to change the scale of our
view, we simply need to double-click and 
153 
00:10:48,452 --> 00:10:53,903 
add scale, open parentheses, and
the scale factor that we'd like. 
154 
00:10:53,903 --> 00:11:00,938 
We hit Return and
the graph network now is zoomed out. 
155 
00:11:00,938 --> 00:11:06,823 
I'm going to try to position it so
we can see at least two countries, 
156 
00:11:06,823 --> 00:11:09,610 
and I'll close my HTML panel. 
157 
00:11:10,920 --> 00:11:12,371 
And so now we can view two countries. 
158 
00:11:12,371 --> 00:11:18,456 
We can see Albania in the upper region,
and Afghanistan in the lower right. 
159 
00:11:18,456 --> 00:11:21,388 
And we can see that there are actors and 
160 
00:11:21,388 --> 00:11:26,970 
organizations that have
relationships with both countries. 
161 
00:11:26,970 --> 00:11:31,384 
Now there are add ons to Neo4j that
make navigating a graph network a little 
162 
00:11:31,384 --> 00:11:32,155 
easier, but 
163 
00:11:32,155 --> 00:11:37,074 
this trick is convenient for those of you
who have not added any Neo4j extensions. 
164 
00:11:40,998 --> 00:11:45,355 
The last thing that we're going to do
is take a look at the sample dataset of 
165 
00:11:45,355 --> 00:11:50,208 
gene-disease associations, and give you
an idea what's going to be expected of 
166 
00:11:50,208 --> 00:11:53,930 
you in the accompanying assignment for
this module. 
167 
00:11:53,930 --> 00:11:58,060 
This data consists of
information associating 
168 
00:11:58,060 --> 00:12:01,200 
different genes with different diseases. 
169 
00:12:01,200 --> 00:12:05,638 
The spreadsheet consists of
columns with headings for geneId, 
170 
00:12:05,638 --> 00:12:10,610 
geneSymbol, geneName, diseaseId, 
171 
00:12:10,610 --> 00:12:15,870 
the diseaseName, the score that
represents the extent to which that gene 
172 
00:12:15,870 --> 00:12:19,302 
is associated with that
particular disease. 
173 
00:12:19,302 --> 00:12:25,554 
The NumberofPubmed articles
containing that information. 
174 
00:12:25,554 --> 00:12:26,944 
The associationTypes, 
175 
00:12:26,944 --> 00:12:31,584 
there are up to three different types of
associations between a gene and a disease. 
176 
00:12:31,584 --> 00:12:34,113 
And then the sources of data and 
177 
00:12:34,113 --> 00:12:39,084 
information that confirm this
gene disease relationship. 
178 
00:12:39,084 --> 00:12:44,830 
Now this dataset contains
over 400,000 rows of data. 
179 
00:12:44,830 --> 00:12:48,322 
So if you have difficulty
importing this entire dataset, 
180 
00:12:48,322 --> 00:12:53,810 
then you'll be better off extracting
the first few thousand rows. 
181 
00:12:53,810 --> 00:12:57,507 
So you're goal will be to
define the load statement, 
182 
00:12:57,507 --> 00:13:02,437 
which includes a CSV with headers,
that will allow you to import enough 
183 
00:13:02,437 --> 00:13:07,384 
data into Neo4j to give you an idea
that you've done it successfully. 
1 
00:00:01,730 --> 00:00:05,250 
We've already learned a little
about the Neo4j interface. 
2 
00:00:05,250 --> 00:00:08,200 
And we've learned how to create
a relatively simple graph with it. 
3 
00:00:09,390 --> 00:00:10,240 
In this lecture, 
4 
00:00:10,240 --> 00:00:14,660 
we're going to learn how to add
another node and an edge to the graph. 
5 
00:00:16,210 --> 00:00:19,880 
We'll also go through
the process of adding a node and 
6 
00:00:19,880 --> 00:00:21,230 
edge incorrectly to the graph. 
7 
00:00:22,360 --> 00:00:24,710 
And we'll learn how to
correct that mistake. 
8 
00:00:24,710 --> 00:00:28,874 
And finally we'll learn how to modify
an existing node's information. 
9 
00:00:34,748 --> 00:00:39,275 
Okay, so here's our network that we
created in the previous lecture. 
10 
00:00:39,275 --> 00:00:41,790 
The first modification
that we want to make, 
11 
00:00:41,790 --> 00:00:43,980 
is adding a single node to the network. 
12 
00:00:45,080 --> 00:00:49,440 
So let's say that Julian has a fiance and 
13 
00:00:49,440 --> 00:00:53,380 
her name is Joyce and
she works as a store clerk. 
14 
00:00:53,380 --> 00:00:57,090 
So let's look at the code that we're
going to use to make that modification. 
15 
00:00:58,340 --> 00:01:01,940 
So this process involves two steps or
two separate commands. 
16 
00:01:01,940 --> 00:01:06,780 
First command requires you to find the
node that you want to add the new node to. 
17 
00:01:06,780 --> 00:01:12,689 
So we use the match command and
we specify the ToyNode named Julian. 
18 
00:01:12,689 --> 00:01:17,716 
Once that command is issued then
we'll use the merge command and 
19 
00:01:17,716 --> 00:01:22,370 
define the relation between Julian and
the new node. 
20 
00:01:22,370 --> 00:01:25,385 
And it's going to be fiancee. 
21 
00:01:25,385 --> 00:01:28,920 
And then the new node
will also be a ToyNode. 
22 
00:01:28,920 --> 00:01:32,680 
And the name is Joyce, and
her job is store clerk. 
23 
00:01:32,680 --> 00:01:36,537 
So let's go ahead and
copy both these lines of code. 
24 
00:01:36,537 --> 00:01:41,430 
And we'll paste them into our command
line, and we'll run these commands. 
25 
00:01:41,430 --> 00:01:44,112 
And the results that
are returned look good. 
26 
00:01:44,112 --> 00:01:50,057 
Neo4j says that it has added 1 label,
created 1 node, set 3 properties, 
27 
00:01:50,057 --> 00:01:56,102 
created 1 relationship, and
it's required 55 milliseconds to execute. 
28 
00:01:56,102 --> 00:01:58,240 
So let's look at that network. 
29 
00:01:58,240 --> 00:02:00,110 
Maybe the easiest way to
view an existing network, 
30 
00:02:00,110 --> 00:02:04,900 
if it's the only one you're working on and
you know the constraints involved, 
31 
00:02:04,900 --> 00:02:10,290 
just ToyNodes, is to expand the panel on
the left and just click the ToyNode node. 
32 
00:02:10,290 --> 00:02:13,550 
And you'll easily see the new network. 
33 
00:02:13,550 --> 00:02:16,600 
And we can confirm that Joyce
has been successfully added, and 
34 
00:02:16,600 --> 00:02:21,220 
when we select that,
we can see that her job is store clerk. 
35 
00:02:21,220 --> 00:02:26,090 
The command that was issued automatically
by clicking the ToyNode button in 
36 
00:02:26,090 --> 00:02:31,270 
the panel Is a little different than what
we've been using to view our networks. 
37 
00:02:31,270 --> 00:02:34,310 
But they both work essentially
the same with this particular network. 
38 
00:02:35,340 --> 00:02:38,888 
So next let's see what happens if
we do something incorrectly, and 
39 
00:02:38,888 --> 00:02:40,389 
how to correct the mistake. 
40 
00:02:45,521 --> 00:02:46,313 
So to do this, 
41 
00:02:46,313 --> 00:02:50,282 
we'll need to get back to our original
network without the added node. 
42 
00:02:50,282 --> 00:02:54,198 
And maybe the easiest way to do
this is to delete everything and 
43 
00:02:54,198 --> 00:02:56,320 
recreate our original network. 
44 
00:02:57,370 --> 00:02:59,470 
So first I'm going to copy and 
45 
00:02:59,470 --> 00:03:03,060 
paste command to delete all
of the nodes and edges. 
46 
00:03:03,060 --> 00:03:08,440 
You'll find this command in the getting
started video supplementary resources. 
47 
00:03:09,600 --> 00:03:13,260 
It involves the match command and
I'm matching all nodes and 
48 
00:03:13,260 --> 00:03:17,620 
all relationships and I'm deleting
those nodes and relationships. 
49 
00:03:17,620 --> 00:03:22,173 
So let's issue that command,
which should say it's deleted 6 nodes and 
50 
00:03:22,173 --> 00:03:24,532 
6 relationships, and sure enough. 
51 
00:03:24,532 --> 00:03:30,109 
Neo4J's command line has a nice feature of
maintaining a history of the commands and 
52 
00:03:30,109 --> 00:03:34,910 
so in OSX we can use the command+up
arrow to cycle through the commands and 
53 
00:03:34,910 --> 00:03:39,160 
find the original create command
that created our network. 
54 
00:03:39,160 --> 00:03:41,950 
On Windows, it's a CTRL+up arrow command. 
55 
00:03:41,950 --> 00:03:46,320 
So here I've found that command and
I'm going to re-execute it. 
56 
00:03:46,320 --> 00:03:49,100 
And we have our original
network back again. 
57 
00:03:49,100 --> 00:03:50,390 
Let's view that network. 
58 
00:03:52,931 --> 00:03:53,810 
And there it is. 
59 
00:03:55,220 --> 00:04:00,480 
So let's say for whatever reason I am
not quite fully understanding how to add 
60 
00:04:00,480 --> 00:04:05,380 
an additional node to an existing network,
and I want to use the create command. 
61 
00:04:07,190 --> 00:04:09,870 
For example, I might be thinking that 
62 
00:04:09,870 --> 00:04:13,370 
this statement right here will
accomplish the same kind of thing. 
63 
00:04:13,370 --> 00:04:18,930 
By using the create command and
by specifying a ToyNode named Julian. 
64 
00:04:20,070 --> 00:04:23,370 
Well, if we go ahead and
give that a try let's see what happens. 
65 
00:04:23,370 --> 00:04:29,700 
So I'll copy this command and I will paste
it into my command line and execute it. 
66 
00:04:31,500 --> 00:04:36,530 
And it says it's added 2 labels and
created 2 nodes and set 4 properties. 
67 
00:04:37,680 --> 00:04:42,806 
And if we look at this, We 
68 
00:04:42,806 --> 00:04:49,240 
see we've actually got another node named
Julian who has a relationship with Joyce. 
69 
00:04:49,240 --> 00:04:54,970 
So that much is correct, but it's not
the same Julian from the original network. 
70 
00:04:54,970 --> 00:04:56,110 
So how do we undo this? 
71 
00:04:57,200 --> 00:05:03,200 
Well, we'll need to specify the Julian
that has a relationship with Joyce and 
72 
00:05:03,200 --> 00:05:05,760 
delete both the Julian and
the Joyce notes. 
73 
00:05:08,750 --> 00:05:12,160 
So here we'll use the match
command once again. 
74 
00:05:12,160 --> 00:05:18,400 
And we'll identify the node, the ToyNode,
with the name Joyce and any relationship 
75 
00:05:18,400 --> 00:05:24,540 
she has with any other node should be
deleted in addition to that other node. 
76 
00:05:24,540 --> 00:05:26,665 
Let's go ahead and copy this. 
77 
00:05:26,665 --> 00:05:32,000 
And we'll paste it into our
command line and execute. 
78 
00:05:32,000 --> 00:05:34,840 
And it says it deleted 2 nodes and
deleted 1 relationship. 
79 
00:05:35,920 --> 00:05:37,740 
Let's view our network again. 
80 
00:05:39,850 --> 00:05:41,660 
And it's back to normal. 
81 
00:05:41,660 --> 00:05:46,416 
So that's one example of how you can
intuitively figure out how to correct 
82 
00:05:46,416 --> 00:05:47,731 
certain mistakes. 
83 
00:05:53,826 --> 00:05:58,400 
Next we're going to modify
information of an existing node. 
84 
00:05:58,400 --> 00:06:04,790 
So if you remember when we first created
our network, Harry didn't have a job. 
85 
00:06:04,790 --> 00:06:08,370 
So let's go ahead and add a job to Harry. 
86 
00:06:08,370 --> 00:06:08,910 
First of all, 
87 
00:06:08,910 --> 00:06:13,700 
we'll need to actually select
the node by using the match command. 
88 
00:06:14,810 --> 00:06:16,520 
So we'll issue that here and 
89 
00:06:16,520 --> 00:06:20,760 
we'll specify that that node
name must be equal to Harry. 
90 
00:06:22,070 --> 00:06:25,540 
And then we're going to
use the set command and 
91 
00:06:25,540 --> 00:06:30,210 
specify that job will be equal to drummer. 
92 
00:06:30,210 --> 00:06:31,430 
So let's go ahead and copy that. 
93 
00:06:33,450 --> 00:06:35,958 
And we'll paste it and execute it. 
94 
00:06:37,954 --> 00:06:42,410 
And the results that are returned
says that one property has been set. 
95 
00:06:42,410 --> 00:06:45,400 
But let's say that Harry does
more than just play drums. 
96 
00:06:45,400 --> 00:06:48,740 
Let's say that he can
also play lead guitar. 
97 
00:06:48,740 --> 00:06:52,810 
So in this case we will be
adding an additional property 
98 
00:06:52,810 --> 00:06:57,050 
to a property that already exists and
we'll see how Neo4J handles that. 
99 
00:06:57,050 --> 00:07:02,150 
It's a relatively simple modification
we make, in which we're setting 
100 
00:07:02,150 --> 00:07:06,535 
the job key equal to the existing job key 
101 
00:07:06,535 --> 00:07:12,620 
+ an additional value to that key,
in this case, lead guitarist. 
102 
00:07:12,620 --> 00:07:14,303 
So let's copy that statement. 
103 
00:07:16,729 --> 00:07:19,730 
And we'll paste it and execute it. 
104 
00:07:19,730 --> 00:07:24,229 
And we get a similar result returned and
then let's look at our network. 
105 
00:07:27,771 --> 00:07:33,480 
And when we select Harry, sure enough
now we see that he has two jobs. 
106 
00:07:33,480 --> 00:07:34,890 
They're separated by a comma. 
107 
00:07:34,890 --> 00:07:37,660 
One is drummer and one is lead guitarist. 
108 
00:07:37,660 --> 00:07:40,474 
There's much more you can do with Neo4j,
but 
109 
00:07:40,474 --> 00:07:44,373 
we'll want to move on and
learn some more advanced capabilities 
110 
00:07:44,373 --> 00:07:48,212 
that work us closer towards
managing our big data challenges. 
1 
00:00:02,250 --> 00:00:06,207 
Next, we will talk about
Path Analytics using Cypher, 
2 
00:00:06,207 --> 00:00:09,100 
the query language for Neo4j. 
3 
00:00:09,100 --> 00:00:13,450 
Here's the listing of the various
queries we will be demonstrating. 
4 
00:00:26,288 --> 00:00:30,040 
There are some things that Cypher
is capable of doing very well. 
5 
00:00:30,040 --> 00:00:33,350 
And there are other things that
require a little bit of creativity 
6 
00:00:33,350 --> 00:00:36,230 
in order to get the results
that you're looking for. 
7 
00:00:36,230 --> 00:00:38,450 
And we'll show you some examples of that. 
8 
00:00:38,450 --> 00:00:43,718 
It's also important to keep in mind
that because we're working with paths, 
9 
00:00:43,718 --> 00:00:47,285 
which are an official
structure in graph networks, 
10 
00:00:47,285 --> 00:00:50,867 
each one of these examples
includes a new variable. 
11 
00:00:50,867 --> 00:00:54,221 
Which in this case,
we're using the letter p to represent for 
12 
00:00:54,221 --> 00:00:58,410 
the actual path objects that
we're going to be returning. 
13 
00:00:58,410 --> 00:01:03,030 
You may also see the complete word
path instead of just the single letter 
14 
00:01:03,030 --> 00:01:05,580 
p to represent these objects. 
15 
00:01:05,580 --> 00:01:09,950 
We're going to continue to use the data
set of a simple road network that we've 
16 
00:01:09,950 --> 00:01:14,970 
already been using in previous
demonstrations that contains 11 nodes and 
17 
00:01:14,970 --> 00:01:16,000 
14 edges. 
18 
00:01:18,610 --> 00:01:21,180 
So the first query we're
going to demonstrate 
19 
00:01:21,180 --> 00:01:24,040 
is finding a path between specific nodes. 
20 
00:01:24,040 --> 00:01:28,739 
So this would be very much like trying
to find a root between two different 
21 
00:01:28,739 --> 00:01:30,829 
locations in our road network. 
22 
00:01:30,829 --> 00:01:35,079 
In this case, we're going to find
a path between the node named H and 
23 
00:01:35,079 --> 00:01:36,210 
the node named P. 
24 
00:01:37,980 --> 00:01:41,411 
To do this,
we'll use the match command and 
25 
00:01:41,411 --> 00:01:47,162 
we'll say match p which is a variable
we're using to represent our path, 
26 
00:01:47,162 --> 00:01:50,524 
= node a, going through an edge to node c. 
27 
00:01:50,524 --> 00:01:55,766 
There's something slightly different about
this edge, and that is that we're using 
28 
00:01:55,766 --> 00:02:00,573 
a star to represent an arbitrary number
of edges in sequence between a and c, and 
29 
00:02:00,573 --> 00:02:05,980 
we'll be returning all of those edges
that are necessary to complete the path. 
30 
00:02:05,980 --> 00:02:09,530 
And in this case we only want
to return a single path. 
31 
00:02:09,530 --> 00:02:13,440 
So when we submit this query,
we see this path. 
32 
00:02:13,440 --> 00:02:17,240 
It consists of eight nodes and
seven edges. 
33 
00:02:17,240 --> 00:02:19,490 
And it begins with H and ends with P. 
34 
00:02:21,600 --> 00:02:25,980 
Now, another common function we
will use frequently with paths 
35 
00:02:25,980 --> 00:02:28,650 
is finding the length
between two specific nodes. 
36 
00:02:29,800 --> 00:02:35,010 
So we'll issue the same two lines of code
and then we'll use this new command, 
37 
00:02:35,010 --> 00:02:37,950 
length, to return an actual value. 
38 
00:02:37,950 --> 00:02:41,070 
We want to be returning an actual path. 
39 
00:02:41,070 --> 00:02:42,780 
And we just want a single value. 
40 
00:02:42,780 --> 00:02:45,820 
And when we submit this query,
we get the result seven. 
41 
00:02:45,820 --> 00:02:49,800 
And we can see that by visually
inspecting the graph or our seven edges. 
42 
00:02:49,800 --> 00:02:52,310 
But because most networks
are much more complex than this, 
43 
00:02:52,310 --> 00:02:56,130 
we would need to understand the necessary
query to return the length. 
44 
00:02:58,005 --> 00:03:00,160 
And ideally,
in the case of our road network, 
45 
00:03:00,160 --> 00:03:04,710 
we would want to find the shortest
path between those two nodes. 
46 
00:03:04,710 --> 00:03:06,220 
So in this case we're introducing yet 
47 
00:03:06,220 --> 00:03:10,150 
another new command specific
to paths called shortestPath. 
48 
00:03:10,150 --> 00:03:13,149 
We will use the same variable key, and 
49 
00:03:13,149 --> 00:03:18,794 
the same descriptions in our syntax,
in connecting node a with node c. 
50 
00:03:18,794 --> 00:03:23,815 
And in this case, were going to look for
the shortest path between node a and 
51 
00:03:23,815 --> 00:03:29,090 
node p, and we're going to return that
path as well as the length of that path. 
52 
00:03:29,090 --> 00:03:30,680 
And we're just going to
return a single path. 
53 
00:03:31,760 --> 00:03:36,870 
And when we submit this query,
we get a path that's five nodes and 
54 
00:03:36,870 --> 00:03:41,990 
four edges long and if we look at
the text results that are returned, 
55 
00:03:41,990 --> 00:03:46,020 
we'll see a length displayed
in the length column. 
56 
00:03:46,020 --> 00:03:49,980 
And that value is 4, and we can see
that by visually inspecting our graph. 
57 
00:03:52,430 --> 00:03:57,236 
The next query we are going to demonstrate
is intended to illustrate that 
58 
00:03:57,236 --> 00:04:00,010 
there may be more than one shortest path. 
59 
00:04:00,010 --> 00:04:04,967 
And so, we may want to know all of the
possible shortest paths in order to make 
60 
00:04:04,967 --> 00:04:08,300 
a choice between which one we prefer. 
61 
00:04:08,300 --> 00:04:14,290 
So we'll be using a command that is built
into Neo4j called, allShortestPaths. 
62 
00:04:14,290 --> 00:04:18,580 
We'll be issuing a similar query
to what we issued previously, 
63 
00:04:18,580 --> 00:04:24,610 
we're going to try to find all of the
shortest paths between node a and node p. 
64 
00:04:24,610 --> 00:04:30,160 
And instead of the letters a and c, we're
using the terms source and destination. 
65 
00:04:30,160 --> 00:04:34,640 
But the results that we're going to return
will actually be in the form of an array. 
66 
00:04:34,640 --> 00:04:39,640 
We're using a new term, extract,
which is based on the following. 
67 
00:04:39,640 --> 00:04:44,370 
Assuming we have matched our path p,
we want to identify 
68 
00:04:44,370 --> 00:04:48,670 
all of the nodes in p and
extract their names. 
69 
00:04:48,670 --> 00:04:53,790 
And we'll return these names as a listing,
which we'll call the variable paths. 
70 
00:04:53,790 --> 00:04:59,700 
If there's more than one shortest path,
we'll get multiple listings of node names. 
71 
00:04:59,700 --> 00:05:04,120 
So when we submit this query, the results
are listed in the rows display and 
72 
00:05:04,120 --> 00:05:08,130 
we see there are actually
two shortest paths. 
73 
00:05:08,130 --> 00:05:11,643 
They each have five nodes and four edges. 
74 
00:05:13,987 --> 00:05:18,060 
Now, we may want to issue a query
that finds the shortest path but 
75 
00:05:18,060 --> 00:05:22,300 
with particular constraints or
conditions that we place on them. 
76 
00:05:23,640 --> 00:05:28,160 
So in this case we still want
to find the shortest path, but 
77 
00:05:28,160 --> 00:05:33,010 
in this case we may want to constrain
the path length to be greater 
78 
00:05:33,010 --> 00:05:35,530 
than a particular value, in this case 5. 
79 
00:05:35,530 --> 00:05:39,050 
And then, we want to return essentially
the same results that we returned in 
80 
00:05:39,050 --> 00:05:40,480 
the previous query. 
81 
00:05:40,480 --> 00:05:45,180 
But we'll also want to return the length
of the resulting path just so 
82 
00:05:45,180 --> 00:05:47,610 
we have that information conveniently. 
83 
00:05:47,610 --> 00:05:49,310 
So when we issue this command 
84 
00:05:50,360 --> 00:05:55,230 
we get a path with length six
between node A and node P. 
85 
00:05:55,230 --> 00:05:59,870 
So it's clearly longer than the shortest
path that we had found earlier. 
86 
00:06:02,710 --> 00:06:06,940 
Now that we are somewhat familiar
with the two shortest path commands, 
87 
00:06:06,940 --> 00:06:09,750 
the shortest path, or a single path and 
88 
00:06:09,750 --> 00:06:14,730 
the all shortest paths command or multiple
shortest paths, we're going to use 
89 
00:06:14,730 --> 00:06:20,530 
that in a little bit of a creative way
to return the diameter of the graph. 
90 
00:06:20,530 --> 00:06:22,750 
And if you remember from
a previous lecture, 
91 
00:06:22,750 --> 00:06:25,500 
the definition of
the diameter of the graph 
92 
00:06:25,500 --> 00:06:31,170 
is actually the longest continuous
path between two nodes in the graph. 
93 
00:06:31,170 --> 00:06:37,440 
So by using the shortest path command, but
returning all possible shortest paths, 
94 
00:06:37,440 --> 00:06:42,725 
we're actually going to get the longest
path included in those results returned. 
95 
00:06:42,725 --> 00:06:44,400 
Now, if we look carefully at this script, 
96 
00:06:44,400 --> 00:06:47,590 
it is a little different
than our previous scripts. 
97 
00:06:47,590 --> 00:06:52,570 
In this case our match command is
matching all nodes of type MyNode. 
98 
00:06:52,570 --> 00:06:55,272 
We'll assign those to the variable end. 
99 
00:06:55,272 --> 00:07:01,001 
We're also matching the all nodes of type
MyNode and assigning that to variable m. 
100 
00:07:01,001 --> 00:07:02,570 
So these matches are the same. 
101 
00:07:02,570 --> 00:07:07,543 
But we want to place a constraint
such that the nodes in n are not 
102 
00:07:07,543 --> 00:07:09,933 
the same as the nodes in m, and 
103 
00:07:09,933 --> 00:07:15,981 
then we want to find all of the shortest
paths between unique nodes in n and m. 
104 
00:07:15,981 --> 00:07:21,410 
And return the names of those nodes as
well as the length of that resulting path. 
105 
00:07:21,410 --> 00:07:24,470 
And the trick is to use
the command order by. 
106 
00:07:24,470 --> 00:07:29,360 
And so for those of you who are familiar
already with SQL query language, 
107 
00:07:29,360 --> 00:07:31,030 
you'll recognize order by. 
108 
00:07:31,030 --> 00:07:34,610 
You'll also recognize the descend command. 
109 
00:07:34,610 --> 00:07:40,310 
So if we order the resulting paths
by their length in descending order, 
110 
00:07:40,310 --> 00:07:45,470 
and only return 1, that path should
actually be the longest path. 
111 
00:07:45,470 --> 00:07:47,420 
And that's equal to
the diameter of the graph. 
112 
00:07:48,500 --> 00:07:52,440 
So when we submit this query,
here's the results that we get. 
113 
00:07:52,440 --> 00:07:57,349 
We get a path between node e and
node l with length severn. 
114 
00:07:57,349 --> 00:08:02,809 
Or maybe it occurs to you that maybe this
is not the only diameter of the graph, 
115 
00:08:02,809 --> 00:08:05,340 
the only path with length of seven. 
116 
00:08:06,580 --> 00:08:13,488 
So we can modify our query just a little
bit and change the limit from one to five. 
117 
00:08:13,488 --> 00:08:16,029 
And we'll see the results. 
118 
00:08:16,029 --> 00:08:18,240 
And sure enough,
we actually get five paths. 
119 
00:08:19,730 --> 00:08:22,650 
And 3 of those have length 7. 
120 
00:08:22,650 --> 00:08:26,757 
So there are actually three
distinct paths which qualify as 
121 
00:08:26,757 --> 00:08:29,391 
a diameter of this particular graph. 
1 
00:00:08,130 --> 00:00:12,340 
So up until now we've been
calculating path length 
2 
00:00:12,340 --> 00:00:16,710 
based on the number of hops between
our beginning node and our end node. 
3 
00:00:16,710 --> 00:00:21,590 
This is roughly equivalent to counting
the number of towns between one town and 
4 
00:00:21,590 --> 00:00:22,820 
another town. 
5 
00:00:22,820 --> 00:00:25,710 
But it doesn't really get at
the value that is usually of 
6 
00:00:25,710 --> 00:00:27,110 
greatest importance to us. 
7 
00:00:27,110 --> 00:00:32,220 
And that is the actual distance between
one location and another location. 
8 
00:00:32,220 --> 00:00:37,220 
Which is found in the values that we've
assigned to the edges between the nodes. 
9 
00:00:37,220 --> 00:00:41,980 
So in this next example, we're going
to perform that kind of a calculation. 
10 
00:00:41,980 --> 00:00:45,430 
So the first two lines of code
we're already fairly familiar with. 
11 
00:00:45,430 --> 00:00:49,312 
We're matching a path between node a and
node c, 
12 
00:00:49,312 --> 00:00:53,396 
where the first node is H,
and the second node is P. 
13 
00:00:53,396 --> 00:00:56,950 
And this third line of code
is also fairly familiar. 
14 
00:00:56,950 --> 00:01:01,940 
We're extracting the names of the nodes
and the path that's been returned, and 
15 
00:01:01,940 --> 00:01:06,140 
we're returning a listing of those
names as well as a length of the path. 
16 
00:01:07,470 --> 00:01:10,870 
All of that is being returned
as the variable pathLength. 
17 
00:01:10,870 --> 00:01:14,680 
We've added a third element
to our return statement, and 
18 
00:01:14,680 --> 00:01:16,820 
that is using the reduce statement. 
19 
00:01:16,820 --> 00:01:21,130 
So what we're doing here is,
the purpose of the reduce statement, 
20 
00:01:21,130 --> 00:01:25,070 
takes a set of values and
reduces them down to a single value. 
21 
00:01:26,270 --> 00:01:31,900 
So in this line of code we begin by
setting a variable s equal to 0. 
22 
00:01:31,900 --> 00:01:36,200 
And then we define a variable E, which
represents the set of relationships in 
23 
00:01:36,200 --> 00:01:39,389 
a path that's returned, or
in other words, the edges. 
24 
00:01:39,389 --> 00:01:42,842 
And we pass that into this variable s,
and add to it, 
25 
00:01:42,842 --> 00:01:46,860 
the value of the distance that
we've assigned to that edge. 
26 
00:01:48,540 --> 00:01:53,060 
So we're performing
an aggregate calculation. 
27 
00:01:53,060 --> 00:01:58,340 
And returning the final results
to a variable called pathDist. 
28 
00:01:58,340 --> 00:02:00,193 
And we're limiting that
results to a single value. 
29 
00:02:00,193 --> 00:02:05,573 
And so when we do this, we should
get a value that is more indicative 
30 
00:02:05,573 --> 00:02:11,460 
of the actual distance between
our source and our destination. 
31 
00:02:11,460 --> 00:02:12,959 
And so here's the results. 
32 
00:02:12,959 --> 00:02:15,977 
The path itself, as we know,
begins in H and ends in P. 
33 
00:02:15,977 --> 00:02:21,376 
And it has a pathLength of 7,
but it has a pathDist of 39. 
34 
00:02:21,376 --> 00:02:25,864 
So we could interpret this to mean
that even though there are 6 towns 
35 
00:02:25,864 --> 00:02:29,408 
between the source town and
the destination town, or 
36 
00:02:29,408 --> 00:02:34,310 
a pathLength of 7, the actual distance
in miles would be a value of 39. 
37 
00:02:36,446 --> 00:02:38,860 
So with that we can apply
Dijkstra's algorithm. 
38 
00:02:39,950 --> 00:02:46,690 
So here I'm going to match the node with
the name A, and the node with the name P. 
39 
00:02:46,690 --> 00:02:52,461 
And we're going to find the shortest
path in terms of hops from A to P. 
40 
00:02:52,461 --> 00:02:56,050 
And we'll set that equal
to the variable path. 
41 
00:02:56,050 --> 00:03:01,722 
Then we'll perform a reduce command,
and set the variable dist = 0. 
42 
00:03:01,722 --> 00:03:03,602 
And we'll go through, and 
43 
00:03:03,602 --> 00:03:08,960 
sum all of the distances of each
of the edges in our shortest path. 
44 
00:03:08,960 --> 00:03:11,480 
And return that value as a distance. 
45 
00:03:11,480 --> 00:03:15,060 
And we'll also return the path variable. 
46 
00:03:15,060 --> 00:03:20,870 
So remember, this is not the path in
our network with the least weights. 
47 
00:03:20,870 --> 00:03:27,260 
It is the weight of the shortest
path based on numbers of hops. 
48 
00:03:27,260 --> 00:03:31,940 
Now that's an inherent feature of
the shortest path command in Cipher. 
49 
00:03:31,940 --> 00:03:33,470 
So here is the path that's returned. 
50 
00:03:33,470 --> 00:03:37,006 
It's a five node path with four edges and 
51 
00:03:37,006 --> 00:03:42,730 
the total sum of the weights of
those edges sums to a value of 22. 
52 
00:03:44,243 --> 00:03:48,493 
So in our previous query,
we specified that we wanted a match for 
53 
00:03:48,493 --> 00:03:52,240 
the source node and the destination node. 
54 
00:03:52,240 --> 00:03:57,670 
But if I don't specify my destination
node, I can apply Dijkstra's single 
55 
00:03:57,670 --> 00:04:02,730 
source shortest path algorithm
from node A to any other node. 
56 
00:04:03,870 --> 00:04:08,637 
So when we apply this query,
the results displayed consist of 
57 
00:04:08,637 --> 00:04:13,233 
the actual original path from
A to P with a distance of 22. 
58 
00:04:13,233 --> 00:04:18,318 
And we'll see a display of all of
the intermediate paths generated in 
59 
00:04:18,318 --> 00:04:23,240 
the process, all the way down to
a single edge path between A and C. 
60 
00:04:24,750 --> 00:04:30,830 
So just to reiterate, what we've
calculated is the shortest hop path 
61 
00:04:30,830 --> 00:04:36,000 
with the weights added, the sum of
the weights of the edges in that path. 
62 
00:04:36,000 --> 00:04:39,310 
This is not the least weight
path of the entire network. 
63 
00:04:42,730 --> 00:04:45,470 
Okay, so let's switch gears for a moment. 
64 
00:04:45,470 --> 00:04:50,320 
As we learned in one of our previous
lectures, we can extract a subset 
65 
00:04:50,320 --> 00:04:55,070 
of nodes and edges from a particular
graph for various reasons. 
66 
00:04:55,070 --> 00:04:59,210 
Let's say for example that we want
to avoid a particular town or 
67 
00:04:59,210 --> 00:05:01,680 
a particular area where
there might be congestion. 
68 
00:05:01,680 --> 00:05:05,040 
And this would be represented
by one of our nodes. 
69 
00:05:05,040 --> 00:05:08,110 
So we're going to perform a similar
match as we've done in the past, 
70 
00:05:08,110 --> 00:05:14,960 
where we're going to match any node n with
any node m, with a two edge between them. 
71 
00:05:14,960 --> 00:05:19,243 
But we want to apply an additional
constrained in which none of the n nodes 
72 
00:05:19,243 --> 00:05:21,150 
are going to include the node D. 
73 
00:05:21,150 --> 00:05:26,920 
And none of the m nodes are going
to include the node D as well. 
74 
00:05:26,920 --> 00:05:29,030 
And then we'll return the resulting graph. 
75 
00:05:30,290 --> 00:05:34,500 
So when we do that, we get a graph but
looks very much as we might expect. 
76 
00:05:34,500 --> 00:05:38,958 
Very similar to our previous graph,
but it is missing node D, so 
77 
00:05:38,958 --> 00:05:43,183 
it only has ten nodes and
it's now been reduced to ten edges. 
78 
00:05:46,005 --> 00:05:50,659 
So now let's say we want to calculate
the shortest path over there graph that we 
79 
00:05:50,659 --> 00:05:53,590 
just returned in the previous query. 
80 
00:05:53,590 --> 00:05:59,690 
So in this case, we're going to match the
shortest path between node A and node P. 
81 
00:05:59,690 --> 00:06:03,830 
But in the second line, we want to
issue sort of a negative statement 
82 
00:06:03,830 --> 00:06:08,530 
in which the resulting list of
node names that we extract using 
83 
00:06:08,530 --> 00:06:12,780 
the extract statement
cannot contain the node D. 
84 
00:06:14,020 --> 00:06:17,710 
And then we'll return that path and
the length of that path. 
85 
00:06:17,710 --> 00:06:22,190 
So when we issue this command,
here's our resulting path. 
86 
00:06:22,190 --> 00:06:25,730 
It's a five node path of length four. 
87 
00:06:25,730 --> 00:06:29,540 
As we recall from one of our earlier
queries where we were trying to calculate 
88 
00:06:29,540 --> 00:06:33,500 
all of the shortest paths,
we returned two paths. 
89 
00:06:33,500 --> 00:06:37,594 
One path that contained D, and this is
the second path, which did not contain D. 
90 
00:06:40,008 --> 00:06:43,538 
So we can make this a little complicated. 
91 
00:06:43,538 --> 00:06:48,728 
Instead of avoiding a single node in
our resulting path, we're looking for 
92 
00:06:48,728 --> 00:06:54,090 
a graph that doesn't contain the immediate
neighborhood of a specific node. 
93 
00:06:54,090 --> 00:06:58,770 
This means all of the nearest, or
the first neighbors of a specific node. 
94 
00:06:58,770 --> 00:07:03,390 
So in this case we're going
to match the same node D, and 
95 
00:07:03,390 --> 00:07:07,010 
all edges between D, and any other node. 
96 
00:07:07,010 --> 00:07:11,590 
And then we are going to issue
a collect command to collect all of 
97 
00:07:11,590 --> 00:07:13,758 
the distinct neighbors of D. 
98 
00:07:13,758 --> 00:07:18,654 
And we'll apply a constraint to that,
in which the returned 
99 
00:07:18,654 --> 00:07:23,960 
list of neighbors cannot contain
the node with the name D. 
100 
00:07:23,960 --> 00:07:27,050 
Likewise, the neighbors list for 
101 
00:07:27,050 --> 00:07:31,070 
the target nodes,
can also not contain the node D. 
102 
00:07:31,070 --> 00:07:35,800 
And when we submit this command,
we see a network that looks like this. 
103 
00:07:35,800 --> 00:07:38,010 
Five nodes and four edges. 
104 
00:07:38,010 --> 00:07:40,130 
And that seems to makes sense. 
105 
00:07:40,130 --> 00:07:45,170 
Node D isn't in the network,
nor are its first neighbors. 
106 
00:07:45,170 --> 00:07:47,590 
But if you recall the original network, 
107 
00:07:47,590 --> 00:07:53,080 
there may be a peculiar result that you
might find a little bit disconcerting. 
108 
00:07:53,080 --> 00:07:56,112 
So let's look at our original network. 
109 
00:07:56,112 --> 00:07:59,720 
Now here's the node D, and
here are all it's nearest neighbors. 
110 
00:07:59,720 --> 00:08:02,360 
So these are the forbidden
neighbors that we want to 
111 
00:08:02,360 --> 00:08:04,040 
remove from our resulting graph. 
112 
00:08:05,100 --> 00:08:07,360 
And we seemed to have
done that successfully. 
113 
00:08:07,360 --> 00:08:11,700 
These are the five nodes that
are retained in the resulting graph, but 
114 
00:08:11,700 --> 00:08:12,890 
there's a node out here. 
115 
00:08:12,890 --> 00:08:19,440 
The node p, which seems to be neglected or
not handled in the results. 
116 
00:08:19,440 --> 00:08:21,520 
It's not a first neighbor of D. 
117 
00:08:22,620 --> 00:08:26,730 
So it, in some ways arguably should
be returned in our results, but 
118 
00:08:26,730 --> 00:08:30,750 
it's not part of the connected
graph that we saw returned. 
119 
00:08:30,750 --> 00:08:36,580 
This is one area in which Cipher does
not handle these situations by default. 
120 
00:08:36,580 --> 00:08:41,640 
So we'll need to supplement our
query with an additional query. 
121 
00:08:41,640 --> 00:08:46,848 
In this case, the node P was a leaf node,
so we want to make sure that 
122 
00:08:46,848 --> 00:08:52,340 
not only matching the nodes that
conform to these constraints above. 
123 
00:08:53,400 --> 00:08:58,890 
But we also want to include the node or
any nodes which are leaf nodes, 
124 
00:08:58,890 --> 00:09:04,200 
which may also be arguably part of the
results that you expect to be returned. 
125 
00:09:04,200 --> 00:09:07,120 
Now in our network,
we do have one root node, 
126 
00:09:07,120 --> 00:09:10,990 
but it doesn't impact the results
in this particular query. 
127 
00:09:10,990 --> 00:09:13,420 
In the interests of being complete, for 
128 
00:09:13,420 --> 00:09:18,002 
any network, most networks being much more
complex than the one we're working with. 
129 
00:09:18,002 --> 00:09:23,450 
We'd want to take into account not only
those leaf nodes that might be left out, 
130 
00:09:23,450 --> 00:09:25,700 
but also any root notes
that might be left out. 
131 
00:09:28,300 --> 00:09:33,400 
And finally, our last query
example extends the previous query 
132 
00:09:33,400 --> 00:09:37,670 
to find the graph which doesn't
contain a selective neighborhood, 
133 
00:09:37,670 --> 00:09:42,050 
in this case,
the two neighborhood of a particular node. 
134 
00:09:42,050 --> 00:09:43,571 
In this example we're
going to use the node F. 
135 
00:09:43,571 --> 00:09:50,020 
And we want to eliminate all of
the second neighbors of that node. 
136 
00:09:50,020 --> 00:09:54,590 
So initially we match all of those
nodes that are second neighbors of F, 
137 
00:09:54,590 --> 00:09:56,400 
including F itself. 
138 
00:09:56,400 --> 00:10:00,750 
And we'll place those in
a variable called MyList. 
139 
00:10:00,750 --> 00:10:03,850 
Then we go back through the network and
match all of the nodes and 
140 
00:10:03,850 --> 00:10:08,690 
edges, where the source nodes are not
part of the nodes in the MyList and 
141 
00:10:08,690 --> 00:10:11,740 
the target nodes are not
contained in MyList. 
142 
00:10:11,740 --> 00:10:13,940 
And then we return those nodes and edges. 
143 
00:10:15,580 --> 00:10:16,990 
And here's the resulting graph. 
144 
00:10:19,020 --> 00:10:24,490 
It does not contain F or
its first or second neighbors. 
145 
00:10:24,490 --> 00:10:31,574 
If we scroll down to look at the original
graph, here's node F, nodes H, 
146 
00:10:31,574 --> 00:10:37,080 
J, C, A and L are all the first and
second neighbors of F. 
147 
00:10:37,080 --> 00:10:42,070 
So those should be eliminated from the
graph that gets returned from our results. 
148 
00:10:42,070 --> 00:10:43,600 
And sure enough, 
149 
00:10:43,600 --> 00:10:49,520 
this subset of nodes represents the
results that were returned from our query. 
150 
00:10:49,520 --> 00:10:54,040 
It consists of nodes B, D, E, G, and P. 
151 
00:10:54,040 --> 00:10:57,440 
And so this concludes our review
of some of the more advanced 
152 
00:10:57,440 --> 00:10:59,490 
path analytics queries. 
153 
00:10:59,490 --> 00:11:01,645 
We were using a simple network, but 
154 
00:11:01,645 --> 00:11:05,310 
we're providing additional data
sets that are much larger and 
155 
00:11:05,310 --> 00:11:09,770 
present a more realistic challenge
in applying pathanalythics queries. 
1 
00:00:00,750 --> 00:00:05,130 
Hello everyone and welcome to this week's
module on graph analytics with Neo4j 
2 
00:00:05,130 --> 00:00:07,460 
using the Cypher query language. 
3 
00:00:07,460 --> 00:00:10,950 
I'm Jeff Sale and I'll be your
instructor for this series of lessons. 
4 
00:00:10,950 --> 00:00:14,360 
I've been an instructional designer at
the San Diego Supercomputer Center for 
5 
00:00:14,360 --> 00:00:19,090 
more than ten years, but I've also had
a passion for scientific visualization and 
6 
00:00:19,090 --> 00:00:22,940 
visual analytics in one form or
another for over two decades. 
7 
00:00:22,940 --> 00:00:26,010 
And I'm very excited about this
opportunity to introduce you to this 
8 
00:00:26,010 --> 00:00:29,590 
free and very powerful graph
analytics tool called Neo4j. 
9 
00:00:30,770 --> 00:00:33,990 
First we realized that many of you
may not have the systems capable of 
10 
00:00:33,990 --> 00:00:37,190 
pushing the boundaries of
Neo4j's performance limits. 
11 
00:00:37,190 --> 00:00:40,620 
Plus the fact that many of you are fitting
this course into your already busy 
12 
00:00:40,620 --> 00:00:45,680 
schedules means we'll be working with data
sets, which will load into Neo4j and can 
13 
00:00:45,680 --> 00:00:51,690 
be analyze in a reasonable length of time,
on the order of minutes not hours or days. 
14 
00:00:51,690 --> 00:00:55,020 
However you can be sure that Neo4j
is capable of processing and 
15 
00:00:55,020 --> 00:00:59,730 
analyzing extremely complex graph networks
consisting of millions of nodes and 
16 
00:00:59,730 --> 00:01:00,360 
relationships. 
17 
00:01:01,580 --> 00:01:05,650 
This module consists of a series of
hands-on demonstrations with Neo4j, 
18 
00:01:05,650 --> 00:01:08,780 
which begin with examples of
some basic cypher queries 
19 
00:01:08,780 --> 00:01:12,590 
that soon progress to some of
the more advanced cypher queries. 
20 
00:01:12,590 --> 00:01:16,440 
We'll begin by using a relatively simple
graph representing a road network, but 
21 
00:01:16,440 --> 00:01:19,260 
we'll also use much larger and
more complex data sets, 
22 
00:01:19,260 --> 00:01:23,280 
including sociological data on
global terrorist groups and 
23 
00:01:23,280 --> 00:01:27,340 
genetics data on associations and
interactions between genes. 
24 
00:01:27,340 --> 00:01:30,910 
These data sets are in fact sub
sets of much larger data sets and 
25 
00:01:30,910 --> 00:01:35,290 
we're making both of the sub sets and
complete data sets available for download. 
26 
00:01:35,290 --> 00:01:38,250 
Once you become comfortable working
with the smaller data sets, 
27 
00:01:38,250 --> 00:01:40,760 
we encourage you to explore
the larger sets on your own. 
28 
00:01:41,810 --> 00:01:45,020 
Finally you'll notice that each
video is accompanied by a text file 
29 
00:01:45,020 --> 00:01:48,560 
containing all of the code used
in the video demonstrations. 
30 
00:01:48,560 --> 00:01:52,680 
These files include dozens of sample
scripts, written in cypher, designed to 
31 
00:01:52,680 --> 00:01:57,250 
make it easy for you to learn, not only
basic cypher cards, but also queries which 
32 
00:01:57,250 --> 00:02:01,858 
focus on more advanced methods such
pathenoids and connectivity analytics. 
33 
00:02:03,040 --> 00:02:06,690 
When you're finished with this module
you'll be able to write cypher scripts to 
34 
00:02:06,690 --> 00:02:10,900 
import and
analyze your own data using Neo4j. 
35 
00:02:10,900 --> 00:02:13,682 
So thank you for
enrolling in this course and let's get 
36 
00:02:13,682 --> 00:02:17,991 
started doing some real graph analytics
with Neo4j and the Cypher query language. 
1 
00:00:00,570 --> 00:00:03,984 
A parallel computation is at
the heart of big data computing. 
2 
00:00:03,984 --> 00:00:08,427 
However, to specify what becomes parallel,
we usually think of a conceptual model 
3 
00:00:08,427 --> 00:00:11,760 
of parallel computation often called,
a programming model. 
4 
00:00:13,630 --> 00:00:16,990 
A parallel programming model is
a way to specify abstractly, 
5 
00:00:16,990 --> 00:00:18,280 
how a parallel program will run. 
6 
00:00:19,310 --> 00:00:21,720 
Naturally for a program to be parallel, 
7 
00:00:21,720 --> 00:00:26,040 
there must be a number of
concurrently operating processes. 
8 
00:00:26,040 --> 00:00:28,540 
But how do these processes communicate and
exchange data? 
9 
00:00:29,760 --> 00:00:32,280 
How do they decide,
when to communicate with each other? 
10 
00:00:33,560 --> 00:00:35,320 
Further, what exactly is done in parallel? 
11 
00:00:36,550 --> 00:00:38,870 
To think of the first question. 
12 
00:00:38,870 --> 00:00:42,870 
Two processes communicate
data by sharing memory. 
13 
00:00:44,490 --> 00:00:48,150 
Indeed, there are architectures in which
all memory in multiple machines can be 
14 
00:00:48,150 --> 00:00:51,480 
made to virtually look like,
one large addressable memory space. 
15 
00:00:52,990 --> 00:00:54,840 
However, two processes, 
16 
00:00:54,840 --> 00:00:58,570 
we also communicate by passing
messages to one another. 
17 
00:00:58,570 --> 00:01:01,210 
Either, directly from one
process to another, or 
18 
00:01:01,210 --> 00:01:04,690 
through a common message carrying pipe,
often called a message bus. 
19 
00:01:06,750 --> 00:01:09,130 
The second question can
also have multiple answers. 
20 
00:01:10,250 --> 00:01:15,050 
Two of the most common ways of achieving
parallelism are pass parallelism and 
21 
00:01:15,050 --> 00:01:16,790 
data parallels. 
22 
00:01:16,790 --> 00:01:21,860 
In task parallelism, a large task can
be decomposed into multiple sub-tasks, 
23 
00:01:21,860 --> 00:01:23,380 
each of which can be run concurrently. 
24 
00:01:24,460 --> 00:01:29,690 
In data parallelism, the data can be
partitioned into many smaller fragments 
25 
00:01:29,690 --> 00:01:32,850 
and operation can run on each partition,
independent of the others. 
26 
00:01:34,480 --> 00:01:38,040 
Typically, these partial operations
have then synchronized and 
27 
00:01:38,040 --> 00:01:41,500 
partially process data combined
to produce a full answer. 
28 
00:01:42,770 --> 00:01:44,870 
Many parallel data management systems, 
29 
00:01:44,870 --> 00:01:46,760 
operate a partition due
to parallel manner. 
30 
00:01:48,240 --> 00:01:51,490 
We need to remember that task
parallelism is somewhat independent of 
31 
00:01:51,490 --> 00:01:52,780 
data parallelism. 
32 
00:01:52,780 --> 00:01:56,440 
And it is possible to have both problems
of parallelism in a programming model. 
33 
00:01:57,540 --> 00:02:00,650 
It is important to emphasize
the issue of a programming model, 
34 
00:02:00,650 --> 00:02:04,320 
should be not be confused with
the issue of a programming language. 
35 
00:02:05,405 --> 00:02:08,295 
A programming language is independent
of the programming model. 
36 
00:02:08,295 --> 00:02:10,765 
And therefore,
a programming model can be implemented 
37 
00:02:10,765 --> 00:02:12,185 
in several different languages. 
38 
00:02:13,425 --> 00:02:17,155 
As I mentioned, the programming model
we are going to consider is BSP. 
39 
00:02:18,425 --> 00:02:22,350 
BSP wasn't initially created for
graph computation. 
40 
00:02:22,350 --> 00:02:24,550 
It was thought of as
a parallel computing model, 
41 
00:02:24,550 --> 00:02:28,740 
that will bridge the gap between software
models of parallel computation, and 
42 
00:02:28,740 --> 00:02:31,890 
hardware capabilities for
supporting parallelism. 
43 
00:02:31,890 --> 00:02:34,100 
The basic idea of BSP is as follows. 
44 
00:02:35,430 --> 00:02:37,880 
There are number of processors. 
45 
00:02:37,880 --> 00:02:41,860 
Each processor can perform local
computation, using its own local memory. 
46 
00:02:43,640 --> 00:02:49,020 
There's a router, which can serve to pass
a message from any processor to any other. 
47 
00:02:50,090 --> 00:02:53,080 
When one pair of nodes
are exchanging messages, 
48 
00:02:53,080 --> 00:02:55,420 
another third node can
still perform computation. 
49 
00:02:58,000 --> 00:03:01,060 
There's a facility that can
synchronize the state of 
50 
00:03:01,060 --> 00:03:02,860 
all auto-substative processes. 
51 
00:03:04,010 --> 00:03:06,880 
This synchronize may either
happen periodically, 
52 
00:03:06,880 --> 00:03:12,100 
at intervals of L time units or
there may be another way of specifying, 
53 
00:03:12,100 --> 00:03:14,490 
when this synchronization
is going to happen. 
54 
00:03:14,490 --> 00:03:19,270 
But when it does, all processors affected
by it, will come to a consistent state. 
55 
00:03:20,490 --> 00:03:25,420 
When synchronization is performed,
a fresh round of computation can start. 
56 
00:03:25,420 --> 00:03:26,905 
We call this. 
57 
00:03:26,905 --> 00:03:28,105 
Synchronization point. 
58 
00:03:28,105 --> 00:03:29,815 
Barrier synchronization. 
59 
00:03:29,815 --> 00:03:33,575 
Because all executed processes
must reach this barrier point, 
60 
00:03:33,575 --> 00:03:36,935 
before the next step of
processing can continue. 
61 
00:03:36,935 --> 00:03:40,935 
A BSP program is broken up into
a sequence stop supersteps. 
62 
00:03:42,325 --> 00:03:46,540 
In each superstep, each processor
will get the data, if needed. 
63 
00:03:46,540 --> 00:03:49,040 
Performance computation if needed and 
64 
00:03:49,040 --> 00:03:51,180 
then, exchange data
with the right partner. 
65 
00:03:52,360 --> 00:03:56,260 
Once all the nodes are done,
the system helps to synchronize. 
66 
00:03:56,260 --> 00:03:57,680 
Then, the next round starts. 
67 
00:03:58,760 --> 00:04:03,140 
Each processor can determine,
if it needs to compute or exchange data. 
68 
00:04:03,140 --> 00:04:05,300 
If not, it will make itself inactive. 
69 
00:04:06,760 --> 00:04:10,540 
If required later, a processor can
be woken up to be active again. 
70 
00:04:12,390 --> 00:04:15,700 
When all processors are inactive,
the computation stops. 
71 
00:04:17,320 --> 00:04:20,720 
In applying BSP model to graphs,
we make a few assumptions. 
72 
00:04:21,830 --> 00:04:24,850 
We assume that a processor
is synonymous with a vertex. 
73 
00:04:26,100 --> 00:04:30,130 
So for
a processor can only send messages to or 
74 
00:04:30,130 --> 00:04:32,810 
receive from, its neighboring processes. 
75 
00:04:34,170 --> 00:04:39,920 
We also assume, a vertex has an ID and
possibly a complex value. 
76 
00:04:39,920 --> 00:04:42,270 
And an edge,
we also have an idea and fact. 
77 
00:04:43,570 --> 00:04:46,170 
Each vertex knows,
which edges it's connected to. 
78 
00:04:48,630 --> 00:04:52,918 
We cannot think of a computation
as a vertex centered task. 
79 
00:04:52,918 --> 00:04:55,260 
We shall [INAUDIBLE] what this means. 
80 
00:04:57,640 --> 00:04:59,760 
In a now famous paper from Google. 
81 
00:04:59,760 --> 00:05:03,030 
This programming model was called,
think like a vertex. 
82 
00:05:04,120 --> 00:05:07,620 
Well to think like a vertex,
we need to know what a vertex can do. 
83 
00:05:08,970 --> 00:05:10,950 
Here's a list of actions,
a vertex can take. 
84 
00:05:13,030 --> 00:05:14,460 
The first one is easy. 
85 
00:05:14,460 --> 00:05:16,020 
A vertex can find its own identifier. 
86 
00:05:17,680 --> 00:05:22,010 
The second operation is to get or
set the value of the node. 
87 
00:05:22,010 --> 00:05:26,450 
This operation may be a little involved,
if the value is a complex data object. 
88 
00:05:26,450 --> 00:05:29,060 
For our purposes,
we'll assume the value is a scalar. 
89 
00:05:30,490 --> 00:05:32,620 
Next, a node can ask for 
90 
00:05:32,620 --> 00:05:36,370 
its own edges, the result of
the operation is a set of edge objects. 
91 
00:05:37,740 --> 00:05:39,720 
A node may also count its edges. 
92 
00:05:40,780 --> 00:05:43,650 
Since we are referring to
outgoing edges throughout, 
93 
00:05:43,650 --> 00:05:45,160 
this is the out degree of the vertex. 
94 
00:05:46,390 --> 00:05:47,170 
Recognize that, 
95 
00:05:47,170 --> 00:05:52,210 
this means a vertex does not natively
have access to its incident notes. 
96 
00:05:54,220 --> 00:05:57,830 
However, it does have control
of the outgoing edges. 
97 
00:05:57,830 --> 00:05:59,990 
So it can get inside the edge values. 
98 
00:06:01,620 --> 00:06:04,470 
There may be two different
ways of specifying an edge. 
99 
00:06:04,470 --> 00:06:07,280 
The edge we have an ID
that the node can get. 
100 
00:06:07,280 --> 00:06:11,460 
Passively, more commonly, an edge is
identified the vertex of targets. 
101 
00:06:12,500 --> 00:06:16,520 
So in our diagram V1 lasts for
the edge, targeting V4. 
102 
00:06:18,950 --> 00:06:24,240 
So in the situation like v3 and v5, we're
there are multiple edges between v3 and 
103 
00:06:24,240 --> 00:06:30,370 
v5, the source v3 can ask for
the values of all edges going to v5. 
104 
00:06:32,520 --> 00:06:35,990 
The operate operation can add or
remove an edge of a vertex. 
105 
00:06:37,270 --> 00:06:43,240 
Finally, since the vertices are processes,
they can start or stop computing. 
106 
00:06:43,240 --> 00:06:46,890 
Typically a node wakes up, if it
receives a message from another node. 
107 
00:06:46,890 --> 00:06:51,450 
Now in comparison to a vertex,
an edge can do far less. 
108 
00:06:52,760 --> 00:06:57,220 
It can get its own ID if the system
allows edge ID's, it can set and 
109 
00:06:57,220 --> 00:07:02,380 
retrieve its own values, and it can get
the ID of the node its pointing to. 
110 
00:07:02,380 --> 00:07:02,880 
That's it. 
111 
00:07:04,080 --> 00:07:07,500 
Now, we still have not defined,
how to think like a vertex. 
112 
00:07:07,500 --> 00:07:09,470 
That's what we'll do next, 
113 
00:07:09,470 --> 00:07:13,330 
using an example that we have
seen several times before. 
114 
00:07:13,330 --> 00:07:17,700 
It's Dijkstra's single source
shortest path, SSSP, algorithm. 
115 
00:07:19,000 --> 00:07:21,120 
We have seen this algorithm before. 
116 
00:07:21,120 --> 00:07:25,710 
But now, we'll show how to compute
it in a parallel setting using BSP. 
117 
00:07:27,140 --> 00:07:30,320 
Here is the edge value,
that's the weight of the edge. 
118 
00:07:31,710 --> 00:07:33,830 
This is known before the algorithm starts. 
119 
00:07:35,010 --> 00:07:39,180 
Each vertex,
runs the exact same routine concurrently. 
120 
00:07:40,200 --> 00:07:41,530 
Each vertex asks. 
121 
00:07:41,530 --> 00:07:44,500 
1, is it super step zero? 
122 
00:07:44,500 --> 00:07:47,810 
2, if yes, 
123 
00:07:47,810 --> 00:07:52,810 
then if this is a source vertex,
it sets the value to zero. 
124 
00:07:53,870 --> 00:07:56,930 
Else, it sets the value to infinity,
which is a large number. 
125 
00:07:58,310 --> 00:07:59,610 
The source vertex, 
126 
00:07:59,610 --> 00:08:03,450 
propagates its edge value to the nodes
at the other end of the edges. 
127 
00:08:03,450 --> 00:08:04,250 
Just the source vertex. 
128 
00:08:05,380 --> 00:08:06,650 
All other vertices are quiet. 
129 
00:08:08,300 --> 00:08:11,170 
The propagation process works like this. 
130 
00:08:11,170 --> 00:08:15,710 
A vertex gets its own value,
which for the source vertex is zero. 
131 
00:08:15,710 --> 00:08:21,990 
It gets its edges, for each edge,
it gets the value of the edge, 
132 
00:08:21,990 --> 00:08:27,020 
adds it to its own value and sends
the result to the end point of the edge. 
133 
00:08:28,080 --> 00:08:31,840 
The blue numbers indicate
that the messages are sent. 
134 
00:08:31,840 --> 00:08:33,410 
All vertices go to halt. 
135 
00:08:33,410 --> 00:08:37,180 
That is, they now have hit
a synchronization barrier. 
136 
00:08:38,290 --> 00:08:41,590 
Notice, that the receiving nodes
do not look at the messages yet. 
137 
00:08:42,590 --> 00:08:46,860 
It's the system's job to ensure that
the messages are available to these nodes 
138 
00:08:46,860 --> 00:08:47,810 
at the next superstep. 
139 
00:08:49,690 --> 00:08:54,520 
All nodes who have received messages
wake up and read the messages. 
140 
00:08:54,520 --> 00:08:58,530 
If a node receives multiple messages,
it picks the minimum. 
141 
00:08:58,530 --> 00:09:04,140 
In our case, the two active nodes
have received only one value each. 
142 
00:09:04,140 --> 00:09:08,720 
We have for the sake of convenience,
colored the processed edges in yellow. 
143 
00:09:08,720 --> 00:09:11,250 
This is just for
visualization purposes in this example. 
144 
00:09:12,470 --> 00:09:17,690 
Now, it compares this band for
a minimum value, to its own value. 
145 
00:09:17,690 --> 00:09:24,790 
And if its own value is greater, it sets
its own value with the minimum value. 
146 
00:09:24,790 --> 00:09:29,480 
In our case, both notes set their value
to that, of the incoming message. 
147 
00:09:30,800 --> 00:09:33,400 
The same propagation routine works again. 
148 
00:09:33,400 --> 00:09:36,320 
So, each note completes the new distance. 
149 
00:09:37,680 --> 00:09:41,830 
And sends the message along an edge
to the other endpoint, then halts. 
150 
00:09:42,990 --> 00:09:46,030 
The same step is repeated
in the next superstep. 
151 
00:09:47,700 --> 00:09:51,690 
At this point,
the nodes have updated their values. 
152 
00:09:51,690 --> 00:09:53,320 
The node with the value 6, 
153 
00:09:53,320 --> 00:09:58,089 
has received our message,
just along one of the three edges on it. 
154 
00:09:59,120 --> 00:09:59,680 
Continuing. 
155 
00:10:01,030 --> 00:10:02,860 
At the end of superstep 2, 
156 
00:10:02,860 --> 00:10:07,690 
all nodes are ready to receive messages
from all their incident edges. 
157 
00:10:09,150 --> 00:10:13,210 
The node with a value 6, received a value
which is lower than its current value. 
158 
00:10:14,290 --> 00:10:18,020 
Now the active nodes,
have no more messages to send. 
159 
00:10:18,020 --> 00:10:20,010 
So each vertex, votes to halt. 
160 
00:10:21,020 --> 00:10:25,300 
The vertex ID and the value
are read out from each vertex, and 
161 
00:10:25,300 --> 00:10:27,600 
then the process starts. 
162 
00:10:27,600 --> 00:10:31,110 
If these nodes are in different
machines of a cluster, 
163 
00:10:31,110 --> 00:10:35,320 
the system will rely on
the underlying platform like YARN. 
164 
00:10:35,320 --> 00:10:37,960 
Or sparks underlying
infrastructure to ensure, 
165 
00:10:37,960 --> 00:10:41,770 
that the edges going across machines can
send and receive messages effectively. 
166 
00:10:42,880 --> 00:10:46,698 
This should give you a sense of the speed
of this process for a large scale network. 
1 
00:00:00,670 --> 00:00:05,260 
As we said earlier,
while the Giraph paradigm implements BSB, 
2 
00:00:05,260 --> 00:00:07,220 
it must also be pragmatic. 
3 
00:00:08,590 --> 00:00:12,590 
One such point of pragmatism is
the computation of aggregate values. 
4 
00:00:13,820 --> 00:00:16,550 
In the think like a vertex paradigm, 
5 
00:00:16,550 --> 00:00:20,360 
operations local to a vertex can
be performed in parallel, and 
6 
00:00:20,360 --> 00:00:24,220 
each vertex only has to work
with its immediate neighborhood. 
7 
00:00:24,220 --> 00:00:28,260 
This is very useful, but
it isn't sufficient at times. 
8 
00:00:28,260 --> 00:00:32,450 
For example, we need to know and use
the total number of edges in the graph. 
9 
00:00:33,470 --> 00:00:37,720 
This will be computed by adding
edges connected to each vertex, but 
10 
00:00:37,720 --> 00:00:41,140 
once aggregated,
it does not belong to any specific vertex. 
11 
00:00:42,440 --> 00:00:45,170 
So whom does this vertex
send the aggregate to? 
12 
00:00:46,750 --> 00:00:50,500 
Also suppose, a vertex creates some
edges as part of its compute step. 
13 
00:00:51,630 --> 00:00:54,060 
When does this information get sent out? 
14 
00:00:55,260 --> 00:00:57,000 
Let's answer the first question first. 
15 
00:00:58,240 --> 00:01:03,337 
The class in charge of these aggregates
is called the DefaultMasterCompute class. 
16 
00:01:04,580 --> 00:01:07,250 
This class specializes MasterCompute. 
17 
00:01:09,070 --> 00:01:12,460 
How does the DefaultMasterCompute
relate to the basic vertex computation? 
18 
00:01:14,430 --> 00:01:19,150 
We have seen that all vertex programs are
created by first defining a vertex class. 
19 
00:01:20,450 --> 00:01:25,314 
This class has a compute function that
performs like the think like a vertex 
20 
00:01:25,314 --> 00:01:25,860 
logic. 
21 
00:01:27,000 --> 00:01:32,665 
Now let's add to it a basic vertex
function and an aggregate function. 
22 
00:01:32,665 --> 00:01:33,530 
Now this function, for 
23 
00:01:33,530 --> 00:01:38,580 
our example of all the total number
of aggregate edges looks like this. 
24 
00:01:40,460 --> 00:01:42,070 
The name of the function is aggregate. 
25 
00:01:43,270 --> 00:01:47,990 
As you can see in yellow, it just gets
the number of edges off this vertex. 
26 
00:01:49,470 --> 00:01:55,003 
What's a little strange is that the first
argument of this aggregation has 
27 
00:01:55,003 --> 00:01:59,690 
an id of something whose
name is total number edges. 
28 
00:02:01,180 --> 00:02:03,740 
What really happens is as follows. 
29 
00:02:03,740 --> 00:02:08,302 
The defaltMasterCompute class, which
is in charge of this global aggregate 
30 
00:02:08,302 --> 00:02:11,610 
operations, is specialized
by your aggregate class. 
31 
00:02:12,730 --> 00:02:14,482 
This class has an ID, and 
32 
00:02:14,482 --> 00:02:20,080 
the aggregator gets registered with
Giraph's defaultMasterCompute class. 
33 
00:02:21,630 --> 00:02:28,220 
The ID we saw before refers to your
registered aggregator's classes ID. 
34 
00:02:28,220 --> 00:02:32,085 
The MasterCompute class performs
the centralized computation between 
35 
00:02:32,085 --> 00:02:33,830 
supersteps. 
36 
00:02:33,830 --> 00:02:37,110 
This class is initiated
on the master node and 
37 
00:02:37,110 --> 00:02:41,560 
will run every superstep
before the workers do. 
38 
00:02:41,560 --> 00:02:45,550 
Communication with the workers
should be performed via aggregators. 
39 
00:02:47,260 --> 00:02:50,080 
The values of the aggregators
are broadcast to the workers before 
40 
00:02:50,080 --> 00:02:52,380 
the vertex compute is called. 
41 
00:02:52,380 --> 00:02:56,670 
And collected by the master before
the master compute is called. 
42 
00:02:56,670 --> 00:03:01,550 
This means the aggregator values
used by the workers are consistent 
43 
00:03:01,550 --> 00:03:06,000 
with the aggregator values from
the master from the same superstep. 
44 
00:03:06,000 --> 00:03:08,750 
And the aggregator used by
the master are consistent 
45 
00:03:08,750 --> 00:03:12,330 
with the aggregator values from
the workers from the previous superstep. 
46 
00:03:13,630 --> 00:03:16,430 
Now let's go back to the big picture for
a second. 
47 
00:03:16,430 --> 00:03:17,980 
Giraph is a big data software. 
48 
00:03:19,090 --> 00:03:24,390 
Why not implement the Giraph system
with a set of MapReduce jobs? 
49 
00:03:24,390 --> 00:03:25,863 
Too much disk requirement. 
50 
00:03:25,863 --> 00:03:28,077 
No in-memory caching. 
51 
00:03:28,077 --> 00:03:30,950 
Every superstep becomes a job. 
52 
00:03:30,950 --> 00:03:34,680 
So all intermediate steps
are written to files, and 
53 
00:03:34,680 --> 00:03:38,750 
that is not a very scalable solution for
iterative operations. 
54 
00:03:38,750 --> 00:03:42,370 
However, it will be incorrect
to think that Giraph works 
55 
00:03:42,370 --> 00:03:44,041 
only in an in memory process. 
56 
00:03:45,180 --> 00:03:48,220 
It can always have less
memory than it needs. 
57 
00:03:48,220 --> 00:03:52,740 
There are two broad categories of
what's called out-of-core computation. 
58 
00:03:54,000 --> 00:03:57,650 
The first situation occurs when the graph
is really large compared to the capacity 
59 
00:03:57,650 --> 00:03:58,680 
of the cluster it's running on. 
60 
00:03:59,980 --> 00:04:04,020 
Each worker stores the vertices assigned
to it inside a set of partitions. 
61 
00:04:05,300 --> 00:04:09,150 
Inside a partition are a subset of
vertices together with their data. 
62 
00:04:10,430 --> 00:04:13,700 
During a superstep, the worker node 
63 
00:04:13,700 --> 00:04:18,610 
processes multiple partitions
concurrently, one per thread. 
64 
00:04:18,610 --> 00:04:23,700 
If a graph is large, then not all
partitions are stored in memory. 
65 
00:04:23,700 --> 00:04:28,120 
Typically only N partitions
are kept in memory at all times and 
66 
00:04:28,120 --> 00:04:31,180 
the rest of the partitions
are swapped into disk. 
67 
00:04:32,980 --> 00:04:37,110 
The second situation occurs when
the number of messages becomes high. 
68 
00:04:38,570 --> 00:04:41,530 
Normally, vertices are processed
in the order of their IDs. 
69 
00:04:42,710 --> 00:04:45,840 
A large number of messages
is handled by creating 
70 
00:04:45,840 --> 00:04:49,950 
temporary message stores which
are sorted by their destination IDs. 
71 
00:04:51,100 --> 00:04:54,930 
All messages going to the same
vertex are placed together. 
72 
00:04:54,930 --> 00:04:59,210 
To do this, messages are sorted
in memory periodically. 
73 
00:04:59,210 --> 00:05:02,220 
The message store files
are accessed based on 
74 
00:05:02,220 --> 00:05:04,170 
which vertices are being
processed at this moment. 
75 
00:05:05,240 --> 00:05:09,271 
This whole system is managed by
Giraph's out of core message processor. 
1 
00:00:00,600 --> 00:00:04,830 
In this lesson, we'll talk about
two dominant systems developed for 
2 
00:00:04,830 --> 00:00:06,030 
large scale graph processing. 
3 
00:00:07,270 --> 00:00:10,630 
The first one, called Giraph,
is from Apache and 
4 
00:00:10,630 --> 00:00:13,340 
implements a BSP model on Hadoop. 
5 
00:00:14,510 --> 00:00:19,440 
The second system is called Graphx,
is developed on the Spark platform, 
6 
00:00:19,440 --> 00:00:24,450 
which as you know, emphasizes on
interactive in memory computations. 
7 
00:00:24,450 --> 00:00:27,560 
While BSP is a popular
graph processing model, 
8 
00:00:27,560 --> 00:00:32,200 
the actual implementation of
BSP in an infrastructure needs 
9 
00:00:32,200 --> 00:00:35,230 
additional programmability beyond
what we have discussed so far. 
10 
00:00:36,680 --> 00:00:41,390 
In Giraph, several additional capabilities
are added to make it more practical. 
11 
00:00:42,760 --> 00:00:47,470 
A thorough coverage of the Giraph platform
is beyond the scoop of these lectures. 
12 
00:00:47,470 --> 00:00:50,464 
However, we'll touch upon
a few of these capabilities. 
13 
00:00:51,862 --> 00:00:57,126 
We'll first consider graph IO,
that is how graphs can come into a system 
14 
00:00:57,126 --> 00:01:02,870 
represented inside the system, and
when completed are written out. 
15 
00:01:02,870 --> 00:01:07,510 
Next, we'll describe how Giraph
interacts with external data sources. 
16 
00:01:07,510 --> 00:01:10,600 
Some of these data sources
use a different data model, 
17 
00:01:10,600 --> 00:01:12,070 
other sources include databases. 
18 
00:01:14,310 --> 00:01:16,100 
Once a graph is imported, 
19 
00:01:16,100 --> 00:01:18,900 
it is important to make sure that
the system runs efficiently. 
20 
00:01:20,490 --> 00:01:25,070 
We will look at a method that uses a
special kind of global aggregate operation 
21 
00:01:25,070 --> 00:01:28,400 
which saves time by reducing
the amount of messaging 
22 
00:01:28,400 --> 00:01:30,730 
to compute aggregate functions
like sum and products. 
23 
00:01:33,200 --> 00:01:37,000 
Finally, we'll recognize that
even if Giraph is designed for 
24 
00:01:37,000 --> 00:01:40,150 
performing iterative,
in memory computation, 
25 
00:01:40,150 --> 00:01:44,000 
there are times where it is absolutely
necessary to store data on disk. 
26 
00:01:45,340 --> 00:01:49,490 
We'll briefly touch upon Giraph's
ability to handle out of core graphs and 
27 
00:01:49,490 --> 00:01:50,820 
out of core messages. 
28 
00:01:51,920 --> 00:01:54,621 
A graph can be written in many ways. 
29 
00:01:54,621 --> 00:02:00,354 
For Neo4J, we saw how graphs can be
important to the database from a CSV file. 
30 
00:02:00,354 --> 00:02:06,340 
In Giraph, two of the most common input
formats are Adjacency List and Edge List. 
31 
00:02:08,010 --> 00:02:12,720 
For an Adjacency List,
each line has the node ID, a node value 
32 
00:02:12,720 --> 00:02:16,200 
which is a single number here, and
a list of destination, weight pairs. 
33 
00:02:18,050 --> 00:02:22,950 
Thus, in line one,
A has a value of 10 and 2 neighbors B and 
34 
00:02:22,950 --> 00:02:25,970 
F with edge weights 2 and 5, respectively. 
35 
00:02:27,300 --> 00:02:30,850 
Since G has no outgoing edge,
the adjacency list is empty. 
36 
00:02:32,350 --> 00:02:35,570 
The current way of representing
graphs is in terms of triplets. 
37 
00:02:35,570 --> 00:02:40,880 
Containing the source and destination
nodes followed by an [INAUDIBLE]. 
38 
00:02:40,880 --> 00:02:42,580 
Notice the way we have shown it here. 
39 
00:02:43,620 --> 00:02:45,160 
And the node values is not represented. 
40 
00:02:46,280 --> 00:02:49,170 
Let us simplify the Adjacency List
representation of it. 
41 
00:02:50,930 --> 00:02:54,400 
We remove the colons, commas, braces, and 
42 
00:02:54,400 --> 00:02:58,650 
parenthesis, and
get a space separated set of lines. 
43 
00:02:58,650 --> 00:02:59,810 
One line for each vertex. 
44 
00:03:00,880 --> 00:03:05,210 
We further replace the node IDs A,
B, C, etc., with 1, 2, 3, etc., 
45 
00:03:05,210 --> 00:03:08,710 
so that these IDs are integers. 
46 
00:03:09,810 --> 00:03:13,700 
So what do we need to specify
to parse this for Giraph? 
47 
00:03:13,700 --> 00:03:20,136 
One, the graph is a text subject and
not, let's say, a database subject. 
48 
00:03:20,136 --> 00:03:25,170 
Two, it is a vertex based representation,
each line is a vertex. 
49 
00:03:26,500 --> 00:03:27,800 
This splitter here is a space. 
50 
00:03:29,450 --> 00:03:31,830 
The idea of the node is a first value for
each line. 
51 
00:03:33,220 --> 00:03:35,250 
The value is a second token. 
52 
00:03:36,740 --> 00:03:40,120 
The next pair of items you find an edge
with the target and the weight, 
53 
00:03:40,120 --> 00:03:41,590 
respectively. 
54 
00:03:41,590 --> 00:03:45,420 
And lastly, there is a list of these
pairs until the end of the line. 
55 
00:03:46,820 --> 00:03:51,124 
Therefore, each line would typically
lead to the creation of both notes and 
56 
00:03:51,124 --> 00:03:52,006 
a set of edges. 
57 
00:03:53,687 --> 00:03:58,020 
This shows a typical reader formula
decency matrix written in Java. 
58 
00:03:58,020 --> 00:04:01,770 
Again, you don't have to know Java
to get the elements of this program. 
59 
00:04:03,280 --> 00:04:06,300 
Our reader is clearly customized for
your specific input. 
60 
00:04:07,500 --> 00:04:12,020 
Very often the starting point is
a basic reader provided by Giraph. 
61 
00:04:14,140 --> 00:04:18,181 
Like the reader that knows how to read
vertices from each line of a text swipe. 
62 
00:04:19,330 --> 00:04:24,070 
To customize it, you extend it and
create your own version. 
63 
00:04:25,350 --> 00:04:27,870 
Now, you need to define
how to get the ID and 
64 
00:04:27,870 --> 00:04:31,430 
value of the vertex by writing
separate message for them. 
65 
00:04:31,430 --> 00:04:36,455 
Notice that the ID comes from
the zeroth item of each line after 
66 
00:04:36,455 --> 00:04:42,390 
the split by white space, and
the value comes from the next open, 
67 
00:04:42,390 --> 00:04:45,170 
the second term, marked by 1 for
the 0 base of the light. 
68 
00:04:46,370 --> 00:04:48,640 
The next code element is this block here. 
69 
00:04:50,000 --> 00:04:54,210 
This specifies how to create edges
by iterating through every line. 
70 
00:04:55,390 --> 00:04:59,377 
To keep the short, we'll remove
the part that gets the edges here. 
71 
00:04:59,377 --> 00:05:03,202 
As Giraph as mature it has
included many specialized 
72 
00:05:03,202 --> 00:05:06,130 
to interoperate with compatible resources. 
73 
00:05:07,190 --> 00:05:10,410 
This diagram is from Giraph where
the show some of these sources. 
74 
00:05:11,780 --> 00:05:14,690 
We can group them into
three different categories. 
75 
00:05:16,210 --> 00:05:20,520 
Group one interoperates with Hive and
HBase. 
76 
00:05:20,520 --> 00:05:24,360 
You possibly remember these
systems from a prior course. 
77 
00:05:24,360 --> 00:05:27,290 
These systems are designed to give
a higher level of data access on 
78 
00:05:27,290 --> 00:05:28,870 
interface on top of MapReduce. 
79 
00:05:30,170 --> 00:05:34,880 
Group two accesses relational
systems like MySQL and Cassandra. 
80 
00:05:35,970 --> 00:05:40,150 
But these systems have accessed indirectly
through a software module called Gora. 
81 
00:05:41,570 --> 00:05:46,164 
Gora uses a JSON schema to map
the relation schema of the SQL database 
82 
00:05:46,164 --> 00:05:48,675 
to a structure that Giraph can read. 
83 
00:05:48,675 --> 00:05:52,864 
Group three accesses graph
databases like Neo4J and DEX, 
84 
00:05:52,864 --> 00:05:55,050 
which is now called Sparksee. 
85 
00:05:56,370 --> 00:05:59,190 
These systems are all taxes indirectly. 
86 
00:05:59,190 --> 00:06:01,750 
Using the [INAUDIBLE]
service of Tinkerpop. 
87 
00:06:01,750 --> 00:06:04,440 
Which is a graph API layer that can use 
88 
00:06:04,440 --> 00:06:07,800 
many different Giraph stores including
[INAUDIBLE] graph and Titan. 
89 
00:06:09,130 --> 00:06:11,190 
Consider a relational
table stored in Hive. 
90 
00:06:12,270 --> 00:06:15,280 
The table shown here is extracted
from the bio grid data source that 
91 
00:06:15,280 --> 00:06:16,290 
we mentioned in module two. 
92 
00:06:17,330 --> 00:06:21,340 
Each row of the table represents
a molecule interaction. 
93 
00:06:21,340 --> 00:06:25,190 
We can create a network from here just
by considering the first two columns. 
94 
00:06:26,250 --> 00:06:30,207 
The first column represents the source
node of an edge colored red. 
95 
00:06:30,207 --> 00:06:33,300 
And the second column represents
the target node of the edge colored blue. 
96 
00:06:34,640 --> 00:06:37,820 
The label on the edge comes from
the fifth column of the table 
97 
00:06:37,820 --> 00:06:39,670 
which is a black bold font. 
98 
00:06:41,100 --> 00:06:43,578 
Let's assume that these predict items, 
99 
00:06:43,578 --> 00:06:47,560 
these are items that we want to
pick up from the Hive table. 
100 
00:06:48,730 --> 00:06:51,460 
The simplest way to get a record from hive 
101 
00:06:51,460 --> 00:06:56,195 
to Giraph is to extend the class
called SimpleHiveRowToEdge. 
102 
00:06:57,970 --> 00:07:02,790 
For this class, we need to specify
the source node, the target node, and 
103 
00:07:02,790 --> 00:07:05,580 
the edge value using three
methods as shown here. 
104 
00:07:07,160 --> 00:07:10,325 
My extension is called MyHiveRowToEdge. 
105 
00:07:11,390 --> 00:07:15,440 
It shows the implementation of these
methods where we just pick up the first, 
106 
00:07:16,850 --> 00:07:20,430 
second, and fifth columns,
as we described before. 
107 
00:07:22,390 --> 00:07:26,640 
Now, as mentioned before,
Giraph interacts with Neo4J through 
108 
00:07:26,640 --> 00:07:30,080 
the Gremlin API provided by Tinkerpop. 
109 
00:07:30,080 --> 00:07:34,470 
One can think of Gremlin as
a traversal API, which means, 
110 
00:07:34,470 --> 00:07:39,220 
it allows one to start from some node and
walk the graph step by step. 
111 
00:07:39,220 --> 00:07:43,280 
To show this,
consider disease gene graph on the right. 
112 
00:07:43,280 --> 00:07:45,050 
Let's call this graph G. 
113 
00:07:46,210 --> 00:07:50,407 
So g.V represents all the vertices of G. 
114 
00:07:50,407 --> 00:07:55,524 
Therefore g.V has name
MC4R selects the node that 
115 
00:07:55,524 --> 00:08:00,410 
has a property called
name whose value is MC4R. 
116 
00:08:01,900 --> 00:08:06,010 
Let's add to this path the condition .out, 
117 
00:08:06,010 --> 00:08:09,640 
which chooses the out
edges of the MC4R node and 
118 
00:08:09,640 --> 00:08:14,050 
then traverses the associatedWith edge
to the orange node called obesity. 
119 
00:08:15,150 --> 00:08:17,250 
For this call, returns the vertex only. 
120 
00:08:18,940 --> 00:08:25,780 
Now, adding the path to values
means gives us obesity. 
121 
00:08:25,780 --> 00:08:28,170 
We can also expand differently
from the obesity node. 
122 
00:08:29,480 --> 00:08:33,600 
When we say inV() we refer
to all nodes that have 
123 
00:08:33,600 --> 00:08:36,050 
incoming edges to the current node. 
124 
00:08:36,050 --> 00:08:38,400 
In this case, there is only one. 
125 
00:08:38,400 --> 00:08:39,000 
The LEPR node. 
126 
00:08:40,500 --> 00:08:44,190 
To this, we add the traversal out beam and 
127 
00:08:44,190 --> 00:08:48,320 
thus we get back the out going edge from
the LEPR node highlighted in together. 
128 
00:08:49,550 --> 00:08:52,660 
We can also look at the Giraph Gremlin
near project connection 
129 
00:08:52,660 --> 00:08:53,760 
from Tinkerpop's viewpoint. 
130 
00:08:55,050 --> 00:08:58,990 
Tinkerpop is trying to create
a standard language for graph reversal, 
131 
00:08:58,990 --> 00:09:03,750 
just like Neo4J is trying to create Open
Cypher as a standard query language for 
132 
00:09:03,750 --> 00:09:04,470 
graph databases. 
133 
00:09:05,960 --> 00:09:10,020 
In trying to create the standard,
Tinkerpop recognizes that the actual 
134 
00:09:10,020 --> 00:09:14,019 
storage management for graph databases
should be provided by another vendor. 
135 
00:09:15,050 --> 00:09:17,950 
The vendor needs to implement
the Gremlin API for access. 
136 
00:09:19,310 --> 00:09:24,560 
Similarly for graphic processing,
including expensive analytic operations 
137 
00:09:24,560 --> 00:09:27,460 
should be performed by what
they call a graph computer. 
138 
00:09:28,460 --> 00:09:31,630 
This is the role played by
Giraph as well as Spark. 
139 
00:09:31,630 --> 00:09:33,677 
Both of which interface with Tinkerpop. 
1 
00:00:01,140 --> 00:00:04,910 
Next, we'll count the number of
vertices and edges, define a min and 
2 
00:00:04,910 --> 00:00:10,140 
max function for Spark's reduce method,
computer the min and max degrees, 
3 
00:00:10,140 --> 00:00:14,390 
and compute the histogram data
of the degree of connectedness. 
4 
00:00:16,490 --> 00:00:19,320 
In this hands on,
we will cover the degree distribution of 
5 
00:00:19,320 --> 00:00:22,810 
the metros graph from
the first hands on exercise. 
6 
00:00:22,810 --> 00:00:27,110 
This will help you practice finding
the degrees of connectedness in a graph. 
7 
00:00:27,110 --> 00:00:28,310 
These numbers will be used for 
8 
00:00:28,310 --> 00:00:31,780 
plotting the visualizations in
the next hands on exercises. 
9 
00:00:34,330 --> 00:00:36,650 
The degree of a vertex is
the number of edges or 
10 
00:00:36,650 --> 00:00:39,810 
connections the vertex has to
other vertices in the graph. 
11 
00:00:40,880 --> 00:00:44,460 
In directed graphs,
each vertex has an in degree, 
12 
00:00:44,460 --> 00:00:47,555 
the number of edges
directed to the vertex. 
13 
00:00:47,555 --> 00:00:52,797 
In and out degree, the number of
edges directed away from the vertex. 
14 
00:00:52,797 --> 00:00:55,580 
The metros graph is an example
of a directed graph. 
15 
00:00:57,080 --> 00:01:01,650 
Each metropolis vertex has one
outgoing edge to a country vertex. 
16 
00:01:02,700 --> 00:01:08,460 
Each country vertex has one or more
incoming edges from metropolis vertices. 
17 
00:01:08,460 --> 00:01:09,725 
This will be a quiz question. 
18 
00:01:16,366 --> 00:01:20,976 
Starting again where we left off from
the previous hands-on exercise, first, 
19 
00:01:20,976 --> 00:01:23,280 
ensure your Cloudera VM is started, and 
20 
00:01:23,280 --> 00:01:27,430 
that you downloaded the dataset
examples of analytics. 
21 
00:01:27,430 --> 00:01:29,110 
The link is in the content for this week. 
22 
00:01:30,460 --> 00:01:34,662 
Use the numEdges attribute to print
the number of edges in metrosGraph. 
23 
00:01:34,662 --> 00:01:37,385 
As you can see the result is 65, 
24 
00:01:37,385 --> 00:01:42,360 
which matches the number of
lines in metro_country.csv. 
25 
00:01:42,360 --> 00:01:46,178 
Now use the numVertices
attribute to print the number 
26 
00:01:46,178 --> 00:01:48,258 
of vertices in metrosGraph. 
27 
00:01:48,258 --> 00:01:53,344 
As you can see the result is 93,
which matches the number of 
28 
00:01:53,344 --> 00:01:59,655 
lines in metro.csv65 plus the number
of lines in country.csv, 28. 
29 
00:02:06,666 --> 00:02:11,061 
Define the max and the min reduce
operation to compute the highest and 
30 
00:02:11,061 --> 00:02:12,586 
lowest degree vertex. 
31 
00:02:15,929 --> 00:02:20,725 
Let us find the vertex with
the most outgoing edges or 
32 
00:02:20,725 --> 00:02:25,848 
the vertex with the largest
out degree by passing the max 
33 
00:02:25,848 --> 00:02:32,065 
function to a reduced operation on
the out degrees of metrosGraph. 
34 
00:02:33,190 --> 00:02:38,310 
The result in this case is vertex
ID five with one outgoing edge. 
35 
00:02:38,310 --> 00:02:42,680 
The result could have been any metropolis
because every metropolis in this graph 
36 
00:02:42,680 --> 00:02:45,600 
has one outgoing edge to its country. 
37 
00:02:45,600 --> 00:02:48,890 
Let us find the vertex with
the most incoming edges, or 
38 
00:02:48,890 --> 00:02:51,620 
the vertex with the largest inDegree. 
39 
00:02:51,620 --> 00:02:54,620 
This is done the same way
as the previous example, 
40 
00:02:54,620 --> 00:02:59,170 
except you'll run the reduce operation
on the inDegrees of metrosGraph. 
41 
00:02:59,170 --> 00:03:04,350 
The result is VertexId 108
with 14 incoming edges. 
42 
00:03:05,390 --> 00:03:11,640 
Apply a filter to the metrosGraph
vertices to find out which vertex is 108. 
43 
00:03:11,640 --> 00:03:14,280 
The answer is the United States. 
44 
00:03:14,280 --> 00:03:20,860 
This means that the United States has
14 metropolises in the metros.csvfile. 
45 
00:03:20,860 --> 00:03:25,500 
We can also compute how many vertices have
one out going edge by applying a filter of 
46 
00:03:25,500 --> 00:03:28,530 
one to the outgoing degrees and
counting the results. 
47 
00:03:29,550 --> 00:03:36,160 
The result is 65 because there are 65
metropolises with one outgoing degree. 
48 
00:03:36,160 --> 00:03:38,520 
None of the countries have
any outgoing degrees. 
49 
00:03:39,790 --> 00:03:42,850 
Let us ignore whether or
not the edge is in or out, and 
50 
00:03:42,850 --> 00:03:45,850 
just find which vertex has the most edges. 
51 
00:03:45,850 --> 00:03:49,460 
Again, we will run the reduce
operation with the max function. 
52 
00:03:49,460 --> 00:03:52,310 
But this time we will run it on
metrosGraph's degrees attribute. 
53 
00:03:53,340 --> 00:03:56,960 
The result is 108 again
with 14 connections. 
54 
00:03:56,960 --> 00:04:01,239 
This means that the United States is
the most connected vertex in metrosGraph. 
55 
00:04:08,252 --> 00:04:11,910 
Finally, let us calculate
the histogram data of the degrees for 
56 
00:04:11,910 --> 00:04:13,750 
the countries in metrosGraph. 
57 
00:04:14,880 --> 00:04:18,010 
First, create a map that
only includes countries. 
58 
00:04:18,010 --> 00:04:22,640 
So create a filter to only include
the vertices with the vertex ID that is 
59 
00:04:22,640 --> 00:04:24,815 
greater than or equal to 100. 
60 
00:04:24,815 --> 00:04:27,888 
Then you will group the map
by the size of the degree and 
61 
00:04:27,888 --> 00:04:30,430 
sort the map from lowest
to highest degree. 
62 
00:04:32,310 --> 00:04:36,620 
The output shows six pairs in an array. 
63 
00:04:36,620 --> 00:04:39,020 
The first number is the number of edges,
and 
64 
00:04:39,020 --> 00:04:43,810 
the second number is the number of
vertices that have that number of edges. 
65 
00:04:43,810 --> 00:04:48,462 
In other words, the result of
the query shows that there are 18 
66 
00:04:48,462 --> 00:04:53,461 
countries with 1 metropolis,
4 countries with 2 metropolises, 
67 
00:04:53,461 --> 00:04:58,805 
2 countries with 3 metropolises,
2 countries with 5 metropolises, 
68 
00:04:58,805 --> 00:05:04,260 
1 country with 9 metropolises, and
1 country with 14 metropolises. 
1 
00:00:00,520 --> 00:00:06,640 
Next, we'll import the GraphX libraries,
import the Vertices, import the Edges, 
2 
00:00:06,640 --> 00:00:12,820 
create a Graph, and use Spark's filter
method to return Vertices in the graph. 
3 
00:00:12,820 --> 00:00:15,670 
Hi, this is Cristine Kirkpatrick,
division director for 
4 
00:00:15,670 --> 00:00:17,720 
Information Technology Systems and 
5 
00:00:17,720 --> 00:00:21,320 
Services at
the San Diego Super Computer Center. 
6 
00:00:21,320 --> 00:00:23,810 
I'll guide you through
the hands-on exercises. 
7 
00:00:23,810 --> 00:00:26,440 
I recommend you follow
along with the video once. 
8 
00:00:26,440 --> 00:00:28,370 
Then, either using the video or 
9 
00:00:28,370 --> 00:00:32,360 
the hands-on reading, go through the
hands-on exercise on your own computer. 
10 
00:00:33,460 --> 00:00:37,440 
This hands-on exercise will show you
how to build a graph using GraphX, 
11 
00:00:37,440 --> 00:00:40,720 
Spark's API for graphs, and
graph-parallel computation. 
12 
00:00:41,750 --> 00:00:44,720 
We'll start with importing the data and
then build a simple graph. 
13 
00:00:45,890 --> 00:00:49,050 
Note that the first four hands on
assignments are meant to be completed 
14 
00:00:49,050 --> 00:00:50,640 
sequentially. 
15 
00:00:50,640 --> 00:00:54,430 
That is, you will need to have your
Cloudera VM running continuously. 
16 
00:00:54,430 --> 00:00:55,840 
If you must shut down your VM and 
17 
00:00:55,840 --> 00:01:00,460 
restart, you will need to run the commands
from this first hands on assignment again. 
18 
00:01:01,470 --> 00:01:05,930 
Otherwise you may receive error messages
because the data has not been imported. 
19 
00:01:05,930 --> 00:01:07,680 
The data references metro areas. 
20 
00:01:08,860 --> 00:01:13,190 
In an effort to disambiguate or
distinguish between metropolitan areas and 
21 
00:01:13,190 --> 00:01:15,700 
an English word also used
to refer to a train or 
22 
00:01:15,700 --> 00:01:19,240 
light rail, when we mean metro
area we will say metropolis. 
23 
00:01:20,340 --> 00:01:23,070 
First, ensure your
cloudera vm is started and 
24 
00:01:23,070 --> 00:01:26,780 
that you've downloaded the data set,
examples of analytics. 
25 
00:01:26,780 --> 00:01:28,600 
The link is in the content for this week. 
26 
00:01:29,720 --> 00:01:32,780 
The download process might name
the zip file something very long. 
27 
00:01:33,960 --> 00:01:39,170 
Copy the examples of analytics.zip
file to the cloudera's home folder. 
28 
00:01:39,170 --> 00:01:41,300 
If the zip file is on the desktop, 
29 
00:01:41,300 --> 00:01:44,880 
simply drag it to the Cloudera's
home folder on the desktop. 
30 
00:01:44,880 --> 00:01:47,820 
If the file is saved elsewhere,
then you will need to navigate 
31 
00:01:47,820 --> 00:01:51,680 
to the folder where it is saved before
dragging it to the Cloudera's home folder. 
32 
00:01:52,750 --> 00:01:57,490 
Open the terminal window by clicking the
terminal icon at the top of the screen. 
33 
00:01:57,490 --> 00:02:01,860 
Use the unzip command to extract the zip
file to the cloudera home directory. 
34 
00:02:01,860 --> 00:02:04,830 
You can copy and
paste the name of the zip file. 
35 
00:02:04,830 --> 00:02:07,750 
Let it extract with the default name,
ExamplesOfAnalytics. 
36 
00:02:08,960 --> 00:02:11,690 
Go into the ExamplesOfAnalytics
directory and 
37 
00:02:11,690 --> 00:02:15,260 
list the contents of
the EOA data directory. 
38 
00:02:15,260 --> 00:02:18,430 
You should see five .csv files and
one .txt file. 
39 
00:02:19,720 --> 00:02:25,460 
In order to access the CSV and TXT files,
we have to first copy the files to HDFS. 
40 
00:02:26,950 --> 00:02:32,500 
Rally HDFS foot command to copy
the EOA data directory to HDFS 
41 
00:02:32,500 --> 00:02:35,990 
then less the contents of
the EOA data directory on HDFS. 
42 
00:02:37,580 --> 00:02:42,140 
Start the spark shell and include the
graph stream and breeze fizz libraries. 
43 
00:02:42,140 --> 00:02:45,030 
Don't worry about copying
the full command from the video. 
44 
00:02:45,030 --> 00:02:48,526 
A text version will be included within
the course reading that contains the full 
45 
00:02:48,526 --> 00:02:50,048 
command for you to copy and paste. 
46 
00:02:53,310 --> 00:02:57,083 
Import the log for j classes and
suppress the notice in info messages. 
47 
00:03:03,571 --> 00:03:09,410 
Import sparks graphx and
rdd classes along with scala source class. 
48 
00:03:09,410 --> 00:03:14,180 
The data set we are going to use in
this hands on is from three files. 
49 
00:03:14,180 --> 00:03:18,860 
The metro.csv file and the country.csv
file contain the vertices for 
50 
00:03:18,860 --> 00:03:22,700 
the graph and
metro underscore country.csv. 
51 
00:03:22,700 --> 00:03:26,740 
contains the edges that make up the
relationships between the metro areas and 
52 
00:03:26,740 --> 00:03:27,980 
the country they belong too. 
53 
00:03:29,010 --> 00:03:33,940 
Before importing any of the CSV files, we
are going to list the first five lines of 
54 
00:03:33,940 --> 00:03:36,710 
each file to verify that
they are indeed CSV files. 
55 
00:03:37,880 --> 00:03:44,540 
Notice that the metric.csv contains an ID,
the metropolis name and the population. 
56 
00:03:44,540 --> 00:03:48,838 
The country.csv file contains
only an ID and the country name. 
57 
00:03:48,838 --> 00:03:52,667 
The metro_country.csv file
contains the metro ID and 
58 
00:03:52,667 --> 00:03:56,339 
the ID of the country that
the metropolis belongs to. 
59 
00:04:02,179 --> 00:04:07,110 
Create a class called PlaceNode to store
the information about the vertices. 
60 
00:04:07,110 --> 00:04:10,870 
Then extend the PlaceNode class
with case classes specifically for 
61 
00:04:10,870 --> 00:04:12,920 
the metro's and the countries vertices. 
62 
00:04:13,920 --> 00:04:14,990 
Create attributes for 
63 
00:04:14,990 --> 00:04:19,900 
the metro class to store the name and
the population from the CSV file. 
64 
00:04:19,900 --> 00:04:22,660 
The country class only needs
an attribute to store the name. 
65 
00:04:23,930 --> 00:04:29,490 
To import the metro.csv file, create
a spark resilient distributive data set or 
66 
00:04:29,490 --> 00:04:35,235 
rdd named metros made up of
a vertex id in the metro class. 
67 
00:04:35,235 --> 00:04:38,310 
An rdd represents an immutable
partition collection 
68 
00:04:38,310 --> 00:04:40,740 
of elements that can be
operated on in parallel. 
69 
00:04:41,800 --> 00:04:44,630 
When the contents of
the .csv files printed, 
70 
00:04:44,630 --> 00:04:47,900 
the line with the column names
started with the symbol #. 
71 
00:04:47,900 --> 00:04:51,590 
So create a filter to ignore any
line that starts with a # so 
72 
00:04:51,590 --> 00:04:54,080 
that the column names
don't import into the RDD. 
73 
00:04:55,540 --> 00:04:59,870 
The .csv files being imported
contain values separated by commas. 
74 
00:04:59,870 --> 00:05:03,770 
So you will need to split each line
in the metro.csv file into rows, 
75 
00:05:03,770 --> 00:05:06,470 
by using a comma as a delimiter. 
76 
00:05:06,470 --> 00:05:08,970 
Map the first row to the vertex ID. 
77 
00:05:08,970 --> 00:05:11,610 
The second row is the metro
name attribute, and 
78 
00:05:11,610 --> 00:05:13,630 
the third row is the population attribute. 
79 
00:05:14,700 --> 00:05:19,162 
You're going to import the country's .csv
feed file the same way as the metro.csv 
80 
00:05:19,162 --> 00:05:20,080 
feed file. 
81 
00:05:20,080 --> 00:05:25,120 
However this time since the ids in both
the metro.csv file and the country.csv 
82 
00:05:25,120 --> 00:05:30,350 
file start with one you will add 100
to the vertex id of the countries. 
83 
00:05:30,350 --> 00:05:34,570 
So that the vertex ids are unique
between both data sets. 
84 
00:05:34,570 --> 00:05:37,400 
If the ids are not unique
that will prevent us from 
85 
00:05:37,400 --> 00:05:39,430 
creating an accurate graph. 
86 
00:05:39,430 --> 00:05:40,011 
This will be on the quiz. 
87 
00:05:46,001 --> 00:05:50,200 
Import the edges into
an RDD named mclinks. 
88 
00:05:50,200 --> 00:05:53,550 
This is done the same way as
the previous two examples. 
89 
00:05:53,550 --> 00:05:56,646 
Remember to add 100 to
the countries vertex ID. 
90 
00:06:02,462 --> 00:06:07,142 
Concatenate the metros and countries
vertices into a single variable and 
91 
00:06:07,142 --> 00:06:11,143 
use GraphX's graph function to
create a graph of the metros and 
92 
00:06:11,143 --> 00:06:14,030 
countries vertices with
the MC links edges. 
93 
00:06:15,140 --> 00:06:17,690 
Let us take a look at
what is in the graph. 
94 
00:06:17,690 --> 00:06:20,879 
Use the vertices and edges attributes
to print five vertices and 
95 
00:06:20,879 --> 00:06:22,599 
five edges from the metros graph. 
96 
00:06:28,613 --> 00:06:31,064 
Query the graph to find
how the metropolises and 
97 
00:06:31,064 --> 00:06:33,340 
the countries are related. 
98 
00:06:33,340 --> 00:06:35,990 
Let us find which country Tokyo is in. 
99 
00:06:35,990 --> 00:06:38,850 
Tokyo has a vertex ID of 1. 
100 
00:06:38,850 --> 00:06:42,450 
Use RDD's filter method to filter
all of the edges in metro's graph 
101 
00:06:42,450 --> 00:06:47,360 
that have a source for text ID of one and
create a map of destination vertex ID's. 
102 
00:06:48,550 --> 00:06:51,420 
The result is 101 which is Japan. 
103 
00:06:51,420 --> 00:06:56,530 
You can verify this by looking at
the metro.csv and country.csv files. 
104 
00:06:56,530 --> 00:07:01,470 
Remember, we added 100 to
the IDs in country.csv. 
105 
00:07:01,470 --> 00:07:02,910 
Now, let us do the opposite and 
106 
00:07:02,910 --> 00:07:07,320 
find all of the metropolises that are in
China which has a vertex ID of 103. 
107 
00:07:07,320 --> 00:07:11,880 
This time we're going to filter
all of the edges in metro's graph 
108 
00:07:11,880 --> 00:07:17,530 
that have a vertex ID of 103 and
create a map of all of the source IDs. 
109 
00:07:17,530 --> 00:07:21,480 
The result is 3,4, 7, 24, and 34. 
110 
00:07:21,480 --> 00:07:28,353 
You can look in metrostats.csv and verify
that those metropolises are in China. 
1 
00:00:00,450 --> 00:00:05,380 
Next, we'll create a new dataset,
join two datasets with JoinVertices, 
2 
00:00:05,380 --> 00:00:10,130 
join two datasets with outerJoinVertices,
and create a new return type for 
3 
00:00:10,130 --> 00:00:11,238 
the joined vertices. 
4 
00:00:11,238 --> 00:00:13,310 
In this hands-on exercise, 
5 
00:00:13,310 --> 00:00:17,370 
we will create a new graph made
up of five airline flights. 
6 
00:00:17,370 --> 00:00:19,720 
The vertices will represent the airports, 
7 
00:00:19,720 --> 00:00:23,690 
and the edges will represent
the departures and the arrivals. 
8 
00:00:23,690 --> 00:00:27,180 
Once the graph has been created
we will add a second dataset 
9 
00:00:27,180 --> 00:00:29,670 
with additional airport information and 
10 
00:00:29,670 --> 00:00:35,570 
practice using GraphX's join methods to
create new graphs by joining the datasets. 
11 
00:00:35,570 --> 00:00:38,140 
We will start a new spark shell session,
so 
12 
00:00:38,140 --> 00:00:40,460 
we will be opening a new terminal window. 
13 
00:00:41,530 --> 00:00:45,000 
Open the new terminal window by
clicking the terminal icon at the top 
14 
00:00:45,000 --> 00:00:45,570 
of the screen. 
15 
00:00:47,060 --> 00:00:48,152 
Start the spark shell. 
16 
00:00:58,295 --> 00:01:02,955 
Import the log for j classes and
suppress the notice and info messages. 
17 
00:01:04,105 --> 00:01:07,925 
Import sparks graphics and rdd classes. 
18 
00:01:07,925 --> 00:01:11,255 
Now we will create the vertices
ourselves from a list. 
19 
00:01:11,255 --> 00:01:15,375 
The list will contain the vertex ID and
the name of the airport. 
20 
00:01:15,375 --> 00:01:17,280 
Now we will create the edges. 
21 
00:01:17,280 --> 00:01:20,730 
The edges will represent a flight
departing from one airport and 
22 
00:01:20,730 --> 00:01:22,150 
arriving at another. 
23 
00:01:22,150 --> 00:01:25,710 
We will set the edge property
to a fake flight number. 
24 
00:01:25,710 --> 00:01:28,380 
Create the flights graph
by joining the vertices and 
25 
00:01:28,380 --> 00:01:30,690 
the edges using graph
flexes graph functions. 
26 
00:01:31,720 --> 00:01:34,050 
Let us explore the graph so far. 
27 
00:01:34,050 --> 00:01:38,430 
Look through each triplet in flights graph
and print the contents of the graph. 
28 
00:01:38,430 --> 00:01:41,690 
We will specify which airport
the flight departs from, 
29 
00:01:41,690 --> 00:01:44,980 
which airport the flight arrives at,
and the flight number. 
30 
00:01:44,980 --> 00:01:47,710 
You'll just put all of
the vertices in the graph. 
31 
00:01:47,710 --> 00:01:51,690 
Now we are willing to create an additional
dataset to store additional information 
32 
00:01:51,690 --> 00:01:53,330 
about each airport. 
33 
00:01:53,330 --> 00:01:58,390 
First, we will create a case class called
airport information with the city and 
34 
00:01:58,390 --> 00:02:00,380 
airport code properties. 
35 
00:02:00,380 --> 00:02:04,320 
Then we are going to create the airport
information vertices from a list 
36 
00:02:04,320 --> 00:02:05,900 
as we did earlier. 
37 
00:02:05,900 --> 00:02:07,830 
We will make sure the vertex ID for 
38 
00:02:07,830 --> 00:02:13,930 
the airport information matches the vertex
ID of the airport defined earlier. 
39 
00:02:13,930 --> 00:02:17,810 
Note, there does not have to be a one
to one relationship between the airport 
40 
00:02:17,810 --> 00:02:20,960 
vertices and
the airport information vertices. 
41 
00:02:20,960 --> 00:02:25,640 
Los Angeles International Airport is
present in the airport vertices, but 
42 
00:02:25,640 --> 00:02:28,990 
it is missing from the airport
information vertices. 
43 
00:02:28,990 --> 00:02:33,380 
The airport information vertices contains
information about airports in London and 
44 
00:02:33,380 --> 00:02:34,370 
Hong Kong. 
45 
00:02:34,370 --> 00:02:37,984 
But we don't have any flights departing or
arriving from those airports. 
46 
00:02:43,926 --> 00:02:46,540 
Let us complete our first join. 
47 
00:02:46,540 --> 00:02:51,050 
A mapping function has to be defined in
order to join vertices with graphics join 
48 
00:02:51,050 --> 00:02:52,890 
vertices method. 
49 
00:02:52,890 --> 00:02:56,680 
We are going to define a map function that
will join the name of the airport from 
50 
00:02:56,680 --> 00:03:01,580 
the airport vertices with the city name
from the airport information vertices. 
51 
00:03:01,580 --> 00:03:05,460 
In order to create a new graph with
the vertices that has the airport name 
52 
00:03:05,460 --> 00:03:07,240 
colon city name. 
53 
00:03:07,240 --> 00:03:08,957 
After the map function in defined, 
54 
00:03:08,957 --> 00:03:12,228 
then we will use the joined vertices
method to create the new graph. 
55 
00:03:12,228 --> 00:03:17,770 
Vertices without a matching value in
the RDD, retain their original value. 
56 
00:03:17,770 --> 00:03:20,540 
Now, print out the vertices
of the new graph. 
57 
00:03:20,540 --> 00:03:23,814 
Notice all of the vertices contain
the airport and city names, 
58 
00:03:23,814 --> 00:03:28,089 
except Los Angeles International Airport,
which retained its original value. 
59 
00:03:33,291 --> 00:03:37,890 
Now we will join the vertices using
GraphX's outerJoinVertices method. 
60 
00:03:38,890 --> 00:03:42,060 
OuterJoinVertices is
similar to JoinVertices, 
61 
00:03:42,060 --> 00:03:46,080 
except that the user-defined map
function is applied to all vertices and 
62 
00:03:46,080 --> 00:03:47,910 
it can change the vertex property type. 
63 
00:03:49,740 --> 00:03:53,580 
Here, we are using the OuterJoinVertices
method to create a new graph where 
64 
00:03:53,580 --> 00:03:57,140 
the vertices contain a property
with the name of the airport and 
65 
00:03:57,140 --> 00:04:00,830 
a property makeup of
the airport information class. 
66 
00:04:00,830 --> 00:04:03,010 
Print the vertices once
the new graph is made. 
67 
00:04:04,160 --> 00:04:08,200 
Notice that the type of the second
vertex property is sum. 
68 
00:04:08,200 --> 00:04:11,250 
This is because
Los Angeles International Airport 
69 
00:04:11,250 --> 00:04:14,970 
does not have a corresponding
vertex in airport information. 
70 
00:04:14,970 --> 00:04:18,990 
Therefore GraphX assigned all of
the properties the sum type so 
71 
00:04:18,990 --> 00:04:21,700 
that all of the vertex
property types are the same. 
72 
00:04:22,960 --> 00:04:24,030 
We can use the get or 
73 
00:04:24,030 --> 00:04:28,400 
else method to assign a default
value if one does not exist. 
74 
00:04:28,400 --> 00:04:31,240 
Now, let us rerun
the outer join vertices but 
75 
00:04:31,240 --> 00:04:35,930 
this time use the get, get or
else method to create an airport 
76 
00:04:35,930 --> 00:04:41,630 
information class with NA as the values
for the city and the airport code. 
77 
00:04:41,630 --> 00:04:44,140 
Print the vertices once
the new graph is made. 
78 
00:04:45,270 --> 00:04:50,196 
Now, the type of the second vertex
property is airport information in 
79 
00:04:50,196 --> 00:04:54,957 
Los Angeles International Airport,
airport information class, 
80 
00:04:54,957 --> 00:04:57,055 
has the properties NA and NA. 
81 
00:05:02,404 --> 00:05:07,295 
Finally, in this last exercise, we will
create a new case class called Airport 
82 
00:05:07,295 --> 00:05:12,490 
that contains property for the name,
city, and code of the airport. 
83 
00:05:12,490 --> 00:05:16,080 
First, we will define the new Airport
case class with the name, city, and 
84 
00:05:16,080 --> 00:05:18,040 
code properties. 
85 
00:05:18,040 --> 00:05:21,580 
Then use the outer join vertices
method to join the data sets, and 
86 
00:05:21,580 --> 00:05:22,350 
create the new graph. 
87 
00:05:23,380 --> 00:05:28,260 
Create a mapping so if the airport has an
airport information vertex, the city and 
88 
00:05:28,260 --> 00:05:32,250 
code are taken from the instance
of the airport information class. 
89 
00:05:32,250 --> 00:05:34,770 
Otherwise, only include the airport name. 
90 
00:05:35,920 --> 00:05:39,370 
Finally, print the vertices
of the new graph. 
91 
00:05:39,370 --> 00:05:44,775 
The new graph has only one vertex property
which is an instance of the airport class. 
92 
00:05:44,775 --> 00:05:48,330 
GraphX has lots of methods for
joining graph datasets. 
93 
00:05:48,330 --> 00:05:52,721 
This hands on example only scratched
the surface of GraphX's power. 
1 
00:00:01,400 --> 00:00:05,190 
Next, we'll create a new graph by
adding the continents dataset, 
2 
00:00:05,190 --> 00:00:07,430 
import the GraphStream library, 
3 
00:00:07,430 --> 00:00:11,120 
import the countriesGraph into
a GraphStream SingleGraph, 
4 
00:00:11,120 --> 00:00:14,680 
visualize the countriesGraph,
and visualize the Facebook graph. 
5 
00:00:15,688 --> 00:00:19,510 
In this hands-on exercise we
will use the GraphStream library 
6 
00:00:19,510 --> 00:00:21,510 
to visualize countries graph. 
7 
00:00:21,510 --> 00:00:25,180 
The metros graph that we have been
working with in the previous exercises. 
8 
00:00:25,180 --> 00:00:27,990 
This time with the continent vertices and
edges added. 
9 
00:00:30,120 --> 00:00:33,391 
Starting again where we left off
from the previous hands-on exercise, 
10 
00:00:33,391 --> 00:00:36,970 
ensure your [INAUDIBLE] has started and
that you've imported the data. 
11 
00:00:43,079 --> 00:00:46,307 
First, in order to make
the graph more interesting, 
12 
00:00:46,307 --> 00:00:50,754 
we are going to import additional
vertices from the continent.csv, and 
13 
00:00:50,754 --> 00:00:54,342 
additional edges from
country_continent.csv to create 
14 
00:00:54,342 --> 00:00:58,520 
a relationship between the country and
the continent it belongs to. 
15 
00:00:59,530 --> 00:01:02,442 
We will show the steps for
importing the datasets, but 
16 
00:01:02,442 --> 00:01:04,351 
we do not cover the steps in detail. 
17 
00:01:04,351 --> 00:01:07,111 
If you aren't sure how to
import a dataset, go back and 
18 
00:01:07,111 --> 00:01:09,700 
view the video called
hands-on building a graph. 
19 
00:01:11,580 --> 00:01:16,250 
Now, we are going to concatenate the
metros, countries, and continents vertices 
20 
00:01:16,250 --> 00:01:20,800 
into a single variable and concatenate
the metros to countries edges, and 
21 
00:01:20,800 --> 00:01:25,010 
countries to continent edges
into another single variable. 
22 
00:01:25,010 --> 00:01:28,271 
Finally, you will create a new
graph called countriesGraph. 
23 
00:01:36,474 --> 00:01:39,540 
Now, import the GraphStream library. 
24 
00:01:39,540 --> 00:01:43,310 
We will not go over in detail how
to use the GraphStream library. 
25 
00:01:43,310 --> 00:01:45,246 
You can review its documentation and 
26 
00:01:45,246 --> 00:01:48,314 
other online resources on your
own if you are interested. 
27 
00:01:54,742 --> 00:01:58,548 
First, create a new instance of
GraphStreams SingleGraph class, 
28 
00:01:58,548 --> 00:02:00,160 
using the countriesGraph. 
29 
00:02:08,574 --> 00:02:12,293 
Next, we will set some attributes for
the graph we are going to visualize, 
30 
00:02:12,293 --> 00:02:14,395 
including setting a style for the graph. 
31 
00:02:14,395 --> 00:02:17,620 
GraphStream uses cascading style sheets or 
32 
00:02:17,620 --> 00:02:22,970 
CSS just like ordinary web pages,
to control the appearance of the graph. 
33 
00:02:22,970 --> 00:02:27,300 
The zip file that you downloaded from
Coursera contain the CSS file that we will 
34 
00:02:27,300 --> 00:02:28,557 
use in this hands-on exercise. 
35 
00:02:30,590 --> 00:02:34,330 
Now, load the countries graph
vertices into the visualization 
36 
00:02:34,330 --> 00:02:37,610 
using GraphStreams add node method. 
37 
00:02:37,610 --> 00:02:41,910 
Notice that we are setting
the style UI.class of each vertex 
38 
00:02:41,910 --> 00:02:46,410 
depending on if the vertex is an instance
of the metro, country or continent class. 
39 
00:02:48,480 --> 00:02:53,470 
Add the edges of country's graph to
the visualization using GraphStreams 
40 
00:02:53,470 --> 00:02:54,220 
add edge method. 
41 
00:02:55,960 --> 00:02:59,740 
Finally, call the display
method to visualize the graph. 
42 
00:02:59,740 --> 00:03:01,090 
The graph will look similar to this. 
43 
00:03:02,090 --> 00:03:06,260 
The small blue dots are the metropolises,
the medium sized red dots 
44 
00:03:06,260 --> 00:03:09,470 
are the countries, and
the large green dots are the continents. 
45 
00:03:10,790 --> 00:03:13,590 
By looking at the clusters
you can easily identify which 
46 
00:03:13,590 --> 00:03:16,790 
continent in the visualization
is Antarctica. 
47 
00:03:16,790 --> 00:03:19,780 
It is the green dot that
has no connections or 
48 
00:03:19,780 --> 00:03:22,500 
it is the least connected
cluster in the network. 
49 
00:03:22,500 --> 00:03:24,950 
I hope your paying attention,
because this will be on the quiz. 
50 
00:03:26,040 --> 00:03:30,205 
The large cluster of dots at the bottom is
Asia because it has the most countries and 
51 
00:03:30,205 --> 00:03:32,380 
metropolises in the graph. 
52 
00:03:32,380 --> 00:03:36,360 
The Asia cluster is the most
connected cluster in the network. 
53 
00:03:36,360 --> 00:03:38,980 
Can you identify which clusters
are the other continents 
54 
00:03:38,980 --> 00:03:40,330 
by looking at the visualization? 
55 
00:03:46,796 --> 00:03:50,572 
Let's look at the visualization
of a much larger dataset, 
56 
00:03:50,572 --> 00:03:53,904 
the Facebook dataset with 90,000 vertices. 
57 
00:03:53,904 --> 00:03:57,244 
You've seen these techniques
before with the metro dataset, but 
58 
00:03:57,244 --> 00:04:00,230 
that graph had only around 90 vertices. 
59 
00:04:00,230 --> 00:04:04,736 
Because we want you to learn the concepts,
not be bogged down with repetitive tasks, 
60 
00:04:04,736 --> 00:04:09,321 
we've supplied this Cloudera file in order
to create the visualization of the graph. 
61 
00:04:09,321 --> 00:04:12,230 
All you need to do is open
a new terminal window. 
62 
00:04:13,290 --> 00:04:16,122 
Go into the examples of
analytic directory and 
63 
00:04:16,122 --> 00:04:19,546 
run the spark shell with
the Facebook.Cloudera file. 
64 
00:04:19,546 --> 00:04:22,610 
Don't worry about copying
the full command from the video. 
65 
00:04:22,610 --> 00:04:25,340 
A text version will be included
within the course reading 
66 
00:04:25,340 --> 00:04:28,230 
that contains the full command for
you to copy and paste. 
67 
00:04:33,442 --> 00:04:35,050 
What do you see? 
68 
00:04:35,050 --> 00:04:37,560 
My first reaction is that
it looks like broccoli. 
69 
00:04:37,560 --> 00:04:41,560 
A green vegetable reviled by many
American children, unjustly. 
70 
00:04:41,560 --> 00:04:44,790 
Social networks are made up of
several clusters of communities or 
71 
00:04:44,790 --> 00:04:47,700 
pockets of people who interact densely 
72 
00:04:47,700 --> 00:04:51,570 
that are brought together by people who
are members of multiple communities. 
73 
00:04:51,570 --> 00:04:54,380 
This interlinking of clusters
gives the social network 
74 
00:04:54,380 --> 00:04:57,140 
a broccoli like shape when visualized. 
75 
00:04:57,140 --> 00:05:00,218 
I hope you heard the last thing I said
because that will be on the quiz. 
1 
00:00:00,570 --> 00:00:04,940 
Next we'll import the BreezeViz library,
define a function to calculate the degree 
2 
00:00:04,940 --> 00:00:09,810 
histogram, calculate the probability
distribution for the degree histogram, and 
3 
00:00:09,810 --> 00:00:11,500 
graph the results. 
4 
00:00:11,500 --> 00:00:14,580 
In this hands on exercise,
we will plot the degree histogram created 
5 
00:00:14,580 --> 00:00:18,130 
in the previous hands on exercise
using the BreezeViz library. 
6 
00:00:24,258 --> 00:00:28,742 
Starting again where we left off
in the previous hands-on exercise, 
7 
00:00:28,742 --> 00:00:30,880 
ensure your is started. 
8 
00:00:30,880 --> 00:00:33,170 
First we will import
the BreezeViz library. 
9 
00:00:40,286 --> 00:00:44,958 
Next, we will define a function to
calculate the degree histogram of Metro's 
10 
00:00:44,958 --> 00:00:47,480 
graph, so we can plot it with BreezeViz. 
11 
00:00:47,480 --> 00:00:51,310 
The definition of the degree histogram
function is nearly identical 
12 
00:00:51,310 --> 00:00:54,680 
to the code from the previous
exercise that helped us create. 
13 
00:00:54,680 --> 00:00:56,050 
The histogram data array. 
14 
00:01:02,003 --> 00:01:06,623 
Now we will calculate the probability
distribution of the vertex degrees over 
15 
00:01:06,623 --> 00:01:11,453 
the whole graph by normalizing the vertex
degree by the total number of vertices so 
16 
00:01:11,453 --> 00:01:14,930 
that the degree probabilities add up to 1. 
17 
00:01:14,930 --> 00:01:20,070 
The output of the first command is 28,
the number of countries in country.csv. 
18 
00:01:20,070 --> 00:01:25,200 
The output of the second command
is an array of numbered pairs. 
19 
00:01:25,200 --> 00:01:27,190 
The first number is the vertex degree, 
20 
00:01:27,190 --> 00:01:31,180 
and the second number is
the probability distribution. 
21 
00:01:31,180 --> 00:01:34,590 
Notice that the sum of all
the probability distributions equals one. 
22 
00:01:40,915 --> 00:01:44,950 
Now we will plot two graphs to
visualize the degree histogram. 
23 
00:01:44,950 --> 00:01:48,340 
The first graph will be a line
graph of the degree histogram and 
24 
00:01:48,340 --> 00:01:51,720 
the second will be
the degree histogram itself. 
25 
00:01:51,720 --> 00:01:56,540 
We will not go over in detail how to use
the BreezeViz library to create graphs. 
26 
00:01:56,540 --> 00:02:00,540 
The project is well documented and there
are many resources online if you want to 
27 
00:02:00,540 --> 00:02:03,070 
explore the BreezeViz library on your own. 
28 
00:02:04,740 --> 00:02:08,970 
For the first graph we will define
the X axis as the vertex degree and 
29 
00:02:08,970 --> 00:02:11,910 
the Y axis as the degree probability. 
30 
00:02:11,910 --> 00:02:14,520 
For the second graph we will
just pass the metro graph 
31 
00:02:14,520 --> 00:02:17,360 
degree attribute to
BreezeViz's histogram function 
32 
00:02:37,513 --> 00:02:41,200 
You should see two graphical
representations of the data. 
33 
00:02:41,200 --> 00:02:43,260 
The bottom one is the histogram. 
34 
00:02:43,260 --> 00:02:46,880 
The top shows the degrees
distribution plotting the histogram. 
35 
00:02:46,880 --> 00:02:51,287 
As we saw in the previous exercise's
array most countries have one metropolis. 
1 
00:00:00,940 --> 00:00:03,250 
Welcome back to the fourth and
final module of the course. 
2 
00:00:04,470 --> 00:00:09,740 
In this module, we'll cover the underlying
principles of large scale graph processing 
3 
00:00:09,740 --> 00:00:11,800 
and the software infrastructure
that supports it. 
4 
00:00:13,380 --> 00:00:16,440 
So, in this module,
you'll learn a programming model for 
5 
00:00:16,440 --> 00:00:19,900 
graph computation and
systems that implement this model. 
6 
00:00:21,200 --> 00:00:25,530 
After this model, you'll be able to
formulate graph analytics computations 
7 
00:00:25,530 --> 00:00:26,930 
in terms of the programming model. 
8 
00:00:27,970 --> 00:00:30,340 
This module will consist of three lessons. 
9 
00:00:31,360 --> 00:00:35,790 
The first lesson, will revisit
the concept of the programming model and 
10 
00:00:35,790 --> 00:00:39,720 
introduce a programming model
called Bulk Synchronous Parallel or 
11 
00:00:39,720 --> 00:00:44,250 
BSP that is designed specifically for
graph oriented computation. 
12 
00:00:45,400 --> 00:00:50,520 
I will discuss two well known versions of
the central idea, Pregel from Google, and 
13 
00:00:50,520 --> 00:00:52,760 
Graphlab from Carnegie Mellon University. 
14 
00:00:53,760 --> 00:00:57,260 
The second lesson,
we'll discuss two software systems, 
15 
00:00:57,260 --> 00:01:01,390 
Giraph that operates on Hadoop,
and GraphX that operates on Spark. 
16 
00:01:02,560 --> 00:01:05,890 
We'll compare the basic architecture
of these two systems and 
17 
00:01:05,890 --> 00:01:07,090 
point out their differences. 
18 
00:01:08,200 --> 00:01:13,170 
The final lesson will show example
computations, using graphics platform. 
19 
00:01:13,170 --> 00:01:15,210 
Since graphics come with Spark, 
20 
00:01:15,210 --> 00:01:19,320 
we've created these examples on the
virtual machine, which you have access to. 
21 
00:01:19,320 --> 00:01:21,181 
The code and
some data sets are available for 
22 
00:01:21,181 --> 00:01:22,849 
those who wish to play with the system. 
1 
00:00:00,830 --> 00:00:02,950 
Now we are going to
introduce you to Graphx, 
2 
00:00:04,080 --> 00:00:06,967 
which we'll primarily cover
as a hands-on presentation. 
3 
00:00:08,490 --> 00:00:12,440 
Graphx was developed by the Amp
lab at UC Berkeley, and 
4 
00:00:12,440 --> 00:00:14,020 
has now become an Apache product. 
5 
00:00:15,280 --> 00:00:19,610 
In this brief introduction, we are using
a few slides from Amp lab presentations. 
6 
00:00:22,450 --> 00:00:26,540 
Like [INAUDIBLE] graphic
uses a property graph model. 
7 
00:00:27,580 --> 00:00:31,690 
That means both nodes and
edges can have attributes and values. 
8 
00:00:33,212 --> 00:00:37,700 
In Graphx, the node properties
are stored in a Vertex Table. 
9 
00:00:37,700 --> 00:00:39,940 
And edge properties
are stored in an Edge Table. 
10 
00:00:41,140 --> 00:00:45,680 
The connectivity information, that is
which edge connects to which nodes, 
11 
00:00:45,680 --> 00:00:49,570 
is stored separately from the nodes and
edge properties. 
12 
00:00:49,570 --> 00:00:51,700 
Since Graphx is based on spark, 
13 
00:00:51,700 --> 00:00:57,179 
whose central information representation
is based on resilient data sets or RDDs. 
14 
00:00:58,230 --> 00:01:02,520 
You may recall that RDDs are typically
in memory information objects. 
15 
00:01:03,700 --> 00:01:07,320 
These objects can be used to perform
an action, which returns a value. 
16 
00:01:08,320 --> 00:01:12,420 
Or they can perform a transformation,
which can produce another RDD. 
17 
00:01:13,865 --> 00:01:17,330 
Graphx is built on special RDDs for
vertices and edges. 
18 
00:01:18,820 --> 00:01:22,810 
Note that VertexIDs are defined
to be unique by design. 
19 
00:01:22,810 --> 00:01:28,680 
VertexRDD A represents a set of vertices
all of which have an attribute called A. 
20 
00:01:30,400 --> 00:01:35,400 
The Edge class is an object with a source
vertex, a destination vertex, and 
21 
00:01:35,400 --> 00:01:35,930 
edge attribute. 
22 
00:01:37,280 --> 00:01:42,230 
The EdgeRDD extends this
basic edge by storing 
23 
00:01:42,230 --> 00:01:47,546 
edges in a columnar format on
each partition of performance. 
24 
00:01:47,546 --> 00:01:52,140 
Very often, it's easier to operate
on a data structure that has both 
25 
00:01:52,140 --> 00:01:54,120 
node properties and edge properties. 
26 
00:01:55,380 --> 00:01:59,620 
If you are familiar with this schema, you
will readily see that this is a three-way 
27 
00:01:59,620 --> 00:02:03,990 
join operation between the node set,
the edge set and the second node set. 
28 
00:02:05,010 --> 00:02:08,600 
Finally, GraphX implements
it's own version of BSP. 
29 
00:02:09,820 --> 00:02:12,820 
This implementation,
not surprisingly, is called Pregel. 
30 
00:02:13,950 --> 00:02:19,610 
It allows a user to write a vertex
program, a send message routine, 
31 
00:02:19,610 --> 00:02:21,990 
and a message combiner routine,
just like BSP. 
32 
00:02:23,160 --> 00:02:26,840 
However, this implementation also performs 
33 
00:02:26,840 --> 00:02:28,970 
vertex partitioning like
we saw in graph lab. 
34 
00:02:30,210 --> 00:02:32,170 
Also, similarly to graph lab, 
35 
00:02:32,170 --> 00:02:36,770 
it enables the message sending computation
to reach the attributes of both vertices. 
36 
00:02:37,960 --> 00:02:40,570 
The code limit shown here
is written in Scala. 
37 
00:02:42,230 --> 00:02:46,495 
The hands on code after this you'll
see are all certain in scatter. 
38 
00:02:46,495 --> 00:02:48,860 
Will have a notice here in no scatter. 
39 
00:02:48,860 --> 00:02:53,640 
We would like to show you
here an example which uses 
40 
00:02:53,640 --> 00:02:58,600 
the Pregel runtime object in GraphX
to implement the simple BSP desk. 
41 
00:02:59,870 --> 00:03:04,280 
We would like to show here how Pregel
runtime objects in Graphx works. 
42 
00:03:05,890 --> 00:03:08,760 
We point out all the functions
that are defined by users 
43 
00:03:08,760 --> 00:03:10,160 
in running the Pregel operation. 
44 
00:03:11,420 --> 00:03:16,301 
The vprogf function,
which takes a vertex and a message and 
45 
00:03:16,301 --> 00:03:20,220 
returns a new attribute value for
that vertex. 
46 
00:03:20,220 --> 00:03:25,660 
The sendMsgr function, which computes
the new message along each edge. 
47 
00:03:26,880 --> 00:03:31,850 
The Option[M] construct says that it is
optional for a vertex to send the message. 
48 
00:03:31,850 --> 00:03:34,520 
That is,
the system should not show an error 
49 
00:03:34,520 --> 00:03:37,650 
if your message is not sent for
the vertex. 
50 
00:03:37,650 --> 00:03:40,230 
The combinef function is
a message combiner that we 
51 
00:03:40,230 --> 00:03:42,210 
mention in passive for graph. 
52 
00:03:42,210 --> 00:03:46,913 
Finally, the user can also specify
the number of reiterations that 
53 
00:03:46,913 --> 00:03:49,483 
the GraphX BSP process will run for. 
1 
00:00:01,040 --> 00:00:01,700 
In 2010, Google 
2 
00:00:03,550 --> 00:00:06,650 
published a paper outlining a system
that they had been working on. 
3 
00:00:09,130 --> 00:00:13,740 
This publication about a system called
Pregel, has been one of the most 
4 
00:00:13,740 --> 00:00:18,370 
influential publications on
large scale graph computing. 
5 
00:00:18,370 --> 00:00:22,160 
The Pregel system essentially implemented
the BSB model that we covered in 
6 
00:00:22,160 --> 00:00:22,850 
the last lecture. 
7 
00:00:24,230 --> 00:00:27,180 
To show how the Pregel
system is programmed, 
8 
00:00:27,180 --> 00:00:32,810 
we present the published code of Google's
most famous algorithm, the PageRank. 
9 
00:00:34,300 --> 00:00:38,080 
Recall that PageRank's task is
to compute note centrality. 
10 
00:00:39,240 --> 00:00:43,160 
The basic philosophy of PageRank is
that a note that is connected to an more 
11 
00:00:43,160 --> 00:00:45,350 
important note gains more importance. 
12 
00:00:46,900 --> 00:00:51,400 
In the filler on the right, B is a very
important node with a high page rank 
13 
00:00:51,400 --> 00:00:55,650 
because a lot of other nodes directly or
indirectly point to it. 
14 
00:00:55,650 --> 00:01:00,130 
So C being his direct neighbor
that receives an edge from B, 
15 
00:01:00,130 --> 00:01:02,050 
also has a high page rank. 
16 
00:01:02,050 --> 00:01:05,190 
All the C itself,
there's not have too many incident edges. 
17 
00:01:06,820 --> 00:01:10,500 
On the left side we have
Google's published C++ code 
18 
00:01:10,500 --> 00:01:13,130 
that implements PageRank method
on the Pregel infrastructure. 
19 
00:01:14,450 --> 00:01:17,730 
You don't have to know C++ to understand
the basic essence of the code. 
20 
00:01:18,840 --> 00:01:22,860 
We will explain the basic elements of
the Vertex program in the next few slides. 
21 
00:01:24,540 --> 00:01:27,690 
Remember from the last lecture
that the VSP technique considers 
22 
00:01:27,690 --> 00:01:28,980 
a vertex to be a process. 
23 
00:01:30,360 --> 00:01:34,100 
Every vertex implements
a method called compute. 
24 
00:01:36,070 --> 00:01:40,760 
This method, implements the logic of what
a vertex should do during the super steps. 
25 
00:01:42,490 --> 00:01:47,350 
The program starts by creating a special
kind of vertex called the PageRank vertex 
26 
00:01:47,350 --> 00:01:49,880 
for which this compute
method is specified. 
27 
00:01:51,100 --> 00:01:55,500 
You will notice that the compute
method starts by saying what happens 
28 
00:01:55,500 --> 00:01:57,810 
when the super step is one or more. 
29 
00:01:59,060 --> 00:02:00,590 
But what happens in super step zero? 
30 
00:02:01,900 --> 00:02:05,691 
Usually, superstep 0 is used for
initialization. 
31 
00:02:05,691 --> 00:02:10,940 
Every vertex, initialization
PageRank value to a same number 
32 
00:02:10,940 --> 00:02:14,870 
which is one divided by the total
number of vertices in the graph. 
33 
00:02:16,090 --> 00:02:21,840 
Computationally, a PageRank of a vertex is
a number calculated by adding two terms. 
34 
00:02:23,380 --> 00:02:27,790 
The first term, depends on the total
number of vertices in the graph, and 
35 
00:02:27,790 --> 00:02:29,120 
is therefore the same every time. 
36 
00:02:29,120 --> 00:02:34,990 
And the second term depends on the page
rank of the neighbors of the vertex. 
37 
00:02:36,420 --> 00:02:38,200 
How does the vertex
compute the second term? 
38 
00:02:39,270 --> 00:02:42,590 
It gets the PageRank values of
the neighbors in its messages, 
39 
00:02:42,590 --> 00:02:43,170 
and adds them up. 
40 
00:02:44,390 --> 00:02:48,950 
As we saw for SSSP,
after a vertex computes its value, 
41 
00:02:48,950 --> 00:02:54,170 
it goes to the propagate step and
sends a message to its outgoing edges. 
42 
00:02:54,170 --> 00:02:57,210 
For PageRank, the message it sends out 
43 
00:02:57,210 --> 00:03:00,680 
is just computed value divided
by the number of outgoing edges. 
44 
00:03:01,890 --> 00:03:06,800 
When this is done, the node halts for
the next superstep, and waits for 
45 
00:03:06,800 --> 00:03:08,190 
some other node to wake it up. 
46 
00:03:09,650 --> 00:03:14,590 
At this point, we have seen two
examples of graph analytic operations 
47 
00:03:14,590 --> 00:03:18,380 
executed on the BSB programming model. 
48 
00:03:18,380 --> 00:03:22,370 
Now, we'll look at the same problem
from a slightly different viewpoint. 
49 
00:03:23,710 --> 00:03:28,220 
GraphLab, originally a project from
Carnegie Mellon University, now turned 
50 
00:03:28,220 --> 00:03:32,450 
into a company called Dato, took a similar
yet different approach to the problem. 
51 
00:03:33,970 --> 00:03:37,540 
In GraphLab, any kind of data can be
associated with a vertex or an edge. 
52 
00:03:38,710 --> 00:03:41,310 
This information is stored in
what is called a data graph. 
53 
00:03:42,610 --> 00:03:45,050 
Let's look at the same
page like I already did. 
54 
00:03:45,050 --> 00:03:48,780 
The syntax is a bit different but
the logical blocks are identical. 
55 
00:03:50,080 --> 00:03:52,500 
These are the same blocks
that we highlighted before. 
56 
00:03:53,820 --> 00:03:58,450 
GraphLab breaks up these blocks into
three different user specified functions 
57 
00:03:59,910 --> 00:04:03,780 
called Gather, Apply,
Scatter, or GAS, for short. 
58 
00:04:05,540 --> 00:04:07,050 
Okay, so what's different? 
59 
00:04:08,110 --> 00:04:11,523 
Let's mention a few important differences. 
60 
00:04:11,523 --> 00:04:13,300 
First, let's consider gather. 
61 
00:04:14,830 --> 00:04:19,234 
Rather than adopting a message passing or
data flow model like Pregel, 
62 
00:04:19,234 --> 00:04:24,230 
GraphLab allows the user defined update
function complete freedom to read and 
63 
00:04:24,230 --> 00:04:28,032 
modify any of the data on
adjacent vertices edges. 
64 
00:04:29,822 --> 00:04:35,180 
In GraphLab, a receiving vertex has
access to the data on adjacent vertices 
65 
00:04:35,180 --> 00:04:40,060 
even if the adjacent vertices did
not schedule the current update. 
66 
00:04:41,450 --> 00:04:46,340 
In contrast, for Pregel the control
is with descending nodes. 
67 
00:04:46,340 --> 00:04:49,420 
An update can happen only when
a node sends out messages. 
68 
00:04:50,530 --> 00:04:54,710 
For some graph analytics operations
like a dynamic version of PageRank, 
69 
00:04:54,710 --> 00:04:55,620 
this is very important. 
70 
00:04:56,830 --> 00:05:02,100 
Further, if vertex can update
its value asynchronously. 
71 
00:05:02,100 --> 00:05:06,260 
That is, as soon as it receives
an update without having to wait for 
72 
00:05:06,260 --> 00:05:08,250 
all nodes like we do in VSP. 
73 
00:05:09,630 --> 00:05:14,520 
This often helps some intuitive algorithms
like page rank converge faster. 
74 
00:05:15,610 --> 00:05:18,620 
When the graph is large and
must be split across machines, 
75 
00:05:19,700 --> 00:05:23,310 
BSP cuts the graph along edges,
as we see on this slide. 
76 
00:05:24,480 --> 00:05:28,770 
So every cut edge results in
a machine to machine communication. 
77 
00:05:29,850 --> 00:05:32,710 
If we increase the number
of across machine edges, 
78 
00:05:32,710 --> 00:05:35,250 
we increase communication cost. 
79 
00:05:35,250 --> 00:05:37,330 
This has an interesting
practical consequence. 
80 
00:05:39,110 --> 00:05:43,650 
As we have mentioned,
many graph applications have communities. 
81 
00:05:43,650 --> 00:05:45,290 
And central nodes that have high degree. 
82 
00:05:46,290 --> 00:05:49,150 
Many young people today have
over 500 Facebook friends. 
83 
00:05:50,410 --> 00:05:53,005 
So when an analytical
operation like Page Rank, 
84 
00:05:53,005 --> 00:05:56,850 
that goes through multiple
iterations until convergence, for 
85 
00:05:56,850 --> 00:06:00,070 
these graphs, the communication
cost can become very high. 
86 
00:06:01,270 --> 00:06:05,540 
In the cluster graph shown here, every
color represents a different machine. 
87 
00:06:06,780 --> 00:06:08,917 
Let's look at that red vertex. 
88 
00:06:08,917 --> 00:06:11,239 
What would happen if we
split it like this instead? 
89 
00:06:13,515 --> 00:06:16,250 
The red node gets split. 
90 
00:06:16,250 --> 00:06:21,260 
And different vertices of different
machines work with their own copy 
91 
00:06:21,260 --> 00:06:21,960 
of the red vertex. 
92 
00:06:23,480 --> 00:06:27,490 
In the diagram,
the primary red vertex is marked zero and 
93 
00:06:27,490 --> 00:06:29,330 
copies are marked one through five. 
94 
00:06:30,970 --> 00:06:33,400 
Now the gather phase happens for
each copy. 
95 
00:06:36,010 --> 00:06:39,590 
Followed by a second operation from
the copies to the primary red vertex. 
96 
00:06:41,100 --> 00:06:45,000 
And this is followed by new user
defined operation called merge 
97 
00:06:45,000 --> 00:06:48,488 
that combines the partial results from
the copies to the primary vertex. 
98 
00:06:48,488 --> 00:06:53,290 
So for PageRank, the merge operation
boils down to computing the total 
99 
00:06:53,290 --> 00:06:57,760 
summation of the partial summation
of the edges computed at the copies. 
100 
00:06:57,760 --> 00:07:02,230 
Thus in this lesson,
we have seen two of the most influential 
101 
00:07:02,230 --> 00:07:06,470 
paradigms of large scale graph
computation when the number of nodes and 
102 
00:07:06,470 --> 00:07:09,400 
edges run into tens of millions and more. 
