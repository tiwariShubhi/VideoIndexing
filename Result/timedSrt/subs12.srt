1
00:00:01,860 --> 00:00:06,740
in this video we will provide a quick summary of the main points from our

2
00:00:06,740 --> 00:00:08,960
first course on introduction to big data 

3
00:00:10,310 --> 00:00:12,930
if you have just completed our first course and

4
00:00:12,930 --> 00:00:16,810
do not need a refresher you may now skip to the next lecture 

5
00:00:18,480 --> 00:00:23,000
after this video you will be able to recall

6
00:00:23,000 --> 00:00:27,480
what started the big data era and the three main big data sources 

7
00:00:29,310 --> 00:00:32,830
summarize the volume variety velocity and

8
00:00:32,830 --> 00:00:35,350
veracity issues related to each source 

9
00:00:36,790 --> 00:00:42,140
explain the five step data science process to gain value from big data 

10
00:00:43,510 --> 00:00:46,680
remember the main elements of the hadoop stack 

11
00:00:49,670 --> 00:00:54,950
we began our first course with an explanation of how a lead torrent of

12
00:00:54,950 --> 00:01:00,690
big data combined with cloud computing capabilities to process data anytime and

13
00:01:00,690 --> 00:01:05,450
anywhere has been at the core of the launch of the big data era 

14
00:01:08,470 --> 00:01:13,240
this big torrent of big data is often boil down to a few varieties of data

15
00:01:13,240 --> 00:01:18,282
generated by machines people and

16
00:01:18,282 --> 00:01:23,810
organizations with machine generated data 

17
00:01:23,810 --> 00:01:28,650
we refer to the data generated from real time sensors and industrial machinery or

18
00:01:28,650 --> 00:01:30,020
vehicles 

19
00:01:30,020 --> 00:01:33,350
web logs that track user behavior online 

20
00:01:33,350 --> 00:01:35,470
environmental sensors 

21
00:01:35,470 --> 00:01:38,680
personal health trackers among many other sense data sources 

22
00:01:40,220 --> 00:01:46,050
with human generated data we really refer to the vast amount of social media data 

23
00:01:46,050 --> 00:01:49,760
status updates tweets photos and videos 

24
00:01:51,260 --> 00:01:56,600
with organization generated data we refer to more traditional types of data

25
00:01:56,600 --> 00:01:59,770
including transaction information data bases and

26
00:01:59,770 --> 00:02:02,740
structure data often stored in data warehouses 

27
00:02:03,960 --> 00:02:10,180
note that big data can be structured semi - structured and unstructured 

28
00:02:10,180 --> 00:02:15,030
which is a topic we will talk about more and in depth later in this course 

29
00:02:18,775 --> 00:02:23,975
whatever your big data application is and the types of big data you are using 

30
00:02:23,975 --> 00:02:29,905
the real value will come from integrating different types of data sources and

31
00:02:29,905 --> 00:02:31,755
analyzing them at scale 

32
00:02:33,225 --> 00:02:36,355
overall by modeling managing and

33
00:02:36,355 --> 00:02:40,700
integrating diverse streams to improve our business and

34
00:02:40,700 --> 00:02:44,880
add value to our big data even before we start analyzing it 

35
00:02:45,910 --> 00:02:47,591
as a part of modeling and

36
00:02:47,591 --> 00:02:52,804
managing big data is focusing on the dimensions of scale availability and

37
00:02:52,804 --> 00:02:58,959
considering the challenges associated with this dimensions to pick the right tools 

38
00:03:02,138 --> 00:03:07,460
volume variety and velocity are the main dimensions which

39
00:03:07,460 --> 00:03:12,480
we characterized big data and describe its challenges 

40
00:03:13,710 --> 00:03:18,550
we have huge amounts of data in different formats and

41
00:03:18,550 --> 00:03:21,940
varying quality which must be processed quickly

42
00:03:24,750 --> 00:03:30,810
veracity refers to the biases noise and abnormality in data 

43
00:03:30,810 --> 00:03:36,190
or the unmeasurable certainty is in the truthfulness and trustworthiness of data 

44
00:03:37,270 --> 00:03:41,470
and valence refers to the connectedness of big data 

45
00:03:41,470 --> 00:03:43,790
such as in the form of graph networks 

46
00:03:46,340 --> 00:03:52,500
each v presents a challenging dimension of big data mainly of size 

47
00:03:52,500 --> 00:03:57,160
complexity speed quality and consecutiveness 

48
00:03:57,160 --> 00:04:00,910
although we can list some other v based on the context 

49
00:04:00,910 --> 00:04:05,390
we prefer to list these five as fundamental dimensions which

50
00:04:05,390 --> 00:04:08,350
this big data specialization helps you work on 

51
00:04:09,570 --> 00:04:15,130
moreover we must be sure to never forget the sixth v : value 

52
00:04:15,130 --> 00:04:18,220
at the heart of the big data challenge is turning

53
00:04:18,220 --> 00:04:21,590
all of the other dimensions into truly useful business value 

54
00:04:22,760 --> 00:04:26,490
how will big data benefit you and your organization 

55
00:04:26,490 --> 00:04:30,960
the idea behind processing all this big data in the first place

56
00:04:30,960 --> 00:04:33,100
is to bring value to the problem at hand 

57
00:04:34,360 --> 00:04:38,310
we need to take steps into big data engineering and

58
00:04:38,310 --> 00:04:42,030
scalable data science to generate value out of big data 

59
00:04:43,910 --> 00:04:44,900
we have all heard it 

60
00:04:44,900 --> 00:04:50,370
data signs turns big data into insides or even actions 

61
00:04:51,490 --> 00:04:52,940
but what does that really mean 

62
00:04:54,340 --> 00:04:59,180
data signs can be taught of as the basis for empirical research 

63
00:04:59,180 --> 00:05:02,940
like data is used to induce information on the observations 

64
00:05:04,120 --> 00:05:06,830
these observations are mainly data 

65
00:05:06,830 --> 00:05:12,280
in our case big data related to a business or scientific use case 

66
00:05:14,550 --> 00:05:20,100
inside is a term we use to refer to the data products of data science 

67
00:05:20,100 --> 00:05:23,450
it is extracted from a diverse amount of data

68
00:05:23,450 --> 00:05:27,620
through a combination of exploratory data analysis and modeling 

69
00:05:28,830 --> 00:05:33,560
the questions are sometimes less specific and it can require looking

70
00:05:33,560 --> 00:05:38,140
carefully at the data for patterns in it to come up with a specific question 

71
00:05:40,400 --> 00:05:45,470
another important point to recognize is that data science is not static

72
00:05:45,470 --> 00:05:47,450
one time analysis 

73
00:05:47,450 --> 00:05:52,990
it involves a process where models where you generate give us insights

74
00:05:52,990 --> 00:05:57,155
are constantly improve to a further and prequel evidence and iterations 

1
00:05:58,390 --> 00:06:00,650
there are many ways to look at this process 

2
00:06:01,790 --> 00:06:08,120
one way of looking at it as two distinct activities 

3
00:06:08,120 --> 00:06:14,000
mainly big data engineering and big data analytics 

4
00:06:14,000 --> 00:06:18,940
or computational big data science as i like to call it 

5
00:06:18,940 --> 00:06:21,890
since more than simple analytics are being performed 

6
00:06:23,840 --> 00:06:31,053
a more detailed way of looking at the process reveals five listing steps or

7
00:06:31,053 --> 00:06:35,125
activities of the data science process 

8
00:06:35,125 --> 00:06:40,850
namely acquire prepare analyze report and act 

9
00:06:40,850 --> 00:06:45,900
we can simply say that data science happens at the boundary of all the steps 

10
00:06:45,900 --> 00:06:49,950
ideally this process should support experimental work 

11
00:06:49,950 --> 00:06:55,930
which is constantly iterated and leads to more scientific exploration 

12
00:06:55,930 --> 00:07:01,130
as well as producing actionable results during these explorations

13
00:07:01,130 --> 00:07:05,495
using dynamic scalability on big data and cloud platforms 

14
00:07:08,095 --> 00:07:12,842
this five step process can be used in alternative ways in real life big data

15
00:07:12,842 --> 00:07:17,685
applications if we add the dependencies of different tools to each other 

16
00:07:19,325 --> 00:07:22,915
the influence of big data pushes for

17
00:07:22,915 --> 00:07:27,335
alternative scalability approaches at each step of the process 

18
00:07:28,600 --> 00:07:32,740
acquire includes anything that helps us retrieve data 

19
00:07:32,740 --> 00:07:36,740
including finding accessing acquiring and moving data 

20
00:07:38,500 --> 00:07:46,170
it includes identification of and authenticated access to all related data 

21
00:07:46,170 --> 00:07:50,490
as well as transportation of data from sources to destinations 

22
00:07:51,890 --> 00:07:57,760
it includes ways to subset and match the data to regions or

23
00:07:57,760 --> 00:08:02,350
times of interest which we sometimes refer to as geospatial querying 

24
00:08:04,130 --> 00:08:08,234
we divide the prepare data step into two sub - steps 

25
00:08:08,234 --> 00:08:11,070
based on the nature of the activity 

26
00:08:12,630 --> 00:08:18,060
the first step in data preparation involves exploring the data

27
00:08:18,060 --> 00:08:22,690
to understand its nature what it means its quality and format 

28
00:08:24,160 --> 00:08:27,550
it often takes a preliminary analysis of data or

29
00:08:27,550 --> 00:08:29,670
samples of data to understand it 

30
00:08:30,700 --> 00:08:33,770
this is why this primary step is called prepare 

31
00:08:36,030 --> 00:08:39,990
once we know more about the data through exploratory analysis 

32
00:08:39,990 --> 00:08:43,420
the next step is pre - processing of data for analysis 

33
00:08:44,540 --> 00:08:49,690
it includes cleaning data subsetting or filtering data and

34
00:08:49,690 --> 00:08:55,600
creating data which programs can read and understand by modelling raw data

35
00:08:55,600 --> 00:09:01,710
into a more defined data model or packaging it using a specific data format 

36
00:09:02,850 --> 00:09:08,270
we will learn more about data models and data formats later in this course 

37
00:09:09,940 --> 00:09:12,780
if there are multiple data sets involved 

38
00:09:12,780 --> 00:09:17,220
this step also includes integration of different data sources or

39
00:09:17,220 --> 00:09:20,745
streams which is a topic we will explore in our course three 

40
00:09:23,980 --> 00:09:29,050
the prepared data then would be passed on to the analysis step 

41
00:09:29,050 --> 00:09:32,670
which involves selection of analytical techniques to use 

42
00:09:32,670 --> 00:09:35,840
building a model of the data and analyzing results 

43
00:09:36,920 --> 00:09:40,520
this step can take a couple of iterations on its own or

44
00:09:40,520 --> 00:09:44,119
might require a data scientist to go back to steps 1 and

45
00:09:44,119 --> 00:09:47,410
2 to get more data or package data in a different way 

46
00:09:48,700 --> 00:09:50,520
so exploration never ends 

47
00:09:53,050 --> 00:09:59,413
step 4 for communicating results includes evaluation of analytical results 

48
00:09:59,413 --> 00:10:04,464
presenting them in a visual way creating reports that include

49
00:10:04,464 --> 00:10:09,250
an assessment of results with respect to success criteria 

50
00:10:09,250 --> 00:10:16,293
activities in this step can often be referred to with terms like interpret 

51
00:10:16,293 --> 00:10:20,720
summarize visualize and post - process 

52
00:10:20,720 --> 00:10:25,880
the last step brings us back to the very first reason we do data science 

53
00:10:25,880 --> 00:10:26,550
for a purpose 

54
00:10:28,090 --> 00:10:33,140
reporting insights from analysis and determining actions from insights based

55
00:10:33,140 --> 00:10:38,267
on the purpose you initially defined is what we refer to as the act step 

56
00:10:40,210 --> 00:10:45,140
we have now seen all of the steps in a typical data science process 

57
00:10:45,140 --> 00:10:49,750
please note that this is an iterative process and findings from

58
00:10:49,750 --> 00:10:54,310
one step may require previous steps to be repeated but need information 

59
00:10:54,310 --> 00:10:58,250
leading for further exploration and application of these steps 

60
00:10:59,650 --> 00:11:04,230
scalability of this process to big data analysis requires the use of

61
00:11:04,230 --> 00:11:07,099
big data platforms like hadoop 

1
00:11:07,450 --> 00:11:11,280
the hadoop ecosystem frameworks and applications

2
00:11:11,280 --> 00:11:15,870
provide such functionality through several overarching themes and goals 

3
00:11:18,000 --> 00:11:22,830
first they provide scalability to store large volumes of data

4
00:11:22,830 --> 00:11:24,150
on commodity hardware 

5
00:11:25,760 --> 00:11:28,570
as the number of systems increase so

6
00:11:28,570 --> 00:11:32,620
does the chance for crashes and hardware failures 

7
00:11:32,620 --> 00:11:37,390
they handle fault tolerance to gracefully recover from these problems 

8
00:11:39,650 --> 00:11:44,595
in addition they are designed to handle big data capacity and compressing text

9
00:11:44,595 --> 00:11:50,185
files graphs of social networks streaming sensor data and raster images 

10
00:11:50,185 --> 00:11:52,635
we can add more data types to this variety 

11
00:11:53,695 --> 00:11:55,355
for any given data type 

12
00:11:56,515 --> 00:12:00,635
you can find several projects in the ecosystem that support it 

13
00:12:02,250 --> 00:12:05,890
finally they facilitate a shared environment 

14
00:12:05,890 --> 00:12:09,050
allow multiple jobs to execute simultaneously 

15
00:12:11,350 --> 00:12:15,930
additionally the hadoop ecosystem includes a wide range of open source

16
00:12:15,930 --> 00:12:21,170
projects backed by a large and active community 

17
00:12:21,170 --> 00:12:25,480
these projects are free to use and easy to find support for 

18
00:12:27,060 --> 00:12:32,690
today there are over 100 big data open source projects 

19
00:12:32,690 --> 00:12:38,250
and this continues to grow many rely on hadoop but some are independent 

20
00:12:40,960 --> 00:12:46,230
here is one way of looking at a subset of tools in the hadoop ecosystem 

21
00:12:47,360 --> 00:12:52,240
this layer diagram is organized vertically based on the interface 

22
00:12:53,390 --> 00:12:59,720
lower level interfaces to storage and scheduling on the bottom and

23
00:12:59,720 --> 00:13:03,520
high level languages and interactivity at the top 

24
00:13:06,080 --> 00:13:12,112
the hadoop distributed file system or hdfs is the foundation for

25
00:13:12,112 --> 00:13:18,777
many big data frameworks since it provides scalable and reliable storage 

26
00:13:18,777 --> 00:13:23,915
as the size of your data increases you can add commodity

27
00:13:23,915 --> 00:13:28,314
hardware to hdfs to increase storage capacity 

28
00:13:28,314 --> 00:13:34,107
so it enables what we call scaling out of your resources 

29
00:13:36,267 --> 00:13:39,960
hadoop yarn provide flexible scheduling and

30
00:13:39,960 --> 00:13:43,660
resource management over the htfs storage 

31
00:13:44,660 --> 00:13:50,955
yarn is use at yahoo to schedule jobs across 40 000 servers 

32
00:13:52,105 --> 00:13:57,045
mapreduce is a programming model that simplifies parallel computing 

33
00:13:57,045 --> 00:14:01,725
instead of dealing with the complexities of synchronization and scheduling you only

34
00:14:01,725 --> 00:14:07,325
need to give mapreduce two functions map and reduce 

35
00:14:07,325 --> 00:14:09,643
this programming model is so

36
00:14:09,643 --> 00:14:14,962
powerful that google previously used it for indexing websites 

37
00:14:14,962 --> 00:14:19,820
mapreduce only assumes a limited model to express data 

38
00:14:19,820 --> 00:14:23,429
hive and pig are two additional programming models 

39
00:14:23,429 --> 00:14:27,421
on top of mapreduce to augment data modeling of mapreduce 

40
00:14:27,421 --> 00:14:31,744
with relational algebra and data flow modeling respectively 

41
00:14:33,869 --> 00:14:38,682
hive was created at facebook to issue sql - like queries using

42
00:14:38,682 --> 00:14:41,280
mapreduce on their data in hdfs 

43
00:14:42,410 --> 00:14:47,773
pig was created at yahoo to model dataflow based programs using mapreduce 

44
00:14:47,773 --> 00:14:52,345
thanks to yarns ability to manage resources not just for

45
00:14:52,345 --> 00:14:55,715
mapreduce but other programming models 

46
00:14:55,715 --> 00:14:59,955
giraph was built for processing large scale graphs efficiently 

47
00:15:01,335 --> 00:15:07,676
for example facebook uses giraph to analyze the social graphs of its users 

48
00:15:07,676 --> 00:15:12,181
similarly storm spark and flink were built for

49
00:15:12,181 --> 00:15:16,595
real time and in - memory processing of big data 

50
00:15:16,595 --> 00:15:21,013
on top of the yarn resource scheduler and hdfs 

51
00:15:21,013 --> 00:15:27,555
in - memory processing is a powerful way of running big data applications 

52
00:15:27,555 --> 00:15:33,598
even faster achieving 100x better performance for some tasks 

53
00:15:33,598 --> 00:15:37,845
sometimes your data processing or tasks are not easily or

54
00:15:37,845 --> 00:15:43,332
efficiently represented using the file and directory model of storage 

55
00:15:43,332 --> 00:15:49,010
examples of this include collections of key values or large sparse tables 

56
00:15:50,282 --> 00:15:55,859
nosql projects such as cassandra mongodb and

57
00:15:55,859 --> 00:15:59,681
hbase handle all these cases 

58
00:15:59,681 --> 00:16:04,396
cassandra was created at facebook and facebook also use hbase for

59
00:16:04,396 --> 00:16:06,290
its messaging platform 

60
00:16:08,934 --> 00:16:14,562
finally running all this tools requires a centralized management system for

61
00:16:14,562 --> 00:16:19,520
synchronization configuration and to ensure high availability 

62
00:16:20,660 --> 00:16:25,828
zookeeper created by yahoo to wrangle services named after animals 

63
00:16:25,828 --> 00:16:27,614
performs these duties 

64
00:16:30,214 --> 00:16:34,023
just looking at the small number of hadoop stack components 

65
00:16:34,023 --> 00:16:38,700
we can already see that most of them are dedicated to data modeling 

66
00:16:38,700 --> 00:16:43,010
management and efficient processing of the data 

67
00:16:43,010 --> 00:16:47,610
in the rest of this course we will give you fundamental knowledge and

68
00:16:47,610 --> 00:16:53,580
some practical skills on how to start modeling and managing your data and

69
00:16:53,580 --> 00:16:59,310
picking the right tools for this activity from a plethora of big data tools 

1
00:17:00,170 --> 00:17:03,780
welcome to course two of the big data specialization 

2
00:17:03,780 --> 00:17:05,390
i am amarnath gupta 

3
00:17:05,390 --> 00:17:06,270
 and i am ilkay altintas 

4
00:17:06,270 --> 00:17:09,830
we are really excited to work with you in this course 

5
00:17:09,830 --> 00:17:13,820
to develop your understanding and skills in big data modeling and management 

6
00:17:15,100 --> 00:17:19,470
you might have just finished our first course and see the potential and

7
00:17:19,470 --> 00:17:21,570
challenges of big data 

8
00:17:21,570 --> 00:17:24,630
if you have not it not required but for

9
00:17:24,630 --> 00:17:28,510
those with less background in the area you might find it valuable 

10
00:17:29,890 --> 00:17:33,070
 let explain what we mean by big data modeling and management 

11
00:17:34,400 --> 00:17:38,470
suppose you have an application where the data is big in a sense and

12
00:17:38,470 --> 00:17:44,470
it has a large volume or has high speed or comes with a lot of variations 

13
00:17:44,470 --> 00:17:48,220
even before you think of how to handle the bigness 

14
00:17:48,220 --> 00:17:50,450
you need to have a sense of what the data looks like 

15
00:17:51,550 --> 00:17:56,860
the goal of data modeling is to formally explore the nature of data so

16
00:17:56,860 --> 00:17:59,870
that you can figure out what kind of storage you need and

17
00:17:59,870 --> 00:18:01,470
what kind of processing you can do on it 

18
00:18:03,010 --> 00:18:06,010
the goal of data management is to figure out

19
00:18:06,010 --> 00:18:09,840
what kind of infrastructure support you would need for the data 

20
00:18:09,840 --> 00:18:14,130
for example does your environment need to keep multiple replicas of the data 

21
00:18:14,130 --> 00:18:17,454
do you need to do statistical computation with the data 

22
00:18:17,454 --> 00:18:22,120
once these operational requirements you will 

23
00:18:22,120 --> 00:18:26,110
be able to choose the right system that will let you perform these operations 

24
00:18:27,990 --> 00:18:32,200
 we will also introduce management of big data as it is

25
00:18:32,200 --> 00:18:37,530
streaming from data sources and talk about storage architectures for big data 

26
00:18:37,530 --> 00:18:41,790
for example how can high velocity data get ingested 

27
00:18:41,790 --> 00:18:47,360
managed stored in order to enable real time analytical capabilities 

28
00:18:47,360 --> 00:18:52,560
or what is the difference between data at rest and data in motion 

29
00:18:52,560 --> 00:18:55,230
and how can a data system enable both 

30
00:18:56,570 --> 00:18:59,680
 once you have understood the basic concepts of data modeling 

31
00:18:59,680 --> 00:19:03,300
data management and streaming data we will introduce you

32
00:19:03,300 --> 00:19:08,120
to the characteristics of large volume data and how to think about that 

33
00:19:08,120 --> 00:19:12,080
thus we will transition from classical database management systems 

34
00:19:12,080 --> 00:19:17,730
that is dbmss to big data management systems or bdmss 

35
00:19:17,730 --> 00:19:22,000
we will present brief overviews of several big data management systems

36
00:19:22,000 --> 00:19:23,430
available in the marketplace today 

37
00:19:24,630 --> 00:19:30,310
we are so excited to show you examples of archived and streaming big data sets 

38
00:19:30,310 --> 00:19:35,500
our goal is to provide you with simple hands on exercises that require

39
00:19:35,500 --> 00:19:40,290
no programming but show you what big data looks like and why various

40
00:19:40,290 --> 00:19:46,030
big data management systems are suitable for specific kinds of big data 

41
00:19:46,030 --> 00:19:49,510
in the end you will be able to design a simple

42
00:19:49,510 --> 00:19:52,110
big data information system using this knowledge 

43
00:19:53,110 --> 00:19:55,280
we wish you a fun time learning and

44
00:19:55,280 --> 00:19:59,540
hope to hear from you in the discussion forums and learner stories 

45
00:19:59,540 --> 00:20:01,529
 happy learning and think big data 

1
00:20:02,000 --> 00:20:07,820
this is the second course in the 2016 version of our big data specialization 

2
00:20:07,820 --> 00:20:11,820
 after listening to learners like you we have changed some of the content and

3
00:20:11,820 --> 00:20:13,149
ordering in the specialization 

4
00:20:14,210 --> 00:20:18,200
this course takes you on the first step of any big data project 

5
00:20:18,200 --> 00:20:20,470
its modeling and management 

6
00:20:20,470 --> 00:20:24,221
 we hope that this background on what big data looks like and

7
00:20:24,221 --> 00:20:28,662
how it is modeled and managed in a large scale system will make you feel

8
00:20:28,662 --> 00:20:33,487
more prepared to take steps towards retrieving and processing big data and

9
00:20:33,487 --> 00:20:38,338
perform big data analytics which are topics coming up in the next courses 

