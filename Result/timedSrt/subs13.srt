1
00:00:03,582 --> 00:00:06,800
the third component of a data model is constraints 

2
00:00:08,080 --> 00:00:10,418
a constraint is a logical statement 

3
00:00:10,418 --> 00:00:15,770
that means one can compute and test whether the statement is true or false 

4
00:00:16,980 --> 00:00:21,130
constraints are part of the data model because they can specify something about

5
00:00:21,130 --> 00:00:24,240
the semantics that is the meaning of the data 

6
00:00:24,240 --> 00:00:27,705
for example the constraint that a week has seven and

7
00:00:27,705 --> 00:00:32,402
only seven days is something that a data system would not know unless this

8
00:00:32,402 --> 00:00:36,025
knowledge is passed on to it in the form of a constraint 

9
00:00:36,025 --> 00:00:39,531
another constraint shown here 

10
00:00:42,132 --> 00:00:46,690
tells the system that the number of titles for a movie is restricted to one 

11
00:00:49,740 --> 00:00:50,800
different data models 

12
00:00:50,800 --> 00:00:54,260
as we will see in the next module will have different kinds of constraints 

13
00:00:56,140 --> 00:00:58,760
there may be many different kinds of constraints 

14
00:01:00,020 --> 00:01:04,039
a value constraint is a logical statement about data values 

15
00:01:05,520 --> 00:01:09,600
on a previous slide we have said that the age that is 

16
00:01:09,600 --> 00:01:13,540
the value of data elements representing the age of an entity can not be negative 

17
00:01:16,310 --> 00:01:20,840
we also saw an example of uniqueness constraint when we said

18
00:01:20,840 --> 00:01:23,880
every movie can have only one title 

19
00:01:25,200 --> 00:01:27,120
in the words of logic 

20
00:01:27,120 --> 00:01:32,460
there should exist no data object that a movie and has more than one title 

21
00:01:33,860 --> 00:01:37,719
it easy to see that enforcing these constraints requires us

22
00:01:37,719 --> 00:01:41,446
to count the number of titles and then verify that it one 

23
00:01:41,446 --> 00:01:46,490
now one can generalize this to count the number of values associated with

24
00:01:46,490 --> 00:01:51,310
each object and check whether it lies between an upper and lower bound 

25
00:01:52,380 --> 00:01:56,010
this is often called a cardinality constraint of data property 

26
00:01:59,530 --> 00:02:04,388
in a medical example here the constraint has a lower limit of 0 and

27
00:02:04,388 --> 00:02:05,931
an upper limit of 3 

28
00:02:10,390 --> 00:02:15,253
a different kind of value constraint can be enforced by restricting the type of

29
00:02:15,253 --> 00:02:17,030
the data allowed in a field 

30
00:02:18,620 --> 00:02:23,210
if we do not have such a constraint we can put any type of data in the field 

31
00:02:23,210 --> 00:02:28,300
for example you can have - 99 as the value of the last name of a person 

32
00:02:28,300 --> 00:02:30,680
of course that would be wrong 

33
00:02:30,680 --> 00:02:34,750
to ensure that this does not happen we can enforce the type of the last name

34
00:02:35,780 --> 00:02:40,300
to be a non - numeric alphabetic string 

35
00:02:40,300 --> 00:02:45,420
this example shows a logical expression for this constraint 

36
00:02:45,420 --> 00:02:50,840
a type constraint is a special kind of domain constraint 

37
00:02:50,840 --> 00:02:52,420
the domain of a data property or

38
00:02:52,420 --> 00:02:57,900
attribute is the possible set of values that are allowed for that attribute 

39
00:02:57,900 --> 00:03:01,026
for example the possible values for

40
00:03:01,026 --> 00:03:05,624
the day part of the date field can be between 1 and 31 

41
00:03:05,624 --> 00:03:09,746
while a month may have the value between 1 and 12 

42
00:03:09,746 --> 00:03:16,200
or alternately a value from the set january february ect till december 

43
00:03:17,870 --> 00:03:21,697
now one can devise a more complex constraint where the value of the date

44
00:03:21,697 --> 00:03:26,099
for april june and september and november are restricted between 1 and 30 

45
00:03:26,099 --> 00:03:30,609
and if you think about it all three constraints that we have

46
00:03:30,609 --> 00:03:35,420
described in the last slide are value constraints 

47
00:03:35,420 --> 00:03:39,380
so they only state how to restrict the values of some data property 

48
00:03:40,470 --> 00:03:45,392
in sharp contrast structural properties restrict the structure of the data 

49
00:03:45,392 --> 00:03:48,700
we will choose a more complex example for this 

50
00:03:50,320 --> 00:03:53,952
suppose we are a matrix as shown in the example and

51
00:03:53,952 --> 00:03:56,794
we have restricted to be a square matrix 

52
00:03:56,794 --> 00:04:00,919
so the number of columns is exactly equal to the number of rows 

53
00:04:03,230 --> 00:04:06,570
we have not put any restriction on the number of rows or columns 

54
00:04:06,570 --> 00:04:07,910
but just that they have to be the same 

55
00:04:09,280 --> 00:04:11,560
now this constrains the structure of the matrix and

56
00:04:11,560 --> 00:04:16,140
implies that the number of entries in the structure will be a squared number 

57
00:04:17,480 --> 00:04:22,600
if we convert this matrix to a three column table as shown and impose

58
00:04:22,600 --> 00:04:28,000
the same squareness constraint it will translate to a more complex condition 

59
00:04:28,000 --> 00:04:31,540
that the number of data rows will be the square of

60
00:04:31,540 --> 00:04:34,200
the number of unique values in column one of the table 

61
00:04:35,900 --> 00:04:39,410
we will encounter some more structural constraints in the next module 

1
00:04:42,208 --> 00:04:46,770
the second component of a data model is a set of operations that can be performed

2
00:04:46,770 --> 00:04:48,390
on the data 

3
00:04:48,390 --> 00:04:49,250
and in this module 

4
00:04:49,250 --> 00:04:54,650
we will discuss the operations without considering the bigness aspect 

5
00:04:54,650 --> 00:04:56,110
in course three 

6
00:04:56,110 --> 00:04:59,710
we will come back to the issue of performing these operations when the data is large 

7
00:05:01,515 --> 00:05:04,830
now operations specified the methods to manipulate the data 

8
00:05:05,840 --> 00:05:10,220
since different data models are typically associated with different structures 

9
00:05:10,220 --> 00:05:13,210
the operations on them will be different 

10
00:05:13,210 --> 00:05:18,410
but some types of operations are usually performed across all data models 

11
00:05:18,410 --> 00:05:20,030
we will describe a few of them here 

12
00:05:22,320 --> 00:05:29,818
one common operation extract a part of a collection based on the condition 

13
00:05:29,818 --> 00:05:33,422
in the example here we have a set of records and

14
00:05:33,422 --> 00:05:38,110
we are looking for a sub set that satisfies the condition that

15
00:05:38,110 --> 00:05:42,548
the fifth field has a value greater than 100 000 

16
00:05:42,548 --> 00:05:46,603
the only one record satisfies this requirement 

17
00:05:46,603 --> 00:05:50,580
note that we called this operation subsetting rather loosely 

18
00:05:51,940 --> 00:05:56,210
depending on the context it also called selection or filtering 

19
00:05:59,410 --> 00:06:05,300
the next common operation is retrieving a part of a structure that is specified 

20
00:06:05,300 --> 00:06:08,440
in this case we specify that we are interested

21
00:06:08,440 --> 00:06:12,130
in just the first two fields of a collection off records 

22
00:06:13,880 --> 00:06:19,340
but this produces a new collection of records which has only these fields 

23
00:06:20,350 --> 00:06:23,460
this operation like before has many names 

24
00:06:23,460 --> 00:06:25,910
the most dominant name is projection 

25
00:06:27,340 --> 00:06:31,204
in the next module we will see several versions of this operation for

26
00:06:31,204 --> 00:06:32,588
different data models 

27
00:06:35,108 --> 00:06:39,176
the next two operations are about combining two collections

28
00:06:39,176 --> 00:06:40,460
into a larger one 

29
00:06:41,540 --> 00:06:45,450
the term combine may be interpreted in various ways 

30
00:06:45,450 --> 00:06:47,830
the most straightforward of them is said union 

31
00:06:48,910 --> 00:06:51,500
the assumption behind the union operation

32
00:06:51,500 --> 00:06:54,930
is that the two collections involved have the same structure 

33
00:06:54,930 --> 00:07:01,050
in other words if one collection has four fields and another has 14 fields or

34
00:07:01,050 --> 00:07:06,230
if one has four fields on people and the dates of birth and the other has four

35
00:07:06,230 --> 00:07:10,350
things about countries and their capitols they cannot be combined through union 

36
00:07:12,650 --> 00:07:16,340
in the example here their two collections have three and

37
00:07:16,340 --> 00:07:21,620
two records respectively with one record that common between them 

38
00:07:21,620 --> 00:07:22,120
the green one 

39
00:07:24,100 --> 00:07:27,170
the result collection has four record 

40
00:07:27,170 --> 00:07:32,300
because duplicates are disallowed because it a set operation 

41
00:07:32,300 --> 00:07:37,110
there is indeed another version of union where duplicates are allowed and

42
00:07:37,110 --> 00:07:39,540
will produce five records instead of four 

43
00:07:42,040 --> 00:07:47,020
the second kind of combining called a join can be done when

44
00:07:47,020 --> 00:07:51,460
the two collections have different data content but have some common elements 

45
00:07:53,430 --> 00:07:55,290
in the example shown 

46
00:07:55,290 --> 00:07:58,920
the first field is the common element between the two collections on the left 

47
00:08:00,040 --> 00:08:03,630
in this kind of data combination there are two stages 

48
00:08:03,630 --> 00:08:09,380
first for each data item think of a record of collection one 

49
00:08:09,380 --> 00:08:12,490
one finds a set of matching data items in collection two 

50
00:08:14,120 --> 00:08:17,960
thus the first records of the two collections match

51
00:08:17,960 --> 00:08:19,060
based on the first field 

52
00:08:20,490 --> 00:08:22,750
in the second phase of the operation 

53
00:08:22,750 --> 00:08:25,710
all fields of the matching record pairs are put together 

54
00:08:26,780 --> 00:08:30,170
in the first record of the result collection shown on the right 

55
00:08:30,170 --> 00:08:33,890
one gets the first four fields on the first collection and

56
00:08:33,890 --> 00:08:36,230
the remaining two fields from the second collection 

57
00:08:37,500 --> 00:08:43,380
now in this one example we found one pair of matching records from the collections 

58
00:08:43,380 --> 00:08:46,790
in general one would find more than one matching record pairs 

59
00:08:48,260 --> 00:08:51,415
as you can see this operation is more complex and

60
00:08:51,415 --> 00:08:55,863
can be very expensive when the size of the true collections are large 

1
00:08:55,800 --> 00:09:01,950
in the big data world we often hear the term structured data that is data having

2
00:09:01,950 --> 00:09:07,490
a structure which is quite different from the so - called unstructured data 

3
00:09:07,490 --> 00:09:08,500
but what is a structure 

4
00:09:09,910 --> 00:09:11,040
let consider file 1 

5
00:09:12,310 --> 00:09:16,960
it a typical csv file that has three lines with different content 

6
00:09:16,960 --> 00:09:21,260
but the file content is uniform in the sense that each line 

7
00:09:21,260 --> 00:09:25,400
call it a record has exactly three fields 

8
00:09:25,400 --> 00:09:28,360
which we sometimes call data properties or attributes 

9
00:09:29,720 --> 00:09:36,420
further the first two of these fields are strings and the third one is a date 

10
00:09:36,420 --> 00:09:39,770
we can add more records that lines with the same pattern of data 

11
00:09:39,770 --> 00:09:41,210
to the file in the same fashion 

12
00:09:42,240 --> 00:09:47,430
the content will grow but the pattern of data organization will remain identical 

13
00:09:48,440 --> 00:09:53,600
this repeatable pattern of data organization makes the file structured 

14
00:09:53,600 --> 00:09:57,870
now let look at file 2 which is four records of five fields each 

15
00:09:58,910 --> 00:10:02,680
except that the third record seems to be missing the last entry 

16
00:10:03,710 --> 00:10:04,660
is this file structured 

17
00:10:05,810 --> 00:10:10,383
we argue that it is because the missing value makes the third record incomplete 

18
00:10:10,383 --> 00:10:15,310
but it does not break the structure or the pattern of the data organization 

19
00:10:15,310 --> 00:10:18,088
let looks at these two files side by side 

20
00:10:18,088 --> 00:10:24,320
clearly file 2 has more fields and hence is sort of wider than the first file 

21
00:10:24,320 --> 00:10:26,980
would you say that they have the same structure 

22
00:10:26,980 --> 00:10:28,990
well on the face of it they do not 

23
00:10:28,990 --> 00:10:30,740
but if you think more broadly 

24
00:10:30,740 --> 00:10:34,260
you would notice that they are both collection of k fields 

25
00:10:35,330 --> 00:10:39,770
the size of the collection respectively three and four differs 

26
00:10:39,770 --> 00:10:43,320
and k is 3 in the first case and 5 in the second 

27
00:10:43,320 --> 00:10:47,330
but we can think of 3 and 5 as parameters 

28
00:10:47,330 --> 00:10:52,280
in that case we will say that these files have been generated by a similar

29
00:10:52,280 --> 00:10:57,160
organizational structure and hence they have the same data model 

30
00:10:58,440 --> 00:11:01,190
now in contrast consider this file 

31
00:11:01,190 --> 00:11:05,480
just looking at it it impossible to figure out how the data is organized and

32
00:11:05,480 --> 00:11:07,420
how to identify subparts of the data 

33
00:11:08,440 --> 00:11:10,850
we would call this data unstructured 

34
00:11:11,860 --> 00:11:16,605
often compressed data like jpeg images mp3 audio files 

35
00:11:16,605 --> 00:11:21,450
mpeg3 video files encrypted data are usually unstructured 

36
00:11:22,490 --> 00:11:27,098
in module two we will elaborate on data models that are not fully structured or

37
00:11:27,098 --> 00:11:28,900
are structured differently 

1
00:11:28,890 --> 00:11:31,790
this is the first of two hands - on exercises involving

2
00:11:31,790 --> 00:11:34,170
sensor data from a weather station 

3
00:11:34,170 --> 00:11:37,530
in this one we will look at static data in a text file 

4
00:11:37,530 --> 00:11:40,900
the next one we will look at live data streaming from the weather station

5
00:11:40,900 --> 00:11:41,490
in real time 

6
00:11:41,490 --> 00:11:45,820
in this exercise we will begin by opening a terminal window and

7
00:11:45,820 --> 00:11:49,330
changing into the directory containing the station measurements 

8
00:11:49,330 --> 00:11:52,560
we will look at these measurements in a text file and then look at the key for

9
00:11:52,560 --> 00:11:55,540
these measurements so we can understand what the values mean 

10
00:11:55,540 --> 00:11:57,310
finally we will plot the measurements 

11
00:11:58,340 --> 00:11:58,840
let begin 

12
00:11:59,870 --> 00:12:03,023
first we will open a terminal window by clicking on the terminal icon on the top

13
00:12:03,023 --> 00:12:03,692
of the toolbar 

14
00:12:06,179 --> 00:12:12,814
next we will cd into the directory

15
00:12:12,814 --> 00:12:18,350
containing the sensor data 

16
00:12:19,380 --> 00:12:26,358
we will run cd downloads big - data - two sensor 

17
00:12:28,680 --> 00:12:35,750
we can write ls to see the contents of this directory 

18
00:12:40,274 --> 00:12:46,554
the data from the weather station is in a text file called wx - data txt 

19
00:12:49,370 --> 00:12:54,550
we can run more wx - data txt to see the contents of this file 

20
00:13:02,970 --> 00:13:05,990
each line of this file is a separate set of measurements 

21
00:13:05,990 --> 00:13:10,390
there are two columns in this file the first column is a time stamp and

22
00:13:10,390 --> 00:13:13,160
it separated by a second column by a tab 

23
00:13:13,160 --> 00:13:19,710
the second column itself has separate columns and these are separated by commas 

24
00:13:19,710 --> 00:13:24,790
the time stamp is the number of seconds since 1970 

25
00:13:24,790 --> 00:13:27,450
you will notice that it increases for each time stamp 

26
00:13:29,830 --> 00:13:32,520
you will notice that it increases for each measurement 

27
00:13:32,520 --> 00:13:34,550
but sometimes measurements come in at the same time 

28
00:13:34,550 --> 00:13:38,028
for example this one at 006 

29
00:13:38,028 --> 00:13:43,053
the measurements we see that the prefix is 0r1 for

30
00:13:43,053 --> 00:13:46,188
most of them but some have 0r2 

31
00:13:46,188 --> 00:13:49,060
if we look at the other measurements 

32
00:13:49,060 --> 00:13:54,646
we see that all the 0r1 measurements start with dn dm dx and so on 

33
00:13:54,646 --> 00:13:59,670
whereas r2 begins with ta ua and pa 

34
00:14:02,310 --> 00:14:05,419
if we scroll down in the text file by hitting the space bar 

35
00:14:05,419 --> 00:14:08,728
we will see there are other measurements besides r1 and r2 

36
00:14:11,860 --> 00:14:17,360
for example there r5 that has th vh vs and so on 

37
00:14:17,360 --> 00:14:21,830
and there r0 which has all the measurements 

38
00:14:21,830 --> 00:14:25,510
so dn dm dx ta ua pa 

39
00:14:28,550 --> 00:14:29,930
and the remaining ones 

40
00:14:32,160 --> 00:14:35,434
next we will open another internal window and look at the key to this

41
00:14:35,434 --> 00:14:38,598
measurements click on the tool bar to open the terminal window 

42
00:14:38,598 --> 00:14:43,900
cd into downloads big data two sensor

43
00:14:51,120 --> 00:14:59,131
and the key to these measurements is in a file called wxt520format txt 

44
00:14:59,131 --> 00:15:04,290
we can run more wxt520format txt to see this file 

45
00:15:12,190 --> 00:15:15,116
this file says where each of the prefix is mean for

46
00:15:15,116 --> 00:15:17,307
example sn is the wind speed minimum 

47
00:15:17,307 --> 00:15:24,430
sm is the wind speed average 

48
00:15:24,430 --> 00:15:28,700
and ta is the air temperature 

49
00:15:28,700 --> 00:15:32,530
so if we go back to our sensor file we see here ta equals 13 9c 

50
00:15:32,530 --> 00:15:38,187
that means the air temperature at this

51
00:15:38,187 --> 00:15:43,690
time was 13 9 degrees celsius 

52
00:15:43,690 --> 00:15:49,022
we can also create a plot of the data in this text file

53
00:15:49,022 --> 00:15:53,994
by running plot - data py wx - data txt ta 

54
00:16:00,727 --> 00:16:05,529
this says to plot the data in the wx - data file and the measure that we

55
00:16:05,529 --> 00:16:10,680
want to apply is ta which according to our key is the air temperature 

56
00:16:14,880 --> 00:16:19,520
when we run it it displays a plot of the air temperature found in the text file 

1
00:16:19,568 --> 00:16:21,240
in this hands - on exercise 

2
00:16:21,240 --> 00:16:24,870
we will be looking at an image file which uses an array data model 

3
00:16:26,050 --> 00:16:30,380
first we will open a terminal window and display an image file on the screen 

4
00:16:31,890 --> 00:16:35,630
next we will examine the structure of the image and finally 

5
00:16:35,630 --> 00:16:38,860
extract pixel values from various locations in the image 

6
00:16:40,550 --> 00:16:41,660
let begin 

7
00:16:41,660 --> 00:16:44,954
first we will open a terminal window by clicking on the terminal icon at the top

8
00:16:44,954 --> 00:16:45,650
of the toolbar 

9
00:16:52,101 --> 00:16:56,989
next we will cdn2 the directory containing the image 

10
00:16:56,989 --> 00:17:00,390
cdn2downloads bigdata2 image 

11
00:17:07,410 --> 00:17:10,470
we can run ls to see the image in different scripts 

12
00:17:13,050 --> 00:17:17,344
the file australia jpg is an image that we want to view 

13
00:17:17,344 --> 00:17:19,090
we can use eog to view it 

14
00:17:19,090 --> 00:17:27,830
run eog australia jpg eog is a common image viewer on linux 

15
00:17:30,192 --> 00:17:34,870
australia jpg is a satellite image of the australian continent 

16
00:17:34,870 --> 00:17:36,560
now let look at the structure of this image file 

17
00:17:40,320 --> 00:17:44,815
if we go back to our terminal window we can run the image

18
00:17:44,815 --> 00:17:49,611
viewer in the background by hitting ctrl + z and then bg 

19
00:17:49,611 --> 00:17:54,319
we can view the dimensions or structure of the array data model of this

20
00:17:54,319 --> 00:17:57,904
image by running dimensions py australia jpg 

21
00:18:02,740 --> 00:18:09,860
this says that the image has 5250 columns and 4320 rows 

22
00:18:09,860 --> 00:18:12,240
so it is a two - dimensional image 

23
00:18:12,240 --> 00:18:14,671
additionally each cell or

24
00:18:14,671 --> 00:18:20,470
pixel within this image is composed of three 8 - bit pixels 

25
00:18:20,470 --> 00:18:25,130
these pixels are composed of three elements red green and blue 

26
00:18:25,130 --> 00:18:28,410
we can extract or view the individual pixel elements

27
00:18:28,410 --> 00:18:32,110
at a specific location in the image by using the pixel py script 

28
00:18:33,440 --> 00:18:40,240
we can run pixel py australia jpg 0 0 to see the value at one location 

29
00:18:48,430 --> 00:18:52,430
the 0 0 location is the corner of the image 

30
00:18:52,430 --> 00:18:58,281
if we go back to the image the corners are all the ocean so they are dark blue 

31
00:18:58,281 --> 00:19:02,995
if we look at the value that was extracted we see that blue has a high

32
00:19:02,995 --> 00:19:07,490
value of 50 whereas red and green are low with 11 and 10 

33
00:19:07,490 --> 00:19:10,991
if we view it at another corner by looking at 5000 0 

34
00:19:10,991 --> 00:19:12,610
we will see the same value 

35
00:19:20,136 --> 00:19:24,679
if we go back to the image the middle of the image which is the land 

36
00:19:24,679 --> 00:19:26,070
is orange or yellow 

37
00:19:26,070 --> 00:19:31,261
it definitely not blue so let take a look at a pixel value there 

38
00:19:31,261 --> 00:19:36,010
okay run pixel py australia jpg 2000 2000 

39
00:19:42,750 --> 00:19:50,070
this says that the red has a value of 118 green is 89 and the blue is 57 

40
00:19:50,070 --> 00:19:53,130
so the red and green are higher than blue so it not ocean 

