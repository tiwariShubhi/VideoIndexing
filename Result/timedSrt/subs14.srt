1
00:00:02,370 --> 00:00:05,969
this is the second hands on exercises for csv data 

2
00:00:05,969 --> 00:00:10,155
in the first we saw how to import a csv file into a spreadsheet and

3
00:00:10,155 --> 00:00:12,430
make a simple plot 

4
00:00:12,430 --> 00:00:15,240
in this one we will learn how to filter data and

5
00:00:15,240 --> 00:00:16,830
perform some aggregate operations 

6
00:00:18,390 --> 00:00:21,659
we will begin by opening a terminal window and starting a spreadsheet 

7
00:00:22,800 --> 00:00:26,440
next we will load the csv data into the spreadsheet and

8
00:00:26,440 --> 00:00:29,490
perform a filter over several columns 

9
00:00:29,490 --> 00:00:32,398
finally we will calculate an average and sum from the data 

10
00:00:32,398 --> 00:00:35,313
let begin 

11
00:00:35,313 --> 00:00:39,470
first we open a terminal window by clicking on the terminal icon in

12
00:00:39,470 --> 00:00:40,290
the top toolbar 

13
00:00:42,770 --> 00:00:46,296
we can start the spreadsheet by writing oocalc 

14
00:00:51,338 --> 00:00:54,590
next let load our csv data into the spreadsheet 

15
00:00:54,590 --> 00:01:02,142
we click on file open census csv 

16
00:01:05,719 --> 00:01:08,426
and click ok on this dialog to load the data 

17
00:01:12,387 --> 00:01:17,344
column f in the spreadsheet is the state and

18
00:01:17,344 --> 00:01:22,704
column h is the census population for 2010 

19
00:01:25,110 --> 00:01:28,480
let create a filter that just shows the data for

20
00:01:28,480 --> 00:01:32,920
california for counties larger than one million people 

21
00:01:34,100 --> 00:01:39,030
we can create this filter by first selecting both the state name column and

22
00:01:39,030 --> 00:01:41,210
the census 2000 population column 

23
00:01:42,790 --> 00:01:48,054
next we go to data filter standard filter 

24
00:01:52,184 --> 00:01:56,025
here we change the field name to be the state name 

25
00:01:56,025 --> 00:02:01,061
the condition we leave at equals and the value we use california 

26
00:02:06,318 --> 00:02:11,055
this filters all the rows unless the state is california 

27
00:02:15,143 --> 00:02:16,498
we then want to filter for

28
00:02:16,498 --> 00:02:20,590
all the counties whose population is greater than one million people 

29
00:02:20,590 --> 00:02:24,400
to do that in this second line here we change the operator to and 

30
00:02:26,180 --> 00:02:31,710
the field name should be census 2010 population and

31
00:02:31,710 --> 00:02:35,040
the condition should be greater than 

32
00:02:35,040 --> 00:02:36,431
then set the value to be one million 

33
00:02:41,923 --> 00:02:44,893
and click ok 

34
00:02:44,893 --> 00:02:49,603
we can see that all the data from the spreadsheet has disappeared except where

35
00:02:49,603 --> 00:02:51,559
the state name is california and

36
00:02:51,559 --> 00:02:54,762
the population is greater than one million people 

37
00:02:56,310 --> 00:03:00,110
we can reset or remove this filter to see all the data again

38
00:03:00,110 --> 00:03:04,240
by going to data filter 

39
00:03:06,260 --> 00:03:07,020
reset filter 

40
00:03:08,390 --> 00:03:11,550
you can perform aggregate operations on the data in a spreadsheet 

41
00:03:13,160 --> 00:03:16,830
next let perform some aggregate operations over the data 

42
00:03:16,830 --> 00:03:19,220
we can compute the average and the sum of the data 

43
00:03:20,460 --> 00:03:23,980
to do this let run these calculations in a separate sheet 

44
00:03:25,470 --> 00:03:28,181
create a new sheet by clicking on the green plus button 

45
00:03:31,854 --> 00:03:37,656
to compute the average we select a cell and enter = average and

46
00:03:37,656 --> 00:03:43,470
then we select the data that we want to compute the average from 

47
00:03:44,870 --> 00:03:46,150
if we go back to sheet one 

48
00:03:46,150 --> 00:03:50,190
we can select some of the data from the h column 

49
00:03:50,190 --> 00:03:54,634
so let just choose several counties in alabama 

50
00:03:54,634 --> 00:03:58,267
when we hit enter it takes us back to sheet one and

51
00:03:58,267 --> 00:04:01,306
we can see that the average is computing 

52
00:04:03,513 --> 00:04:12,220
similarly we can compute the sum by entering = sum open parentheses 

53
00:04:12,220 --> 00:04:16,120
going back to sheet one and selecting the columns we want to sum 

54
00:04:16,120 --> 00:04:21,360
when we are done hit enter and the sum is computed 

1
00:04:23,360 --> 00:04:26,130
in this hands on exercise we will be looking at json data 

2
00:04:28,040 --> 00:04:30,600
first we will open a terminal window and

3
00:04:30,600 --> 00:04:34,300
then look at the contents of a json file containing tweets from twitter 

4
00:04:35,860 --> 00:04:38,800
next we will examine the schema of this json file 

5
00:04:39,810 --> 00:04:42,920
finally we will extract different fields from the json data 

6
00:04:45,340 --> 00:04:46,470
let begin 

7
00:04:46,470 --> 00:04:48,307
first we will open a terminal window 

8
00:04:48,307 --> 00:04:50,964
by clicking on the terminal icon at the top of the toolbar 

9
00:04:58,240 --> 00:05:03,005
next we will see cd into the directory containing the json data 

10
00:05:03,005 --> 00:05:06,594
by running cddownload big data 2 json 

11
00:05:13,298 --> 00:05:15,520
we can run ls to see the json file 

12
00:05:19,836 --> 00:05:25,030
the json file is called twitter json 

13
00:05:25,030 --> 00:05:28,980
we can run more twitter json to view the contents of this file 

14
00:05:37,280 --> 00:05:41,990
the json data contains semi - structured data which is nested several levels 

15
00:05:41,990 --> 00:05:46,350
there are many tweets in this file and it hard to read using the more command 

16
00:05:46,350 --> 00:05:50,690
you can use space to scroll down and when we are done hit q 

17
00:05:53,420 --> 00:05:58,358
we can run the jsonschema pi command to view the schema of this data 

18
00:05:58,358 --> 00:06:03,179
we run jsonschema pitwitter json

19
00:06:10,715 --> 00:06:13,670
and we will add a pipe more at the end 

20
00:06:17,665 --> 00:06:21,572
this shows the nested fields within this data 

21
00:06:21,572 --> 00:06:27,139
at the top level there are fields like contributors text and id and so

22
00:06:27,139 --> 00:06:32,900
on but there are also fields nested within these top level fields for

23
00:06:32,900 --> 00:06:38,372
example entities also contains called symbols media hashtags and

24
00:06:38,372 --> 00:06:41,691
so on if we scroll down by hitting space 

25
00:06:41,691 --> 00:06:46,270
we will see that there several levels of nesting 

26
00:06:46,270 --> 00:06:53,320
for example user also has follow request sent id and so on 

27
00:06:57,105 --> 00:07:01,466
we can run the print json script to view the contents of a particular tweet and

28
00:07:01,466 --> 00:07:03,746
a particular field within that tweet 

29
00:07:03,746 --> 00:07:08,018
let run print json py 

30
00:07:08,018 --> 00:07:11,812
it asks for the file name so

31
00:07:11,812 --> 00:07:16,300
we will enter twitter json 

32
00:07:19,407 --> 00:07:22,434
and we will look at tweet 99 

33
00:07:24,645 --> 00:07:27,340
so let look at the top level field called text 

34
00:07:31,506 --> 00:07:35,884
so here we see the text for note the 99th tweet in this file 

35
00:07:35,884 --> 00:07:40,110
we could also look at a nested field within the file by running print json

36
00:07:40,110 --> 00:07:45,276
again the file name is twitter json 

37
00:07:47,975 --> 00:07:49,833
we will look at tweet 99 again 

38
00:07:53,097 --> 00:07:56,240
and we will look at the field entities hashtags 

39
00:07:56,240 --> 00:08:00,565
the hashtags that are embedded or nested within the entities field 

1
00:08:02,390 --> 00:08:08,427
this is the first of two hands on exercises involving csv data 

2
00:08:08,427 --> 00:08:12,568
in this exercise we will import a csv file into a spreadsheet and

3
00:08:12,568 --> 00:08:13,860
make a simple plot 

4
00:08:15,290 --> 00:08:20,350
we will begin by opening a terminal window and looking at a csv file in the terminal 

5
00:08:20,350 --> 00:08:23,330
next we will start the spreadsheet application and

6
00:08:23,330 --> 00:08:25,410
import the csv data into the spreadsheet 

7
00:08:25,410 --> 00:08:30,760
we can then look at the rows and columns of the csv file and make a simple plot 

8
00:08:33,088 --> 00:08:35,837
let begin first open a terminal shell by

9
00:08:35,837 --> 00:08:39,423
clicking on the black terminal icon at the top of the toolbar 

10
00:08:47,759 --> 00:08:52,914
next let cd into the directory containing the csv data 

11
00:08:52,914 --> 00:08:58,587
we will run cd space download big - data - 2 csv 

12
00:08:58,587 --> 00:09:04,042
we can run ls to

13
00:09:04,042 --> 00:09:11,325
see the csv files 

14
00:09:11,325 --> 00:09:16,190
the file census csv contains census data for the united states 

15
00:09:16,190 --> 00:09:21,367
we can run the command more census csv to see the contents of this file 

16
00:09:29,262 --> 00:09:32,925
the first line of this file is the header with the columns separated by commas 

17
00:09:32,925 --> 00:09:37,705
you can go down in the file by hitting space 

18
00:09:41,295 --> 00:09:42,735
hit q to quit more 

19
00:09:45,570 --> 00:09:48,210
next let start a spreadsheet application 

20
00:09:48,210 --> 00:09:52,446
we run oocalc to start this 

21
00:10:00,318 --> 00:10:04,453
we can import the census data csv file into

22
00:10:04,453 --> 00:10:08,598
the spreadsheet by going to file open 

23
00:10:12,568 --> 00:10:16,067
clicking on downloads 

24
00:10:16,067 --> 00:10:22,468
big data 2 csv census csv 

25
00:10:27,531 --> 00:10:29,109
in this dialog click ok 

26
00:10:44,945 --> 00:10:47,355
you can see into this spreadsheet 

27
00:10:47,355 --> 00:10:50,900
import of the csv data to a bunch of rows and columns 

28
00:10:52,880 --> 00:10:56,460
each column that was separated by a comma in the csv file 

29
00:10:56,460 --> 00:10:58,580
is a column in the spreadsheet 

30
00:10:59,700 --> 00:11:02,590
we can see that our csv file was successfully imported into

31
00:11:02,590 --> 00:11:03,250
the spreadsheet 

32
00:11:05,650 --> 00:11:07,892
if we scroll down to the bottom of the spreadsheet 

33
00:11:07,892 --> 00:11:10,094
we can see how many rows there were in the csv file 

34
00:11:19,939 --> 00:11:25,372
there are 3194 rows in the csv file 

35
00:11:25,372 --> 00:11:29,149
if this file instead had millions or 10 millions of rows 

36
00:11:29,149 --> 00:11:33,237
then we would have to use a big data system such as hadoop or hdfs 

37
00:11:36,882 --> 00:11:38,089
let scroll back to the top 

38
00:11:43,592 --> 00:11:49,351
next let make a simple plot of some of the data in the csv file 

39
00:11:49,351 --> 00:11:54,993
let plot the population estimates for several years for the state of alabama 

40
00:11:54,993 --> 00:12:00,558
the state of alabama is given in the second row and

41
00:12:00,558 --> 00:12:06,813
the population estimates are given in these columns 

42
00:12:06,813 --> 00:12:12,225
let select j through o so you get the population

43
00:12:12,225 --> 00:12:16,657
estimate for 2010 through 2015 

44
00:12:20,086 --> 00:12:24,331
we can create a plot of these values by clicking on the chart button 

45
00:12:29,393 --> 00:12:32,806
and clicking finish 

46
00:12:32,806 --> 00:12:36,442
in the second hands on for csv data we will perform some filtering and

47
00:12:36,442 --> 00:12:38,669
some aggregate operations over the data 

1
00:12:40,170 --> 00:12:40,670
welcome 

2
00:12:41,800 --> 00:12:44,200
in this module we will talk about data models 

3
00:12:45,530 --> 00:12:48,770
if you completed the introductory course of this specialization 

4
00:12:48,770 --> 00:12:50,940
you might recall our video on data variety 

5
00:12:52,220 --> 00:12:54,840
one way to characterize data variety

6
00:12:54,840 --> 00:12:59,290
is to identify the different models of data that are used in any application 

7
00:13:01,120 --> 00:13:02,480
so what is a data model 

8
00:13:02,480 --> 00:13:05,980
and why do we care about data models in the context of big data 

9
00:13:05,980 --> 00:13:10,010
in this lesson we will introduce you to three components of a data model and

10
00:13:10,010 --> 00:13:11,380
what they tell us about the data 

11
00:13:12,550 --> 00:13:17,620
so after this lesson you will be able to distinguish between structured and

12
00:13:17,620 --> 00:13:19,270
unstructured data 

13
00:13:19,270 --> 00:13:24,880
describe four basic data operations namely selection projection union and join 

14
00:13:24,880 --> 00:13:29,120
and enumerate different types of data constraints like type value and

15
00:13:29,120 --> 00:13:31,240
structural constraints 

16
00:13:31,240 --> 00:13:35,760
you will also be able to explain why constraints are useful to specify

17
00:13:35,760 --> 00:13:38,320
the semantics of data 

18
00:13:38,320 --> 00:13:42,980
now regardless of whether the data is big or small one needs to know or determine

19
00:13:42,980 --> 00:13:48,100
the characteristics of data before one can manipulate or analyze them meaningfully 

20
00:13:48,100 --> 00:13:52,320
let use a simple example suppose you have data is

21
00:13:52,320 --> 00:13:57,330
a file of records with fields called first name last name and date of birth of

22
00:13:57,330 --> 00:14:02,840
the employees in the company that this file consists of records with fields 

23
00:14:02,840 --> 00:14:07,290
and not for instance plain text gives us more insight

24
00:14:07,290 --> 00:14:12,770
into the organization of the data in the file and hence is part of the data model 

25
00:14:12,770 --> 00:14:15,810
this aspect is called structure 

26
00:14:15,810 --> 00:14:19,930
similarly the consideration that we can perform

27
00:14:19,930 --> 00:14:22,980
data arithmetic with the date of birth field and

28
00:14:22,980 --> 00:14:28,340
not with the first name field is also part of our understanding of data model 

29
00:14:28,340 --> 00:14:30,620
these are called operations 

30
00:14:30,620 --> 00:14:35,300
finally we may know that in this company no one age

31
00:14:35,300 --> 00:14:38,950
that is today date minus the date of birth cannot be less than 18 

32
00:14:38,950 --> 00:14:44,000
so it gives us a way to detect records with blatantly erroneous dates of birth 

33
00:14:44,000 --> 00:14:45,940
in the following three videos 

34
00:14:45,940 --> 00:14:49,790
we will look at these three aspects of data models more carefully 

1
00:14:50,120 --> 00:14:54,140
we mentioned before that a data model is characterized by the structure of

2
00:14:54,140 --> 00:14:58,430
the data that it admits the operations on that structure and

3
00:14:58,430 --> 00:15:00,090
a way to specify constraints 

4
00:15:01,290 --> 00:15:02,750
in this lesson 

5
00:15:02,750 --> 00:15:07,950
we will present a more detailed description of a number of common data models 

6
00:15:07,950 --> 00:15:09,800
we will start with relational data 

7
00:15:11,460 --> 00:15:15,780
it is one of the simplest and most frequently used data models today and

8
00:15:15,780 --> 00:15:19,320
forms the basis of many other traditional database management systems 

9
00:15:19,320 --> 00:15:22,930
like mysql oracle teradata and so forth 

10
00:15:24,480 --> 00:15:27,940
so after this video you will be able to

11
00:15:27,940 --> 00:15:30,680
describe the structural components of a relational data model 

12
00:15:32,320 --> 00:15:35,430
demonstrate which components become a data model schema 

13
00:15:37,760 --> 00:15:40,760
explain the purpose of primary and foreign keys 

14
00:15:41,770 --> 00:15:44,370
and describe join and other operations 

15
00:15:47,070 --> 00:15:48,610
the primary data structure for

16
00:15:48,610 --> 00:15:52,380
a relational model is a table like the one shown here for a toy application 

17
00:15:54,030 --> 00:15:58,740
but we need to be careful about relational tables which are also called relations 

18
00:16:00,110 --> 00:16:04,695
this table actually represents a set of tuples 

19
00:16:06,550 --> 00:16:11,390
this is a relational tuple represented as a row in the table 

20
00:16:12,500 --> 00:16:15,140
we were informally calling this a record before 

21
00:16:16,350 --> 00:16:22,110
but a relational tuple implies that unless otherwise stated the elements of it

22
00:16:22,110 --> 00:16:26,600
like 203 or 204 mary and so forth are atomic 

23
00:16:27,950 --> 00:16:32,300
that is they represent one unit of information and

24
00:16:32,300 --> 00:16:33,740
cannot be decomposed further 

25
00:16:34,830 --> 00:16:37,150
we will return to this issue in the next few slides 

26
00:16:38,490 --> 00:16:42,935
thus this is a relation of six tuples 

27
00:16:44,468 --> 00:16:47,490
remember the definition of sets 

28
00:16:47,490 --> 00:16:52,270
it a collection of distinct elements of the same type 

29
00:16:53,420 --> 00:16:59,920
that means i cannot add this tuple to the solution 

30
00:16:59,920 --> 00:17:03,350
because if i do it will be introducing a duplicate 

31
00:17:04,820 --> 00:17:10,330
now in practice many systems will allow duplicate tuples in a relation but

32
00:17:10,330 --> 00:17:15,450
mechanisms are provided to prevent duplicate entries if the user so chooses 

33
00:17:15,450 --> 00:17:17,170
so i cannot add it 

34
00:17:17,170 --> 00:17:20,170
here is another tuple i cannot add 

35
00:17:20,170 --> 00:17:22,680
it has all the right pieces of information but

36
00:17:22,680 --> 00:17:24,410
it all in the wrong order 

37
00:17:24,410 --> 00:17:28,300
so it is a tuple dissimilar with the other six tuples in the relation 

38
00:17:29,300 --> 00:17:32,690
okay so how does the system know that this tuple is different 

39
00:17:34,390 --> 00:17:38,700
this brings our attention to the very first row that is the header of this table

40
00:17:38,700 --> 00:17:39,710
painted in black 

41
00:17:41,680 --> 00:17:44,910
this row is part of the scheme of the table 

42
00:17:44,910 --> 00:17:45,420
lets look at it 

43
00:17:46,890 --> 00:17:50,190
it tells us the name of the table in this case employee 

44
00:17:51,580 --> 00:17:55,560
this also tells us the names of the six columns called attributes of the relation 

45
00:17:56,740 --> 00:17:58,090
and for each column 

46
00:17:58,090 --> 00:18:02,370
it tells us the allowed data type that is the type constraint for each column 

47
00:18:03,610 --> 00:18:05,340
given this schema 

48
00:18:05,340 --> 00:18:09,890
it should now be clear why the last red row does not belong to this table 

49
00:18:10,940 --> 00:18:15,180
the schema in a relational table can also specify constraints 

50
00:18:16,330 --> 00:18:19,150
shown in yellow in the third line of the schema row 

51
00:18:20,720 --> 00:18:25,050
it says that the minimum salary of a person has to be greater than 25000 

52
00:18:27,150 --> 00:18:33,670
further it states that every employee must have a first and last name 

53
00:18:33,670 --> 00:18:35,660
they cannot be left null that means without a value 

54
00:18:37,840 --> 00:18:40,900
why does not department or title column have this constraint 

55
00:18:42,500 --> 00:18:46,990
one answer can be that a newly hired employee may not be assigned

56
00:18:46,990 --> 00:18:50,350
a department or a title yet but can still be an entry in the table 

57
00:18:51,560 --> 00:18:55,040
however the department column has another constraint 

58
00:18:56,130 --> 00:19:00,390
it restricts the possible values that is the domain of the attribute

59
00:19:00,390 --> 00:19:03,060
to only four possibilities 

60
00:19:03,060 --> 00:19:06,900
hr it research and business 

61
00:19:08,620 --> 00:19:13,840
finally the first says that id is a primary key 

62
00:19:15,180 --> 00:19:18,960
this means it is unique for each employee 

63
00:19:18,960 --> 00:19:22,200
and for every employee knowing the primary key for

64
00:19:22,200 --> 00:19:27,720
the employee will also uniquely know the other five attributes of that employee 

65
00:19:29,180 --> 00:19:32,240
you should now see that a table with a primary key

66
00:19:32,240 --> 00:19:37,590
logically implies that the table cannot have a duplicate record because if we do 

67
00:19:37,590 --> 00:19:40,870
it will violate the uniqueness constraint associated with the primary key 

68
00:19:42,330 --> 00:19:46,140
let us introduce a new table containing the salary history of employees 

69
00:19:47,410 --> 00:19:51,010
the employees are identified with the column empid but

70
00:19:51,010 --> 00:19:53,760
these are not new values that this table happens to have 

71
00:19:54,900 --> 00:19:59,460
they are the same ids that are present in the id column of the employee table 

72
00:19:59,460 --> 00:20:00,140
presented earlier 

73
00:20:02,420 --> 00:20:04,570
this is reflected in the statement made on the right 

74
00:20:05,760 --> 00:20:12,530
the term references means the values in this column can exist

75
00:20:12,530 --> 00:20:17,760
only if the same values if you are in employees the table being referenced 

76
00:20:17,760 --> 00:20:19,700
also called the parent table 

77
00:20:21,150 --> 00:20:27,920
so in the terminology of the relational model the empid column of empsalaries

78
00:20:27,920 --> 00:20:33,660
table is called a foreign key that refers to the primary key of the employees table 

79
00:20:35,560 --> 00:20:41,650
note that empid is not a primary key in this empsalaries table 

80
00:20:41,650 --> 00:20:45,420
because it is multiple to post with the same empid

81
00:20:45,420 --> 00:20:47,920
reflecting the salary of the employee at different times 

82
00:20:49,700 --> 00:20:53,070
you will remember join is a common operation that we discussed before 

83
00:20:54,150 --> 00:20:58,200
so here is an example of a relational join performed

84
00:20:58,200 --> 00:21:02,810
on the first three columns of employee and empsalaries table 

85
00:21:02,810 --> 00:21:09,630
where employees id and empsalaries empid columns are matched for equality 

86
00:21:10,770 --> 00:21:13,130
the output table shows all the columns involved 

87
00:21:14,360 --> 00:21:16,360
the common column is represented once 

88
00:21:17,680 --> 00:21:20,210
this form of join is called a natural join 

89
00:21:21,690 --> 00:21:26,770
it is important to understand that join is one of the most expensive

90
00:21:26,770 --> 00:21:30,570
that means time consuming and space consuming operations 

91
00:21:31,770 --> 00:21:37,270
as data becomes larger and tables contain hundreds of millions of tuples the join

92
00:21:37,270 --> 00:21:41,480
operation can easily become a bottleneck in a larger analytic application 

93
00:21:42,880 --> 00:21:48,280
so for analytical big data application that needs joins it very important to

94
00:21:48,280 --> 00:21:53,890
choose a suitable data management platform that makes this operation efficient 

95
00:21:53,890 --> 00:21:56,090
we will return to this issue in module four 

96
00:21:57,760 --> 00:21:59,600
we end this video on a practical note 

97
00:22:00,830 --> 00:22:02,060
in many scientific and

98
00:22:02,060 --> 00:22:05,990
business applications people start with csv files 

99
00:22:05,990 --> 00:22:10,650
manipulate them with the spreadsheet then migrate their relational system only

100
00:22:10,650 --> 00:22:14,790
as an afterthought where the data becomes too large to handle the spreadsheet 

101
00:22:16,490 --> 00:22:19,500
while the spreadsheet offers many useful features 

102
00:22:19,500 --> 00:22:25,037
it does not conform and enforce many principles of relational data models 

103
00:22:26,700 --> 00:22:31,210
consequently a large amount of time may be spent in cleaning up and

104
00:22:31,210 --> 00:22:34,480
correcting data errors after the migration actually happens 

105
00:22:35,670 --> 00:22:40,860
let me show a few examples from a spreadsheet that has 125 000 rows and

106
00:22:40,860 --> 00:22:41,430
over 100 columns 

107
00:22:42,950 --> 00:22:48,280
the spreadsheet here lists terrorism attacks gathered from news media 

108
00:22:48,280 --> 00:22:51,030
so each row represents one attack 

109
00:22:52,080 --> 00:22:56,210
this is a valuable piece of data for people who study terrorism 

110
00:22:56,210 --> 00:22:59,420
but we are going to look at it from a relational data modelling viewpoint 

111
00:23:01,200 --> 00:23:04,160
first notice the column marked in green 

112
00:23:05,750 --> 00:23:09,900
it lists two weapons used in the attack separated by a semicolon 

113
00:23:11,330 --> 00:23:13,390
why is this really common 

114
00:23:13,390 --> 00:23:15,950
it makes this column non - atomic 

115
00:23:15,950 --> 00:23:20,450
it means that this column actually has two different values 

116
00:23:21,570 --> 00:23:26,140
in a relational design this information will be moved to another table

117
00:23:26,140 --> 00:23:29,870
just like the multiple salaries of employees were placed in a separate table 

118
00:23:31,100 --> 00:23:34,740
next notice the column outlined in red 

119
00:23:36,290 --> 00:23:41,370
it describes the amount of property damaged by a possible terrorist attack 

120
00:23:42,890 --> 00:23:48,880
in this column the intended legitimate values are unknown 

121
00:23:48,880 --> 00:23:50,540
minor major and catastrophic 

122
00:23:51,670 --> 00:23:56,350
however the value in the highlighted part of the spreadsheet is minor and

123
00:23:56,350 --> 00:24:01,012
then within bracket likely less than 1 million 

124
00:24:01,012 --> 00:24:05,700
which means a query like find all attacks for

125
00:24:05,700 --> 00:24:09,390
which the property damage is equal to minor cannot be answered directly 

126
00:24:10,680 --> 00:24:14,150
instead we need to perform a substring search for

127
00:24:14,150 --> 00:24:16,630
minor in the beginning of the description 

128
00:24:16,630 --> 00:24:19,420
which is doable but it a more expensive operation 

129
00:24:20,960 --> 00:24:24,730
this shows the columns of the spreadsheet 

130
00:24:24,730 --> 00:24:26,950
so this is part of the schema of the data 

131
00:24:28,120 --> 00:24:31,310
if you observe carefully you will see a recurring pattern 

132
00:24:33,120 --> 00:24:38,290
the designer of the data table determined that there can be at most three types of

133
00:24:38,290 --> 00:24:43,380
attacks within a single encounter and represented with three separate columns 

134
00:24:44,790 --> 00:24:50,220
now in proper relational modeling one would say that there is a one to many

135
00:24:50,220 --> 00:24:55,650
relationship between the attack and the number of attack types 

136
00:24:57,210 --> 00:24:59,980
in such a case it would be more prudent

137
00:24:59,980 --> 00:25:04,030
to place these attack type columns in a separate table and

138
00:25:04,030 --> 00:25:08,480
connect with the parent using a primary key foreign key relationship 

139
00:25:09,920 --> 00:25:13,240
here another block with a similar pattern 

140
00:25:14,490 --> 00:25:18,420
this time this is about the types and subtypes of weapons used 

141
00:25:19,620 --> 00:25:22,690
now can you determine how you might be able to reorganize this block 

142
00:25:24,130 --> 00:25:25,761
we will leave this as an exercise 

