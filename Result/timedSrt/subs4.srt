1
00:00:01,150 --> 00:00:05,590
in this video we will talk about a new that is usually not covered much 

2
00:00:06,820 --> 00:00:07,510
it called valence 

3
00:00:15,960 --> 00:00:19,440
simply put valence refers to connectedness 

4
00:00:20,440 --> 00:00:23,660
the more connected data is the higher it valences 

5
00:00:24,830 --> 00:00:27,000
the term valence comes from chemistry 

6
00:00:28,030 --> 00:00:32,800
in chemistry we talk about core electrons and valence electrons of an atom 

7
00:00:33,910 --> 00:00:38,810
valence electrons are in the outer most shell have the highest energy level and

8
00:00:38,810 --> 00:00:41,960
are responsible for bonding with other atoms 

9
00:00:41,960 --> 00:00:46,560
that higher valence results in greater boding that is greater connectedness 

10
00:00:47,740 --> 00:00:51,670
this idea is carried over into our definition of the term valence

11
00:00:51,670 --> 00:00:53,180
in the context of big data 

12
00:00:55,010 --> 00:00:58,750
data items are often directly connected to one another 

13
00:00:58,750 --> 00:01:01,490
a city is connected to the country it belongs to 

14
00:01:02,530 --> 00:01:06,030
two facebook users are connected because they are friends 

15
00:01:06,030 --> 00:01:08,340
an employee is connected to his work place 

16
00:01:09,640 --> 00:01:11,470
data could also be indirectly connected 

17
00:01:12,710 --> 00:01:16,150
two scientists are connected because they are both physicists 

18
00:01:17,530 --> 00:01:23,520
for a data collection valence measures the ratio of actually connected data items

19
00:01:23,520 --> 00:01:27,390
to the possible number of connections that could occur within the collection 

20
00:01:28,460 --> 00:01:31,100
the most important aspect of valence

21
00:01:31,100 --> 00:01:33,870
is that the data connectivity increases over time 

22
00:01:34,910 --> 00:01:39,620
the series of network graphs comes from a social experiment where scientists

23
00:01:39,620 --> 00:01:44,230
attending a conference were asked to meet other scientists they did not know before 

24
00:01:44,230 --> 00:01:45,830
after several rounds of meetings 

25
00:01:45,830 --> 00:01:49,840
they found new connections shown by their red edges 

26
00:01:49,840 --> 00:01:55,500
increase in valence can lead to emergent group behavior in people networks 

27
00:01:55,500 --> 00:02:00,310
like creation of new groups and coalitions that have shared values and goals 

28
00:02:02,760 --> 00:02:04,770
a high valence data set is denser 

29
00:02:06,060 --> 00:02:09,670
this makes many regular analytic critiques very inefficient 

30
00:02:10,930 --> 00:02:14,350
more complex analytical methods must be adopted to account for

31
00:02:14,350 --> 00:02:15,490
the increasing density 

32
00:02:16,780 --> 00:02:20,760
more interesting challenges arise due to the dynamic behavior of the data 

33
00:02:22,030 --> 00:02:24,170
now there is a need to model and

34
00:02:24,170 --> 00:02:28,480
predict how valence of a connected data set may change with time and volume 

35
00:02:29,890 --> 00:02:34,660
the dynamic behavior also leads to the problem of event detection 

36
00:02:34,660 --> 00:02:38,510
such as bursts in the local cohesion in parts of the data 

37
00:02:38,510 --> 00:02:40,760
and emergent behavior in the whole data set 

38
00:02:40,760 --> 00:02:43,570
such as increased polarization in a community 

1
00:02:44,450 --> 00:02:47,620
now we will talk about a form of scalability called variety 

2
00:02:49,040 --> 00:02:53,500
in this case scale does not refer to the largeness of data 

3
00:02:53,500 --> 00:02:55,398
it refers to increased diversity 

4
00:03:04,985 --> 00:03:07,450
here is an important mantra you need to think about 

5
00:03:08,660 --> 00:03:13,530
when we as data scientists think of data variety we think of the additional

6
00:03:13,530 --> 00:03:18,450
complexity that results from more kinds of data that we need to store process 

7
00:03:18,450 --> 00:03:20,010
and combine 

8
00:03:20,010 --> 00:03:22,970
now many years ago when i started studying data management 

9
00:03:23,980 --> 00:03:26,030
we always thought of data as tables 

10
00:03:27,310 --> 00:03:33,060
these tables could be in spreadsheets or databases or just files but somehow

11
00:03:33,060 --> 00:03:36,290
they will be modeled and manipulated as rows and columns of of tables 

12
00:03:37,610 --> 00:03:43,620
now tables are still really important and dominant however today a much wider

13
00:03:43,620 --> 00:03:47,800
variety of data are collected stored and analyzed to solve real world problems 

14
00:03:48,990 --> 00:03:53,610
image data text data network data geographic maps computer

15
00:03:53,610 --> 00:03:58,390
generated simulations are only a few of the types of data we encounter everyday 

16
00:03:59,460 --> 00:04:04,010
the heterogeneity of data can be characterized along several dimensions 

17
00:04:04,010 --> 00:04:05,990
we mentioned four such axes here 

18
00:04:07,910 --> 00:04:10,630
structural variety refers to the difference in

19
00:04:10,630 --> 00:04:12,870
the representation of the data 

20
00:04:12,870 --> 00:04:16,600
for example an ekg signal is very different from a newspaper article 

21
00:04:17,690 --> 00:04:22,630
a satellite image of wildfires from nasa is very different from tweets

22
00:04:22,630 --> 00:04:25,110
sent out by people who are seeing the fire spread 

23
00:04:26,230 --> 00:04:31,100
media variety refers to the medium in which the data gets delivered 

24
00:04:31,100 --> 00:04:35,150
the audio of a speech versus the transcript of the speech

25
00:04:35,150 --> 00:04:38,300
may represent the same information in two different media 

26
00:04:39,460 --> 00:04:42,872
data objects like news video may have multiple media 

27
00:04:42,872 --> 00:04:46,436
an image sequence an audio and closed captioned text 

28
00:04:46,436 --> 00:04:48,730
all time synchronized to each other 

29
00:04:49,850 --> 00:04:54,160
semantic variety is best described two examples 

30
00:04:54,160 --> 00:04:58,480
we often use different units for quantities we measure 

31
00:04:58,480 --> 00:05:02,870
sometimes we also use qualitative versus quantitative measures 

32
00:05:02,870 --> 00:05:06,230
for example age can be a number or

33
00:05:06,230 --> 00:05:09,440
we represent it by terms like infant juvenile or adult 

34
00:05:11,140 --> 00:05:14,030
another kind of semantic variety comes from different

35
00:05:14,030 --> 00:05:16,450
assumptions of conditions on the data 

36
00:05:16,450 --> 00:05:22,510
for example if we conduct two income surveys on two different groups of people 

37
00:05:22,510 --> 00:05:23,970
we may not be able to compare or

38
00:05:23,970 --> 00:05:27,020
combine them without knowing more about the populations themselves 

39
00:05:28,300 --> 00:05:30,620
the variation and availability takes many forms 

40
00:05:31,730 --> 00:05:35,510
for one data can be available real time 

41
00:05:35,510 --> 00:05:39,110
like sensor data or it can be stored like patient records 

42
00:05:40,350 --> 00:05:43,430
similarly data can be accessible continuously for

43
00:05:43,430 --> 00:05:44,630
example from a traffic cam 

44
00:05:45,640 --> 00:05:47,490
versus intermittently for

45
00:05:47,490 --> 00:05:50,750
example only when the satellite is over the region of interest 

46
00:05:51,780 --> 00:05:55,650
this makes a difference between what operations one can do with data 

47
00:05:55,650 --> 00:05:58,020
especially if the volume of the data is large 

48
00:05:59,260 --> 00:06:02,700
we will cover this in more detail in course two

49
00:06:02,700 --> 00:06:07,049
when we explore the different genres of data and how we model them 

50
00:06:08,620 --> 00:06:11,470
we should not think that a single data object or

51
00:06:11,470 --> 00:06:15,880
a collection of similar data objects will be all uniform in themselves 

52
00:06:15,880 --> 00:06:18,040
emails for example is a hybrid entity 

53
00:06:19,480 --> 00:06:23,110
some of this information can be a table like shown here 

54
00:06:24,310 --> 00:06:26,940
now the body of the email usually has text in it 

55
00:06:28,040 --> 00:06:31,360
however some of the text may have ornaments around them 

56
00:06:31,360 --> 00:06:32,370
for example 

57
00:06:32,370 --> 00:06:37,320
the part highlighted in yellow represents something called a markup on text 

58
00:06:38,420 --> 00:06:41,600
we will get to markups later in the course 

59
00:06:41,600 --> 00:06:43,500
emails contain attachments 

60
00:06:43,500 --> 00:06:45,920
these are files or embedded images 

61
00:06:45,920 --> 00:06:49,140
or other multimedia objects that the mailer allows 

62
00:06:49,140 --> 00:06:53,150
this screenshot from my outlook shows the image of a scanned image

63
00:06:53,150 --> 00:06:54,470
of a handwritten note 

64
00:06:55,540 --> 00:06:59,230
when you take a collection of all emails from your mailbox or

65
00:06:59,230 --> 00:07:03,360
that from an organization you will see that senders and

66
00:07:03,360 --> 00:07:05,490
receivers form a communication network 

67
00:07:07,130 --> 00:07:11,917
in 2001 there was a famous scandal around a company called enron that engaged in

68
00:07:11,917 --> 00:07:15,390
fraudulent financial reporting practices 

69
00:07:15,390 --> 00:07:20,450
their email network partly shown here has been studied by data scientist to find

70
00:07:20,450 --> 00:07:25,330
usual and unusual patterns of connections among the people in the organization 

71
00:07:26,810 --> 00:07:30,080
an email collection can also have it own semantics 

72
00:07:30,080 --> 00:07:34,340
for example an email cannot refer to that means cannot copy or

73
00:07:34,340 --> 00:07:35,640
forward a previous email 

74
00:07:36,920 --> 00:07:41,170
finally an email server is a real - time data source 

75
00:07:41,170 --> 00:07:43,160
but an email repository is not 

76
00:07:44,190 --> 00:07:48,610
does email and email collections demonstrate significant

77
00:07:48,610 --> 00:07:53,270
internal variation in structure media semantics and availability 

1
00:07:53,090 --> 00:07:56,478
characteristics of big data - velocity 

2
00:08:12,075 --> 00:08:16,612
velocity refers to the increasing speed at which big data is created and

3
00:08:16,612 --> 00:08:21,160
the increasing speed at which the data needs to be stored and analyzed 

4
00:08:22,250 --> 00:08:27,310
processing of data in real - time to match its production rate as it gets generated

5
00:08:27,310 --> 00:08:30,810
is a particular goal of big data analytics 

6
00:08:30,810 --> 00:08:34,090
for example this type of capability allows for

7
00:08:34,090 --> 00:08:38,110
personalization of advertisement on the web pages you visit

8
00:08:38,110 --> 00:08:42,610
based on your recent search viewing and purchase history 

9
00:08:42,610 --> 00:08:48,350
if a business cannot take advantage of the data as it gets generated or

10
00:08:48,350 --> 00:08:52,540
at the speed analysis of it is needed they often miss opportunities 

11
00:08:53,620 --> 00:08:58,540
in order to build a case for the importance of this dimension of big data 

12
00:08:58,540 --> 00:09:00,290
let imagine we are taking a road trip 

13
00:09:01,850 --> 00:09:05,610
you are looking for some better information to start packing 

14
00:09:05,610 --> 00:09:07,730
in this case the newer the information 

15
00:09:07,730 --> 00:09:12,140
the higher its relevance in deciding what to pack 

16
00:09:12,140 --> 00:09:14,420
would you use last month weather information or

17
00:09:14,420 --> 00:09:17,310
data from last year at this time 

18
00:09:17,310 --> 00:09:22,550
or would you use the weather information from this week yesterday or

19
00:09:22,550 --> 00:09:23,630
better today 

20
00:09:24,820 --> 00:09:28,750
it makes sense to obtain the latest information about weather and

21
00:09:28,750 --> 00:09:31,830
process it in a way that makes your decisions easier 

22
00:09:31,830 --> 00:09:37,600
if the information is old it does not matter how accurate it is 

23
00:09:38,950 --> 00:09:42,190
being able to catch up with the velocity of big data and

24
00:09:42,190 --> 00:09:47,640
analyzing it as it gets generated can even impact the quality of human life 

25
00:09:47,640 --> 00:09:53,700
sensors and smart devices monitoring the human body can detect abnormalities

26
00:09:53,700 --> 00:09:59,510
in real time and trigger immediate action potentially saving lives 

27
00:09:59,510 --> 00:10:03,950
this type of processing is what we call real time processing 

28
00:10:03,950 --> 00:10:08,210
real - time processing is quite different from its remote relative 

29
00:10:08,210 --> 00:10:08,800
batch processing 

30
00:10:11,270 --> 00:10:15,320
batch processing was the norm until a couple of years ago 

31
00:10:15,320 --> 00:10:19,160
large amounts of data would be fed into large machines and

32
00:10:19,160 --> 00:10:20,870
processed for days at a time 

33
00:10:22,230 --> 00:10:26,870
while this type of processing is still very common today decisions based on

34
00:10:26,870 --> 00:10:32,490
information that is even few days old can be catastrophic to some businesses 

35
00:10:34,620 --> 00:10:38,320
organizations which make decisions on latest data

36
00:10:38,320 --> 00:10:39,960
are more likely to hit the target 

37
00:10:41,860 --> 00:10:45,780
for this reason it important to match the speed of processing

38
00:10:45,780 --> 00:10:50,345
with the speed of information generation and get real time decision making power 

39
00:10:50,345 --> 00:10:54,497
in addition today sensor - powered

40
00:10:54,497 --> 00:10:59,843
socioeconomic climate requires faster decisions 

41
00:10:59,843 --> 00:11:04,332
hence we can not wait for all the data to be first produced 

42
00:11:04,332 --> 00:11:06,190
then fed into a machine 

43
00:11:07,480 --> 00:11:11,830
there are many applications where new information is streaming and

44
00:11:11,830 --> 00:11:15,120
needs to be integrated with existing data to

45
00:11:15,120 --> 00:11:19,825
produce decisions such as emergency response planning in a tornado or

46
00:11:19,825 --> 00:11:25,500
deciding trading strategies in real time or getting estimates in advertising 

47
00:11:27,520 --> 00:11:33,340
we have to digest chunks of data as they are produced and give meaningful results 

48
00:11:35,900 --> 00:11:37,385
as more data comes in 

49
00:11:37,385 --> 00:11:41,930
your results will need to adapt to reflect this change in the input 

50
00:11:43,060 --> 00:11:49,160
decisions based on processing of already acquired data such as batch processing 

51
00:11:49,160 --> 00:11:51,660
may give an incomplete picture 

52
00:11:51,660 --> 00:11:57,550
and hence the applications need real time status of the context at hand 

53
00:11:57,550 --> 00:11:58,910
that is streaming analysis 

54
00:12:00,280 --> 00:12:05,420
fortunately with the event of cheap sensors technology 

55
00:12:05,420 --> 00:12:10,520
mobile phones and social media we can obtain the latest information

56
00:12:10,520 --> 00:12:14,940
at a much rapid rate and in real time in comparison with the past 

57
00:12:15,990 --> 00:12:19,650
so how do you make sure we match the velocity of the expectations

58
00:12:19,650 --> 00:12:21,910
to gain insights from big data 

59
00:12:21,910 --> 00:12:24,900
with the velocity of the big data 

60
00:12:24,900 --> 00:12:27,050
rate of generation retrieval 

61
00:12:27,050 --> 00:12:30,650
or processing of data is application specific 

62
00:12:31,760 --> 00:12:36,660
the need for real time data - driven actions within a business case is

63
00:12:36,660 --> 00:12:41,620
what in the end dictates the velocity of analytics over big data 

64
00:12:42,920 --> 00:12:46,620
sometimes precision of a minute is needed 

65
00:12:46,620 --> 00:12:48,750
sometimes half a day 

66
00:12:48,750 --> 00:12:53,710
let look at these four paths and discuss when to pick the right one for

67
00:12:53,710 --> 00:12:55,270
your analysis 

68
00:12:55,270 --> 00:12:59,937
the dollar signs next to the numbers in this example indicate how costly

69
00:12:59,937 --> 00:13:02,000
the operation is 

70
00:13:02,000 --> 00:13:04,160
the more dollars the higher the cost 

71
00:13:05,600 --> 00:13:09,750
when the timeliness of processed information plays no role in decision

72
00:13:09,750 --> 00:13:14,950
making the speed at which data is generated becomes irrelevant 

73
00:13:14,950 --> 00:13:20,220
in other words you can wait for as long as it takes to process data 

74
00:13:20,220 --> 00:13:22,730
days months weeks 

75
00:13:22,730 --> 00:13:26,120
and once processing is over you will look at the results and

76
00:13:26,120 --> 00:13:27,550
probably share them with someone 

77
00:13:28,680 --> 00:13:33,640
when timeliness is not an issue you can choose any of the four paths 

78
00:13:34,800 --> 00:13:37,150
you will likely pick the cheapest one 

79
00:13:38,290 --> 00:13:41,720
when timeliness of end result is an issue

80
00:13:41,720 --> 00:13:45,810
deciding which of the four paths to choose is not so simple 

81
00:13:45,810 --> 00:13:49,620
you will have to make a decision based on cost of hardware 

82
00:13:49,620 --> 00:13:54,110
time sensitivity of information future scenarios 

83
00:13:54,110 --> 00:13:59,480
in other words this becomes a business driven question 

84
00:13:59,480 --> 00:14:05,930
for example if speed is really important at all costs you will pick path four 

85
00:14:05,930 --> 00:14:12,130
as a summary we need to pay attention to the velocity of big data 

86
00:14:12,130 --> 00:14:16,460
streaming data gives information on what is going on right now 

87
00:14:16,460 --> 00:14:21,670
streaming data has velocity meaning it gets generated at various rates 

88
00:14:21,670 --> 00:14:26,370
and analysis of such data in real time gives agility and

89
00:14:26,370 --> 00:14:30,620
adaptability to maximize benefits you want to extract 

1
00:14:31,640 --> 00:14:34,377
characteristics of big data veracity 

2
00:14:50,366 --> 00:14:53,980
veracity of big data refers to the quality of the data 

3
00:14:55,010 --> 00:14:59,760
it sometimes gets referred to as validity or

4
00:14:59,760 --> 00:15:03,250
volatility referring to the lifetime of the data 

5
00:15:04,840 --> 00:15:08,360
veracity is very important for making big data operational 

6
00:15:09,810 --> 00:15:13,900
because big data can be noisy and uncertain 

7
00:15:13,900 --> 00:15:20,885
it can be full of biases abnormalities and it can be imprecise 

8
00:15:20,885 --> 00:15:24,741
data is of no value if it not accurate 

9
00:15:24,741 --> 00:15:29,900
the results of big data analysis are only as good as the data being analyzed 

10
00:15:31,480 --> 00:15:36,840
this is often described in analytics as junk in equals junk out 

11
00:15:38,040 --> 00:15:42,760
so we can say although big data provides many opportunities to make

12
00:15:42,760 --> 00:15:48,100
data enabled decisions the evidence provided by data

13
00:15:48,100 --> 00:15:52,710
is only valuable if the data is of a satisfactory quality 

14
00:15:53,880 --> 00:15:56,870
there are many different ways to define data quality 

15
00:15:57,890 --> 00:16:00,200
in the context of big data 

16
00:16:00,200 --> 00:16:04,040
quality can be defined as a function of a couple of different variables 

17
00:16:05,230 --> 00:16:11,310
accuracy of the data the trustworthiness or reliability of the data source 

18
00:16:11,310 --> 00:16:14,940
and how the data was generated are all important factors

19
00:16:14,940 --> 00:16:16,720
that affect the quality of data 

20
00:16:18,100 --> 00:16:23,910
additionally how meaningful the data is with respect to the program that

21
00:16:23,910 --> 00:16:30,360
analyzes it is an important factor and makes context a part of the quality 

22
00:16:32,040 --> 00:16:37,850
in this chart from 2015 we see the volumes of data increasing 

23
00:16:37,850 --> 00:16:42,380
starting with small amounts of enterprise data to larger 

24
00:16:42,380 --> 00:16:47,480
people generated voice over ip and social media data and

25
00:16:47,480 --> 00:16:51,010
even larger machine generated sensor data 

26
00:16:51,010 --> 00:16:56,040
we also see that the uncertainty of the data increases as we go from

27
00:16:56,040 --> 00:16:58,840
enterprise data to sensor data 

28
00:16:58,840 --> 00:17:01,320
this is as we would expect it to be 

29
00:17:01,320 --> 00:17:05,980
traditional enterprise data in warehouses have

30
00:17:05,980 --> 00:17:11,080
standardized quality solutions like master processes for extract 

31
00:17:11,080 --> 00:17:16,310
transform and load of the data which we referred to as before as etl 

32
00:17:16,310 --> 00:17:21,080
as enterprises started incorporating less structured and unstructured people and

33
00:17:21,080 --> 00:17:24,870
machine data into their big data solutions 

34
00:17:24,870 --> 00:17:28,670
the data become messier and more uncertain 

35
00:17:28,670 --> 00:17:30,210
there are many reasons for this 

36
00:17:31,220 --> 00:17:37,620
first unstructured data on the internet is imprecise and uncertain 

37
00:17:37,620 --> 00:17:43,730
in addition high velocity big data leaves very little or no time for

38
00:17:43,730 --> 00:17:49,810
etl and in turn hindering the quality assurance processes of the data 

39
00:17:49,810 --> 00:17:54,090
let look at these product reviews for a banana slicer on amazon com 

40
00:17:54,090 --> 00:17:58,960
one of the five star reviews say that

41
00:17:58,960 --> 00:18:04,190
it saved her marriage and compared it to the greatest inventions in history 

42
00:18:04,190 --> 00:18:08,900
another five star reviewer said that his parole officer recommended

43
00:18:08,900 --> 00:18:12,470
the slicer as he is not allowed to be around knives 

44
00:18:12,470 --> 00:18:14,400
these are obviously fake reviewers 

45
00:18:15,410 --> 00:18:19,070
now think of an automated product assessment going through such

46
00:18:19,070 --> 00:18:23,930
splendid reviews and estimating lots of sales for the banana slicer and

47
00:18:23,930 --> 00:18:28,390
in turn suggesting stocking more of the slicer in the inventory 

48
00:18:28,390 --> 00:18:29,670
amazon will have problems 

49
00:18:30,760 --> 00:18:36,420
for a more serious case let look at the google flu trends case from 2013 

50
00:18:36,420 --> 00:18:41,788
for january 2013 the google friends actually

51
00:18:41,788 --> 00:18:47,280
estimated almost twice as many flu cases as was reported by cdc 

52
00:18:47,280 --> 00:18:50,870
the centers for disease control and prevention 

53
00:18:52,390 --> 00:18:56,960
the primary reason behind this was that google flu trends used a big data on

54
00:18:56,960 --> 00:19:02,070
the internet and did not account properly for uncertainties about the data 

55
00:19:03,210 --> 00:19:07,780
maybe the news and social media attention paid to the particularly

56
00:19:07,780 --> 00:19:12,450
serious level of flu that year effected the estimate 

57
00:19:12,450 --> 00:19:16,840
and resulted in what we call an over estimation 

58
00:19:16,840 --> 00:19:19,050
this is a perfect example for

59
00:19:19,050 --> 00:19:25,470
how inaccurate the results can be if only big data is used in the analysis 

60
00:19:25,470 --> 00:19:29,820
imagine the economical impact of making health care preparations for

61
00:19:29,820 --> 00:19:31,920
twice the amount of flu cases 

62
00:19:31,920 --> 00:19:33,610
that would be huge 

63
00:19:33,610 --> 00:19:37,850
the google flu trends example also brings up the need for

64
00:19:37,850 --> 00:19:43,350
being able to identify where exactly the big data they used comes from 

65
00:19:43,350 --> 00:19:46,300
what transformation did big data go through up

66
00:19:46,300 --> 00:19:49,420
until the moment it was used for a estimate 

67
00:19:49,420 --> 00:19:53,460
this is what we refer to as data providence 

68
00:19:53,460 --> 00:19:57,630
just like we refer to an artifacts provenance 

69
00:19:57,630 --> 00:20:03,180
as a summary the growing torrents of big data pushes for

70
00:20:03,180 --> 00:20:06,590
fast solutions to utilize it in analytical solutions 

71
00:20:07,630 --> 00:20:12,000
this creates challenges on keeping track of data quality 

72
00:20:12,000 --> 00:20:16,510
what has been collected where it came from and

73
00:20:16,510 --> 00:20:18,940
how it was analyzed prior to its use 

74
00:20:20,150 --> 00:20:22,910
this is akin to an art artifact

75
00:20:22,910 --> 00:20:25,490
having providence of everything it has gone through 

76
00:20:26,540 --> 00:20:31,560
but even more complicated to achieve with large volumes of data coming

77
00:20:31,560 --> 00:20:34,130
in varieties and velocities 

1
00:20:34,830 --> 00:20:37,232
characteristics of big data - volume 

2
00:20:47,965 --> 00:20:53,190
volume is the big data dimension that relates to the sheer size of big data 

3
00:20:54,410 --> 00:20:58,580
this volume can come from large datasets being shared or

4
00:20:58,580 --> 00:21:04,240
many small data pieces and events being collected over time 

5
00:21:05,460 --> 00:21:09,335
every minute 204 million emails are sent 

6
00:21:09,335 --> 00:21:15,308
200 000 photos are uploaded and 1 8 million likes are generated on facebook 

7
00:21:15,308 --> 00:21:22,710
on youtube 1 3 million videos are viewed and 72 hours of video are uploaded 

8
00:21:25,160 --> 00:21:28,120
but how much data are we talking about 

9
00:21:28,120 --> 00:21:33,170
the size and the scale of storage for big data can be massive 

10
00:21:33,170 --> 00:21:37,310
you heard me say words that start with peta exa and

11
00:21:37,310 --> 00:21:43,220
yotta to define size but what does all that really mean 

12
00:21:43,220 --> 00:21:49,210
for comparison 100 megabytes will hold a couple of encyclopedias 

13
00:21:50,460 --> 00:21:54,340
a dvd is around 5 gbs and

14
00:21:54,340 --> 00:21:59,837
1 tb would hold around 300 hours of good quality video 

15
00:21:59,837 --> 00:22:06,530
a data - oriented business currently collects data in the order of terabytes 

16
00:22:06,530 --> 00:22:09,880
but petabytes are becoming more common to our daily lives 

17
00:22:10,930 --> 00:22:16,290
cern large hadron collider generates 15 petabytes a year 

18
00:22:16,290 --> 00:22:21,340
according to predictions by an idc report sponsored by a big data company called

19
00:22:21,340 --> 00:22:28,150
emc digital data will grow by a factor of 44 until the year 2020 

20
00:22:28,150 --> 00:22:32,190
this is a growth from 0 8 zetabytes 

21
00:22:33,750 --> 00:22:38,280
in 2009 to 35 2 zettabytes in 2020 

22
00:22:38,280 --> 00:22:45,090
a zettabyte is 1 trillion gigabytes that 10 to the power of 21 

23
00:22:45,090 --> 00:22:48,140
the effects of it will be huge ! 

24
00:22:49,160 --> 00:22:54,420
think of all the time cost energy that will be used to store and

25
00:22:54,420 --> 00:22:57,860
make sense of such an amount of data 

26
00:22:57,860 --> 00:23:00,430
the next era will be yottabytes 

27
00:23:00,430 --> 00:23:05,050
ten to the power of 24 and brontobytes ten to the power of 27 

28
00:23:05,050 --> 00:23:10,410
which is really hard to imagine for most of us at this time 

29
00:23:10,410 --> 00:23:15,740
this is also what we call data at an astronomical scale 

30
00:23:15,740 --> 00:23:18,280
the choice of putting the milky way galaxy

31
00:23:19,540 --> 00:23:23,560
in the middle of the circle is not just for aesthetics 

32
00:23:24,660 --> 00:23:28,520
this is what we would see if we were to scale up 10 to

33
00:23:28,520 --> 00:23:30,780
the 21 times into the universe 

34
00:23:30,780 --> 00:23:31,510
cool is not it 

35
00:23:32,750 --> 00:23:35,760
please refer to the reading in this module called 

36
00:23:35,760 --> 00:23:41,220
what does astronomical scale mean for a nice video on the powers of ten 

37
00:23:41,220 --> 00:23:48,400
all of these point to an exponential growth in data volume and storage 

38
00:23:48,400 --> 00:23:51,200
what is the relevance of this much data in our world 

39
00:23:52,260 --> 00:23:54,210
remember the planes collecting big data 

40
00:23:55,550 --> 00:23:59,170
our hope as passengers is data means better flight safety 

41
00:24:00,540 --> 00:24:06,070
the idea is to understand that businesses and organizations are collecting and

42
00:24:06,070 --> 00:24:10,520
leveraging large volumes of data to improve their end products 

43
00:24:10,520 --> 00:24:14,366
whether it is safety reliability healthcare or governance 

44
00:24:14,366 --> 00:24:19,300
in general in business the goal

45
00:24:19,300 --> 00:24:24,660
is to turn this much data into some form of business advantage 

46
00:24:24,660 --> 00:24:29,140
the question is how do we utilize larger volumes of data

47
00:24:29,140 --> 00:24:31,920
to improve our end product quality 

48
00:24:31,920 --> 00:24:34,790
despite a number of challenges related to it 

49
00:24:35,840 --> 00:24:40,650
there are a number of challenges related to the massive volumes of big data 

50
00:24:42,210 --> 00:24:45,970
the most obvious one is of course storage 

51
00:24:45,970 --> 00:24:48,800
as the size of the data increases so

52
00:24:48,800 --> 00:24:52,740
does the amount of storage space required to store that data efficiently 

53
00:24:53,810 --> 00:24:58,340
however we also need to be able to retrieve that large amount of

54
00:24:58,340 --> 00:25:00,030
data fast enough and

55
00:25:00,030 --> 00:25:06,140
move it to processing units in a timely fashion to get results when we need them 

56
00:25:06,140 --> 00:25:09,900
this brings additional challenges such as networking bandwidth 

57
00:25:09,900 --> 00:25:11,205
cost of storing data 

58
00:25:11,205 --> 00:25:15,670
in - house versus cloud storage and things like that 

59
00:25:15,670 --> 00:25:20,530
additional challenges arise during processing of such large data 

60
00:25:20,530 --> 00:25:24,450
most existing analytical methods wo not scale to such sums of

61
00:25:24,450 --> 00:25:27,500
data in terms of memory processing or io needs 

62
00:25:29,150 --> 00:25:30,980
this means their performance will drop 

63
00:25:32,060 --> 00:25:36,220
you might be able to get good performance for data from hundreds of customers 

64
00:25:36,220 --> 00:25:43,314
but how about scaling your solution to 1 000 or 10 000 customers 

65
00:25:43,314 --> 00:25:49,270
as the volume increases performance and cost start becoming a challenge 

66
00:25:50,880 --> 00:25:55,530
businesses need a holistic strategy to handle processing of

67
00:25:55,530 --> 00:26:00,090
large scale data to their benefit in the most cost effective manner 

68
00:26:00,090 --> 00:26:03,690
evaluating the options across the dimensions mentioned here 

69
00:26:03,690 --> 00:26:07,580
is the first step when it comes to continuously increasing data size 

70
00:26:07,580 --> 00:26:12,290
we will revisit this topic later on in this course 

71
00:26:12,290 --> 00:26:18,330
as a summary volume is the dimension of big data related to its size and

72
00:26:18,330 --> 00:26:19,540
its exponential growth 

73
00:26:20,770 --> 00:26:26,126
the challenges with working with volumes of big data include cost scalability 

74
00:26:26,126 --> 00:26:30,725
and performance related to their storage access and processing 

