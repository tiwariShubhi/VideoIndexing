1
00:00:00,870 --> 00:00:04,540
in this short video we ll talk about how meltwater

2
00:00:04,540 --> 00:00:09,240
helped danone using sentiment analysis 

3
00:00:09,240 --> 00:00:12,600
meltwater is a company that helps other companies

4
00:00:12,600 --> 00:00:17,100
analyze what people are saying about them and manage their online reputation 

5
00:00:18,700 --> 00:00:22,980
one of the case studies on their website is about danone baby nutrition 

6
00:00:24,180 --> 00:00:29,358
meltwater helped danone to monitor the opinions through social media for

7
00:00:29,358 --> 00:00:31,873
one of their marketing campaigns 

8
00:00:31,873 --> 00:00:35,204
they were able to measure what was impactful and what was not 

9
00:00:35,204 --> 00:00:36,690
through such monitoring 

10
00:00:37,890 --> 00:00:42,610
meltwater also helped danone manage a potential reputation issue 

11
00:00:42,610 --> 00:00:45,380
when a crisis occurred related to horse dna

12
00:00:45,380 --> 00:00:48,320
being in some meat products across europe 

13
00:00:48,320 --> 00:00:52,830
while danone was confident that they did not have an issue with their products 

14
00:00:52,830 --> 00:00:57,730
having the information a couple of hours before news hit the uk press 

15
00:00:57,730 --> 00:00:59,690
allowed them to check and

16
00:00:59,690 --> 00:01:04,580
reassure their customers that their products were safe to consume 

17
00:01:04,580 --> 00:01:09,040
you can imagine millions of mothers having been reassured and happy for

18
00:01:09,040 --> 00:01:11,180
danone efforts on this 

19
00:01:11,180 --> 00:01:16,375
this is an excellent story about how big data helped manage public opinion 

20
00:01:16,375 --> 00:01:21,136
and i am sure meltwater was able to help them to measure the opinion impact through

21
00:01:21,136 --> 00:01:22,448
social media as well 

1
00:01:23,240 --> 00:01:26,508
big data is now being generated all around us 

2
00:01:26,508 --> 00:01:27,750
so what 

3
00:01:27,750 --> 00:01:29,450
it the applications 

4
00:01:29,450 --> 00:01:34,170
it is the way in which big data can serve human needs that makes it valued 

5
00:01:35,430 --> 00:01:39,971
let look at a few examples of the applications big data is allowing us to

6
00:01:39,971 --> 00:01:41,226
imagine and build 

7
00:01:59,796 --> 00:02:05,420
big data allows us to build better models which produce higher precision results 

8
00:02:06,470 --> 00:02:11,021
we are witnessing hugely innovative approaches in how companies

9
00:02:11,021 --> 00:02:13,758
market themselves and sell products 

10
00:02:13,758 --> 00:02:15,958
how human resources are managed 

11
00:02:15,958 --> 00:02:18,178
how disasters are responded to 

12
00:02:18,178 --> 00:02:22,389
and many other applications that evidenced based data is being

13
00:02:22,389 --> 00:02:24,460
used to influence decisions 

14
00:02:26,660 --> 00:02:28,670
what exactly does that mean 

15
00:02:28,670 --> 00:02:30,150
here is one example 

16
00:02:30,150 --> 00:02:32,640
many of you might have experienced it i do 

17
00:02:34,540 --> 00:02:38,140
data amazon keeps some things i have been looking at

18
00:02:38,140 --> 00:02:41,610
allows them to personalize what they show me 

19
00:02:41,610 --> 00:02:46,300
which hopefully helps narrow down the huge raft of options i might get

20
00:02:46,300 --> 00:02:49,380
than just searching on dinner plates 

21
00:02:49,380 --> 00:02:54,307
now businesses can leverage technology to make better informed decisions

22
00:02:54,307 --> 00:02:59,246
that are actually based on signals generated by actual consumers like me 

23
00:03:01,454 --> 00:03:05,980
big data enables you to hear the voice of each consumer as

24
00:03:05,980 --> 00:03:08,590
opposed to consumers at large 

25
00:03:09,920 --> 00:03:13,500
now many companies including walmart and target 

26
00:03:13,500 --> 00:03:19,160
use this information to personalize their communications with their costumers which

27
00:03:19,160 --> 00:03:23,240
in turns leads to better met consumer expectations and happier customers 

28
00:03:25,550 --> 00:03:31,628
which basically is to say big data has enabled personalized marketing 

29
00:03:31,628 --> 00:03:36,250
consumers are copiously generating publicly accessible data through

30
00:03:36,250 --> 00:03:38,560
social media sites like twitter or facebook 

31
00:03:39,690 --> 00:03:44,130
through such data the companies are able to see their purchase history 

32
00:03:44,130 --> 00:03:48,970
what they searched for what they watched where they have been and

33
00:03:48,970 --> 00:03:51,650
what they are interested in through their likes and shares 

34
00:03:52,920 --> 00:03:57,717
let look at some examples of how companies are putting this information to

35
00:03:57,717 --> 00:04:01,868
build better marketing campaigns and reach the right customers 

36
00:04:04,718 --> 00:04:08,985
one area we are all familiar with are the recommendation engines 

37
00:04:08,985 --> 00:04:14,190
these engines leverage user patterns and product features

38
00:04:14,190 --> 00:04:20,028
to predict best match product for enriching the user experience 

39
00:04:20,028 --> 00:04:22,303
if you ever shopped on amazon 

40
00:04:22,303 --> 00:04:26,596
you know you get recommendations based on your purchase 

41
00:04:26,596 --> 00:04:29,861
similarly netflix would recommend you to watch

42
00:04:29,861 --> 00:04:32,590
new shows based on your viewing history 

43
00:04:34,610 --> 00:04:40,550
another technique that companies use is sentiment analysis or in simple terms 

44
00:04:40,550 --> 00:04:44,550
analysis of the feelings around events and products 

45
00:04:45,880 --> 00:04:49,900
remember the blue plates i purchased on amazon com 

46
00:04:49,900 --> 00:04:53,680
i not only can read the reviews before purchasing them 

47
00:04:53,680 --> 00:04:57,460
i can also write a product review once i receive my plates 

48
00:04:59,280 --> 00:05:02,500
this way other customers can be informed 

49
00:05:03,880 --> 00:05:09,110
but more importantly amazon can keep a watch on the product reviews and

50
00:05:09,110 --> 00:05:11,220
trends for a particular product 

51
00:05:11,220 --> 00:05:13,951
in this case blue plates 

52
00:05:13,951 --> 00:05:20,300
for example they can judge if a product review is positive or negative 

53
00:05:21,940 --> 00:05:25,140
in this case while the first review is negative 

54
00:05:27,140 --> 00:05:29,530
the next two reviews are positive 

55
00:05:31,180 --> 00:05:35,130
since these reviews are written in english using a technique called natural

56
00:05:35,130 --> 00:05:38,780
language processing and other analytical methods 

57
00:05:38,780 --> 00:05:44,120
amazon can analyze the general opinion of a person or public about such a product 

58
00:05:46,130 --> 00:05:51,128
this is why sentiment analysis often gets referred to as opinion mining 

59
00:05:53,298 --> 00:05:57,729
news channels are filled with twitter feed analysis every time

60
00:05:57,729 --> 00:06:01,420
an event of importance occurs such as elections 

61
00:06:02,810 --> 00:06:08,162
brands utilize sentiment analysis to understand how customers

62
00:06:08,162 --> 00:06:13,630
relate to their product positively negatively neutral 

63
00:06:13,630 --> 00:06:16,448
this depends heavily on use of natural language processing 

64
00:06:19,183 --> 00:06:21,289
mobile devices are ubiquitous and

65
00:06:21,289 --> 00:06:24,780
people almost always carry their cellphones with them 

66
00:06:25,810 --> 00:06:29,200
mobile advertising is a huge market for businesses 

67
00:06:30,760 --> 00:06:35,581
platforms utilize the sensors in mobile devices 

68
00:06:35,581 --> 00:06:40,847
such as gps and provide real time location based ads 

69
00:06:40,847 --> 00:06:45,456
offer discounts based on this deluge of data 

70
00:06:45,456 --> 00:06:50,063
this time let imagine that i bought a new house and

71
00:06:50,063 --> 00:06:54,178
i happen to be in a few miles range of a home depot 

72
00:06:54,178 --> 00:06:57,444
sending me mobile coupons about paint shelves and

73
00:06:57,444 --> 00:07:01,290
other new home related purchases would remind me of home depot 

74
00:07:02,550 --> 00:07:05,590
there a big chance i would stop by home depot 

75
00:07:05,590 --> 00:07:06,790
bingo ! 

76
00:07:06,790 --> 00:07:11,470
now i would like to take a moment to analyze what kinds of big data are needed

77
00:07:11,470 --> 00:07:12,270
to make this happen 

78
00:07:13,495 --> 00:07:17,990
there definitely the integration of my consumer information and

79
00:07:17,990 --> 00:07:22,710
the online and offline databases that include my recent purchases 

80
00:07:22,710 --> 00:07:27,569
but more importantly the geolocation data that falls under

81
00:07:27,569 --> 00:07:31,128
a larger type of big data spacial big data 

82
00:07:31,128 --> 00:07:33,780
we will talk about spacial data later in this class 

83
00:07:35,190 --> 00:07:39,770
let now talk about how the global consumer behavior can be used for

84
00:07:39,770 --> 00:07:40,360
product growth 

85
00:07:42,170 --> 00:07:45,760
we are now moving from personalize marketing

86
00:07:45,760 --> 00:07:48,230
to the consumer behavior as a whole 

87
00:07:49,920 --> 00:07:54,758
every business wants to understand their consumer s collective

88
00:07:54,758 --> 00:07:59,258
behavior in order to capture the ever - changing landscape 

89
00:07:59,258 --> 00:08:04,809
several big data products enable this by developing models to capture user

90
00:08:04,809 --> 00:08:10,730
behavior and allow businesses to target the right audience for their product 

91
00:08:12,340 --> 00:08:15,250
or develop new products for uncharted territories 

92
00:08:17,570 --> 00:08:19,550
let look at this example 

93
00:08:19,550 --> 00:08:22,797
after an analysis of their sales for weekdays 

94
00:08:22,797 --> 00:08:28,319
an airline company might notice that their morning flights are always sold out 

95
00:08:28,319 --> 00:08:31,908
while their afternoon flights run below capacity 

96
00:08:31,908 --> 00:08:37,545
this company might decide to add more morning flights based on such analysis 

97
00:08:38,900 --> 00:08:43,550
notice that they are not using individual consumer choices but

98
00:08:43,550 --> 00:08:48,760
using all the flights purchased without consideration to who purchased them 

99
00:08:50,200 --> 00:08:51,150
they might however 

100
00:08:51,150 --> 00:08:56,400
decide to pay closer attention to the demographic of these consumers

101
00:08:56,400 --> 00:09:00,850
using big data to also add similar flights in other geographical regions 

102
00:09:02,350 --> 00:09:07,240
with rapid advances in genome sequencing technology 

103
00:09:07,240 --> 00:09:13,580
the life sciences industry is experiencing an enormous draw in biomedical big data 

104
00:09:15,120 --> 00:09:21,168
this biomedical data is being used by many applications in research and

105
00:09:21,168 --> 00:09:23,398
personalized medicine 

106
00:09:23,398 --> 00:09:28,400
did you know genomics data is one of the largest growing big data types 

107
00:09:28,400 --> 00:09:35,940
between 100 million and 2 billion human genomes could be sequenced by year 2025 

108
00:09:35,940 --> 00:09:36,520
impressive 

109
00:09:38,700 --> 00:09:41,080
this inaudible sequence data demands for

110
00:09:41,080 --> 00:09:46,580
between 2 exabytes and 40 exabytes in data storage 

111
00:09:46,580 --> 00:09:52,190
in comparison all of youtube only requires 1 to 2 exabytes a year 

112
00:09:54,530 --> 00:09:57,886
an exabyte is 10 to the power 18 bites 

113
00:09:57,886 --> 00:10:03,818
that is 18 zeros after 40 

114
00:10:03,818 --> 00:10:10,270
of course analysis of such massive volumes of sequence data is expensive 

115
00:10:10,270 --> 00:10:12,680
it could take up to 10 000 trillion cpu hours 

116
00:10:16,580 --> 00:10:21,040
one of the biomedical applications that this much data is enabling

117
00:10:21,040 --> 00:10:22,450
is personalized medicine 

118
00:10:24,060 --> 00:10:28,879
before personalized medicine most patients without a specific type and

119
00:10:28,879 --> 00:10:31,862
stage of cancer received the same treatment 

120
00:10:31,862 --> 00:10:34,858
which worked better for some than the others 

121
00:10:36,968 --> 00:10:42,086
research in this area is enabling development of methods to analyze

122
00:10:42,086 --> 00:10:47,472
large scale data to develop solutions that tailor to each individual 

123
00:10:47,472 --> 00:10:50,900
and hence hypothesize to be more effective 

124
00:10:52,450 --> 00:10:58,260
a person with cancer may now still receive a treatment plan that is standard 

125
00:10:58,260 --> 00:11:00,620
such as surgery to remove a tumor 

126
00:11:01,870 --> 00:11:05,750
however the doctor may also be able to recommend

127
00:11:05,750 --> 00:11:07,460
some type of personalized cancer treatment 

128
00:11:09,150 --> 00:11:13,900
a big challenge in biomedical big data applications like many other fields 

129
00:11:13,900 --> 00:11:18,860
is how we can integrate many types of data sources to gain further insight problem 

130
00:11:20,300 --> 00:11:23,648
in one of our future lectures my colleagues here at

131
00:11:23,648 --> 00:11:28,670
the supercomputer center will explain how he and his colleague have

132
00:11:28,670 --> 00:11:33,940
used big data from a variety of sources for personalized patient interventions 

133
00:11:35,840 --> 00:11:41,010
another application of big data comes from interconnected mesh of large

134
00:11:41,010 --> 00:11:46,490
number of sensors implanted across smart cities 

135
00:11:46,490 --> 00:11:50,790
analysis of data generated from sensors in real time

136
00:11:50,790 --> 00:11:55,140
allows cities to deliver better service quality to inhabitants 

137
00:11:55,140 --> 00:12:00,340
and reduce unwanted affect such as pollution traffic congestion 

138
00:12:00,340 --> 00:12:03,470
higher than optimal cost on delivering urban services 

139
00:12:04,910 --> 00:12:06,750
let take our city san diego 

140
00:12:08,370 --> 00:12:13,794
san diego generates a huge volumes of data from many sources 

141
00:12:13,794 --> 00:12:19,676
traffic sensors satellites camera networks and more 

142
00:12:19,676 --> 00:12:21,694
what if we could integrate and

143
00:12:21,694 --> 00:12:26,140
synthesize these data streams to do even more for our community 

144
00:12:27,260 --> 00:12:29,000
using such big data 

145
00:12:29,000 --> 00:12:34,170
we can work toward making san diego the prototype digital city 

146
00:12:34,170 --> 00:12:37,020
not only for life - threatening hazards but

147
00:12:37,020 --> 00:12:42,280
making our daily lives better such as managing traffic flow more efficiently or

148
00:12:42,280 --> 00:12:46,870
maximizing energy savings even as we will see next wildfires 

149
00:12:48,450 --> 00:12:53,390
if you want to read more here a link to the at kearney report 

150
00:12:53,390 --> 00:12:56,120
where they talk about other areas using big data 

151
00:12:57,580 --> 00:13:01,880
as a summary big data has a huge potential

152
00:13:01,880 --> 00:13:06,140
to enable models with higher precision in many application areas 

153
00:13:07,230 --> 00:13:12,175
and these highly precise models are influencing and transforming business 

1
00:14:35,140 --> 00:14:38,170
big data generated by people how is it being used 

2
00:14:39,740 --> 00:14:42,370
we listed a number of challenges for

3
00:14:42,370 --> 00:14:45,400
using unstructured data generated by human activities 

4
00:14:47,080 --> 00:14:51,730
now let look at some of the emerging technologies to tackle these challenges 

5
00:14:51,730 --> 00:14:56,930
and see some examples that turn unstructured data into valuable insights 

6
00:15:15,968 --> 00:15:20,780
although unstructured data specially the kind generated by people has

7
00:15:20,780 --> 00:15:23,320
a number of challenges 

8
00:15:23,320 --> 00:15:27,300
the good news is that the business culture of today is shifting

9
00:15:27,300 --> 00:15:31,070
to tackle these challenges and take full advantage of such data 

10
00:15:32,150 --> 00:15:36,120
as it is often said a challenge is a perfect opportunity 

11
00:15:37,130 --> 00:15:40,150
this is certainly the case for big data and

12
00:15:40,150 --> 00:15:43,680
these challenges have created a tech industry of it own 

13
00:15:44,860 --> 00:15:49,730
this industry is mostly centered or as we would say layered or

14
00:15:49,730 --> 00:15:55,493
stacked around a few fundamental open source big data frameworks 

15
00:15:55,493 --> 00:15:59,320
need big data tools are designed from scratch

16
00:15:59,320 --> 00:16:02,990
to manage unstructured information and analyze it 

17
00:16:02,990 --> 00:16:06,860
a majority of these tools are based on an open source

18
00:16:06,860 --> 00:16:08,620
big data framework called hadoop 

19
00:16:09,840 --> 00:16:14,010
hadoop is designed to support the processing of large data sets

20
00:16:14,010 --> 00:16:16,960
in a distributed computing environment 

21
00:16:16,960 --> 00:16:21,510
this definition would already give you a hint that it tackles the first challenge 

22
00:16:21,510 --> 00:16:25,140
namely the volume of unstructured information 

23
00:16:26,150 --> 00:16:29,740
hadoop can handle big batches of distributed information but

24
00:16:29,740 --> 00:16:32,250
most often there a need for

25
00:16:32,250 --> 00:16:37,650
a real time processing of people generated data like twitter or facebook updates 

26
00:16:39,220 --> 00:16:43,950
financial compliance monitoring is another area of our central time processing is

27
00:16:43,950 --> 00:16:47,380
needed in particular to reduce market data 

28
00:16:48,750 --> 00:16:54,780
social media and market data are two types of what we call high velocity data 

29
00:16:55,840 --> 00:16:59,720
storm and spark are two other open source frameworks

30
00:16:59,720 --> 00:17:03,480
that handle such real time data generated at a fast rate 

31
00:17:04,490 --> 00:17:05,490
both storm and

32
00:17:05,490 --> 00:17:10,230
spark can integrate data with any database or data storage technology 

33
00:17:11,560 --> 00:17:15,810
as we have emphasized before unstructured data

34
00:17:15,810 --> 00:17:19,740
does not have a relational data model so it does not generally

35
00:17:19,740 --> 00:17:24,150
fit into the traditional data warehouse model based on relational databases 

36
00:17:25,350 --> 00:17:29,870
data warehouses are central repositories of integrated data from one or

37
00:17:29,870 --> 00:17:30,490
more sources 

38
00:17:32,300 --> 00:17:38,260
the data that gets stored in warehouses gets extracted from multiple sources 

39
00:17:39,460 --> 00:17:43,850
it gets transformed into a common structured form and

40
00:17:43,850 --> 00:17:47,120
it can slow that into the central database for

41
00:17:47,120 --> 00:17:51,400
use by workers creating analytical reports throughout an enterprise 

42
00:17:52,650 --> 00:17:59,613
this exact transform load process is commonly called etl 

43
00:17:59,613 --> 00:18:03,480
this approach was fairly standard in enterprise data systems until recently 

44
00:18:04,560 --> 00:18:08,530
as you probably noticed it is fairly static and

45
00:18:08,530 --> 00:18:11,350
does not fit well with today dynamic big data world 

46
00:18:12,550 --> 00:18:17,073
so how do today businesses get around this problem 

47
00:18:17,073 --> 00:18:21,834
many businesses today are using a hybrid approach in which their smaller

48
00:18:21,834 --> 00:18:25,894
structured data remains in their relational databases and

49
00:18:25,894 --> 00:18:30,750
large unstructured datasets get stored in nosql databases in the cloud 

50
00:18:32,200 --> 00:18:37,840
nosql data technologies are based on non - relational concepts and

51
00:18:37,840 --> 00:18:42,310
provide data storage options typically on computing clouds

52
00:18:42,310 --> 00:18:46,180
beyond the traditional relational databases centered rate houses 

53
00:18:48,170 --> 00:18:53,640
the main advantage of using nosql solutions is their ability

54
00:18:53,640 --> 00:18:58,430
to organize the data for scalable access to fit the problem and

55
00:18:58,430 --> 00:19:01,330
objectives pertaining to how the data will be used 

56
00:19:03,130 --> 00:19:08,990
for example if the data will be used in an analysis to find connections

57
00:19:08,990 --> 00:19:14,130
between data sets then the best solution is a graph database 

58
00:19:15,960 --> 00:19:18,800
neo4j is an example of a graph database 

59
00:19:19,910 --> 00:19:23,630
graph networks is a topic that the graph analytics course

60
00:19:23,630 --> 00:19:27,140
later in this specialization we will explain in depth 

61
00:19:27,140 --> 00:19:33,350
if the data will be best accessed using key value pairs like a search engine

62
00:19:33,350 --> 00:19:39,490
scenario the best solution is probably a dedicated key value paired database 

63
00:19:42,710 --> 00:19:45,660
cassandra is an example of a key value database 

64
00:19:47,060 --> 00:19:48,010
these and

65
00:19:48,010 --> 00:19:52,710
many other types of nosql systems will be explained further in course two 

66
00:19:54,190 --> 00:19:58,160
so we are now confident that there are emerging technologies for

67
00:19:58,160 --> 00:20:02,410
individual challenges to manage people generated unstructured data 

68
00:20:03,610 --> 00:20:07,800
but how does one take advantage of these to generate value 

69
00:20:09,710 --> 00:20:17,150
as we saw big data must pass through a series of steps before it generates value 

70
00:20:17,150 --> 00:20:21,520
namely data access storage cleaning and analysis 

71
00:20:23,220 --> 00:20:29,210
one approach to solve this problem is to run each stage as a different layer 

72
00:20:30,400 --> 00:20:34,210
and use tools available to fit the problem at hand and

73
00:20:34,210 --> 00:20:37,730
scale analytical solutions to big data 

74
00:20:37,730 --> 00:20:42,110
in coming lectures we will see important tools that you can use

75
00:20:42,110 --> 00:20:45,840
to solve your big data problems in addition to the ones you have seen today 

76
00:20:47,070 --> 00:20:51,960
now let take a step back and remind ourselves what some of the value was 

77
00:20:53,220 --> 00:20:57,920
remember how companies can listen to the real voice of customers using big data 

78
00:20:59,540 --> 00:21:03,150
it is this type of generated data that enabled it 

79
00:21:04,190 --> 00:21:09,150
sentiment analysis analyzes social media and other data to find

80
00:21:09,150 --> 00:21:14,750
whether people associate positively or negatively with you business 

81
00:21:14,750 --> 00:21:19,210
organizations are utilizing processing of personal data to

82
00:21:19,210 --> 00:21:21,980
understand the true preferences of their customers 

83
00:21:23,130 --> 00:21:28,110
now let take a fun quiz to guess how much twitter data companies analyze

84
00:21:28,110 --> 00:21:31,510
every day to measure sentiment around their product 

85
00:21:33,080 --> 00:21:35,900
the answer is 12 terabytes a day 

86
00:21:37,040 --> 00:21:42,160
for comparison you would need to listen continuously for

87
00:21:42,160 --> 00:21:45,620
two years to finish listening to 1 terabyte of music 

88
00:21:47,000 --> 00:21:49,332
another example application area for

89
00:21:49,332 --> 00:21:52,960
people generated data is customer behavior modeling and prediction 

90
00:21:54,100 --> 00:21:58,730
amazon netflix and a lot of other organizations 

91
00:21:58,730 --> 00:22:02,400
use analytics to analyze preferences of their customers 

92
00:22:03,710 --> 00:22:09,260
based on consumer behavior organizations suggest better products to customers 

93
00:22:10,350 --> 00:22:14,430
and in turn have happier customers and higher profits 

94
00:22:15,610 --> 00:22:21,250
another application area where the value comes in the form of societal impact and

95
00:22:21,250 --> 00:22:24,540
social welfare is disaster management 

96
00:22:25,720 --> 00:22:28,550
as you have seen in my wildfire example 

97
00:22:28,550 --> 00:22:32,030
there are many types of big data that can help with disaster response 

98
00:22:33,450 --> 00:22:37,420
data in the form of pictures and tweets helps facilitate

99
00:22:37,420 --> 00:22:42,380
a collective response to disaster situations such as evacuations through

100
00:22:42,380 --> 00:22:46,310
the safest route based on community feedback through social media 

101
00:22:47,340 --> 00:22:50,820
there are also networks that turn crowd sourcing and

102
00:22:50,820 --> 00:22:54,650
big data analytics into collective disaster response tools 

103
00:22:55,920 --> 00:22:58,990
the international network of crisis mappers 

104
00:22:58,990 --> 00:23:03,950
also called crisis mappers net is the largest of such networks and

105
00:23:03,950 --> 00:23:08,360
includes an active international community of volunteers 

106
00:23:08,360 --> 00:23:14,190
crisis mappers use big data in the form of aerial and satellite imagery 

107
00:23:14,190 --> 00:23:19,440
participatory maps and live twitter updates to analyze

108
00:23:19,440 --> 00:23:25,611
the data using geospatial platforms advanced visualization 

109
00:23:25,611 --> 00:23:30,300
live simulation and computational and statistical models 

110
00:23:31,780 --> 00:23:37,220
once analyzed the results get reported to rapid response and humanitarian agencies

111
00:23:38,220 --> 00:23:44,020
in the form of mobile and web applications 

112
00:23:44,020 --> 00:23:50,510
in 2015 right after the nepal earthquake crises mappers crowd source the analysis

113
00:23:50,510 --> 00:23:55,990
of tweets and mainstream media to rapidly access disaster damage and

114
00:23:55,990 --> 00:24:01,210
needs and to identify where humanitarian help is needed 

115
00:24:01,210 --> 00:24:06,663
this example is amazing and shows how big data can have huge impacts for

116
00:24:06,663 --> 00:24:09,263
social welfare in times of need 

117
00:24:09,263 --> 00:24:12,400
you can learn more about this story at the following link 

118
00:24:14,790 --> 00:24:19,679
as a summary although there are challenges in working with unstructured

119
00:24:19,679 --> 00:24:24,270
people generated data at a scale and speed that applications demand 

120
00:24:24,270 --> 00:24:28,124
there are also emerging technologies and solutions that are being

121
00:24:28,124 --> 00:24:32,890
used by many applications to generate value from the rich source of information 

1
00:39:06,920 --> 00:39:11,485
big data generated by people the unstructured challenge 

2
00:39:27,404 --> 00:39:32,471
people are generating massive amounts of data every day through their activities on

3
00:39:32,471 --> 00:39:37,405
various social media networking sites like facebook twitter and linkedin 

4
00:39:37,405 --> 00:39:42,655
or online photo sharing sites like instagram flickr or picasa 

5
00:39:44,540 --> 00:39:46,620
and video sharing websites like youtube 

6
00:39:48,020 --> 00:39:53,100
in addition an enormous amount of information gets generated via

7
00:39:53,100 --> 00:39:58,250
blogging and commenting internet searches more via text messages 

8
00:39:59,320 --> 00:40:02,460
email and through personal documents 

9
00:40:03,920 --> 00:40:08,711
most of this data is text - heavy and unstructured 

10
00:40:08,711 --> 00:40:14,780
that is non - conforming to a well - defined data model 

11
00:40:14,780 --> 00:40:18,690
we can also consider this data to be content with

12
00:40:18,690 --> 00:40:21,630
occasionally some description attached to it 

13
00:40:21,630 --> 00:40:26,360
this much activity leads to a huge growth in data 

14
00:40:27,480 --> 00:40:31,980
did you know that in a single day facebook users produce

15
00:40:31,980 --> 00:40:36,550
more data than combined us academic research libraries 

16
00:40:38,100 --> 00:40:41,130
let look at some similar daily data volume numbers

17
00:40:42,130 --> 00:40:44,460
from some of the biggest online platforms 

18
00:40:45,720 --> 00:40:49,790
it is amazing that some of these numbers are in the petabyte range for

19
00:40:49,790 --> 00:40:50,700
daily activity 

20
00:40:51,840 --> 00:40:54,930
a petabyte is a thousand terabytes 

21
00:40:56,470 --> 00:41:01,360
the sheer size of mostly unstructured data generated by humans

22
00:41:01,360 --> 00:41:02,950
brings a lot of challenges 

23
00:41:04,730 --> 00:41:11,250
unstructured data refers to data that does not conform to a predefined data model 

24
00:41:13,200 --> 00:41:16,380
so no relation model and no sql 

25
00:41:17,980 --> 00:41:22,190
it is mostly anything that we do not store in a traditional

26
00:41:22,190 --> 00:41:23,640
relational database management system 

27
00:41:25,170 --> 00:41:28,480
consider a sales receipt that you get from a grocery store 

28
00:41:29,580 --> 00:41:32,790
it has a section for a date a section for

29
00:41:32,790 --> 00:41:36,690
store name and a section for total amount 

30
00:41:38,200 --> 00:41:40,310
this is an example of structure 

31
00:41:41,340 --> 00:41:46,550
humans generate a lot of unstructured data in form of text 

32
00:41:46,550 --> 00:41:48,960
there no given format to that 

33
00:41:48,960 --> 00:41:52,180
look at all the documents that you have written with your hand so far 

34
00:41:53,220 --> 00:41:58,550
collectively it is a bank of unstructured data you have personally generated 

35
00:41:59,580 --> 00:42:04,160
in fact 80 to 90 of all data in

36
00:42:04,160 --> 00:42:08,980
the world is unstructured and this number is rapidly growing 

37
00:42:10,490 --> 00:42:15,531
examples of unstructured data generated by people includes texts images 

38
00:42:15,531 --> 00:42:22,190
videos audio internet searches and emails 

39
00:42:22,190 --> 00:42:27,580
in addition to it rapid growth major challenges of unstructured data

40
00:42:27,580 --> 00:42:32,768
include multiple data formats like webpages images pdfs 

41
00:42:32,768 --> 00:42:40,170
power point xml and other formats that were mainly built for human consumption 

42
00:42:40,170 --> 00:42:47,030
think of it although i can sort my email with date sender and subject 

43
00:42:47,030 --> 00:42:50,690
it would be really difficult to write a program 

44
00:42:50,690 --> 00:42:55,430
to categorize all my email messages based on their content and

45
00:42:55,430 --> 00:43:01,460
organize them for me accordingly another challenge of human generated data

46
00:43:01,460 --> 00:43:07,560
is the volume and fast generation of data which is what we call velocity 

47
00:43:07,560 --> 00:43:13,504
just take a moment to study this info graphic and observe what happens in one

48
00:43:13,504 --> 00:43:18,728
minute on the internet and consider how much to contribute to it 

49
00:43:21,088 --> 00:43:28,586
moreover confirmation of unstructured data is often time consuming and costly 

50
00:43:28,586 --> 00:43:34,130
the costs and time of the process of acquiring storing 

51
00:43:34,130 --> 00:43:40,110
cleaning retrieving and processing unstructured data can add up to quite and

52
00:43:40,110 --> 00:43:43,220
investment before we can start reaping value from this process 

53
00:43:45,230 --> 00:43:47,790
it can be pretty hard to find the tools and

54
00:43:47,790 --> 00:43:51,810
people to implement such a process and reap value in the end 

55
00:43:53,030 --> 00:43:56,570
as a summary although there is an enormous amount of

56
00:43:56,570 --> 00:44:00,780
data generated by people most of this data is unstructured 

57
00:44:01,890 --> 00:44:05,230
the challenges of working with unstructured data should not

58
00:44:05,230 --> 00:44:07,170
be taken lightly 

59
00:44:07,170 --> 00:44:13,310
next we will look at how businesses are tackling these challenges to gain insight 

60
00:44:13,310 --> 00:44:16,770
and thus value out of working with people generated data 

1
01:23:23,120 --> 01:23:27,970
as we have already seen there are many different exciting applications

2
01:23:27,970 --> 01:23:30,060
that are being enabled by the big data era 

3
01:23:31,160 --> 01:23:35,500
as part of my core research here at the san diego supercomputer center 

4
01:23:35,500 --> 01:23:37,420
i work on building methodologies and

5
01:23:37,420 --> 01:23:42,150
tools to make big data useful to dynamic data driven scientific applications 

6
01:23:43,170 --> 01:23:47,600
my colleagues and i work on many grand challenge data science applications 

7
01:23:47,600 --> 01:23:52,903
in all areas of science and engineering including genomics geoinformatics 

8
01:23:52,903 --> 01:23:58,050
metro science energy management biomedicine and personalized health 

9
01:23:59,660 --> 01:24:04,492
what is common to all these applications is their unique way of bringing

10
01:24:04,492 --> 01:24:08,046
together new modes of data and computing research 

11
01:24:08,046 --> 01:24:14,195
let me tell you the one i am passionate

12
01:24:14,195 --> 01:24:19,192
about wildfire analytics 

13
01:24:19,192 --> 01:24:25,342
which breaks up into two components 

14
01:24:25,342 --> 01:24:29,779
prediction and response 

15
01:24:40,713 --> 01:24:43,629
why is this so important 

16
01:24:43,629 --> 01:24:47,608
on may 2014 in san diego county where

17
01:24:47,608 --> 01:24:52,804
the instructors of this specialization live and work 

18
01:24:52,804 --> 01:24:58,330
there were 14 fires burning as many as nine at one time 

19
01:24:58,330 --> 01:25:04,964
which burned a total of 26 000 acres 11 000 hectares 

20
01:25:04,964 --> 01:25:10,520
an area just less than the size of the city of san francisco 

21
01:25:12,710 --> 01:25:18,087
six people were injured and one person died 

22
01:25:18,087 --> 01:25:22,093
and these wildfires resulted in a total cost of

23
01:25:22,093 --> 01:25:26,620
over 60 million us in damage and firefighting 

24
01:25:27,870 --> 01:25:32,925
these wildfires can become so severe that we actually call them firestorms 

25
01:25:34,150 --> 01:25:37,170
although we cannot control such fire storms 

26
01:25:37,170 --> 01:25:41,070
something we can do is to get ahead of them by predicting their behavior 

27
01:25:42,790 --> 01:25:47,670
this is why disaster management of ongoing wildfires relies

28
01:25:47,670 --> 01:25:51,470
heavily on understanding their direction and rate of spread 

29
01:25:52,560 --> 01:25:55,000
as these fires are a part of our lives 

30
01:25:55,000 --> 01:26:00,410
we wanted to see if we can use big data to monitor predict and manage a firestorm 

31
01:26:02,400 --> 01:26:04,337
why can big data help 

32
01:26:04,337 --> 01:26:08,685
as we will see in this video indeed wildfire prevention and

33
01:26:08,685 --> 01:26:12,879
response can benefit from many streams in our data torrent 

34
01:26:12,879 --> 01:26:18,087
some streams are generated by people through devices they carry 

35
01:26:18,087 --> 01:26:23,713
a lot come from sensors and satellites things that measure environmental factors 

36
01:26:23,713 --> 01:26:27,855
and some come from organizational data including area maps 

37
01:26:27,855 --> 01:26:32,839
better service updates and field content databases which archive how much

38
01:26:32,839 --> 01:26:37,991
registers vegetation and other types of fuel are in the way of a potential fire 

39
01:26:39,840 --> 01:26:42,337
what makes this a big data problem 

40
01:26:42,337 --> 01:26:47,233
because novel approaches and responses can be taken if

41
01:26:47,233 --> 01:26:51,712
we can integrate this many diverse data streams 

42
01:26:51,712 --> 01:26:56,504
many such data sources have already existed for quite some time 

43
01:26:56,504 --> 01:27:01,835
but what is lacking in disaster management today is a dynamic system

44
01:27:01,835 --> 01:27:06,979
integration of real time sensor networks satellite imagery 

45
01:27:06,979 --> 01:27:12,405
near real time data management tools wildfire simulation tools 

46
01:27:12,405 --> 01:27:16,334
connectivity to emergency command centers and

47
01:27:16,334 --> 01:27:20,470
all these before during and after a firestorm 

48
01:27:21,800 --> 01:27:25,450
as you will see the integration of diverse streams and

49
01:27:25,450 --> 01:27:29,760
novel ways is really what is driving our ability to see new things and

50
01:27:29,760 --> 01:27:33,270
develop predictive analytics which may help improve our world 

51
01:27:34,770 --> 01:27:36,140
what are these diverse sources 

52
01:27:37,940 --> 01:27:42,870
one of the most important data sources is sensor data streaming in from weather

53
01:27:42,870 --> 01:27:48,540
stations and satellites such sensed data include temperature 

54
01:27:48,540 --> 01:27:49,960
humidity air pressure 

55
01:27:51,280 --> 01:27:55,346
we can also include image data streaming from

56
01:27:55,346 --> 01:28:00,379
mountaintop cameras and satellites in this category 

57
01:28:00,379 --> 01:28:03,967
another important data source comes from institutions

58
01:28:03,967 --> 01:28:06,867
such as the san diego supercomputer center 

59
01:28:06,867 --> 01:28:10,250
which generate data related to wildfire modeling 

60
01:28:10,250 --> 01:28:15,585
these include past and current fire perimeter maps put together by

61
01:28:15,585 --> 01:28:20,824
the authorities and fuel maps that tell us about the vegetation 

62
01:28:20,824 --> 01:28:24,171
and other types of fuel in a fire path 

63
01:28:24,171 --> 01:28:30,171
these types of data sources are often static or updated at a slow rate 

64
01:28:30,171 --> 01:28:36,083
but they provide valuable data that is well - curated and verified 

65
01:28:37,560 --> 01:28:42,450
a huge part of data on fires is actually generated by the public

66
01:28:42,450 --> 01:28:46,820
on social media sites such as twitter which support photo sharing resources 

67
01:28:48,650 --> 01:28:54,050
these are the hardest data sources to streamline during an existing fire but

68
01:28:54,050 --> 01:28:58,340
they can be very valuable once integrated with other data sources 

69
01:29:00,820 --> 01:29:06,380
imagine synthesizing all the pictures on twitter about an ongoing fire or

70
01:29:06,380 --> 01:29:09,850
checking the public sentiment around the boundaries of a fire 

71
01:29:11,960 --> 01:29:15,710
once you have access to such information at your fingertips 

72
01:29:15,710 --> 01:29:18,610
there are many things you can do with such data 

73
01:29:18,610 --> 01:29:23,300
you can simply monitor it or maybe you can visualize it 

74
01:29:25,310 --> 01:29:30,370
but it not until you bring all these different types of data sources together

75
01:29:30,370 --> 01:29:34,910
and integrate them with real time analysis and predictive modeling that you can

76
01:29:34,910 --> 01:29:39,570
really make contributions in predicting and responding to wildfire emergencies 

77
01:29:41,000 --> 01:29:44,221
so now i will like you to take a moment and

78
01:29:44,221 --> 01:29:49,296
imagine how big data might help with firefighting in the future 

79
01:29:49,296 --> 01:29:54,280
all these streams of data will come together in 3d displays that can show

80
01:29:54,280 --> 01:29:59,182
all the related information along with weather and fire predictions 

81
01:29:59,182 --> 01:30:02,462
just like the way tornadoes are managed today 

1
02:53:25,270 --> 02:53:30,840
let look at a 2nd example where big data can have a big impact on saving lives 

2
02:53:30,840 --> 02:53:34,428
i mean literally saving lives one life at a time 

3
02:53:34,428 --> 02:53:38,508
i collaborated with a number of world - class researchers in san diego and

4
02:53:38,508 --> 02:53:42,911
an industrial group who are dedicated to improving human health through research

5
02:53:42,911 --> 02:53:44,970
and practice of precision medicine 

6
02:53:46,400 --> 02:53:48,250
what is precision medicine 

7
02:53:48,250 --> 02:53:52,950
it is an emerging area of medicine targeted toward an individual person 

8
02:53:52,950 --> 02:53:57,910
analysing her genetics her environment her daily activities so

9
02:53:57,910 --> 02:54:01,880
that one can detect or predict a health problem early 

10
02:54:01,880 --> 02:54:06,570
help prevent disease and in case of illness provide the right drug

11
02:54:06,570 --> 02:54:10,690
at the right dose that is suitable just for her 

12
02:54:10,690 --> 02:54:15,587
very recently the white house and the national institute of health here

13
02:54:15,587 --> 02:54:20,401
in the american have declared it to be a top priority area for research and

14
02:54:20,401 --> 02:54:22,779
development for the next decade 

15
02:54:22,779 --> 02:54:25,689
the expected learning outcome of this video is for

16
02:54:25,689 --> 02:54:28,668
you to give example of sensor organizational and

17
02:54:28,668 --> 02:54:31,870
people - generated data used in precision medicine 

18
02:54:33,050 --> 02:54:37,650
and explain why the integration of different kinds of data is critical

19
02:54:37,650 --> 02:54:38,940
in advancing healthcare 

20
02:54:40,150 --> 02:54:44,000
for any technology to succeed in real life we need

21
02:54:44,000 --> 02:54:48,940
not only a certain level of maturity of the technology itself but a number of

22
02:54:48,940 --> 02:54:54,050
enabling factors including social economic environment market demands 

23
02:54:54,050 --> 02:54:58,860
consumer readiness cost effectiveness all of which must work together 

24
02:55:00,430 --> 02:55:03,790
why is big data for precision medicine important now 

25
02:55:03,790 --> 02:55:04,290
let see 

26
02:55:05,480 --> 02:55:08,900
an important aspect of precision medicine is to utilize

27
02:55:08,900 --> 02:55:13,570
an individual genetic profile for his or her own diagnoses and treatment 

28
02:55:14,910 --> 02:55:16,630
analyzing the human genome 

29
02:55:16,630 --> 02:55:20,670
which holds the key to human health is rapidly becoming more affordable 

30
02:55:21,950 --> 02:55:25,110
today cost to sequence a genome is less

31
02:55:25,110 --> 02:55:27,760
than 10 of what it cost just back in 2008 

32
02:55:27,760 --> 02:55:33,070
but human genomic data is big 

33
02:55:33,070 --> 02:55:34,390
how big 

34
02:55:34,390 --> 02:55:38,660
in a perfect world just the three billion letters of your genome

35
02:55:38,660 --> 02:55:41,220
would require about 700 megabytes to store 

36
02:55:42,300 --> 02:55:47,280
in the real world meaning the kind of data generated from genome sequencing

37
02:55:47,280 --> 02:55:51,540
machines we need 200gb to store a genome 

38
02:55:52,620 --> 02:55:56,700
and it takes now about a day to sequence a genome 

39
02:55:56,700 --> 02:56:01,520
we are finally beginning to create more electronic records that can be stored and

40
02:56:01,520 --> 02:56:03,130
manipulated in digital media 

41
02:56:04,270 --> 02:56:09,180
most doctors offices and hospitals now use electronic health record systems

42
02:56:09,180 --> 02:56:13,130
which contain all details of a patient visit and lab test 

43
02:56:14,410 --> 02:56:15,290
how big is this data 

44
02:56:16,710 --> 02:56:21,710
as a quick example the samaritan medical center watertown new york at 294 that

45
02:56:21,710 --> 02:56:27,690
community hospital reported 120 terabytes as of 2013 

46
02:56:27,690 --> 02:56:31,490
the data value more than double in just the last two years 

47
02:56:32,860 --> 02:56:37,860
so clearly just in a past two years dramatic changes have prepared the health

48
02:56:37,860 --> 02:56:42,920
care industry to produce and analyze larger mounts of complex patient data 

49
02:56:44,170 --> 02:56:49,570
to summarize what we have seen so far the key components of these changes are : 

50
02:56:49,570 --> 02:56:55,460
reduced cost of data generation and analysis increased availability of cheap

51
02:56:55,460 --> 02:57:01,430
large data storage and they increased digitization of previously paper records 

52
02:57:02,430 --> 02:57:07,560
but we need one more capability to advance toward the promised land of individualized

53
02:57:07,560 --> 02:57:08,410
health care practices 

54
02:57:09,950 --> 02:57:14,420
we need to combine various types of data produce by different groups in

55
02:57:14,420 --> 02:57:15,130
a meaningful way 

56
02:57:16,580 --> 02:57:20,150
let look at this issue from the same point of view as ilka did 

57
02:57:20,150 --> 02:57:23,760
with her discussion of how big data can help with wildfire analytics 

58
02:57:25,040 --> 02:57:28,980
the key is the integration of multiple types of data sources 

59
02:57:28,980 --> 02:57:33,130
data from sensors organizations and people 

60
02:57:33,130 --> 02:57:36,200
in the next few slides we look at each of these and

61
02:57:36,200 --> 02:57:41,820
then i will share a story about some of the new and really exciting ways people data

62
02:57:41,820 --> 02:57:46,660
especially has the potential to change healthcare big data landscape 

63
02:57:48,240 --> 02:57:49,320
let start with sensor data 

64
02:57:50,600 --> 02:57:55,000
sure digital hospital equipment have been producing sensor data for

65
02:57:55,000 --> 02:57:59,720
years but it was unlikely that the data was ever stored or

66
02:57:59,720 --> 02:58:03,790
shared let alone analyzed retrospectively 

67
02:58:03,790 --> 02:58:05,025
these were intended for

68
02:58:05,025 --> 02:58:09,880
real - time use to inform healthcare professionals and then got discarded 

69
02:58:11,110 --> 02:58:14,320
now we have many more sensors and deployment 

70
02:58:14,320 --> 02:58:16,850
and many more places that are capturing and

71
02:58:16,850 --> 02:58:19,970
explicitly gathering information to be stored and analyzed 

72
02:58:21,190 --> 02:58:23,970
let just take a new kind of data

73
02:58:23,970 --> 02:58:26,720
that increasingly becoming common in our daily lives 

74
02:58:28,960 --> 02:58:30,600
fitness devices are everywhere now

75
02:58:31,770 --> 02:58:35,320
their sales have skyrocketed in the last few years 

76
02:58:35,320 --> 02:58:40,360
they are in wristbands watches shoes and vests directly communicating with your

77
02:58:40,360 --> 02:58:45,160
personal mobile device tracking several activity variables like blood pressure 

78
02:58:45,160 --> 02:58:50,080
different types of activities blood glucose levels etc at every moment 

79
02:58:50,080 --> 02:58:52,850
their goal is to improve wellness 

80
02:58:52,850 --> 02:58:56,080
by having you monitor your daily status and

81
02:58:56,080 --> 02:58:59,410
hopefully improve your lifestyle to stay healthy 

82
02:58:59,410 --> 02:59:04,080
but the data they generate can be very useful medical information

83
02:59:04,080 --> 02:59:08,100
because this data is about what happens in your normal life and

84
02:59:08,100 --> 02:59:09,700
not just when you go to the doctor 

85
02:59:11,450 --> 02:59:13,660
how much data do they generate 

86
02:59:13,660 --> 02:59:17,890
the device called fitbit can produce several gigabytes a day 

87
02:59:17,890 --> 02:59:22,900
could this data be used to save healthcare costs effect a healthier lifestyle 

88
02:59:22,900 --> 02:59:23,660
that a question mark 

89
02:59:25,090 --> 02:59:29,660
it safe to guess that this data alone would not drive the dream of

90
02:59:29,660 --> 02:59:31,350
precision medicine 

91
02:59:31,350 --> 02:59:35,290
but what if we consider integrating it with other sources of data

92
02:59:35,290 --> 02:59:38,060
like electronic health records or a genomic profile 

93
02:59:39,150 --> 02:59:41,310
this remains an open question 

94
02:59:42,320 --> 02:59:46,460
this is an open arena for research that my colleagues at scripts are doing 

95
02:59:46,460 --> 02:59:51,090
it also a potentially significant area for product and business development 

96
02:59:51,090 --> 02:59:54,480
let look at some examples of health related data being generated by

97
02:59:54,480 --> 02:59:55,120
organizations 

98
02:59:56,630 --> 02:59:59,380
many public databases including those curated and

99
02:59:59,380 --> 03:00:03,690
managed by ncbi the national center for biotechnology information 

100
03:00:03,690 --> 03:00:07,180
had been created to capture the basic scientific data and knowledge for

101
03:00:07,180 --> 03:00:12,090
humans and other model organisms at the different building blocks of life 

102
03:00:12,090 --> 03:00:17,220
these databases carry both experimental and computed data that are necessary to

103
03:00:17,220 --> 03:00:20,193
observations for unconquered diseases like cancer 

104
03:00:20,193 --> 03:00:24,730
in addition many have created knoweledge - bases

105
03:00:24,730 --> 03:00:28,500
like the geneontology and the unified medical language system

106
03:00:28,500 --> 03:00:32,050
to assemble human knowledge in a machine processable form 

107
03:00:32,050 --> 03:00:35,660
these are just a few examples of organizational data sources and

108
03:00:35,660 --> 03:00:39,040
governmental data gathered by health care systems around the world

109
03:00:39,040 --> 03:00:42,050
could also be used as a massive source of information 

110
03:00:43,380 --> 03:00:46,210
but really some of the most interesting and

111
03:00:46,210 --> 03:00:51,040
novel opportunities seem likely to come from the area of people generated data 

112
03:00:52,410 --> 03:00:56,450
mobile healths apps is an area that is growing significantly 

113
03:00:56,450 --> 03:00:59,720
there are apps now to monitor heart rates blood pressure and

114
03:00:59,720 --> 03:01:01,310
test oxygen saturation levels 

115
03:01:02,860 --> 03:01:06,630
apps we might say record data from sensors but

116
03:01:06,630 --> 03:01:09,040
are also obviously generated from people 

117
03:01:10,230 --> 03:01:14,160
but there is more people generated data that interesting beyond censure

118
03:01:15,530 --> 03:01:16,120
measurements 

119
03:01:16,120 --> 03:01:19,280
in 2015 the webby people voice award

120
03:01:19,280 --> 03:01:22,310
went to an app which supports meditation and mindfulness 

121
03:01:23,410 --> 03:01:26,400
rather than an electronic sensing device 

122
03:01:26,400 --> 03:01:31,259
a human would indicate how many minutes per day they spent meditating 

123
03:01:31,259 --> 03:01:35,078
if they interact with the app which reminds them to be mindful 

124
03:01:35,078 --> 03:01:39,630
then we have human generated behavior that we could not get from a sensor 

125
03:01:40,840 --> 03:01:46,510
there are well over 100 000 health apps today in either itunes or google play 

126
03:01:46,510 --> 03:01:49,710
and by some estimates the mobile health app market

127
03:01:49,710 --> 03:01:53,064
may be worth 27 billion dollars by 2017 

128
03:01:54,070 --> 03:01:59,500
so really we are just seen the beginning of what data might be generated here

129
03:01:59,500 --> 03:02:05,490
from what is being called human sensors but to really understand where the power

130
03:02:05,490 --> 03:02:09,980
of people generated data might take us in the era of big data for healthcare 

131
03:02:09,980 --> 03:02:12,360
let imagine how things stand now 

132
03:02:12,360 --> 03:02:15,660
in general a patient goes to see their doctor and

133
03:02:15,660 --> 03:02:19,960
maybe their doctor asks if they have had any side effects from their medications 

134
03:02:21,060 --> 03:02:22,430
the accuracy and

135
03:02:22,430 --> 03:02:26,470
hence the quality of data patients provide in this kind of setting is very low 

136
03:02:27,620 --> 03:02:29,170
not that it really the patients fault 

137
03:02:30,240 --> 03:02:33,930
it might have been days or weeks ago that they experienced something 

138
03:02:33,930 --> 03:02:37,760
they may be unsure whether something they experienced was actually a reaction

139
03:02:37,760 --> 03:02:39,380
to then report it 

140
03:02:39,380 --> 03:02:42,590
and there might be details about exactly when they took a medication that

141
03:02:42,590 --> 03:02:44,790
are meaningful but they have forgotten it after the fact 

142
03:02:46,520 --> 03:02:52,120
today people are self reporting reactions and experiences they are having 

143
03:02:52,120 --> 03:02:56,540
we are on twitter on blog sites online support groups 

144
03:02:56,540 --> 03:03:01,030
online data sharing services : these are sources of data that we have 

145
03:03:01,030 --> 03:03:05,660
never had before that can be used to understand in a far more detailed and

146
03:03:05,660 --> 03:03:10,360
personal rate the impact of drug integrations are responses to certain

147
03:03:11,640 --> 03:03:14,430
if applications were designed to integrate doctor and

148
03:03:14,430 --> 03:03:19,040
hospital records with information on when drugs were taken and

149
03:03:19,040 --> 03:03:24,000
then to further mine social media or collect self reports from patients 

150
03:03:24,000 --> 03:03:27,230
who knows what kinds of questions we will be able to answer 

151
03:03:27,230 --> 03:03:29,008
or new questions we may be able to ask