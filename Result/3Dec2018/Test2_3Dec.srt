1
00:00:00,870 --> 00:00:04,540
in this short video we ll talk about how meltwater

2
00:00:04,540 --> 00:00:09,240
helped danone using sentiment analysis 

3
00:00:09,240 --> 00:00:12,600
meltwater is a company that helps other companies

4
00:00:12,600 --> 00:00:17,100
analyze what people are saying about them and manage their online reputation 

5
00:00:18,700 --> 00:00:22,980
one of the case studies on their website is about danone baby nutrition 

6
00:00:24,180 --> 00:00:29,358
meltwater helped danone to monitor the opinions through social media for

7
00:00:29,358 --> 00:00:31,873
one of their marketing campaigns 

8
00:00:31,873 --> 00:00:35,204
they were able to measure what was impactful and what was not 

9
00:00:35,204 --> 00:00:36,690
through such monitoring 

10
00:00:37,890 --> 00:00:42,610
meltwater also helped danone manage a potential reputation issue 

11
00:00:42,610 --> 00:00:45,380
when a crisis occurred related to horse dna

12
00:00:45,380 --> 00:00:48,320
being in some meat products across europe 

13
00:00:48,320 --> 00:00:52,830
while danone was confident that they did not have an issue with their products 

14
00:00:52,830 --> 00:00:57,730
having the information a couple of hours before news hit the uk press 

15
00:00:57,730 --> 00:00:59,690
allowed them to check and

16
00:00:59,690 --> 00:01:04,580
reassure their customers that their products were safe to consume 

17
00:01:04,580 --> 00:01:09,040
you can imagine millions of mothers having been reassured and happy for

18
00:01:09,040 --> 00:01:11,180
danone efforts on this 

19
00:01:11,180 --> 00:01:16,375
this is an excellent story about how big data helped manage public opinion 

20
00:01:16,375 --> 00:01:21,136
and i am sure meltwater was able to help them to measure the opinion impact through

21
00:01:21,136 --> 00:01:22,448
social media as well 

1
00:01:23,240 --> 00:01:26,508
big data is now being generated all around us 

2
00:01:26,508 --> 00:01:27,750
so what 

3
00:01:27,750 --> 00:01:29,450
it the applications 

4
00:01:29,450 --> 00:01:34,170
it is the way in which big data can serve human needs that makes it valued 

5
00:01:35,430 --> 00:01:39,971
let look at a few examples of the applications big data is allowing us to

6
00:01:39,971 --> 00:01:41,226
imagine and build 

7
00:01:59,796 --> 00:02:05,420
big data allows us to build better models which produce higher precision results 

8
00:02:06,470 --> 00:02:11,021
we are witnessing hugely innovative approaches in how companies

9
00:02:11,021 --> 00:02:13,758
market themselves and sell products 

10
00:02:13,758 --> 00:02:15,958
how human resources are managed 

11
00:02:15,958 --> 00:02:18,178
how disasters are responded to 

12
00:02:18,178 --> 00:02:22,389
and many other applications that evidenced based data is being

13
00:02:22,389 --> 00:02:24,460
used to influence decisions 

14
00:02:26,660 --> 00:02:28,670
what exactly does that mean 

15
00:02:28,670 --> 00:02:30,150
here is one example 

16
00:02:30,150 --> 00:02:32,640
many of you might have experienced it i do 

17
00:02:34,540 --> 00:02:38,140
data amazon keeps some things i have been looking at

18
00:02:38,140 --> 00:02:41,610
allows them to personalize what they show me 

19
00:02:41,610 --> 00:02:46,300
which hopefully helps narrow down the huge raft of options i might get

20
00:02:46,300 --> 00:02:49,380
than just searching on dinner plates 

21
00:02:49,380 --> 00:02:54,307
now businesses can leverage technology to make better informed decisions

22
00:02:54,307 --> 00:02:59,246
that are actually based on signals generated by actual consumers like me 

23
00:03:01,454 --> 00:03:05,980
big data enables you to hear the voice of each consumer as

24
00:03:05,980 --> 00:03:08,590
opposed to consumers at large 

25
00:03:09,920 --> 00:03:13,500
now many companies including walmart and target 

26
00:03:13,500 --> 00:03:19,160
use this information to personalize their communications with their costumers which

27
00:03:19,160 --> 00:03:23,240
in turns leads to better met consumer expectations and happier customers 

28
00:03:25,550 --> 00:03:31,628
which basically is to say big data has enabled personalized marketing 

29
00:03:31,628 --> 00:03:36,250
consumers are copiously generating publicly accessible data through

30
00:03:36,250 --> 00:03:38,560
social media sites like twitter or facebook 

31
00:03:39,690 --> 00:03:44,130
through such data the companies are able to see their purchase history 

32
00:03:44,130 --> 00:03:48,970
what they searched for what they watched where they have been and

33
00:03:48,970 --> 00:03:51,650
what they are interested in through their likes and shares 

34
00:03:52,920 --> 00:03:57,717
let look at some examples of how companies are putting this information to

35
00:03:57,717 --> 00:04:01,868
build better marketing campaigns and reach the right customers 

36
00:04:04,718 --> 00:04:08,985
one area we are all familiar with are the recommendation engines 

37
00:04:08,985 --> 00:04:14,190
these engines leverage user patterns and product features

38
00:04:14,190 --> 00:04:20,028
to predict best match product for enriching the user experience 

39
00:04:20,028 --> 00:04:22,303
if you ever shopped on amazon 

40
00:04:22,303 --> 00:04:26,596
you know you get recommendations based on your purchase 

41
00:04:26,596 --> 00:04:29,861
similarly netflix would recommend you to watch

42
00:04:29,861 --> 00:04:32,590
new shows based on your viewing history 

43
00:04:34,610 --> 00:04:40,550
another technique that companies use is sentiment analysis or in simple terms 

44
00:04:40,550 --> 00:04:44,550
analysis of the feelings around events and products 

45
00:04:45,880 --> 00:04:49,900
remember the blue plates i purchased on amazon com 

46
00:04:49,900 --> 00:04:53,680
i not only can read the reviews before purchasing them 

47
00:04:53,680 --> 00:04:57,460
i can also write a product review once i receive my plates 

48
00:04:59,280 --> 00:05:02,500
this way other customers can be informed 

49
00:05:03,880 --> 00:05:09,110
but more importantly amazon can keep a watch on the product reviews and

50
00:05:09,110 --> 00:05:11,220
trends for a particular product 

51
00:05:11,220 --> 00:05:13,951
in this case blue plates 

52
00:05:13,951 --> 00:05:20,300
for example they can judge if a product review is positive or negative 

53
00:05:21,940 --> 00:05:25,140
in this case while the first review is negative 

54
00:05:27,140 --> 00:05:29,530
the next two reviews are positive 

55
00:05:31,180 --> 00:05:35,130
since these reviews are written in english using a technique called natural

56
00:05:35,130 --> 00:05:38,780
language processing and other analytical methods 

57
00:05:38,780 --> 00:05:44,120
amazon can analyze the general opinion of a person or public about such a product 

58
00:05:46,130 --> 00:05:51,128
this is why sentiment analysis often gets referred to as opinion mining 

59
00:05:53,298 --> 00:05:57,729
news channels are filled with twitter feed analysis every time

60
00:05:57,729 --> 00:06:01,420
an event of importance occurs such as elections 

61
00:06:02,810 --> 00:06:08,162
brands utilize sentiment analysis to understand how customers

62
00:06:08,162 --> 00:06:13,630
relate to their product positively negatively neutral 

63
00:06:13,630 --> 00:06:16,448
this depends heavily on use of natural language processing 

64
00:06:19,183 --> 00:06:21,289
mobile devices are ubiquitous and

65
00:06:21,289 --> 00:06:24,780
people almost always carry their cellphones with them 

66
00:06:25,810 --> 00:06:29,200
mobile advertising is a huge market for businesses 

67
00:06:30,760 --> 00:06:35,581
platforms utilize the sensors in mobile devices 

68
00:06:35,581 --> 00:06:40,847
such as gps and provide real time location based ads 

69
00:06:40,847 --> 00:06:45,456
offer discounts based on this deluge of data 

70
00:06:45,456 --> 00:06:50,063
this time let imagine that i bought a new house and

71
00:06:50,063 --> 00:06:54,178
i happen to be in a few miles range of a home depot 

72
00:06:54,178 --> 00:06:57,444
sending me mobile coupons about paint shelves and

73
00:06:57,444 --> 00:07:01,290
other new home related purchases would remind me of home depot 

74
00:07:02,550 --> 00:07:05,590
there a big chance i would stop by home depot 

75
00:07:05,590 --> 00:07:06,790
bingo ! 

76
00:07:06,790 --> 00:07:11,470
now i would like to take a moment to analyze what kinds of big data are needed

77
00:07:11,470 --> 00:07:12,270
to make this happen 

78
00:07:13,495 --> 00:07:17,990
there definitely the integration of my consumer information and

79
00:07:17,990 --> 00:07:22,710
the online and offline databases that include my recent purchases 

80
00:07:22,710 --> 00:07:27,569
but more importantly the geolocation data that falls under

81
00:07:27,569 --> 00:07:31,128
a larger type of big data spacial big data 

82
00:07:31,128 --> 00:07:33,780
we will talk about spacial data later in this class 

83
00:07:35,190 --> 00:07:39,770
let now talk about how the global consumer behavior can be used for

84
00:07:39,770 --> 00:07:40,360
product growth 

85
00:07:42,170 --> 00:07:45,760
we are now moving from personalize marketing

86
00:07:45,760 --> 00:07:48,230
to the consumer behavior as a whole 

87
00:07:49,920 --> 00:07:54,758
every business wants to understand their consumer s collective

88
00:07:54,758 --> 00:07:59,258
behavior in order to capture the ever - changing landscape 

89
00:07:59,258 --> 00:08:04,809
several big data products enable this by developing models to capture user

90
00:08:04,809 --> 00:08:10,730
behavior and allow businesses to target the right audience for their product 

91
00:08:12,340 --> 00:08:15,250
or develop new products for uncharted territories 

92
00:08:17,570 --> 00:08:19,550
let look at this example 

93
00:08:19,550 --> 00:08:22,797
after an analysis of their sales for weekdays 

94
00:08:22,797 --> 00:08:28,319
an airline company might notice that their morning flights are always sold out 

95
00:08:28,319 --> 00:08:31,908
while their afternoon flights run below capacity 

96
00:08:31,908 --> 00:08:37,545
this company might decide to add more morning flights based on such analysis 

97
00:08:38,900 --> 00:08:43,550
notice that they are not using individual consumer choices but

98
00:08:43,550 --> 00:08:48,760
using all the flights purchased without consideration to who purchased them 

99
00:08:50,200 --> 00:08:51,150
they might however 

100
00:08:51,150 --> 00:08:56,400
decide to pay closer attention to the demographic of these consumers

101
00:08:56,400 --> 00:09:00,850
using big data to also add similar flights in other geographical regions 

102
00:09:02,350 --> 00:09:07,240
with rapid advances in genome sequencing technology 

103
00:09:07,240 --> 00:09:13,580
the life sciences industry is experiencing an enormous draw in biomedical big data 

104
00:09:15,120 --> 00:09:21,168
this biomedical data is being used by many applications in research and

105
00:09:21,168 --> 00:09:23,398
personalized medicine 

106
00:09:23,398 --> 00:09:28,400
did you know genomics data is one of the largest growing big data types 

107
00:09:28,400 --> 00:09:35,940
between 100 million and 2 billion human genomes could be sequenced by year 2025 

108
00:09:35,940 --> 00:09:36,520
impressive 

109
00:09:38,700 --> 00:09:41,080
this inaudible sequence data demands for

110
00:09:41,080 --> 00:09:46,580
between 2 exabytes and 40 exabytes in data storage 

111
00:09:46,580 --> 00:09:52,190
in comparison all of youtube only requires 1 to 2 exabytes a year 

112
00:09:54,530 --> 00:09:57,886
an exabyte is 10 to the power 18 bites 

113
00:09:57,886 --> 00:10:03,818
that is 18 zeros after 40 

114
00:10:03,818 --> 00:10:10,270
of course analysis of such massive volumes of sequence data is expensive 

115
00:10:10,270 --> 00:10:12,680
it could take up to 10 000 trillion cpu hours 

116
00:10:16,580 --> 00:10:21,040
one of the biomedical applications that this much data is enabling

117
00:10:21,040 --> 00:10:22,450
is personalized medicine 

118
00:10:24,060 --> 00:10:28,879
before personalized medicine most patients without a specific type and

119
00:10:28,879 --> 00:10:31,862
stage of cancer received the same treatment 

120
00:10:31,862 --> 00:10:34,858
which worked better for some than the others 

121
00:10:36,968 --> 00:10:42,086
research in this area is enabling development of methods to analyze

122
00:10:42,086 --> 00:10:47,472
large scale data to develop solutions that tailor to each individual 

123
00:10:47,472 --> 00:10:50,900
and hence hypothesize to be more effective 

124
00:10:52,450 --> 00:10:58,260
a person with cancer may now still receive a treatment plan that is standard 

125
00:10:58,260 --> 00:11:00,620
such as surgery to remove a tumor 

126
00:11:01,870 --> 00:11:05,750
however the doctor may also be able to recommend

127
00:11:05,750 --> 00:11:07,460
some type of personalized cancer treatment 

128
00:11:09,150 --> 00:11:13,900
a big challenge in biomedical big data applications like many other fields 

129
00:11:13,900 --> 00:11:18,860
is how we can integrate many types of data sources to gain further insight problem 

130
00:11:20,300 --> 00:11:23,648
in one of our future lectures my colleagues here at

131
00:11:23,648 --> 00:11:28,670
the supercomputer center will explain how he and his colleague have

132
00:11:28,670 --> 00:11:33,940
used big data from a variety of sources for personalized patient interventions 

133
00:11:35,840 --> 00:11:41,010
another application of big data comes from interconnected mesh of large

134
00:11:41,010 --> 00:11:46,490
number of sensors implanted across smart cities 

135
00:11:46,490 --> 00:11:50,790
analysis of data generated from sensors in real time

136
00:11:50,790 --> 00:11:55,140
allows cities to deliver better service quality to inhabitants 

137
00:11:55,140 --> 00:12:00,340
and reduce unwanted affect such as pollution traffic congestion 

138
00:12:00,340 --> 00:12:03,470
higher than optimal cost on delivering urban services 

139
00:12:04,910 --> 00:12:06,750
let take our city san diego 

140
00:12:08,370 --> 00:12:13,794
san diego generates a huge volumes of data from many sources 

141
00:12:13,794 --> 00:12:19,676
traffic sensors satellites camera networks and more 

142
00:12:19,676 --> 00:12:21,694
what if we could integrate and

143
00:12:21,694 --> 00:12:26,140
synthesize these data streams to do even more for our community 

144
00:12:27,260 --> 00:12:29,000
using such big data 

145
00:12:29,000 --> 00:12:34,170
we can work toward making san diego the prototype digital city 

146
00:12:34,170 --> 00:12:37,020
not only for life - threatening hazards but

147
00:12:37,020 --> 00:12:42,280
making our daily lives better such as managing traffic flow more efficiently or

148
00:12:42,280 --> 00:12:46,870
maximizing energy savings even as we will see next wildfires 

149
00:12:48,450 --> 00:12:53,390
if you want to read more here a link to the at kearney report 

150
00:12:53,390 --> 00:12:56,120
where they talk about other areas using big data 

151
00:12:57,580 --> 00:13:01,880
as a summary big data has a huge potential

152
00:13:01,880 --> 00:13:06,140
to enable models with higher precision in many application areas 

153
00:13:07,230 --> 00:13:12,175
and these highly precise models are influencing and transforming business 

1
00:14:35,140 --> 00:14:38,170
big data generated by people how is it being used 

2
00:14:39,740 --> 00:14:42,370
we listed a number of challenges for

3
00:14:42,370 --> 00:14:45,400
using unstructured data generated by human activities 

4
00:14:47,080 --> 00:14:51,730
now let look at some of the emerging technologies to tackle these challenges 

5
00:14:51,730 --> 00:14:56,930
and see some examples that turn unstructured data into valuable insights 

6
00:15:15,968 --> 00:15:20,780
although unstructured data specially the kind generated by people has

7
00:15:20,780 --> 00:15:23,320
a number of challenges 

8
00:15:23,320 --> 00:15:27,300
the good news is that the business culture of today is shifting

9
00:15:27,300 --> 00:15:31,070
to tackle these challenges and take full advantage of such data 

10
00:15:32,150 --> 00:15:36,120
as it is often said a challenge is a perfect opportunity 

11
00:15:37,130 --> 00:15:40,150
this is certainly the case for big data and

12
00:15:40,150 --> 00:15:43,680
these challenges have created a tech industry of it own 

13
00:15:44,860 --> 00:15:49,730
this industry is mostly centered or as we would say layered or

14
00:15:49,730 --> 00:15:55,493
stacked around a few fundamental open source big data frameworks 

15
00:15:55,493 --> 00:15:59,320
need big data tools are designed from scratch

16
00:15:59,320 --> 00:16:02,990
to manage unstructured information and analyze it 

17
00:16:02,990 --> 00:16:06,860
a majority of these tools are based on an open source

18
00:16:06,860 --> 00:16:08,620
big data framework called hadoop 

19
00:16:09,840 --> 00:16:14,010
hadoop is designed to support the processing of large data sets

20
00:16:14,010 --> 00:16:16,960
in a distributed computing environment 

21
00:16:16,960 --> 00:16:21,510
this definition would already give you a hint that it tackles the first challenge 

22
00:16:21,510 --> 00:16:25,140
namely the volume of unstructured information 

23
00:16:26,150 --> 00:16:29,740
hadoop can handle big batches of distributed information but

24
00:16:29,740 --> 00:16:32,250
most often there a need for

25
00:16:32,250 --> 00:16:37,650
a real time processing of people generated data like twitter or facebook updates 

26
00:16:39,220 --> 00:16:43,950
financial compliance monitoring is another area of our central time processing is

27
00:16:43,950 --> 00:16:47,380
needed in particular to reduce market data 

28
00:16:48,750 --> 00:16:54,780
social media and market data are two types of what we call high velocity data 

29
00:16:55,840 --> 00:16:59,720
storm and spark are two other open source frameworks

30
00:16:59,720 --> 00:17:03,480
that handle such real time data generated at a fast rate 

31
00:17:04,490 --> 00:17:05,490
both storm and

32
00:17:05,490 --> 00:17:10,230
spark can integrate data with any database or data storage technology 

33
00:17:11,560 --> 00:17:15,810
as we have emphasized before unstructured data

34
00:17:15,810 --> 00:17:19,740
does not have a relational data model so it does not generally

35
00:17:19,740 --> 00:17:24,150
fit into the traditional data warehouse model based on relational databases 

36
00:17:25,350 --> 00:17:29,870
data warehouses are central repositories of integrated data from one or

37
00:17:29,870 --> 00:17:30,490
more sources 

38
00:17:32,300 --> 00:17:38,260
the data that gets stored in warehouses gets extracted from multiple sources 

39
00:17:39,460 --> 00:17:43,850
it gets transformed into a common structured form and

40
00:17:43,850 --> 00:17:47,120
it can slow that into the central database for

41
00:17:47,120 --> 00:17:51,400
use by workers creating analytical reports throughout an enterprise 

42
00:17:52,650 --> 00:17:59,613
this exact transform load process is commonly called etl 

43
00:17:59,613 --> 00:18:03,480
this approach was fairly standard in enterprise data systems until recently 

44
00:18:04,560 --> 00:18:08,530
as you probably noticed it is fairly static and

45
00:18:08,530 --> 00:18:11,350
does not fit well with today dynamic big data world 

46
00:18:12,550 --> 00:18:17,073
so how do today businesses get around this problem 

47
00:18:17,073 --> 00:18:21,834
many businesses today are using a hybrid approach in which their smaller

48
00:18:21,834 --> 00:18:25,894
structured data remains in their relational databases and

49
00:18:25,894 --> 00:18:30,750
large unstructured datasets get stored in nosql databases in the cloud 

50
00:18:32,200 --> 00:18:37,840
nosql data technologies are based on non - relational concepts and

51
00:18:37,840 --> 00:18:42,310
provide data storage options typically on computing clouds

52
00:18:42,310 --> 00:18:46,180
beyond the traditional relational databases centered rate houses 

53
00:18:48,170 --> 00:18:53,640
the main advantage of using nosql solutions is their ability

54
00:18:53,640 --> 00:18:58,430
to organize the data for scalable access to fit the problem and

55
00:18:58,430 --> 00:19:01,330
objectives pertaining to how the data will be used 

56
00:19:03,130 --> 00:19:08,990
for example if the data will be used in an analysis to find connections

57
00:19:08,990 --> 00:19:14,130
between data sets then the best solution is a graph database 

58
00:19:15,960 --> 00:19:18,800
neo4j is an example of a graph database 

59
00:19:19,910 --> 00:19:23,630
graph networks is a topic that the graph analytics course

60
00:19:23,630 --> 00:19:27,140
later in this specialization we will explain in depth 

61
00:19:27,140 --> 00:19:33,350
if the data will be best accessed using key value pairs like a search engine

62
00:19:33,350 --> 00:19:39,490
scenario the best solution is probably a dedicated key value paired database 

63
00:19:42,710 --> 00:19:45,660
cassandra is an example of a key value database 

64
00:19:47,060 --> 00:19:48,010
these and

65
00:19:48,010 --> 00:19:52,710
many other types of nosql systems will be explained further in course two 

66
00:19:54,190 --> 00:19:58,160
so we are now confident that there are emerging technologies for

67
00:19:58,160 --> 00:20:02,410
individual challenges to manage people generated unstructured data 

68
00:20:03,610 --> 00:20:07,800
but how does one take advantage of these to generate value 

69
00:20:09,710 --> 00:20:17,150
as we saw big data must pass through a series of steps before it generates value 

70
00:20:17,150 --> 00:20:21,520
namely data access storage cleaning and analysis 

71
00:20:23,220 --> 00:20:29,210
one approach to solve this problem is to run each stage as a different layer 

72
00:20:30,400 --> 00:20:34,210
and use tools available to fit the problem at hand and

73
00:20:34,210 --> 00:20:37,730
scale analytical solutions to big data 

74
00:20:37,730 --> 00:20:42,110
in coming lectures we will see important tools that you can use

75
00:20:42,110 --> 00:20:45,840
to solve your big data problems in addition to the ones you have seen today 

76
00:20:47,070 --> 00:20:51,960
now let take a step back and remind ourselves what some of the value was 

77
00:20:53,220 --> 00:20:57,920
remember how companies can listen to the real voice of customers using big data 

78
00:20:59,540 --> 00:21:03,150
it is this type of generated data that enabled it 

79
00:21:04,190 --> 00:21:09,150
sentiment analysis analyzes social media and other data to find

80
00:21:09,150 --> 00:21:14,750
whether people associate positively or negatively with you business 

81
00:21:14,750 --> 00:21:19,210
organizations are utilizing processing of personal data to

82
00:21:19,210 --> 00:21:21,980
understand the true preferences of their customers 

83
00:21:23,130 --> 00:21:28,110
now let take a fun quiz to guess how much twitter data companies analyze

84
00:21:28,110 --> 00:21:31,510
every day to measure sentiment around their product 

85
00:21:33,080 --> 00:21:35,900
the answer is 12 terabytes a day 

86
00:21:37,040 --> 00:21:42,160
for comparison you would need to listen continuously for

87
00:21:42,160 --> 00:21:45,620
two years to finish listening to 1 terabyte of music 

88
00:21:47,000 --> 00:21:49,332
another example application area for

89
00:21:49,332 --> 00:21:52,960
people generated data is customer behavior modeling and prediction 

90
00:21:54,100 --> 00:21:58,730
amazon netflix and a lot of other organizations 

91
00:21:58,730 --> 00:22:02,400
use analytics to analyze preferences of their customers 

92
00:22:03,710 --> 00:22:09,260
based on consumer behavior organizations suggest better products to customers 

93
00:22:10,350 --> 00:22:14,430
and in turn have happier customers and higher profits 

94
00:22:15,610 --> 00:22:21,250
another application area where the value comes in the form of societal impact and

95
00:22:21,250 --> 00:22:24,540
social welfare is disaster management 

96
00:22:25,720 --> 00:22:28,550
as you have seen in my wildfire example 

97
00:22:28,550 --> 00:22:32,030
there are many types of big data that can help with disaster response 

98
00:22:33,450 --> 00:22:37,420
data in the form of pictures and tweets helps facilitate

99
00:22:37,420 --> 00:22:42,380
a collective response to disaster situations such as evacuations through

100
00:22:42,380 --> 00:22:46,310
the safest route based on community feedback through social media 

101
00:22:47,340 --> 00:22:50,820
there are also networks that turn crowd sourcing and

102
00:22:50,820 --> 00:22:54,650
big data analytics into collective disaster response tools 

103
00:22:55,920 --> 00:22:58,990
the international network of crisis mappers 

104
00:22:58,990 --> 00:23:03,950
also called crisis mappers net is the largest of such networks and

105
00:23:03,950 --> 00:23:08,360
includes an active international community of volunteers 

106
00:23:08,360 --> 00:23:14,190
crisis mappers use big data in the form of aerial and satellite imagery 

107
00:23:14,190 --> 00:23:19,440
participatory maps and live twitter updates to analyze

108
00:23:19,440 --> 00:23:25,611
the data using geospatial platforms advanced visualization 

109
00:23:25,611 --> 00:23:30,300
live simulation and computational and statistical models 

110
00:23:31,780 --> 00:23:37,220
once analyzed the results get reported to rapid response and humanitarian agencies

111
00:23:38,220 --> 00:23:44,020
in the form of mobile and web applications 

112
00:23:44,020 --> 00:23:50,510
in 2015 right after the nepal earthquake crises mappers crowd source the analysis

113
00:23:50,510 --> 00:23:55,990
of tweets and mainstream media to rapidly access disaster damage and

114
00:23:55,990 --> 00:24:01,210
needs and to identify where humanitarian help is needed 

115
00:24:01,210 --> 00:24:06,663
this example is amazing and shows how big data can have huge impacts for

116
00:24:06,663 --> 00:24:09,263
social welfare in times of need 

117
00:24:09,263 --> 00:24:12,400
you can learn more about this story at the following link 

118
00:24:14,790 --> 00:24:19,679
as a summary although there are challenges in working with unstructured

119
00:24:19,679 --> 00:24:24,270
people generated data at a scale and speed that applications demand 

120
00:24:24,270 --> 00:24:28,124
there are also emerging technologies and solutions that are being

121
00:24:28,124 --> 00:24:32,890
used by many applications to generate value from the rich source of information 

1
00:39:06,920 --> 00:39:11,485
big data generated by people the unstructured challenge 

2
00:39:27,404 --> 00:39:32,471
people are generating massive amounts of data every day through their activities on

3
00:39:32,471 --> 00:39:37,405
various social media networking sites like facebook twitter and linkedin 

4
00:39:37,405 --> 00:39:42,655
or online photo sharing sites like instagram flickr or picasa 

5
00:39:44,540 --> 00:39:46,620
and video sharing websites like youtube 

6
00:39:48,020 --> 00:39:53,100
in addition an enormous amount of information gets generated via

7
00:39:53,100 --> 00:39:58,250
blogging and commenting internet searches more via text messages 

8
00:39:59,320 --> 00:40:02,460
email and through personal documents 

9
00:40:03,920 --> 00:40:08,711
most of this data is text - heavy and unstructured 

10
00:40:08,711 --> 00:40:14,780
that is non - conforming to a well - defined data model 

11
00:40:14,780 --> 00:40:18,690
we can also consider this data to be content with

12
00:40:18,690 --> 00:40:21,630
occasionally some description attached to it 

13
00:40:21,630 --> 00:40:26,360
this much activity leads to a huge growth in data 

14
00:40:27,480 --> 00:40:31,980
did you know that in a single day facebook users produce

15
00:40:31,980 --> 00:40:36,550
more data than combined us academic research libraries 

16
00:40:38,100 --> 00:40:41,130
let look at some similar daily data volume numbers

17
00:40:42,130 --> 00:40:44,460
from some of the biggest online platforms 

18
00:40:45,720 --> 00:40:49,790
it is amazing that some of these numbers are in the petabyte range for

19
00:40:49,790 --> 00:40:50,700
daily activity 

20
00:40:51,840 --> 00:40:54,930
a petabyte is a thousand terabytes 

21
00:40:56,470 --> 00:41:01,360
the sheer size of mostly unstructured data generated by humans

22
00:41:01,360 --> 00:41:02,950
brings a lot of challenges 

23
00:41:04,730 --> 00:41:11,250
unstructured data refers to data that does not conform to a predefined data model 

24
00:41:13,200 --> 00:41:16,380
so no relation model and no sql 

25
00:41:17,980 --> 00:41:22,190
it is mostly anything that we do not store in a traditional

26
00:41:22,190 --> 00:41:23,640
relational database management system 

27
00:41:25,170 --> 00:41:28,480
consider a sales receipt that you get from a grocery store 

28
00:41:29,580 --> 00:41:32,790
it has a section for a date a section for

29
00:41:32,790 --> 00:41:36,690
store name and a section for total amount 

30
00:41:38,200 --> 00:41:40,310
this is an example of structure 

31
00:41:41,340 --> 00:41:46,550
humans generate a lot of unstructured data in form of text 

32
00:41:46,550 --> 00:41:48,960
there no given format to that 

33
00:41:48,960 --> 00:41:52,180
look at all the documents that you have written with your hand so far 

34
00:41:53,220 --> 00:41:58,550
collectively it is a bank of unstructured data you have personally generated 

35
00:41:59,580 --> 00:42:04,160
in fact 80 to 90 of all data in

36
00:42:04,160 --> 00:42:08,980
the world is unstructured and this number is rapidly growing 

37
00:42:10,490 --> 00:42:15,531
examples of unstructured data generated by people includes texts images 

38
00:42:15,531 --> 00:42:22,190
videos audio internet searches and emails 

39
00:42:22,190 --> 00:42:27,580
in addition to it rapid growth major challenges of unstructured data

40
00:42:27,580 --> 00:42:32,768
include multiple data formats like webpages images pdfs 

41
00:42:32,768 --> 00:42:40,170
power point xml and other formats that were mainly built for human consumption 

42
00:42:40,170 --> 00:42:47,030
think of it although i can sort my email with date sender and subject 

43
00:42:47,030 --> 00:42:50,690
it would be really difficult to write a program 

44
00:42:50,690 --> 00:42:55,430
to categorize all my email messages based on their content and

45
00:42:55,430 --> 00:43:01,460
organize them for me accordingly another challenge of human generated data

46
00:43:01,460 --> 00:43:07,560
is the volume and fast generation of data which is what we call velocity 

47
00:43:07,560 --> 00:43:13,504
just take a moment to study this info graphic and observe what happens in one

48
00:43:13,504 --> 00:43:18,728
minute on the internet and consider how much to contribute to it 

49
00:43:21,088 --> 00:43:28,586
moreover confirmation of unstructured data is often time consuming and costly 

50
00:43:28,586 --> 00:43:34,130
the costs and time of the process of acquiring storing 

51
00:43:34,130 --> 00:43:40,110
cleaning retrieving and processing unstructured data can add up to quite and

52
00:43:40,110 --> 00:43:43,220
investment before we can start reaping value from this process 

53
00:43:45,230 --> 00:43:47,790
it can be pretty hard to find the tools and

54
00:43:47,790 --> 00:43:51,810
people to implement such a process and reap value in the end 

55
00:43:53,030 --> 00:43:56,570
as a summary although there is an enormous amount of

56
00:43:56,570 --> 00:44:00,780
data generated by people most of this data is unstructured 

57
00:44:01,890 --> 00:44:05,230
the challenges of working with unstructured data should not

58
00:44:05,230 --> 00:44:07,170
be taken lightly 

59
00:44:07,170 --> 00:44:13,310
next we will look at how businesses are tackling these challenges to gain insight 

60
00:44:13,310 --> 00:44:16,770
and thus value out of working with people generated data 

1
01:23:23,120 --> 01:23:27,970
as we have already seen there are many different exciting applications

2
01:23:27,970 --> 01:23:30,060
that are being enabled by the big data era 

3
01:23:31,160 --> 01:23:35,500
as part of my core research here at the san diego supercomputer center 

4
01:23:35,500 --> 01:23:37,420
i work on building methodologies and

5
01:23:37,420 --> 01:23:42,150
tools to make big data useful to dynamic data driven scientific applications 

6
01:23:43,170 --> 01:23:47,600
my colleagues and i work on many grand challenge data science applications 

7
01:23:47,600 --> 01:23:52,903
in all areas of science and engineering including genomics geoinformatics 

8
01:23:52,903 --> 01:23:58,050
metro science energy management biomedicine and personalized health 

9
01:23:59,660 --> 01:24:04,492
what is common to all these applications is their unique way of bringing

10
01:24:04,492 --> 01:24:08,046
together new modes of data and computing research 

11
01:24:08,046 --> 01:24:14,195
let me tell you the one i am passionate

12
01:24:14,195 --> 01:24:19,192
about wildfire analytics 

13
01:24:19,192 --> 01:24:25,342
which breaks up into two components 

14
01:24:25,342 --> 01:24:29,779
prediction and response 

15
01:24:40,713 --> 01:24:43,629
why is this so important 

16
01:24:43,629 --> 01:24:47,608
on may 2014 in san diego county where

17
01:24:47,608 --> 01:24:52,804
the instructors of this specialization live and work 

18
01:24:52,804 --> 01:24:58,330
there were 14 fires burning as many as nine at one time 

19
01:24:58,330 --> 01:25:04,964
which burned a total of 26 000 acres 11 000 hectares 

20
01:25:04,964 --> 01:25:10,520
an area just less than the size of the city of san francisco 

21
01:25:12,710 --> 01:25:18,087
six people were injured and one person died 

22
01:25:18,087 --> 01:25:22,093
and these wildfires resulted in a total cost of

23
01:25:22,093 --> 01:25:26,620
over 60 million us in damage and firefighting 

24
01:25:27,870 --> 01:25:32,925
these wildfires can become so severe that we actually call them firestorms 

25
01:25:34,150 --> 01:25:37,170
although we cannot control such fire storms 

26
01:25:37,170 --> 01:25:41,070
something we can do is to get ahead of them by predicting their behavior 

27
01:25:42,790 --> 01:25:47,670
this is why disaster management of ongoing wildfires relies

28
01:25:47,670 --> 01:25:51,470
heavily on understanding their direction and rate of spread 

29
01:25:52,560 --> 01:25:55,000
as these fires are a part of our lives 

30
01:25:55,000 --> 01:26:00,410
we wanted to see if we can use big data to monitor predict and manage a firestorm 

31
01:26:02,400 --> 01:26:04,337
why can big data help 

32
01:26:04,337 --> 01:26:08,685
as we will see in this video indeed wildfire prevention and

33
01:26:08,685 --> 01:26:12,879
response can benefit from many streams in our data torrent 

34
01:26:12,879 --> 01:26:18,087
some streams are generated by people through devices they carry 

35
01:26:18,087 --> 01:26:23,713
a lot come from sensors and satellites things that measure environmental factors 

36
01:26:23,713 --> 01:26:27,855
and some come from organizational data including area maps 

37
01:26:27,855 --> 01:26:32,839
better service updates and field content databases which archive how much

38
01:26:32,839 --> 01:26:37,991
registers vegetation and other types of fuel are in the way of a potential fire 

39
01:26:39,840 --> 01:26:42,337
what makes this a big data problem 

40
01:26:42,337 --> 01:26:47,233
because novel approaches and responses can be taken if

41
01:26:47,233 --> 01:26:51,712
we can integrate this many diverse data streams 

42
01:26:51,712 --> 01:26:56,504
many such data sources have already existed for quite some time 

43
01:26:56,504 --> 01:27:01,835
but what is lacking in disaster management today is a dynamic system

44
01:27:01,835 --> 01:27:06,979
integration of real time sensor networks satellite imagery 

45
01:27:06,979 --> 01:27:12,405
near real time data management tools wildfire simulation tools 

46
01:27:12,405 --> 01:27:16,334
connectivity to emergency command centers and

47
01:27:16,334 --> 01:27:20,470
all these before during and after a firestorm 

48
01:27:21,800 --> 01:27:25,450
as you will see the integration of diverse streams and

49
01:27:25,450 --> 01:27:29,760
novel ways is really what is driving our ability to see new things and

50
01:27:29,760 --> 01:27:33,270
develop predictive analytics which may help improve our world 

51
01:27:34,770 --> 01:27:36,140
what are these diverse sources 

52
01:27:37,940 --> 01:27:42,870
one of the most important data sources is sensor data streaming in from weather

53
01:27:42,870 --> 01:27:48,540
stations and satellites such sensed data include temperature 

54
01:27:48,540 --> 01:27:49,960
humidity air pressure 

55
01:27:51,280 --> 01:27:55,346
we can also include image data streaming from

56
01:27:55,346 --> 01:28:00,379
mountaintop cameras and satellites in this category 

57
01:28:00,379 --> 01:28:03,967
another important data source comes from institutions

58
01:28:03,967 --> 01:28:06,867
such as the san diego supercomputer center 

59
01:28:06,867 --> 01:28:10,250
which generate data related to wildfire modeling 

60
01:28:10,250 --> 01:28:15,585
these include past and current fire perimeter maps put together by

61
01:28:15,585 --> 01:28:20,824
the authorities and fuel maps that tell us about the vegetation 

62
01:28:20,824 --> 01:28:24,171
and other types of fuel in a fire path 

63
01:28:24,171 --> 01:28:30,171
these types of data sources are often static or updated at a slow rate 

64
01:28:30,171 --> 01:28:36,083
but they provide valuable data that is well - curated and verified 

65
01:28:37,560 --> 01:28:42,450
a huge part of data on fires is actually generated by the public

66
01:28:42,450 --> 01:28:46,820
on social media sites such as twitter which support photo sharing resources 

67
01:28:48,650 --> 01:28:54,050
these are the hardest data sources to streamline during an existing fire but

68
01:28:54,050 --> 01:28:58,340
they can be very valuable once integrated with other data sources 

69
01:29:00,820 --> 01:29:06,380
imagine synthesizing all the pictures on twitter about an ongoing fire or

70
01:29:06,380 --> 01:29:09,850
checking the public sentiment around the boundaries of a fire 

71
01:29:11,960 --> 01:29:15,710
once you have access to such information at your fingertips 

72
01:29:15,710 --> 01:29:18,610
there are many things you can do with such data 

73
01:29:18,610 --> 01:29:23,300
you can simply monitor it or maybe you can visualize it 

74
01:29:25,310 --> 01:29:30,370
but it not until you bring all these different types of data sources together

75
01:29:30,370 --> 01:29:34,910
and integrate them with real time analysis and predictive modeling that you can

76
01:29:34,910 --> 01:29:39,570
really make contributions in predicting and responding to wildfire emergencies 

77
01:29:41,000 --> 01:29:44,221
so now i will like you to take a moment and

78
01:29:44,221 --> 01:29:49,296
imagine how big data might help with firefighting in the future 

79
01:29:49,296 --> 01:29:54,280
all these streams of data will come together in 3d displays that can show

80
01:29:54,280 --> 01:29:59,182
all the related information along with weather and fire predictions 

81
01:29:59,182 --> 01:30:02,462
just like the way tornadoes are managed today 

1
02:53:25,270 --> 02:53:30,840
let look at a 2nd example where big data can have a big impact on saving lives 

2
02:53:30,840 --> 02:53:34,428
i mean literally saving lives one life at a time 

3
02:53:34,428 --> 02:53:38,508
i collaborated with a number of world - class researchers in san diego and

4
02:53:38,508 --> 02:53:42,911
an industrial group who are dedicated to improving human health through research

5
02:53:42,911 --> 02:53:44,970
and practice of precision medicine 

6
02:53:46,400 --> 02:53:48,250
what is precision medicine 

7
02:53:48,250 --> 02:53:52,950
it is an emerging area of medicine targeted toward an individual person 

8
02:53:52,950 --> 02:53:57,910
analysing her genetics her environment her daily activities so

9
02:53:57,910 --> 02:54:01,880
that one can detect or predict a health problem early 

10
02:54:01,880 --> 02:54:06,570
help prevent disease and in case of illness provide the right drug

11
02:54:06,570 --> 02:54:10,690
at the right dose that is suitable just for her 

12
02:54:10,690 --> 02:54:15,587
very recently the white house and the national institute of health here

13
02:54:15,587 --> 02:54:20,401
in the american have declared it to be a top priority area for research and

14
02:54:20,401 --> 02:54:22,779
development for the next decade 

15
02:54:22,779 --> 02:54:25,689
the expected learning outcome of this video is for

16
02:54:25,689 --> 02:54:28,668
you to give example of sensor organizational and

17
02:54:28,668 --> 02:54:31,870
people - generated data used in precision medicine 

18
02:54:33,050 --> 02:54:37,650
and explain why the integration of different kinds of data is critical

19
02:54:37,650 --> 02:54:38,940
in advancing healthcare 

20
02:54:40,150 --> 02:54:44,000
for any technology to succeed in real life we need

21
02:54:44,000 --> 02:54:48,940
not only a certain level of maturity of the technology itself but a number of

22
02:54:48,940 --> 02:54:54,050
enabling factors including social economic environment market demands 

23
02:54:54,050 --> 02:54:58,860
consumer readiness cost effectiveness all of which must work together 

24
02:55:00,430 --> 02:55:03,790
why is big data for precision medicine important now 

25
02:55:03,790 --> 02:55:04,290
let see 

26
02:55:05,480 --> 02:55:08,900
an important aspect of precision medicine is to utilize

27
02:55:08,900 --> 02:55:13,570
an individual genetic profile for his or her own diagnoses and treatment 

28
02:55:14,910 --> 02:55:16,630
analyzing the human genome 

29
02:55:16,630 --> 02:55:20,670
which holds the key to human health is rapidly becoming more affordable 

30
02:55:21,950 --> 02:55:25,110
today cost to sequence a genome is less

31
02:55:25,110 --> 02:55:27,760
than 10 of what it cost just back in 2008 

32
02:55:27,760 --> 02:55:33,070
but human genomic data is big 

33
02:55:33,070 --> 02:55:34,390
how big 

34
02:55:34,390 --> 02:55:38,660
in a perfect world just the three billion letters of your genome

35
02:55:38,660 --> 02:55:41,220
would require about 700 megabytes to store 

36
02:55:42,300 --> 02:55:47,280
in the real world meaning the kind of data generated from genome sequencing

37
02:55:47,280 --> 02:55:51,540
machines we need 200gb to store a genome 

38
02:55:52,620 --> 02:55:56,700
and it takes now about a day to sequence a genome 

39
02:55:56,700 --> 02:56:01,520
we are finally beginning to create more electronic records that can be stored and

40
02:56:01,520 --> 02:56:03,130
manipulated in digital media 

41
02:56:04,270 --> 02:56:09,180
most doctors offices and hospitals now use electronic health record systems

42
02:56:09,180 --> 02:56:13,130
which contain all details of a patient visit and lab test 

43
02:56:14,410 --> 02:56:15,290
how big is this data 

44
02:56:16,710 --> 02:56:21,710
as a quick example the samaritan medical center watertown new york at 294 that

45
02:56:21,710 --> 02:56:27,690
community hospital reported 120 terabytes as of 2013 

46
02:56:27,690 --> 02:56:31,490
the data value more than double in just the last two years 

47
02:56:32,860 --> 02:56:37,860
so clearly just in a past two years dramatic changes have prepared the health

48
02:56:37,860 --> 02:56:42,920
care industry to produce and analyze larger mounts of complex patient data 

49
02:56:44,170 --> 02:56:49,570
to summarize what we have seen so far the key components of these changes are : 

50
02:56:49,570 --> 02:56:55,460
reduced cost of data generation and analysis increased availability of cheap

51
02:56:55,460 --> 02:57:01,430
large data storage and they increased digitization of previously paper records 

52
02:57:02,430 --> 02:57:07,560
but we need one more capability to advance toward the promised land of individualized

53
02:57:07,560 --> 02:57:08,410
health care practices 

54
02:57:09,950 --> 02:57:14,420
we need to combine various types of data produce by different groups in

55
02:57:14,420 --> 02:57:15,130
a meaningful way 

56
02:57:16,580 --> 02:57:20,150
let look at this issue from the same point of view as ilka did 

57
02:57:20,150 --> 02:57:23,760
with her discussion of how big data can help with wildfire analytics 

58
02:57:25,040 --> 02:57:28,980
the key is the integration of multiple types of data sources 

59
02:57:28,980 --> 02:57:33,130
data from sensors organizations and people 

60
02:57:33,130 --> 02:57:36,200
in the next few slides we look at each of these and

61
02:57:36,200 --> 02:57:41,820
then i will share a story about some of the new and really exciting ways people data

62
02:57:41,820 --> 02:57:46,660
especially has the potential to change healthcare big data landscape 

63
02:57:48,240 --> 02:57:49,320
let start with sensor data 

64
02:57:50,600 --> 02:57:55,000
sure digital hospital equipment have been producing sensor data for

65
02:57:55,000 --> 02:57:59,720
years but it was unlikely that the data was ever stored or

66
02:57:59,720 --> 02:58:03,790
shared let alone analyzed retrospectively 

67
02:58:03,790 --> 02:58:05,025
these were intended for

68
02:58:05,025 --> 02:58:09,880
real - time use to inform healthcare professionals and then got discarded 

69
02:58:11,110 --> 02:58:14,320
now we have many more sensors and deployment 

70
02:58:14,320 --> 02:58:16,850
and many more places that are capturing and

71
02:58:16,850 --> 02:58:19,970
explicitly gathering information to be stored and analyzed 

72
02:58:21,190 --> 02:58:23,970
let just take a new kind of data

73
02:58:23,970 --> 02:58:26,720
that increasingly becoming common in our daily lives 

74
02:58:28,960 --> 02:58:30,600
fitness devices are everywhere now

75
02:58:31,770 --> 02:58:35,320
their sales have skyrocketed in the last few years 

76
02:58:35,320 --> 02:58:40,360
they are in wristbands watches shoes and vests directly communicating with your

77
02:58:40,360 --> 02:58:45,160
personal mobile device tracking several activity variables like blood pressure 

78
02:58:45,160 --> 02:58:50,080
different types of activities blood glucose levels etc at every moment 

79
02:58:50,080 --> 02:58:52,850
their goal is to improve wellness 

80
02:58:52,850 --> 02:58:56,080
by having you monitor your daily status and

81
02:58:56,080 --> 02:58:59,410
hopefully improve your lifestyle to stay healthy 

82
02:58:59,410 --> 02:59:04,080
but the data they generate can be very useful medical information

83
02:59:04,080 --> 02:59:08,100
because this data is about what happens in your normal life and

84
02:59:08,100 --> 02:59:09,700
not just when you go to the doctor 

85
02:59:11,450 --> 02:59:13,660
how much data do they generate 

86
02:59:13,660 --> 02:59:17,890
the device called fitbit can produce several gigabytes a day 

87
02:59:17,890 --> 02:59:22,900
could this data be used to save healthcare costs effect a healthier lifestyle 

88
02:59:22,900 --> 02:59:23,660
that a question mark 

89
02:59:25,090 --> 02:59:29,660
it safe to guess that this data alone would not drive the dream of

90
02:59:29,660 --> 02:59:31,350
precision medicine 

91
02:59:31,350 --> 02:59:35,290
but what if we consider integrating it with other sources of data

92
02:59:35,290 --> 02:59:38,060
like electronic health records or a genomic profile 

93
02:59:39,150 --> 02:59:41,310
this remains an open question 

94
02:59:42,320 --> 02:59:46,460
this is an open arena for research that my colleagues at scripts are doing 

95
02:59:46,460 --> 02:59:51,090
it also a potentially significant area for product and business development 

96
02:59:51,090 --> 02:59:54,480
let look at some examples of health related data being generated by

97
02:59:54,480 --> 02:59:55,120
organizations 

98
02:59:56,630 --> 02:59:59,380
many public databases including those curated and

99
02:59:59,380 --> 03:00:03,690
managed by ncbi the national center for biotechnology information 

100
03:00:03,690 --> 03:00:07,180
had been created to capture the basic scientific data and knowledge for

101
03:00:07,180 --> 03:00:12,090
humans and other model organisms at the different building blocks of life 

102
03:00:12,090 --> 03:00:17,220
these databases carry both experimental and computed data that are necessary to

103
03:00:17,220 --> 03:00:20,193
observations for unconquered diseases like cancer 

104
03:00:20,193 --> 03:00:24,730
in addition many have created knoweledge - bases

105
03:00:24,730 --> 03:00:28,500
like the geneontology and the unified medical language system

106
03:00:28,500 --> 03:00:32,050
to assemble human knowledge in a machine processable form 

107
03:00:32,050 --> 03:00:35,660
these are just a few examples of organizational data sources and

108
03:00:35,660 --> 03:00:39,040
governmental data gathered by health care systems around the world

109
03:00:39,040 --> 03:00:42,050
could also be used as a massive source of information 

110
03:00:43,380 --> 03:00:46,210
but really some of the most interesting and

111
03:00:46,210 --> 03:00:51,040
novel opportunities seem likely to come from the area of people generated data 

112
03:00:52,410 --> 03:00:56,450
mobile healths apps is an area that is growing significantly 

113
03:00:56,450 --> 03:00:59,720
there are apps now to monitor heart rates blood pressure and

114
03:00:59,720 --> 03:01:01,310
test oxygen saturation levels 

115
03:01:02,860 --> 03:01:06,630
apps we might say record data from sensors but

116
03:01:06,630 --> 03:01:09,040
are also obviously generated from people 

117
03:01:10,230 --> 03:01:14,160
but there is more people generated data that interesting beyond censure

118
03:01:15,530 --> 03:01:16,120
measurements 

119
03:01:16,120 --> 03:01:19,280
in 2015 the webby people voice award

120
03:01:19,280 --> 03:01:22,310
went to an app which supports meditation and mindfulness 

121
03:01:23,410 --> 03:01:26,400
rather than an electronic sensing device 

122
03:01:26,400 --> 03:01:31,259
a human would indicate how many minutes per day they spent meditating 

123
03:01:31,259 --> 03:01:35,078
if they interact with the app which reminds them to be mindful 

124
03:01:35,078 --> 03:01:39,630
then we have human generated behavior that we could not get from a sensor 

125
03:01:40,840 --> 03:01:46,510
there are well over 100 000 health apps today in either itunes or google play 

126
03:01:46,510 --> 03:01:49,710
and by some estimates the mobile health app market

127
03:01:49,710 --> 03:01:53,064
may be worth 27 billion dollars by 2017 

128
03:01:54,070 --> 03:01:59,500
so really we are just seen the beginning of what data might be generated here

129
03:01:59,500 --> 03:02:05,490
from what is being called human sensors but to really understand where the power

130
03:02:05,490 --> 03:02:09,980
of people generated data might take us in the era of big data for healthcare 

131
03:02:09,980 --> 03:02:12,360
let imagine how things stand now 

132
03:02:12,360 --> 03:02:15,660
in general a patient goes to see their doctor and

133
03:02:15,660 --> 03:02:19,960
maybe their doctor asks if they have had any side effects from their medications 

134
03:02:21,060 --> 03:02:22,430
the accuracy and

135
03:02:22,430 --> 03:02:26,470
hence the quality of data patients provide in this kind of setting is very low 

136
03:02:27,620 --> 03:02:29,170
not that it really the patients fault 

137
03:02:30,240 --> 03:02:33,930
it might have been days or weeks ago that they experienced something 

138
03:02:33,930 --> 03:02:37,760
they may be unsure whether something they experienced was actually a reaction

139
03:02:37,760 --> 03:02:39,380
to then report it 

140
03:02:39,380 --> 03:02:42,590
and there might be details about exactly when they took a medication that

141
03:02:42,590 --> 03:02:44,790
are meaningful but they have forgotten it after the fact 

142
03:02:46,520 --> 03:02:52,120
today people are self reporting reactions and experiences they are having 

143
03:02:52,120 --> 03:02:56,540
we are on twitter on blog sites online support groups 

144
03:02:56,540 --> 03:03:01,030
online data sharing services : these are sources of data that we have 

145
03:03:01,030 --> 03:03:05,660
never had before that can be used to understand in a far more detailed and

146
03:03:05,660 --> 03:03:10,360
personal rate the impact of drug integrations are responses to certain

147
03:03:11,640 --> 03:03:14,430
if applications were designed to integrate doctor and

148
03:03:14,430 --> 03:03:19,040
hospital records with information on when drugs were taken and

149
03:03:19,040 --> 03:03:24,000
then to further mine social media or collect self reports from patients 

150
03:03:24,000 --> 03:03:27,230
who knows what kinds of questions we will be able to answer 

151
03:03:27,230 --> 03:03:29,008
or new questions we may be able to ask 

1
05:56:54,690 --> 05:56:56,830
where does big data come from 

2
05:56:56,830 --> 05:57:00,720
the first thing i would like to say before talking about big data

3
05:57:00,720 --> 05:57:02,010
is that it is not new 

4
05:57:03,040 --> 05:57:07,600
most of the big data sources existed before but the scale we use and

5
05:57:07,600 --> 05:57:10,020
apply them today has changed 

6
05:57:10,020 --> 05:57:14,590
just look at this image of open link data on the internet 

7
05:57:14,590 --> 05:57:16,740
i thought this image was so cool 

8
05:57:16,740 --> 05:57:20,350
it shows not only there are so many sources of data but

9
05:57:20,350 --> 05:57:21,490
they are also connected 

10
05:57:22,690 --> 05:57:25,147
if you want to check it out yourself 

11
05:57:25,147 --> 05:57:28,408
we will give you the link at the end of this video 

12
05:57:28,408 --> 05:57:33,859
big data is often boiled down to a few varieties of data generated by machines 

13
05:57:33,859 --> 05:57:37,010
people and organizations 

14
05:57:37,010 --> 05:57:42,057
with machine generated data we refer to data generated from real time sensors in

15
05:57:42,057 --> 05:57:47,029
industrial machinery or vehicles that logs that track user behavior online 

16
05:57:47,029 --> 05:57:48,687
environmental sensors or

17
05:57:48,687 --> 05:57:52,930
personal health trackers and many other sense data resources 

18
05:57:54,080 --> 05:57:58,560
the large hadron collider generates 40 terabytes of data

19
05:57:58,560 --> 05:58:01,560
every second during experiments 

20
05:58:01,560 --> 05:58:07,078
but human generated data we refer to the vast amount of social media data 

21
05:58:07,078 --> 05:58:11,420
status updates tweets photos and medias 

22
05:58:11,420 --> 05:58:16,943
with organizational generated data we refer to more traditional types of data 

23
05:58:16,943 --> 05:58:20,598
including transaction information in databases and

24
05:58:20,598 --> 05:58:24,026
structured data open stored in data warehouses 

25
05:58:24,026 --> 05:58:28,437
note that big data can be either structured semi - structured or

26
05:58:28,437 --> 05:58:33,390
unstructured which is a topic we will talk about more later in this course 

27
05:58:34,530 --> 05:58:40,180
in most business use cases any single source of data on its own is not useful 

28
05:58:41,490 --> 05:58:46,880
real value often comes from combining these streams of big data sources

29
05:58:46,880 --> 05:58:51,210
with each other and analyzing them to generate new insights 

30
05:58:51,210 --> 05:58:53,430
which then goes back into being big data themselves 

31
05:58:54,890 --> 05:58:56,870
once you have such insights 

32
05:58:56,870 --> 05:59:01,320
it then enables what we call data enabled decisions and actions 

33
05:59:02,670 --> 05:59:06,858
let now look into these different types of big data in more detail 

1
11:56:00,081 --> 11:56:03,312
why is big data generated by machines useful 

2
11:56:27,325 --> 11:56:30,997
let go back for a second to our first example for

3
11:56:30,997 --> 11:56:33,715
machine generated big data planes 

4
11:56:34,790 --> 11:56:37,940
what is producing all that data on the plane 

5
11:56:39,430 --> 11:56:42,180
if you look at some of the sensors that contribute

6
11:56:42,180 --> 11:56:45,600
to the half terabyte of data generated on a plane 

7
11:56:45,600 --> 11:56:51,040
we will find that some of it comes from accelerometers that measure turbulence 

8
11:56:52,350 --> 11:56:57,393
there are also sensors built into the engines for temperature pressure 

9
11:56:57,393 --> 11:57:01,666
many other measurable factors to detect engine malfunctions 

10
11:57:01,666 --> 11:57:06,487
constant real - time analysis of all the data collected provides help

11
11:57:06,487 --> 11:57:10,496
monitoring and problem detection at 40 000 feet 

12
11:57:10,496 --> 11:57:14,622
that approximately 12 000 meters above ground 

13
11:57:16,800 --> 11:57:21,210
we call this type of analytical processing in - situ 

14
11:57:21,210 --> 11:57:26,940
previously in traditional relational database management systems 

15
11:57:26,940 --> 11:57:31,160
data was often moved to computational space for processing 

16
11:57:31,160 --> 11:57:36,000
in big data space in - situ means bringing the computation

17
11:57:36,000 --> 11:57:39,865
to where data is located or in this case generated 

18
11:57:42,200 --> 11:57:46,936
a key feature of these types of real - time notifications is

19
11:57:46,936 --> 11:57:50,090
that they enable real - time actions 

20
11:57:50,090 --> 11:57:54,756
however using such a capability would require you to approach your

21
11:57:54,756 --> 11:57:57,580
application and your work differently 

22
11:57:59,000 --> 11:58:04,090
if you are using an activity tracker you should probably come up with a strategy

23
11:58:04,090 --> 11:58:08,540
for how you will incorporate the usage of these useful gadgets into your lifestyle 

24
11:58:09,780 --> 11:58:14,520
just like that if you are planning to incorporate big data driven insights into

25
11:58:14,520 --> 11:58:20,610
your organization you need to define a new strategy and a new way of working 

26
11:58:22,570 --> 11:58:28,184
most big data centric businesses have updated their culture to be more real - time

27
11:58:28,184 --> 11:58:33,160
action oriented refining real - time processes to handle anything from customer

28
11:58:33,160 --> 11:58:37,080
relations and fraud detection to system monitoring and control 

29
11:58:38,200 --> 11:58:43,150
in addition such volumes of real - time data and analytical operations that need

30
11:58:43,150 --> 11:58:48,880
to take place requires an increased use of scalable computing systems 

31
11:58:48,880 --> 11:58:52,870
which need to be a part of the planning for an organizational big data strategy 

32
11:58:54,700 --> 11:58:58,580
they see affects of such changes also in scada system 

33
11:58:59,920 --> 11:59:03,630
scada stands for supervisory control and data acquisition 

34
11:59:05,580 --> 11:59:10,710
scada is a type of industrial control system for remote monitoring and

35
11:59:10,710 --> 11:59:16,190
control of industrial processes that exists in the physical world 

36
11:59:16,190 --> 11:59:20,020
potentially including multiple sites many types of sensors 

37
11:59:22,080 --> 11:59:27,410
in addition to monitoring and control scada system can be used to define

38
11:59:27,410 --> 11:59:32,390
actions for reduced waste and improved efficiency in industrial processes 

39
11:59:32,390 --> 11:59:38,100
including those of manufacturing and power generation public or

40
11:59:38,100 --> 11:59:42,820
private infrastructure processes including water treatment oil and

41
11:59:42,820 --> 11:59:48,040
gas pipelines and electrical power transmission and

42
11:59:48,040 --> 11:59:53,300
facility processes including buildings airports ships and space stations 

43
11:59:54,550 --> 11:59:59,240
they can even be used in smart building applications to monitor and

44
11:59:59,240 --> 12:00:04,910
control heating ventilation air conditioning systems like hvac 

45
12:00:04,910 --> 12:00:07,030
access and energy consumption 

46
12:00:08,330 --> 12:00:13,040
again the management of these processes once the trends patterns and

47
12:00:13,040 --> 12:00:19,460
anomalies are identified in real - time needs to be decided in the big data case 

48
12:00:19,460 --> 12:00:25,960
as a summary as the largest and fastest type of big data machine generated

49
12:00:25,960 --> 12:00:30,690
data can uniquely enable real - time actions in many systems and processes 

50
12:00:31,740 --> 12:00:39,198
however a culture shift is needed for its computing and real - time action 

1
23:56:38,910 --> 23:56:40,630
big data generated by machines 

2
23:56:41,680 --> 23:56:43,508
it everywhere and there a lot 

3
23:56:54,258 --> 23:56:57,120
do big planes require big data 

4
23:56:57,120 --> 23:56:58,650
absolutely ! 

5
23:56:58,650 --> 23:57:02,770
did you know that a boeing 787 produces

6
23:57:02,770 --> 23:57:05,450
half a terabyte of data every time it flies 

7
23:57:06,640 --> 23:57:11,680
really almost every part of the plane updates both the flight and

8
23:57:11,680 --> 23:57:13,990
the ground team about its status constantly 

9
23:57:15,105 --> 23:57:16,530
where all this data coming from 

10
23:57:17,560 --> 23:57:21,360
this is an example of machine - generated data coming from sensors 

11
23:57:22,730 --> 23:57:25,730
if you look at all sources of big data 

12
23:57:25,730 --> 23:57:29,470
machine data is the largest source of big data 

13
23:57:29,470 --> 23:57:32,850
additionally it is very complex 

14
23:57:32,850 --> 23:57:38,960
in general we call machines that provide some type of sensing capability smart 

15
23:57:38,960 --> 23:57:42,760
have you ever wondered why you call your cell phone a smartphone 

16
23:57:43,890 --> 23:57:46,790
because it gives you a way to track many things 

17
23:57:46,790 --> 23:57:50,740
including your geolocation and connect you to other things 

18
23:57:51,930 --> 23:57:54,716
so what makes a smart device smart smart 

19
23:57:54,716 --> 23:58:00,110
generally speaking there are three main properties of smart devices

20
23:58:00,110 --> 23:58:05,200
based on what they do with sensors and things they encapsulate 

21
23:58:05,200 --> 23:58:07,480
they can connect to other devices or

22
23:58:07,480 --> 23:58:13,490
networks they can execute services and collect data autonomously 

23
23:58:13,490 --> 23:58:18,230
that means on their own they have some knowledge of the environment 

24
23:58:19,550 --> 23:58:24,940
the widespread availability of the smart devices and their interconnectivity

25
23:58:24,940 --> 23:58:31,080
led to a new term being coined the internet of things 

26
23:58:31,080 --> 23:58:36,450
think of a world of smart devices at home in your car in the office 

27
23:58:36,450 --> 23:58:42,110
city remote rural areas the sky even the ocean 

28
23:58:42,110 --> 23:58:45,340
all connected and all generating data 

29
23:58:46,360 --> 23:58:50,590
let look at an example of a device that has some of these things in it 

30
23:58:52,350 --> 23:58:57,930
an activity tracker is a device or application for monitoring and

31
23:58:57,930 --> 23:59:03,790
tracking fitness - related metrics such as distance walked or run 

32
23:59:03,790 --> 23:59:10,670
calorie consumption and in some cases heartbeat and quality of sleep 

33
23:59:10,670 --> 23:59:15,140
what if everyone in new york city wore an activity tracker 

34
23:59:15,140 --> 23:59:18,350
what if everyone wore several 

35
23:59:18,350 --> 23:59:23,300
i personally have three activity trackers that i use on a daily basis 

36
23:59:23,300 --> 23:59:28,200
one to track my sleep another one to track my physical activity and

37
23:59:28,200 --> 23:59:32,250
a third my smartphone that goes everywhere with me 

38
23:59:32,250 --> 23:59:38,030
so it is not that unusual to imagine that this will be the case for many people 

39
23:59:38,030 --> 23:59:43,320
as you have already heard from in a previous lecture on personalized data 

40
23:59:43,320 --> 23:59:46,760
such activity trackers have enabled a new way

41
23:59:46,760 --> 23:59:50,210
of doing patient intervention via personalized medicine 

42
23:59:51,670 --> 23:59:56,440
similarly the sensors in planes have generated a new way of looking at

43
23:59:56,440 --> 23:59:59,370
fleet management and flight safety 

44
23:59:59,370 --> 00:00:04,055
as a summary machines collect data 24 7 via

45
00:00:04,055 --> 00:00:09,980
their built - in sensors both at personal and industrial scales 

46
00:00:09,980 --> 00:00:13,308
and thus they are the largest of all the big data sources 

1
23:56:52,300 --> 23:56:53,963
organization - generated data 

2
23:56:53,963 --> 23:57:00,580
benefits come from combining with other data types 

3
23:57:00,580 --> 23:57:03,380
how are some organizations benefiting from big data 

4
23:57:20,278 --> 23:57:22,844
let look at real world examples to see the advantages these organizations

5
23:57:22,844 --> 23:57:23,800
are getting out of big data 

6
23:57:24,960 --> 23:57:27,120
one of these companies is ups 

7
23:57:28,720 --> 23:57:32,650
ups delivers 16 million shipments per day 

8
23:57:32,650 --> 23:57:37,360
they get around 40 million tracking requests 

9
23:57:37,360 --> 23:57:37,900
that huge 

10
23:57:39,270 --> 23:57:45,431
an estimate of how much data ups has on its operations is 16 petabytes 

11
23:57:46,880 --> 23:57:51,580
can you guess how much money ups can save by reducing each driver route

12
23:57:51,580 --> 23:57:52,620
by just one mile 

13
23:57:54,410 --> 23:58:00,113
if they can reduce distance traveled by each truck by even one mile 

14
23:58:00,113 --> 23:58:04,830
ups can save a whopping 50 million american per year 

15
23:58:06,490 --> 23:58:10,010
this is where big data steps in 

16
23:58:10,010 --> 23:58:13,610
utilizing complex optimization over large datasets

17
23:58:13,610 --> 23:58:18,250
can lead to route optimizations that were previously not visible to the company 

18
23:58:20,110 --> 23:58:23,900
big data together with smart processing 

19
23:58:23,900 --> 23:58:27,720
enables ups to manage thousands of route optimizations 

20
23:58:28,900 --> 23:58:34,310
let travel from package delivery to the retail domain 

21
23:58:34,310 --> 23:58:37,870
an organization from the retail shopping domain

22
23:58:37,870 --> 23:58:40,760
that heavily utilizes big data is walmart 

23
23:58:42,590 --> 23:58:49,220
walmart is a big organization that gets 250 million customers in 10 000 stores 

24
23:58:51,820 --> 23:58:58,570
did you know they collect 2 5 petabytes of data per hour 

25
23:59:00,280 --> 23:59:05,699
they collect data on twitter tweets local events local weather 

26
23:59:05,699 --> 23:59:10,640
in - store purchases online clicks and

27
23:59:10,640 --> 23:59:14,530
many other sales customer and product related data 

28
23:59:16,070 --> 23:59:22,080
they use this data to find patterns such as which products are frequently

29
23:59:22,080 --> 23:59:27,878
purchased together and what is the best new product to introduce in their stores 

30
23:59:27,878 --> 23:59:32,530
to predict demand at the particular location 

31
23:59:34,670 --> 23:59:38,898
and to customize customer recommendations 

32
23:59:38,898 --> 23:59:41,430
overall by leveraging big data and

33
23:59:41,430 --> 23:59:46,390
analytics walmart has maintained its position as a top retailer 

34
23:59:47,650 --> 23:59:53,130
ups and walmart examples were just two out of a number of companies using big data 

35
23:59:54,380 --> 23:59:58,760
big data is producing results for companies in all sectors 

36
00:00:00,670 --> 00:00:05,370
studies forecast spending on big data technologies to go up

37
00:00:05,370 --> 00:00:07,630
drastically in the next five years 

38
00:00:09,320 --> 00:00:16,280
a study by bane and company suggests that early adopters of big

39
00:00:16,280 --> 00:00:21,650
data analytics have gained a significant lead over the rest of the corporate world 

40
00:00:23,620 --> 00:00:25,430
for graphics referenced here 

41
00:00:26,450 --> 00:00:31,700
we see that companies that use analytics are twice as likely

42
00:00:31,700 --> 00:00:36,160
to be in the top quartile of financial performance within their industries 

43
00:00:37,330 --> 00:00:42,368
five times as likely to make decisions much faster than market peers 

44
00:00:43,440 --> 00:00:47,180
three times as likely to execute decisions as intended 

45
00:00:47,180 --> 00:00:53,880
and twice as likely to use data very frequently when making decisions 

46
00:00:55,040 --> 00:00:57,610
this points to the growth and demand of people and

47
00:00:57,610 --> 00:01:03,450
technology centered around or specializing in big data applications 

48
00:01:03,450 --> 00:01:08,870
as a summary organizations are gaining significant benefit from integrating

49
00:01:08,870 --> 00:01:13,920
big data practices into their culture and breaking their silos 

50
00:01:13,920 --> 00:01:19,080
some major benefits to organizations are operational efficiency improved marketing

51
00:01:19,080 --> 00:01:23,630
outcomes higher profits and improved customer satisfaction 

1
23:58:14,870 --> 23:58:20,650
big data generated by organizations structured but often siloed 

2
23:58:20,650 --> 23:58:25,930
the last type of big data we will discuss is big data generated by organizations 

3
23:58:27,260 --> 23:58:32,740
this type of data is the closest to what most businesses currently have 

4
23:58:32,740 --> 23:58:35,550
but it considered a bit out of fashion or

5
23:58:35,550 --> 23:58:39,180
traditional compared to other types of big data 

6
23:58:39,180 --> 23:58:43,760
however it is at least as important as other types of big data 

7
23:59:01,566 --> 23:59:05,350
so how do organizations produce data 

8
23:59:05,350 --> 23:59:08,940
the answer to how an organization generates data

9
23:59:08,940 --> 23:59:11,260
is very unique to the organization and context 

10
23:59:12,590 --> 23:59:16,540
each organization has distinct operation practices and

11
23:59:16,540 --> 23:59:21,860
business models which result in a variety of data generation platforms 

12
23:59:21,860 --> 23:59:26,697
for example the type and source of data that a bank gets is very different from

13
23:59:26,697 --> 23:59:29,650
what a hardware equipment manufacturer gets 

14
23:59:31,610 --> 23:59:37,808
some common types of organizational big data come from commercial transactions 

15
23:59:37,808 --> 23:59:43,033
credit cards government institutions e - commerce banking or

16
23:59:43,033 --> 23:59:49,070
stock records medical records sensors transactions clicks and so on 

17
23:59:50,350 --> 23:59:53,930
almost every event can be potentially stored 

18
23:59:55,500 --> 23:59:59,320
organizations store this data for current and

19
23:59:59,320 --> 00:00:03,040
future use as well as for analysis of the past 

20
00:00:04,380 --> 00:00:08,970
let say you are an organization that collects sales transactions 

21
00:00:08,970 --> 00:00:13,500
you can use this data for pattern recognition to detect correlated products 

22
00:00:14,530 --> 00:00:19,990
to estimate demand for products likely to go up in sales and

23
00:00:19,990 --> 00:00:21,780
capture fraudulent activity 

24
00:00:22,930 --> 00:00:28,560
moreover when you know your sales record and can correlate it

25
00:00:28,560 --> 00:00:34,370
with your marketing records you can find which campaigns really made an impact 

26
00:00:34,370 --> 00:00:38,340
you are already becoming a data savvy organization 

27
00:00:38,340 --> 00:00:43,180
now think of bringing your sales data together with other

28
00:00:43,180 --> 00:00:48,680
external open public data such as major world events in the news 

29
00:00:48,680 --> 00:00:52,420
you can ask was it savvy marketing or

30
00:00:52,420 --> 00:00:57,310
a consequence of external events that triggered your sales 

31
00:00:57,310 --> 00:00:59,037
using proper analytics 

32
00:00:59,037 --> 00:01:04,800
you can now build inventories to match your predicted growth and demand 

33
00:01:04,800 --> 00:01:11,060
in addition organizations build and apply processes to record and

34
00:01:11,060 --> 00:01:16,310
monitor business events of interest such as registering a customer 

35
00:01:16,310 --> 00:01:19,116
manufacturing a product or taking an order 

36
00:01:19,116 --> 00:01:24,930
these processes collect highly structured data

37
00:01:24,930 --> 00:01:28,460
that include transactions reference tables and

38
00:01:28,460 --> 00:01:33,810
relationships as well as the metadata that sets its context 

39
00:01:35,130 --> 00:01:40,580
usually structured data is stored in relational database management systems 

40
00:01:42,030 --> 00:01:45,805
however we call any data that is in the form of

41
00:01:45,805 --> 00:01:50,708
a record located in a fixed field or file structured data 

42
00:01:50,708 --> 00:01:54,300
this definition also includes spreadsheets 

43
00:01:55,750 --> 00:02:00,490
as i have mentioned before traditionally this type of

44
00:02:00,490 --> 00:02:05,070
highly structured data is the vast majority of what it managed and

45
00:02:05,070 --> 00:02:09,420
processed in both operational and business intelligence systems 

46
00:02:10,750 --> 00:02:14,410
let look at the sales transaction data in our previous example 

47
00:02:15,890 --> 00:02:19,575
if you look at the data in the relational table on the right 

48
00:02:19,575 --> 00:02:23,687
as the name structured suggests 

49
00:02:23,687 --> 00:02:28,649
the table is organized to store data using

50
00:02:28,649 --> 00:02:32,493
a structure defined by a model 

51
00:02:32,493 --> 00:02:38,450
each column is tagged to tell us what data that column intends to store 

52
00:02:38,450 --> 00:02:40,950
this is what we call a data model 

53
00:02:42,310 --> 00:02:45,680
a data model defines each of these columns and

54
00:02:45,680 --> 00:02:51,290
fields in the table and defines relationships between them 

55
00:02:51,290 --> 00:02:57,960
if you look at the product id column you will see that it includes only

56
00:02:57,960 --> 00:03:02,940
identifiers that can potentially be linked to another table defining these products 

57
00:03:04,350 --> 00:03:09,990
the ability to define such relationships easily make structured data 

58
00:03:09,990 --> 00:03:15,220
or in this case relational databases highly adopted by many organizations 

59
00:03:16,460 --> 00:03:20,330
there are commonly used languages like sql 

60
00:03:20,330 --> 00:03:25,600
the structured query language to extract data of interest from such tables 

61
00:03:26,690 --> 00:03:29,800
this is referred to as querying the data 

62
00:03:31,160 --> 00:03:37,060
however it could even be a challenge to integrate such structured data 

63
00:03:37,060 --> 00:03:42,460
this image shows us a continuum of technologies to model 

64
00:03:42,460 --> 00:03:46,880
collect and query unstructured data coming from software and

65
00:03:46,880 --> 00:03:49,180
hardware components within an organization 

66
00:03:50,400 --> 00:03:55,150
in the past such challenges led to information being stored

67
00:03:55,150 --> 00:03:59,050
in what we call silos even within an organization 

68
00:04:00,400 --> 00:04:05,700
many organizations have traditionally captured data at the department level 

69
00:04:05,700 --> 00:04:10,950
without proper infrastructure and policy to share and integrate this data 

70
00:04:10,950 --> 00:04:15,550
this has hindered the growth of scalable pattern recognition

71
00:04:15,550 --> 00:04:18,510
to the benefits of the entire organization 

72
00:04:18,510 --> 00:04:22,700
because no one system has access to all data that the organization owns 

73
00:04:24,480 --> 00:04:28,080
each data set is compartmentalized 

74
00:04:28,080 --> 00:04:34,320
if such silos are left untouched organizations risk having outdated 

75
00:04:34,320 --> 00:04:38,320
unsynchronized and even invisible data sets 

76
00:04:39,570 --> 00:04:43,260
organizations are realizing the detrimental outcomes of this rigid

77
00:04:43,260 --> 00:04:44,820
structure 

78
00:04:44,820 --> 00:04:50,270
and changing policies and infrastructure to enable integrated processing of

79
00:04:50,270 --> 00:04:53,025
all data to the entire organization benefit 

80
00:04:53,025 --> 00:04:58,090
cloud - based solutions are seen as agile and

81
00:04:58,090 --> 00:05:01,550
low capital intensive solutions in this area 

82
00:05:01,550 --> 00:05:07,309
as a summary while highly structured organizational data is very useful and

83
00:05:07,309 --> 00:05:11,671
trustworthy and thus a valuable source of information 

84
00:05:11,671 --> 00:05:15,860
organizations must pay special attention to breaking up

85
00:05:15,860 --> 00:05:20,243
the silos of information to make full use of its potential 

1
00:03:34,660 --> 00:03:37,542
the key integrating diverse data 

2
00:03:47,178 --> 00:03:50,267
whatever your big data application is and

3
00:03:50,267 --> 00:03:55,527
the types of big data you are using the real value will come from integrating

4
00:03:55,527 --> 00:04:00,056
different types of data sources and analyzing them at scale 

5
00:04:00,056 --> 00:04:02,947
so how do we start getting this value 

6
00:04:02,947 --> 00:04:04,706
sometimes all it takes 

7
00:04:04,706 --> 00:04:09,620
is looking at the data you already collect in a different way 

8
00:04:09,620 --> 00:04:13,060
and it can mean a big difference in your return on investment 

9
00:04:14,590 --> 00:04:19,970
this new story from june 2015 mentions that carnival cruises

10
00:04:19,970 --> 00:04:24,760
is using structured and unstructured data from a variety of sources 

11
00:04:26,420 --> 00:04:29,200
carnival turns it into profit

12
00:04:29,200 --> 00:04:32,640
using price optimization techniques on the integrated data 

13
00:04:33,640 --> 00:04:36,460
for you to achieve such a success story 

14
00:04:36,460 --> 00:04:41,030
you will need to include data integration into your big data practice 

15
00:04:41,030 --> 00:04:45,950
however there are some unique challenges when attempting to integrate these

16
00:04:45,950 --> 00:04:50,120
diverse data sources and scaling the solutions 

17
00:04:50,120 --> 00:04:54,220
course two on on the specialization we will teach you more about these issues 

18
00:04:55,260 --> 00:05:00,010
but let take a moment to define why effect of data integration is useful 

19
00:05:01,060 --> 00:05:07,040
data integration means bringing together data from diverse sources and

20
00:05:07,040 --> 00:05:11,170
turning them into coherent and more useful information 

21
00:05:12,440 --> 00:05:14,390
we also call this knowledge 

22
00:05:15,740 --> 00:05:21,970
the main objective here is taming or more technically managing data and

23
00:05:21,970 --> 00:05:26,520
turning it into something you can make use of programmatically 

24
00:05:26,520 --> 00:05:31,460
a data integration process involves many parts 

25
00:05:31,460 --> 00:05:37,250
it starts with discovering accessing and monitoring data and

26
00:05:37,250 --> 00:05:43,340
continues with modeling and transforming data from a variety of sources 

27
00:05:43,340 --> 00:05:47,160
but why do we need data integration in the first place 

28
00:05:47,160 --> 00:05:50,900
let start by focusing on differences between big data sets

29
00:05:50,900 --> 00:05:52,140
coming from different sources 

30
00:05:53,500 --> 00:05:58,640
you might have flat file formatted data relational database data 

31
00:05:58,640 --> 00:06:05,470
data encoded in xml or json both common for internet generated data 

32
00:06:05,470 --> 00:06:09,170
these different formats and models are useful

33
00:06:09,170 --> 00:06:12,690
because they are designed to express different data in unique ways 

34
00:06:13,880 --> 00:06:16,560
in a way different data formats and

35
00:06:16,560 --> 00:06:23,100
models make big data more useful and more challenging all at the same time 

36
00:06:23,100 --> 00:06:27,700
when you integrate data in different formats you make the final

37
00:06:27,700 --> 00:06:32,030
product richer in the number of features you describe the data with 

38
00:06:32,030 --> 00:06:37,300
for example by integrating environmental sensor and

39
00:06:37,300 --> 00:06:42,300
camera data with geographical information system data such as in

40
00:06:42,300 --> 00:06:48,178
my wildfire prediction application i can use the spacial data capabilities

41
00:06:48,178 --> 00:06:53,970
with non - spacial data to more accurately run fire simulations 

42
00:06:55,230 --> 00:06:59,920
in the past although we were able to see the images of the fire

43
00:07:01,280 --> 00:07:05,630
from mountain top cameras just like this image we were still

44
00:07:05,630 --> 00:07:09,880
not able to tell what the exact location of the fire is automatically 

45
00:07:11,460 --> 00:07:15,000
now when a fire is detected from a mountain top camera 

46
00:07:16,180 --> 00:07:21,640
viewsheds are used to estimate the location of the fire 

47
00:07:23,490 --> 00:07:28,760
this location information can be fed into the fire simulator

48
00:07:28,760 --> 00:07:32,070
as early as it detected to predict the size and

49
00:07:32,070 --> 00:07:36,090
location of the fire in the next hour more accurately and faster 

50
00:07:37,730 --> 00:07:42,920
similarly i can use real time data with eye curve data sets and

51
00:07:42,920 --> 00:07:43,940
use them all together 

52
00:07:44,980 --> 00:07:49,520
additionally by bringing the data together and

53
00:07:49,520 --> 00:07:56,100
providing programmable access to it i am now making each data set more accessible 

54
00:07:57,200 --> 00:08:02,020
moreover integration of diverse datasets significantly

55
00:08:02,020 --> 00:08:06,220
reduces the overall data complexity in my data - driven product 

56
00:08:07,320 --> 00:08:13,610
the data becomes more available for use and unified as a system of its own 

57
00:08:14,860 --> 00:08:19,010
one advantage of such an integration is not often mentioned 

58
00:08:20,500 --> 00:08:23,830
such a streamlined and integrated data system

59
00:08:23,830 --> 00:08:27,550
can increase the collaboration between different parts of your data systems 

60
00:08:28,930 --> 00:08:32,430
each part can now clearly see

61
00:08:32,430 --> 00:08:36,000
how their data is integrated into the overall system 

62
00:08:36,000 --> 00:08:41,860
including the user scenarios and the security and privacy processes around it 

63
00:08:43,080 --> 00:08:50,320
overall by integrating diverse data streams you add value to your big data and

64
00:08:50,320 --> 00:08:54,320
improve your business even before you start analyzing it 

65
00:08:55,640 --> 00:08:59,783
next we will focus on the dimensions of the scalability and

66
00:08:59,783 --> 00:09:03,937
discuss how we can start tackling some of these challenges 

1
00:12:38,310 --> 00:12:40,380
what launched the big data era 

2
00:12:41,730 --> 00:12:46,092
in this video you will learn about two new opportunities that have

3
00:12:46,092 --> 00:12:49,088
contributed to the launch of the big data era 

4
00:13:04,737 --> 00:13:09,070
opportunities are often a signal of changing times 

5
00:13:11,110 --> 00:13:17,100
in 2013 an influential report by a company called mckinsey claimed that

6
00:13:17,100 --> 00:13:22,900
the area of data science will be the number one catalyst for economic growth 

7
00:13:24,380 --> 00:13:27,320
mckinsey identified one of our new

8
00:13:27,320 --> 00:13:31,366
opportunities that contributed to the launch of the big data era 

9
00:13:31,366 --> 00:13:36,190
a growing torrent of data 

10
00:13:36,190 --> 00:13:42,150
this refers to the idea that data seems to be coming continuously and at a fast rate 

11
00:13:43,340 --> 00:13:48,330
think about this today you can buy a hard drive to store

12
00:13:48,330 --> 00:13:52,624
all the music in the world for only 600 

13
00:13:53,710 --> 00:14:01,600
that an amazing storage capability over any previous forms of music storage 

14
00:14:01,600 --> 00:14:06,286
in 2010 there were 5 billion mobile phones in use 

15
00:14:06,286 --> 00:14:10,018
you can be sure that there are more today and

16
00:14:10,018 --> 00:14:14,476
as i am sure you will understand these phones and

17
00:14:14,476 --> 00:14:19,244
the apps we install on them are a big source of big data 

18
00:14:19,244 --> 00:14:24,342
which all the time every day contributes to our core 

19
00:14:24,342 --> 00:14:27,444
and facebook which recently just set

20
00:14:27,444 --> 00:14:32,144
a record of having one billion people login in a single day 

21
00:14:32,144 --> 00:14:37,320
has more that 30 billion pieces of content shared every month 

22
00:14:38,430 --> 00:14:40,620
well that number from 2013 

23
00:14:40,620 --> 00:14:44,310
so i am sure that it much higher than that now 

24
00:14:45,720 --> 00:14:48,950
does it make you think how many facebook share you made last month 

25
00:14:50,760 --> 00:14:54,105
all this leads to projections of serious growth 

26
00:14:54,105 --> 00:15:01,860
40 in global data per year and 5 in global it spending 

27
00:15:01,860 --> 00:15:06,080
this much data has sure pushed the data science field

28
00:15:06,080 --> 00:15:09,420
to start remaining itself and the business world of today 

29
00:15:10,450 --> 00:15:15,970
but there something else contributing to the catalyzing power of data science 

30
00:15:15,970 --> 00:15:18,820
it is called cloud computing 

31
00:15:18,820 --> 00:15:21,600
we call this on demand computing 

32
00:15:21,600 --> 00:15:25,480
cloud computing is one of the ways in which computing

33
00:15:25,480 --> 00:15:30,220
has now become something that we ca do anytime and anywhere 

34
00:15:30,220 --> 00:15:33,170
you may be surprised to know that some of your favorite

35
00:15:33,170 --> 00:15:36,900
apps are from businesses being run from coffee shops 

36
00:15:36,900 --> 00:15:41,420
this new ability combined with our torrent of data 

37
00:15:41,420 --> 00:15:47,220
gives us the opportunity to perform novel dynamic and scalable data analysis 

38
00:15:47,220 --> 00:15:50,824
to tell us new things about our world and ourself 

39
00:15:50,824 --> 00:15:57,037
to summarize a new torrent of big data combined with computing capability

40
00:15:57,037 --> 00:16:02,968
anytime anywhere has been at the core of the launch of the big data era 

1
00:28:40,080 --> 00:28:41,360
asking the right questions 

2
00:28:56,893 --> 00:29:02,810
the first step in any process is to define what it is you are trying to tackle 

3
00:29:04,030 --> 00:29:06,975
what is the problem that needs to be addressed or

4
00:29:06,975 --> 00:29:09,240
the opportunity that needs to be ascertained 

5
00:29:10,280 --> 00:29:14,223
without this you wo not have a clear goal in mind or

6
00:29:14,223 --> 00:29:17,153
know when you have solved your problem 

7
00:29:17,153 --> 00:29:21,833
an example question is how can sales figures and

8
00:29:21,833 --> 00:29:27,072
call center logs be combined to evaluate a new product 

9
00:29:27,072 --> 00:29:32,534
or in a manufacturing process how can data from multiple

10
00:29:32,534 --> 00:29:38,573
sensors in an instrument be used to detect instrument failure 

11
00:29:38,573 --> 00:29:41,745
how can we understand our customers and

12
00:29:41,745 --> 00:29:46,273
market better to achieve effective target marketing 

13
00:29:46,273 --> 00:29:50,320
next you need to assess the situation with respect to the problem or

14
00:29:50,320 --> 00:29:53,730
the opportunity you have defined 

15
00:29:53,730 --> 00:29:59,011
this is a step where you need to exercise caution analyzing risks 

16
00:29:59,011 --> 00:30:02,609
costs benefits contingencies 

17
00:30:02,609 --> 00:30:07,650
regulations resources and requirements of the situation 

18
00:30:07,650 --> 00:30:09,910
what are the requirements of the problem 

19
00:30:09,910 --> 00:30:12,850
what are the assumptions and constraints 

20
00:30:12,850 --> 00:30:14,450
what resources are available 

21
00:30:14,450 --> 00:30:18,614
this is in terms of both personnel and capital 

22
00:30:18,614 --> 00:30:22,685
such as computer systems instruments etc 

23
00:30:22,685 --> 00:30:26,425
what are the main costs associated with this project 

24
00:30:26,425 --> 00:30:28,335
what are the potential benefits 

25
00:30:28,335 --> 00:30:31,625
what risks are there in pursuing the project 

26
00:30:31,625 --> 00:30:36,125
what are the contingencies to potential risks and so on 

27
00:30:36,125 --> 00:30:41,570
answers to these questions will help you get a better overview of the situation 

28
00:30:41,570 --> 00:30:44,700
and better understanding of what the project involves 

29
00:30:44,700 --> 00:30:49,172
then you need to define your goals and objectives 

30
00:30:49,172 --> 00:30:52,813
based on the answers to these questions 

31
00:30:52,813 --> 00:30:56,850
defining success criteria is also very important 

32
00:30:56,850 --> 00:30:59,770
what do you hope to achieve by the end of this project 

33
00:30:59,770 --> 00:31:01,368
having clear goals and and

34
00:31:01,368 --> 00:31:07,280
success criteria will help you to assess the project throughout its life cycle 

35
00:31:07,280 --> 00:31:11,780
once you know the problem you want to address and understand the constraints and

36
00:31:11,780 --> 00:31:16,990
goals then you can formulate the plan to come up with the answer 

37
00:31:16,990 --> 00:31:20,330
that is the solution to your business problem 

38
00:31:20,330 --> 00:31:25,920
as a summary defining the questions you are looking to find answers for

39
00:31:25,920 --> 00:31:30,250
is a huge factor contributing to the success of a data science project 

40
00:31:31,260 --> 00:31:35,830
by following the explained set of steps you can formulate better questions

41
00:31:35,830 --> 00:31:40,500
to solve using analytical skills and link them to business value 

1
01:00:22,028 --> 01:00:23,788
building a big data strategy 

2
01:00:42,698 --> 01:00:48,760
before we focus on big data strategy let look at what strategy means 

3
01:00:49,790 --> 01:00:52,090
although it is associated with a military term 

4
01:00:53,600 --> 01:00:58,490
a dictionary search on strategy shows the meaning as a plan of action or

5
01:00:58,490 --> 01:01:02,820
policy designed to achieve a major or overall aim 

6
01:01:05,200 --> 01:01:10,990
this definition calls out the four major parts that need to be in any strategy 

7
01:01:10,990 --> 01:01:17,110
namely aim policy plan and action 

8
01:01:18,280 --> 01:01:21,620
now we are talking about a big data strategy 

9
01:01:21,620 --> 01:01:24,770
so what do these four terms mean for us 

10
01:01:24,770 --> 01:01:29,382
when building our big data strategy we look at what we have 

11
01:01:29,382 --> 01:01:34,793
what high level goals we want to achieve what we need to do to get there 

12
01:01:34,793 --> 01:01:39,878
and what are the policies around data from the beginning to the end 

13
01:01:43,568 --> 01:01:48,210
a big data strategy starts with big objectives 

14
01:01:48,210 --> 01:01:52,939
notice that i did not say it starts with collecting data

15
01:01:52,939 --> 01:01:58,171
because in this activity we are really trying to identify what

16
01:01:58,171 --> 01:02:03,118
data is useful and why by focusing on what data to collect 

17
01:02:03,118 --> 01:02:07,410
every organization or team is unique 

18
01:02:07,410 --> 01:02:10,120
different projects have different objectives 

19
01:02:10,120 --> 01:02:14,960
hence it important to first define what your team goals are 

20
01:02:14,960 --> 01:02:19,570
have you ever had the scenario where you see the temperature on the weather report

21
01:02:19,570 --> 01:02:22,350
and someone else highlights the humidity instead 

22
01:02:23,500 --> 01:02:26,820
to find problems relevant to solve and

23
01:02:26,820 --> 01:02:32,290
data related to it it might be useful to start with your objectives 

24
01:02:32,290 --> 01:02:34,500
once you define these objectives or

25
01:02:34,500 --> 01:02:40,030
more generally speaking questions to turn big data into advantage for your business 

26
01:02:40,030 --> 01:02:44,450
you can look at what you have and analyze the gaps and actions to get there 

27
01:02:45,940 --> 01:02:49,192
it is important to focus on both short term and

28
01:02:49,192 --> 01:02:52,108
long term objectives in this activity 

29
01:02:52,108 --> 01:02:56,791
these objectives should also be linked to big data analytics with business

30
01:02:56,791 --> 01:02:57,720
objectives 

31
01:02:57,720 --> 01:03:02,620
to make the best use of big data each company needs to evaluate how

32
01:03:02,620 --> 01:03:07,220
data science or big data analytics would add value to their business objectives 

33
01:03:09,010 --> 01:03:12,730
once you have established that analytics can help your business 

34
01:03:12,730 --> 01:03:16,550
you need to create a culture to embrace it 

35
01:03:16,550 --> 01:03:18,820
the first and foremost ingredient for

36
01:03:18,820 --> 01:03:23,790
a successful data science program is organizational buy - in 

37
01:03:23,790 --> 01:03:26,810
a big data strategy must have commitment and

38
01:03:26,810 --> 01:03:30,010
sponsorship from the company leadership 

39
01:03:30,010 --> 01:03:35,930
goals for using big data analytics should be developed with all stakeholders and

40
01:03:35,930 --> 01:03:39,640
clearly communicated to everyone in the organization 

41
01:03:39,640 --> 01:03:43,740
so that its value is understood and appreciated by all 

42
01:03:46,350 --> 01:03:49,790
the next step is to build your data science team 

43
01:03:51,160 --> 01:03:55,930
a diverse team with data scientists information technologists 

44
01:03:55,930 --> 01:04:01,520
application developers and business owners is necessary to be effective 

45
01:04:01,520 --> 01:04:05,830
as well as the mentality that everyone works together as partners with

46
01:04:05,830 --> 01:04:06,950
common goals 

47
01:04:06,950 --> 01:04:07,970
remember one for all 

48
01:04:09,690 --> 01:04:13,960
no one is a customer or service provider of another 

49
01:04:13,960 --> 01:04:18,360
rather everyone works together and delivers as a team 

50
01:04:21,470 --> 01:04:24,930
since big data is a team game and multi - disciplinary 

51
01:04:24,930 --> 01:04:29,180
a big part of a big data strategy is constant training

52
01:04:29,180 --> 01:04:32,400
of team members on new big data tools and analytics 

53
01:04:32,400 --> 01:04:36,050
as well as business practices and objectives 

54
01:04:36,050 --> 01:04:41,095
this becomes even more critical if your business depends on deep expertise

55
01:04:41,095 --> 01:04:46,381
on one or more subject areas with subject matter experts working on problems 

56
01:04:46,381 --> 01:04:47,838
utilizing big data 

57
01:04:50,947 --> 01:04:56,138
such businesses might have subject matter experts who can be trained to add big

58
01:04:56,138 --> 01:05:01,730
data skills and provide more value added support than a newcomer would have 

59
01:05:01,730 --> 01:05:06,024
similarly any project member would be trained to understand what

60
01:05:06,024 --> 01:05:09,564
the business objectives and products are and how he or

61
01:05:09,564 --> 01:05:14,478
she can utilize big data to improve those objectives using his or her skills 

62
01:05:17,218 --> 01:05:23,186
many organizations might benefit by having a small data science team whose main job

63
01:05:23,186 --> 01:05:29,000
is do data experiments and test new ideas before they get deployed at full scale 

64
01:05:31,740 --> 01:05:34,200
they might come up with a new idea themselves

65
01:05:34,200 --> 01:05:35,990
based on the analysis they perform 

66
01:05:37,190 --> 01:05:39,220
they take more research level role 

67
01:05:40,240 --> 01:05:45,010
however their findings can drastically shape your business strategy

68
01:05:45,010 --> 01:05:46,540
almost on a daily basis 

69
01:05:48,060 --> 01:05:51,620
the impact of such teams becomes evident over time

70
01:05:51,620 --> 01:05:55,950
as other parts of your organization starts to see the results of their finding and

71
01:05:55,950 --> 01:05:57,790
analysis affecting their strategies 

72
01:05:59,110 --> 01:06:04,550
they become strategic partners of all verticals in your business 

73
01:06:04,550 --> 01:06:06,820
once you see that something works 

74
01:06:06,820 --> 01:06:11,210
you can start collecting more data to see similar results at organizational scale 

75
01:06:14,270 --> 01:06:19,330
since data is key to any big data initiative it is essential

76
01:06:19,330 --> 01:06:23,870
that data across the organization is easily accessed and integrated 

77
01:06:25,280 --> 01:06:29,700
data silos as you know are like a death knell on effective analytics 

78
01:06:31,820 --> 01:06:35,470
so barriers to data access must be removed 

79
01:06:35,470 --> 01:06:40,430
opening up the silos must be encouraged and supported from the organization 

80
01:06:40,430 --> 01:06:45,020
leaders in order to promote a data sharing mindset for the company 

81
01:06:47,360 --> 01:06:50,900
another aspect of defining your big data strategy

82
01:06:50,900 --> 01:06:53,700
is defining the policies around big data 

83
01:06:54,810 --> 01:06:59,190
although it has an amazing amount of potential for your business 

84
01:06:59,190 --> 01:07:03,540
using big data should also raise some concerns in long term planning for data 

85
01:07:04,870 --> 01:07:08,270
although this is a very complex issue 

86
01:07:08,270 --> 01:07:12,230
here are some questions you should think of addressing around policy 

87
01:07:12,230 --> 01:07:14,350
what are the privacy concerns 

88
01:07:14,350 --> 01:07:17,770
who should have access to or control data 

89
01:07:17,770 --> 01:07:23,190
what is the lifetime of data which is sometimes defined as volatility 

90
01:07:23,190 --> 01:07:24,290
anatomy of big data 

91
01:07:25,970 --> 01:07:28,890
how does data get curated and cleaned up 

92
01:07:30,780 --> 01:07:33,320
what ensures data quality in the long term 

93
01:07:34,570 --> 01:07:38,530
how do different parts of your organization communicate or

94
01:07:38,530 --> 01:07:40,400
interoperate using this data 

95
01:07:41,440 --> 01:07:45,070
are there any legal and regulatory standards in place 

96
01:07:47,295 --> 01:07:51,270
cultivating an analytics driven culture is crucial

97
01:07:51,270 --> 01:07:53,600
to the success of a big data strategy 

98
01:07:55,090 --> 01:07:58,690
the mindset that you want to establish is that analytics

99
01:07:58,690 --> 01:08:03,200
is an integral part of doing business not a separate afterthought 

100
01:08:04,550 --> 01:08:09,170
analytics activities must be tied to your business objectives and

101
01:08:09,170 --> 01:08:12,480
you must be willing to use analytics in driving business decisions 

102
01:08:13,790 --> 01:08:18,598
analytics and business together bring about exciting opportunities and

103
01:08:18,598 --> 01:08:20,750
growth to your big data strategy 

104
01:08:23,980 --> 01:08:27,570
finally one size does not fit all 

105
01:08:27,570 --> 01:08:29,780
hence big data technologies and

106
01:08:29,780 --> 01:08:34,490
analytics is growing rapidly as your business is an evolving entity 

107
01:08:35,740 --> 01:08:40,180
you have to iterate your strategy to take advantage of new advances and

108
01:08:40,180 --> 01:08:43,310
also make your business more dynamic in the face of change 

109
01:08:46,300 --> 01:08:50,380
as a summary when building a big data strategy 

110
01:08:50,380 --> 01:08:54,130
it is important to integrate big data analytics with business objectives 

111
01:08:55,590 --> 01:08:59,450
communicate goals and provide organizational buy - in for

112
01:08:59,450 --> 01:09:01,420
analytics projects 

113
01:09:01,420 --> 01:09:05,990
build teams with diverse talents and establish a teamwork mindset 

114
01:09:07,000 --> 01:09:11,370
remove barriers to data access and integration 

115
01:09:11,370 --> 01:09:16,160
finally these activities need to be iterated to respond to new business

116
01:09:16,160 --> 01:09:18,568
goals and technological advances 

1
02:09:38,150 --> 02:09:42,590
in this video we will talk about a new that is usually not covered much 

2
02:09:43,820 --> 02:09:44,510
it called valence 

3
02:09:52,960 --> 02:09:56,440
simply put valence refers to connectedness 

4
02:09:57,440 --> 02:10:00,660
the more connected data is the higher it valences 

5
02:10:01,830 --> 02:10:04,000
the term valence comes from chemistry 

6
02:10:05,030 --> 02:10:09,800
in chemistry we talk about core electrons and valence electrons of an atom 

7
02:10:10,910 --> 02:10:15,810
valence electrons are in the outer most shell have the highest energy level and

8
02:10:15,810 --> 02:10:18,960
are responsible for bonding with other atoms 

9
02:10:18,960 --> 02:10:23,560
that higher valence results in greater boding that is greater connectedness 

10
02:10:24,740 --> 02:10:28,670
this idea is carried over into our definition of the term valence

11
02:10:28,670 --> 02:10:30,180
in the context of big data 

12
02:10:32,010 --> 02:10:35,750
data items are often directly connected to one another 

13
02:10:35,750 --> 02:10:38,490
a city is connected to the country it belongs to 

14
02:10:39,530 --> 02:10:43,030
two facebook users are connected because they are friends 

15
02:10:43,030 --> 02:10:45,340
an employee is connected to his work place 

16
02:10:46,640 --> 02:10:48,470
data could also be indirectly connected 

17
02:10:49,710 --> 02:10:53,150
two scientists are connected because they are both physicists 

18
02:10:54,530 --> 02:11:00,520
for a data collection valence measures the ratio of actually connected data items

19
02:11:00,520 --> 02:11:04,390
to the possible number of connections that could occur within the collection 

20
02:11:05,460 --> 02:11:08,100
the most important aspect of valence

21
02:11:08,100 --> 02:11:10,870
is that the data connectivity increases over time 

22
02:11:11,910 --> 02:11:16,620
the series of network graphs comes from a social experiment where scientists

23
02:11:16,620 --> 02:11:21,230
attending a conference were asked to meet other scientists they did not know before 

24
02:11:21,230 --> 02:11:22,830
after several rounds of meetings 

25
02:11:22,830 --> 02:11:26,840
they found new connections shown by their red edges 

26
02:11:26,840 --> 02:11:32,500
increase in valence can lead to emergent group behavior in people networks 

27
02:11:32,500 --> 02:11:37,310
like creation of new groups and coalitions that have shared values and goals 

28
02:11:39,760 --> 02:11:41,770
a high valence data set is denser 

29
02:11:43,060 --> 02:11:46,670
this makes many regular analytic critiques very inefficient 

30
02:11:47,930 --> 02:11:51,350
more complex analytical methods must be adopted to account for

31
02:11:51,350 --> 02:11:52,490
the increasing density 

32
02:11:53,780 --> 02:11:57,760
more interesting challenges arise due to the dynamic behavior of the data 

33
02:11:59,030 --> 02:12:01,170
now there is a need to model and

34
02:12:01,170 --> 02:12:05,480
predict how valence of a connected data set may change with time and volume 

35
02:12:06,890 --> 02:12:11,660
the dynamic behavior also leads to the problem of event detection 

36
02:12:11,660 --> 02:12:15,510
such as bursts in the local cohesion in parts of the data 

37
02:12:15,510 --> 02:12:17,760
and emergent behavior in the whole data set 

38
02:12:17,760 --> 02:12:20,570
such as increased polarization in a community 

1
04:21:58,450 --> 04:22:01,620
now we will talk about a form of scalability called variety 

2
04:22:03,040 --> 04:22:07,500
in this case scale does not refer to the largeness of data 

3
04:22:07,500 --> 04:22:09,398
it refers to increased diversity 

4
04:22:18,985 --> 04:22:21,450
here is an important mantra you need to think about 

5
04:22:22,660 --> 04:22:27,530
when we as data scientists think of data variety we think of the additional

6
04:22:27,530 --> 04:22:32,450
complexity that results from more kinds of data that we need to store process 

7
04:22:32,450 --> 04:22:34,010
and combine 

8
04:22:34,010 --> 04:22:36,970
now many years ago when i started studying data management 

9
04:22:37,980 --> 04:22:40,030
we always thought of data as tables 

10
04:22:41,310 --> 04:22:47,060
these tables could be in spreadsheets or databases or just files but somehow

11
04:22:47,060 --> 04:22:50,290
they will be modeled and manipulated as rows and columns of of tables 

12
04:22:51,610 --> 04:22:57,620
now tables are still really important and dominant however today a much wider

13
04:22:57,620 --> 04:23:01,800
variety of data are collected stored and analyzed to solve real world problems 

14
04:23:02,990 --> 04:23:07,610
image data text data network data geographic maps computer

15
04:23:07,610 --> 04:23:12,390
generated simulations are only a few of the types of data we encounter everyday 

16
04:23:13,460 --> 04:23:18,010
the heterogeneity of data can be characterized along several dimensions 

17
04:23:18,010 --> 04:23:19,990
we mentioned four such axes here 

18
04:23:21,910 --> 04:23:24,630
structural variety refers to the difference in

19
04:23:24,630 --> 04:23:26,870
the representation of the data 

20
04:23:26,870 --> 04:23:30,600
for example an ekg signal is very different from a newspaper article 

21
04:23:31,690 --> 04:23:36,630
a satellite image of wildfires from nasa is very different from tweets

22
04:23:36,630 --> 04:23:39,110
sent out by people who are seeing the fire spread 

23
04:23:40,230 --> 04:23:45,100
media variety refers to the medium in which the data gets delivered 

24
04:23:45,100 --> 04:23:49,150
the audio of a speech versus the transcript of the speech

25
04:23:49,150 --> 04:23:52,300
may represent the same information in two different media 

26
04:23:53,460 --> 04:23:56,872
data objects like news video may have multiple media 

27
04:23:56,872 --> 04:24:00,436
an image sequence an audio and closed captioned text 

28
04:24:00,436 --> 04:24:02,730
all time synchronized to each other 

29
04:24:03,850 --> 04:24:08,160
semantic variety is best described two examples 

30
04:24:08,160 --> 04:24:12,480
we often use different units for quantities we measure 

31
04:24:12,480 --> 04:24:16,870
sometimes we also use qualitative versus quantitative measures 

32
04:24:16,870 --> 04:24:20,230
for example age can be a number or

33
04:24:20,230 --> 04:24:23,440
we represent it by terms like infant juvenile or adult 

34
04:24:25,140 --> 04:24:28,030
another kind of semantic variety comes from different

35
04:24:28,030 --> 04:24:30,450
assumptions of conditions on the data 

36
04:24:30,450 --> 04:24:36,510
for example if we conduct two income surveys on two different groups of people 

37
04:24:36,510 --> 04:24:37,970
we may not be able to compare or

38
04:24:37,970 --> 04:24:41,020
combine them without knowing more about the populations themselves 

39
04:24:42,300 --> 04:24:44,620
the variation and availability takes many forms 

40
04:24:45,730 --> 04:24:49,510
for one data can be available real time 

41
04:24:49,510 --> 04:24:53,110
like sensor data or it can be stored like patient records 

42
04:24:54,350 --> 04:24:57,430
similarly data can be accessible continuously for

43
04:24:57,430 --> 04:24:58,630
example from a traffic cam 

44
04:24:59,640 --> 04:25:01,490
versus intermittently for

45
04:25:01,490 --> 04:25:04,750
example only when the satellite is over the region of interest 

46
04:25:05,780 --> 04:25:09,650
this makes a difference between what operations one can do with data 

47
04:25:09,650 --> 04:25:12,020
especially if the volume of the data is large 

48
04:25:13,260 --> 04:25:16,700
we will cover this in more detail in course two

49
04:25:16,700 --> 04:25:21,049
when we explore the different genres of data and how we model them 

50
04:25:22,620 --> 04:25:25,470
we should not think that a single data object or

51
04:25:25,470 --> 04:25:29,880
a collection of similar data objects will be all uniform in themselves 

52
04:25:29,880 --> 04:25:32,040
emails for example is a hybrid entity 

53
04:25:33,480 --> 04:25:37,110
some of this information can be a table like shown here 

54
04:25:38,310 --> 04:25:40,940
now the body of the email usually has text in it 

55
04:25:42,040 --> 04:25:45,360
however some of the text may have ornaments around them 

56
04:25:45,360 --> 04:25:46,370
for example 

57
04:25:46,370 --> 04:25:51,320
the part highlighted in yellow represents something called a markup on text 

58
04:25:52,420 --> 04:25:55,600
we will get to markups later in the course 

59
04:25:55,600 --> 04:25:57,500
emails contain attachments 

60
04:25:57,500 --> 04:25:59,920
these are files or embedded images 

61
04:25:59,920 --> 04:26:03,140
or other multimedia objects that the mailer allows 

62
04:26:03,140 --> 04:26:07,150
this screenshot from my outlook shows the image of a scanned image

63
04:26:07,150 --> 04:26:08,470
of a handwritten note 

64
04:26:09,540 --> 04:26:13,230
when you take a collection of all emails from your mailbox or

65
04:26:13,230 --> 04:26:17,360
that from an organization you will see that senders and

66
04:26:17,360 --> 04:26:19,490
receivers form a communication network 

67
04:26:21,130 --> 04:26:25,917
in 2001 there was a famous scandal around a company called enron that engaged in

68
04:26:25,917 --> 04:26:29,390
fraudulent financial reporting practices 

69
04:26:29,390 --> 04:26:34,450
their email network partly shown here has been studied by data scientist to find

70
04:26:34,450 --> 04:26:39,330
usual and unusual patterns of connections among the people in the organization 

71
04:26:40,810 --> 04:26:44,080
an email collection can also have it own semantics 

72
04:26:44,080 --> 04:26:48,340
for example an email cannot refer to that means cannot copy or

73
04:26:48,340 --> 04:26:49,640
forward a previous email 

74
04:26:50,920 --> 04:26:55,170
finally an email server is a real - time data source 

75
04:26:55,170 --> 04:26:57,160
but an email repository is not 

76
04:26:58,190 --> 04:27:02,610
does email and email collections demonstrate significant

77
04:27:02,610 --> 04:27:07,270
internal variation in structure media semantics and availability 

1
08:49:04,090 --> 08:49:07,478
characteristics of big data - velocity 

2
08:49:23,075 --> 08:49:27,612
velocity refers to the increasing speed at which big data is created and

3
08:49:27,612 --> 08:49:32,160
the increasing speed at which the data needs to be stored and analyzed 

4
08:49:33,250 --> 08:49:38,310
processing of data in real - time to match its production rate as it gets generated

5
08:49:38,310 --> 08:49:41,810
is a particular goal of big data analytics 

6
08:49:41,810 --> 08:49:45,090
for example this type of capability allows for

7
08:49:45,090 --> 08:49:49,110
personalization of advertisement on the web pages you visit

8
08:49:49,110 --> 08:49:53,610
based on your recent search viewing and purchase history 

9
08:49:53,610 --> 08:49:59,350
if a business cannot take advantage of the data as it gets generated or

10
08:49:59,350 --> 08:50:03,540
at the speed analysis of it is needed they often miss opportunities 

11
08:50:04,620 --> 08:50:09,540
in order to build a case for the importance of this dimension of big data 

12
08:50:09,540 --> 08:50:11,290
let imagine we are taking a road trip 

13
08:50:12,850 --> 08:50:16,610
you are looking for some better information to start packing 

14
08:50:16,610 --> 08:50:18,730
in this case the newer the information 

15
08:50:18,730 --> 08:50:23,140
the higher its relevance in deciding what to pack 

16
08:50:23,140 --> 08:50:25,420
would you use last month weather information or

17
08:50:25,420 --> 08:50:28,310
data from last year at this time 

18
08:50:28,310 --> 08:50:33,550
or would you use the weather information from this week yesterday or

19
08:50:33,550 --> 08:50:34,630
better today 

20
08:50:35,820 --> 08:50:39,750
it makes sense to obtain the latest information about weather and

21
08:50:39,750 --> 08:50:42,830
process it in a way that makes your decisions easier 

22
08:50:42,830 --> 08:50:48,600
if the information is old it does not matter how accurate it is 

23
08:50:49,950 --> 08:50:53,190
being able to catch up with the velocity of big data and

24
08:50:53,190 --> 08:50:58,640
analyzing it as it gets generated can even impact the quality of human life 

25
08:50:58,640 --> 08:51:04,700
sensors and smart devices monitoring the human body can detect abnormalities

26
08:51:04,700 --> 08:51:10,510
in real time and trigger immediate action potentially saving lives 

27
08:51:10,510 --> 08:51:14,950
this type of processing is what we call real time processing 

28
08:51:14,950 --> 08:51:19,210
real - time processing is quite different from its remote relative 

29
08:51:19,210 --> 08:51:19,800
batch processing 

30
08:51:22,270 --> 08:51:26,320
batch processing was the norm until a couple of years ago 

31
08:51:26,320 --> 08:51:30,160
large amounts of data would be fed into large machines and

32
08:51:30,160 --> 08:51:31,870
processed for days at a time 

33
08:51:33,230 --> 08:51:37,870
while this type of processing is still very common today decisions based on

34
08:51:37,870 --> 08:51:43,490
information that is even few days old can be catastrophic to some businesses 

35
08:51:45,620 --> 08:51:49,320
organizations which make decisions on latest data

36
08:51:49,320 --> 08:51:50,960
are more likely to hit the target 

37
08:51:52,860 --> 08:51:56,780
for this reason it important to match the speed of processing

38
08:51:56,780 --> 08:52:01,345
with the speed of information generation and get real time decision making power 

39
08:52:01,345 --> 08:52:05,497
in addition today sensor - powered

40
08:52:05,497 --> 08:52:10,843
socioeconomic climate requires faster decisions 

41
08:52:10,843 --> 08:52:15,332
hence we can not wait for all the data to be first produced 

42
08:52:15,332 --> 08:52:17,190
then fed into a machine 

43
08:52:18,480 --> 08:52:22,830
there are many applications where new information is streaming and

44
08:52:22,830 --> 08:52:26,120
needs to be integrated with existing data to

45
08:52:26,120 --> 08:52:30,825
produce decisions such as emergency response planning in a tornado or

46
08:52:30,825 --> 08:52:36,500
deciding trading strategies in real time or getting estimates in advertising 

47
08:52:38,520 --> 08:52:44,340
we have to digest chunks of data as they are produced and give meaningful results 

48
08:52:46,900 --> 08:52:48,385
as more data comes in 

49
08:52:48,385 --> 08:52:52,930
your results will need to adapt to reflect this change in the input 

50
08:52:54,060 --> 08:53:00,160
decisions based on processing of already acquired data such as batch processing 

51
08:53:00,160 --> 08:53:02,660
may give an incomplete picture 

52
08:53:02,660 --> 08:53:08,550
and hence the applications need real time status of the context at hand 

53
08:53:08,550 --> 08:53:09,910
that is streaming analysis 

54
08:53:11,280 --> 08:53:16,420
fortunately with the event of cheap sensors technology 

55
08:53:16,420 --> 08:53:21,520
mobile phones and social media we can obtain the latest information

56
08:53:21,520 --> 08:53:25,940
at a much rapid rate and in real time in comparison with the past 

57
08:53:26,990 --> 08:53:30,650
so how do you make sure we match the velocity of the expectations

58
08:53:30,650 --> 08:53:32,910
to gain insights from big data 

59
08:53:32,910 --> 08:53:35,900
with the velocity of the big data 

60
08:53:35,900 --> 08:53:38,050
rate of generation retrieval 

61
08:53:38,050 --> 08:53:41,650
or processing of data is application specific 

62
08:53:42,760 --> 08:53:47,660
the need for real time data - driven actions within a business case is

63
08:53:47,660 --> 08:53:52,620
what in the end dictates the velocity of analytics over big data 

64
08:53:53,920 --> 08:53:57,620
sometimes precision of a minute is needed 

65
08:53:57,620 --> 08:53:59,750
sometimes half a day 

66
08:53:59,750 --> 08:54:04,710
let look at these four paths and discuss when to pick the right one for

67
08:54:04,710 --> 08:54:06,270
your analysis 

68
08:54:06,270 --> 08:54:10,937
the dollar signs next to the numbers in this example indicate how costly

69
08:54:10,937 --> 08:54:13,000
the operation is 

70
08:54:13,000 --> 08:54:15,160
the more dollars the higher the cost 

71
08:54:16,600 --> 08:54:20,750
when the timeliness of processed information plays no role in decision

72
08:54:20,750 --> 08:54:25,950
making the speed at which data is generated becomes irrelevant 

73
08:54:25,950 --> 08:54:31,220
in other words you can wait for as long as it takes to process data 

74
08:54:31,220 --> 08:54:33,730
days months weeks 

75
08:54:33,730 --> 08:54:37,120
and once processing is over you will look at the results and

76
08:54:37,120 --> 08:54:38,550
probably share them with someone 

77
08:54:39,680 --> 08:54:44,640
when timeliness is not an issue you can choose any of the four paths 

78
08:54:45,800 --> 08:54:48,150
you will likely pick the cheapest one 

79
08:54:49,290 --> 08:54:52,720
when timeliness of end result is an issue

80
08:54:52,720 --> 08:54:56,810
deciding which of the four paths to choose is not so simple 

81
08:54:56,810 --> 08:55:00,620
you will have to make a decision based on cost of hardware 

82
08:55:00,620 --> 08:55:05,110
time sensitivity of information future scenarios 

83
08:55:05,110 --> 08:55:10,480
in other words this becomes a business driven question 

84
08:55:10,480 --> 08:55:16,930
for example if speed is really important at all costs you will pick path four 

85
08:55:16,930 --> 08:55:23,130
as a summary we need to pay attention to the velocity of big data 

86
08:55:23,130 --> 08:55:27,460
streaming data gives information on what is going on right now 

87
08:55:27,460 --> 08:55:32,670
streaming data has velocity meaning it gets generated at various rates 

88
08:55:32,670 --> 08:55:37,370
and analysis of such data in real time gives agility and

89
08:55:37,370 --> 08:55:41,620
adaptability to maximize benefits you want to extract 

1
17:44:46,640 --> 17:44:49,377
characteristics of big data veracity 

2
17:45:05,366 --> 17:45:08,980
veracity of big data refers to the quality of the data 

3
17:45:10,010 --> 17:45:14,760
it sometimes gets referred to as validity or

4
17:45:14,760 --> 17:45:18,250
volatility referring to the lifetime of the data 

5
17:45:19,840 --> 17:45:23,360
veracity is very important for making big data operational 

6
17:45:24,810 --> 17:45:28,900
because big data can be noisy and uncertain 

7
17:45:28,900 --> 17:45:35,885
it can be full of biases abnormalities and it can be imprecise 

8
17:45:35,885 --> 17:45:39,741
data is of no value if it not accurate 

9
17:45:39,741 --> 17:45:44,900
the results of big data analysis are only as good as the data being analyzed 

10
17:45:46,480 --> 17:45:51,840
this is often described in analytics as junk in equals junk out 

11
17:45:53,040 --> 17:45:57,760
so we can say although big data provides many opportunities to make

12
17:45:57,760 --> 17:46:03,100
data enabled decisions the evidence provided by data

13
17:46:03,100 --> 17:46:07,710
is only valuable if the data is of a satisfactory quality 

14
17:46:08,880 --> 17:46:11,870
there are many different ways to define data quality 

15
17:46:12,890 --> 17:46:15,200
in the context of big data 

16
17:46:15,200 --> 17:46:19,040
quality can be defined as a function of a couple of different variables 

17
17:46:20,230 --> 17:46:26,310
accuracy of the data the trustworthiness or reliability of the data source 

18
17:46:26,310 --> 17:46:29,940
and how the data was generated are all important factors

19
17:46:29,940 --> 17:46:31,720
that affect the quality of data 

20
17:46:33,100 --> 17:46:38,910
additionally how meaningful the data is with respect to the program that

21
17:46:38,910 --> 17:46:45,360
analyzes it is an important factor and makes context a part of the quality 

22
17:46:47,040 --> 17:46:52,850
in this chart from 2015 we see the volumes of data increasing 

23
17:46:52,850 --> 17:46:57,380
starting with small amounts of enterprise data to larger 

24
17:46:57,380 --> 17:47:02,480
people generated voice over ip and social media data and

25
17:47:02,480 --> 17:47:06,010
even larger machine generated sensor data 

26
17:47:06,010 --> 17:47:11,040
we also see that the uncertainty of the data increases as we go from

27
17:47:11,040 --> 17:47:13,840
enterprise data to sensor data 

28
17:47:13,840 --> 17:47:16,320
this is as we would expect it to be 

29
17:47:16,320 --> 17:47:20,980
traditional enterprise data in warehouses have

30
17:47:20,980 --> 17:47:26,080
standardized quality solutions like master processes for extract 

31
17:47:26,080 --> 17:47:31,310
transform and load of the data which we referred to as before as etl 

32
17:47:31,310 --> 17:47:36,080
as enterprises started incorporating less structured and unstructured people and

33
17:47:36,080 --> 17:47:39,870
machine data into their big data solutions 

34
17:47:39,870 --> 17:47:43,670
the data become messier and more uncertain 

35
17:47:43,670 --> 17:47:45,210
there are many reasons for this 

36
17:47:46,220 --> 17:47:52,620
first unstructured data on the internet is imprecise and uncertain 

37
17:47:52,620 --> 17:47:58,730
in addition high velocity big data leaves very little or no time for

38
17:47:58,730 --> 17:48:04,810
etl and in turn hindering the quality assurance processes of the data 

39
17:48:04,810 --> 17:48:09,090
let look at these product reviews for a banana slicer on amazon com 

40
17:48:09,090 --> 17:48:13,960
one of the five star reviews say that

41
17:48:13,960 --> 17:48:19,190
it saved her marriage and compared it to the greatest inventions in history 

42
17:48:19,190 --> 17:48:23,900
another five star reviewer said that his parole officer recommended

43
17:48:23,900 --> 17:48:27,470
the slicer as he is not allowed to be around knives 

44
17:48:27,470 --> 17:48:29,400
these are obviously fake reviewers 

45
17:48:30,410 --> 17:48:34,070
now think of an automated product assessment going through such

46
17:48:34,070 --> 17:48:38,930
splendid reviews and estimating lots of sales for the banana slicer and

47
17:48:38,930 --> 17:48:43,390
in turn suggesting stocking more of the slicer in the inventory 

48
17:48:43,390 --> 17:48:44,670
amazon will have problems 

49
17:48:45,760 --> 17:48:51,420
for a more serious case let look at the google flu trends case from 2013 

50
17:48:51,420 --> 17:48:56,788
for january 2013 the google friends actually

51
17:48:56,788 --> 17:49:02,280
estimated almost twice as many flu cases as was reported by cdc 

52
17:49:02,280 --> 17:49:05,870
the centers for disease control and prevention 

53
17:49:07,390 --> 17:49:11,960
the primary reason behind this was that google flu trends used a big data on

54
17:49:11,960 --> 17:49:17,070
the internet and did not account properly for uncertainties about the data 

55
17:49:18,210 --> 17:49:22,780
maybe the news and social media attention paid to the particularly

56
17:49:22,780 --> 17:49:27,450
serious level of flu that year effected the estimate 

57
17:49:27,450 --> 17:49:31,840
and resulted in what we call an over estimation 

58
17:49:31,840 --> 17:49:34,050
this is a perfect example for

59
17:49:34,050 --> 17:49:40,470
how inaccurate the results can be if only big data is used in the analysis 

60
17:49:40,470 --> 17:49:44,820
imagine the economical impact of making health care preparations for

61
17:49:44,820 --> 17:49:46,920
twice the amount of flu cases 

62
17:49:46,920 --> 17:49:48,610
that would be huge 

63
17:49:48,610 --> 17:49:52,850
the google flu trends example also brings up the need for

64
17:49:52,850 --> 17:49:58,350
being able to identify where exactly the big data they used comes from 

65
17:49:58,350 --> 17:50:01,300
what transformation did big data go through up

66
17:50:01,300 --> 17:50:04,420
until the moment it was used for a estimate 

67
17:50:04,420 --> 17:50:08,460
this is what we refer to as data providence 

68
17:50:08,460 --> 17:50:12,630
just like we refer to an artifacts provenance 

69
17:50:12,630 --> 17:50:18,180
as a summary the growing torrents of big data pushes for

70
17:50:18,180 --> 17:50:21,590
fast solutions to utilize it in analytical solutions 

71
17:50:22,630 --> 17:50:27,000
this creates challenges on keeping track of data quality 

72
17:50:27,000 --> 17:50:31,510
what has been collected where it came from and

73
17:50:31,510 --> 17:50:33,940
how it was analyzed prior to its use 

74
17:50:35,150 --> 17:50:37,910
this is akin to an art artifact

75
17:50:37,910 --> 17:50:40,490
having providence of everything it has gone through 

76
17:50:41,540 --> 17:50:46,560
but even more complicated to achieve with large volumes of data coming

77
17:50:46,560 --> 17:50:49,130
in varieties and velocities 

1
11:35:34,830 --> 11:35:37,232
characteristics of big data - volume 

2
11:35:47,965 --> 11:35:53,190
volume is the big data dimension that relates to the sheer size of big data 

3
11:35:54,410 --> 11:35:58,580
this volume can come from large datasets being shared or

4
11:35:58,580 --> 11:36:04,240
many small data pieces and events being collected over time 

5
11:36:05,460 --> 11:36:09,335
every minute 204 million emails are sent 

6
11:36:09,335 --> 11:36:15,308
200 000 photos are uploaded and 1 8 million likes are generated on facebook 

7
11:36:15,308 --> 11:36:22,710
on youtube 1 3 million videos are viewed and 72 hours of video are uploaded 

8
11:36:25,160 --> 11:36:28,120
but how much data are we talking about 

9
11:36:28,120 --> 11:36:33,170
the size and the scale of storage for big data can be massive 

10
11:36:33,170 --> 11:36:37,310
you heard me say words that start with peta exa and

11
11:36:37,310 --> 11:36:43,220
yotta to define size but what does all that really mean 

12
11:36:43,220 --> 11:36:49,210
for comparison 100 megabytes will hold a couple of encyclopedias 

13
11:36:50,460 --> 11:36:54,340
a dvd is around 5 gbs and

14
11:36:54,340 --> 11:36:59,837
1 tb would hold around 300 hours of good quality video 

15
11:36:59,837 --> 11:37:06,530
a data - oriented business currently collects data in the order of terabytes 

16
11:37:06,530 --> 11:37:09,880
but petabytes are becoming more common to our daily lives 

17
11:37:10,930 --> 11:37:16,290
cern large hadron collider generates 15 petabytes a year 

18
11:37:16,290 --> 11:37:21,340
according to predictions by an idc report sponsored by a big data company called

19
11:37:21,340 --> 11:37:28,150
emc digital data will grow by a factor of 44 until the year 2020 

20
11:37:28,150 --> 11:37:32,190
this is a growth from 0 8 zetabytes 

21
11:37:33,750 --> 11:37:38,280
in 2009 to 35 2 zettabytes in 2020 

22
11:37:38,280 --> 11:37:45,090
a zettabyte is 1 trillion gigabytes that 10 to the power of 21 

23
11:37:45,090 --> 11:37:48,140
the effects of it will be huge ! 

24
11:37:49,160 --> 11:37:54,420
think of all the time cost energy that will be used to store and

25
11:37:54,420 --> 11:37:57,860
make sense of such an amount of data 

26
11:37:57,860 --> 11:38:00,430
the next era will be yottabytes 

27
11:38:00,430 --> 11:38:05,050
ten to the power of 24 and brontobytes ten to the power of 27 

28
11:38:05,050 --> 11:38:10,410
which is really hard to imagine for most of us at this time 

29
11:38:10,410 --> 11:38:15,740
this is also what we call data at an astronomical scale 

30
11:38:15,740 --> 11:38:18,280
the choice of putting the milky way galaxy

31
11:38:19,540 --> 11:38:23,560
in the middle of the circle is not just for aesthetics 

32
11:38:24,660 --> 11:38:28,520
this is what we would see if we were to scale up 10 to

33
11:38:28,520 --> 11:38:30,780
the 21 times into the universe 

34
11:38:30,780 --> 11:38:31,510
cool is not it 

35
11:38:32,750 --> 11:38:35,760
please refer to the reading in this module called 

36
11:38:35,760 --> 11:38:41,220
what does astronomical scale mean for a nice video on the powers of ten 

37
11:38:41,220 --> 11:38:48,400
all of these point to an exponential growth in data volume and storage 

38
11:38:48,400 --> 11:38:51,200
what is the relevance of this much data in our world 

39
11:38:52,260 --> 11:38:54,210
remember the planes collecting big data 

40
11:38:55,550 --> 11:38:59,170
our hope as passengers is data means better flight safety 

41
11:39:00,540 --> 11:39:06,070
the idea is to understand that businesses and organizations are collecting and

42
11:39:06,070 --> 11:39:10,520
leveraging large volumes of data to improve their end products 

43
11:39:10,520 --> 11:39:14,366
whether it is safety reliability healthcare or governance 

44
11:39:14,366 --> 11:39:19,300
in general in business the goal

45
11:39:19,300 --> 11:39:24,660
is to turn this much data into some form of business advantage 

46
11:39:24,660 --> 11:39:29,140
the question is how do we utilize larger volumes of data

47
11:39:29,140 --> 11:39:31,920
to improve our end product quality 

48
11:39:31,920 --> 11:39:34,790
despite a number of challenges related to it 

49
11:39:35,840 --> 11:39:40,650
there are a number of challenges related to the massive volumes of big data 

50
11:39:42,210 --> 11:39:45,970
the most obvious one is of course storage 

51
11:39:45,970 --> 11:39:48,800
as the size of the data increases so

52
11:39:48,800 --> 11:39:52,740
does the amount of storage space required to store that data efficiently 

53
11:39:53,810 --> 11:39:58,340
however we also need to be able to retrieve that large amount of

54
11:39:58,340 --> 11:40:00,030
data fast enough and

55
11:40:00,030 --> 11:40:06,140
move it to processing units in a timely fashion to get results when we need them 

56
11:40:06,140 --> 11:40:09,900
this brings additional challenges such as networking bandwidth 

57
11:40:09,900 --> 11:40:11,205
cost of storing data 

58
11:40:11,205 --> 11:40:15,670
in - house versus cloud storage and things like that 

59
11:40:15,670 --> 11:40:20,530
additional challenges arise during processing of such large data 

60
11:40:20,530 --> 11:40:24,450
most existing analytical methods wo not scale to such sums of

61
11:40:24,450 --> 11:40:27,500
data in terms of memory processing or io needs 

62
11:40:29,150 --> 11:40:30,980
this means their performance will drop 

63
11:40:32,060 --> 11:40:36,220
you might be able to get good performance for data from hundreds of customers 

64
11:40:36,220 --> 11:40:43,314
but how about scaling your solution to 1 000 or 10 000 customers 

65
11:40:43,314 --> 11:40:49,270
as the volume increases performance and cost start becoming a challenge 

66
11:40:50,880 --> 11:40:55,530
businesses need a holistic strategy to handle processing of

67
11:40:55,530 --> 11:41:00,090
large scale data to their benefit in the most cost effective manner 

68
11:41:00,090 --> 11:41:03,690
evaluating the options across the dimensions mentioned here 

69
11:41:03,690 --> 11:41:07,580
is the first step when it comes to continuously increasing data size 

70
11:41:07,580 --> 11:41:12,290
we will revisit this topic later on in this course 

71
11:41:12,290 --> 11:41:18,330
as a summary volume is the dimension of big data related to its size and

72
11:41:18,330 --> 11:41:19,540
its exponential growth 

73
11:41:20,770 --> 11:41:26,126
the challenges with working with volumes of big data include cost scalability 

74
11:41:26,126 --> 11:41:30,725
and performance related to their storage access and processing 

1
23:17:06,826 --> 23:17:08,162
data science 

2
23:17:08,162 --> 23:17:10,298
getting value out of big data 

3
23:17:30,242 --> 23:17:35,860
we have all heard data science turned data into insights or even actions 

4
23:17:35,860 --> 23:17:38,250
but what does that really mean 

5
23:17:38,250 --> 23:17:40,990
data science can be thought of as a basis for

6
23:17:40,990 --> 23:17:45,440
empirical research where data is used to induce information for observations 

7
23:17:46,490 --> 23:17:50,310
these observations are mainly data in our case 

8
23:17:50,310 --> 23:17:53,985
big data related to a business or scientific case 

9
23:17:53,985 --> 23:17:58,430
insight is a term

10
23:17:58,430 --> 23:18:03,050
we use to refer to the data products of data science 

11
23:18:03,050 --> 23:18:07,523
it is extracted from a diverse amount of data through a combination of

12
23:18:07,523 --> 23:18:11,686
exploratory data analysis and modeling 

13
23:18:11,686 --> 23:18:17,440
the questions are sometimes more specific and sometimes it requires

14
23:18:17,440 --> 23:18:22,170
looking at the data and patterns in it to come up with the specific question 

15
23:18:24,420 --> 23:18:29,550
another important point to recognize is that data science is not static 

16
23:18:29,550 --> 23:18:32,020
it is not one time analysis 

17
23:18:32,020 --> 23:18:36,920
it involves a process where models generated to lead to insights

18
23:18:36,920 --> 23:18:41,630
are constantly improved through further empirical evidence or simply data 

19
23:18:43,640 --> 23:18:49,060
for example a book retailer like amazon com can constantly improve

20
23:18:49,060 --> 23:18:53,560
the model of a customer book preferences using the customer demographic 

21
23:18:54,580 --> 23:18:59,630
his or her previous purchases and the book reviews of the customer 

22
23:19:01,920 --> 23:19:06,350
the book retailer can also uses information to predict

23
23:19:06,350 --> 23:19:11,219
which customers are likely to like any book 

24
23:19:11,219 --> 23:19:15,560
and take action to market the book to those customers 

25
23:19:17,840 --> 23:19:20,955
this is where we see insights being turned into action 

26
23:19:23,834 --> 23:19:28,718
as we have seen in the book marketing example using data science and analysis

27
23:19:28,718 --> 23:19:33,240
of the past and current information data science generates actions 

28
23:19:34,350 --> 23:19:37,200
this is not just an analysis of the past but

29
23:19:37,200 --> 23:19:40,980
rather generation of actionable information for the future 

30
23:19:42,370 --> 23:19:46,070
this is what we can call a prediction like the weather forecast 

31
23:19:47,680 --> 23:19:53,140
when you decide what to wear for the day based on the forecast of the day 

32
23:19:53,140 --> 23:19:57,880
you are taking action based on insight delivered to you 

33
23:19:57,880 --> 23:20:02,110
just like this business leaders and decision makers

34
23:20:02,110 --> 23:20:06,170
take action based on the evidence provided by their data science teams 

35
23:20:08,140 --> 23:20:09,870
we set data science teams 

36
23:20:10,980 --> 23:20:15,290
this comes from the breadth of information and skill that it takes to make it happen 

37
23:20:16,590 --> 23:20:21,010
you have probably seen diagrams like this one that describe data science 

38
23:20:22,090 --> 23:20:26,220
data science happens at the intersection of computer science 

39
23:20:26,220 --> 23:20:28,820
mathematics and business expertise 

40
23:20:31,170 --> 23:20:33,750
if we zoom deeper into this diagram and

41
23:20:33,750 --> 23:20:39,030
open the sets of expertise we will see a variation of this figure 

42
23:20:40,410 --> 23:20:46,690
even at this level all of these circles require deeper knowledge and skills

43
23:20:46,690 --> 23:20:52,140
in areas like domain expertise data engineering statistics and computing 

44
23:20:55,185 --> 23:20:59,790
an even deeper analysis of these skills will lead you to skills like

45
23:20:59,790 --> 23:21:04,110
machine learning statistical modeling relational algebra 

46
23:21:04,110 --> 23:21:08,640
business passion problem solving and data visualization 

47
23:21:08,640 --> 23:21:11,582
that a lot of skills to have for a single person 

48
23:21:16,922 --> 23:21:21,862
these wide range of skills and definitions of data scientists having them

49
23:21:21,862 --> 23:21:25,855
all led to discussions like are data scientists unicorns 

50
23:21:26,895 --> 23:21:28,115
meaning they do not exist 

51
23:21:29,375 --> 23:21:33,795
there are data science experts who have expertise in more than one of these

52
23:21:33,795 --> 23:21:35,380
skills for sure 

53
23:21:35,380 --> 23:21:37,795
but they are relatively rare and

54
23:21:37,795 --> 23:21:41,470
still would probably need help from an expert on some of these areas 

55
23:21:42,730 --> 23:21:48,550
so in reality data scientists are teams of people who act like one 

56
23:21:49,560 --> 23:21:52,860
they are passionate about the story and the meaning behind data 

57
23:21:54,310 --> 23:21:58,700
they understand they problem they are trying to solve and

58
23:21:58,700 --> 23:22:01,910
aim to find the right analytical methods to solve this problem 

59
23:22:01,910 --> 23:22:08,350
and they all have an interest in engineering solutions to solve problems 

60
23:22:10,230 --> 23:22:15,360
they also have curiosity about each others work and have communication

61
23:22:15,360 --> 23:22:19,940
skills to interact with the team and present their ideas and results to others 

62
23:22:21,900 --> 23:22:27,460
as a summary a data science team often comes together to analyze situations 

63
23:22:27,460 --> 23:22:33,370
business or scientific cases which none of the individuals can solve on their own 

64
23:22:33,370 --> 23:22:36,090
there are lots of moving parts to the solution 

65
23:22:36,090 --> 23:22:40,370
but in the end all these parts should come together to provide

66
23:22:40,370 --> 23:22:43,055
actionable insight based on big data 

67
23:22:45,807 --> 23:22:50,732
being able to use evidence - based insight in business decisions is more

68
23:22:50,732 --> 23:22:52,517
important now than ever 

69
23:22:52,517 --> 23:22:56,594
data scientists have a combination of technical business and

70
23:22:56,594 --> 23:22:58,716
soft skills to make this happen 

1
22:40:02,930 --> 22:40:05,310
getting started characteristics of big data 

2
22:40:23,143 --> 22:40:27,786
by now you have seen that big data is a blanket term that is used to refer to any

3
22:40:27,786 --> 22:40:32,140
collection of data so large and complex that it exceeds the processing

4
22:40:32,140 --> 22:40:37,530
capability of conventional data management systems and techniques 

5
22:40:37,530 --> 22:40:41,040
the applications of big data are endless 

6
22:40:41,040 --> 22:40:46,000
every part of business and society are changing in front our eyes due to that

7
22:40:46,000 --> 22:40:50,870
fact that we now have so much more data and the ability for analyzing 

8
22:40:52,220 --> 22:40:54,120
but how can we characterize big data 

9
22:40:55,310 --> 22:40:58,630
you can say i know it when i see it 

10
22:40:58,630 --> 22:41:01,440
but there are easier ways to do it 

11
22:41:01,440 --> 22:41:05,030
big data is commonly characterized using a number of v 

12
22:41:06,040 --> 22:41:11,720
the first three are volume velocity and variety 

13
22:41:11,720 --> 22:41:17,580
volume refers to the vast amounts of data that is generated every second 

14
22:41:17,580 --> 22:41:21,470
minutes hour and day in our digitized world 

15
22:41:23,130 --> 22:41:29,480
variety refers to the ever increasing different forms that data can come in

16
22:41:29,480 --> 22:41:34,090
such as text images voice and geospatial data 

17
22:41:36,100 --> 22:41:41,440
velocity refers to the speed at which data is being generated and

18
22:41:41,440 --> 22:41:44,980
the pace at which data moves from one point to the next 

19
22:41:46,290 --> 22:41:48,510
volume variety and

20
22:41:48,510 --> 22:41:54,430
velocity are the three main dimensions that characterize big data 

21
22:41:54,430 --> 22:41:55,790
and describe its challenges 

22
22:41:57,180 --> 22:42:02,410
we have huge amounts of data in different formats and

23
22:42:02,410 --> 22:42:06,160
varying quality which must be processed quickly 

24
22:42:07,200 --> 22:42:10,470
more vs have been introduced to the big data community

25
22:42:10,470 --> 22:42:14,070
as we discover new challenges and ways to define big data 

26
22:42:15,310 --> 22:42:19,830
veracity and valence are two of these additional v we

27
22:42:19,830 --> 22:42:23,720
will pay special attention to as a part of this specialization 

28
22:42:24,860 --> 22:42:31,360
veracity refers to the biases noise and abnormality in data 

29
22:42:31,360 --> 22:42:36,680
or better yet it refers to the often unmeasurable uncertainties and

30
22:42:36,680 --> 22:42:40,540
truthfulness and trustworthiness of data 

31
22:42:40,540 --> 22:42:46,150
valence refers to the connectedness of big data in the form of graphs 

32
22:42:46,150 --> 22:42:47,180
just like atoms 

33
22:42:48,220 --> 22:42:54,180
moreover we must be sure to never forget our sixth v value 

34
22:42:55,220 --> 22:42:57,890
how do big data benefit you and your organization 

35
22:42:59,560 --> 22:43:01,640
without a clear strategy and

36
22:43:01,640 --> 22:43:05,700
an objective with the value they are getting from big data 

37
22:43:05,700 --> 22:43:09,800
it is easy to imagine that organizations will be sidetracked by all these

38
22:43:09,800 --> 22:43:14,630
challenges of big data and not be able to turn them into opportunities 

39
22:43:15,990 --> 22:43:19,993
now let start looking into the first five of these v in detail 

1
21:23:23,060 --> 21:23:25,400
how does data science happen 

2
21:23:25,400 --> 21:23:26,850
five p of data science 

3
21:23:28,810 --> 21:23:34,220
now that we identified what data science is and how companies can strategize around

4
21:23:34,220 --> 21:23:39,660
big data to start building a purpose let come back to using data science

5
21:23:39,660 --> 21:23:44,870
to get value out of big data around the purpose or questions they defined 

6
21:24:02,348 --> 21:24:04,591
our experience with building and

7
21:24:04,591 --> 21:24:10,040
observing successful data science projects led to a method around the craft with

8
21:24:10,040 --> 21:24:16,255
five distinct components that can be defined as components of data science 

9
21:24:16,255 --> 21:24:20,859
here we define data science as a multi - disciplinary

10
21:24:20,859 --> 21:24:24,972
craft that combines people teaming up around

11
21:24:24,972 --> 21:24:29,910
application - specific purpose that can be achieved through a process 

12
21:24:31,420 --> 21:24:35,100
big data computing platforms and programmability 

13
21:24:38,260 --> 21:24:43,460
all of these should lead to products where the focus really

14
21:24:43,460 --> 21:24:47,760
is on the questions or purpose that are defined by your big data strategy ideas 

15
21:24:49,960 --> 21:24:54,540
there are many technology data and analytical research and

16
21:24:54,540 --> 21:24:58,110
development related activities around the questions 

17
21:24:58,110 --> 21:25:02,490
but in the end everything we do in this phase is to reach to that

18
21:25:02,490 --> 21:25:04,820
final product based on our purposes 

19
21:25:05,950 --> 21:25:08,830
so it makes sense to start with it and

20
21:25:08,830 --> 21:25:11,860
build a process around how we make this product happen 

21
21:25:13,740 --> 21:25:16,520
remember the wild fire prediction project i described 

22
21:25:17,830 --> 21:25:22,070
one of the products we described there was the rate of spread and

23
21:25:22,070 --> 21:25:23,870
direction of an ongoing fire 

24
21:25:25,460 --> 21:25:27,780
we have identified questions and

25
21:25:27,780 --> 21:25:32,060
the process that led us to the product in the end to solve it 

26
21:25:33,960 --> 21:25:38,280
we brought together experts around the table for fire modeling 

27
21:25:38,280 --> 21:25:42,620
data management time series analysis scalable computing 

28
21:25:42,620 --> 21:25:46,060
geographical information systems and emergency response 

29
21:25:49,050 --> 21:25:52,770
i asked them let not dive into the techniques yet 

30
21:25:52,770 --> 21:25:54,080
what is the problem at large 

31
21:25:55,100 --> 21:26:01,380
how do we see ourselves solving it 

32
21:26:01,380 --> 21:26:05,780
a typical conversation around the process starts with this question 

33
21:26:07,230 --> 21:26:12,600
then from then on drilling down to many areas of expertise 

34
21:26:12,600 --> 21:26:14,830
often we blur lines between the steps 

35
21:26:16,510 --> 21:26:21,280
my wildfire team would start listing things like we do not have an integrated

36
21:26:21,280 --> 21:26:26,400
system or we do not have real - time access to data programmatically 

37
21:26:26,400 --> 21:26:29,210
so we cannot analyze fires on the fly 

38
21:26:29,210 --> 21:26:33,950
or they can say i cannot integrate sensor data with satellite data 

39
21:26:35,370 --> 21:26:41,080
all of this leads me to challenges i can then use to define problems 

40
21:26:42,300 --> 21:26:46,560
there are many dimensions of data science to think about within this discussion 

41
21:26:47,570 --> 21:26:52,700
let start with the obvious ones people and purpose 

42
21:26:55,535 --> 21:27:00,800
people refers to a data science team or the projects stakeholders 

43
21:27:00,800 --> 21:27:03,700
as you know by now they are expert in data and

44
21:27:03,700 --> 21:27:08,028
analytics business computing science or big data management 

45
21:27:08,028 --> 21:27:14,154
like all the set of experts i listed in my wildfire scenario 

46
21:27:14,154 --> 21:27:20,560
the purpose refers to the challenge or set of challenges defined by your

47
21:27:20,560 --> 21:27:25,940
big data strategy like solving the question related to the rate of spread and

48
21:27:25,940 --> 21:27:29,170
direction of the fire perimeter in the wildfire case 

49
21:27:33,003 --> 21:27:38,099
since there a predefined team with a purpose a great place for

50
21:27:38,099 --> 21:27:43,190
this team to start with is a process they could iterate on 

51
21:27:43,190 --> 21:27:48,470
we can simply say people with purpose will define a process to collaborate and

52
21:27:48,470 --> 21:27:49,360
communicate around 

53
21:27:51,010 --> 21:27:54,500
the process is conceptual in the beginning and

54
21:27:54,500 --> 21:27:58,390
defines the set of steps an how everyone can contribute to it 

55
21:28:02,060 --> 21:28:04,230
there are many ways to look at the process 

56
21:28:05,890 --> 21:28:11,250
one way of looking at it is as two distinct activities 

57
21:28:11,250 --> 21:28:14,130
mainly big data engineering and

58
21:28:14,130 --> 21:28:18,500
big data analytics or computational big data science 

59
21:28:18,500 --> 21:28:22,910
as i like to call it as more than simple analytics is being performed here 

60
21:28:24,840 --> 21:28:31,180
a more detailed way of looking at the process reveals five distinct steps or

61
21:28:31,180 --> 21:28:36,214
activities of this data science process 

62
21:28:36,214 --> 21:28:42,018
namely acquire prepare 

63
21:28:42,018 --> 21:28:47,540
analyze report and act 

64
21:28:47,540 --> 21:28:50,190
we can simply say that data science happens

65
21:28:50,190 --> 21:28:52,950
at the boundary of all these steps 

66
21:28:52,950 --> 21:28:57,510
ideally this process should support experimental work and

67
21:28:57,510 --> 21:29:01,980
dynamic scalability on the big data and computing platforms 

68
21:29:05,220 --> 21:29:10,050
this five step process can be used in alternative ways in real life big data

69
21:29:10,050 --> 21:29:14,580
applications if we add the dependencies of different tools to each other 

70
21:29:15,930 --> 21:29:19,490
the influence of big data pushes for

71
21:29:19,490 --> 21:29:23,930
alternative scalability approaches at each step of the process 

72
21:29:25,010 --> 21:29:28,080
just like you would scale each step on its own 

73
21:29:28,080 --> 21:29:31,780
you can scale the whole process as a whole in the end 

74
21:29:35,613 --> 21:29:41,900
one can simply say all of these steps have reporting needs in different forms 

75
21:29:44,280 --> 21:29:51,290
or there is a need to draw all these activities as an iterating process 

76
21:29:51,290 --> 21:29:56,710
including build explore and scale for big data as steps 

77
21:29:59,550 --> 21:30:03,550
big data analysis needs alternative data management techniques and

78
21:30:03,550 --> 21:30:08,290
systems as well as analytical tools and methods 

79
21:30:10,160 --> 21:30:15,730
multiple modes of scalability is needed based on dynamic data and computing loads 

80
21:30:16,750 --> 21:30:20,800
in addition change in physical infrastructure 

81
21:30:20,800 --> 21:30:25,320
streaming data specific urgencies arising from special events

82
21:30:25,320 --> 21:30:28,780
can also require multiple modes of scalability 

83
21:30:30,230 --> 21:30:32,490
in this intro course for simplicity 

84
21:30:32,490 --> 21:30:38,470
we will refer to the process as a set of five sequential activities that iterate 

85
21:30:39,500 --> 21:30:44,550
however we will touch on scalability as needed in our example applications 

86
21:30:47,840 --> 21:30:51,190
as a part of building your big data process 

87
21:30:51,190 --> 21:30:55,230
it important to simply mention two other p 

88
21:30:55,230 --> 21:31:00,620
the first one is big data platforms like the ones in the hadoop framework 

89
21:31:00,620 --> 21:31:04,610
or other computing platforms to scale different steps 

90
21:31:04,610 --> 21:31:08,060
the scalability should be in the mind of all team members and

91
21:31:08,060 --> 21:31:10,030
get communicated as an expectation 

92
21:31:12,390 --> 21:31:16,970
in addition the scalable process should be programmable through

93
21:31:16,970 --> 21:31:22,830
utilization of reusable and reproducible programming interfaces

94
21:31:22,830 --> 21:31:27,308
to libraries like systems middleware analytical tools 

95
21:31:27,308 --> 21:31:31,395
visualization environments and end user reporting environments 

96
21:31:35,010 --> 21:31:37,980
thinking of big data applications as a process 

97
21:31:37,980 --> 21:31:43,150
including a set of activities that the team members can collaborate over 

98
21:31:43,150 --> 21:31:47,720
also helps to build metrics for accountability to be built into it 

99
21:31:47,720 --> 21:31:52,000
this way expectations on cost time 

100
21:31:52,000 --> 21:31:55,610
optimization of deliverables and time lines can be discussed

101
21:31:55,610 --> 21:32:00,210
between the the members starting with the beginning of the data science process 

102
21:32:03,325 --> 21:32:06,890
sometimes we may not be able to do this in one step 

103
21:32:08,550 --> 21:32:13,420
and joint explorations like statistical evaluations of intermediate results or

104
21:32:13,420 --> 21:32:16,060
accuracy of sample data sets become important 

105
21:32:18,150 --> 21:32:23,430
as a summary data science can be defined as a craft of using the five p 

106
21:32:23,430 --> 21:32:28,380
identified in this lecture leading to a sixth p the data product 

107
21:32:29,580 --> 21:32:33,870
having a process within the more business - driven ps like people and

108
21:32:33,870 --> 21:32:38,120
purpose and the more technically driven p like platforms and

109
21:32:38,120 --> 21:32:43,220
programmability leads to a streamlined approach that starts and

110
21:32:43,220 --> 21:32:47,630
ends with the product team accountability and collaboration in mind 

111
21:32:48,740 --> 21:32:52,311
data science process provides guidelines for

112
21:32:52,311 --> 21:32:57,621
implementing big data solution as it helps to organize efforts and

113
21:32:57,621 --> 21:33:04,137
ensures all critical steps taken conforms to pre - define and agreed upon metrics 

1
18:56:28,008 --> 18:56:30,098
step one acquiring data 

2
18:56:44,898 --> 18:56:49,420
the first step in the data science process is to acquire the data 

3
18:56:50,660 --> 18:56:54,860
you need to obtain the source material before analyzing or acting on it 

4
18:56:56,950 --> 18:57:02,690
the first step in acquiring data is to determine what data is available 

5
18:57:02,690 --> 18:57:06,680
leave no stone unturned when it comes to finding the right data sources 

6
18:57:07,770 --> 18:57:11,150
you want to identify suitable data related to your problem and

7
18:57:12,200 --> 18:57:16,540
make use of all data that is relevant to your problem for analysis 

8
18:57:17,720 --> 18:57:21,580
leaving out even a small amount of important data

9
18:57:21,580 --> 18:57:23,570
can lead to incorrect conclusions 

10
18:57:26,190 --> 18:57:29,460
data comes from many places local and

11
18:57:29,460 --> 18:57:34,280
remote in many varieties structured and un - structured 

12
18:57:34,280 --> 18:57:37,250
and with different velocities 

13
18:57:37,250 --> 18:57:42,780
there are many techniques and technologies to access these different types of data 

14
18:57:42,780 --> 18:57:44,420
let discuss a few examples 

15
18:57:46,360 --> 18:57:51,020
a lot of data exists in conventional relational databases 

16
18:57:51,020 --> 18:57:53,560
like structure big data from organizations 

17
18:57:54,600 --> 18:58:00,270
the tool of choice to access data from databases is structured query language or

18
18:58:00,270 --> 18:58:05,360
sql which is supported by all relational databases management systems 

19
18:58:06,680 --> 18:58:12,260
additionally most data base systems come with a graphical application

20
18:58:12,260 --> 18:58:17,250
environment that allows you to query and explore the data sets in the database 

21
18:58:19,990 --> 18:58:26,870
data can also exist in files such as text files and excel spreadsheets 

22
18:58:26,870 --> 18:58:30,638
scripting languages are generally used to get data from files 

23
18:58:30,638 --> 18:58:36,200
a scripting language is a high level programming language

24
18:58:36,200 --> 18:58:41,020
that can be either general purpose or specialized for specific functions 

25
18:58:42,960 --> 18:58:49,010
common scripting languages with support for processing files are java script 

26
18:58:49,010 --> 18:58:54,300
python php perl r and matlab and are many others 

27
18:58:56,780 --> 18:59:01,230
an increasingly popular way to get data is from websites 

28
18:59:01,230 --> 18:59:05,956
web pages are written using a set of standards approved by

29
18:59:05,956 --> 18:59:09,906
a world wide web consortium or shortly w3c 

30
18:59:09,906 --> 18:59:13,650
this includes a variety of formats and services 

31
18:59:14,920 --> 18:59:20,860
one common format is the extensible markup language or xml 

32
18:59:20,860 --> 18:59:25,640
which uses markup symbols or tabs to describe the contents on a webpage 

33
18:59:27,250 --> 18:59:32,920
many websites also host web services which produce program access to their data 

34
18:59:35,310 --> 18:59:38,110
there are several types of web services 

35
18:59:38,110 --> 18:59:41,990
the most popular is rest because it so easy to use 

36
18:59:43,080 --> 18:59:47,280
rest stand for representational state transfer 

37
18:59:47,280 --> 18:59:51,640
and it is an approach to implementing web services with performance 

38
18:59:51,640 --> 18:59:54,200
scalability and maintainability in mind 

39
18:59:55,940 --> 18:59:59,400
web socket services are also becoming more popular

40
18:59:59,400 --> 19:00:02,440
since they allow real time modifications from web sites 

41
19:00:05,040 --> 19:00:09,840
nosql storage systems are increasingly used to manage a variety of data

42
19:00:09,840 --> 19:00:10,770
types in big data 

43
19:00:11,950 --> 19:00:15,710
these data stores are databases that do not represent data

44
19:00:15,710 --> 19:00:20,580
in a table format with columns and rows as with conventional relational databases 

45
19:00:21,910 --> 19:00:27,180
examples of these data stores include cassandra mongodb and hbase 

46
19:00:28,815 --> 19:00:33,350
nosql data stores provide apis to allow users to access data 

47
19:00:34,460 --> 19:00:39,940
these apis can be used directly or in an application that needs to access the data 

48
19:00:41,660 --> 19:00:46,240
additionally most nosql systems provide data access

49
19:00:46,240 --> 19:00:48,960
via a web service interface such a rest 

50
19:00:51,780 --> 19:00:54,860
now let discuss our wildfire case study

51
19:00:54,860 --> 19:00:59,350
as a real project that acquires data using several different mechanisms 

52
19:01:00,780 --> 19:01:07,020
the wifire project stores sensor data from weather stations in a relational database 

53
19:01:08,140 --> 19:01:13,110
we use sql to retrieve this data from the database to create

54
19:01:13,110 --> 19:01:17,800
models to identify weather patterns associated with santa anna conditions 

55
19:01:19,450 --> 19:01:23,910
to determine whether a particular weather station is currently experiencing

56
19:01:23,910 --> 19:01:30,340
santa anna conditions we access real time data using a web socket service 

57
19:01:31,610 --> 19:01:35,170
once we start listening to this service 

58
19:01:35,170 --> 19:01:37,980
we receive weather station measurements as they occur 

59
19:01:39,330 --> 19:01:44,450
this data is then processed and compared to patterns found by our models

60
19:01:44,450 --> 19:01:48,390
to determine if a weather station is experiencing santa ana conditions 

61
19:01:50,060 --> 19:01:53,130
at the same time tweets are retrieved

62
19:01:53,130 --> 19:01:57,870
using hashtags related to any fire that is occurring in the region 

63
19:01:58,950 --> 19:02:03,080
the tweet messages are retrieves using the twitter rest service 

64
19:02:03,080 --> 19:02:08,260
the idea is to determine the sentiment of these tweets to see if people

65
19:02:08,260 --> 19:02:15,540
are expressing fear anger or are simply nonchalant about the nearby fire 

66
19:02:15,540 --> 19:02:20,617
the combination of sensor data and tweet sentiments helps

67
19:02:20,617 --> 19:02:25,389
to give us a sense of the urgency of the fire situation 

68
19:02:25,389 --> 19:02:29,570
as a summary big data comes from many places 

69
19:02:30,570 --> 19:02:33,000
finding and evaluating data

70
19:02:33,000 --> 19:02:37,490
useful to your big data analytics is important before you start acquiring data 

71
19:02:38,740 --> 19:02:40,080
depending on the source and

72
19:02:40,080 --> 19:02:44,240
structure of data there are alternative ways to access it 

1
13:59:11,992 --> 13:59:14,893
step 2 - a : exploring data

2
13:59:27,798 --> 13:59:32,194
after you have put together the data that you need for your application 

3
13:59:32,194 --> 13:59:36,460
you might be tempted to immediately build models to analyze the data 

4
13:59:37,520 --> 13:59:38,830
resist this temptation 

5
13:59:39,890 --> 13:59:44,370
the first step after getting your data is to explore it 

6
13:59:44,370 --> 13:59:48,340
exploring data is a part of the two - step data preparation process 

7
13:59:50,450 --> 13:59:53,560
you want to do some preliminary investigation

8
13:59:53,560 --> 13:59:59,080
in order to gain a better understanding of the specific characteristics of your data 

9
13:59:59,080 --> 14:00:01,620
in this step you will be looking for

10
14:00:01,620 --> 14:00:05,350
things like correlations general trends and outliers 

11
14:00:06,530 --> 14:00:10,700
without this step you will not be able to use the data effectively 

12
14:00:13,100 --> 14:00:16,520
correlation graphs can be used to explore the dependencies

13
14:00:16,520 --> 14:00:18,630
between different variables in the data 

14
14:00:20,210 --> 14:00:24,910
graphing the general trends of variables will show you if there is

15
14:00:24,910 --> 14:00:30,120
a consistent direction in which the values of these variables are moving towards 

16
14:00:31,150 --> 14:00:33,320
like sales prices going up or down 

17
14:00:35,200 --> 14:00:43,320
in statistics an outlier is a data point that distant from other data points 

18
14:00:43,320 --> 14:00:46,256
plotting outliers will help you double check for

19
14:00:46,256 --> 14:00:48,698
errors in the data due to measurements 

20
14:00:48,698 --> 14:00:54,390
in some cases outliers that are not errors might make you find a rare event 

21
14:00:56,670 --> 14:01:02,860
additionally summary statistics provide numerical values to describe your data 

22
14:01:04,040 --> 14:01:09,195
summary statistics are quantities that capture various characteristics

23
14:01:09,195 --> 14:01:13,540
of a set of values with a single number or a small set of numbers 

24
14:01:15,540 --> 14:01:19,674
some basic summary statistics that you should compute for

25
14:01:19,674 --> 14:01:24,490
your data set are mean median range and standard deviation 

26
14:01:26,000 --> 14:01:31,510
mean and median are measures of the location of a set of values 

27
14:01:31,510 --> 14:01:35,590
mode is the value that occurs most frequently in your data set 

28
14:01:36,920 --> 14:01:41,220
and range and standard deviation are measures of spread in your data 

29
14:01:42,640 --> 14:01:47,810
looking at these measures will give you an idea of the nature of your data 

30
14:01:49,630 --> 14:01:52,340
they can tell you if there something wrong with your data 

31
14:01:53,340 --> 14:01:57,957
for example if the range of the values for age in your data includes

32
14:01:57,957 --> 14:02:01,926
negative numbers or a number much greater than 100 

33
14:02:01,926 --> 14:02:06,708
there something suspicious in the data that needs to be examined 

34
14:02:09,629 --> 14:02:14,900
visualization techniques also provide a quick and effective and

35
14:02:14,900 --> 14:02:20,830
overall a very useful way to look at data in this preliminary analysis step 

36
14:02:20,830 --> 14:02:24,250
a heat map such as the one shown here 

37
14:02:24,250 --> 14:02:28,530
can quickly give you the idea of where the hotspots are 

38
14:02:28,530 --> 14:02:31,020
many other different types of graphs can be used 

39
14:02:32,340 --> 14:02:36,320
histograms show that the distribution of the data and

40
14:02:36,320 --> 14:02:39,520
can show skewness or unusual dispersion 

41
14:02:40,920 --> 14:02:45,168
boxplots are another type of plot for showing data distribution 

42
14:02:47,448 --> 14:02:53,410
line graphs are useful for seeing how values in your data change over time 

43
14:02:53,410 --> 14:02:56,140
spikes in the data are also easy to spot 

44
14:02:58,550 --> 14:03:03,070
scatter plots can show you correlation between two variables 

45
14:03:03,070 --> 14:03:06,900
overall there are many types of graph to visualize data 

46
14:03:07,935 --> 14:03:11,030
they are very useful in helping you understand the data you have 

47
14:03:13,120 --> 14:03:17,720
in summary what you get by exploring your data is a better

48
14:03:17,720 --> 14:03:21,510
understanding of the complexity of the data you have to work with 

49
14:03:22,780 --> 14:03:25,418
this in turn will guide the rest of your process 

1
04:02:34,005 --> 04:02:40,278
step 2 - b : pre - processing data

2
04:02:54,283 --> 04:02:58,904
the raw data that you get directly from your sources are never in the format that

3
04:02:58,904 --> 04:03:00,875
you need to perform analysis on 

4
04:03:01,995 --> 04:03:05,885
there are two main goals in the data pre - processing step 

5
04:03:05,885 --> 04:03:10,455
the first is to clean the data to address data quality issues and

6
04:03:10,455 --> 04:03:15,320
the second is to transform the raw data to make it suitable for analysis 

7
04:03:17,850 --> 04:03:21,050
a very important part of data preparation

8
04:03:21,050 --> 04:03:23,606
is to address quality of issues in your data 

9
04:03:23,606 --> 04:03:28,000
real - world data is messy 

10
04:03:28,000 --> 04:03:34,160
there are many examples of quality issues with data from real applications including

11
04:03:34,160 --> 04:03:39,990
inconsistent data like a customer with two different addresses duplicate customer

12
04:03:39,990 --> 04:03:45,460
records for example customers address recorded at two different sales locations 

13
04:03:46,490 --> 04:03:48,600
and the two recordings do not agree 

14
04:03:50,190 --> 04:03:53,160
missing customer agent demographics or studies 

15
04:03:55,010 --> 04:03:59,750
missing values like missing a customer age in the demographic studies 

16
04:04:01,000 --> 04:04:06,160
invalid data like an invalid zip code for example a six digit code 

17
04:04:07,350 --> 04:04:13,060
and outliers like a sense of failure causing values to be much higher or

18
04:04:13,060 --> 04:04:15,480
lower than expected for a period of time 

19
04:04:17,340 --> 04:04:19,860
since we get the data downstream

20
04:04:19,860 --> 04:04:22,960
we usually have little control over how the data is collected 

21
04:04:24,390 --> 04:04:29,120
preventing data quality problems as the data is being collected is not

22
04:04:29,120 --> 04:04:30,070
often an option 

23
04:04:31,740 --> 04:04:33,950
so we have the data that we get and

24
04:04:33,950 --> 04:04:37,770
we have to address quality issues by detecting and correcting them 

25
04:04:39,710 --> 04:04:43,170
here are some approaches we can take to address this quality issues 

26
04:04:45,780 --> 04:04:48,630
we can remove data records with missing values 

27
04:04:50,330 --> 04:04:53,290
we can merge duplicate records 

28
04:04:53,290 --> 04:04:57,430
this will require a way to determine how to resolve conflicting values 

29
04:04:58,780 --> 04:05:03,320
perhaps it makes sense to retain the newer value whenever there a conflict 

30
04:05:05,030 --> 04:05:08,500
for invalid values the best estimate for

31
04:05:08,500 --> 04:05:12,330
a reasonable value can be used as a replacement 

32
04:05:12,330 --> 04:05:16,800
for example for a missing age value for an employee 

33
04:05:16,800 --> 04:05:21,320
a reasonable value can be estimated based on the employee length of employment 

34
04:05:23,820 --> 04:05:28,340
outliers can also be removed if they are not important to the task 

35
04:05:30,590 --> 04:05:34,370
in order to address data quality issues effectively 

36
04:05:34,370 --> 04:05:38,590
knowledge about the application such as how the data was collected 

37
04:05:38,590 --> 04:05:43,880
the user population and the intended uses of the application is important 

38
04:05:45,680 --> 04:05:49,280
this domain knowledge is essential to making informed

39
04:05:49,280 --> 04:05:53,070
decisions on how to handle incomplete or incorrect data 

40
04:05:56,740 --> 04:05:59,480
the second part of preparing data

41
04:05:59,480 --> 04:06:03,299
is to manipulate the clean data into the format needed for analysis 

42
04:06:04,500 --> 04:06:06,910
the step is known by many names 

43
04:06:08,940 --> 04:06:13,894
data manipulation data preprocessing data wrangling 

44
04:06:13,894 --> 04:06:17,419
and even data munging some operations for

45
04:06:17,419 --> 04:06:23,420
this type of operation i mean data munging wrangling preprocessing 

46
04:06:23,420 --> 04:06:28,373
include scaling transformation feature selection 

47
04:06:28,373 --> 04:06:32,778
dimensionality reduction and data manipulation 

48
04:06:35,628 --> 04:06:42,840
scaling involves changing the range of values to be between a specified range 

49
04:06:42,840 --> 04:06:44,970
such as from zero to one 

50
04:06:46,220 --> 04:06:50,460
this is done to avoid having certain features that large values from

51
04:06:50,460 --> 04:06:52,570
dominating the results 

52
04:06:52,570 --> 04:06:56,990
for example in analyzing data with height and weight 

53
04:06:56,990 --> 04:07:01,350
to magnitude of weight values is much greater than of the height values 

54
04:07:03,220 --> 04:07:06,880
so scaling all values to be between zero and

55
04:07:06,880 --> 04:07:11,670
one will equalize contributions from both height and weight features 

56
04:07:14,900 --> 04:07:20,000
various transformations can be performed on the data to reduce noise and

57
04:07:20,000 --> 04:07:20,720
variability 

58
04:07:22,380 --> 04:07:24,970
one such transformation is aggregation 

59
04:07:26,430 --> 04:07:31,260
aggregate data generally results in data with less variability 

60
04:07:31,260 --> 04:07:32,890
which may help with your analysis 

61
04:07:34,190 --> 04:07:39,720
for example daily sales figures may have many serious changes 

62
04:07:40,730 --> 04:07:45,880
aggregating values to weekly or monthly sales figures will result in similar data 

63
04:07:48,720 --> 04:07:53,340
other filtering techniques can also be used to remove variability in the data 

64
04:07:53,340 --> 04:07:57,530
of course this comes at the cost of less detailed data 

65
04:07:57,530 --> 04:08:01,760
so these factors must be weighed for the specific application 

66
04:08:04,570 --> 04:08:10,400
future selection can involve removing redundant or irrelevant features 

67
04:08:10,400 --> 04:08:14,300
combining features and creating new features 

68
04:08:15,460 --> 04:08:18,070
during the exploring data step 

69
04:08:18,070 --> 04:08:21,860
you might have discovered that two features are correlated 

70
04:08:23,030 --> 04:08:26,350
in that case one of these features can be removed

71
04:08:26,350 --> 04:08:29,090
without negatively affecting the analysis results 

72
04:08:30,170 --> 04:08:33,870
for example the purchase price of a product and

73
04:08:33,870 --> 04:08:37,360
the amount of sales tax paid are likely to be correlated 

74
04:08:38,800 --> 04:08:42,550
eliminating the sales tax amount then will be beneficial 

75
04:08:44,860 --> 04:08:46,810
removing redundant or

76
04:08:46,810 --> 04:08:51,010
irrelevant features will make the subsequent analysis much simpler 

77
04:08:53,320 --> 04:08:59,220
in other cases you may want to combine features or create new ones 

78
04:08:59,220 --> 04:09:03,540
for example adding the applicant education level

79
04:09:03,540 --> 04:09:06,820
as a feature to a loan approval application would make sense 

80
04:09:08,940 --> 04:09:13,790
there are also algorithms to automatically determine the most relevant features 

81
04:09:13,790 --> 04:09:16,130
based on various mathematical properties 

82
04:09:19,440 --> 04:09:24,200
dimensionality reduction is useful when the data set has a large number of

83
04:09:24,200 --> 04:09:24,750
dimensions 

84
04:09:26,020 --> 04:09:29,830
it involves finding a smaller subset of dimensions that

85
04:09:29,830 --> 04:09:32,350
captures most of the variation in the data 

86
04:09:34,030 --> 04:09:37,600
this reduces the dimensions of the data

87
04:09:37,600 --> 04:09:42,400
while eliminating irrelevant features and makes analysis simpler 

88
04:09:44,130 --> 04:09:46,089
a technique commonly used for

89
04:09:46,089 --> 04:09:50,878
dimensional reduction is called principle component analysis or pca 

90
04:09:54,528 --> 04:10:00,120
raw data often has to be manipulated to be in the correct format for analysis 

91
04:10:01,140 --> 04:10:06,890
for example from samples recording daily changes in stock prices 

92
04:10:06,890 --> 04:10:09,520
we may want the capture price changes for

93
04:10:09,520 --> 04:10:12,990
a particular market segments like real estate or health care 

94
04:10:14,230 --> 04:10:19,150
this would require determining which stocks belong to which market segment 

95
04:10:19,150 --> 04:10:23,540
grouping them together and perhaps computing the mean range 

96
04:10:23,540 --> 04:10:25,530
standard deviation for each group 

97
04:10:28,120 --> 04:10:28,760
in summary 

98
04:10:29,910 --> 04:10:34,470
data preparation is a very important part of the data science process 

99
04:10:34,470 --> 04:10:39,530
in fact this is where you will spend most of your time on any data science effort 

100
04:10:40,710 --> 04:10:45,680
it can be a tedious process but it is a crucial step 

101
04:10:45,680 --> 04:10:49,200
always remember garbage in garbage out 

102
04:10:49,200 --> 04:10:50,987
if you do not spend the time and

103
04:10:50,987 --> 04:10:55,250
effort to create good data for the analysis you will not get good results

104
04:10:55,250 --> 04:10:59,399
no matter how sophisticated the analysis technique you are using is 

1
08:13:33,005 --> 08:13:36,004
step 3 : analyzing data 

2
08:13:45,608 --> 08:13:48,650
now that you have your data nicely prepared 

3
08:13:48,650 --> 08:13:51,140
the next step is to analyze the data 

4
08:13:52,350 --> 08:13:56,620
data analysis involves building a model from your data 

5
08:13:56,620 --> 08:13:58,350
which is called input data 

6
08:13:59,620 --> 08:14:05,370
the input data is used by the analysis technique to build a model 

7
08:14:07,010 --> 08:14:11,340
what your model generates is the output data 

8
08:14:11,340 --> 08:14:14,140
there are different types of problems and

9
08:14:14,140 --> 08:14:16,890
so there are different types of analysis techniques 

10
08:14:18,370 --> 08:14:23,424
the main categories of analysis techniques are classification regression 

11
08:14:23,424 --> 08:14:29,330
clustering association analysis and graph analysis 

12
08:14:29,330 --> 08:14:31,520
we will describe each one 

13
08:14:31,520 --> 08:14:36,650
in classification the goal is to predict the category of the input data 

14
08:14:37,740 --> 08:14:43,102
an example of this is predicting the weather as being sunny 

15
08:14:43,102 --> 08:14:46,850
rainy windy or cloudy in this case 

16
08:14:46,850 --> 08:14:52,820
another example is to classify a tumor as either benign or malignant 

17
08:14:54,170 --> 08:15:00,270
in this case the classification is referred to as binary classification 

18
08:15:00,270 --> 08:15:02,830
since there are only two categories 

19
08:15:02,830 --> 08:15:05,660
but you can have many categories as well 

20
08:15:05,660 --> 08:15:10,530
as the weather prediction problem shown here having four categories 

21
08:15:10,530 --> 08:15:14,780
another example is to identify handwritten digits as

22
08:15:14,780 --> 08:15:18,860
being in one of the ten categories from zero to nine 

23
08:15:21,160 --> 08:15:26,310
when your model has to predict a numeric value instead of a category 

24
08:15:26,310 --> 08:15:29,080
then the task becomes a regression problem 

25
08:15:30,380 --> 08:15:34,210
an example of regression is to predict the price of a stock 

26
08:15:35,410 --> 08:15:39,250
the stock price is a numeric value not a category 

27
08:15:39,250 --> 08:15:42,560
so this is a regression task instead of a classification task 

28
08:15:44,300 --> 08:15:48,020
other examples of regression are estimating the weekly sales of a new

29
08:15:48,020 --> 08:15:52,740
product and predicting the score on a test 

30
08:15:52,740 --> 08:15:58,910
in clustering the goal is to organize similar items into groups 

31
08:15:58,910 --> 08:16:04,390
an example is grouping a company customer base into distinct segments for

32
08:16:04,390 --> 08:16:09,360
more effective targeted marketing like seniors adults and

33
08:16:09,360 --> 08:16:11,420
teenagers as we see here 

34
08:16:11,420 --> 08:16:16,050
another such example is identifying areas of similar topography 

35
08:16:16,050 --> 08:16:20,420
like mountains deserts plains for land use application 

36
08:16:20,420 --> 08:16:25,300
yet another example is determining different groups of weather patterns 

37
08:16:25,300 --> 08:16:28,150
like rainy cold or snowy 

38
08:16:28,150 --> 08:16:31,740
the goal in association analysis is to come up with a set

39
08:16:31,740 --> 08:16:36,330
of rules to capture associations within items or events 

40
08:16:36,330 --> 08:16:40,935
the rules are used to determine when items or events occur together 

41
08:16:40,935 --> 08:16:45,300
a common application of association analysis is known as market

42
08:16:45,300 --> 08:16:50,710
basket analysis which is used to understand customer purchasing behavior 

43
08:16:50,710 --> 08:16:55,660
for example association analysis can reveal that banking customers

44
08:16:55,660 --> 08:17:00,040
who have certificate of deposit accounts surety cds also

45
08:17:00,040 --> 08:17:04,790
tend to be interested in other investment vehicles such as money market accounts 

46
08:17:04,790 --> 08:17:08,000
this information can be used for cross - selling 

47
08:17:08,000 --> 08:17:12,140
if you advertise money market accounts to your customers with cds 

48
08:17:12,140 --> 08:17:15,100
they are likely to open such an account 

49
08:17:15,100 --> 08:17:20,150
according to data mining folklore a supermarket chain used association

50
08:17:20,150 --> 08:17:24,990
analysis to discover a connection between two seemingly unrelated products 

51
08:17:24,990 --> 08:17:29,940
they discovered that many customers who go to the supermarket late on sunday night

52
08:17:29,940 --> 08:17:35,330
to buy diapers also tend to buy beer who are likely to be fathers 

53
08:17:35,330 --> 08:17:38,590
this information was then used to place beer and

54
08:17:38,590 --> 08:17:43,720
diapers close together and they saw a jump in sales of both items 

55
08:17:43,720 --> 08:17:46,760
this is the famous diaper beer connection 

56
08:17:46,760 --> 08:17:51,090
when your data can be transformed into a graph representation with nodes and

57
08:17:51,090 --> 08:17:55,330
links then you want to use graph analytics to analyze your data 

58
08:17:55,330 --> 08:17:59,220
this kind of data comes about when you have a lot of entities and

59
08:17:59,220 --> 08:18:03,150
connections between those entities like social networks 

60
08:18:03,150 --> 08:18:08,111
some examples where graph analytics can be useful are exploring the spread of

61
08:18:08,111 --> 08:18:12,780
a disease or epidemic by analyzing hospitals and doctors records 

62
08:18:12,780 --> 08:18:17,849
identification of security threats by monitoring social media 

63
08:18:17,849 --> 08:18:19,483
email and text data 

64
08:18:19,483 --> 08:18:24,015
and optimization of mobile communications network traffic 

65
08:18:24,015 --> 08:18:29,110
and optimization of mobile telecommunications network traffic 

66
08:18:29,110 --> 08:18:32,690
to ensure call quality and reduce dropped calls 

67
08:18:32,690 --> 08:18:36,850
modeling starts with selecting one of the techniques we listed

68
08:18:36,850 --> 08:18:41,680
as the appropriate analysis technique depending on the type of problem you have 

69
08:18:41,680 --> 08:18:45,690
then you construct the model using the data you have prepared 

70
08:18:45,690 --> 08:18:50,210
to validate the model you apply it to new data samples 

71
08:18:50,210 --> 08:18:52,960
this is to evaluate how well the model

72
08:18:52,960 --> 08:18:55,540
does on data that was used to construct it 

73
08:18:55,540 --> 08:18:59,870
the common practice is to divide the prepared data into a set of data for

74
08:18:59,870 --> 08:19:03,170
constructing the model and reserving some of the data for

75
08:19:03,170 --> 08:19:06,330
evaluating the model after it has been constructed 

76
08:19:06,330 --> 08:19:10,740
you can also use new data prepared the same way as with the data that was used to

77
08:19:10,740 --> 08:19:12,060
construct model 

78
08:19:12,060 --> 08:19:16,950
evaluating the model depends on the type of analysis techniques you used 

79
08:19:16,950 --> 08:19:20,595
let briefly look at how to evaluate each technique 

80
08:19:20,595 --> 08:19:25,595
for classification and regression you will have the correct output for

81
08:19:25,595 --> 08:19:27,655
each sample in your input data 

82
08:19:27,655 --> 08:19:29,945
comparing the correct output and

83
08:19:29,945 --> 08:19:34,880
the output predicted by the model provides a way to evaluate the model 

84
08:19:34,880 --> 08:19:38,150
for clustering the groups resulting from clustering

85
08:19:38,150 --> 08:19:41,850
should be examined to see if they make sense for your application 

86
08:19:41,850 --> 08:19:46,250
for example do the customer segments reflect your customer base 

87
08:19:46,250 --> 08:19:50,310
are they helpful for use in your targeted marketing campaigns 

88
08:19:50,310 --> 08:19:52,170
for association analysis and

89
08:19:52,170 --> 08:19:57,940
graph analysis some investigation will be needed to see if the results are correct 

90
08:19:57,940 --> 08:20:00,920
for example network traffic delays

91
08:20:00,920 --> 08:20:06,220
need to be investigated to see what your model predicts is actually happening 

92
08:20:06,220 --> 08:20:09,830
and whether the sources of the delays are where they are predicted

93
08:20:09,830 --> 08:20:11,050
to be in the real system 

94
08:20:12,160 --> 08:20:17,450
after you have evaluated your model to get a sense of its performance on your data 

95
08:20:17,450 --> 08:20:20,510
you will be able to determine the next steps 

96
08:20:20,510 --> 08:20:24,000
some questions to consider are should the analysis be

97
08:20:24,000 --> 08:20:28,340
performed with more data in order to get a better model performance 

98
08:20:28,340 --> 08:20:30,760
would using different data types help 

99
08:20:30,760 --> 08:20:32,990
for example in your clustering results 

100
08:20:32,990 --> 08:20:37,070
is it difficult to distinguish customers from distinct regions 

101
08:20:37,070 --> 08:20:40,050
would adding zip code to your input data help

102
08:20:40,050 --> 08:20:43,150
to generate finer grained customer segments 

103
08:20:43,150 --> 08:20:47,060
do the analysis results suggest a more detailed look at

104
08:20:47,060 --> 08:20:48,900
some aspect of the problem 

105
08:20:48,900 --> 08:20:52,690
for example predicting sunny weather gives very good results 

106
08:20:52,690 --> 08:20:55,390
but rainy weather predictions are just so - so 

107
08:20:55,390 --> 08:21:00,840
this means that you should take a closer look at your examples for rainy weather 

108
08:21:00,840 --> 08:21:04,520
perhaps you just need more samples of rainy weather or

109
08:21:04,520 --> 08:21:07,845
perhaps there are some anomalies in those samples 

110
08:21:07,845 --> 08:21:13,380
or maybe there are some missing data that needs to be included in order

111
08:21:13,380 --> 08:21:15,440
to completely capture rainy weather 

112
08:21:15,440 --> 08:21:20,037
the ideal situation would be that your model platforms very well with respect to

113
08:21:20,037 --> 08:21:22,620
the success criteria that were determined

114
08:21:22,620 --> 08:21:26,380
when you defined the problem at the beginning of the project 

115
08:21:26,380 --> 08:21:29,920
in that case you are ready to move on to communicating and

116
08:21:29,920 --> 08:21:33,870
acting on the results that you obtained from your analysis 

117
08:21:33,870 --> 08:21:38,670
as a summary data analysis involves selecting the appropriate technique for

118
08:21:38,670 --> 08:21:42,940
your problem building the model then evaluating the results 

119
08:21:44,150 --> 08:21:46,516
as there are different types of problems 

120
08:21:46,516 --> 08:21:49,678
there are also different types of analysis techniques 

1
16:35:24,925 --> 16:35:27,646
step four reporting insights 

2
16:35:37,628 --> 16:35:42,498
the fourth step in our data science process is reporting the insights gained

3
16:35:42,498 --> 16:35:43,800
from our analysis 

4
16:35:45,660 --> 16:35:49,840
this is a very important step to communicate your insights and

5
16:35:49,840 --> 16:35:52,320
make a case for what actions should follow 

6
16:35:54,040 --> 16:35:59,380
it can change shape based on your audience and should not be taken lightly 

7
16:36:00,510 --> 16:36:01,760
so how do you get started 

8
16:36:05,150 --> 16:36:10,040
the first thing to do is to look at your analysis results and

9
16:36:10,040 --> 16:36:15,720
decide what to present or report as the biggest value or biggest set of values 

10
16:36:16,990 --> 16:36:21,530
in deciding what to present you should ask yourself these questions 

11
16:36:23,260 --> 16:36:24,740
what is the punchline 

12
16:36:24,740 --> 16:36:27,330
in other words what are the main results 

13
16:36:29,930 --> 16:36:34,590
what added value do these results provide or

14
16:36:34,590 --> 16:36:36,870
how can the model add to the application 

15
16:36:39,120 --> 16:36:43,840
how do the results compare to the success criteria determined at

16
16:36:43,840 --> 16:36:45,260
the beginning of the project 

17
16:36:48,030 --> 16:36:52,220
answers to these questions are the items you need to include in your report or

18
16:36:52,220 --> 16:36:52,990
presentation 

19
16:36:54,120 --> 16:36:58,336
so make them the main topics and gather facts to back them up 

20
16:37:01,198 --> 16:37:06,330
keep in mind that not all of your results may be rosy 

21
16:37:06,330 --> 16:37:11,650
your analysis may show results that are counter to what you were hoping to find 

22
16:37:11,650 --> 16:37:15,410
or results that are inconclusive or puzzling 

23
16:37:16,600 --> 16:37:18,530
you need to show these results as well 

24
16:37:20,620 --> 16:37:24,950
domain experts may find some of these results to be puzzling and

25
16:37:24,950 --> 16:37:28,932
inconclusive findings may lead to additional analysis 

26
16:37:30,020 --> 16:37:33,920
remember the point of reporting your findings

27
16:37:33,920 --> 16:37:36,410
is to determine what the next step should be 

28
16:37:39,070 --> 16:37:43,538
all findings must be presented so that informed decisions can be made 

29
16:37:46,818 --> 16:37:51,180
visualization is an important tool in presenting your results 

30
16:37:52,220 --> 16:37:57,400
the techniques that we discuss and explore in data can be used here as well 

31
16:37:57,400 --> 16:37:58,760
what were they 

32
16:37:58,760 --> 16:38:03,690
scatter plots line graphs heat maps and other types of graphs

33
16:38:05,040 --> 16:38:08,390
are effective ways to present your results visually 

34
16:38:10,330 --> 16:38:14,150
this time you are not plotting the input data but

35
16:38:14,150 --> 16:38:17,250
you are plotting the output data with similar tools 

36
16:38:19,120 --> 16:38:23,238
you should also have tables with details from your analysis as backups 

37
16:38:23,238 --> 16:38:26,418
if someone wants to take a deeper dive into the results 

38
16:38:29,398 --> 16:38:32,080
there are many visualization tools that are available 

39
16:38:33,660 --> 16:38:36,750
some of the most popular open source ones are listed here 

40
16:38:38,205 --> 16:38:42,370
r is a software package for general data analysis 

41
16:38:43,990 --> 16:38:46,830
it has powerful visualization capabilities as well 

42
16:38:48,300 --> 16:38:52,800
python is a general purpose programming language

43
16:38:52,800 --> 16:38:57,090
that also has a number of packages to support data analysis and graphics 

44
16:38:58,510 --> 16:39:01,690
d3 is a javascript library for

45
16:39:01,690 --> 16:39:06,820
producing interactive web based visualizations and data driven documents 

46
16:39:08,500 --> 16:39:13,325
leaflet is a lightweight mobile friendly javascript library

47
16:39:13,325 --> 16:39:15,650
to create interactive maps 

48
16:39:17,360 --> 16:39:22,656
tableau public allows you to create visualizations 

49
16:39:22,656 --> 16:39:29,916
in your public profile and share them or put them on a site or blog 

50
16:39:29,916 --> 16:39:34,925
google charts provides cross - browser compatibility 

51
16:39:34,925 --> 16:39:39,940
and closed platform portability to iphones and android 

52
16:39:41,430 --> 16:39:49,865
timeline is a javascript library that allows you to create timelines 

53
16:39:49,865 --> 16:39:55,787
in summary you want to report your findings by presenting your results and

54
16:39:55,787 --> 16:39:59,780
value add with graphs using visualization tools 

1
09:15:23,010 --> 09:15:26,922
step 5 act turning insights into action 

2
09:15:36,758 --> 09:15:41,048
now that you have evaluated the results from your analysis and

3
09:15:41,048 --> 09:15:46,329
generated reports on the potential value of the results the next step is to

4
09:15:46,329 --> 09:15:52,075
determine what action or actions should be taken based on the insights gained 

5
09:15:52,075 --> 09:15:55,290
remember why we started bringing together the data and

6
09:15:55,290 --> 09:15:57,570
analyzing it in the first place 

7
09:15:57,570 --> 09:16:02,100
to find actionable insights within all these data sets 

8
09:16:02,100 --> 09:16:05,970
to answer questions or for improving business processes 

9
09:16:07,120 --> 09:16:08,470
for example 

10
09:16:08,470 --> 09:16:12,440
is there something in your process that should change to remove bottle necks 

11
09:16:13,580 --> 09:16:17,640
is there data that should be added to your application to make it more accurate 

12
09:16:18,810 --> 09:16:23,440
should you segment your population into more well defined groups for

13
09:16:23,440 --> 09:16:25,240
more effective targeted marketing 

14
09:16:26,510 --> 09:16:30,508
this is the first step in turning insights into action 

15
09:16:30,508 --> 09:16:33,733
now that you have determined what action to take 

16
09:16:33,733 --> 09:16:38,450
the next step is figuring out how to implement the action 

17
09:16:38,450 --> 09:16:42,610
what is necessary to add this action into your process or application 

18
09:16:43,820 --> 09:16:45,080
how should it be automated 

19
09:16:46,440 --> 09:16:52,110
the stakeholders need to be identified and become involved in this change 

20
09:16:52,110 --> 09:16:57,090
just as with any process improvement changes we need to monitor and

21
09:16:57,090 --> 09:17:01,020
measure the impact of the action on the process or application 

22
09:17:02,300 --> 09:17:06,600
assessing the impact leads to an evaluation 

23
09:17:06,600 --> 09:17:10,710
evaluating results from the implemented action will determine your next steps 

24
09:17:11,740 --> 09:17:16,640
is there additional analysis that need to be performed in order to yield

25
09:17:16,640 --> 09:17:17,730
even better results 

26
09:17:19,100 --> 09:17:20,740
what data should be revisited 

27
09:17:21,950 --> 09:17:25,980
are there additional opportunities that should be explored 

28
09:17:25,980 --> 09:17:30,685
for example let not forget what big data enables us to do 

29
09:17:30,685 --> 09:17:37,553
real - time actions based on high velocity streaming information 

30
09:17:37,553 --> 09:17:42,067
we need to define what part of our business needs real - time action to be

31
09:17:42,067 --> 09:17:46,670
able to influence the operations or the interaction with the customer 

32
09:17:48,450 --> 09:17:53,430
once we define these real time actions we need to make sure that

33
09:17:53,430 --> 09:17:58,080
there are automated systems or processes to perform such actions and

34
09:17:58,080 --> 09:18:02,320
provide failure recovery in case of problems 

35
09:18:02,320 --> 09:18:04,547
as a summary big data and

36
09:18:04,547 --> 09:18:10,320
data science are only useful if the insights can be turned into action 

37
09:18:10,320 --> 09:18:15,198
and if the actions are carefully defined and evaluated 

1
18:33:37,360 --> 18:33:39,545
steps in the data science process 

2
18:33:47,405 --> 18:33:52,739
we have already seen a simple linear form of data science process 

3
18:33:52,739 --> 18:33:57,900
including five distinct activities that depend on each other 

4
18:33:59,140 --> 18:34:04,095
let summarize each activity further before we go into the details of each 

5
18:34:04,095 --> 18:34:10,617
acquire includes anything that makes us retrieve data including; finding 

6
18:34:10,617 --> 18:34:15,210
accessing acquiring and moving data 

7
18:34:15,210 --> 18:34:21,640
it includes identification of and authenticated access to all related data 

8
18:34:21,640 --> 18:34:26,000
and transportation of data from sources to distributed files systems 

9
18:34:27,720 --> 18:34:33,340
it includes way to subset and match the data to regions or times of interest 

10
18:34:33,340 --> 18:34:36,830
as we sometimes refer to it as geo - spacial query 

11
18:34:38,150 --> 18:34:44,010
the next activity is prepare data we divide the pre - data activity 

12
18:34:44,010 --> 18:34:47,574
into two steps based on the nature of the activity 

13
18:34:47,574 --> 18:34:52,990
namely explore data and pre - process data 

14
18:34:52,990 --> 18:34:57,890
the first step in data preparation involves literally looking

15
18:34:57,890 --> 18:35:02,750
at the data to understand its nature what it means its quality and format 

16
18:35:04,130 --> 18:35:08,320
it often takes a preliminary analysis of data or

17
18:35:08,320 --> 18:35:10,230
samples of data to understand it 

18
18:35:11,440 --> 18:35:14,230
this is why this step is called explore 

19
18:35:15,350 --> 18:35:19,100
once we know more about the data through exploratory analysis 

20
18:35:19,100 --> 18:35:22,625
the next step is pre - processing of data for analysis 

21
18:35:22,625 --> 18:35:28,180
pre - processing includes cleaning data sub - setting or

22
18:35:28,180 --> 18:35:33,660
filtering data creating data which programs can read and

23
18:35:33,660 --> 18:35:39,660
understand such as modeling raw data into a more defined data model 

24
18:35:39,660 --> 18:35:43,260
or packaging it using a specific data format 

25
18:35:44,810 --> 18:35:47,630
if there are multiple data sets involved 

26
18:35:47,630 --> 18:35:53,300
this step also includes integration of multiple data sources or streams 

27
18:35:53,300 --> 18:35:58,070
the prepared data then would be passed onto the analysis step 

28
18:35:58,070 --> 18:36:00,590
which involves selection of analytical techniques to use 

29
18:36:01,800 --> 18:36:05,190
building a model of the data and analyzing results 

30
18:36:06,330 --> 18:36:10,120
this step can take a couple of iterations on its own or

31
18:36:10,120 --> 18:36:13,180
might require data scientists to go back to steps one and

32
18:36:13,180 --> 18:36:18,350
two to get more data or package data in a different way 

33
18:36:18,350 --> 18:36:24,070
step four for communicating results includes evaluation of analytical results 

34
18:36:24,070 --> 18:36:28,400
presenting them in a visual way creating reports that include

35
18:36:28,400 --> 18:36:32,080
an assessment of results with respect to success criteria 

36
18:36:33,100 --> 18:36:37,690
activities in this step can often be referred to with terms like interpret 

37
18:36:37,690 --> 18:36:40,780
summarize visualize or post process 

38
18:36:41,900 --> 18:36:46,390
the last step brings us back to the very first reason we do data science 

39
18:36:47,680 --> 18:36:48,240
the purpose 

40
18:36:49,340 --> 18:36:54,470
reporting insights from analysis and determining actions from insights based

41
18:36:54,470 --> 18:36:59,300
on the purpose you initially defined is what we refer to as the act step 

42
18:37:00,700 --> 18:37:05,010
we have now seen all the steps in a typical data science process 

43
18:37:06,220 --> 18:37:11,320
please note that this is an iterative process and findings from one step

44
18:37:11,320 --> 18:37:14,970
may require the previous step to be repeated with new information 

1
13:10:52,391 --> 13:10:56,640
the sixth v value 

2
13:10:56,640 --> 13:10:57,970
in this module 

3
13:10:57,970 --> 13:11:03,160
we described the five ways which are considered to be dimensions of big data 

4
13:11:04,190 --> 13:11:09,947
each way presented a challenging dimension of big data namely 

5
13:11:09,947 --> 13:11:15,500
size complexity speed quality and connectedness 

6
13:11:17,480 --> 13:11:22,850
although we can list some other rays base on the context we prefer to list these

7
13:11:22,850 --> 13:11:28,490
five s fundamental dimensions that this big data specialization helps you work on 

8
13:11:29,570 --> 13:11:33,940
however at the heart of the big data challenge

9
13:11:33,940 --> 13:11:38,860
is turning all of the other dimensions into truly useful business value 

10
13:11:40,110 --> 13:11:44,640
the idea behind processing all this big data in the first place

11
13:11:44,640 --> 13:11:46,640
is to bring value to the problem at hand 

12
13:11:47,830 --> 13:11:52,230
in week two we will explore how to take the first steps into

13
13:11:52,230 --> 13:11:55,270
starting to generate value out of big data 

14
13:11:56,430 --> 13:12:02,465
now that we saw all the ways let focus on an example of a big data challenge 

15
13:12:02,465 --> 13:12:06,940
let imagine now that you are part of a company called eglence inc 

16
13:12:08,160 --> 13:12:12,992
one of the products of eglence inc is a highly popular mobile game

17
13:12:12,992 --> 13:12:15,506
called catch the pink flamingo 

18
13:12:15,506 --> 13:12:20,046
it a multi - user game where the users have to catch special types

19
13:12:20,046 --> 13:12:24,188
of pink flamingos that randomly pop up on the world map on their

20
13:12:24,188 --> 13:12:28,670
screens based on the mission that gets updated randomly 

21
13:12:28,670 --> 13:12:33,120
the game is played by millions of people online throughout the world 

22
13:12:33,120 --> 13:12:37,110
one of the goals of the game is to form a network of players to collectively

23
13:12:37,110 --> 13:12:42,470
cover the world map with pink flamingo sightings and compete other groups 

24
13:12:42,470 --> 13:12:45,670
users can pick their groups based on player stats 

25
13:12:46,780 --> 13:12:51,530
the game website sends free cool stuff to registered users 

26
13:12:51,530 --> 13:12:57,483
registration requires users to enter demographic information such gender 

27
13:12:57,483 --> 13:13:02,542
year of birth city highest education and things like that 

28
13:13:02,542 --> 13:13:07,675
however most of the users enter inaccurate information about themselves 

29
13:13:07,675 --> 13:13:10,140
just like most of us do 

30
13:13:10,140 --> 13:13:14,980
to help improve the game the game collects realtime usage activity

31
13:13:14,980 --> 13:13:19,740
data from each player and feeds them to it data servers 

32
13:13:19,740 --> 13:13:25,290
the players of this game are enthusiastically active on social media 

33
13:13:25,290 --> 13:13:28,480
and have strong associations with the game 

34
13:13:28,480 --> 13:13:33,424
a popular twitter hashtag for this game is catchthepinkflamingo 

35
13:13:33,424 --> 13:13:38,780
which gets more than 200 000 mentions worldwide per day 

36
13:13:38,780 --> 13:13:43,695
there are strong communities of users who meet via social media and

37
13:13:43,695 --> 13:13:45,984
get together to play the game 

38
13:13:45,984 --> 13:13:52,960
now imagine yourself as the big data solutions architect for fun games inc 

39
13:13:52,960 --> 13:13:57,640
there are definitely examples of all three types of data sources in this example 

40
13:13:58,920 --> 13:14:03,510
the mobile app generates data for the analysis of user activity 

41
13:14:03,510 --> 13:14:08,407
twitter conversations of players form a rich source of unstructured data from

42
13:14:08,407 --> 13:14:09,005
people 

43
13:14:09,005 --> 13:14:10,191
and the customer and

44
13:14:10,191 --> 13:14:14,250
game records are examples of data that this organization collects 

45
13:14:16,160 --> 13:14:19,010
this is a challenging big data example

46
13:14:19,010 --> 13:14:22,680
where all characteristics of big data are represented 

47
13:14:22,680 --> 13:14:25,240
there are high volumes of player game and

48
13:14:25,240 --> 13:14:29,560
twitter data which also speaks to the variety of data 

49
13:14:29,560 --> 13:14:33,465
the data streams from the mobile app website and

50
13:14:33,465 --> 13:14:38,979
social media in real - time which can be defined as high velocity data 

51
13:14:38,979 --> 13:14:43,203
the quality of demographic data users enter is not clear and

52
13:14:43,203 --> 13:14:48,334
there are networks of players which are related to the balance of big data 

1
02:25:40,240 --> 02:25:43,918
cloud computing : an important big data enabler 

2
02:25:58,598 --> 02:26:03,586
in our first lecture in this course we mentioned the cloud as one of

3
02:26:03,586 --> 02:26:07,450
the two influences of the launch of the big data era 

4
02:26:08,930 --> 02:26:12,050
we called it on - demand computing and

5
02:26:12,050 --> 02:26:16,390
we said that it enables us to compute any time any anywhere 

6
02:26:16,390 --> 02:26:18,110
simply whenever we demand it 

7
02:26:19,170 --> 02:26:20,540
in this video 

8
02:26:20,540 --> 02:26:25,260
we will see how we deploy the cloud to our benefit in our big data applications 

9
02:26:26,470 --> 02:26:28,990
the main idea behind cloud computing

10
02:26:28,990 --> 02:26:33,340
is to transform computing infrastructure into a commodity 

11
02:26:33,340 --> 02:26:38,220
so application developers can focus on solving application - specific challenges

12
02:26:38,220 --> 02:26:41,880
instead of trying to build infrastructure to run on 

13
02:26:41,880 --> 02:26:43,760
so how does this happen 

14
02:26:43,760 --> 02:26:46,840
we can simply define a cloud computing service 

15
02:26:46,840 --> 02:26:49,390
as a rental service for computing 

16
02:26:49,390 --> 02:26:52,700
you rent what you want and return upon usage 

17
02:26:53,710 --> 02:26:56,650
think about this you would not buy or

18
02:26:56,650 --> 02:27:01,950
even build a truck every time you have to move a piece of furniture 

19
02:27:01,950 --> 02:27:02,880
you would simply rent 

20
02:27:04,560 --> 02:27:07,750
why build a computing cluster when you can rent 

21
02:27:07,750 --> 02:27:10,930
especially if you are not using it all the time 

22
02:27:13,110 --> 02:27:17,900
similarly you can rent a car or a bike when you are on vacation 

23
02:27:17,900 --> 02:27:21,035
so you can bike anytime anywhere 

24
02:27:22,660 --> 02:27:24,970
let dig into this question 

25
02:27:24,970 --> 02:27:29,240
what factors do you consider when you are developing a solution for your yourself or

26
02:27:29,240 --> 02:27:29,800
your client 

27
02:27:30,860 --> 02:27:33,880
should you build a hardware and software resources yourself 

28
02:27:34,930 --> 02:27:39,090
or should you rent these resources from the cloud 

29
02:27:40,860 --> 02:27:44,390
let look at in - house hardware and software resource building first 

30
02:27:46,450 --> 02:27:50,975
if you choose to develop in - house capabilities you have to hire people and

31
02:27:50,975 --> 02:27:53,630
buy hardware that suits your requirements 

32
02:27:54,660 --> 02:28:00,750
these includes but not limited to buying networking hardware 

33
02:28:00,750 --> 02:28:07,040
storage disks upgrading hardware when it becomes obsolete and so on 

34
02:28:07,040 --> 02:28:10,360
not to forget the real estate cost of keeping the hardware 

35
02:28:11,370 --> 02:28:14,190
how do you estimate the size of your hardware needs 

36
02:28:14,190 --> 02:28:17,910
do you make a five year estimate or ten year 

37
02:28:19,920 --> 02:28:22,110
in today fast changing world 

38
02:28:22,110 --> 02:28:24,620
it is becoming harder to estimate future demands 

39
02:28:26,070 --> 02:28:30,220
getting the software that fits your needs is equally challenging 

40
02:28:30,220 --> 02:28:33,700
most software installations require a lot of tweaking and

41
02:28:33,700 --> 02:28:36,030
manual intervention that require a lot of skills 

42
02:28:37,220 --> 02:28:40,025
you will need your engineers to do this 

43
02:28:40,025 --> 02:28:43,300
compatibility issues bring problems that are hard to foresee 

44
02:28:44,340 --> 02:28:47,750
most software is updated on a daily basis 

45
02:28:47,750 --> 02:28:50,210
you must ensure you are updated 

46
02:28:50,210 --> 02:28:54,870
this insures you avoid security risks and get the best 

47
02:28:56,670 --> 02:29:03,770
over all building your own data center or computing power house can be expensive 

48
02:29:03,770 --> 02:29:10,200
and it can be time consuming maintaining it is a task by itself 

49
02:29:10,200 --> 02:29:13,760
this requires high initial capital investments and

50
02:29:13,760 --> 02:29:17,380
efficient operation of several departments in your business 

51
02:29:17,380 --> 02:29:20,540
which you might not have if you are a startup company 

52
02:29:20,540 --> 02:29:24,800
most people forget to include the cost of disposing old hardware 

53
02:29:24,800 --> 02:29:26,788
now lets see what the cloud can do for us 

54
02:29:28,896 --> 02:29:33,660
cloud benefits are similar to what you would get from a rental car company 

55
02:29:35,190 --> 02:29:39,720
you pay for what you use which means a low capital investment 

56
02:29:41,160 --> 02:29:45,680
you do not need to go to the dealership do a negotiation get a bank loan 

57
02:29:45,680 --> 02:29:46,420
get insurance 

58
02:29:47,450 --> 02:29:50,680
that means quick implementation of your projects 

59
02:29:51,930 --> 02:29:55,350
just like you do not need to buy a car if you only need a car for

60
02:29:55,350 --> 02:30:00,200
a limited use deploying your application on a server that is geographically

61
02:30:00,200 --> 02:30:05,200
closer to your client can give you fast service and happy customers 

62
02:30:07,060 --> 02:30:12,430
for startup and small business it can be challenging to do so 

63
02:30:12,430 --> 02:30:14,620
cloud lets you do this with a click 

64
02:30:16,030 --> 02:30:20,950
you can be sitting at a coffeeshop or your home and starting your internet business 

65
02:30:20,950 --> 02:30:24,700
without a huge capital investment thanks to the cloud 

66
02:30:24,700 --> 02:30:29,230
and you do not need to have a five or ten year resource estimation plan 

67
02:30:29,230 --> 02:30:33,527
adapt to your requirements faster if your business is growing faster than you

68
02:30:33,527 --> 02:30:34,130
thought 

69
02:30:35,440 --> 02:30:39,330
cloud lets you forget about the resource management problems and

70
02:30:39,330 --> 02:30:44,310
lets you focus on your business products or domain expertise with minimal cost 

71
02:30:45,600 --> 02:30:50,500
just as you can rent a truck or a convertible at a rental car company 

72
02:30:50,500 --> 02:30:53,520
you can build your own custom machine on cloud 

73
02:30:53,520 --> 02:30:57,430
with a custom machine we mean a commodity cluster

74
02:30:57,430 --> 02:31:00,940
made out of the right type of computing nodes for your application 

75
02:31:02,020 --> 02:31:05,930
you pick not only a cpu or a gpu but

76
02:31:05,930 --> 02:31:10,420
pick from a whole menu of compute memory and storage choices 

77
02:31:10,420 --> 02:31:12,590
it a buffet on the cloud 

78
02:31:12,590 --> 02:31:17,900
design machines to suit your application requirements data size and analytics 

79
02:31:19,330 --> 02:31:21,740
get what you want and pay for what you use 

80
02:31:23,130 --> 02:31:25,380
compare this with buying and

81
02:31:25,380 --> 02:31:29,980
maintaining all combinations of hardware that you possibly would use 

82
02:31:29,980 --> 02:31:33,860
that is so costly and not possible at all times 

83
02:31:35,770 --> 02:31:40,710
thanks to all these advantages there are many cloud server providers today 

84
02:31:40,710 --> 02:31:41,990
and the numbers are growing 

85
02:31:43,240 --> 02:31:46,820
here we list some of the players in the cloud computing market 

86
02:31:48,620 --> 02:31:50,940
take a moment to look at them 

87
02:31:50,940 --> 02:31:53,990
you will probably recognize some big names and

88
02:31:53,990 --> 02:31:56,730
some others you have not even heard of before 

89
02:31:58,630 --> 02:32:03,670
as a summary cloud does the heavy lifting so your team can extract

90
02:32:03,670 --> 02:32:08,300
value from data with getting bogged down in the infrastructure details 

91
02:32:09,880 --> 02:32:11,680
cloud provides convenient and

92
02:32:11,680 --> 02:32:16,485
viable solutions for scaling your prototype to a full fledged application 

93
02:32:16,485 --> 02:32:21,400
you can leverage the experts to handle security 

94
02:32:21,400 --> 02:32:24,450
robustness and let them handle the technical issues 

95
02:32:25,480 --> 02:32:29,560
your team can work on utilizing your strengths

96
02:32:29,560 --> 02:32:32,320
to solve your domain specific problem 

1
04:58:12,690 --> 04:58:15,528
cloud service models : exploration of choices 

2
04:58:30,538 --> 04:58:34,709
there are many levels of services that you can get from cloud providers 

3
04:58:35,860 --> 04:58:39,880
any cloud computing discussion will involve terms like application

4
04:58:39,880 --> 04:58:44,800
as a service platform as a service and infrastructure as a service 

5
04:58:46,450 --> 04:58:50,660
all of these refer to business models around using the cloud

6
04:58:50,660 --> 04:58:55,460
with different levels of engagement and servicing similar to rental agreements 

7
04:58:57,860 --> 04:59:05,470
iaas infrastructure as a service can be defined as a bare minimum rental service 

8
04:59:07,030 --> 04:59:12,630
this is like renting a truck from a company that you can assume has hardware

9
04:59:12,630 --> 04:59:17,978
and you do the packing of your furniture and drive to your new house 

10
04:59:21,118 --> 04:59:25,430
you as the user of the service install and maintain an operating system 

11
04:59:25,430 --> 04:59:29,400
and other applications in the infrastructure as a service model 

12
04:59:30,810 --> 04:59:34,180
the amazon ec2 cloud is a good example for this model 

13
04:59:36,470 --> 04:59:39,940
paas platform as a service 

14
04:59:39,940 --> 04:59:45,840
is the model where a user is provided with an entire computing platform 

15
04:59:45,840 --> 04:59:50,130
this could include the operating system and programming languages that you need 

16
04:59:52,310 --> 04:59:57,440
it could extend to include the database of your choice or even a web server 

17
04:59:57,440 --> 04:59:58,920
you can develop and

18
04:59:58,920 --> 05:00:02,150
run your own application software on top of these layers 

19
05:00:04,120 --> 05:00:09,510
the google app engine and microsoft azure are two examples of this model 

20
05:00:11,460 --> 05:00:17,770
saas the software as a service model is the model

21
05:00:17,770 --> 05:00:23,090
in which the cloud service provider takes the responsibilities for the hardware and

22
05:00:23,090 --> 05:00:28,710
software environment such as the operating system and the application software 

23
05:00:29,940 --> 05:00:35,720
this means you can work on using the application to solve your problem 

24
05:00:38,180 --> 05:00:41,848
dropbox is a very popular software as a service platform 

25
05:00:44,638 --> 05:00:49,205
ultimately the decision of which service you want to explore is a function of

26
05:00:49,205 --> 05:00:50,460
several variables 

27
05:00:51,470 --> 05:00:55,760
it depends on the skill level of your team to handle computing environment 

28
05:00:55,760 --> 05:00:58,080
development and maintenance 

29
05:00:58,080 --> 05:01:01,010
it also depends on how you might need to use the service 

30
05:01:03,170 --> 05:01:05,990
you need to pick the right service model that

31
05:01:05,990 --> 05:01:08,520
best fits you in terms of long term goals 

32
05:01:09,590 --> 05:01:14,610
finally when you are deploying a cloud service you also have to understand

33
05:01:14,610 --> 05:01:19,480
all the security risks since your data resides on third party service 

34
05:01:21,730 --> 05:01:26,026
security is a very important aspect in today world of growing

35
05:01:26,026 --> 05:01:28,207
digitization of information 

36
05:01:28,207 --> 05:01:31,990
you must make your client data safety a top priority 

37
05:01:31,990 --> 05:01:35,859
and hence this should be an important criteria in your decision 

38
05:01:37,790 --> 05:01:41,870
all the security risks must be understood and

39
05:01:41,870 --> 05:01:45,000
evaluated as your data resides on third party servers 

40
05:01:47,670 --> 05:01:50,370
we are seeing other forms of services being added

41
05:01:50,370 --> 05:01:52,080
to the family of cloud services 

42
05:01:53,260 --> 05:01:55,710
the logic of infrastructure platform and

43
05:01:55,710 --> 05:01:58,550
software as a service is getting extended further 

44
05:02:00,040 --> 05:02:03,580
xaas is an umbrella term that signifies

45
05:02:03,580 --> 05:02:07,980
even finer - grain control over computing resources that you want to rent 

46
05:02:07,980 --> 05:02:13,520
for example storage as a service communication as a service 

47
05:02:13,520 --> 05:02:15,410
marketing as a service and so on 

48
05:02:16,990 --> 05:02:24,370
as a summary infrastructure as a service platform as a service and application

49
05:02:24,370 --> 05:02:28,800
as a service are three main class service models that are being used with success 

50
05:02:29,960 --> 05:02:35,788
picking one will depend on the number of variables which are company goals 

51
05:02:35,788 --> 05:02:38,840
these three models have inspired many similar models

52
05:02:38,840 --> 05:02:40,880
to emerge around cloud computing 

1
10:00:50,210 --> 10:00:54,331
in this lecture we will be learning about the basic file manipulation commands for

2
10:00:54,331 --> 10:00:55,953
the hadoop file system or hdfs 

3
10:00:55,953 --> 10:00:59,440
we will first start by downloading a text file of words 

4
10:00:59,440 --> 10:01:05,040
we will use this text file to copy to and from the local file system in hdfs and

5
10:01:05,040 --> 10:01:07,730
later on we will use it to run word count on 

6
10:01:08,770 --> 10:01:11,960
after we download the text file we will open a terminal shell and

7
10:01:11,960 --> 10:01:15,220
copy the text file from the local file system to hdfs 

8
10:01:16,310 --> 10:01:19,870
next we will copy the file within hdfs and

9
10:01:19,870 --> 10:01:24,880
also see how to copy file from hdfs to the local file system 

10
10:01:24,880 --> 10:01:28,223
finally we will see how to delete a file in hdfs 

11
10:01:28,223 --> 10:01:29,410
let start 

12
10:01:29,410 --> 10:01:32,390
we are going to download text file to copy into hdfs 

13
10:01:32,390 --> 10:01:36,610
it does not matter what the contents of the text file is so we will download

14
10:01:36,610 --> 10:01:40,380
the complete works of shakespeare since it contains interesting text 

15
10:01:43,343 --> 10:01:46,840
first click on the icon here to launch a web browser 

16
10:01:50,860 --> 10:01:53,780
now we will search google for the complete works of shakespeare 

17
10:02:07,633 --> 10:02:09,880
i am going to be using this first link here 

18
10:02:09,880 --> 10:02:13,500
and we will provide this link in the reading section 

19
10:02:16,500 --> 10:02:20,080
so this is the complete works of shakespeare and

20
10:02:20,080 --> 10:02:25,700
we will save it to a text file in the local file system by clicking on the icon here 

21
10:02:25,700 --> 10:02:30,790
the open menu and selecting save page 

22
10:02:33,690 --> 10:02:37,105
so we will call it words txt 

23
10:02:37,105 --> 10:02:42,090
and the save in folder it going to save it into the downloads directory 

24
10:02:47,733 --> 10:02:52,800
once that completes we will open a terminal window by clicking on the icon here 

25
10:02:56,183 --> 10:03:03,480
so if we go into the downloads ddrectory by typing a cd downloads and

26
10:03:03,480 --> 10:03:11,320
running ls we can see that words txt was successfully downloaded 

27
10:03:14,120 --> 10:03:21,100
moving on let copy words txt from the local file system to the hdfs file system 

28
10:03:21,100 --> 10:03:28,711
the command to do this is hadoop fs - copyfromlocal words txt 

29
10:03:30,850 --> 10:03:33,330
when i run this it will copy it from the local directory and

30
10:03:33,330 --> 10:03:35,595
local file system to hdfs 

31
10:03:37,710 --> 10:03:42,681
we can see that the file was copied by running hadoop fs - ls 

32
10:03:42,681 --> 10:03:47,630
you can see that the file was successfully copied 

33
10:03:50,270 --> 10:03:55,673
next we can copy this file to another file within hdfs 

34
10:03:55,673 --> 10:04:04,170
we can do this by running hadoop fs - cp words txt words2 txt 

35
10:04:04,170 --> 10:04:10,500
the first words txt is the file that already exists in hdfs 

36
10:04:12,160 --> 10:04:14,760
the second words 2 txt

37
10:04:14,760 --> 10:04:17,230
is the new file that we are going to create when we run this command 

38
10:04:18,340 --> 10:04:23,182
let run it and again we are can run

39
10:04:23,182 --> 10:04:28,033
hadoopfs - ls to see the files in hdfs 

40
10:04:28,033 --> 10:04:34,364
we can see the original file words txt and the copy that was made words2 txt 

41
10:04:36,723 --> 10:04:42,680
let copy words2 txt from hdfs to the local filesystem 

42
10:04:42,680 --> 10:04:50,909
we can do this by running hadoop fs - copytolocal words2 txt 

43
10:04:53,813 --> 10:04:59,210
after i run this command i can call ls to see the contents of the local file system 

44
10:05:01,570 --> 10:05:06,455
so now we have the new file words2 txt which we have just copied from hdfs 

45
10:05:09,640 --> 10:05:12,980
the last step in this lecture is to delete a file in hdfs 

46
10:05:12,980 --> 10:05:18,881
we can delete words2 txt by running hadoop fs 

47
10:05:18,881 --> 10:05:22,404
but is rn words2 txt 

48
10:05:25,330 --> 10:05:27,930
as you can see it printed that it deleted the file 

49
10:05:27,930 --> 10:05:31,806
we can also run hadoop fs - ls to verify that the file is deleted 

50
10:05:35,246 --> 10:05:40,872
you can see that there only the original words txt and words2 txt was deleted 

1
20:06:30,710 --> 20:06:32,260
getting started 

2
20:06:32,260 --> 20:06:34,310
why do we worry about foundations 

3
20:06:44,575 --> 20:06:49,079
starting next week you will start diving into the details of the hadoop

4
20:06:49,079 --> 20:06:51,650
framework for big data 

5
20:06:51,650 --> 20:06:56,460
before you start taking a little bit of time to understand some core

6
20:06:56,460 --> 20:07:01,580
concepts will help you to digest the information on hadoop better and faster 

7
20:07:02,950 --> 20:07:06,670
imagine yourself attending a chemistry lab 

8
20:07:06,670 --> 20:07:10,700
before you start hearing about the tubes and mixtures you really need

9
20:07:10,700 --> 20:07:15,650
to understand the chemistry or theory of the practical concepts in the lab 

10
20:07:15,650 --> 20:07:19,650
similarly learning these concepts now will help

11
20:07:19,650 --> 20:07:23,880
you with your understanding of the practical concepts in the hadoop lectures 

12
20:07:25,000 --> 20:07:30,290
in addition we want to prepare you to understand the tools beyond hadoop 

13
20:07:30,290 --> 20:07:36,410
any big data system that you find will be built on these core concepts 

14
20:07:36,410 --> 20:07:39,200
so these foundations will help you beyond this course 

15
20:07:40,280 --> 20:07:41,460
now let get started 

1
16:14:13,022 --> 16:14:14,322
getting started 

2
16:14:14,322 --> 16:14:16,063
why hadoop 

3
16:14:16,063 --> 16:14:18,254
we have all heard that hadoop and

4
16:14:18,254 --> 16:14:23,150
related projects in this ecosystem are great for big data 

5
16:14:23,150 --> 16:14:29,332
this module will answer the four ws and an h about why this statement is true 

6
16:14:41,473 --> 16:14:44,788
before we dive further into the details of hadoop 

7
16:14:44,788 --> 16:14:49,890
let take a moment to analyze the characteristics of the hadoop ecosystem 

8
16:14:51,560 --> 16:14:53,680
what is in the ecosystem 

9
16:14:53,680 --> 16:14:54,890
why is it beneficial 

10
16:14:56,150 --> 16:14:57,040
where is it used 

11
16:14:58,420 --> 16:14:59,130
who uses it 

12
16:15:00,910 --> 16:15:02,740
and how do these tools work 

13
16:15:05,830 --> 16:15:10,642
the hadoop ecosystem frameworks and applications that we will describe in this

14
16:15:10,642 --> 16:15:14,670
module have several overarching themes and goals 

15
16:15:14,670 --> 16:15:19,930
first they provide scalability to store large volumes of data

16
16:15:19,930 --> 16:15:21,180
on commodity hardware 

17
16:15:23,160 --> 16:15:27,820
as the number of systems increases so does the chance for crashes and

18
16:15:27,820 --> 16:15:29,510
hardware failures 

19
16:15:29,510 --> 16:15:34,570
a second goal supported by most frameworks in the hadoop ecosystem 

20
16:15:34,570 --> 16:15:37,590
is the ability to gracefully recover from these problems 

21
16:15:39,730 --> 16:15:45,070
in addition as we have mentioned before big data comes in

22
16:15:45,070 --> 16:15:51,560
a variety of flavors such as text files graph of social networks 

23
16:15:51,560 --> 16:15:57,681
streaming sensor data and raster images 

24
16:15:57,681 --> 16:16:01,250
a third goal for the hadoop ecosystem then 

25
16:16:01,250 --> 16:16:06,920
is the ability to handle these different data types for any given type of data 

26
16:16:08,040 --> 16:16:11,590
you can find several projects in the ecosystem that support it 

27
16:16:13,350 --> 16:16:16,890
a fourth goal of the hadoop ecosystem

28
16:16:16,890 --> 16:16:19,998
is the ability to facilitate a shared environment 

29
16:16:19,998 --> 16:16:26,030
since even modest - sized clusters can have many cores 

30
16:16:26,030 --> 16:16:29,640
it is important to allow multiple jobs to execute simultaneously 

31
16:16:31,070 --> 16:16:34,450
why buy servers only to let them sit idle 

32
16:16:36,670 --> 16:16:41,750
another goal of the hadoop ecosystem is providing value for your enterprise 

33
16:16:43,910 --> 16:16:48,630
the ecosystem includes a wide range of open source projects

34
16:16:48,630 --> 16:16:51,270
backed by a large active community 

35
16:16:52,640 --> 16:16:57,500
these projects are free to use and easy to find support for 

36
16:16:59,640 --> 16:17:01,750
in the following lectures in this module 

37
16:17:01,750 --> 16:17:06,260
we will take a more detailed look at the hadoop ecosystem 

38
16:17:06,260 --> 16:17:10,590
first we will explore the kinds of projects available and

39
16:17:10,590 --> 16:17:12,760
the types of capabilities they provide 

40
16:17:14,100 --> 16:17:20,300
next we will take a deeper look at the three main parts of hadoop 

41
16:17:20,300 --> 16:17:23,130
the hadoop distributed file system or hdfs 

42
16:17:24,490 --> 16:17:26,910
yarn the scheduler and resource manager 

43
16:17:28,170 --> 16:17:32,400
and mapreduce a programming model for processing big data 

44
16:17:34,210 --> 16:17:39,060
we will then discuss cloud computing and the types of service models it provides 

45
16:17:40,980 --> 16:17:45,865
we will also describe situations in which hadoop is not the best solution 

46
16:17:47,935 --> 16:17:51,945
this module then concludes with two readings involving hands - on

47
16:17:51,945 --> 16:17:55,245
experience with hdfs and mapreduce 

48
16:17:55,245 --> 16:17:56,325
so let get started 

1
08:32:08,873 --> 08:32:12,670
mapreduce simple programming for big results 

2
08:32:28,723 --> 08:32:33,650
mapreduce is a programming model for the hadoop ecosystem 

3
08:32:33,650 --> 08:32:36,020
it relies on yarn to schedule and

4
08:32:36,020 --> 08:32:40,430
execute parallel processing over the distributed file blocks in hdfs 

5
08:32:41,600 --> 08:32:46,292
there are several tools that use the mapreduce model to provide a higher level

6
08:32:46,292 --> 08:32:48,829
interface to other programming models 

7
08:32:48,829 --> 08:32:53,747
hive has a sql - like interface that adds capabilities that help with

8
08:32:53,747 --> 08:32:56,220
relational data modeling 

9
08:32:56,220 --> 08:32:59,460
and pig is a high level data flow language

10
08:32:59,460 --> 08:33:02,470
that adds capabilities that help with process map modeling 

11
08:33:04,070 --> 08:33:09,200
traditional parallel programming requires expertise on a number of computing and

12
08:33:09,200 --> 08:33:11,160
systems concepts 

13
08:33:11,160 --> 08:33:16,170
for example synchronization mechanisms like locks semaphores 

14
08:33:16,170 --> 08:33:18,460
and monitors are essential 

15
08:33:18,460 --> 08:33:22,450
and incorrectly using them can either crash your program or

16
08:33:22,450 --> 08:33:24,460
severely impact performance 

17
08:33:26,080 --> 08:33:28,600
this high learning curve makes it difficult 

18
08:33:29,930 --> 08:33:34,465
it is also error prone since your code can run on hundreds or

19
08:33:34,465 --> 08:33:37,813
thousands of nodes each having many cores 

20
08:33:37,813 --> 08:33:40,685
and any problem related to these parallel processes 

21
08:33:40,685 --> 08:33:43,130
needs to be handled by your parallel program 

22
08:33:44,510 --> 08:33:50,200
the mapreduce programming model greatly simplifies running code in parallel

23
08:33:50,200 --> 08:33:53,250
since you do not have to deal with any of these issues 

24
08:33:53,250 --> 08:33:59,450
instead you only need to create and map and reduce tasks and you do not 

25
08:33:59,450 --> 08:34:03,670
have to worry about multiple threads synchronization or concurrency issues 

26
08:34:05,900 --> 08:34:08,110
so what is a map and reduce 

27
08:34:09,760 --> 08:34:14,522
map and reduce are two concepts based on functional programming

28
08:34:14,522 --> 08:34:19,160
where the output the function is based solely on the input 

29
08:34:21,010 --> 08:34:27,060
just like in a mathematical function f x = y y depends on x 

30
08:34:28,480 --> 08:34:32,360
you provide a function or operation for a map and reduce 

31
08:34:33,890 --> 08:34:37,570
and the runtime executes it over the data 

32
08:34:37,570 --> 08:34:42,890
for map the operation is applied on each data element 

33
08:34:42,890 --> 08:34:47,110
and in reduce the operation summarizes elements in some manner 

34
08:34:48,510 --> 08:34:54,180
an example using map and reduce will make this concepts more clear 

35
08:34:56,230 --> 08:34:59,180
hello word is a traditional first program you code

36
08:34:59,180 --> 08:35:01,310
when you start to learning programming languages 

37
08:35:02,730 --> 08:35:08,661
the first program to learn or hello word of map reduce is often wordcount 

38
08:35:10,735 --> 08:35:15,128
wordcount reads one or more text files and

39
08:35:15,128 --> 08:35:21,313
counts the number of occurrences of each word in these files 

40
08:35:21,313 --> 08:35:24,654
the output will be a text file with a list of words and

41
08:35:24,654 --> 08:35:27,940
their occurrence frequencies in the input data 

42
08:35:29,320 --> 08:35:32,560
let examine each step of wordcount 

43
08:35:33,820 --> 08:35:38,790
for simplification we are assuming we have one big file as an input 

44
08:35:40,200 --> 08:35:46,040
before wordcount runs the input file is stored in hdfs 

45
08:35:46,040 --> 08:35:47,880
as you know now 

46
08:35:47,880 --> 08:35:52,200
hdfs partitions the blocks across multiple nodes in the cluster 

47
08:35:53,470 --> 08:36:00,503
in this case four partitions labeled a b c and d 

48
08:36:00,503 --> 08:36:05,550
the first step in mapreduce is to run a map operation on each node 

49
08:36:07,100 --> 08:36:12,190
as the input partitions are read from htfs map is called for

50
08:36:12,190 --> 08:36:13,760
each line in the input 

51
08:36:15,020 --> 08:36:19,330
let look at the first lines of the input partitions a and

52
08:36:19,330 --> 08:36:21,810
b and start counting the words 

53
08:36:22,920 --> 08:36:28,162
the first line in the partition on node a 

54
08:36:28,162 --> 08:36:33,133
says my apple is red and my rose is blue 

55
08:36:33,133 --> 08:36:39,990
similarly the first line on partition b says you are the apple of my eye 

56
08:36:41,370 --> 08:36:45,170
let now see what happens in the first map node for partition a 

57
08:36:47,138 --> 08:36:49,910
map creates a key value for

58
08:36:49,910 --> 08:36:56,510
each word on the line containing the word as the key and 1 as the value 

59
08:36:56,510 --> 08:37:02,470
in this example the word apple is read from the line in partition a 

60
08:37:03,550 --> 08:37:08,447
map produces a key value of apple 1 

61
08:37:08,447 --> 08:37:15,273
similarly the word my is seen on the first line of a twice 

62
08:37:15,273 --> 08:37:21,759
so the key values of my 1 are created 

63
08:37:21,759 --> 08:37:27,040
note that map goes to each node containing a data block for

64
08:37:27,040 --> 08:37:32,050
the file instead of the data moving to map 

65
08:37:32,050 --> 08:37:34,410
this is moving computation to data 

66
08:37:35,790 --> 08:37:40,863
let now see what the same map operation generates for partition b 

67
08:37:40,863 --> 08:37:44,221
since each word only happens to occur once 

68
08:37:44,221 --> 08:37:49,770
a list of all the words with one key - value pairing each gets generated 

69
08:37:50,950 --> 08:37:55,170
please take a moment to observe the outputs of map and

70
08:37:55,170 --> 08:37:59,480
each key - value pair associated to a word 

71
08:38:09,003 --> 08:38:16,570
next all the key - values that were output from map are sorted based on their key 

72
08:38:16,570 --> 08:38:23,810
and the key values with the same word are moved or shuffled to the same node 

73
08:38:24,980 --> 08:38:30,850
to simplify this figure each node only has a single word in orange boxes 

74
08:38:31,970 --> 08:38:36,010
but in general a node will have many different words 

75
08:38:36,010 --> 08:38:39,890
just like our example from the two lines in a and b partitions 

76
08:38:40,960 --> 08:38:46,480
here we see that you and apple are assigned to the first node 

77
08:38:46,480 --> 08:38:50,543
the word is to the second node 

78
08:38:50,543 --> 08:38:54,710
and the words rose and red to the third 

79
08:38:55,900 --> 08:39:01,960
although for simplicity we drew four map nodes and three shuffle nodes 

80
08:39:01,960 --> 08:39:05,760
the number of nodes can be extended as much as the application demands 

81
08:39:08,903 --> 08:39:14,083
next the reduce operation executes on these nodes to

82
08:39:14,083 --> 08:39:19,045
add values for key - value pairs with the same keys 

83
08:39:19,045 --> 08:39:23,875
for example apple 1 and

84
08:39:23,875 --> 08:39:30,959
another apple 1 becomes apple 2 

85
08:39:30,959 --> 08:39:35,161
the result of reduce is a single key pair for

86
08:39:35,161 --> 08:39:39,263
each word that was read in the input file 

87
08:39:39,263 --> 08:39:43,160
the key is the word and the value is the number of occurrences 

88
08:39:45,790 --> 08:39:49,650
if we look back at our wordcount example 

89
08:39:49,650 --> 08:39:52,640
we see that there were three distinct steps 

90
08:39:52,640 --> 08:39:59,818
namely the map step the shuffle and sort step and the reduce step 

91
08:39:59,818 --> 08:40:05,840
although the wordcount example is pretty simple 

92
08:40:05,840 --> 08:40:10,320
it represents a large number of applications to which these three steps

93
08:40:10,320 --> 08:40:14,000
can be applied in order to achieve data parallel scalability 

94
08:40:15,540 --> 08:40:21,213
for example now that you have seen the wordcount application 

95
08:40:21,213 --> 08:40:26,063
consider changing the wordcount algorithm to index all

96
08:40:26,063 --> 08:40:29,170
the urls by words after a web crawl 

97
08:40:30,320 --> 08:40:36,225
this means instead of pointing to a number the keys would refer to urls 

98
08:40:38,100 --> 08:40:43,350
after the map with this new function which by the way is called a user

99
08:40:43,350 --> 08:40:48,760
defined function the output of shuffle and sort would look like this 

100
08:40:53,333 --> 08:41:00,450
now when we reduce the urls all the urls that mention apple would look like this 

101
08:41:02,943 --> 08:41:08,010
this is in fact one of the ways a search engine like google works 

102
08:41:09,900 --> 08:41:15,150
so now if somebody came to the interface built for this application 

103
08:41:15,150 --> 08:41:19,078
to search for the word apple and entered apple 

104
08:41:19,078 --> 08:41:24,490
it would be easy to get all the urls as the word itself 

105
08:41:25,840 --> 08:41:29,943
no wonder the first mapreduce paper was produced by google 

106
08:41:29,943 --> 08:41:34,760
we will give you a link to the original google paper on mapreduce from

107
08:41:34,760 --> 08:41:37,390
2004 at the end of this lecture 

108
08:41:39,030 --> 08:41:41,080
it is pretty technical but

109
08:41:41,080 --> 08:41:45,040
it gives you a simple overview without the current system implementations 

110
08:41:46,720 --> 08:41:50,440
we just saw how mapreduce can be used in search engines

111
08:41:50,440 --> 08:41:52,800
in addition to counting the words and documents 

112
08:41:54,180 --> 08:41:58,900
although it possible to add many more applications let stop here for

113
08:41:58,900 --> 08:42:02,150
a general discussion on how the points of

114
08:42:02,150 --> 08:42:06,670
data parallelism can be used in search in this three step pattern 

115
08:42:08,480 --> 08:42:12,780
there is definitely parallelization during the map step 

116
08:42:13,810 --> 08:42:16,953
this parallelization is over the input 

117
08:42:16,953 --> 08:42:20,918
as each partition gets processed one line at a time 

118
08:42:20,918 --> 08:42:25,440
to achieve this type of data parallelism we must decide on

119
08:42:25,440 --> 08:42:30,430
the data granularity of each parallel competition 

120
08:42:30,430 --> 08:42:32,540
in this case it will be a line 

121
08:42:33,860 --> 08:42:40,410
we also see parallel grouping of data in the shuffle and sort phase 

122
08:42:40,410 --> 08:42:44,710
this time the parallelization is over the intermediate products 

123
08:42:45,740 --> 08:42:48,570
that is the individual key - value pairs 

124
08:42:49,940 --> 08:42:53,400
and after the grouping of the intermediate products 

125
08:42:53,400 --> 08:42:58,630
the reduce step gets parallelized to construct one output file 

126
08:43:00,090 --> 08:43:03,340
you have probably noticed that the data gets reduced

127
08:43:03,340 --> 08:43:05,140
to a smaller set at each step 

128
08:43:06,840 --> 08:43:13,208
this overview gave us an idea of what kinds of tasks that mapreduce is good for 

129
08:43:13,208 --> 08:43:18,604
while mapreduce excels at independent batch tasks similar to our applications 

130
08:43:18,604 --> 08:43:23,700
there are certain kinds of tasks that you would not want to use mapreduce for 

131
08:43:24,965 --> 08:43:29,300
for example if your data is frequently changing 

132
08:43:29,300 --> 08:43:34,120
mapreduce is slow since it reads the entire input data set each time 

133
08:43:35,350 --> 08:43:38,540
the mapreduce model requires that maps and

134
08:43:38,540 --> 08:43:42,320
reduces execute independently of each other 

135
08:43:42,320 --> 08:43:44,920
this greatly simplifies your job as a designer 

136
08:43:44,920 --> 08:43:48,910
since you do not have to deal with synchronization issues 

137
08:43:48,910 --> 08:43:53,060
however it means that computations that do have dependencies 

138
08:43:53,060 --> 08:43:54,880
cannot be expressed with mapreduce 

139
08:43:56,600 --> 08:44:00,760
finally mapreduce does not return any results

140
08:44:00,760 --> 08:44:03,490
until the entire process is finished 

141
08:44:03,490 --> 08:44:06,060
it must read the entire input data set 

142
08:44:07,130 --> 08:44:11,240
this makes it unsuitable for interactive applications where the results must be

143
08:44:11,240 --> 08:44:16,200
presented to the user very quickly expecting a return from the user 

144
08:44:17,880 --> 08:44:22,830
as a summary mapreduce hides complexities of parallel programming and

145
08:44:22,830 --> 08:44:25,450
greatly simplifies building parallel applications 

146
08:44:26,530 --> 08:44:28,996
many types of tasks suitable for

147
08:44:28,996 --> 08:44:34,130
mapreduce include search engine page ranking and topic mapping 

148
08:44:34,130 --> 08:44:37,929
please see the reading after this lecture on making pasta sauce

149
08:44:39,382 --> 08:44:45,320
with mapreduce for another fun application using the mapreduce programming model 

1
17:16:53,058 --> 17:16:55,042
programming models for big data 

2
17:17:14,264 --> 17:17:18,664
we have seen that scalable computing over the internet to achieve

3
17:17:18,664 --> 17:17:24,375
data - parallel scalability for big data applications is now a possibility 

4
17:17:24,375 --> 17:17:26,625
thanks to commodity clusters 

5
17:17:26,625 --> 17:17:30,970
cost - effective commodity clusters together with advances in distributed

6
17:17:30,970 --> 17:17:33,670
file systems to move computation to data 

7
17:17:33,670 --> 17:17:38,280
provide a potential to conduct scalable big data analytics 

8
17:17:38,280 --> 17:17:41,050
the next thing we will talk about is how to take

9
17:17:41,050 --> 17:17:43,050
advantage of these infrastructure advances 

10
17:17:44,100 --> 17:17:45,560
what are the right programming models 

11
17:17:46,680 --> 17:17:52,460
a programming model is an abstraction or existing machinery or infrastructure 

12
17:17:52,460 --> 17:17:55,970
it is a set of abstract runtime libraries and

13
17:17:55,970 --> 17:18:00,310
programming languages that form a model of computation 

14
17:18:00,310 --> 17:18:06,110
this abstraction level can be low - level as in machine language in computers 

15
17:18:06,110 --> 17:18:12,030
or very high as in high - level programming languages for example java 

16
17:18:12,030 --> 17:18:15,470
so we can say if the enabling infrastructure for

17
17:18:15,470 --> 17:18:20,320
big data analysis is distributed file systems as we mentioned 

18
17:18:20,320 --> 17:18:25,120
then the programming model for big data should enable the programmability

19
17:18:25,120 --> 17:18:29,100
of the operations within distributed file systems 

20
17:18:29,100 --> 17:18:33,840
what we mean by this being able to write computer programs that work

21
17:18:33,840 --> 17:18:38,410
efficiently on top of distributed file systems using big data and

22
17:18:38,410 --> 17:18:42,430
making it easy to cope with all the potential issues 

23
17:18:42,430 --> 17:18:44,320
based on everything we discussed so

24
17:18:44,320 --> 17:18:49,680
far let describe the requirements for big data programming models 

25
17:18:49,680 --> 17:18:54,480
first of all such a programming model for big data should support

26
17:18:54,480 --> 17:18:59,280
common big data operations like splitting large volumes of data 

27
17:19:00,420 --> 17:19:04,460
this means for partitioning and placement of data in and

28
17:19:04,460 --> 17:19:10,220
out of computer memory along with a model to synchronize the datasets later on 

29
17:19:10,220 --> 17:19:13,463
the access to data should be achieved in a fast way 

30
17:19:13,463 --> 17:19:18,291
it should allow fast distribution to nodes within a rack and these are potentially 

31
17:19:18,291 --> 17:19:20,815
the data nodes we moved the computation to 

32
17:19:20,815 --> 17:19:26,020
this means scheduling of many parallel tasks at once 

33
17:19:26,020 --> 17:19:29,950
it should also enable reliability of the computing and

34
17:19:29,950 --> 17:19:32,100
full tolerance from failures 

35
17:19:32,100 --> 17:19:35,768
this means it should enable programmable replications and

36
17:19:35,768 --> 17:19:37,758
recovery of files when needed 

37
17:19:37,758 --> 17:19:42,518
it should be easily scalable to the distributed notes where the data gets

38
17:19:42,518 --> 17:19:43,305
produced 

39
17:19:43,305 --> 17:19:48,647
it should also enable adding new resources to take advantage of distributive

40
17:19:48,647 --> 17:19:53,670
computers and scale to more or faster data without losing performance 

41
17:19:53,670 --> 17:19:57,280
this is called scaling out if needed 

42
17:19:57,280 --> 17:20:00,220
since there are a variety of different types of data 

43
17:20:00,220 --> 17:20:05,140
such as documents graphs tables key values etc 

44
17:20:05,140 --> 17:20:09,100
a programming model should enable operations over a particular set

45
17:20:09,100 --> 17:20:10,680
of these types 

46
17:20:10,680 --> 17:20:15,261
not every type of data may be supported by a particular model but

47
17:20:15,261 --> 17:20:19,020
the models should be optimized for at least one type 

48
17:20:19,020 --> 17:20:21,540
is it getting a little complicated 

49
17:20:21,540 --> 17:20:23,153
it does not have to have to be 

50
17:20:23,153 --> 17:20:28,696
in fact we apply similar models in our daily lives for everyday tasks 

51
17:20:28,696 --> 17:20:33,520
let look at the scenario where you might unknowingly apply this model 

52
17:20:33,520 --> 17:20:37,020
imagine a peaceful saturday afternoon 

53
17:20:37,020 --> 17:20:39,840
you receive a phone call from a friend and she says 

54
17:20:39,840 --> 17:20:42,050
she they will be at your house in an hour for dinner 

55
17:20:43,290 --> 17:20:48,157
it seems like you completely forgot that you had invited your friends for dinner 

56
17:20:48,157 --> 17:20:51,309
so you say you are looking forward to it and head to the kitchen 

57
17:20:51,309 --> 17:20:57,035
as a quick solution you decide to cook pasta with some tomato sauce 

58
17:20:57,035 --> 17:20:59,487
you need to take advantage of parallelization so

59
17:20:59,487 --> 17:21:03,315
that the dinner is ready by the time your guest arrive that within an hour 

60
17:21:03,315 --> 17:21:07,593
you call your spouse and your teenage kids to action in the kitchen 

61
17:21:07,593 --> 17:21:12,464
now you need to give them directions to start dicing the ingredients for you 

62
17:21:12,464 --> 17:21:19,010
but in the heat of the moment you end up mixing the onions tomatoes and peppers 

63
17:21:19,010 --> 17:21:20,890
instead of sorting them first 

64
17:21:20,890 --> 17:21:25,380
you give everyone a randomly mixed batch of different types of vegetables 

65
17:21:25,380 --> 17:21:30,360
they are required to use their computer powers to chop the vegetables 

66
17:21:30,360 --> 17:21:34,210
they need to ensure not mix different types of veggies 

67
17:21:34,210 --> 17:21:39,562
when everyone is done chopping you want to group the veggies by their types 

68
17:21:39,562 --> 17:21:44,877
you ask each helper to collect items of the same type put them in a large

69
17:21:44,877 --> 17:21:50,016
bowl and label this large bowl with the sum of individual bowl weights

70
17:21:50,016 --> 17:21:55,975
like tomatoes in one bowl peppers in another and the onions in the third bowl 

71
17:21:55,975 --> 17:21:56,781
in the end 

72
17:21:56,781 --> 17:22:02,209
you have nice large bowls with the total weight of each vegetable labeled on it 

73
17:22:02,209 --> 17:22:07,378
your helpers are soon done with their work while you are focused on coordinating their

74
17:22:07,378 --> 17:22:12,284
actions and other dinner tasks in the kitchen you can start cooking your pasta 

75
17:22:12,284 --> 17:22:17,230
what you have just seen is an excellent example of big data modeling in action 

76
17:22:17,230 --> 17:22:22,439
only it is really the data processed by human processors 

77
17:22:22,439 --> 17:22:26,600
this scenario can be modeled by a common programming model for big data 

78
17:22:26,600 --> 17:22:28,609
namely mapreduce 

79
17:22:28,609 --> 17:22:32,625
mapreduce is a big data programming model that supports all

80
17:22:32,625 --> 17:22:36,249
the requirements of big data modeling we mentioned 

81
17:22:36,249 --> 17:22:39,093
it can model processing large data 

82
17:22:39,093 --> 17:22:43,406
split complications into different parallel tasks and

83
17:22:43,406 --> 17:22:49,562
make efficient use of large commodity clusters and distributed file systems 

84
17:22:49,562 --> 17:22:53,542
in addition it abstracts out the details of parallelzation 

85
17:22:53,542 --> 17:22:58,050
full tolerance data distribution monitoring and load balancing 

86
17:22:59,140 --> 17:23:00,700
as a programming model 

87
17:23:00,700 --> 17:23:04,590
it has been implemented in a few different big data frameworks 

88
17:23:04,590 --> 17:23:08,123
next week we will see more details on mapreduce and

89
17:23:08,123 --> 17:23:10,788
how its hadoop implementation works 

90
17:23:10,788 --> 17:23:13,362
to summarize programming models for

91
17:23:13,362 --> 17:23:17,267
big data are abstractions over distributed file systems 

92
17:23:17,267 --> 17:23:22,022
the desired programming models for big data should handle large volumes and

93
17:23:22,022 --> 17:23:23,289
varieties of data 

94
17:23:23,289 --> 17:23:27,492
support full tolerance and provide scale out functionality 

95
17:23:27,492 --> 17:23:29,535
mapreduce is one of these models 

96
17:23:29,535 --> 17:23:33,063
implemented in a variety of frameworks including hadoop 

97
17:23:33,063 --> 17:23:37,360
we will summarize the inner workings of the hadoop implementation next week 

1
10:40:29,230 --> 10:40:32,230
in this lecture we will use hadoop to run wordcount 

2
10:40:32,230 --> 10:40:34,487
first we will open a terminal shell and

3
10:40:34,487 --> 10:40:37,680
explore the hadoop - provided mapreduce programs 

4
10:40:37,680 --> 10:40:41,657
next we will verify the input file exists in hdfs 

5
10:40:41,657 --> 10:40:46,130
we will then run wordcount and explore the wordcount output directory 

6
10:40:46,130 --> 10:40:48,560
after that we will copy the wordcount results

7
10:40:48,560 --> 10:40:51,556
from hdfs to the local file system and view them 

8
10:40:51,556 --> 10:40:53,493
let begin 

9
10:40:53,493 --> 10:40:56,930
first we will open a terminal shell by

10
10:40:56,930 --> 10:40:59,320
clicking on the icon at top of the window here 

11
10:41:02,430 --> 10:41:05,811
next we will look at the map produced programs that come with hadoop 

12
10:41:05,811 --> 10:41:11,961
we can do this by running hadoop jars user jars hadoop examples jar 

13
10:41:15,666 --> 10:41:20,616
this command says we are going to use the jar command to run a program

14
10:41:20,616 --> 10:41:23,030
in hadoop from a jar file 

15
10:41:23,030 --> 10:41:28,730
and the jar file that we are running from is in usr jars hadoop - examples jar 

16
10:41:28,730 --> 10:41:32,510
many programs written in java are distributed via jar files 

17
10:41:32,510 --> 10:41:39,520
if we run this command we will see a list of different programs that come with hadoop 

18
10:41:39,520 --> 10:41:42,090
so for example wordcount 

19
10:41:42,090 --> 10:41:43,780
count the words in a text file 

20
10:41:44,990 --> 10:41:48,350
wordmean count the average length of words 

21
10:41:48,350 --> 10:41:54,220
and other programs such as sorting and calculating the length of pi 

22
10:41:56,748 --> 10:42:00,558
in the previous lecture we downloaded the works of shakespeare and

23
10:42:00,558 --> 10:42:01,721
saved it into hdfs 

24
10:42:01,721 --> 10:42:06,822
let make sure that file is still there by running hadoop fs - ls 

25
10:42:09,387 --> 10:42:13,734
we can see that the file is still there and it called words txt 

26
10:42:15,900 --> 10:42:20,390
we can run wordcount by running hadoop jar

27
10:42:20,390 --> 10:42:26,180
 usr jars hadoop - examples jar wordcount 

28
10:42:26,180 --> 10:42:28,090
this command says that we are going to run a jar 

29
10:42:28,090 --> 10:42:31,690
and this is the name of the jar containing the program 

30
10:42:31,690 --> 10:42:34,690
and the program we are going to run is wordcount 

31
10:42:34,690 --> 10:42:37,450
when we run it we see that it prints the command line usage for

32
10:42:37,450 --> 10:42:39,050
how to run wordcount 

33
10:42:39,050 --> 10:42:46,310
this says that wordcount takes one or more input files and an output name 

34
10:42:46,310 --> 10:42:50,837
now both the input and the output are located in hdfs 

35
10:42:50,837 --> 10:42:56,010
so we have the input file that we just listed words txt in hdfs 

36
10:42:56,010 --> 10:42:56,940
we can run wordcount 

37
10:42:58,400 --> 10:43:05,148
so we will run hadoop jar usr jars hadoop - examples jar

38
10:43:05,148 --> 10:43:08,351
wordcount words txt out 

39
10:43:09,390 --> 10:43:14,250
this is saying we are going to run the wordcount program using words txt

40
10:43:14,250 --> 10:43:18,170
as the input and put the output in a directory called out 

41
10:43:19,470 --> 10:43:20,251
so we will run it 

42
10:43:27,381 --> 10:43:30,190
as wordcount is running your prints progress to the screen 

43
10:43:32,247 --> 10:43:35,760
it will print the percentage of map and reduce completed 

44
10:43:35,760 --> 10:43:39,124
and when both of these reach 100 then the job is done 

45
10:43:43,664 --> 10:43:46,440
now that the job is complete let look at the results 

46
10:43:48,900 --> 10:43:52,220
we can run hadoop fs - ls to see the output 

47
10:43:55,220 --> 10:43:59,930
this shows that out was created and this is where our results are stored 

48
10:43:59,930 --> 10:44:03,580
notice that it a directory with a d here 

49
10:44:03,580 --> 10:44:08,870
so hadoop word count created the directory to contain the output 

50
10:44:08,870 --> 10:44:13,412
let look inside that directory by running hadoop fs - ls out 

51
10:44:13,412 --> 10:44:18,580
 blank audio we can see that there are two files in this directory 

52
10:44:18,580 --> 10:44:24,280
the first is success this means that the wordcount job completed successfully 

53
10:44:24,280 --> 10:44:28,279
the other file part - r - 00000 is a text file

54
10:44:28,279 --> 10:44:32,284
containing the output from the wordcount command

55
10:44:34,198 --> 10:44:40,260
now let copy this text file to the local file system from hdfs and then view it 

56
10:44:40,260 --> 10:44:47,357
we could copy it by running hadoop fs - copytolocal

57
10:44:47,357 --> 10:44:52,481
out part - r - 00000 local 

58
10:44:54,760 --> 10:44:57,520
and we will say local txt is the name 

59
10:44:57,520 --> 10:45:02,340
you can view the results of this 

60
10:45:02,340 --> 10:45:04,320
we are running more local txt 

61
10:45:07,170 --> 10:45:08,740
this will view the contents of the file 

62
10:45:11,700 --> 10:45:13,310
we can hit spacebar to scroll down 

63
10:45:16,130 --> 10:45:19,450
we see the results of wordcount in this file 

64
10:45:19,450 --> 10:45:24,980
each line is a particular word and the second column is the count

65
10:45:24,980 --> 10:45:29,020
of how many words of this particular word was found in the input file 

66
10:45:31,820 --> 10:45:33,159
you can hit q to quit

1
21:26:03,110 --> 21:26:05,128
scalable computing over the internet 

2
21:26:21,074 --> 21:26:25,040
most computing is done on a single compute node 

3
21:26:26,900 --> 21:26:31,960
if the computation needs more than a node or parallel processing 

4
21:26:31,960 --> 21:26:36,070
like many scientific computing problems we use parallel computers 

5
21:26:37,070 --> 21:26:42,420
simply put a parallel computer is a very large number of

6
21:26:42,420 --> 21:26:48,041
single computing nodes with specialized capabilities connected to other network 

7
21:26:49,130 --> 21:26:56,525
for example the gordon supercomputer here at the san diego supercomputer center 

8
21:26:56,525 --> 21:27:05,170
has 1 024 compute nodes with 16 cores each equalling 16 384 compute cores in total 

9
21:27:06,230 --> 21:27:09,890
this type of specialized computer is pretty costly

10
21:27:09,890 --> 21:27:15,180
compared to its most recent cousin the commodity cluster 

11
21:27:15,180 --> 21:27:19,920
the term commodity cluster is often heard in big data conversations 

12
21:27:21,280 --> 21:27:23,560
have you ever wondered what it exactly means 

13
21:27:24,960 --> 21:27:29,740
commodity clusters are affordable parallel computers

14
21:27:29,740 --> 21:27:31,780
with an average number of computing nodes 

15
21:27:33,070 --> 21:27:37,650
they are not as powerful as traditional parallel computers and

16
21:27:37,650 --> 21:27:40,280
are often built out of less specialized nodes 

17
21:27:41,400 --> 21:27:44,660
in fact the nodes in the commodity cluster

18
21:27:44,660 --> 21:27:47,640
are more generic in their computing capabilities 

19
21:27:48,960 --> 21:27:53,320
the service - oriented computing community over the internet have pushed for

20
21:27:53,320 --> 21:27:58,800
computing to be done on commodity clusters as distributed computations 

21
21:27:58,800 --> 21:28:04,030
and in turn reducing the cost of computing over the internet 

22
21:28:05,680 --> 21:28:11,390
in commodity clusters the computing nodes are clustered in racks

23
21:28:13,400 --> 21:28:17,480
connected to each other via a fast network 

24
21:28:18,860 --> 21:28:22,150
there might be many of such racks in extensible amounts 

25
21:28:23,600 --> 21:28:28,410
computing in one or more of these clusters across

26
21:28:28,410 --> 21:28:33,910
a local area network or the internet is called distributed computing 

27
21:28:33,910 --> 21:28:38,860
such architectures enable what we call data - parallelism 

28
21:28:38,860 --> 21:28:43,416
in data - parallelism many jobs that share nothing can work on

29
21:28:43,416 --> 21:28:46,580
different data sets or parts of a data set 

30
21:28:47,930 --> 21:28:53,060
this type of parallelism sometimes gets called as job level parallelism 

31
21:28:53,060 --> 21:28:57,950
but in this specialization we will refer to it as data - parallelism

32
21:28:57,950 --> 21:29:02,020
in the context of big - data computing 

33
21:29:02,020 --> 21:29:07,020
large volumes and varieties of big data can be analyzed using this mode

34
21:29:07,020 --> 21:29:13,540
of parallelism achieving scalability performance and cost reduction 

35
21:29:13,540 --> 21:29:18,070
as you can imagine there are many points of failure inside systems 

36
21:29:19,500 --> 21:29:25,200
a node or an entire rack can fail at any given time 

37
21:29:25,200 --> 21:29:29,850
the connectivity of a rack to the network can stop or

38
21:29:29,850 --> 21:29:33,300
the connections between individual nodes can break 

39
21:29:34,820 --> 21:29:39,370
it is not practical to restart everything every time if failure happens 

40
21:29:40,500 --> 21:29:46,069
the ability to recover from such failures is called fault - tolerance 

41
21:29:46,069 --> 21:29:51,245
for fault - tolerance of such systems two neat solutions emerged 

42
21:29:51,245 --> 21:29:56,230
namely redundant data storage and

43
21:29:56,230 --> 21:29:59,480
restart of failed individual parallel jobs 

44
21:30:00,700 --> 21:30:02,780
we will explain these two solutions next 

45
21:30:04,100 --> 21:30:08,970
as a summary the commodity clusters are a cost effective way

46
21:30:08,970 --> 21:30:13,360
of achieving data parallel scalability for big data applications 

47
21:30:13,360 --> 21:30:18,750
these type of systems have a higher potential for partial failures 

48
21:30:18,750 --> 21:30:21,850
it is this type of distributed computing that pushed for

49
21:30:21,850 --> 21:30:25,385
a change towards cost effective reliable and

50
21:30:25,385 --> 21:30:29,550
fault - tolerant systems for management and analysis of big data 

1
18:56:33,950 --> 18:56:38,672
the hadoop distributed file system a storage system for big data 

2
18:56:52,994 --> 18:56:59,920
as a storage layer the hadoop distributed file system or the way we call it hdfs 

3
18:57:01,070 --> 18:57:04,890
serves as the foundation for most tools in the hadoop ecosystem 

4
18:57:06,700 --> 18:57:10,900
it provides two capabilities that are essential for managing big data 

5
18:57:11,900 --> 18:57:14,500
scalability to large data sets 

6
18:57:14,500 --> 18:57:17,570
and reliability to cope with hardware failures 

7
18:57:19,760 --> 18:57:24,060
hdfs allows you to store and access large datasets 

8
18:57:24,060 --> 18:57:28,700
according to hortonworks a leading vendor of hadoop services 

9
18:57:28,700 --> 18:57:34,950
hdfs has shown production scalability up to 200 petabytes and

10
18:57:34,950 --> 18:57:37,980
a single cluster of 4 500 servers 

11
18:57:37,980 --> 18:57:40,684
with close to a billion files and blocks 

12
18:57:42,504 --> 18:57:47,610
if you run out of space you can simply add more nodes to increase the space 

13
18:57:50,000 --> 18:57:53,470
hdfs achieves scalability by partitioning or

14
18:57:53,470 --> 18:57:57,630
splitting large files across multiple computers 

15
18:57:57,630 --> 18:58:02,040
this allows parallel access to very large files since the computations run in

16
18:58:02,040 --> 18:58:05,300
parallel on each node where the data is stored 

17
18:58:06,760 --> 18:58:09,920
typical file size is gigabytes to terabytes 

18
18:58:11,150 --> 18:58:18,900
the default chunk size the size of each piece of a file is 64 megabytes 

19
18:58:18,900 --> 18:58:21,390
but you can configure this to any size 

20
18:58:22,950 --> 18:58:26,180
by spreading the file across many nodes 

21
18:58:26,180 --> 18:58:30,740
the chances are increased that a node storing one of the blocks will fail 

22
18:58:32,650 --> 18:58:34,440
what happens next 

23
18:58:34,440 --> 18:58:36,970
do we lose the information stored in block c 

24
18:58:40,560 --> 18:58:43,350
hdfs is designed for full tolerance in such case 

25
18:58:44,640 --> 18:58:47,080
hdfs replicates or

26
18:58:47,080 --> 18:58:52,530
makes a copy of file blocks on different nodes to prevent data loss 

27
18:58:54,820 --> 18:58:59,945
in this example the node that crashed stored block c 

28
18:58:59,945 --> 18:59:08,270
but block c was replicated on two other nodes in the cluster 

29
18:59:10,820 --> 18:59:15,380
by default hdfs maintains three copies of every block 

30
18:59:16,760 --> 18:59:18,850
this is the default replication factor 

31
18:59:19,890 --> 18:59:23,300
but you can change it globally for every file or

32
18:59:23,300 --> 18:59:28,180
on a per file basis 

33
18:59:28,180 --> 18:59:32,370
hdfs is also designed to handle a variety of data types aligned with

34
18:59:32,370 --> 18:59:33,450
big data variety 

35
18:59:35,370 --> 18:59:41,220
to read a file in hdfs you must specify the input file format 

36
18:59:41,220 --> 18:59:45,610
similarly to write the file you must provide the output file format 

37
18:59:47,885 --> 18:59:52,040
hdfs provides a set of formats for common data types 

38
18:59:52,040 --> 18:59:58,310
but this is extensible and you can provide custom formats for your data types 

39
18:59:58,310 --> 19:00:01,830
for example text files can be read 

40
19:00:03,030 --> 19:00:05,620
line by line or a word at a time 

41
19:00:07,950 --> 19:00:13,650
geospatial data can be read as vectors or rasters 

42
19:00:14,660 --> 19:00:20,210
data formats specific to geospatial data or

43
19:00:20,210 --> 19:00:23,670
other domain specific data formats 

44
19:00:23,670 --> 19:00:28,588
like fasta or fastq formats for sequence data genomics 

45
19:00:30,660 --> 19:00:33,665
hdfs is comprised of two components 

46
19:00:33,665 --> 19:00:37,582
namenode and datanode 

47
19:00:37,582 --> 19:00:42,180
these operate using a master slave relationship 

48
19:00:43,350 --> 19:00:47,960
where the namenode issues comments to datanodes across the cluster 

49
19:00:49,360 --> 19:00:53,110
the namenode is responsible for metadata 

50
19:00:53,110 --> 19:00:56,072
and datanodes provide block storage 

51
19:00:58,174 --> 19:01:02,064
there is usually one namenode per cluster 

52
19:01:02,064 --> 19:01:06,910
a datanode however runs on each node in the cluster 

53
19:01:08,920 --> 19:01:13,280
in some sense the namenode is the administrator or

54
19:01:13,280 --> 19:01:15,850
the coordinator of the hdfs cluster 

55
19:01:17,030 --> 19:01:21,560
when the file is created the namenode records the name 

56
19:01:21,560 --> 19:01:25,010
location in the directory hierarchy and other metadata 

57
19:01:26,820 --> 19:01:32,730
the namenode also decides which data nodes to store the contents of the file and

58
19:01:32,730 --> 19:01:34,370
remembers this mapping 

59
19:01:36,420 --> 19:01:39,400
the datanode runs on each node in the cluster 

60
19:01:40,460 --> 19:01:43,880
and is responsible for storing the file blocks 

61
19:01:45,600 --> 19:01:50,328
the data node listens to commands from the name node for block creation 

62
19:01:50,328 --> 19:01:56,980
deletion and replication 

63
19:01:56,980 --> 19:02:00,530
replication provides two key capabilities 

64
19:02:00,530 --> 19:02:03,260
fault tolerance and data locality 

65
19:02:04,390 --> 19:02:09,730
as discussed earlier when a machine in the cluster has a hardware failure there

66
19:02:09,730 --> 19:02:14,100
are two other copies of each block that are stored on that node 

67
19:02:14,100 --> 19:02:15,470
so no data is lost 

68
19:02:16,660 --> 19:02:20,920
replication also means that the same block will be stored on different nodes on

69
19:02:20,920 --> 19:02:25,780
the system which are in different geographical locations 

70
19:02:27,370 --> 19:02:32,280
a location may mean a specific rack or a data center in a different town 

71
19:02:34,140 --> 19:02:38,950
the location is important since we want to move computation to data and

72
19:02:38,950 --> 19:02:39,890
not the other way around 

73
19:02:42,100 --> 19:02:46,030
we will talk about what moving computation to data means later in this module 

74
19:02:47,650 --> 19:02:52,120
as i mentioned earlier the default replication factor is three but

75
19:02:52,120 --> 19:02:54,080
you can change this 

76
19:02:54,080 --> 19:02:59,370
a high replication factor means more protection against hardware failures 

77
19:02:59,370 --> 19:03:02,350
and better chances for data locality 

78
19:03:02,350 --> 19:03:05,660
but it also means increased storage space is used 

79
19:03:08,190 --> 19:03:11,050
as a summary hdfs provides

80
19:03:11,050 --> 19:03:15,440
scalable big data storage by partitioning files over multiple nodes 

81
19:03:16,950 --> 19:03:21,160
this helps to scale big data analytics to large data volumes 

82
19:03:22,530 --> 19:03:25,730
the application protects against hardware failures and

83
19:03:25,730 --> 19:03:30,420
provides data locality when we move analytical complications to data 

1
14:00:03,252 --> 14:00:06,592
the hadoop ecosystem : so much free stuff ! 

2
14:00:21,275 --> 14:00:25,046
how did the big data open - source movement begin 

3
14:00:25,046 --> 14:00:29,492
in 2004 google published a paper about their

4
14:00:29,492 --> 14:00:34,811
in - house processing framework they called mapreduce 

5
14:00:34,811 --> 14:00:38,699
the next year yahoo released an open - source

6
14:00:38,699 --> 14:00:43,369
implementation based on this framework called hadoop 

7
14:00:43,369 --> 14:00:46,379
in the following years other frameworks and

8
14:00:46,379 --> 14:00:50,650
tools were released to the community as open - source projects 

9
14:00:50,650 --> 14:00:54,764
these frameworks provided new capabilities missing in hadoop 

10
14:00:54,764 --> 14:00:57,990
such as sql like querying or high level scripting 

11
14:01:00,240 --> 14:01:04,220
today there are over 100 open - source projects for

12
14:01:04,220 --> 14:01:07,880
big data and this number continues to grow 

13
14:01:09,160 --> 14:01:12,181
many rely on hadoop but some are independent 

14
14:01:14,711 --> 14:01:19,420
with so many frameworks and tools available how do we learn what they do 

15
14:01:20,600 --> 14:01:27,590
we can organize them with a layer diagram to understand their capabilities 

16
14:01:27,590 --> 14:01:31,830
sometimes we also used the term stack instead of a layer diagram 

17
14:01:33,560 --> 14:01:38,400
in a layer diagram a component uses the functionality or

18
14:01:38,400 --> 14:01:42,958
capabilities of the components in the layer below it 

19
14:01:42,958 --> 14:01:48,470
usually components at the same layer do not communicate 

20
14:01:48,470 --> 14:01:55,669
and a component never assumes a specific tool or component is above it 

21
14:01:55,669 --> 14:02:01,901
in this example component a is in the bottom layer 

22
14:02:01,901 --> 14:02:05,370
which components b and c use 

23
14:02:07,170 --> 14:02:10,390
component d uses b but not c 

24
14:02:12,240 --> 14:02:14,790
and d does not directly use a 

25
14:02:18,060 --> 14:02:22,630
let look at one set of tools in the hadoop ecosystem as a layer diagram 

26
14:02:24,920 --> 14:02:31,795
this layer diagram is organized vertically based on the interface 

27
14:02:31,795 --> 14:02:36,054
low level interfaces so storage and scheduling on the bottom 

28
14:02:36,054 --> 14:02:40,315
and high level languages and interactivity at the top 

29
14:02:42,915 --> 14:02:48,315
the hadoop distributed file system or hdfs is the foundation for

30
14:02:48,315 --> 14:02:54,570
many big data frameworks since it provides scaleable and reliable storage 

31
14:02:55,740 --> 14:03:01,380
as the size of your data increases you can add commodity hardware

32
14:03:01,380 --> 14:03:06,660
to hdfs to increase storage capacity so

33
14:03:06,660 --> 14:03:10,250
it enables scaling out of your resources 

34
14:03:12,460 --> 14:03:16,388
hadoop yarn provides flexible scheduling and

35
14:03:16,388 --> 14:03:20,220
resource management over the hdfs storage 

36
14:03:21,910 --> 14:03:26,988
yarn is used at yahoo to schedule jobs across 40 000 servers 

37
14:03:29,370 --> 14:03:34,430
mapreduce is a programming model that simplifies parallel computing 

38
14:03:35,720 --> 14:03:40,300
instead of dealing with the complexities of synchronization and scheduling you

39
14:03:40,300 --> 14:03:46,920
only need to give mapreduce two functions map and reduce as you heard before 

40
14:03:49,020 --> 14:03:51,080
this programming model is so

41
14:03:51,080 --> 14:03:56,740
powerful that google previously used it for indexing websites 

42
14:04:00,450 --> 14:04:06,009
mapreduce only assume a limited model to express data 

43
14:04:06,009 --> 14:04:11,094
hive and pig are two additional programming models on

44
14:04:11,094 --> 14:04:16,292
top of mapreduce to augment data modeling of mapreduce

45
14:04:16,292 --> 14:04:22,398
with relational algebra and data flow modeling respectively 

46
14:04:22,398 --> 14:04:26,718
hive was created at facebook to issue sql - like

47
14:04:26,718 --> 14:04:31,038
queries using mapreduce on their data in hdfs 

48
14:04:31,038 --> 14:04:38,951
pig was created at yahoo to model data flow based programs using mapreduce 

49
14:04:38,951 --> 14:04:42,431
thanks to yarn stability to manage resources 

50
14:04:42,431 --> 14:04:46,959
not just for mapreduce but other programming models as well 

51
14:04:46,959 --> 14:04:51,787
giraph was built for processing large - scale graphs efficiently 

52
14:04:51,787 --> 14:04:58,904
for example facebook uses giraph to analyze the social graphs of its users 

53
14:04:58,904 --> 14:05:04,196
similarly storm spark and flink were built for

54
14:05:04,196 --> 14:05:08,899
real time and in memory processing of big data on

55
14:05:08,899 --> 14:05:13,381
top of the yarn resource scheduler and hdfs 

56
14:05:13,381 --> 14:05:18,986
in - memory processing is a powerful way of running big data applications even faster 

57
14:05:18,986 --> 14:05:22,787
achieving 100x better performance for some tasks 

58
14:05:25,446 --> 14:05:29,405
sometimes your data or processing tasks are not easily or

59
14:05:29,405 --> 14:05:34,330
efficiently represented using the file and directory model of storage 

60
14:05:35,620 --> 14:05:40,810
examples of this include collections of key - values or large sparse tables 

61
14:05:42,990 --> 14:05:50,381
nosql projects such as cassandra mongodb and hbase handle these cases 

62
14:05:50,381 --> 14:05:55,180
cassandra was created at facebook but facebook also used hbase for

63
14:05:55,180 --> 14:05:57,010
its messaging platform 

64
14:05:59,320 --> 14:06:04,560
finally running all of these tools requires a centralized management system

65
14:06:04,560 --> 14:06:09,490
for synchronization configuration and to ensure high availability 

66
14:06:10,860 --> 14:06:14,210
zookeeper performs these duties 

67
14:06:14,210 --> 14:06:18,700
it was created by yahoo to wrangle services named after animals 

68
14:06:21,060 --> 14:06:23,990
a major benefit of the hadoop ecosystem

69
14:06:23,990 --> 14:06:26,340
is that all these tools are open - source projects 

70
14:06:27,450 --> 14:06:29,920
you can download and use them for free 

71
14:06:31,220 --> 14:06:35,934
each project has a community of users and developers that

72
14:06:35,934 --> 14:06:40,756
answer questions fix bugs and implement new features 

73
14:06:40,756 --> 14:06:45,370
you can mix and match to get only the tools you need to achieve your goals 

74
14:06:47,010 --> 14:06:51,330
alternatively there are several pre - built stacks of these tools

75
14:06:51,330 --> 14:06:55,670
offered by companies such as cloudera mapr and hortonworks 

76
14:06:57,090 --> 14:07:01,850
these companies provide the core software stacks for free and

77
14:07:01,850 --> 14:07:04,740
offer commercial support for production environments 

78
14:07:06,800 --> 14:07:10,110
as a summary the hadoop ecosystem

79
14:07:10,110 --> 14:07:14,672
consists of a growing number of open - source tools 

80
14:07:14,672 --> 14:07:18,502
providing opportunities to pick the right tool for

81
14:07:18,502 --> 14:07:22,788
the right tasks for better performance and lower costs 

82
14:07:22,788 --> 14:07:26,284
we will reveal some of these tools in further detail and

83
14:07:26,284 --> 14:07:30,620
provide an analysis of when to use which in the next set of lectures 

1
04:07:31,610 --> 04:07:34,160
generating value from hadoop and

2
04:07:34,160 --> 04:07:38,996
pre - built hadoop images that come as off the shelf products 

3
04:07:49,576 --> 04:07:54,922
assembling your own software stack from scratch can be messy and

4
04:07:54,922 --> 04:07:57,298
a lot of work for beginners 

5
04:07:57,298 --> 04:08:02,661
the task of setting up the whole stack could consume a lot of project time and

6
04:08:02,661 --> 04:08:06,550
man power reducing time to deployment 

7
04:08:06,550 --> 04:08:12,490
getting pre - built images is similar to buying pre - assembled furniture 

8
04:08:12,490 --> 04:08:17,701
you can obtain a ready to go software stack which contains a pre - installed

9
04:08:17,701 --> 04:08:22,510
operating system required libraries and application software 

10
04:08:23,830 --> 04:08:26,380
it saves you from the trouble of putting the different

11
04:08:26,380 --> 04:08:29,090
parts together in the right orientation 

12
04:08:29,090 --> 04:08:31,030
you can start using the furniture right away 

13
04:08:32,600 --> 04:08:36,620
packaging of these pre - built software images is enabled

14
04:08:36,620 --> 04:08:39,420
by virtual machines using virtualization software 

15
04:08:40,640 --> 04:08:45,380
without going into too much detail one of the benefits of virtualization

16
04:08:45,380 --> 04:08:49,710
software is that it lets you run a ready made software stack within minutes 

17
04:08:50,890 --> 04:08:54,390
your software stack comes as a large file 

18
04:08:54,390 --> 04:08:57,800
virtualization software provides a platform where your stack can run 

19
04:08:59,160 --> 04:09:04,240
many companies provide images for their version of the hadoop platform 

20
04:09:04,240 --> 04:09:07,100
including a number of tools of their choice 

21
04:09:08,100 --> 04:09:12,570
hortonworks is one of the companies that provides a pre - built software stack for

22
04:09:12,570 --> 04:09:14,370
both mac and windows platforms 

23
04:09:15,520 --> 04:09:19,420
cloudera is another company that provides pre - installed and

24
04:09:19,420 --> 04:09:22,120
assembled software stack images 

25
04:09:22,120 --> 04:09:25,230
cloudera image is what we will be working with in this course 

26
04:09:26,480 --> 04:09:29,020
many other companies provide similar images 

27
04:09:30,100 --> 04:09:35,831
additionally lots of online tutorials for beginners are on vendors websites for

28
04:09:35,831 --> 04:09:39,740
self - training of users working with these images and

29
04:09:39,740 --> 04:09:41,580
the open source tools they include 

30
04:09:43,270 --> 04:09:47,200
once you choose the vendor you can check out their website for

31
04:09:47,200 --> 04:09:49,550
tutorials on how to get started quickly 

32
04:09:50,590 --> 04:09:52,980
there are plenty of resources online for that 

33
04:09:54,240 --> 04:09:57,080
you can deploy pre - built images over the cloud 

34
04:09:58,110 --> 04:10:01,490
this would further accelerate your application deployment process 

35
04:10:02,850 --> 04:10:07,170
it is always best to evaluate which approach is most cost effective for

36
04:10:07,170 --> 04:10:09,320
your business model and organization 

37
04:10:10,560 --> 04:10:14,485
companies such as cloudera hortonworks and others 

38
04:10:14,485 --> 04:10:20,090
provide step - by - step guides on how to set up pre - built images on the cloud 

39
04:10:21,180 --> 04:10:26,460
as a summary using pre - built software packages have a number of benefits and

40
04:10:26,460 --> 04:10:29,280
can significantly accelerate your big data projects 

41
04:10:30,500 --> 04:10:37,558
even small teams can quickly prototype deploy and validate their project ideas 

42
04:10:37,558 --> 04:10:42,480
the developed analytical solutions can be scaled to larger volumes and

43
04:10:42,480 --> 04:10:46,560
increase velocities of data in a matter of hours 

44
04:10:46,560 --> 04:10:51,162
these companies also provide enterprise level solutions for large 

45
04:10:51,162 --> 04:10:52,837
full - fledged applications 

46
04:10:53,940 --> 04:10:58,395
an added benefit is that there are plenty of companies which provide

47
04:10:58,395 --> 04:11:00,250
ready - made solutions 

48
04:11:00,250 --> 04:11:06,270
that means lots of choices for you to pick the one most suited to your project 

1
08:18:37,540 --> 08:18:39,593
what is a distributed file system 

2
08:18:53,485 --> 08:18:56,336
most of us have file cabinets in our offices or

3
08:18:56,336 --> 08:18:59,420
homes that help us store our printed documents 

4
08:19:01,170 --> 08:19:04,280
everyone has their own method of organizing files 

5
08:19:04,280 --> 08:19:08,410
including the way we bin similar documents into one file or

6
08:19:08,410 --> 08:19:12,210
the way we sort them in alphabetical or date order 

7
08:19:12,210 --> 08:19:15,380
when computers first came out the information and

8
08:19:15,380 --> 08:19:17,970
programs were stored in punch cards 

9
08:19:19,280 --> 08:19:22,374
these punch cards were stored in file cabinets 

10
08:19:22,374 --> 08:19:25,760
just like the physical file cabinets today 

11
08:19:25,760 --> 08:19:28,940
this is where the name file system comes from 

12
08:19:28,940 --> 08:19:32,970
the need to store information in files comes from a larger need

13
08:19:32,970 --> 08:19:35,720
to store information in the long - term 

14
08:19:35,720 --> 08:19:40,360
this way the information lives after the computer program or

15
08:19:40,360 --> 08:19:44,350
what we call process that produced it terminates 

16
08:19:44,350 --> 08:19:49,450
if we do not have files our access to such information would not be possible

17
08:19:49,450 --> 08:19:52,560
once a program using or producing it 

18
08:19:52,560 --> 08:19:57,430
even during the process we might need to store large amounts of information

19
08:19:57,430 --> 08:20:02,240
that we cannot store within the program components or computer memory 

20
08:20:02,240 --> 08:20:05,690
in addition once the data is in a file 

21
08:20:05,690 --> 08:20:10,120
multiple processes can access the same information if needed 

22
08:20:10,120 --> 08:20:15,940
for all these reasons we store information in files on a hard disk 

23
08:20:15,940 --> 08:20:18,350
there are many of these files and

24
08:20:18,350 --> 08:20:22,750
they get managed by your operating system like windows or linux 

25
08:20:22,750 --> 08:20:27,640
how the operating system manages files is called a file system 

26
08:20:27,640 --> 08:20:32,930
how this information is stored on disk drives has high impact

27
08:20:32,930 --> 08:20:39,090
on the efficiency and speed of access to data especially in the big data case 

28
08:20:39,090 --> 08:20:44,420
while the files have exact addresses for their locations in the drive referring

29
08:20:44,420 --> 08:20:50,274
to the data units of sequence of these blocks that called the flat structure 

30
08:20:50,274 --> 08:20:55,510
or hierarchy construction of index records that called the database 

31
08:20:55,510 --> 08:21:01,710
they also have human readable symbolic names generally followed by an extension 

32
08:21:02,750 --> 08:21:06,140
extensions tell what kind of file it is in general 

33
08:21:06,140 --> 08:21:10,640
programs and users can access files with their names 

34
08:21:10,640 --> 08:21:16,010
the contents of a file can be numeric alphabetic alphanumeric 

35
08:21:16,010 --> 08:21:17,160
or binary executables 

36
08:21:18,180 --> 08:21:22,250
most computer users work on personal laptops or

37
08:21:22,250 --> 08:21:25,050
desktop computers with a single hard drive 

38
08:21:26,050 --> 08:21:30,780
in this model the user is limited to the capacity of their hard drive 

39
08:21:30,780 --> 08:21:34,060
the capacity of different devices vary 

40
08:21:34,060 --> 08:21:36,460
for example while your phone or

41
08:21:36,460 --> 08:21:41,380
tablet might have a storage capacity in the order of gigabytes your

42
08:21:41,380 --> 08:21:47,210
laptop computer might have a terabyte of storage but what if you have more data 

43
08:21:47,210 --> 08:21:50,290
some of you probably had issues in the past with

44
08:21:50,290 --> 08:21:52,950
running out of space on your hard drive 

45
08:21:52,950 --> 08:21:56,250
a way to solve this is to have an external hard drive and

46
08:21:56,250 --> 08:22:00,390
store your files there or you can buy a bigger disk 

47
08:22:00,390 --> 08:22:05,580
both options are a bit of a hassle to copy the data to a new disk are not they 

48
08:22:05,580 --> 08:22:07,520
they might not even be an option sometimes 

49
08:22:08,560 --> 08:22:12,620
now imagine you having two computers and

50
08:22:12,620 --> 08:22:18,050
storing some of your data in one and the rest of your data in another 

51
08:22:18,050 --> 08:22:22,560
how you organize and partition your data between these computers is up to you 

52
08:22:23,570 --> 08:22:27,400
you might want to store your work data in one computer and

53
08:22:27,400 --> 08:22:30,030
your personal data in another 

54
08:22:30,030 --> 08:22:34,120
distributing data on multiple computers might be an option but

55
08:22:34,120 --> 08:22:36,030
it raises new issues 

56
08:22:36,030 --> 08:22:41,800
in this situation you need to know where to find the files you need 

57
08:22:41,800 --> 08:22:43,630
depending on what you re doing 

58
08:22:43,630 --> 08:22:47,080
you can find it manageable if it s just your data 

59
08:22:47,080 --> 08:22:51,500
but now imagine having thousands of computers

60
08:22:51,500 --> 08:22:55,373
to store your data with big volumes and variety 

61
08:22:55,373 --> 08:23:00,240
would not it be good to have a system that can handle the data access and

62
08:23:00,240 --> 08:23:01,330
do this for you 

63
08:23:01,330 --> 08:23:06,450
this is a case that can be handled by a distributive file system 

64
08:23:06,450 --> 08:23:10,590
now let assume that there are racks of these computers 

65
08:23:10,590 --> 08:23:14,758
often even distributed across the local or

66
08:23:14,758 --> 08:23:20,400
wide area network because such file systems distributed file systems 

67
08:23:21,910 --> 08:23:25,800
data sets or parts of a data set 

68
08:23:25,800 --> 08:23:29,900
can be replicated across the nodes of a distributed file system 

69
08:23:30,900 --> 08:23:35,900
since data is already on these nodes then analysis of parts of the data

70
08:23:35,900 --> 08:23:42,030
is needed in a data parallel fashion computation can be moved to these nodes 

71
08:23:43,500 --> 08:23:48,590
additionally distributed file systems replicate the data

72
08:23:48,590 --> 08:23:53,720
between the racks and also computers distributed across geographical regions 

73
08:23:55,180 --> 08:23:59,160
data replication makes the system more fault tolerant 

74
08:24:00,250 --> 08:24:05,290
that means if some nodes or a rack goes down 

75
08:24:05,290 --> 08:24:10,764
there are other parts of the system the same data can be found and analyzed 

76
08:24:10,764 --> 08:24:17,080
data replication also helps with scaling the access to this data by many users 

77
08:24:18,170 --> 08:24:24,110
often if the data is popular many reader processes will want access to it 

78
08:24:25,310 --> 08:24:28,240
in a highly parallelized replication 

79
08:24:28,240 --> 08:24:32,680
each reader can get their own node to access to and analyze data 

80
08:24:33,920 --> 08:24:36,450
this increases overall system performance 

81
08:24:37,935 --> 08:24:42,600
note that a problem with having such a distributive replication is 

82
08:24:42,600 --> 08:24:46,490
that it is hard to make changes to data over time 

83
08:24:46,490 --> 08:24:52,090
however in most big data systems the data is written once and

84
08:24:52,090 --> 08:24:57,570
the updates to data is maintained as additional data sets over time 

85
08:24:57,570 --> 08:25:02,500
as a summary a file system is responsible from the organization of

86
08:25:02,500 --> 08:25:05,970
the long term information storage in a computer 

87
08:25:05,970 --> 08:25:10,991
when many storage computers are connected through the network 

88
08:25:10,991 --> 08:25:13,916
we call it a distributed file system 

89
08:25:13,916 --> 08:25:19,035
distributed file systems provide data scalability fault tolerance and

90
08:25:19,035 --> 08:25:24,321
high concurrency through partitioning and replication of data on many nodes 

1
16:44:04,148 --> 16:44:06,208
when to reconsider hadoop 

2
16:44:20,098 --> 16:44:23,960
the hadoop ecosystem is growing at a fast pace 

3
16:44:24,970 --> 16:44:27,937
this means a lot of stuff that was difficult or

4
16:44:27,937 --> 16:44:31,180
not supportive is becoming possible 

5
16:44:33,480 --> 16:44:34,990
in this lecture 

6
16:44:34,990 --> 16:44:38,730
we will look at some aspects that clearly make a good match for hadoop 

7
16:44:40,200 --> 16:44:44,220
we will also look at several aspects that might motivate you

8
16:44:44,220 --> 16:44:46,510
to evaluate hadoop at a deeper level 

9
16:44:48,060 --> 16:44:52,560
and does hadoop really make sense for your specific problem 

10
16:44:55,190 --> 16:44:59,790
first let look at the key features that make a problem hadoop friendly 

11
16:45:00,950 --> 16:45:05,390
if you see a large scale growth in amount of data you will tackle 

12
16:45:05,390 --> 16:45:07,310
probably it makes sense to use hadoop 

13
16:45:08,610 --> 16:45:13,230
when you want quick access to your old data which would otherwise go on tape

14
16:45:13,230 --> 16:45:18,180
drives for archival storage hadoop might provide a good alternative 

15
16:45:21,080 --> 16:45:23,650
other hadoop friendly features include

16
16:45:23,650 --> 16:45:29,020
scenarios when you want to use multiple applications over the same data store 

17
16:45:29,020 --> 16:45:32,683
high volume or high variety are also great indicators for

18
16:45:32,683 --> 16:45:34,598
hadoop as a platform choice 

19
16:45:38,148 --> 16:45:41,860
small data set processing should raise your eyebrows 

20
16:45:41,860 --> 16:45:43,410
do you really need hadoop for that 

21
16:45:44,560 --> 16:45:50,178
dig deeper and find out exactly why you want to use hadoop before going ahead 

22
16:45:54,238 --> 16:45:57,330
hadoop is good for data parallelism 

23
16:45:57,330 --> 16:46:02,320
as you know data parallelism is the simultaneous execution of the same

24
16:46:02,320 --> 16:46:06,520
function on multiple nodes across the elements of a dataset 

25
16:46:07,630 --> 16:46:12,060
on the other hand task parallelism as you see in this graph 

26
16:46:13,640 --> 16:46:18,160
is the simultaneous execution of many different functions

27
16:46:18,160 --> 16:46:22,260
on multiple nodes across the same or different data sets 

28
16:46:23,930 --> 16:46:27,950
if your problem has task - level parallelism you must do further

29
16:46:27,950 --> 16:46:31,990
analysis as to which tools you plan to deploy from the hadoop ecosystem 

30
16:46:33,910 --> 16:46:37,260
what are the precise benefits that these tools provide 

31
16:46:38,790 --> 16:46:39,920
proceed with caution 

32
16:46:42,330 --> 16:46:45,330
not all algorithms are scalable in hadoop or

33
16:46:45,330 --> 16:46:49,030
reducible to one of the programming models supported by yarn 

34
16:46:51,090 --> 16:46:56,590
hence if you are looking to deploy highly coupled data processing algorithms

35
16:46:56,590 --> 16:46:57,860
proceed with caution 

36
16:46:59,300 --> 16:47:02,270
do a thorough analysis before using hadoop 

37
16:47:02,270 --> 16:47:06,680
are you thinking of

38
16:47:06,680 --> 16:47:11,350
throwing away your existing database solutions and replacing them with hadoop 

39
16:47:11,350 --> 16:47:11,850
think again 

40
16:47:13,130 --> 16:47:18,100
hadoop may be a good platform where your diverse data sets can land and

41
16:47:18,100 --> 16:47:22,210
get processed into a form digestible with your database 

42
16:47:23,660 --> 16:47:28,070
hadoop may not be the best data store solution for your business case 

43
16:47:28,070 --> 16:47:30,470
evaluate and proceed with caution 

44
16:47:31,960 --> 16:47:36,560
hdfs stores data in blocks of 64 megabytes or larger 

45
16:47:36,560 --> 16:47:41,940
so you may have to read an entire file just to pick one data entry 

46
16:47:43,580 --> 16:47:46,858
that makes it a bit harder to perform random data access 

47
16:47:49,078 --> 16:47:53,370
the hadoop ecosystem is growing at a faster pace than ever 

48
16:47:54,820 --> 16:47:58,530
this slide shows some of the moving targets in the hadoop ecosystem and

49
16:47:58,530 --> 16:48:01,650
the additional needs which must be addressed by new tools

50
16:48:01,650 --> 16:48:03,470
to the hadoop ecosystem 

51
16:48:03,470 --> 16:48:06,260
mainly advanced analytical queries 

52
16:48:07,270 --> 16:48:11,580
latency sensitive tasks and cyber security of sensitive data 

53
16:48:13,050 --> 16:48:17,510
here we give pointers to tools you might want to look into further

54
16:48:17,510 --> 16:48:20,090
to understand the challenges these need tools address 

55
16:48:22,000 --> 16:48:28,590
as a summary although hadoop is good with scalability of many algorithms it is just

56
16:48:28,590 --> 16:48:33,520
one model and does not solve all issues in managing and processing big data 

57
16:48:34,630 --> 16:48:39,590
although it would be possible to find counterexamples we can generally say

58
16:48:39,590 --> 16:48:44,110
that the hadoop framework is not the best for working with small data sets 

59
16:48:44,110 --> 16:48:48,120
advanced algorithms that require a specific hardware type 

60
16:48:48,120 --> 16:48:53,190
task level parallelism infrastructure replacement or random data access 

1
09:32:56,028 --> 09:32:58,748
yarn the resource manager for hadoop 

2
09:33:15,825 --> 09:33:21,260
yarn is a resource manage layer that sits just above the storage layer hdfs 

3
09:33:22,510 --> 09:33:27,210
yarn interacts with applications and schedules resources for their use 

4
09:33:28,390 --> 09:33:33,390
yarn enables running multiple applications over hdfc increases

5
09:33:33,390 --> 09:33:38,160
resource efficiency and let you go beyond the map reduce or

6
09:33:38,160 --> 09:33:40,720
even beyond the data parallel programming model 

7
09:33:42,910 --> 09:33:46,630
when hadoop was first created this was not the case 

8
09:33:46,630 --> 09:33:51,560
in fact the original hadoop stack had no resource manager 

9
09:33:51,560 --> 09:33:56,480
these two stacked diagrams show some of it evolution over the last ten years 

10
09:33:57,980 --> 09:34:01,580
one of the biggest limitations of hadoop one point zero 

11
09:34:01,580 --> 09:34:05,318
was it inability to support non - mapreduce applications 

12
09:34:05,318 --> 09:34:09,200
it had several terrible resource utilization 

13
09:34:09,200 --> 09:34:13,770
this meant that for advanced applications such as graph analysis that

14
09:34:13,770 --> 09:34:16,020
required different ways of modelling and

15
09:34:16,020 --> 09:34:20,950
looking at data you would need to move your data to another platform 

16
09:34:20,950 --> 09:34:23,310
that a lot of work if your data is big 

17
09:34:25,540 --> 09:34:30,510
adding yarn in between hdfs and the applications enabled

18
09:34:30,510 --> 09:34:35,510
new systems to be built focusing on different types of big data applications

19
09:34:35,510 --> 09:34:40,280
such as giraph for graph data analysis storm for

20
09:34:40,280 --> 09:34:44,190
streaming data analysis and spark for in - memory analysis 

21
09:34:45,360 --> 09:34:49,150
yarn does so by providing a standard framework

22
09:34:49,150 --> 09:34:53,530
that supports customized application development in the hadoop ecosystem 

23
09:34:54,990 --> 09:34:58,850
yarn lets you extract maximum benefits from your data sets

24
09:34:58,850 --> 09:35:03,320
by letting you use the tools you think are best for your big data 

25
09:35:04,860 --> 09:35:08,810
let take a peek into the architecture of yarn without getting too technical 

26
09:35:10,430 --> 09:35:15,620
in this picture notice the resource manager in the center and

27
09:35:15,620 --> 09:35:19,810
the node managers on each of the three nodes on the right 

28
09:35:21,910 --> 09:35:26,450
the resource manager controls all the resources and decides who gets what 

29
09:35:28,290 --> 09:35:33,580
node manager operates at machine level and is in charge of a single machine 

30
09:35:35,490 --> 09:35:38,180
together the resource manager and

31
09:35:38,180 --> 09:35:41,240
the node manager form the data computation framework 

32
09:35:42,410 --> 09:35:45,530
each application gets an application master 

33
09:35:46,560 --> 09:35:50,190
it negotiates resource from the resource manager and

34
09:35:50,190 --> 09:35:53,640
it talks to node manager to get its tasks completed 

35
09:35:55,820 --> 09:36:00,900
notice the ovals labeled container the container

36
09:36:00,900 --> 09:36:06,240
is an abstract notions that signifies a resource that is

37
09:36:06,240 --> 09:36:10,760
a collection of cpu memory disk network and

38
09:36:10,760 --> 09:36:15,680
other resources within the compute note to simplify and

39
09:36:15,680 --> 09:36:20,020
be less precise you can think of a container and the machine 

40
09:36:21,440 --> 09:36:24,460
we looked at the essential gears of the yarn

41
09:36:24,460 --> 09:36:28,880
engine to give you an idea of the key components of yarn 

42
09:36:28,880 --> 09:36:32,580
now when you hear terms like resource manager node manager and

43
09:36:32,580 --> 09:36:37,670
container you will have an understanding of what tasks they are responsible for 

44
09:36:40,150 --> 09:36:46,400
here is a real life example to show the strength hadoop 2 0 over 1 0 

45
09:36:46,400 --> 09:36:49,940
yahoo was able to run almost

46
09:36:49,940 --> 09:36:53,910
twice as many jobs per day with yarn than with hadoop 1 0 

47
09:36:53,910 --> 09:37:01,100
they also experienced a substantial increase in cpu utilization 

48
09:37:01,100 --> 09:37:02,130
yahoo ! 

49
09:37:02,130 --> 09:37:05,710
even claimed that upgrading to yarn was equal into

50
09:37:05,710 --> 09:37:09,850
adding 1000 machines to their 2500 machine cluster 

51
09:37:09,850 --> 09:37:10,480
that big 

52
09:37:12,450 --> 09:37:16,790
yarn success is evident from an explosive growth of

53
09:37:16,790 --> 09:37:19,940
different application that the hadoop ecosystem now has 

54
09:37:21,330 --> 09:37:22,660
new to yarn 

55
09:37:22,660 --> 09:37:26,750
you can use the tool of your choice over your big data without any hassle 

56
09:37:27,920 --> 09:37:33,170
compare this with hadoop 1 0 which was limited to mapreduce alone 

57
09:37:35,260 --> 09:37:39,630
let review a summary of the key take - aways about yarn 

58
09:37:39,630 --> 09:37:44,540
yarn gives you many ways for applications to extract value from data 

59
09:37:45,720 --> 09:37:50,490
it lets you run many distributed applications over the same hadoop cluster 

60
09:37:51,860 --> 09:37:56,630
in addition yarn reduces the need to move data around and

61
09:37:56,630 --> 09:38:01,110
supports higher resource utilization resulting in lower costs 

62
09:38:02,690 --> 09:38:07,310
it a scalable platform that has enabled growth of several applications

63
09:38:07,310 --> 09:38:11,490
over the hdfs enriching the hadoop ecosystem 

1
19:11:05,880 --> 19:11:11,100
at this point you hopefully have a good general idea of what big data means and

2
19:11:11,100 --> 19:11:12,380
why big data is important 

3
19:11:13,590 --> 19:11:16,190
so now we need to focus on what to do

4
19:11:16,190 --> 19:11:18,090
when we have an application that uses big data 

5
19:11:19,980 --> 19:11:23,780
in this video we focus on the problem of managing big data 

6
19:11:24,780 --> 19:11:25,970
so at the end of the video 

7
19:11:25,970 --> 19:11:30,390
you should be able to describe what data management means in general and then

8
19:11:30,390 --> 19:11:34,180
specifically recognize the issues that are involved in the management of big data 

9
19:11:36,760 --> 19:11:39,620
first let see what data management means in general 

10
19:11:40,910 --> 19:11:44,120
instead of giving you definitions of data management 

11
19:11:44,120 --> 19:11:46,760
let think of some questions that must be asked and

12
19:11:46,760 --> 19:11:49,880
answered well if you are to manage a reasonable amount of data 

13
19:11:51,250 --> 19:11:54,350
now we can not possibly cover all questions one should ask for

14
19:11:54,350 --> 19:11:59,020
a data - centric application but here are some important ones which range from how

15
19:11:59,020 --> 19:12:04,290
we get the data to how we work with it to how we secure it from malicious users 

16
19:12:05,300 --> 19:12:07,386
we will visit these issues one at a time 

1
14:23:13,280 --> 14:23:15,990
ingestion means the process of getting the data

2
14:23:15,990 --> 14:23:19,040
into the data system that we are building or using 

3
14:23:19,040 --> 14:23:22,440
now you might think why is it worth talking about 

4
14:23:23,600 --> 14:23:26,560
we will just read the data from somewhere like a file 

5
14:23:26,560 --> 14:23:29,320
and then using some command place it into the data system 

6
14:23:30,550 --> 14:23:34,180
or we will have have some kind of a web form or

7
14:23:34,180 --> 14:23:37,870
other visual interface and just fill it in so that the data goes into the system 

8
14:23:39,500 --> 14:23:43,180
both of these ways of data ingestion are valid 

9
14:23:43,180 --> 14:23:44,650
in fact they are valid for

10
14:23:44,650 --> 14:23:47,420
some big data systems like your airline reservation system 

11
14:23:48,470 --> 14:23:51,850
however when you think of a large scale system

12
14:23:51,850 --> 14:23:55,990
you wold like to have more automation in the data ingestion processes 

13
14:23:55,990 --> 14:24:00,060
and data ingestion then becomes a part of the big data management infrastructure 

14
14:24:01,200 --> 14:24:05,920
so here are some questions you might want to ask when you automate data ingestion 

15
14:24:07,140 --> 14:24:08,830
now take a minute to read the questions 

16
14:24:11,140 --> 14:24:15,350
we will look at two examples to explore them in greater detail 

17
14:24:16,660 --> 14:24:20,380
the first example is that of a hospital information system that we discussed in

18
14:24:20,380 --> 14:24:23,600
course one in the context of precision medicine 

19
14:24:23,600 --> 14:24:27,650
we said that hospitals collect terabytes of medical record

20
14:24:27,650 --> 14:24:30,780
from different departments and be considered big data systems 

21
14:24:32,450 --> 14:24:36,630
the second example is a cloud based data store where many people upload their

22
14:24:36,630 --> 14:24:41,030
messages chats pictures videos music and so fourth 

23
14:24:41,030 --> 14:24:45,100
the cloud storage also supports active communication between the members and

24
14:24:45,100 --> 14:24:46,920
store their communication in real time 

25
14:24:48,650 --> 14:24:53,060
so let think of a hypothetical hospital information information and

26
14:24:53,060 --> 14:24:55,970
the answer to depressions that we are putting there 

27
14:24:55,970 --> 14:24:57,750
now do not take the numbers to be very accurate 

28
14:24:57,750 --> 14:24:58,630
they are just examples 

29
14:24:59,630 --> 14:25:02,330
but it illustrates some important points 

30
14:25:02,330 --> 14:25:08,260
one note that there are two kinds of likeness associated with data 

31
14:25:08,260 --> 14:25:12,410
some data like medical images are large data objects by themselves 

32
14:25:14,300 --> 14:25:18,880
secondly the records themselves are quite small but

33
14:25:18,880 --> 14:25:21,740
the size of the total collection of records is very high 

34
14:25:23,580 --> 14:25:28,546
two while there is a lot of patient data the number of data sources that is

35
14:25:28,546 --> 14:25:33,517
the different departmental systems contributing to the total information

36
14:25:33,517 --> 14:25:36,408
system does not change very much over time 

37
14:25:36,408 --> 14:25:41,739
three the rate of data ingestion is not enormous and is often proportional

38
14:25:41,739 --> 14:25:46,668
to the number of patient activities that takes place at the hospital 

39
14:25:46,668 --> 14:25:49,993
four the system contains medical records so

40
14:25:49,993 --> 14:25:54,778
data can never be discarded even when there are errors in the data 

41
14:25:54,778 --> 14:26:00,020
the errors in this specific case are flagged but the data is retained 

42
14:26:01,590 --> 14:26:05,740
now this is the kind of rule called an error handling policy 

43
14:26:05,740 --> 14:26:08,120
which might be different for different application problems 

44
14:26:09,960 --> 14:26:15,180
an air handling policy is part of a larger scheme of policies called

45
14:26:15,180 --> 14:26:16,050
ingestion policies 

46
14:26:18,530 --> 14:26:22,630
another kind of ingestion policy involves decisions regarding what the system should

47
14:26:22,630 --> 14:26:26,480
do if the data rate suddenly increases or becomes suspiciously low 

48
14:26:27,540 --> 14:26:31,420
in this example we have deliberately decided not to include it in the design 

49
14:26:32,630 --> 14:26:36,150
now compare the previous case with the case of the online store of

50
14:26:36,150 --> 14:26:38,000
personal information 

51
14:26:38,000 --> 14:26:40,800
again this is just an imaginary example 

52
14:26:40,800 --> 14:26:43,270
so do not think of all the parameters to be exact 

53
14:26:44,670 --> 14:26:49,390
now in this case one the store will have a fast growing membership 

54
14:26:50,680 --> 14:26:54,370
each member will use multiple devices to capture and ingest their data 

55
14:26:56,060 --> 14:27:02,510
two over all members together the site will be updated at a really fast rate 

56
14:27:02,510 --> 14:27:08,182
making this a large volume data store with a fast ingest rate 

57
14:27:08,182 --> 14:27:13,150
three in this system our primary challenge is to keep up with the data

58
14:27:13,150 --> 14:27:18,780
rate and hence erroneous data will be discarded after just one edit to reinvest 

59
14:27:20,760 --> 14:27:25,380
four now there is an actual policy for handling data overflow 

60
14:27:25,380 --> 14:27:29,720
which essentially says keep the excess data in a site store 

61
14:27:29,720 --> 14:27:32,560
and ingest them when the data rate becomes slower 

62
14:27:32,560 --> 14:27:36,660
but if the site store starts getting full

63
14:27:36,660 --> 14:27:40,980
start dropping some incoming data at a rate of 0 1 at a time 

64
14:27:42,470 --> 14:27:45,220
now we should see why data ingestion together with it 

65
14:27:45,220 --> 14:27:48,320
policies should be an integral part of a big data system 

66
14:27:48,320 --> 14:27:51,390
especially when it involves storing fast data 

1
04:51:03,770 --> 04:51:08,720
a very significant aspect of data management is to document define 

2
04:51:08,720 --> 04:51:12,580
implement and test the set of operations that are required for

3
04:51:12,580 --> 04:51:13,610
a specific application 

4
04:51:14,910 --> 04:51:19,570
as we will see later in the specialization some operations are independent of

5
04:51:19,570 --> 04:51:24,770
the type of data and some others would require us to know the nature of the data

6
04:51:24,770 --> 04:51:27,890
because the operations make use of a particular data model 

7
04:51:27,890 --> 04:51:29,180
that is the way that it is structured 

8
04:51:30,760 --> 04:51:34,130
in general there are two broad divisions of operations 

9
04:51:35,620 --> 04:51:38,950
those that work on a singular object and

10
04:51:38,950 --> 04:51:41,590
those that work on collections of data objects 

11
04:51:43,020 --> 04:51:47,520
in the first case an operation that crops an image 

12
04:51:47,520 --> 04:51:52,120
that means extracts a sub area from an area of pixels 

13
04:51:52,120 --> 04:51:56,280
is a single object operation because we consider the image as a single object 

14
04:51:58,250 --> 04:52:01,860
one can think of many subclasses of the second category

15
04:52:01,860 --> 04:52:04,790
where the operations are on data collections 

16
04:52:04,790 --> 04:52:09,300
we briefly referred to three very common operations that can be done

17
04:52:09,300 --> 04:52:10,640
regardless of the nature of the data 

18
04:52:11,820 --> 04:52:16,610
the first is to take a collection and filter out a subset of that collection 

19
04:52:16,610 --> 04:52:20,180
the most obvious case is selecting a subset from a set 

20
04:52:20,180 --> 04:52:24,010
in this example we select circles whose number is greater than three 

21
04:52:25,050 --> 04:52:29,830
a second case is merging two collections together to form a larger collection 

22
04:52:31,220 --> 04:52:32,780
in the example shown 

23
04:52:32,780 --> 04:52:38,380
two three structure data items are merged by fusing the node with a common property 

24
04:52:38,380 --> 04:52:38,880
that is two 

25
04:52:40,520 --> 04:52:44,650
in the last case we compute a function on a collection and

26
04:52:44,650 --> 04:52:46,530
return the value of the function 

27
04:52:46,530 --> 04:52:49,120
so in this example the function is a simple count 

28
04:52:50,450 --> 04:52:54,230
in the real world this kind of aggregate function can be very complicated 

29
04:52:55,540 --> 04:53:00,470
we will come back to this issue when we talk more about map readings but

30
04:53:00,470 --> 04:53:03,670
in this course we will talk about many different data operations 

31
04:53:04,750 --> 04:53:07,580
every operator must be efficient 

32
04:53:07,580 --> 04:53:12,550
that means every operator must perform its task as fast as possible

33
04:53:12,550 --> 04:53:16,400
by taking up as little memory or our disk as possible 

34
04:53:17,580 --> 04:53:20,170
obviously the time to perform an operation

35
04:53:20,170 --> 04:53:23,490
will depend on the size of the input and the size of the output 

36
04:53:24,590 --> 04:53:29,300
so if there is an opportunity to use concurrency where the operator can split

37
04:53:29,300 --> 04:53:33,880
its data and have different threads operate on the pieces at the same time 

38
04:53:33,880 --> 04:53:35,080
it should definitely do so 

39
04:53:36,820 --> 04:53:40,120
we present a simple example of an operator we saw on the previous slide 

40
04:53:41,290 --> 04:53:43,600
so this operator called selection 

41
04:53:43,600 --> 04:53:48,390
refers to choosing a subset of a set based on some conditions 

42
04:53:48,390 --> 04:53:52,120
here we are choosing a subset of circles whose numbers are even 

43
04:53:53,460 --> 04:53:58,080
to make it more efficient we can take the input data and

44
04:53:58,080 --> 04:54:02,150
partition it randomly into two groups 

45
04:54:02,150 --> 04:54:04,040
now for each group 

46
04:54:04,040 --> 04:54:08,850
we can concurrently run the subset algorithm and get the partial results 

47
04:54:10,020 --> 04:54:13,880
for this operation the partial results can be directly sent to the output

48
04:54:13,880 --> 04:54:15,880
without any additional processing step 

1
09:45:18,790 --> 09:45:22,740
okay we in essence store the data efficiently 

2
09:45:22,740 --> 09:45:23,670
but is it any good 

3
09:45:24,930 --> 09:45:28,970
are there ways of knowing if the data is potentially error free and useful for

4
09:45:28,970 --> 09:45:29,770
the intended purpose 

5
09:45:30,940 --> 09:45:33,500
this is the issue of data quality 

6
09:45:33,500 --> 09:45:36,300
there are many reasons why any data application 

7
09:45:36,300 --> 09:45:40,470
especially larger applications need to be mindful of data quality 

8
09:45:41,490 --> 09:45:44,650
here are three reasons of course there are more that we do not mention 

9
09:45:45,730 --> 09:45:50,480
the first reason emphasizes that the ultimate use of big data

10
09:45:50,480 --> 09:45:53,040
is its ability to give us actionable insight 

11
09:45:54,130 --> 09:45:58,030
poor quality data leads to poor analysis and hence to poor decisions 

12
09:45:59,970 --> 09:46:04,820
the second related data in regulated industries in areas like clinical

13
09:46:04,820 --> 09:46:08,710
trials for pharmaceutical companies or financial data like from banks 

14
09:46:09,970 --> 09:46:14,440
errors in data in these industries can regulate regulations leading to

15
09:46:14,440 --> 09:46:15,290
legal complications 

16
09:46:16,980 --> 09:46:20,230
the third factor is different than the first two 

17
09:46:20,230 --> 09:46:24,620
it says if your big data should be used by other people or

18
09:46:24,620 --> 09:46:27,290
a third party software it very important for

19
09:46:27,290 --> 09:46:31,059
the data to give good quality to gain trust as a leader provider 

20
09:46:32,260 --> 09:46:36,270
a class of big data applications is scientific where large 

21
09:46:36,270 --> 09:46:40,180
integrated collections of data are created by human experts to

22
09:46:40,180 --> 09:46:42,570
understand scientific questions 

23
09:46:42,570 --> 09:46:46,960
ensuring accuracy of data will lead to correct human engagement and

24
09:46:46,960 --> 09:46:48,220
interaction with the data system 

25
09:46:50,370 --> 09:46:53,170
gartner the well known technology research and

26
09:46:53,170 --> 09:46:57,720
advising company created a 2015 industry report on big data qualities 

27
09:46:59,300 --> 09:47:02,240
in this report they identify the approaches to meeting

28
09:47:02,240 --> 09:47:04,210
the data quality requirements in the industry 

29
09:47:05,240 --> 09:47:09,690
this methods include the adherence to standards where applicable 

30
09:47:09,690 --> 09:47:13,150
it also refers to the need to create the rules in the data system

31
09:47:13,150 --> 09:47:17,870
that can be use to check if the data passes a set of correct this qualities 

32
09:47:17,870 --> 09:47:20,950
like is even employed above 18 

33
09:47:20,950 --> 09:47:25,490
it also includes methods to clean the data if it found to have errors or

34
09:47:25,490 --> 09:47:26,260
inconsistencies 

35
09:47:27,720 --> 09:47:32,772
further the data quality management should include a well define work flow on how

36
09:47:32,772 --> 09:47:37,690
low quality data could be corrected to bring it back to a high level of quality 

1
19:32:56,920 --> 19:32:58,770
there are many ways of looking at scalability 

2
19:32:59,880 --> 19:33:02,600
and we will consider them as we go forward in the course 

3
19:33:03,930 --> 19:33:07,870
one way is to consider scaling up and scaling out 

4
19:33:09,360 --> 19:33:13,300
simply put it is a decision between making a machine

5
19:33:13,300 --> 19:33:18,264
that makes a server more powerful versus adding more machines 

6
19:33:18,264 --> 19:33:22,240
the first choice will involve adding more memory 

7
19:33:22,240 --> 19:33:26,020
replacing processes with process of more course and

8
19:33:26,020 --> 19:33:30,060
adding more processes within a system with a very fast internet connection speed 

9
19:33:31,360 --> 19:33:36,520
the second choice will involve adding more machines to a relatively slower network 

10
19:33:37,880 --> 19:33:39,290
now there are no absolutes here 

11
19:33:40,560 --> 19:33:44,490
in many cases we will choose the former to get more performance for

12
19:33:44,490 --> 19:33:45,740
large leader systems 

13
19:33:47,920 --> 19:33:49,760
the general trend in the big data world 

14
19:33:49,760 --> 19:33:52,590
however is to target the scale out option 

15
19:33:54,330 --> 19:33:56,950
most big data management systems today

16
19:33:56,950 --> 19:34:00,140
are designed to operate over a cluster up machines and

17
19:34:00,140 --> 19:34:05,490
have the ability to adjust as more machines are added and when machines fail 

18
19:34:06,740 --> 19:34:11,060
cluster management and management of data operations over a cluster

19
19:34:11,060 --> 19:34:14,930
is an important component in today big data management systems 

20
19:34:17,110 --> 19:34:20,990
now we will briefly touch upon the complex issue of data security 

21
19:34:22,310 --> 19:34:26,700
it is obvious that having more sensitive data implies the need for more security 

22
19:34:27,960 --> 19:34:30,650
if the data is within the walls of an organization 

23
19:34:30,650 --> 19:34:33,180
we will still need a security plan 

24
19:34:33,180 --> 19:34:36,700
however if a big data system is deployed in the cloud 

25
19:34:36,700 --> 19:34:40,900
over multiple machines security of data becomes an even bigger challenge 

26
19:34:42,200 --> 19:34:45,800
so now we need to ensure the security for not only the machines 

27
19:34:45,800 --> 19:34:49,880
but also the network which will be heavily used during data transfer

28
19:34:49,880 --> 19:34:52,610
across different phases of data operations 

29
19:34:52,610 --> 19:34:58,200
for example if the data store and the data analysis are performed over

30
19:34:58,200 --> 19:35:03,760
different sets of servers that every analysis operation gets to

31
19:35:03,760 --> 19:35:08,630
an additional overhead of encrypting the data as the data gets to the network and

32
19:35:08,630 --> 19:35:11,030
decrypting when it gets to the processing server 

33
19:35:12,190 --> 19:35:14,880
this effectively increases operational cost 

34
19:35:16,310 --> 19:35:21,050
as of today while there are many security products the methods for

35
19:35:21,050 --> 19:35:26,290
ensuring security and achieving data processing efficiency at the same time

36
19:35:26,290 --> 19:35:28,620
remains a research issue in big data management 

1
15:08:23,530 --> 15:08:28,150
now the goal of a storage infrastructure obviously is to store data 

2
15:08:28,150 --> 15:08:30,330
there are two storage related issues we consider here 

3
15:08:32,590 --> 15:08:35,920
the first is the issue of capacity 

4
15:08:35,920 --> 15:08:38,110
how much storage should we allocate 

5
15:08:38,110 --> 15:08:41,990
that means what should be the size of the memory how large and

6
15:08:41,990 --> 15:08:44,280
how many disk units should we have and so forth 

7
15:08:45,700 --> 15:08:49,760
there is also the issue of scalability 

8
15:08:49,760 --> 15:08:53,150
should the storage devices be attached directly to the computers

9
15:08:53,150 --> 15:08:56,690
to make the direct io fast but less scalable 

10
15:08:56,690 --> 15:08:59,310
or should the storage be attached to the network

11
15:09:00,340 --> 15:09:02,820
that connect the computers in the cluster 

12
15:09:02,820 --> 15:09:05,690
this will make disk access a bit slower but

13
15:09:05,690 --> 15:09:08,210
allows one to add more storage to the system easily 

14
15:09:09,230 --> 15:09:12,060
now these questions do not have a simple answer 

15
15:09:12,060 --> 15:09:15,200
if you are interested you may look up a website given on your reading list 

16
15:09:16,470 --> 15:09:21,080
a different class of questions deals with the speed of the iu operation 

17
15:09:22,090 --> 15:09:26,620
this question is often addressed with this kind of diagram here called a memory

18
15:09:26,620 --> 15:09:30,480
hierarchy or storage hierarchy or sometimes memory storage hierarchy 

19
15:09:32,270 --> 15:09:36,740
the top of the pyramid structure shows a part of memory called cache memory 

20
15:09:37,840 --> 15:09:40,410
that lives inside the cpu and is very fast 

21
15:09:40,410 --> 15:09:44,650
there are different levels of cache called l1 l2 l3 

22
15:09:44,650 --> 15:09:50,229
where l3 is the slowest but still faster than what we call memory 

23
15:09:50,229 --> 15:09:53,110
shown here in orange near the middle 

24
15:09:54,400 --> 15:09:56,990
the figure shows their speed in terms of response times 

25
15:09:58,070 --> 15:10:01,410
notice the memory streamed here is 65 nanoseconds per access 

26
15:10:02,930 --> 15:10:03,892
in contrast 

27
15:10:03,892 --> 15:10:09,030
the speed of the traditional hard disk is of the order of 10 milliseconds 

28
15:10:10,510 --> 15:10:14,918
this gap has prompted the design of many data structures and

29
15:10:14,918 --> 15:10:19,768
algorithms that use a hard disk but tries to minimize the cost of

30
15:10:19,768 --> 15:10:24,455
the io operations between the fast memory and the slower disk 

31
15:10:24,455 --> 15:10:27,543
but more recently a newer kind of storage 

32
15:10:30,329 --> 15:10:32,463
very similar to the flash drives or

33
15:10:32,463 --> 15:10:36,738
usbs that we regularly use have made an entry as a new storage medium 

34
15:10:36,738 --> 15:10:40,820
these devices are called ssds or solid state devices 

35
15:10:40,820 --> 15:10:42,780
they are much faster than spinning hard disks 

36
15:10:43,990 --> 15:10:47,795
an even newer addition is the method called nvme 

37
15:10:47,795 --> 15:10:50,787
nvm stands for non - volatile memory 

38
15:10:50,787 --> 15:10:55,600
that makes data transfer between ssds and memory much faster 

39
15:10:56,940 --> 15:11:01,317
what all this means in a big data system is that now we have the choice of

40
15:11:01,317 --> 15:11:06,077
architecting a storage infrastructure by choosing how much of each type of

41
15:11:06,077 --> 15:11:07,684
storage we need to have 

42
15:11:07,684 --> 15:11:12,842
in my own research with large amounts of data i have found that using ssds

43
15:11:12,842 --> 15:11:18,597
speed up all look up operations in data by at least a factor of ten over hard drives 

44
15:11:18,597 --> 15:11:22,057
of course the flip side of this is the cost factor 

45
15:11:22,057 --> 15:11:26,305
the components become increasingly more expensive as we go from the lower layers

46
15:11:26,305 --> 15:11:28,187
of the pyramid to the upper layers 

47
15:11:28,187 --> 15:11:31,972
so ultimately it becomes an issue of cost - benefit tradeoff 

1
06:19:54,025 --> 06:19:59,173
 sound let us consider a real life application to

2
06:19:59,173 --> 06:20:05,360
demonstrate the utility and the challenges of big data 

3
06:20:05,360 --> 06:20:08,540
many industries naturally deal with large amounts of data 

4
06:20:09,540 --> 06:20:13,585
for our discussion we consider an energy company that provides gas and

5
06:20:13,585 --> 06:20:16,248
electricity to its consumers in an urban area 

6
06:20:18,230 --> 06:20:22,354
in this news report you can see that commonwealth edison or con ed 

7
06:20:22,354 --> 06:20:24,829
the gas and electric provider of new york 

8
06:20:24,829 --> 06:20:29,440
decided to place smart meters all through its jurisdictions 

9
06:20:29,440 --> 06:20:32,000
that comes to 4 7 million smart meters 

10
06:20:33,540 --> 06:20:39,150
now smart meters are smart because aside from measuring energy consumption 

11
06:20:39,150 --> 06:20:42,730
they have a two way communication capability between the meter and

12
06:20:42,730 --> 06:20:45,800
the central system at the gas and electric company 

13
06:20:45,800 --> 06:20:50,590
in other words they generate real time data from the meters to be stored and

14
06:20:50,590 --> 06:20:52,060
processed at the central facility 

15
06:20:53,710 --> 06:20:54,280
how much data 

16
06:20:55,610 --> 06:20:57,070
according to this report 

17
06:20:57,070 --> 06:21:00,680
the number of data received at the center is 1 5 billion per day 

18
06:21:00,680 --> 06:21:07,480
so the system will not only consume this data but process it and

19
06:21:08,650 --> 06:21:14,750
produce output at 15 - minute intervals and sometimes 5 minute intervals 

20
06:21:14,750 --> 06:21:16,270
let do the math 

21
06:21:16,270 --> 06:21:18,650
that comes to ingesting and processing

22
06:21:20,620 --> 06:21:24,360
about 10 5 million data points per 15 minutes 

23
06:21:26,150 --> 06:21:30,830
so what kind of computation must take place within these 15 minutes 

24
06:21:30,830 --> 06:21:35,760
well one obvious computation is billing where one needs to compute who especially

25
06:21:35,760 --> 06:21:40,290
in the commercial sector who actually owns the meter and should be billed 

26
06:21:41,490 --> 06:21:43,980
this requires combining the meter data

27
06:21:43,980 --> 06:21:47,640
with the data in the customer database maintained by the company 

28
06:21:47,640 --> 06:21:51,390
but let just consider computation related to analytics 

29
06:21:51,390 --> 06:21:55,180
we can list at least four different kinds of computations 

30
06:21:56,610 --> 06:22:01,980
the first is computing the consumption pattern per user not per meter 

31
06:22:01,980 --> 06:22:06,840
per user where the output is a histogram of hourly usage 

32
06:22:06,840 --> 06:22:10,270
so the x axis of the histogram is hourly intervals and

33
06:22:10,270 --> 06:22:13,380
the y - axis is a number of units consumed 

34
06:22:14,950 --> 06:22:19,310
this leads to the computed both daily and over larger time periods 

35
06:22:19,310 --> 06:22:22,560
to determine the hourly requirements for this consumer 

36
06:22:24,430 --> 06:22:27,970
the second computation relates to estimating the effects of

37
06:22:27,970 --> 06:22:31,160
outdoor temperature on the electricity consumption of each consumer 

38
06:22:32,380 --> 06:22:34,890
for those you who are statistically inclined 

39
06:22:34,890 --> 06:22:39,860
this often involves fitting a piece - wise linear progression model to the data 

40
06:22:42,000 --> 06:22:47,080
the third task is to extract the daily consumption trends that occur

41
06:22:47,080 --> 06:22:49,470
regardless of the outdoor temperature 

42
06:22:49,470 --> 06:22:53,590
this is again a statistical computation and may require something like

43
06:22:53,590 --> 06:22:57,570
a periodic alter regression algorithm for time series theta 

44
06:22:57,570 --> 06:22:59,290
the algorithm is not that important there 

45
06:23:00,410 --> 06:23:05,360
what is more important is the ability to make make good prediction has a direct

46
06:23:05,360 --> 06:23:09,690
economic impact because the company needs to buy energy from others 

47
06:23:09,690 --> 06:23:14,280
for example an under - prediction implies they will end up paying more for

48
06:23:14,280 --> 06:23:17,940
buying energy at the last moment to meet the consumer requirements 

49
06:23:19,870 --> 06:23:24,590
the fourth task is to find groups of similar consumers based on their usage

50
06:23:24,590 --> 06:23:28,860
pattern so that the company can determine how many distinct groups of customers

51
06:23:28,860 --> 06:23:33,330
there are and design targeted energy saving campaigns for each group 

52
06:23:34,370 --> 06:23:39,730
this requires finding similarities over large number of time series data 

53
06:23:39,730 --> 06:23:41,140
which is a complex computation 

54
06:23:42,430 --> 06:23:46,110
regardless of the number and complexity of computation required 

55
06:23:46,110 --> 06:23:51,220
the company constrained by the fact that it has only 15 minutes to process the data

56
06:23:51,220 --> 06:23:56,942
before the next and computation has to be performed 

57
06:23:56,942 --> 06:24:00,550
that issue is not just the bigness of the data but

58
06:24:00,550 --> 06:24:03,610
the strip and strings of the arrival to output time 

59
06:24:05,520 --> 06:24:12,150
the analytics has value only if it can be completed within the life - cycle deadline 

60
06:24:12,150 --> 06:24:16,220
so if we were to design a big data system for such a company you would need to

61
06:24:16,220 --> 06:24:20,730
understand how much are the computation can be executed in parallel and

62
06:24:20,730 --> 06:24:25,180
how many machines with what kind of capability are required to handle the data

63
06:24:25,180 --> 06:24:29,510
rate and the number and complexity of the analytical computations needed 

1
12:44:30,370 --> 12:44:31,820
hi my name is chad berkley 

2
12:44:31,820 --> 12:44:36,750
i am the cto of flightstats and i am here today to talk to you a little bit about

3
12:44:36,750 --> 12:44:40,740
our platform and how we acquire and process data 

4
12:44:40,740 --> 12:44:44,320
but first of all i would like to start by just kind of introducing the company and

5
12:44:44,320 --> 12:44:49,540
telling you a little bit about what we are all about 

6
12:44:49,540 --> 12:44:53,940
so flightstats is a data company and

7
12:44:53,940 --> 12:44:59,340
we basically are the leading provider of global real - time flight status data 

8
12:44:59,340 --> 12:45:04,110
we pull in data from over 500 sources and we aggregate that data back together and

9
12:45:04,110 --> 12:45:09,140
we sell it out to our customers which our other businesses as well as consumers 

10
12:45:10,290 --> 12:45:13,920
so just to give you a little bit of information about the scope and

11
12:45:13,920 --> 12:45:15,460
scale of what we do 

12
12:45:15,460 --> 12:45:18,210
like i said we have over 500 sources of data 

13
12:45:18,210 --> 12:45:23,300
and on a daily basis we process about 15 million flight events 

14
12:45:23,300 --> 12:45:27,380
those that includes landings arrivals departures 

15
12:45:28,510 --> 12:45:33,600
any time the status of the flight changes we got some sort of message on that 

16
12:45:33,600 --> 12:45:38,790
we process about 260 million aircraft positions per day so we have an extensive

17
12:45:38,790 --> 12:45:44,980
network that monitors graph positions for realtime flight tracking applications 

18
12:45:44,980 --> 12:45:49,020
and we also handle about one million pnrs or passenger name records 

19
12:45:49,020 --> 12:45:54,590
which are the actual data type of an itenerary any time you

20
12:45:54,590 --> 12:45:59,520
book travel a pnr is created for you for your travel 

21
12:45:59,520 --> 12:46:04,770
and it includes all of the segments like air travel ferries 

22
12:46:04,770 --> 12:46:09,110
hotels taxis anyhting that can be scheduled on your trip 

23
12:46:10,390 --> 12:46:12,650
and we basically take in all that data and

24
12:46:12,650 --> 12:46:17,080
we aggregate it together and we sell it back out 

25
12:46:17,080 --> 12:46:20,930
and how most people kind of know us for flightstats com that our

26
12:46:20,930 --> 12:46:25,260
consumer site for business where we handle about 2 million daily requests 

27
12:46:25,260 --> 12:46:27,550
and we handle about 1 million mobile app requests 

28
12:46:28,790 --> 12:46:35,710
that our b to b side we cert out a lot of data by apis and real time data feeds 

29
12:46:35,710 --> 12:46:39,480
people make about 15 million api requests to us everyday 

30
12:46:39,480 --> 12:46:43,180
and we also send out about one and half million flight and trip notifications 

31
12:46:43,180 --> 12:46:47,310
so if you get a push notification to your phone 

32
12:46:47,310 --> 12:46:49,930
telling you that your flight is delayed or on time 

33
12:46:49,930 --> 12:46:52,010
that possibly has come from us 

34
12:46:52,010 --> 12:46:57,530
so a little bit about how the data flows through our company 

35
12:46:57,530 --> 12:47:00,820
we bring in all these different types of data and

36
12:47:00,820 --> 12:47:04,260
our sources and it flows through our data acquisition team 

37
12:47:04,260 --> 12:47:10,380
we have a team whose primary purpose is to pull in all sorts of raw data 

38
12:47:10,380 --> 12:47:12,520
a very heterogeneous datasets 

39
12:47:12,520 --> 12:47:18,150
and process that into a normalized form 

40
12:47:18,150 --> 12:47:21,420
so if you kind of follow the blue arrow in this diagram you can see that it goes

41
12:47:21,420 --> 12:47:23,840
through this raw data channel through the data hub 

42
12:47:23,840 --> 12:47:26,660
which the data hub is a central component of our system that i will talk

43
12:47:26,660 --> 12:47:29,000
a little bit more about in a second 

44
12:47:29,000 --> 12:47:32,960
so the blue data the blue line is raw data coming in from the source

45
12:47:32,960 --> 12:47:35,300
it goes through our data acquisition system 

46
12:47:35,300 --> 12:47:39,360
it turns into that purple line which is a normalized form 

47
12:47:39,360 --> 12:47:43,640
it then goes back through our data hub again and into our processing engine 

48
12:47:43,640 --> 12:47:47,210
our processing engine is really where most of the business logic happens 

49
12:47:47,210 --> 12:47:50,210
the first thing we have to do is we have to match any piece of

50
12:47:50,210 --> 12:47:53,390
flight information against a flight that we know about and

51
12:47:53,390 --> 12:47:56,220
primarily the way we know about flights is through schedules 

52
12:47:56,220 --> 12:48:01,560
so we import schedules on a daily basis from one of our partner

53
12:48:01,560 --> 12:48:02,510
schedule providers 

54
12:48:03,750 --> 12:48:08,065
that data once it matched is then processed and

55
12:48:08,065 --> 12:48:12,635
the processing basically looks at each message and tries to determine if

56
12:48:12,635 --> 12:48:15,825
we think that that message needs to be passed on to consumers 

57
12:48:15,825 --> 12:48:19,940
so you know it looks at things like have we seen that message before 

58
12:48:19,940 --> 12:48:21,560
or is it a duplicate 

59
12:48:21,560 --> 12:48:23,530
is it from a data source that we trust 

60
12:48:24,530 --> 12:48:28,120
are there other things going on that we need to know about that

61
12:48:28,120 --> 12:48:31,300
may impact whether that message is true or not 

62
12:48:31,300 --> 12:48:34,370
and once we decide that a message should be passed through 

63
12:48:34,370 --> 12:48:36,020
if you follow the green line 

64
12:48:36,020 --> 12:48:38,860
it goes into our process data channel on our hub and

65
12:48:38,860 --> 12:48:41,400
it then pushed out to a couple different places 

66
12:48:41,400 --> 12:48:42,080
so first of all 

67
12:48:42,080 --> 12:48:46,510
it goes into our production database which is where all of our real time data lives 

68
12:48:46,510 --> 12:48:50,750
that database serves data to our websites 

69
12:48:50,750 --> 12:48:55,230
to our mobile apps and a variety of other places 

70
12:48:55,230 --> 12:48:57,060
it also goes into our data warehouse which 

71
12:48:57,060 --> 12:49:00,330
is where our analytics products use it 

72
12:49:00,330 --> 12:49:02,670
i will talk a bit more about that in a minute 

73
12:49:02,670 --> 12:49:06,910
and then the stream of data actually goes up to so many of our customers 

74
12:49:06,910 --> 12:49:08,490
we do not need a database on our side 

75
12:49:08,490 --> 12:49:10,650
they would rather build a database on their side so

76
12:49:10,650 --> 12:49:14,490
we actually just stream all of the processed data directly to them 

77
12:49:14,490 --> 12:49:17,930
they then host it within their own systems 

78
12:49:20,600 --> 12:49:27,920
so a little bit more about the hub the hub is central to how we move data around 

79
12:49:27,920 --> 12:49:30,960
it a technology that we developed in - house 

80
12:49:30,960 --> 12:49:34,125
and it an object storage based scalable highly available 

81
12:49:34,125 --> 12:49:38,810
multi - channel data queuing and eventing system 

82
12:49:38,810 --> 12:49:44,690
the object storage part is we use amazon s3 to store this data 

83
12:49:44,690 --> 12:49:47,310
so it an object storage system 

84
12:49:47,310 --> 12:49:49,600
it scalable we can scale it horizontally or

85
12:49:49,600 --> 12:49:55,140
vertically depending on but the what type of data is flowing through it 

86
12:49:55,140 --> 12:49:56,250
it highly available meaning 

87
12:49:56,250 --> 12:49:59,350
that we have multiple instances of it in different data centers 

88
12:49:59,350 --> 12:50:02,410
so if one that goes down we can easily pull another one up or

89
12:50:02,410 --> 12:50:05,490
it we are going to cross multiple instances 

90
12:50:05,490 --> 12:50:07,600
and then it multi channel 

91
12:50:07,600 --> 12:50:12,030
so it got a rest interface and any surface can create

92
12:50:12,030 --> 12:50:16,760
a new channel within the system and start posting data to it 

93
12:50:16,760 --> 12:50:21,260
that data is then queued based on the time that it comes in and

94
12:50:21,260 --> 12:50:24,490
other services can be listening for events on those channels 

95
12:50:24,490 --> 12:50:28,240
so as soon as a new piece of data comes into one of those channels 

96
12:50:28,240 --> 12:50:32,330
any service that listening on that channel gets an event notification 

97
12:50:32,330 --> 12:50:35,470
they that service can then act upon that piece of data 

98
12:50:35,470 --> 12:50:39,700
and do whatever processing it may need to do 

99
12:50:39,700 --> 12:50:44,690
this project is open source and anybody can download it and use it 

100
12:50:47,490 --> 12:50:51,230
so a little bit about some of the data that we collect and aggregate 

101
12:50:51,230 --> 12:50:56,560
and flifo is kind of the industry term for flight information 

102
12:50:56,560 --> 12:51:02,360
and primarily we look at kind of the five different parts of flight 

103
12:51:02,360 --> 12:51:06,870
so we pull in information on gate departure and then that becomes a runway

104
12:51:06,870 --> 12:51:10,855
departure basically when the wheels go up that is a runway departure 

105
12:51:10,855 --> 12:51:17,080
we do in - flight positional tracking so when your flight is moving along 

106
12:51:17,080 --> 12:51:21,330
about once every ten seconds we get notified of its latitude and longitude 

107
12:51:21,330 --> 12:51:23,530
and its heading and its speed and

108
12:51:23,530 --> 12:51:27,790
its vertical center descent rate and several other variables 

109
12:51:28,960 --> 12:51:30,230
then once it lands 

110
12:51:30,230 --> 12:51:34,300
as soon as the wheels touch the ground we are notified of a runway arrival and

111
12:51:34,300 --> 12:51:38,890
when the door is opened at the gate we have gate arrival information 

112
12:51:38,890 --> 12:51:44,480
all five of these data fields come in three different forms 

113
12:51:44,480 --> 12:51:47,460
so we have a scheduled scheduled departure and arrival 

114
12:51:47,460 --> 12:51:52,310
we have estimated departure and arrival which can come from a variety of sources 

115
12:51:52,310 --> 12:51:56,710
either airlines airports positional data et cetera 

116
12:51:56,710 --> 12:52:01,120
and then we have actuals so if we have an airport or

117
12:52:01,120 --> 12:52:05,460
an airline that sending us data about exactly when the wheels touch down or

118
12:52:05,460 --> 12:52:09,770
exactly when that door opens on that aircraft we push that data as well 

119
12:52:09,770 --> 12:52:14,100
we also generate some data at flight stops 

120
12:52:14,100 --> 12:52:19,850
so special incidents if an aircraft has an issue it in the news 

121
12:52:19,850 --> 12:52:24,920
we do flag our content with a message from our support staff 

122
12:52:24,920 --> 12:52:27,790
we do some prediction 

123
12:52:27,790 --> 12:52:31,030
right now we are just starting to get into that market or we are actually trying to

124
12:52:31,030 --> 12:52:36,490
predict 24 hours out whether a flight will be delayed disrupted or on time 

125
12:52:37,500 --> 12:52:42,550
we do some synthetic positions so over oceans primarily 

126
12:52:42,550 --> 12:52:47,310
we do not get tracking data on aircraft over the oceans 

127
12:52:47,310 --> 12:52:51,480
there is currently no satellite - based tracking system for aircraft 

128
12:52:51,480 --> 12:52:57,010
so we basically take the last known position a heading a speed 

129
12:52:57,010 --> 12:53:01,330
and if we have a flight plan we will use a flight plan to synthesize the positions

130
12:53:01,330 --> 12:53:05,140
when we are not getting actual positions over large bodies of water 

131
12:53:06,320 --> 12:53:10,560
we also generate notifications so the push alerts 

132
12:53:10,560 --> 12:53:15,680
the preflight emails delay notifications those types of things 

133
12:53:15,680 --> 12:53:19,570
we create those based on what we see in the data that coming in to us 

134
12:53:21,970 --> 12:53:25,630
we store all of our historical data in a data warehouse 

135
12:53:25,630 --> 12:53:28,860
so right now we have six years of historical flight data 

136
12:53:28,860 --> 12:53:31,080
and that powers our analytics products 

137
12:53:31,080 --> 12:53:35,570
so we allow airlines to do competitive analysis and route analysis 

138
12:53:36,690 --> 12:53:38,320
routes are very important to airlines 

139
12:53:38,320 --> 12:53:40,220
that how they compete with each other and

140
12:53:40,220 --> 12:53:45,800
that primarily how they are judged by the faa and

141
12:53:45,800 --> 12:53:50,429
other governmental organizations on whether they are on - time or not 

142
12:53:50,429 --> 12:53:55,070
we also do airport operations analysis things like taxi in and taxi out times 

143
12:53:55,070 --> 12:53:58,910
very important for lots of airports runway utilization 

144
12:53:58,910 --> 12:54:03,230
hourly passenger flows through airports that type of information 

145
12:54:03,230 --> 12:54:05,830
and we do on - time performance metrics so 

146
12:54:05,830 --> 12:54:08,290
airlines can look at how they are doing 

147
12:54:08,290 --> 12:54:10,020
how many flights did they complete 

148
12:54:10,020 --> 12:54:12,870
how many flights were on time within 14 minutes 

149
12:54:14,640 --> 12:54:17,630
and they can compare themselves to their competitors 

150
12:54:19,890 --> 12:54:24,310
so we host all of this in a hybrid cloud architecture 

151
12:54:24,310 --> 12:54:28,560
hybrid cloud basically means that we have our own private datacenter resources and

152
12:54:28,560 --> 12:54:33,120
we also host resources in the amazon web services cloud 

153
12:54:34,870 --> 12:54:38,680
most of our core data processing and service layer is in our private data

154
12:54:38,680 --> 12:54:42,250
center and we are getting ready to spin up a second private data center as well 

155
12:54:42,250 --> 12:54:47,270
right now our main data center is in portland oregon and we are going to spin

156
12:54:47,270 --> 12:54:52,499
another one up on the east coast of the united states probably in q2 or q3 

157
12:54:54,210 --> 12:54:58,050
for our api we try to keep those close to our customers so

158
12:54:58,050 --> 12:55:01,810
api end points and web end points live in amazon 

159
12:55:01,810 --> 12:55:07,170
and they are automatically routed to whichever end point is closest to you 

160
12:55:07,170 --> 12:55:08,640
you will automatically be routed to them 

161
12:55:09,720 --> 12:55:13,700
all of our private infrastructure is virtualized with vmware 

162
12:55:13,700 --> 12:55:17,560
we pretty much have a fully virtualized environment 

163
12:55:20,530 --> 12:55:27,870
and we are an agile shop so we have six small fast teams 

164
12:55:27,870 --> 12:55:32,300
those are product centric teams we allow them to be as customer interactive

165
12:55:32,300 --> 12:55:35,610
as they need to be and we try to make our teams semi autonomous 

166
12:55:35,610 --> 12:55:39,300
so teams get to choose their own tools they get to choose their

167
12:55:39,300 --> 12:55:44,340
own development methodologies they choose a variety of things 

168
12:55:44,340 --> 12:55:47,020
and we physically allow them

169
12:55:47,020 --> 12:55:49,770
to do what they need to do to get their job done as quickly as possible 

170
12:55:51,030 --> 12:55:53,010
we try to automate everything 

171
12:55:53,010 --> 12:55:56,120
you do something once manually and then the next time you write a script or

172
12:55:56,120 --> 12:55:57,480
program to do it 

173
12:55:57,480 --> 12:55:58,700
and we also measure everything 

174
12:55:58,700 --> 12:56:02,190
right now we are taking in about 2 5 billion metrics per month off

175
12:56:02,190 --> 12:56:03,480
of our systems 

176
12:56:03,480 --> 12:56:07,680
and we use those metrics to monitor our application performance 

177
12:56:07,680 --> 12:56:11,890
to monitor revenue to monitor pretty much everything we do in the company 

178
12:56:11,890 --> 12:56:14,390
we really try to enable total system awareness 

179
12:56:14,390 --> 12:56:19,200
everything from the hardware layer up to the website is monitored 

180
12:56:20,240 --> 12:56:24,190
and we use industry best practices and tools and of course we try to recruit and

181
12:56:24,190 --> 12:56:28,740
hire the best talent possible a little bit about our software stock 

182
12:56:28,740 --> 12:56:30,140
we are primarily a java shop 

183
12:56:31,220 --> 12:56:34,830
our core processing services are all written in java 

184
12:56:34,830 --> 12:56:38,870
we do use node js in our microservice edge layer 

185
12:56:38,870 --> 12:56:43,090
and node js is actually starting to move more down into the processing service

186
12:56:43,090 --> 12:56:44,750
layer as well 

187
12:56:44,750 --> 12:56:46,760
we use many different types of data bases 

188
12:56:46,760 --> 12:56:50,220
our primary realtime database is post press 

189
12:56:50,220 --> 12:56:54,140
and we use mongo for the backend of our api services 

190
12:56:55,280 --> 12:56:59,940
on the website we are all html5 and we are moving to react and redux and

191
12:56:59,940 --> 12:57:04,750
we are making use of elasticsearch for quick searching and indexing on our data 

192
12:57:04,750 --> 12:57:08,750
and of course we have ios and android mobile applications 

193
12:57:08,750 --> 12:57:14,610
so you can find out more about flightstats on our website 

194
12:57:14,610 --> 12:57:16,950
if you need data for your applications 

195
12:57:16,950 --> 12:57:20,860
please go to the developer center at developer flightstats com 

196
12:57:20,860 --> 12:57:23,790
you can sign up for a free test account and

197
12:57:23,790 --> 12:57:26,980
be able to pull data directly off of our apis 

198
12:57:26,980 --> 12:57:29,660
if you are interested in the hub like i said that open source 

199
12:57:29,660 --> 12:57:33,540
please check out the git hub page and if you have any additional questions 

200
12:57:33,540 --> 12:57:37,270
feel free to contact myself john berkeley and

201
12:57:37,270 --> 12:57:41,360
i would be happy to answer any of your questions via email 

202
12:57:41,360 --> 12:57:43,690
thanks for listening today and hope you have a great day 

203
12:57:43,690 --> 12:57:44,190
bye 

1
01:42:17,512 --> 01:42:21,240
different data sources in the game industry include using your finger 

2
01:42:21,240 --> 01:42:23,408
what type of device is coming from 

3
01:42:23,408 --> 01:42:25,600
the amount of headsets 

4
01:42:25,600 --> 01:42:26,953
it pretty much infinite 

5
01:42:26,953 --> 01:42:29,720
as far as the number of ways we can bring data in from the game 

6
01:42:29,720 --> 01:42:34,602
and it could be joystick or mouse keyboards there lots of ways

7
01:42:34,602 --> 01:42:38,726
as well as what happens inside the game itself as far cars or

8
01:42:38,726 --> 01:42:42,537
the driving tires flying machines anything 

9
01:42:49,179 --> 01:42:52,479
the volume of data it really depends on the type of game and

10
01:42:52,479 --> 01:42:54,592
how often they want to send the data in 

11
01:42:54,592 --> 01:42:58,260
how many types of events they have tagged and how many users are playing the game 

12
01:42:58,260 --> 01:43:01,891
so if you have a user you have 5 million users that are playing your game and

13
01:43:01,891 --> 01:43:04,844
you are tapping and you are tracking each tap of the screen 

14
01:43:04,844 --> 01:43:07,963
of where they went or each click of the mouse as they were using it 

15
01:43:07,963 --> 01:43:09,982
you are going to get a lot of volume of data 

16
01:43:09,982 --> 01:43:14,854
and so you need to be prepared to bring in a lot

17
01:43:14,854 --> 01:43:20,140
of different data very very quickly 

18
01:43:20,140 --> 01:43:23,322
as far as the variety of data it depends 

19
01:43:23,322 --> 01:43:25,520
you have round pizzas and you have round tires 

20
01:43:25,520 --> 01:43:26,513
i mean they are completely different 

21
01:43:26,513 --> 01:43:30,816
they are both round but there different ways that you are going to want to know how

22
01:43:30,816 --> 01:43:34,104
many pepperoni are on one pizza and how many lug nuts go on a tire 

23
01:43:34,104 --> 01:43:36,510
so the variety is really unlimited as well 

24
01:43:36,510 --> 01:43:38,122
it really just depends on each game 

25
01:43:38,122 --> 01:43:41,430
so you have to be prepared to bring in all kinds of data 

26
01:43:41,430 --> 01:43:45,851
touch data wheel data track speeds anything 

27
01:43:45,851 --> 01:43:50,628
and if you put taxonomy together that you can define as an example of verb 

28
01:43:50,628 --> 01:43:56,021
object location value and any number of other sources then you can basically

29
01:43:56,021 --> 01:44:01,290
track anything you want as long as they all fall into the same kind of buckets 

30
01:44:01,290 --> 01:44:03,705
and the buckets can be different sizes based on the type of events 

31
01:44:03,705 --> 01:44:06,804
they do not all need to be 4 some can be 2 some can be 20 

32
01:44:06,804 --> 01:44:08,218
it does not really matter 

33
01:44:15,280 --> 01:44:19,384
the modeling challenge has really come down to who designs the structure at which

34
01:44:19,384 --> 01:44:22,355
you store your data and how you want to retrieve that data 

35
01:44:22,355 --> 01:44:26,488
those kind of of storage and retrieval models are very very important 

36
01:44:26,488 --> 01:44:29,098
because what it really comes down to is speed 

37
01:44:29,098 --> 01:44:32,119
you can record a lot of data and it can take you five years to query it 

38
01:44:32,119 --> 01:44:33,710
it does not really do you any good 

39
01:44:33,710 --> 01:44:38,347
so you need to make sure you plan for reporting speed because that ultimately

40
01:44:38,347 --> 01:44:42,865
what within the organisation needs is the ability to report on it very quickly 

41
01:44:42,865 --> 01:44:47,781
the management challenges really come down to trying to figure out what data to

42
01:44:47,781 --> 01:44:48,306
store 

43
01:44:48,306 --> 01:44:51,657
a lot of times we go into various companies and you have got producers sitting

44
01:44:51,657 --> 01:44:54,810
across the hall from designers and they do not even know each other 

45
01:44:54,810 --> 01:44:57,823
they do not realize what they want and the programmer says 

46
01:44:57,823 --> 01:45:01,606
i am going to put these events in and the product manager who wants to figure out

47
01:45:01,606 --> 01:45:05,118
how many times somebody crashed says well i need these events in 

48
01:45:05,118 --> 01:45:09,436
so unless they are communicating you are going to get the wrong type of data 

49
01:45:09,436 --> 01:45:13,241
so the management challenge is trying to make sure everybody communicates 

50
01:45:13,241 --> 01:45:15,639
they decide on the taxonomy and the structure and

51
01:45:15,639 --> 01:45:19,350
then we can go forward with tagging and getting in the entire game working 

52
01:45:25,226 --> 01:45:27,469
we process stayed in two main ways 

53
01:45:27,469 --> 01:45:29,107
one is streaming data 

54
01:45:29,107 --> 01:45:31,667
one is batched or scheduled data 

55
01:45:31,667 --> 01:45:35,653
streaming data has scripts that run instantly the minute the data arrives 

56
01:45:35,653 --> 01:45:38,222
and so as the data come in it gets processed and

57
01:45:38,222 --> 01:45:40,160
then stored in a reporting format 

58
01:45:40,160 --> 01:45:46,347
so they can easily generate reports up to the second very very quickly 

59
01:45:46,347 --> 01:45:49,671
batch processing data really depends on the type of data where it coming from 

60
01:45:49,671 --> 01:45:53,689
most of the time when we download data from itunes or youtube or

61
01:45:53,689 --> 01:45:57,885
something like that it comes in a csv or a very similar format 

62
01:45:57,885 --> 01:46:00,038
there not really a lot of processing we need to do 

63
01:46:00,038 --> 01:46:02,451
it more of ingesting that data 

64
01:46:02,451 --> 01:46:06,788
there are processes we need to run with some type of batch data but

65
01:46:06,788 --> 01:46:10,040
most of the batch data we receive comes in as a csv or

66
01:46:10,040 --> 01:46:12,914
other similar already processed formats 

67
01:46:12,914 --> 01:46:17,228
so typically while you can do processing in both modes most processing typically

68
01:46:17,228 --> 01:46:21,497
happens with the streaming real - time data than it does with offline batch data 

69
01:46:27,394 --> 01:46:33,260
we actually did not use any technology in the a big data space 

70
01:46:33,260 --> 01:46:35,684
we created our own from scratch 

71
01:46:35,684 --> 01:46:39,280
what we did was we decided what kind of model we wanted 

72
01:46:39,280 --> 01:46:40,475
how we were going to store data 

73
01:46:40,475 --> 01:46:42,112
how we were going to retrieve data 

74
01:46:42,112 --> 01:46:44,574
and ultimately how we were going to reduce the data 

75
01:46:44,574 --> 01:46:47,772
because the more and more and more data you have the slower it is to actually do

76
01:46:47,772 --> 01:46:50,692
a query because you have to look through all the different pieces of data 

77
01:46:50,692 --> 01:46:54,148
a lot of databases solve this problem for you but they were really doing it in more

78
01:46:54,148 --> 01:46:57,170
generic way were we needed something very specific 

79
01:46:57,170 --> 01:47:02,020
so we started from scratch building our own data storage and retrieval and

80
01:47:02,020 --> 01:47:04,003
reporting from the ground up 

81
01:47:04,003 --> 01:47:08,097
when it came to scalability it was really about designing the parts of

82
01:47:08,097 --> 01:47:10,906
the system that could be independently scaled 

83
01:47:10,906 --> 01:47:14,773
so if data coming in in real - time we send that into what we call a gateway 

84
01:47:14,773 --> 01:47:17,155
and the gateway could be three gateways let say 

85
01:47:17,155 --> 01:47:18,922
but if the data starts getting over loaded 

86
01:47:18,922 --> 01:47:20,362
i just have to add another gateway 

87
01:47:20,362 --> 01:47:23,383
and a gateway is just this little light layer that just receives data 

88
01:47:23,383 --> 01:47:26,605
passes it on and goes back to it job does not do anything else 

89
01:47:26,605 --> 01:47:29,296
so it can receive a lot of data very quickly but

90
01:47:29,296 --> 01:47:31,124
also i can just add another one 

91
01:47:31,124 --> 01:47:34,309
and it just automatically logs in and adds itself to a list and

92
01:47:34,309 --> 01:47:37,813
now the data is being distributed amongst four gateways instead 

93
01:47:37,813 --> 01:47:39,043
query engine is the same way 

94
01:47:39,043 --> 01:47:41,720
when you are doing queries to try to get data out of the system so

95
01:47:41,720 --> 01:47:42,704
you can build reports 

96
01:47:42,704 --> 01:47:46,151
if i need more query engines because people are doing more reporting 

97
01:47:46,151 --> 01:47:47,733
we can add more query engines 

98
01:47:47,733 --> 01:47:51,722
so the idea behind scalability is trying to break the services up into

99
01:47:51,722 --> 01:47:54,390
the type of services that they most make sense 

100
01:47:54,390 --> 01:47:55,466
so that if you need to 

101
01:47:55,466 --> 01:47:58,820
you can add just the service without rebuilding the entire platform 

102
01:48:05,367 --> 01:48:08,825
my advice for people designing systems for big data is to first 

103
01:48:08,825 --> 01:48:11,398
try to understand what you want to accomplish 

104
01:48:11,398 --> 01:48:12,763
what is the goal 

105
01:48:12,763 --> 01:48:15,695
i mean we are going to ingest everything and we are going to report on everything 

106
01:48:15,695 --> 01:48:22,518
it not really something that you can achieve without some special thought 

107
01:48:22,518 --> 01:48:25,790
if you are going to focus on your area going to be say gardening 

108
01:48:25,790 --> 01:48:29,419
then look at what kind of things you are going to do in the gardening area and

109
01:48:29,419 --> 01:48:31,880
try to focus on what that type of data is going to be 

110
01:48:31,880 --> 01:48:34,722
this is not going to restrict you to only being a gardener but

111
01:48:34,722 --> 01:48:37,507
it is going to give you focus on how to design your system so

112
01:48:37,507 --> 01:48:39,701
that they are actually going to work for you 

113
01:48:39,701 --> 01:48:43,912
you are going to continually evolve your systems add more things to them and

114
01:48:43,912 --> 01:48:44,580
grow them 

115
01:48:44,580 --> 01:48:47,804
i would not suggest starting with an unlimited variety of options and

116
01:48:47,804 --> 01:48:49,944
hoping you are going to solve all the problems 

117
01:48:49,944 --> 01:48:56,185
start with the goal of what your current solution is and expand from there 

118
01:49:02,600 --> 01:49:05,971
data can be fun but it can also be overwhelming 

119
01:49:05,971 --> 01:49:11,051
so try to keep the data in mind without keeping the world in mind and

120
01:49:11,051 --> 01:49:13,291
i think you will be just fine 

1
03:31:21,860 --> 03:31:26,740
in this video we will provide a quick summary of the main points from our

2
03:31:26,740 --> 03:31:28,960
first course on introduction to big data 

3
03:31:30,310 --> 03:31:32,930
if you have just completed our first course and

4
03:31:32,930 --> 03:31:36,810
do not need a refresher you may now skip to the next lecture 

5
03:31:38,480 --> 03:31:43,000
after this video you will be able to recall

6
03:31:43,000 --> 03:31:47,480
what started the big data era and the three main big data sources 

7
03:31:49,310 --> 03:31:52,830
summarize the volume variety velocity and

8
03:31:52,830 --> 03:31:55,350
veracity issues related to each source 

9
03:31:56,790 --> 03:32:02,140
explain the five step data science process to gain value from big data 

10
03:32:03,510 --> 03:32:06,680
remember the main elements of the hadoop stack 

11
03:32:09,670 --> 03:32:14,950
we began our first course with an explanation of how a lead torrent of

12
03:32:14,950 --> 03:32:20,690
big data combined with cloud computing capabilities to process data anytime and

13
03:32:20,690 --> 03:32:25,450
anywhere has been at the core of the launch of the big data era 

14
03:32:28,470 --> 03:32:33,240
this big torrent of big data is often boil down to a few varieties of data

15
03:32:33,240 --> 03:32:38,282
generated by machines people and

16
03:32:38,282 --> 03:32:43,810
organizations with machine generated data 

17
03:32:43,810 --> 03:32:48,650
we refer to the data generated from real time sensors and industrial machinery or

18
03:32:48,650 --> 03:32:50,020
vehicles 

19
03:32:50,020 --> 03:32:53,350
web logs that track user behavior online 

20
03:32:53,350 --> 03:32:55,470
environmental sensors 

21
03:32:55,470 --> 03:32:58,680
personal health trackers among many other sense data sources 

22
03:33:00,220 --> 03:33:06,050
with human generated data we really refer to the vast amount of social media data 

23
03:33:06,050 --> 03:33:09,760
status updates tweets photos and videos 

24
03:33:11,260 --> 03:33:16,600
with organization generated data we refer to more traditional types of data

25
03:33:16,600 --> 03:33:19,770
including transaction information data bases and

26
03:33:19,770 --> 03:33:22,740
structure data often stored in data warehouses 

27
03:33:23,960 --> 03:33:30,180
note that big data can be structured semi - structured and unstructured 

28
03:33:30,180 --> 03:33:35,030
which is a topic we will talk about more and in depth later in this course 

29
03:33:38,775 --> 03:33:43,975
whatever your big data application is and the types of big data you are using 

30
03:33:43,975 --> 03:33:49,905
the real value will come from integrating different types of data sources and

31
03:33:49,905 --> 03:33:51,755
analyzing them at scale 

32
03:33:53,225 --> 03:33:56,355
overall by modeling managing and

33
03:33:56,355 --> 03:34:00,700
integrating diverse streams to improve our business and

34
03:34:00,700 --> 03:34:04,880
add value to our big data even before we start analyzing it 

35
03:34:05,910 --> 03:34:07,591
as a part of modeling and

36
03:34:07,591 --> 03:34:12,804
managing big data is focusing on the dimensions of scale availability and

37
03:34:12,804 --> 03:34:18,959
considering the challenges associated with this dimensions to pick the right tools 

38
03:34:22,138 --> 03:34:27,460
volume variety and velocity are the main dimensions which

39
03:34:27,460 --> 03:34:32,480
we characterized big data and describe its challenges 

40
03:34:33,710 --> 03:34:38,550
we have huge amounts of data in different formats and

41
03:34:38,550 --> 03:34:41,940
varying quality which must be processed quickly

42
03:34:44,750 --> 03:34:50,810
veracity refers to the biases noise and abnormality in data 

43
03:34:50,810 --> 03:34:56,190
or the unmeasurable certainty is in the truthfulness and trustworthiness of data 

44
03:34:57,270 --> 03:35:01,470
and valence refers to the connectedness of big data 

45
03:35:01,470 --> 03:35:03,790
such as in the form of graph networks 

46
03:35:06,340 --> 03:35:12,500
each v presents a challenging dimension of big data mainly of size 

47
03:35:12,500 --> 03:35:17,160
complexity speed quality and consecutiveness 

48
03:35:17,160 --> 03:35:20,910
although we can list some other v based on the context 

49
03:35:20,910 --> 03:35:25,390
we prefer to list these five as fundamental dimensions which

50
03:35:25,390 --> 03:35:28,350
this big data specialization helps you work on 

51
03:35:29,570 --> 03:35:35,130
moreover we must be sure to never forget the sixth v : value 

52
03:35:35,130 --> 03:35:38,220
at the heart of the big data challenge is turning

53
03:35:38,220 --> 03:35:41,590
all of the other dimensions into truly useful business value 

54
03:35:42,760 --> 03:35:46,490
how will big data benefit you and your organization 

55
03:35:46,490 --> 03:35:50,960
the idea behind processing all this big data in the first place

56
03:35:50,960 --> 03:35:53,100
is to bring value to the problem at hand 

57
03:35:54,360 --> 03:35:58,310
we need to take steps into big data engineering and

58
03:35:58,310 --> 03:36:02,030
scalable data science to generate value out of big data 

59
03:36:03,910 --> 03:36:04,900
we have all heard it 

60
03:36:04,900 --> 03:36:10,370
data signs turns big data into insides or even actions 

61
03:36:11,490 --> 03:36:12,940
but what does that really mean 

62
03:36:14,340 --> 03:36:19,180
data signs can be taught of as the basis for empirical research 

63
03:36:19,180 --> 03:36:22,940
like data is used to induce information on the observations 

64
03:36:24,120 --> 03:36:26,830
these observations are mainly data 

65
03:36:26,830 --> 03:36:32,280
in our case big data related to a business or scientific use case 

66
03:36:34,550 --> 03:36:40,100
inside is a term we use to refer to the data products of data science 

67
03:36:40,100 --> 03:36:43,450
it is extracted from a diverse amount of data

68
03:36:43,450 --> 03:36:47,620
through a combination of exploratory data analysis and modeling 

69
03:36:48,830 --> 03:36:53,560
the questions are sometimes less specific and it can require looking

70
03:36:53,560 --> 03:36:58,140
carefully at the data for patterns in it to come up with a specific question 

71
03:37:00,400 --> 03:37:05,470
another important point to recognize is that data science is not static

72
03:37:05,470 --> 03:37:07,450
one time analysis 

73
03:37:07,450 --> 03:37:12,990
it involves a process where models where you generate give us insights

74
03:37:12,990 --> 03:37:17,155
are constantly improve to a further and prequel evidence and iterations 

1
07:08:38,390 --> 07:08:40,650
there are many ways to look at this process 

2
07:08:41,790 --> 07:08:48,120
one way of looking at it as two distinct activities 

3
07:08:48,120 --> 07:08:54,000
mainly big data engineering and big data analytics 

4
07:08:54,000 --> 07:08:58,940
or computational big data science as i like to call it 

5
07:08:58,940 --> 07:09:01,890
since more than simple analytics are being performed 

6
07:09:03,840 --> 07:09:11,053
a more detailed way of looking at the process reveals five listing steps or

7
07:09:11,053 --> 07:09:15,125
activities of the data science process 

8
07:09:15,125 --> 07:09:20,850
namely acquire prepare analyze report and act 

9
07:09:20,850 --> 07:09:25,900
we can simply say that data science happens at the boundary of all the steps 

10
07:09:25,900 --> 07:09:29,950
ideally this process should support experimental work 

11
07:09:29,950 --> 07:09:35,930
which is constantly iterated and leads to more scientific exploration 

12
07:09:35,930 --> 07:09:41,130
as well as producing actionable results during these explorations

13
07:09:41,130 --> 07:09:45,495
using dynamic scalability on big data and cloud platforms 

14
07:09:48,095 --> 07:09:52,842
this five step process can be used in alternative ways in real life big data

15
07:09:52,842 --> 07:09:57,685
applications if we add the dependencies of different tools to each other 

16
07:09:59,325 --> 07:10:02,915
the influence of big data pushes for

17
07:10:02,915 --> 07:10:07,335
alternative scalability approaches at each step of the process 

18
07:10:08,600 --> 07:10:12,740
acquire includes anything that helps us retrieve data 

19
07:10:12,740 --> 07:10:16,740
including finding accessing acquiring and moving data 

20
07:10:18,500 --> 07:10:26,170
it includes identification of and authenticated access to all related data 

21
07:10:26,170 --> 07:10:30,490
as well as transportation of data from sources to destinations 

22
07:10:31,890 --> 07:10:37,760
it includes ways to subset and match the data to regions or

23
07:10:37,760 --> 07:10:42,350
times of interest which we sometimes refer to as geospatial querying 

24
07:10:44,130 --> 07:10:48,234
we divide the prepare data step into two sub - steps 

25
07:10:48,234 --> 07:10:51,070
based on the nature of the activity 

26
07:10:52,630 --> 07:10:58,060
the first step in data preparation involves exploring the data

27
07:10:58,060 --> 07:11:02,690
to understand its nature what it means its quality and format 

28
07:11:04,160 --> 07:11:07,550
it often takes a preliminary analysis of data or

29
07:11:07,550 --> 07:11:09,670
samples of data to understand it 

30
07:11:10,700 --> 07:11:13,770
this is why this primary step is called prepare 

31
07:11:16,030 --> 07:11:19,990
once we know more about the data through exploratory analysis 

32
07:11:19,990 --> 07:11:23,420
the next step is pre - processing of data for analysis 

33
07:11:24,540 --> 07:11:29,690
it includes cleaning data subsetting or filtering data and

34
07:11:29,690 --> 07:11:35,600
creating data which programs can read and understand by modelling raw data

35
07:11:35,600 --> 07:11:41,710
into a more defined data model or packaging it using a specific data format 

36
07:11:42,850 --> 07:11:48,270
we will learn more about data models and data formats later in this course 

37
07:11:49,940 --> 07:11:52,780
if there are multiple data sets involved 

38
07:11:52,780 --> 07:11:57,220
this step also includes integration of different data sources or

39
07:11:57,220 --> 07:12:00,745
streams which is a topic we will explore in our course three 

40
07:12:03,980 --> 07:12:09,050
the prepared data then would be passed on to the analysis step 

41
07:12:09,050 --> 07:12:12,670
which involves selection of analytical techniques to use 

42
07:12:12,670 --> 07:12:15,840
building a model of the data and analyzing results 

43
07:12:16,920 --> 07:12:20,520
this step can take a couple of iterations on its own or

44
07:12:20,520 --> 07:12:24,119
might require a data scientist to go back to steps 1 and

45
07:12:24,119 --> 07:12:27,410
2 to get more data or package data in a different way 

46
07:12:28,700 --> 07:12:30,520
so exploration never ends 

47
07:12:33,050 --> 07:12:39,413
step 4 for communicating results includes evaluation of analytical results 

48
07:12:39,413 --> 07:12:44,464
presenting them in a visual way creating reports that include

49
07:12:44,464 --> 07:12:49,250
an assessment of results with respect to success criteria 

50
07:12:49,250 --> 07:12:56,293
activities in this step can often be referred to with terms like interpret 

51
07:12:56,293 --> 07:13:00,720
summarize visualize and post - process 

52
07:13:00,720 --> 07:13:05,880
the last step brings us back to the very first reason we do data science 

53
07:13:05,880 --> 07:13:06,550
for a purpose 

54
07:13:08,090 --> 07:13:13,140
reporting insights from analysis and determining actions from insights based

55
07:13:13,140 --> 07:13:18,267
on the purpose you initially defined is what we refer to as the act step 

56
07:13:20,210 --> 07:13:25,140
we have now seen all of the steps in a typical data science process 

57
07:13:25,140 --> 07:13:29,750
please note that this is an iterative process and findings from

58
07:13:29,750 --> 07:13:34,310
one step may require previous steps to be repeated but need information 

59
07:13:34,310 --> 07:13:38,250
leading for further exploration and application of these steps 

60
07:13:39,650 --> 07:13:44,230
scalability of this process to big data analysis requires the use of

61
07:13:44,230 --> 07:13:47,099
big data platforms like hadoop 

1
14:22:24,450 --> 14:22:28,280
the hadoop ecosystem frameworks and applications

2
14:22:28,280 --> 14:22:32,870
provide such functionality through several overarching themes and goals 

3
14:22:35,000 --> 14:22:39,830
first they provide scalability to store large volumes of data

4
14:22:39,830 --> 14:22:41,150
on commodity hardware 

5
14:22:42,760 --> 14:22:45,570
as the number of systems increase so

6
14:22:45,570 --> 14:22:49,620
does the chance for crashes and hardware failures 

7
14:22:49,620 --> 14:22:54,390
they handle fault tolerance to gracefully recover from these problems 

8
14:22:56,650 --> 14:23:01,595
in addition they are designed to handle big data capacity and compressing text

9
14:23:01,595 --> 14:23:07,185
files graphs of social networks streaming sensor data and raster images 

10
14:23:07,185 --> 14:23:09,635
we can add more data types to this variety 

11
14:23:10,695 --> 14:23:12,355
for any given data type 

12
14:23:13,515 --> 14:23:17,635
you can find several projects in the ecosystem that support it 

13
14:23:19,250 --> 14:23:22,890
finally they facilitate a shared environment 

14
14:23:22,890 --> 14:23:26,050
allow multiple jobs to execute simultaneously 

15
14:23:28,350 --> 14:23:32,930
additionally the hadoop ecosystem includes a wide range of open source

16
14:23:32,930 --> 14:23:38,170
projects backed by a large and active community 

17
14:23:38,170 --> 14:23:42,480
these projects are free to use and easy to find support for 

18
14:23:44,060 --> 14:23:49,690
today there are over 100 big data open source projects 

19
14:23:49,690 --> 14:23:55,250
and this continues to grow many rely on hadoop but some are independent 

20
14:23:57,960 --> 14:24:03,230
here is one way of looking at a subset of tools in the hadoop ecosystem 

21
14:24:04,360 --> 14:24:09,240
this layer diagram is organized vertically based on the interface 

22
14:24:10,390 --> 14:24:16,720
lower level interfaces to storage and scheduling on the bottom and

23
14:24:16,720 --> 14:24:20,520
high level languages and interactivity at the top 

24
14:24:23,080 --> 14:24:29,112
the hadoop distributed file system or hdfs is the foundation for

25
14:24:29,112 --> 14:24:35,777
many big data frameworks since it provides scalable and reliable storage 

26
14:24:35,777 --> 14:24:40,915
as the size of your data increases you can add commodity

27
14:24:40,915 --> 14:24:45,314
hardware to hdfs to increase storage capacity 

28
14:24:45,314 --> 14:24:51,107
so it enables what we call scaling out of your resources 

29
14:24:53,267 --> 14:24:56,960
hadoop yarn provide flexible scheduling and

30
14:24:56,960 --> 14:25:00,660
resource management over the htfs storage 

31
14:25:01,660 --> 14:25:07,955
yarn is use at yahoo to schedule jobs across 40 000 servers 

32
14:25:09,105 --> 14:25:14,045
mapreduce is a programming model that simplifies parallel computing 

33
14:25:14,045 --> 14:25:18,725
instead of dealing with the complexities of synchronization and scheduling you only

34
14:25:18,725 --> 14:25:24,325
need to give mapreduce two functions map and reduce 

35
14:25:24,325 --> 14:25:26,643
this programming model is so

36
14:25:26,643 --> 14:25:31,962
powerful that google previously used it for indexing websites 

37
14:25:31,962 --> 14:25:36,820
mapreduce only assumes a limited model to express data 

38
14:25:36,820 --> 14:25:40,429
hive and pig are two additional programming models 

39
14:25:40,429 --> 14:25:44,421
on top of mapreduce to augment data modeling of mapreduce 

40
14:25:44,421 --> 14:25:48,744
with relational algebra and data flow modeling respectively 

41
14:25:50,869 --> 14:25:55,682
hive was created at facebook to issue sql - like queries using

42
14:25:55,682 --> 14:25:58,280
mapreduce on their data in hdfs 

43
14:25:59,410 --> 14:26:04,773
pig was created at yahoo to model dataflow based programs using mapreduce 

44
14:26:04,773 --> 14:26:09,345
thanks to yarns ability to manage resources not just for

45
14:26:09,345 --> 14:26:12,715
mapreduce but other programming models 

46
14:26:12,715 --> 14:26:16,955
giraph was built for processing large scale graphs efficiently 

47
14:26:18,335 --> 14:26:24,676
for example facebook uses giraph to analyze the social graphs of its users 

48
14:26:24,676 --> 14:26:29,181
similarly storm spark and flink were built for

49
14:26:29,181 --> 14:26:33,595
real time and in - memory processing of big data 

50
14:26:33,595 --> 14:26:38,013
on top of the yarn resource scheduler and hdfs 

51
14:26:38,013 --> 14:26:44,555
in - memory processing is a powerful way of running big data applications 

52
14:26:44,555 --> 14:26:50,598
even faster achieving 100x better performance for some tasks 

53
14:26:50,598 --> 14:26:54,845
sometimes your data processing or tasks are not easily or

54
14:26:54,845 --> 14:27:00,332
efficiently represented using the file and directory model of storage 

55
14:27:00,332 --> 14:27:06,010
examples of this include collections of key values or large sparse tables 

56
14:27:07,282 --> 14:27:12,859
nosql projects such as cassandra mongodb and

57
14:27:12,859 --> 14:27:16,681
hbase handle all these cases 

58
14:27:16,681 --> 14:27:21,396
cassandra was created at facebook and facebook also use hbase for

59
14:27:21,396 --> 14:27:23,290
its messaging platform 

60
14:27:25,934 --> 14:27:31,562
finally running all this tools requires a centralized management system for

61
14:27:31,562 --> 14:27:36,520
synchronization configuration and to ensure high availability 

62
14:27:37,660 --> 14:27:42,828
zookeeper created by yahoo to wrangle services named after animals 

63
14:27:42,828 --> 14:27:44,614
performs these duties 

64
14:27:47,214 --> 14:27:51,023
just looking at the small number of hadoop stack components 

65
14:27:51,023 --> 14:27:55,700
we can already see that most of them are dedicated to data modeling 

66
14:27:55,700 --> 14:28:00,010
management and efficient processing of the data 

67
14:28:00,010 --> 14:28:04,610
in the rest of this course we will give you fundamental knowledge and

68
14:28:04,610 --> 14:28:10,580
some practical skills on how to start modeling and managing your data and

69
14:28:10,580 --> 14:28:16,310
picking the right tools for this activity from a plethora of big data tools 

1
04:50:41,170 --> 04:50:44,780
welcome to course two of the big data specialization 

2
04:50:44,780 --> 04:50:46,390
i am amarnath gupta 

3
04:50:46,390 --> 04:50:47,270
 and i am ilkay altintas 

4
04:50:47,270 --> 04:50:50,830
we are really excited to work with you in this course 

5
04:50:50,830 --> 04:50:54,820
to develop your understanding and skills in big data modeling and management 

6
04:50:56,100 --> 04:51:00,470
you might have just finished our first course and see the potential and

7
04:51:00,470 --> 04:51:02,570
challenges of big data 

8
04:51:02,570 --> 04:51:05,630
if you have not it not required but for

9
04:51:05,630 --> 04:51:09,510
those with less background in the area you might find it valuable 

10
04:51:10,890 --> 04:51:14,070
 let explain what we mean by big data modeling and management 

11
04:51:15,400 --> 04:51:19,470
suppose you have an application where the data is big in a sense and

12
04:51:19,470 --> 04:51:25,470
it has a large volume or has high speed or comes with a lot of variations 

13
04:51:25,470 --> 04:51:29,220
even before you think of how to handle the bigness 

14
04:51:29,220 --> 04:51:31,450
you need to have a sense of what the data looks like 

15
04:51:32,550 --> 04:51:37,860
the goal of data modeling is to formally explore the nature of data so

16
04:51:37,860 --> 04:51:40,870
that you can figure out what kind of storage you need and

17
04:51:40,870 --> 04:51:42,470
what kind of processing you can do on it 

18
04:51:44,010 --> 04:51:47,010
the goal of data management is to figure out

19
04:51:47,010 --> 04:51:50,840
what kind of infrastructure support you would need for the data 

20
04:51:50,840 --> 04:51:55,130
for example does your environment need to keep multiple replicas of the data 

21
04:51:55,130 --> 04:51:58,454
do you need to do statistical computation with the data 

22
04:51:58,454 --> 04:52:03,120
once these operational requirements you will 

23
04:52:03,120 --> 04:52:07,110
be able to choose the right system that will let you perform these operations 

24
04:52:08,990 --> 04:52:13,200
 we will also introduce management of big data as it is

25
04:52:13,200 --> 04:52:18,530
streaming from data sources and talk about storage architectures for big data 

26
04:52:18,530 --> 04:52:22,790
for example how can high velocity data get ingested 

27
04:52:22,790 --> 04:52:28,360
managed stored in order to enable real time analytical capabilities 

28
04:52:28,360 --> 04:52:33,560
or what is the difference between data at rest and data in motion 

29
04:52:33,560 --> 04:52:36,230
and how can a data system enable both 

30
04:52:37,570 --> 04:52:40,680
 once you have understood the basic concepts of data modeling 

31
04:52:40,680 --> 04:52:44,300
data management and streaming data we will introduce you

32
04:52:44,300 --> 04:52:49,120
to the characteristics of large volume data and how to think about that 

33
04:52:49,120 --> 04:52:53,080
thus we will transition from classical database management systems 

34
04:52:53,080 --> 04:52:58,730
that is dbmss to big data management systems or bdmss 

35
04:52:58,730 --> 04:53:03,000
we will present brief overviews of several big data management systems

36
04:53:03,000 --> 04:53:04,430
available in the marketplace today 

37
04:53:05,630 --> 04:53:11,310
we are so excited to show you examples of archived and streaming big data sets 

38
04:53:11,310 --> 04:53:16,500
our goal is to provide you with simple hands on exercises that require

39
04:53:16,500 --> 04:53:21,290
no programming but show you what big data looks like and why various

40
04:53:21,290 --> 04:53:27,030
big data management systems are suitable for specific kinds of big data 

41
04:53:27,030 --> 04:53:30,510
in the end you will be able to design a simple

42
04:53:30,510 --> 04:53:33,110
big data information system using this knowledge 

43
04:53:34,110 --> 04:53:36,280
we wish you a fun time learning and

44
04:53:36,280 --> 04:53:40,540
hope to hear from you in the discussion forums and learner stories 

45
04:53:40,540 --> 04:53:42,529
 happy learning and think big data 

1
09:44:23,000 --> 09:44:28,820
this is the second course in the 2016 version of our big data specialization 

2
09:44:28,820 --> 09:44:32,820
 after listening to learners like you we have changed some of the content and

3
09:44:32,820 --> 09:44:34,149
ordering in the specialization 

4
09:44:35,210 --> 09:44:39,200
this course takes you on the first step of any big data project 

5
09:44:39,200 --> 09:44:41,470
its modeling and management 

6
09:44:41,470 --> 09:44:45,221
 we hope that this background on what big data looks like and

7
09:44:45,221 --> 09:44:49,662
how it is modeled and managed in a large scale system will make you feel

8
09:44:49,662 --> 09:44:54,487
more prepared to take steps towards retrieving and processing big data and

9
09:44:54,487 --> 09:44:59,338
perform big data analytics which are topics coming up in the next courses 

1
19:29:24,582 --> 19:29:27,800
the third component of a data model is constraints 

2
19:29:29,080 --> 19:29:31,418
a constraint is a logical statement 

3
19:29:31,418 --> 19:29:36,770
that means one can compute and test whether the statement is true or false 

4
19:29:37,980 --> 19:29:42,130
constraints are part of the data model because they can specify something about

5
19:29:42,130 --> 19:29:45,240
the semantics that is the meaning of the data 

6
19:29:45,240 --> 19:29:48,705
for example the constraint that a week has seven and

7
19:29:48,705 --> 19:29:53,402
only seven days is something that a data system would not know unless this

8
19:29:53,402 --> 19:29:57,025
knowledge is passed on to it in the form of a constraint 

9
19:29:57,025 --> 19:30:00,531
another constraint shown here 

10
19:30:03,132 --> 19:30:07,690
tells the system that the number of titles for a movie is restricted to one 

11
19:30:10,740 --> 19:30:11,800
different data models 

12
19:30:11,800 --> 19:30:15,260
as we will see in the next module will have different kinds of constraints 

13
19:30:17,140 --> 19:30:19,760
there may be many different kinds of constraints 

14
19:30:21,020 --> 19:30:25,039
a value constraint is a logical statement about data values 

15
19:30:26,520 --> 19:30:30,600
on a previous slide we have said that the age that is 

16
19:30:30,600 --> 19:30:34,540
the value of data elements representing the age of an entity can not be negative 

17
19:30:37,310 --> 19:30:41,840
we also saw an example of uniqueness constraint when we said

18
19:30:41,840 --> 19:30:44,880
every movie can have only one title 

19
19:30:46,200 --> 19:30:48,120
in the words of logic 

20
19:30:48,120 --> 19:30:53,460
there should exist no data object that a movie and has more than one title 

21
19:30:54,860 --> 19:30:58,719
it easy to see that enforcing these constraints requires us

22
19:30:58,719 --> 19:31:02,446
to count the number of titles and then verify that it one 

23
19:31:02,446 --> 19:31:07,490
now one can generalize this to count the number of values associated with

24
19:31:07,490 --> 19:31:12,310
each object and check whether it lies between an upper and lower bound 

25
19:31:13,380 --> 19:31:17,010
this is often called a cardinality constraint of data property 

26
19:31:20,530 --> 19:31:25,388
in a medical example here the constraint has a lower limit of 0 and

27
19:31:25,388 --> 19:31:26,931
an upper limit of 3 

28
19:31:31,390 --> 19:31:36,253
a different kind of value constraint can be enforced by restricting the type of

29
19:31:36,253 --> 19:31:38,030
the data allowed in a field 

30
19:31:39,620 --> 19:31:44,210
if we do not have such a constraint we can put any type of data in the field 

31
19:31:44,210 --> 19:31:49,300
for example you can have - 99 as the value of the last name of a person 

32
19:31:49,300 --> 19:31:51,680
of course that would be wrong 

33
19:31:51,680 --> 19:31:55,750
to ensure that this does not happen we can enforce the type of the last name

34
19:31:56,780 --> 19:32:01,300
to be a non - numeric alphabetic string 

35
19:32:01,300 --> 19:32:06,420
this example shows a logical expression for this constraint 

36
19:32:06,420 --> 19:32:11,840
a type constraint is a special kind of domain constraint 

37
19:32:11,840 --> 19:32:13,420
the domain of a data property or

38
19:32:13,420 --> 19:32:18,900
attribute is the possible set of values that are allowed for that attribute 

39
19:32:18,900 --> 19:32:22,026
for example the possible values for

40
19:32:22,026 --> 19:32:26,624
the day part of the date field can be between 1 and 31 

41
19:32:26,624 --> 19:32:30,746
while a month may have the value between 1 and 12 

42
19:32:30,746 --> 19:32:37,200
or alternately a value from the set january february ect till december 

43
19:32:38,870 --> 19:32:42,697
now one can devise a more complex constraint where the value of the date

44
19:32:42,697 --> 19:32:47,099
for april june and september and november are restricted between 1 and 30 

45
19:32:47,099 --> 19:32:51,609
and if you think about it all three constraints that we have

46
19:32:51,609 --> 19:32:56,420
described in the last slide are value constraints 

47
19:32:56,420 --> 19:33:00,380
so they only state how to restrict the values of some data property 

48
19:33:01,470 --> 19:33:06,392
in sharp contrast structural properties restrict the structure of the data 

49
19:33:06,392 --> 19:33:09,700
we will choose a more complex example for this 

50
19:33:11,320 --> 19:33:14,952
suppose we are a matrix as shown in the example and

51
19:33:14,952 --> 19:33:17,794
we have restricted to be a square matrix 

52
19:33:17,794 --> 19:33:21,919
so the number of columns is exactly equal to the number of rows 

53
19:33:24,230 --> 19:33:27,570
we have not put any restriction on the number of rows or columns 

54
19:33:27,570 --> 19:33:28,910
but just that they have to be the same 

55
19:33:30,280 --> 19:33:32,560
now this constrains the structure of the matrix and

56
19:33:32,560 --> 19:33:37,140
implies that the number of entries in the structure will be a squared number 

57
19:33:38,480 --> 19:33:43,600
if we convert this matrix to a three column table as shown and impose

58
19:33:43,600 --> 19:33:49,000
the same squareness constraint it will translate to a more complex condition 

59
19:33:49,000 --> 19:33:52,540
that the number of data rows will be the square of

60
19:33:52,540 --> 19:33:55,200
the number of unique values in column one of the table 

61
19:33:56,900 --> 19:34:00,410
we will encounter some more structural constraints in the next module 

1
15:03:24,208 --> 15:03:28,770
the second component of a data model is a set of operations that can be performed

2
15:03:28,770 --> 15:03:30,390
on the data 

3
15:03:30,390 --> 15:03:31,250
and in this module 

4
15:03:31,250 --> 15:03:36,650
we will discuss the operations without considering the bigness aspect 

5
15:03:36,650 --> 15:03:38,110
in course three 

6
15:03:38,110 --> 15:03:41,710
we will come back to the issue of performing these operations when the data is large 

7
15:03:43,515 --> 15:03:46,830
now operations specified the methods to manipulate the data 

8
15:03:47,840 --> 15:03:52,220
since different data models are typically associated with different structures 

9
15:03:52,220 --> 15:03:55,210
the operations on them will be different 

10
15:03:55,210 --> 15:04:00,410
but some types of operations are usually performed across all data models 

11
15:04:00,410 --> 15:04:02,030
we will describe a few of them here 

12
15:04:04,320 --> 15:04:11,818
one common operation extract a part of a collection based on the condition 

13
15:04:11,818 --> 15:04:15,422
in the example here we have a set of records and

14
15:04:15,422 --> 15:04:20,110
we are looking for a sub set that satisfies the condition that

15
15:04:20,110 --> 15:04:24,548
the fifth field has a value greater than 100 000 

16
15:04:24,548 --> 15:04:28,603
the only one record satisfies this requirement 

17
15:04:28,603 --> 15:04:32,580
note that we called this operation subsetting rather loosely 

18
15:04:33,940 --> 15:04:38,210
depending on the context it also called selection or filtering 

19
15:04:41,410 --> 15:04:47,300
the next common operation is retrieving a part of a structure that is specified 

20
15:04:47,300 --> 15:04:50,440
in this case we specify that we are interested

21
15:04:50,440 --> 15:04:54,130
in just the first two fields of a collection off records 

22
15:04:55,880 --> 15:05:01,340
but this produces a new collection of records which has only these fields 

23
15:05:02,350 --> 15:05:05,460
this operation like before has many names 

24
15:05:05,460 --> 15:05:07,910
the most dominant name is projection 

25
15:05:09,340 --> 15:05:13,204
in the next module we will see several versions of this operation for

26
15:05:13,204 --> 15:05:14,588
different data models 

27
15:05:17,108 --> 15:05:21,176
the next two operations are about combining two collections

28
15:05:21,176 --> 15:05:22,460
into a larger one 

29
15:05:23,540 --> 15:05:27,450
the term combine may be interpreted in various ways 

30
15:05:27,450 --> 15:05:29,830
the most straightforward of them is said union 

31
15:05:30,910 --> 15:05:33,500
the assumption behind the union operation

32
15:05:33,500 --> 15:05:36,930
is that the two collections involved have the same structure 

33
15:05:36,930 --> 15:05:43,050
in other words if one collection has four fields and another has 14 fields or

34
15:05:43,050 --> 15:05:48,230
if one has four fields on people and the dates of birth and the other has four

35
15:05:48,230 --> 15:05:52,350
things about countries and their capitols they cannot be combined through union 

36
15:05:54,650 --> 15:05:58,340
in the example here their two collections have three and

37
15:05:58,340 --> 15:06:03,620
two records respectively with one record that common between them 

38
15:06:03,620 --> 15:06:04,120
the green one 

39
15:06:06,100 --> 15:06:09,170
the result collection has four record 

40
15:06:09,170 --> 15:06:14,300
because duplicates are disallowed because it a set operation 

41
15:06:14,300 --> 15:06:19,110
there is indeed another version of union where duplicates are allowed and

42
15:06:19,110 --> 15:06:21,540
will produce five records instead of four 

43
15:06:24,040 --> 15:06:29,020
the second kind of combining called a join can be done when

44
15:06:29,020 --> 15:06:33,460
the two collections have different data content but have some common elements 

45
15:06:35,430 --> 15:06:37,290
in the example shown 

46
15:06:37,290 --> 15:06:40,920
the first field is the common element between the two collections on the left 

47
15:06:42,040 --> 15:06:45,630
in this kind of data combination there are two stages 

48
15:06:45,630 --> 15:06:51,380
first for each data item think of a record of collection one 

49
15:06:51,380 --> 15:06:54,490
one finds a set of matching data items in collection two 

50
15:06:56,120 --> 15:06:59,960
thus the first records of the two collections match

51
15:06:59,960 --> 15:07:01,060
based on the first field 

52
15:07:02,490 --> 15:07:04,750
in the second phase of the operation 

53
15:07:04,750 --> 15:07:07,710
all fields of the matching record pairs are put together 

54
15:07:08,780 --> 15:07:12,170
in the first record of the result collection shown on the right 

55
15:07:12,170 --> 15:07:15,890
one gets the first four fields on the first collection and

56
15:07:15,890 --> 15:07:18,230
the remaining two fields from the second collection 

57
15:07:19,500 --> 15:07:25,380
now in this one example we found one pair of matching records from the collections 

58
15:07:25,380 --> 15:07:28,790
in general one would find more than one matching record pairs 

59
15:07:30,260 --> 15:07:33,415
as you can see this operation is more complex and

60
15:07:33,415 --> 15:07:37,863
can be very expensive when the size of the true collections are large 

1
06:10:58,800 --> 06:11:04,950
in the big data world we often hear the term structured data that is data having

2
06:11:04,950 --> 06:11:10,490
a structure which is quite different from the so - called unstructured data 

3
06:11:10,490 --> 06:11:11,500
but what is a structure 

4
06:11:12,910 --> 06:11:14,040
let consider file 1 

5
06:11:15,310 --> 06:11:19,960
it a typical csv file that has three lines with different content 

6
06:11:19,960 --> 06:11:24,260
but the file content is uniform in the sense that each line 

7
06:11:24,260 --> 06:11:28,400
call it a record has exactly three fields 

8
06:11:28,400 --> 06:11:31,360
which we sometimes call data properties or attributes 

9
06:11:32,720 --> 06:11:39,420
further the first two of these fields are strings and the third one is a date 

10
06:11:39,420 --> 06:11:42,770
we can add more records that lines with the same pattern of data 

11
06:11:42,770 --> 06:11:44,210
to the file in the same fashion 

12
06:11:45,240 --> 06:11:50,430
the content will grow but the pattern of data organization will remain identical 

13
06:11:51,440 --> 06:11:56,600
this repeatable pattern of data organization makes the file structured 

14
06:11:56,600 --> 06:12:00,870
now let look at file 2 which is four records of five fields each 

15
06:12:01,910 --> 06:12:05,680
except that the third record seems to be missing the last entry 

16
06:12:06,710 --> 06:12:07,660
is this file structured 

17
06:12:08,810 --> 06:12:13,383
we argue that it is because the missing value makes the third record incomplete 

18
06:12:13,383 --> 06:12:18,310
but it does not break the structure or the pattern of the data organization 

19
06:12:18,310 --> 06:12:21,088
let looks at these two files side by side 

20
06:12:21,088 --> 06:12:27,320
clearly file 2 has more fields and hence is sort of wider than the first file 

21
06:12:27,320 --> 06:12:29,980
would you say that they have the same structure 

22
06:12:29,980 --> 06:12:31,990
well on the face of it they do not 

23
06:12:31,990 --> 06:12:33,740
but if you think more broadly 

24
06:12:33,740 --> 06:12:37,260
you would notice that they are both collection of k fields 

25
06:12:38,330 --> 06:12:42,770
the size of the collection respectively three and four differs 

26
06:12:42,770 --> 06:12:46,320
and k is 3 in the first case and 5 in the second 

27
06:12:46,320 --> 06:12:50,330
but we can think of 3 and 5 as parameters 

28
06:12:50,330 --> 06:12:55,280
in that case we will say that these files have been generated by a similar

29
06:12:55,280 --> 06:13:00,160
organizational structure and hence they have the same data model 

30
06:13:01,440 --> 06:13:04,190
now in contrast consider this file 

31
06:13:04,190 --> 06:13:08,480
just looking at it it impossible to figure out how the data is organized and

32
06:13:08,480 --> 06:13:10,420
how to identify subparts of the data 

33
06:13:11,440 --> 06:13:13,850
we would call this data unstructured 

34
06:13:14,860 --> 06:13:19,605
often compressed data like jpeg images mp3 audio files 

35
06:13:19,605 --> 06:13:24,450
mpeg3 video files encrypted data are usually unstructured 

36
06:13:25,490 --> 06:13:30,098
in module two we will elaborate on data models that are not fully structured or

37
06:13:30,098 --> 06:13:31,900
are structured differently 

1
12:24:29,890 --> 12:24:32,790
this is the first of two hands - on exercises involving

2
12:24:32,790 --> 12:24:35,170
sensor data from a weather station 

3
12:24:35,170 --> 12:24:38,530
in this one we will look at static data in a text file 

4
12:24:38,530 --> 12:24:41,900
the next one we will look at live data streaming from the weather station

5
12:24:41,900 --> 12:24:42,490
in real time 

6
12:24:42,490 --> 12:24:46,820
in this exercise we will begin by opening a terminal window and

7
12:24:46,820 --> 12:24:50,330
changing into the directory containing the station measurements 

8
12:24:50,330 --> 12:24:53,560
we will look at these measurements in a text file and then look at the key for

9
12:24:53,560 --> 12:24:56,540
these measurements so we can understand what the values mean 

10
12:24:56,540 --> 12:24:58,310
finally we will plot the measurements 

11
12:24:59,340 --> 12:24:59,840
let begin 

12
12:25:00,870 --> 12:25:04,023
first we will open a terminal window by clicking on the terminal icon on the top

13
12:25:04,023 --> 12:25:04,692
of the toolbar 

14
12:25:07,179 --> 12:25:13,814
next we will cd into the directory

15
12:25:13,814 --> 12:25:19,350
containing the sensor data 

16
12:25:20,380 --> 12:25:27,358
we will run cd downloads big - data - two sensor 

17
12:25:29,680 --> 12:25:36,750
we can write ls to see the contents of this directory 

18
12:25:41,274 --> 12:25:47,554
the data from the weather station is in a text file called wx - data txt 

19
12:25:50,370 --> 12:25:55,550
we can run more wx - data txt to see the contents of this file 

20
12:26:03,970 --> 12:26:06,990
each line of this file is a separate set of measurements 

21
12:26:06,990 --> 12:26:11,390
there are two columns in this file the first column is a time stamp and

22
12:26:11,390 --> 12:26:14,160
it separated by a second column by a tab 

23
12:26:14,160 --> 12:26:20,710
the second column itself has separate columns and these are separated by commas 

24
12:26:20,710 --> 12:26:25,790
the time stamp is the number of seconds since 1970 

25
12:26:25,790 --> 12:26:28,450
you will notice that it increases for each time stamp 

26
12:26:30,830 --> 12:26:33,520
you will notice that it increases for each measurement 

27
12:26:33,520 --> 12:26:35,550
but sometimes measurements come in at the same time 

28
12:26:35,550 --> 12:26:39,028
for example this one at 006 

29
12:26:39,028 --> 12:26:44,053
the measurements we see that the prefix is 0r1 for

30
12:26:44,053 --> 12:26:47,188
most of them but some have 0r2 

31
12:26:47,188 --> 12:26:50,060
if we look at the other measurements 

32
12:26:50,060 --> 12:26:55,646
we see that all the 0r1 measurements start with dn dm dx and so on 

33
12:26:55,646 --> 12:27:00,670
whereas r2 begins with ta ua and pa 

34
12:27:03,310 --> 12:27:06,419
if we scroll down in the text file by hitting the space bar 

35
12:27:06,419 --> 12:27:09,728
we will see there are other measurements besides r1 and r2 

36
12:27:12,860 --> 12:27:18,360
for example there r5 that has th vh vs and so on 

37
12:27:18,360 --> 12:27:22,830
and there r0 which has all the measurements 

38
12:27:22,830 --> 12:27:26,510
so dn dm dx ta ua pa 

39
12:27:29,550 --> 12:27:30,930
and the remaining ones 

40
12:27:33,160 --> 12:27:36,434
next we will open another internal window and look at the key to this

41
12:27:36,434 --> 12:27:39,598
measurements click on the tool bar to open the terminal window 

42
12:27:39,598 --> 12:27:44,900
cd into downloads big data two sensor

43
12:27:52,120 --> 12:28:00,131
and the key to these measurements is in a file called wxt520format txt 

44
12:28:00,131 --> 12:28:05,290
we can run more wxt520format txt to see this file 

45
12:28:13,190 --> 12:28:16,116
this file says where each of the prefix is mean for

46
12:28:16,116 --> 12:28:18,307
example sn is the wind speed minimum 

47
12:28:18,307 --> 12:28:25,430
sm is the wind speed average 

48
12:28:25,430 --> 12:28:29,700
and ta is the air temperature 

49
12:28:29,700 --> 12:28:33,530
so if we go back to our sensor file we see here ta equals 13 9c 

50
12:28:33,530 --> 12:28:39,187
that means the air temperature at this

51
12:28:39,187 --> 12:28:44,690
time was 13 9 degrees celsius 

52
12:28:44,690 --> 12:28:50,022
we can also create a plot of the data in this text file

53
12:28:50,022 --> 12:28:54,994
by running plot - data py wx - data txt ta 

54
12:29:01,727 --> 12:29:06,529
this says to plot the data in the wx - data file and the measure that we

55
12:29:06,529 --> 12:29:11,680
want to apply is ta which according to our key is the air temperature 

56
12:29:15,880 --> 12:29:20,520
when we run it it displays a plot of the air temperature found in the text file 

1
00:53:49,568 --> 00:53:51,240
in this hands - on exercise 

2
00:53:51,240 --> 00:53:54,870
we will be looking at an image file which uses an array data model 

3
00:53:56,050 --> 00:54:00,380
first we will open a terminal window and display an image file on the screen 

4
00:54:01,890 --> 00:54:05,630
next we will examine the structure of the image and finally 

5
00:54:05,630 --> 00:54:08,860
extract pixel values from various locations in the image 

6
00:54:10,550 --> 00:54:11,660
let begin 

7
00:54:11,660 --> 00:54:14,954
first we will open a terminal window by clicking on the terminal icon at the top

8
00:54:14,954 --> 00:54:15,650
of the toolbar 

9
00:54:22,101 --> 00:54:26,989
next we will cdn2 the directory containing the image 

10
00:54:26,989 --> 00:54:30,390
cdn2downloads bigdata2 image 

11
00:54:37,410 --> 00:54:40,470
we can run ls to see the image in different scripts 

12
00:54:43,050 --> 00:54:47,344
the file australia jpg is an image that we want to view 

13
00:54:47,344 --> 00:54:49,090
we can use eog to view it 

14
00:54:49,090 --> 00:54:57,830
run eog australia jpg eog is a common image viewer on linux 

15
00:55:00,192 --> 00:55:04,870
australia jpg is a satellite image of the australian continent 

16
00:55:04,870 --> 00:55:06,560
now let look at the structure of this image file 

17
00:55:10,320 --> 00:55:14,815
if we go back to our terminal window we can run the image

18
00:55:14,815 --> 00:55:19,611
viewer in the background by hitting ctrl + z and then bg 

19
00:55:19,611 --> 00:55:24,319
we can view the dimensions or structure of the array data model of this

20
00:55:24,319 --> 00:55:27,904
image by running dimensions py australia jpg 

21
00:55:32,740 --> 00:55:39,860
this says that the image has 5250 columns and 4320 rows 

22
00:55:39,860 --> 00:55:42,240
so it is a two - dimensional image 

23
00:55:42,240 --> 00:55:44,671
additionally each cell or

24
00:55:44,671 --> 00:55:50,470
pixel within this image is composed of three 8 - bit pixels 

25
00:55:50,470 --> 00:55:55,130
these pixels are composed of three elements red green and blue 

26
00:55:55,130 --> 00:55:58,410
we can extract or view the individual pixel elements

27
00:55:58,410 --> 00:56:02,110
at a specific location in the image by using the pixel py script 

28
00:56:03,440 --> 00:56:10,240
we can run pixel py australia jpg 0 0 to see the value at one location 

29
00:56:18,430 --> 00:56:22,430
the 0 0 location is the corner of the image 

30
00:56:22,430 --> 00:56:28,281
if we go back to the image the corners are all the ocean so they are dark blue 

31
00:56:28,281 --> 00:56:32,995
if we look at the value that was extracted we see that blue has a high

32
00:56:32,995 --> 00:56:37,490
value of 50 whereas red and green are low with 11 and 10 

33
00:56:37,490 --> 00:56:40,991
if we view it at another corner by looking at 5000 0 

34
00:56:40,991 --> 00:56:42,610
we will see the same value 

35
00:56:50,136 --> 00:56:54,679
if we go back to the image the middle of the image which is the land 

36
00:56:54,679 --> 00:56:56,070
is orange or yellow 

37
00:56:56,070 --> 00:57:01,261
it definitely not blue so let take a look at a pixel value there 

38
00:57:01,261 --> 00:57:06,010
okay run pixel py australia jpg 2000 2000 

39
00:57:12,750 --> 00:57:20,070
this says that the red has a value of 118 green is 89 and the blue is 57 

40
00:57:20,070 --> 00:57:23,130
so the red and green are higher than blue so it not ocean 

1
01:51:14,370 --> 01:51:17,969
this is the second hands on exercises for csv data 

2
01:51:17,969 --> 01:51:22,155
in the first we saw how to import a csv file into a spreadsheet and

3
01:51:22,155 --> 01:51:24,430
make a simple plot 

4
01:51:24,430 --> 01:51:27,240
in this one we will learn how to filter data and

5
01:51:27,240 --> 01:51:28,830
perform some aggregate operations 

6
01:51:30,390 --> 01:51:33,659
we will begin by opening a terminal window and starting a spreadsheet 

7
01:51:34,800 --> 01:51:38,440
next we will load the csv data into the spreadsheet and

8
01:51:38,440 --> 01:51:41,490
perform a filter over several columns 

9
01:51:41,490 --> 01:51:44,398
finally we will calculate an average and sum from the data 

10
01:51:44,398 --> 01:51:47,313
let begin 

11
01:51:47,313 --> 01:51:51,470
first we open a terminal window by clicking on the terminal icon in

12
01:51:51,470 --> 01:51:52,290
the top toolbar 

13
01:51:54,770 --> 01:51:58,296
we can start the spreadsheet by writing oocalc 

14
01:52:03,338 --> 01:52:06,590
next let load our csv data into the spreadsheet 

15
01:52:06,590 --> 01:52:14,142
we click on file open census csv 

16
01:52:17,719 --> 01:52:20,426
and click ok on this dialog to load the data 

17
01:52:24,387 --> 01:52:29,344
column f in the spreadsheet is the state and

18
01:52:29,344 --> 01:52:34,704
column h is the census population for 2010 

19
01:52:37,110 --> 01:52:40,480
let create a filter that just shows the data for

20
01:52:40,480 --> 01:52:44,920
california for counties larger than one million people 

21
01:52:46,100 --> 01:52:51,030
we can create this filter by first selecting both the state name column and

22
01:52:51,030 --> 01:52:53,210
the census 2000 population column 

23
01:52:54,790 --> 01:53:00,054
next we go to data filter standard filter 

24
01:53:04,184 --> 01:53:08,025
here we change the field name to be the state name 

25
01:53:08,025 --> 01:53:13,061
the condition we leave at equals and the value we use california 

26
01:53:18,318 --> 01:53:23,055
this filters all the rows unless the state is california 

27
01:53:27,143 --> 01:53:28,498
we then want to filter for

28
01:53:28,498 --> 01:53:32,590
all the counties whose population is greater than one million people 

29
01:53:32,590 --> 01:53:36,400
to do that in this second line here we change the operator to and 

30
01:53:38,180 --> 01:53:43,710
the field name should be census 2010 population and

31
01:53:43,710 --> 01:53:47,040
the condition should be greater than 

32
01:53:47,040 --> 01:53:48,431
then set the value to be one million 

33
01:53:53,923 --> 01:53:56,893
and click ok 

34
01:53:56,893 --> 01:54:01,603
we can see that all the data from the spreadsheet has disappeared except where

35
01:54:01,603 --> 01:54:03,559
the state name is california and

36
01:54:03,559 --> 01:54:06,762
the population is greater than one million people 

37
01:54:08,310 --> 01:54:12,110
we can reset or remove this filter to see all the data again

38
01:54:12,110 --> 01:54:16,240
by going to data filter 

39
01:54:18,260 --> 01:54:19,020
reset filter 

40
01:54:20,390 --> 01:54:23,550
you can perform aggregate operations on the data in a spreadsheet 

41
01:54:25,160 --> 01:54:28,830
next let perform some aggregate operations over the data 

42
01:54:28,830 --> 01:54:31,220
we can compute the average and the sum of the data 

43
01:54:32,460 --> 01:54:35,980
to do this let run these calculations in a separate sheet 

44
01:54:37,470 --> 01:54:40,181
create a new sheet by clicking on the green plus button 

45
01:54:43,854 --> 01:54:49,656
to compute the average we select a cell and enter = average and

46
01:54:49,656 --> 01:54:55,470
then we select the data that we want to compute the average from 

47
01:54:56,870 --> 01:54:58,150
if we go back to sheet one 

48
01:54:58,150 --> 01:55:02,190
we can select some of the data from the h column 

49
01:55:02,190 --> 01:55:06,634
so let just choose several counties in alabama 

50
01:55:06,634 --> 01:55:10,267
when we hit enter it takes us back to sheet one and

51
01:55:10,267 --> 01:55:13,306
we can see that the average is computing 

52
01:55:15,513 --> 01:55:24,220
similarly we can compute the sum by entering = sum open parentheses 

53
01:55:24,220 --> 01:55:28,120
going back to sheet one and selecting the columns we want to sum 

54
01:55:28,120 --> 01:55:33,360
when we are done hit enter and the sum is computed 

1
03:46:47,360 --> 03:46:50,130
in this hands on exercise we will be looking at json data 

2
03:46:52,040 --> 03:46:54,600
first we will open a terminal window and

3
03:46:54,600 --> 03:46:58,300
then look at the contents of a json file containing tweets from twitter 

4
03:46:59,860 --> 03:47:02,800
next we will examine the schema of this json file 

5
03:47:03,810 --> 03:47:06,920
finally we will extract different fields from the json data 

6
03:47:09,340 --> 03:47:10,470
let begin 

7
03:47:10,470 --> 03:47:12,307
first we will open a terminal window 

8
03:47:12,307 --> 03:47:14,964
by clicking on the terminal icon at the top of the toolbar 

9
03:47:22,240 --> 03:47:27,005
next we will see cd into the directory containing the json data 

10
03:47:27,005 --> 03:47:30,594
by running cddownload big data 2 json 

11
03:47:37,298 --> 03:47:39,520
we can run ls to see the json file 

12
03:47:43,836 --> 03:47:49,030
the json file is called twitter json 

13
03:47:49,030 --> 03:47:52,980
we can run more twitter json to view the contents of this file 

14
03:48:01,280 --> 03:48:05,990
the json data contains semi - structured data which is nested several levels 

15
03:48:05,990 --> 03:48:10,350
there are many tweets in this file and it hard to read using the more command 

16
03:48:10,350 --> 03:48:14,690
you can use space to scroll down and when we are done hit q 

17
03:48:17,420 --> 03:48:22,358
we can run the jsonschema pi command to view the schema of this data 

18
03:48:22,358 --> 03:48:27,179
we run jsonschema pitwitter json

19
03:48:34,715 --> 03:48:37,670
and we will add a pipe more at the end 

20
03:48:41,665 --> 03:48:45,572
this shows the nested fields within this data 

21
03:48:45,572 --> 03:48:51,139
at the top level there are fields like contributors text and id and so

22
03:48:51,139 --> 03:48:56,900
on but there are also fields nested within these top level fields for

23
03:48:56,900 --> 03:49:02,372
example entities also contains called symbols media hashtags and

24
03:49:02,372 --> 03:49:05,691
so on if we scroll down by hitting space 

25
03:49:05,691 --> 03:49:10,270
we will see that there several levels of nesting 

26
03:49:10,270 --> 03:49:17,320
for example user also has follow request sent id and so on 

27
03:49:21,105 --> 03:49:25,466
we can run the print json script to view the contents of a particular tweet and

28
03:49:25,466 --> 03:49:27,746
a particular field within that tweet 

29
03:49:27,746 --> 03:49:32,018
let run print json py 

30
03:49:32,018 --> 03:49:35,812
it asks for the file name so

31
03:49:35,812 --> 03:49:40,300
we will enter twitter json 

32
03:49:43,407 --> 03:49:46,434
and we will look at tweet 99 

33
03:49:48,645 --> 03:49:51,340
so let look at the top level field called text 

34
03:49:55,506 --> 03:49:59,884
so here we see the text for note the 99th tweet in this file 

35
03:49:59,884 --> 03:50:04,110
we could also look at a nested field within the file by running print json

36
03:50:04,110 --> 03:50:09,276
again the file name is twitter json 

37
03:50:11,975 --> 03:50:13,833
we will look at tweet 99 again 

38
03:50:17,097 --> 03:50:20,240
and we will look at the field entities hashtags 

39
03:50:20,240 --> 03:50:24,565
the hashtags that are embedded or nested within the entities field 

1
07:37:11,390 --> 07:37:17,427
this is the first of two hands on exercises involving csv data 

2
07:37:17,427 --> 07:37:21,568
in this exercise we will import a csv file into a spreadsheet and

3
07:37:21,568 --> 07:37:22,860
make a simple plot 

4
07:37:24,290 --> 07:37:29,350
we will begin by opening a terminal window and looking at a csv file in the terminal 

5
07:37:29,350 --> 07:37:32,330
next we will start the spreadsheet application and

6
07:37:32,330 --> 07:37:34,410
import the csv data into the spreadsheet 

7
07:37:34,410 --> 07:37:39,760
we can then look at the rows and columns of the csv file and make a simple plot 

8
07:37:42,088 --> 07:37:44,837
let begin first open a terminal shell by

9
07:37:44,837 --> 07:37:48,423
clicking on the black terminal icon at the top of the toolbar 

10
07:37:56,759 --> 07:38:01,914
next let cd into the directory containing the csv data 

11
07:38:01,914 --> 07:38:07,587
we will run cd space download big - data - 2 csv 

12
07:38:07,587 --> 07:38:13,042
we can run ls to

13
07:38:13,042 --> 07:38:20,325
see the csv files 

14
07:38:20,325 --> 07:38:25,190
the file census csv contains census data for the united states 

15
07:38:25,190 --> 07:38:30,367
we can run the command more census csv to see the contents of this file 

16
07:38:38,262 --> 07:38:41,925
the first line of this file is the header with the columns separated by commas 

17
07:38:41,925 --> 07:38:46,705
you can go down in the file by hitting space 

18
07:38:50,295 --> 07:38:51,735
hit q to quit more 

19
07:38:54,570 --> 07:38:57,210
next let start a spreadsheet application 

20
07:38:57,210 --> 07:39:01,446
we run oocalc to start this 

21
07:39:09,318 --> 07:39:13,453
we can import the census data csv file into

22
07:39:13,453 --> 07:39:17,598
the spreadsheet by going to file open 

23
07:39:21,568 --> 07:39:25,067
clicking on downloads 

24
07:39:25,067 --> 07:39:31,468
big data 2 csv census csv 

25
07:39:36,531 --> 07:39:38,109
in this dialog click ok 

26
07:39:53,945 --> 07:39:56,355
you can see into this spreadsheet 

27
07:39:56,355 --> 07:39:59,900
import of the csv data to a bunch of rows and columns 

28
07:40:01,880 --> 07:40:05,460
each column that was separated by a comma in the csv file 

29
07:40:05,460 --> 07:40:07,580
is a column in the spreadsheet 

30
07:40:08,700 --> 07:40:11,590
we can see that our csv file was successfully imported into

31
07:40:11,590 --> 07:40:12,250
the spreadsheet 

32
07:40:14,650 --> 07:40:16,892
if we scroll down to the bottom of the spreadsheet 

33
07:40:16,892 --> 07:40:19,094
we can see how many rows there were in the csv file 

34
07:40:28,939 --> 07:40:34,372
there are 3194 rows in the csv file 

35
07:40:34,372 --> 07:40:38,149
if this file instead had millions or 10 millions of rows 

36
07:40:38,149 --> 07:40:42,237
then we would have to use a big data system such as hadoop or hdfs 

37
07:40:45,882 --> 07:40:47,089
let scroll back to the top 

38
07:40:52,592 --> 07:40:58,351
next let make a simple plot of some of the data in the csv file 

39
07:40:58,351 --> 07:41:03,993
let plot the population estimates for several years for the state of alabama 

40
07:41:03,993 --> 07:41:09,558
the state of alabama is given in the second row and

41
07:41:09,558 --> 07:41:15,813
the population estimates are given in these columns 

42
07:41:15,813 --> 07:41:21,225
let select j through o so you get the population

43
07:41:21,225 --> 07:41:25,657
estimate for 2010 through 2015 

44
07:41:29,086 --> 07:41:33,331
we can create a plot of these values by clicking on the chart button 

45
07:41:38,393 --> 07:41:41,806
and clicking finish 

46
07:41:41,806 --> 07:41:45,442
in the second hands on for csv data we will perform some filtering and

47
07:41:45,442 --> 07:41:47,669
some aggregate operations over the data 

1
15:18:58,170 --> 15:18:58,670
welcome 

2
15:18:59,800 --> 15:19:02,200
in this module we will talk about data models 

3
15:19:03,530 --> 15:19:06,770
if you completed the introductory course of this specialization 

4
15:19:06,770 --> 15:19:08,940
you might recall our video on data variety 

5
15:19:10,220 --> 15:19:12,840
one way to characterize data variety

6
15:19:12,840 --> 15:19:17,290
is to identify the different models of data that are used in any application 

7
15:19:19,120 --> 15:19:20,480
so what is a data model 

8
15:19:20,480 --> 15:19:23,980
and why do we care about data models in the context of big data 

9
15:19:23,980 --> 15:19:28,010
in this lesson we will introduce you to three components of a data model and

10
15:19:28,010 --> 15:19:29,380
what they tell us about the data 

11
15:19:30,550 --> 15:19:35,620
so after this lesson you will be able to distinguish between structured and

12
15:19:35,620 --> 15:19:37,270
unstructured data 

13
15:19:37,270 --> 15:19:42,880
describe four basic data operations namely selection projection union and join 

14
15:19:42,880 --> 15:19:47,120
and enumerate different types of data constraints like type value and

15
15:19:47,120 --> 15:19:49,240
structural constraints 

16
15:19:49,240 --> 15:19:53,760
you will also be able to explain why constraints are useful to specify

17
15:19:53,760 --> 15:19:56,320
the semantics of data 

18
15:19:56,320 --> 15:20:00,980
now regardless of whether the data is big or small one needs to know or determine

19
15:20:00,980 --> 15:20:06,100
the characteristics of data before one can manipulate or analyze them meaningfully 

20
15:20:06,100 --> 15:20:10,320
let use a simple example suppose you have data is

21
15:20:10,320 --> 15:20:15,330
a file of records with fields called first name last name and date of birth of

22
15:20:15,330 --> 15:20:20,840
the employees in the company that this file consists of records with fields 

23
15:20:20,840 --> 15:20:25,290
and not for instance plain text gives us more insight

24
15:20:25,290 --> 15:20:30,770
into the organization of the data in the file and hence is part of the data model 

25
15:20:30,770 --> 15:20:33,810
this aspect is called structure 

26
15:20:33,810 --> 15:20:37,930
similarly the consideration that we can perform

27
15:20:37,930 --> 15:20:40,980
data arithmetic with the date of birth field and

28
15:20:40,980 --> 15:20:46,340
not with the first name field is also part of our understanding of data model 

29
15:20:46,340 --> 15:20:48,620
these are called operations 

30
15:20:48,620 --> 15:20:53,300
finally we may know that in this company no one age

31
15:20:53,300 --> 15:20:56,950
that is today date minus the date of birth cannot be less than 18 

32
15:20:56,950 --> 15:21:02,000
so it gives us a way to detect records with blatantly erroneous dates of birth 

33
15:21:02,000 --> 15:21:03,940
in the following three videos 

34
15:21:03,940 --> 15:21:07,790
we will look at these three aspects of data models more carefully 

1
06:40:04,120 --> 06:40:08,140
we mentioned before that a data model is characterized by the structure of

2
06:40:08,140 --> 06:40:12,430
the data that it admits the operations on that structure and

3
06:40:12,430 --> 06:40:14,090
a way to specify constraints 

4
06:40:15,290 --> 06:40:16,750
in this lesson 

5
06:40:16,750 --> 06:40:21,950
we will present a more detailed description of a number of common data models 

6
06:40:21,950 --> 06:40:23,800
we will start with relational data 

7
06:40:25,460 --> 06:40:29,780
it is one of the simplest and most frequently used data models today and

8
06:40:29,780 --> 06:40:33,320
forms the basis of many other traditional database management systems 

9
06:40:33,320 --> 06:40:36,930
like mysql oracle teradata and so forth 

10
06:40:38,480 --> 06:40:41,940
so after this video you will be able to

11
06:40:41,940 --> 06:40:44,680
describe the structural components of a relational data model 

12
06:40:46,320 --> 06:40:49,430
demonstrate which components become a data model schema 

13
06:40:51,760 --> 06:40:54,760
explain the purpose of primary and foreign keys 

14
06:40:55,770 --> 06:40:58,370
and describe join and other operations 

15
06:41:01,070 --> 06:41:02,610
the primary data structure for

16
06:41:02,610 --> 06:41:06,380
a relational model is a table like the one shown here for a toy application 

17
06:41:08,030 --> 06:41:12,740
but we need to be careful about relational tables which are also called relations 

18
06:41:14,110 --> 06:41:18,695
this table actually represents a set of tuples 

19
06:41:20,550 --> 06:41:25,390
this is a relational tuple represented as a row in the table 

20
06:41:26,500 --> 06:41:29,140
we were informally calling this a record before 

21
06:41:30,350 --> 06:41:36,110
but a relational tuple implies that unless otherwise stated the elements of it

22
06:41:36,110 --> 06:41:40,600
like 203 or 204 mary and so forth are atomic 

23
06:41:41,950 --> 06:41:46,300
that is they represent one unit of information and

24
06:41:46,300 --> 06:41:47,740
cannot be decomposed further 

25
06:41:48,830 --> 06:41:51,150
we will return to this issue in the next few slides 

26
06:41:52,490 --> 06:41:56,935
thus this is a relation of six tuples 

27
06:41:58,468 --> 06:42:01,490
remember the definition of sets 

28
06:42:01,490 --> 06:42:06,270
it a collection of distinct elements of the same type 

29
06:42:07,420 --> 06:42:13,920
that means i cannot add this tuple to the solution 

30
06:42:13,920 --> 06:42:17,350
because if i do it will be introducing a duplicate 

31
06:42:18,820 --> 06:42:24,330
now in practice many systems will allow duplicate tuples in a relation but

32
06:42:24,330 --> 06:42:29,450
mechanisms are provided to prevent duplicate entries if the user so chooses 

33
06:42:29,450 --> 06:42:31,170
so i cannot add it 

34
06:42:31,170 --> 06:42:34,170
here is another tuple i cannot add 

35
06:42:34,170 --> 06:42:36,680
it has all the right pieces of information but

36
06:42:36,680 --> 06:42:38,410
it all in the wrong order 

37
06:42:38,410 --> 06:42:42,300
so it is a tuple dissimilar with the other six tuples in the relation 

38
06:42:43,300 --> 06:42:46,690
okay so how does the system know that this tuple is different 

39
06:42:48,390 --> 06:42:52,700
this brings our attention to the very first row that is the header of this table

40
06:42:52,700 --> 06:42:53,710
painted in black 

41
06:42:55,680 --> 06:42:58,910
this row is part of the scheme of the table 

42
06:42:58,910 --> 06:42:59,420
lets look at it 

43
06:43:00,890 --> 06:43:04,190
it tells us the name of the table in this case employee 

44
06:43:05,580 --> 06:43:09,560
this also tells us the names of the six columns called attributes of the relation 

45
06:43:10,740 --> 06:43:12,090
and for each column 

46
06:43:12,090 --> 06:43:16,370
it tells us the allowed data type that is the type constraint for each column 

47
06:43:17,610 --> 06:43:19,340
given this schema 

48
06:43:19,340 --> 06:43:23,890
it should now be clear why the last red row does not belong to this table 

49
06:43:24,940 --> 06:43:29,180
the schema in a relational table can also specify constraints 

50
06:43:30,330 --> 06:43:33,150
shown in yellow in the third line of the schema row 

51
06:43:34,720 --> 06:43:39,050
it says that the minimum salary of a person has to be greater than 25000 

52
06:43:41,150 --> 06:43:47,670
further it states that every employee must have a first and last name 

53
06:43:47,670 --> 06:43:49,660
they cannot be left null that means without a value 

54
06:43:51,840 --> 06:43:54,900
why does not department or title column have this constraint 

55
06:43:56,500 --> 06:44:00,990
one answer can be that a newly hired employee may not be assigned

56
06:44:00,990 --> 06:44:04,350
a department or a title yet but can still be an entry in the table 

57
06:44:05,560 --> 06:44:09,040
however the department column has another constraint 

58
06:44:10,130 --> 06:44:14,390
it restricts the possible values that is the domain of the attribute

59
06:44:14,390 --> 06:44:17,060
to only four possibilities 

60
06:44:17,060 --> 06:44:20,900
hr it research and business 

61
06:44:22,620 --> 06:44:27,840
finally the first says that id is a primary key 

62
06:44:29,180 --> 06:44:32,960
this means it is unique for each employee 

63
06:44:32,960 --> 06:44:36,200
and for every employee knowing the primary key for

64
06:44:36,200 --> 06:44:41,720
the employee will also uniquely know the other five attributes of that employee 

65
06:44:43,180 --> 06:44:46,240
you should now see that a table with a primary key

66
06:44:46,240 --> 06:44:51,590
logically implies that the table cannot have a duplicate record because if we do 

67
06:44:51,590 --> 06:44:54,870
it will violate the uniqueness constraint associated with the primary key 

68
06:44:56,330 --> 06:45:00,140
let us introduce a new table containing the salary history of employees 

69
06:45:01,410 --> 06:45:05,010
the employees are identified with the column empid but

70
06:45:05,010 --> 06:45:07,760
these are not new values that this table happens to have 

71
06:45:08,900 --> 06:45:13,460
they are the same ids that are present in the id column of the employee table 

72
06:45:13,460 --> 06:45:14,140
presented earlier 

73
06:45:16,420 --> 06:45:18,570
this is reflected in the statement made on the right 

74
06:45:19,760 --> 06:45:26,530
the term references means the values in this column can exist

75
06:45:26,530 --> 06:45:31,760
only if the same values if you are in employees the table being referenced 

76
06:45:31,760 --> 06:45:33,700
also called the parent table 

77
06:45:35,150 --> 06:45:41,920
so in the terminology of the relational model the empid column of empsalaries

78
06:45:41,920 --> 06:45:47,660
table is called a foreign key that refers to the primary key of the employees table 

79
06:45:49,560 --> 06:45:55,650
note that empid is not a primary key in this empsalaries table 

80
06:45:55,650 --> 06:45:59,420
because it is multiple to post with the same empid

81
06:45:59,420 --> 06:46:01,920
reflecting the salary of the employee at different times 

82
06:46:03,700 --> 06:46:07,070
you will remember join is a common operation that we discussed before 

83
06:46:08,150 --> 06:46:12,200
so here is an example of a relational join performed

84
06:46:12,200 --> 06:46:16,810
on the first three columns of employee and empsalaries table 

85
06:46:16,810 --> 06:46:23,630
where employees id and empsalaries empid columns are matched for equality 

86
06:46:24,770 --> 06:46:27,130
the output table shows all the columns involved 

87
06:46:28,360 --> 06:46:30,360
the common column is represented once 

88
06:46:31,680 --> 06:46:34,210
this form of join is called a natural join 

89
06:46:35,690 --> 06:46:40,770
it is important to understand that join is one of the most expensive

90
06:46:40,770 --> 06:46:44,570
that means time consuming and space consuming operations 

91
06:46:45,770 --> 06:46:51,270
as data becomes larger and tables contain hundreds of millions of tuples the join

92
06:46:51,270 --> 06:46:55,480
operation can easily become a bottleneck in a larger analytic application 

93
06:46:56,880 --> 06:47:02,280
so for analytical big data application that needs joins it very important to

94
06:47:02,280 --> 06:47:07,890
choose a suitable data management platform that makes this operation efficient 

95
06:47:07,890 --> 06:47:10,090
we will return to this issue in module four 

96
06:47:11,760 --> 06:47:13,600
we end this video on a practical note 

97
06:47:14,830 --> 06:47:16,060
in many scientific and

98
06:47:16,060 --> 06:47:19,990
business applications people start with csv files 

99
06:47:19,990 --> 06:47:24,650
manipulate them with the spreadsheet then migrate their relational system only

100
06:47:24,650 --> 06:47:28,790
as an afterthought where the data becomes too large to handle the spreadsheet 

101
06:47:30,490 --> 06:47:33,500
while the spreadsheet offers many useful features 

102
06:47:33,500 --> 06:47:39,037
it does not conform and enforce many principles of relational data models 

103
06:47:40,700 --> 06:47:45,210
consequently a large amount of time may be spent in cleaning up and

104
06:47:45,210 --> 06:47:48,480
correcting data errors after the migration actually happens 

105
06:47:49,670 --> 06:47:54,860
let me show a few examples from a spreadsheet that has 125 000 rows and

106
06:47:54,860 --> 06:47:55,430
over 100 columns 

107
06:47:56,950 --> 06:48:02,280
the spreadsheet here lists terrorism attacks gathered from news media 

108
06:48:02,280 --> 06:48:05,030
so each row represents one attack 

109
06:48:06,080 --> 06:48:10,210
this is a valuable piece of data for people who study terrorism 

110
06:48:10,210 --> 06:48:13,420
but we are going to look at it from a relational data modelling viewpoint 

111
06:48:15,200 --> 06:48:18,160
first notice the column marked in green 

112
06:48:19,750 --> 06:48:23,900
it lists two weapons used in the attack separated by a semicolon 

113
06:48:25,330 --> 06:48:27,390
why is this really common 

114
06:48:27,390 --> 06:48:29,950
it makes this column non - atomic 

115
06:48:29,950 --> 06:48:34,450
it means that this column actually has two different values 

116
06:48:35,570 --> 06:48:40,140
in a relational design this information will be moved to another table

117
06:48:40,140 --> 06:48:43,870
just like the multiple salaries of employees were placed in a separate table 

118
06:48:45,100 --> 06:48:48,740
next notice the column outlined in red 

119
06:48:50,290 --> 06:48:55,370
it describes the amount of property damaged by a possible terrorist attack 

120
06:48:56,890 --> 06:49:02,880
in this column the intended legitimate values are unknown 

121
06:49:02,880 --> 06:49:04,540
minor major and catastrophic 

122
06:49:05,670 --> 06:49:10,350
however the value in the highlighted part of the spreadsheet is minor and

123
06:49:10,350 --> 06:49:15,012
then within bracket likely less than 1 million 

124
06:49:15,012 --> 06:49:19,700
which means a query like find all attacks for

125
06:49:19,700 --> 06:49:23,390
which the property damage is equal to minor cannot be answered directly 

126
06:49:24,680 --> 06:49:28,150
instead we need to perform a substring search for

127
06:49:28,150 --> 06:49:30,630
minor in the beginning of the description 

128
06:49:30,630 --> 06:49:33,420
which is doable but it a more expensive operation 

129
06:49:34,960 --> 06:49:38,730
this shows the columns of the spreadsheet 

130
06:49:38,730 --> 06:49:40,950
so this is part of the schema of the data 

131
06:49:42,120 --> 06:49:45,310
if you observe carefully you will see a recurring pattern 

132
06:49:47,120 --> 06:49:52,290
the designer of the data table determined that there can be at most three types of

133
06:49:52,290 --> 06:49:57,380
attacks within a single encounter and represented with three separate columns 

134
06:49:58,790 --> 06:50:04,220
now in proper relational modeling one would say that there is a one to many

135
06:50:04,220 --> 06:50:09,650
relationship between the attack and the number of attack types 

136
06:50:11,210 --> 06:50:13,980
in such a case it would be more prudent

137
06:50:13,980 --> 06:50:18,030
to place these attack type columns in a separate table and

138
06:50:18,030 --> 06:50:22,480
connect with the parent using a primary key foreign key relationship 

139
06:50:23,920 --> 06:50:27,240
here another block with a similar pattern 

140
06:50:28,490 --> 06:50:32,420
this time this is about the types and subtypes of weapons used 

141
06:50:33,620 --> 06:50:36,690
now can you determine how you might be able to reorganize this block 

142
06:50:38,130 --> 06:50:39,761
we will leave this as an exercise 

1
13:30:45,030 --> 13:30:47,956
it can be said without a doubt and the internet and

2
13:30:47,956 --> 13:30:50,960
the worldwide web changed everything in our lives 

3
13:30:52,450 --> 13:30:57,100
the worldwide web is indeed the largest information source there is today 

4
13:30:57,100 --> 13:30:58,630
but what is the data model behind the web 

5
13:30:59,800 --> 13:31:02,740
we will say that it is the semi - structure data model 

6
13:31:04,310 --> 13:31:08,560
so after going through this video you will be able to distinguish between

7
13:31:08,560 --> 13:31:11,870
the structured data model that we talked about the last time and

8
13:31:11,870 --> 13:31:14,805
semi - structured data model 

9
13:31:14,805 --> 13:31:18,110
further you will recognize that the most

10
13:31:18,110 --> 13:31:22,680
times the semi - structured data refers to tree structured data 

11
13:31:23,720 --> 13:31:29,160
and you can explain why tree navigation operations are important for

12
13:31:29,160 --> 13:31:31,370
formats like xml and json 

13
13:31:33,050 --> 13:31:35,290
let a take a very simple web page 

14
13:31:37,090 --> 13:31:40,740
now this page does not have a lot of content or stylization 

15
13:31:40,740 --> 13:31:43,970
it does not even have links to other pages but

16
13:31:43,970 --> 13:31:46,390
let look at the corresponding html code 

17
13:31:48,560 --> 13:31:53,340
this code is used by the browser so that it can render the html and

18
13:31:53,340 --> 13:31:55,580
notice a few things in this data 

19
13:31:55,580 --> 13:32:01,710
the entire data comes within the html and slash html blocks 

20
13:32:03,760 --> 13:32:10,590
and we similarly have a body begin and end a header begin and

21
13:32:10,590 --> 13:32:15,950
end a list begin and end and a paragraph begin and end 

22
13:32:18,020 --> 13:32:22,330
everywhere here a block is nested within a larger block 

23
13:32:23,770 --> 13:32:28,350
the second item to notice is that unlike a relational structure

24
13:32:28,350 --> 13:32:31,940
there are multiple list items and multiple paragraphs 

25
13:32:31,940 --> 13:32:35,040
and any single document would have a different number of them 

26
13:32:36,100 --> 13:32:41,250
this means while the date object has some structure it is more flexible 

27
13:32:42,360 --> 13:32:47,930
so this is the hallmark office semi structure date model 

28
13:32:47,930 --> 13:32:49,020
now xml or

29
13:32:49,020 --> 13:32:54,100
the extensible markup language is another well known standard to represent data 

30
13:32:54,100 --> 13:32:58,880
you can think of xml as a generalization of html where the elements that 

31
13:32:58,880 --> 13:33:03,270
the beginning and end markers within the angular brackets can be any string 

32
13:33:03,270 --> 13:33:06,240
and not like the ones allowed by standard html 

33
13:33:07,530 --> 13:33:10,840
let see an example from a biological case 

34
13:33:12,140 --> 13:33:16,770
as you can see there are two elements called sample attribute 

35
13:33:18,100 --> 13:33:21,130
they do structurally different because they

36
13:33:21,130 --> 13:33:24,320
have different numbers of sub elements called the value 

37
13:33:26,030 --> 13:33:28,960
another interesting issue about xml data processing

38
13:33:28,960 --> 13:33:32,530
is that you can actually credit for the structure elements 

39
13:33:32,530 --> 13:33:37,560
for example it is perfectly fine to ask what is the name of the element

40
13:33:37,560 --> 13:33:41,250
which contains a sub - element whose textual content is cell type 

41
13:33:42,390 --> 13:33:48,990
as you can see you will get two results sample attribute 

42
13:33:48,990 --> 13:33:53,604
an experimental factor because sample attribute has a sub - element called

43
13:33:53,604 --> 13:33:57,940
category and experimental factor has a subelement called link and

44
13:33:57,940 --> 13:34:00,860
each of these subelements have the value celltape 

45
13:34:02,090 --> 13:34:05,860
now we cannot perform an operation like this in a relational data model 

46
13:34:05,860 --> 13:34:10,090
for example we cannot say which relation has a column with a value john 

47
13:34:12,730 --> 13:34:17,740
the same idea can also be seen in json or the java script object notation which

48
13:34:17,740 --> 13:34:23,610
is a very popular format used for many different data like twitter and facebook 

49
13:34:23,610 --> 13:34:29,170
consider the example here all of the format looks different 

50
13:34:29,170 --> 13:34:32,390
we have a similar nested structure varies that is lists

51
13:34:32,390 --> 13:34:37,030
containing other lists which will contain topples which consists of p value ps 

52
13:34:39,080 --> 13:34:43,160
so the key value pairs at atomic property names and their values 

53
13:34:44,660 --> 13:34:49,760
but one way to generalize about all these different forms of semi structured data

54
13:34:49,760 --> 13:34:52,460
is to model them as trees 

55
13:34:52,460 --> 13:34:54,380
let go back to xml 

56
13:34:54,380 --> 13:34:57,210
the left side shows an xml document and

57
13:34:57,210 --> 13:34:59,440
the right side shows the corresponding tree 

58
13:35:00,660 --> 13:35:04,030
since the top object of the root element is document 

59
13:35:04,030 --> 13:35:05,230
it is also the root of the tree 

60
13:35:06,550 --> 13:35:10,620
now under document we have a report element with author and

61
13:35:10,620 --> 13:35:16,450
date under it and also a paper element with title author and source under it 

62
13:35:16,450 --> 13:35:20,670
the actual values like is the textual content of an element 

63
13:35:22,480 --> 13:35:25,760
since a text data item cannot have any further components 

64
13:35:25,760 --> 13:35:28,590
these text values are always the leaves of the tree 

65
13:35:30,760 --> 13:35:35,950
now modeling a document as a tree has significant advantages 

66
13:35:35,950 --> 13:35:37,740
a tree is a well - known data structure 

67
13:35:37,740 --> 13:35:42,100
that allows what is called a navigational access to data 

68
13:35:42,100 --> 13:35:44,150
imagine you are standing on the note paper 

69
13:35:45,460 --> 13:35:50,610
now you can perform a getparent operation and navigate the document 

70
13:35:50,610 --> 13:35:56,460
or you can perform a getchildren operation to get to the title author and source 

71
13:35:56,460 --> 13:36:00,550
you can even perform a getsiblings operation and get to the report 

72
13:36:01,810 --> 13:36:06,970
you can also ask a textual query like which strings have the substring data and

73
13:36:06,970 --> 13:36:13,340
seek their root - to - node path to get to the path from document to the text nodes 

74
13:36:13,340 --> 13:36:16,590
you can possibly see how queries can be evaluated on the tree 

75
13:36:17,670 --> 13:36:19,250
now let us take the query 

76
13:36:19,250 --> 13:36:21,990
who is the author of xml query data model 

77
13:36:23,900 --> 13:36:29,987
in one evaluation scheme we can navigate up from the text note to title 

78
13:36:29,987 --> 13:36:35,580
to paper and then navigate down to author and then to don robie 

79
13:36:35,580 --> 13:36:38,990
well how do we know that we have to get up to paper before reversing the direction 

80
13:36:40,050 --> 13:36:43,470
well paper is the least that the lowest in the tree 

81
13:36:43,470 --> 13:36:49,170
common ancestor of the author note and the xm query data model note 

82
13:36:49,170 --> 13:36:52,207
we will come back to semi structure data in a later module 

1
03:07:35,851 --> 03:07:39,973
in this hands - on activity we will be looking at graph data in gephi 

2
03:07:39,973 --> 03:07:44,510
first we will import data into gephi and then examine the properties of the graph 

3
03:07:45,720 --> 03:07:49,160
next we will perform some statistical operations on the graph data 

4
03:07:49,160 --> 03:07:51,750
and then run some different layout algorithms 

5
03:07:54,090 --> 03:07:55,585
let begin 

6
03:07:55,585 --> 03:08:00,370
we are not running gephi in the cloudera virtual machine on the coursera website 

7
03:08:00,370 --> 03:08:04,450
there will be a reading with instructions on how to download install and

8
03:08:04,450 --> 03:08:08,050
run gephi on your native hardware instead of in the virtual machine 

9
03:08:09,960 --> 03:08:13,693
once you have gephi started let import the data into gephi 

10
03:08:13,693 --> 03:08:20,908
we will go to file import spreadsheet and in the csv dialog 

11
03:08:20,908 --> 03:08:26,338
we will click the button with dot dot dot 

12
03:08:26,338 --> 03:08:31,679
we will choose diseasegraph csv and click open 

13
03:08:35,336 --> 03:08:40,420
make sure that as table says edges table click next 

14
03:08:43,570 --> 03:08:47,535
and make sure create missing nodes is checked 

15
03:08:47,535 --> 03:08:51,215
we will click finish to import the csv file as a graph 

16
03:08:51,215 --> 03:08:53,630
gephi now shows the graph in the center pane 

17
03:08:55,270 --> 03:08:57,930
the little black circles are the nodes of the graph and

18
03:08:57,930 --> 03:09:00,500
the lines between them are the edges 

19
03:09:00,500 --> 03:09:06,707
in the top right we can see that there are 777 nodes and 998 edges 

20
03:09:06,707 --> 03:09:10,380
next let perform some statistical operations on this graph 

21
03:09:11,510 --> 03:09:16,145
in the statistics pane we can see average degree 

22
03:09:16,145 --> 03:09:18,980
let compute the average degree of the graph by clicking on run 

23
03:09:23,320 --> 03:09:27,260
this says that the average degree is 2 569 

24
03:09:27,260 --> 03:09:32,693
let close this let compute the connected

25
03:09:32,693 --> 03:09:36,583
components we will click on run 

26
03:09:36,583 --> 03:09:39,400
we will leave this as a directed since the graph is directed 

27
03:09:41,140 --> 03:09:43,930
click ok it says that there

28
03:09:43,930 --> 03:09:48,940
are 5 weakly connected components and 761 strongly connected components 

29
03:09:50,440 --> 03:09:51,400
let close this 

30
03:09:52,470 --> 03:09:55,710
next let run some different layout algorithms over the graph 

31
03:09:57,240 --> 03:09:59,760
the bottom left we will go to choose layout 

32
03:10:01,550 --> 03:10:06,019
we will choose force atlas and click run 

33
03:10:24,345 --> 03:10:26,080
click stop to stop the layout 

34
03:10:27,655 --> 03:10:32,200
we can see that gephi has grouped strongly connected components together

35
03:10:32,200 --> 03:10:33,320
in different clusters 

36
03:10:33,320 --> 03:10:35,920
we can also see that they are parts of the graph that are not connected 

37
03:10:37,440 --> 03:10:38,830
let run a different layout algorithm 

38
03:10:40,080 --> 03:10:45,395
the combo box choose fruchterman reingold click run 

39
03:10:52,234 --> 03:10:57,125
after it runs for a few seconds click stop then click the magnifying glass 

40
03:10:57,125 --> 03:10:59,680
center on graph to see the whole graph 

41
03:11:02,320 --> 03:11:05,700
in this layout all the nodes appear to be equally spaced 

42
03:11:05,700 --> 03:11:08,537
but we can also see the nodes with many edges 

1
06:18:43,350 --> 06:18:46,350
in this hands on activity we will be working with lucene 

2
06:18:46,350 --> 06:18:49,310
a search engine that uses a vector space model to index data 

3
06:18:50,430 --> 06:18:52,580
first we will open a terminal window and

4
06:18:52,580 --> 06:18:56,200
change into the directory containing the data and scripts 

5
06:18:56,200 --> 06:19:00,700
next we will index some text documents and query terms in lucene 

6
06:19:01,960 --> 06:19:04,538
after that we will query using weighted terms or

7
06:19:04,538 --> 06:19:07,810
boosting to see how this changes the rankings 

8
06:19:07,810 --> 06:19:12,045
finally we will show the term frequency - inverse document frequency or

9
06:19:12,045 --> 06:19:14,988
tf - idf in terms 

10
06:19:14,988 --> 06:19:17,300
let begin 

11
06:19:17,300 --> 06:19:21,139
first let open a terminal window by clicking on the terminal icon at the top

12
06:19:21,139 --> 06:19:21,953
of the tool bar 

13
06:19:25,413 --> 06:19:29,090
next let cd into the directory containing the scripts and data 

14
06:19:29,090 --> 06:19:30,679
we will run cd

15
06:19:30,679 --> 06:19:40,560
downloads big - data - 2 vector 

16
06:19:40,560 --> 06:19:44,201
we will run ls to see the scripts 

17
06:19:48,543 --> 06:19:51,670
the data directory contains three text files 

18
06:19:51,670 --> 06:19:55,310
each of these text files contains news data about elections 

19
06:19:57,730 --> 06:20:03,497
let index these files by running runlucenequery sh data 

20
06:20:10,974 --> 06:20:17,740
next let query lucene for some terms in these text documents and see the rankings 

21
06:20:17,740 --> 06:20:19,970
let query for the term voters 

22
06:20:25,242 --> 06:20:29,973
you can see the rankings and scores that news1 csv ranked first 

23
06:20:29,973 --> 06:20:34,900
news2 csv was the second ranking and the third was news3 csv 

24
06:20:34,900 --> 06:20:38,359
let query for delegates 

25
06:20:38,359 --> 06:20:43,324
for this term we see that news2 csv

26
06:20:43,324 --> 06:20:48,288
was first and news1 was second and

27
06:20:48,288 --> 06:20:53,260
news3 did not contain the term at all 

28
06:20:55,710 --> 06:20:59,275
now let query for both terms voters and delegates 

29
06:20:59,275 --> 06:21:06,935
in this result we see that news2 was ranked first 

30
06:21:06,935 --> 06:21:11,566
news1 was ranked second and

31
06:21:11,566 --> 06:21:15,500
news3 was ranked third 

32
06:21:17,230 --> 06:21:19,000
now lets use query term waiting or

33
06:21:19,000 --> 06:21:22,920
boosting to increase the relevance of voters 

34
06:21:22,920 --> 06:21:27,900
i can do this by ensuring voters carat 5 delegates 

35
06:21:27,900 --> 06:21:35,524
the carat 5 notation is a syntax for

36
06:21:35,524 --> 06:21:40,360
lucene for boosting 

37
06:21:41,890 --> 06:21:47,990
when we run this we see that now news1 is ranked first 

38
06:21:47,990 --> 06:21:52,170
news2 is ranked second and news3 is ranked third 

39
06:21:52,170 --> 06:21:56,500
notice this is different from the original query with voters and delegates 

40
06:21:56,500 --> 06:22:00,100
where news2 is ranked first and news1 was ranked second 

41
06:22:02,210 --> 06:22:05,890
now let look at the term frequency inverse document frequency or tf - idf 

42
06:22:05,890 --> 06:22:10,465
we will enter q to quit this and

43
06:22:10,465 --> 06:22:15,960
we will run lucene tf - idf sh data 

44
06:22:27,380 --> 06:22:33,000
let look at the tf - idf for voters 

45
06:22:33,000 --> 06:22:35,320
you can see that it ranked number 1 for news1 

46
06:22:36,370 --> 06:22:40,160
second news 2 and news 3 is last 

47
06:22:40,160 --> 06:22:41,170
lets try delegates 

48
06:22:47,110 --> 06:22:51,040
here we see that news 2 had a higher score than news 1 

49
06:22:51,040 --> 06:22:54,850
and news 3 is not listed because news 3 does not contain this term 

50
06:22:55,870 --> 06:22:56,590
hit q to quit 

1
12:41:38,800 --> 12:41:43,529
so the next category of data we discuss has the form of graphs or networks 

2
12:41:43,529 --> 12:41:46,749
the most obvious example being social networks 

3
12:41:46,749 --> 12:41:48,782
now speaking of social networks 

4
12:41:48,782 --> 12:41:53,760
tim libzek created a social network from the lord of the rings trilogy 

5
12:41:53,760 --> 12:41:56,545
this graph represents the characters allegiances 

6
12:41:56,545 --> 12:41:58,744
that is who is faithful to whom in the books 

7
12:41:58,744 --> 12:42:02,670
so the nodes are characters and other entities like cities and

8
12:42:02,670 --> 12:42:07,070
the edges connecting pairs of nodes represent allegiances 

9
12:42:07,070 --> 12:42:13,480
so after this video you will be able to identify graph data in practical problems

10
12:42:13,480 --> 12:42:19,150
and describe path neighborhood and connectivity operations in graphs 

11
12:42:19,150 --> 12:42:22,860
but this specialization includes a separate course in graph analytics

12
12:42:22,860 --> 12:42:26,470
that provides a much more detailed treatment on the subject 

13
12:42:26,470 --> 12:42:29,810
now what distinguishes a graph from other data models

14
12:42:29,810 --> 12:42:33,400
is that it bears two kinds of information 

15
12:42:33,400 --> 12:42:38,080
one properties and attributes of entities and relationships and

16
12:42:38,080 --> 12:42:42,130
two the connectivity structure that constitutes the network itself 

17
12:42:43,480 --> 12:42:46,260
one way to look at this data is shown in the figure 

18
12:42:46,260 --> 12:42:48,070
borrowed from the apache spark system 

19
12:42:49,130 --> 12:42:50,360
in this representation 

20
12:42:50,360 --> 12:42:54,390
the graph on the left is represented by two tables on the right 

21
12:42:54,390 --> 12:43:00,095
the vertex or node table gives ids to nodes and lists their properties 

22
12:43:01,170 --> 12:43:03,720
the edge table has two parts 

23
12:43:03,720 --> 12:43:07,210
the colored part represents the properties of the edge 

24
12:43:07,210 --> 12:43:11,600
whereas the white part contains just the direction of the arrows in the network 

25
12:43:11,600 --> 12:43:17,280
thus since there is a directed edge going from node 3 to node 7 

26
12:43:17,280 --> 12:43:21,830
there is a tupple 3 7 in that part of the edge table 

27
12:43:21,830 --> 12:43:26,540
now this form of the graph model is called the property graph model 

28
12:43:26,540 --> 12:43:30,760
which we will see many times in this course and in the specialization 

29
12:43:30,760 --> 12:43:35,740
now representing connectivity information gives graph data a new kind of

30
12:43:35,740 --> 12:43:39,910
computing ability that different from other data models we have seen so far 

31
12:43:41,090 --> 12:43:45,450
even without looking at the properties of the nodes and edges one can get very

32
12:43:45,450 --> 12:43:50,640
interesting information just by analyzing or querying this connectivity structure 

33
12:43:51,800 --> 12:43:56,990
consider a social network with three types of nodes user city and

34
12:43:56,990 --> 12:44:02,250
restaurant and three types of edges friend likes and lives in 

35
12:44:03,300 --> 12:44:06,110
the leftmost node ag represents me 

36
12:44:06,110 --> 12:44:09,930
and i am interested in finding a good italian restaurant in new york

37
12:44:09,930 --> 12:44:14,360
that my friends or their friends who also live in new york like 

38
12:44:14,360 --> 12:44:19,765
i shall possibly choose it3 because it has the highest number of

39
12:44:19,765 --> 12:44:25,700
like edges coming into it from people who have a lives in edge to new york 

40
12:44:25,700 --> 12:44:27,008
and at the same time 

41
12:44:27,008 --> 12:44:30,869
can be reached by following the friend edges going out from me 

42
12:44:30,869 --> 12:44:35,306
now this shows a very important class of operations and ground data namely

43
12:44:35,306 --> 12:44:39,830
traversal that involves edge following based on some sort of conditions 

44
12:44:41,090 --> 12:44:45,660
a number of path operations required some sort of optimization 

45
12:44:45,660 --> 12:44:50,885
the simplest among these is the well known shortest path query which is applied to

46
12:44:50,885 --> 12:44:55,750
node networks to find the best route from a source location to a target location 

47
12:44:55,750 --> 12:44:59,215
the second class of optimization operations is required to find

48
12:44:59,215 --> 12:45:04,050
an optimal path that must include some user specified nodes for

49
12:45:04,050 --> 12:45:08,420
the operation has to determine the order in which the nodes once we visited 

50
12:45:08,420 --> 12:45:10,795
the classical application is a trip planner 

51
12:45:10,795 --> 12:45:14,610
where the user specifies the cities she wishes to visit and

52
12:45:14,610 --> 12:45:18,740
the operation will optimize the criterion like the total distance covered 

53
12:45:18,740 --> 12:45:23,260
the third category is a case where the system must find the best possible

54
12:45:23,260 --> 12:45:25,690
path in the network given two or

55
12:45:25,690 --> 12:45:30,070
more optimization criteria which cannot be satisfied simultaneously 

56
12:45:30,070 --> 12:45:34,350
for example if i want to travel from my house to the airport

57
12:45:34,350 --> 12:45:39,150
using the shortest distance but also minimizing the amount of highway travel 

58
12:45:39,150 --> 12:45:41,710
the algorithm must find a best compromise 

59
12:45:41,710 --> 12:45:45,510
this is called a pareto - optimality problem on graphs 

60
12:45:45,510 --> 12:45:51,120
the neighborhood of a node n in a graph is a set of edges directly connected to it 

61
12:45:51,120 --> 12:45:56,360
a k neighborhood of n is a collection of edges between nodes that are 

62
12:45:56,360 --> 12:45:59,160
at most k steps away from n 

63
12:45:59,160 --> 12:46:02,710
so going back to our mini social network graph bob jill 

64
12:46:02,710 --> 12:46:07,630
and sarah are the first neighbors of ag while max tim and

65
12:46:07,630 --> 12:46:12,570
pam belong to the second neighborhood and not the first neighborhood of ag 

66
12:46:12,570 --> 12:46:15,910
finally jen is a third level neighbor 

67
12:46:16,910 --> 12:46:21,790
an important class of analysis to perform with neighborhoods is community finding 

68
12:46:21,790 --> 12:46:25,740
a community and a social network can be a very close group of friends 

69
12:46:25,740 --> 12:46:29,340
so the graph shown in this figure has four communities 

70
12:46:29,340 --> 12:46:34,327
one can see in the figure that each community has a higher density of edges

71
12:46:34,327 --> 12:46:40,480
within the community and a lower density across two different communities 

72
12:46:40,480 --> 12:46:43,440
finding densely connected parts of a graph

73
12:46:43,440 --> 12:46:47,860
helps identify neighborhoods that can be recognized as communities 

74
12:46:47,860 --> 12:46:53,032
a more complex class of operations include finding the best possible clusters 

75
12:46:53,032 --> 12:46:55,620
which is another name for communities in a graph so

76
12:46:55,620 --> 12:47:00,030
that any other grouping of nodes into communities will be less effective 

77
12:47:00,030 --> 12:47:05,430
now as graphs become bigger and denser these methods become harder to compute 

78
12:47:05,430 --> 12:47:10,240
thus neighborhood - based optimization operation present

79
12:47:10,240 --> 12:47:12,520
significant scalability challenges 

80
12:47:12,520 --> 12:47:16,650
if we inspect the neighborhood of every node in a graph sometimes 

81
12:47:16,650 --> 12:47:20,650
we will find neighborhoods that are different from all others 

82
12:47:20,650 --> 12:47:23,390
these neighborhoods are called anomalous 

83
12:47:23,390 --> 12:47:27,720
consider the following four graphs and on the central red node 

84
12:47:27,720 --> 12:47:32,177
the first graph is odd because it almost perfectly star shaped 

85
12:47:32,177 --> 12:47:36,751
that is the nodes that the red node is connected to are almost unconnected

86
12:47:36,751 --> 12:47:38,137
amongst themselves 

87
12:47:38,137 --> 12:47:42,000
that really odd because it does not happen in reality much 

88
12:47:42,000 --> 12:47:43,620
so it an anomalous node 

89
12:47:43,620 --> 12:47:47,630
the second figure shows a neighborhood to which

90
12:47:47,630 --> 12:47:51,770
a significantly large number of neighbors has connected amongst themselves 

91
12:47:51,770 --> 12:47:56,860
this makes the graph very cliquish where a clique refers to a neighborhood

92
12:47:56,860 --> 12:48:01,280
where each node is connected to all other neighborhood nodes in the neighborhood 

93
12:48:01,280 --> 12:48:03,770
the third figure shows a neighborhood 

94
12:48:03,770 --> 12:48:09,040
where some edges have an unusually heavy weight compared to the others 

95
12:48:09,040 --> 12:48:12,578
the fourth figure shows a special case of the third 

96
12:48:12,578 --> 12:48:16,930
where one edge is predominantly high rate compared to all the other edges 

97
12:48:18,120 --> 12:48:20,890
connectedness is a fundamental property of a graph 

98
12:48:21,960 --> 12:48:23,390
in a connected graph 

99
12:48:23,390 --> 12:48:27,570
each node is reachable from every other node through some path 

100
12:48:27,570 --> 12:48:32,750
if a graph is not connected but there are subgraphs of it which are connected 

101
12:48:32,750 --> 12:48:36,745
then these subgraphs are called connected components of the original graph 

102
12:48:36,745 --> 12:48:39,799
in the figure on the right there are four connected components 

103
12:48:39,799 --> 12:48:43,805
a search gradient like finding optimal paths

104
12:48:43,805 --> 12:48:47,075
should be performed only within each component and not across them 

105
12:48:48,215 --> 12:48:52,325
for large graphs there are several new parallelized techniques for

106
12:48:52,325 --> 12:48:53,835
the detection of connected components 

107
12:48:55,070 --> 12:48:57,680
we will discuss a map reduce based technique for

108
12:48:57,680 --> 12:48:59,830
connected components in a later course 

1
01:30:40,200 --> 01:30:42,569
we have discussed quite a few data models but

2
01:30:42,569 --> 01:30:45,669
there are many other data models that have been developed for

3
01:30:45,669 --> 01:30:50,500
various purposes and we really cannot cover all of them in a single course 

4
01:30:50,500 --> 01:30:54,360
we will end these lectures on data models with an example that may give you

5
01:30:54,360 --> 01:30:58,560
an insight into a class of objects that define in many different applications 

6
01:30:59,850 --> 01:31:04,972
so after this video you will be able to describe how arrays can serve

7
01:31:04,972 --> 01:31:10,278
as a data model explain why images can be modeled as vector arrays 

8
01:31:10,278 --> 01:31:14,689
specify a set of operations on scalar and vector arrays 

9
01:31:16,969 --> 01:31:19,690
now we have all seen the arrays 

10
01:31:19,690 --> 01:31:23,390
in the simplest case an array is a matrix like this 

11
01:31:23,390 --> 01:31:25,380
let call this array a 

12
01:31:27,960 --> 01:31:32,140
the top row in yellow gives the column numbers and

13
01:31:32,140 --> 01:31:34,890
the left column also in yellow gives the row numbers 

14
01:31:35,970 --> 01:31:40,263
when we need to refer to a value of the array as a 3 2 

15
01:31:40,263 --> 01:31:44,530
we mean the value of the cell in row 3 and column 2 

16
01:31:44,530 --> 01:31:49,270
this is called indexed structure where 3 and

17
01:31:49,270 --> 01:31:55,070
2 are the row and column indices that are necessary to get the value of a data item 

18
01:31:56,430 --> 01:31:58,830
the area has two dimensions 

19
01:31:58,830 --> 01:32:01,050
so hence there are two indexes 

20
01:32:01,050 --> 01:32:04,650
if these were a three dimensional array we would have three indexes 

21
01:32:05,860 --> 01:32:06,390
now earlier 

22
01:32:06,390 --> 01:32:11,360
we have seen that we can represent the two dimensional array as a three column table 

23
01:32:11,360 --> 01:32:14,790
one column for the row index one column for the column index and

24
01:32:14,790 --> 01:32:15,849
the last column for the value 

25
01:32:17,310 --> 01:32:22,310
thus a k dimensional array can be represented as a relation with k

26
01:32:22,310 --> 01:32:23,090
plus one columns 

27
01:32:24,690 --> 01:32:28,100
the number of tuples in this representation will be the product of

28
01:32:28,100 --> 01:32:32,220
the size of the first dimension times the size of the second dimension and so forth 

29
01:32:33,670 --> 01:32:38,000
then in this case the size is five in each dimension 

30
01:32:38,000 --> 01:32:45,070
so there are 25 c column tuples in a relation representing the array 

31
01:32:45,070 --> 01:32:48,890
a more useful situation occurs when the cells of an array have

32
01:32:48,890 --> 01:32:50,490
a vectors as values 

33
01:32:51,620 --> 01:32:56,610
as you can see in the 2d vector array here each cell has a three vector 

34
01:32:56,610 --> 01:32:58,340
that is a vector with three elements 

35
01:32:59,400 --> 01:33:02,150
therefore if we want to receive a cell value and

36
01:33:02,150 --> 01:33:04,930
treat it like before we will get back the whole vector 

37
01:33:06,220 --> 01:33:08,680
now this type of data should look familiar to you 

38
01:33:08,680 --> 01:33:12,840
because images often have a red green and blue channels per pixel 

39
01:33:14,030 --> 01:33:15,050
in other words 

40
01:33:15,050 --> 01:33:20,780
images of vector valued arrays where each array cell has a three color vector 

41
01:33:22,270 --> 01:33:26,000
we can also think of the array model in the context of satellite images 

42
01:33:26,000 --> 01:33:29,850
where there are many more channels depending on the range of wavelengths

43
01:33:29,850 --> 01:33:31,100
each channel catches 

44
01:33:32,360 --> 01:33:35,710
let us consider the operations on arrays of vectors 

45
01:33:35,710 --> 01:33:40,430
because it is a combination of two models one can create different combinations of

46
01:33:40,430 --> 01:33:45,140
array operations vector operations and composite operations 

47
01:33:45,140 --> 01:33:45,650
here are some 

48
01:33:46,810 --> 01:33:49,580
the dimension of the array here the first operation is two 

49
01:33:50,940 --> 01:33:53,289
if we pick up any dimension say one 

50
01:33:53,289 --> 01:33:57,741
the size of it is also two because they are two elements 

51
01:33:57,741 --> 01:34:00,260
zero and one in each dimension 

52
01:34:00,260 --> 01:34:08,259
as we saw before the value of the cell 1 1 is a vector 16 301 74 

53
01:34:08,259 --> 01:34:14,979
while the value of a11 component 2 is 74 

54
01:34:14,979 --> 01:34:19,279
the length of the vector is a square root of the sum of the elements of the vector 

55
01:34:19,279 --> 01:34:25,999
so length of a11 would come to 310 375 

56
01:34:25,999 --> 01:34:27,623
the distance function can be so

57
01:34:27,623 --> 01:34:31,184
simple like the euclidean distance function between two vectors or

58
01:34:31,184 --> 01:34:34,820
the cosine of an angle between them as we saw in the previous lecture 

59
01:34:36,080 --> 01:34:39,580
but it can also be something more complex based on the needs of the application 

60
01:34:41,320 --> 01:34:45,390
obviously one can also perform operations like selection over indices so

61
01:34:45,390 --> 01:34:50,170
we can ask which cells had the zero value greater than 25 

62
01:34:50,170 --> 01:34:53,060
giving as the result zero one and one zero 

63
01:34:54,190 --> 01:34:57,050
you will experience some of these operations in your hands on session 

1
03:05:36,265 --> 03:05:40,497
next we will look at a data model that has been successfully used to

2
03:05:40,497 --> 03:05:44,150
retrieve data from large collections of text and images 

3
03:05:45,170 --> 03:05:46,230
let stay with text for now 

4
03:05:47,590 --> 03:05:51,490
text is often thought of as unstructured data 

5
03:05:51,490 --> 03:05:54,315
primarily because it does not really have attributes and relationships 

6
03:05:55,530 --> 03:05:59,740
instead it is a sequence of strings punctuated by line and parent of breaks 

7
03:06:01,810 --> 03:06:08,370
so one is to think of a different way to find and analyze text data 

8
03:06:08,370 --> 03:06:13,130
in this video we will describe that finding text from a huge collection of

9
03:06:13,130 --> 03:06:16,880
text data is a little different from the data modules we have seen so far 

10
03:06:18,080 --> 03:06:22,710
to find text we not only need the text data itself but

11
03:06:22,710 --> 03:06:27,810
we need a different structure that is computed from the text data 

12
03:06:27,810 --> 03:06:29,020
to create the structure 

13
03:06:29,020 --> 03:06:35,000
we will introduce the notion of the document vector model which we call a vector model 

14
03:06:36,760 --> 03:06:39,510
further you will see that finding a document

15
03:06:39,510 --> 03:06:42,090
is not really an exact search problem 

16
03:06:43,380 --> 03:06:47,090
here we will give a query document and

17
03:06:47,090 --> 03:06:51,130
ask the system to find all documents that are similar to it 

18
03:06:52,440 --> 03:06:54,990
after this video you will be able to describe

19
03:06:54,990 --> 03:06:59,380
how the similarity is computed and how it is used to search documents 

20
03:07:00,930 --> 03:07:05,770
finally you will see that search engines use some form of vector models and

21
03:07:05,770 --> 03:07:07,850
similarity search to locate text data 

22
03:07:09,480 --> 03:07:13,970
and you will see that the same principle can be use for finding similar images 

23
03:07:16,740 --> 03:07:19,410
let us describe the concept of a document to an example 

24
03:07:20,540 --> 03:07:23,170
so lets consider three types of document shown here 

25
03:07:25,040 --> 03:07:26,470
now we will create a matrix 

26
03:07:30,040 --> 03:07:31,790
the rows of the matrix stand for

27
03:07:31,790 --> 03:07:36,950
the documents and columns represent the words in the documents 

28
03:07:36,950 --> 03:07:39,770
we put the number of occurrences of returning the document in

29
03:07:39,770 --> 03:07:41,400
the appropriate cell of the matrix 

30
03:07:42,630 --> 03:07:48,380
in this case the count of each term in each document happens to be one 

31
03:07:48,380 --> 03:07:50,700
this is called the term frequency matrix 

32
03:07:53,340 --> 03:07:56,620
so now that we have created the term frequency matrix 

33
03:07:56,620 --> 03:07:58,540
which we call tf for short 

34
03:07:59,595 --> 03:08:04,350
we will create a new vector called the inverse document frequency for each term 

35
03:08:05,810 --> 03:08:08,990
we will explain why we need this vector on the next slide 

36
03:08:08,990 --> 03:08:11,160
first let see how it computed 

37
03:08:13,280 --> 03:08:17,435
the number of documents n here is 3 

38
03:08:17,435 --> 03:08:20,270
the term new occurs twice in the collection 

39
03:08:21,520 --> 03:08:26,950
so the inverse document frequency or idf of the term new

40
03:08:26,950 --> 03:08:31,810
is log to the base 2 n divided by term count 

41
03:08:31,810 --> 03:08:38,464
that is log to the base 2 3 divided by 2 which is 0 584 

42
03:08:39,980 --> 03:08:43,210
we will show the ideal score for all six terms here 

43
03:08:44,770 --> 03:08:49,735
now some of you may wonder why we use log to the base 2 instead of let 

44
03:08:49,735 --> 03:08:51,540
say log to the base 10 

45
03:08:51,540 --> 03:08:54,350
there is no deep scientific reason for it 

46
03:08:54,350 --> 03:08:57,970
it more of a convention in many areas of computer science

47
03:08:57,970 --> 03:09:00,110
when many important numbers are powers of two 

48
03:09:01,150 --> 03:09:06,153
in reality log to the base two of x is the same

49
03:09:06,153 --> 03:09:13,156
number as log to the base ten of x times log to the base two of ten 

50
03:09:13,156 --> 03:09:18,340
the second number that is log to the base two of ten is a constant 

51
03:09:18,340 --> 03:09:24,080
so the relative score of idf does not change regardless of the base we use 

52
03:09:24,080 --> 03:09:26,170
now let understand this number one more time 

53
03:09:27,400 --> 03:09:31,829
the document frequency of a term is the count of that term in the whole

54
03:09:31,829 --> 03:09:35,029
collection divided by the number of documents 

55
03:09:36,748 --> 03:09:41,168
here we take the inverse of the document frequency so

56
03:09:41,168 --> 03:09:45,510
that n the number of documents is in the numerator 

57
03:09:47,580 --> 03:09:52,270
now before we continue let understand the intuition behind the idf vector 

58
03:09:53,500 --> 03:09:59,210
now suppose you have 100 random newspaper articles and

59
03:09:59,210 --> 03:10:02,110
let say 10 of them cover elections 

60
03:10:03,210 --> 03:10:07,710
which means all the others cover all other subjects 

61
03:10:07,710 --> 03:10:13,730
now in this article let say we will find the term election 50 times in total 

62
03:10:15,660 --> 03:10:20,000
how often do you think you will find the term is as in the verb is 

63
03:10:21,320 --> 03:10:24,440
you can imagine that it will occur in all hundred of them and

64
03:10:24,440 --> 03:10:26,160
that too multiple times 

65
03:10:27,370 --> 03:10:31,240
we can safely assume that the number of occurrences of is

66
03:10:31,240 --> 03:10:33,700
will be 300 at the very least 

67
03:10:35,000 --> 03:10:40,940
thus the document frequency of is is six times the document frequency of election 

68
03:10:41,940 --> 03:10:44,460
but that does not sound right does it 

69
03:10:44,460 --> 03:10:49,030
is is such a common word that it prevalence has a negative impact

70
03:10:49,030 --> 03:10:51,240
on its informativeness 

71
03:10:51,240 --> 03:10:55,520
so now if you want to compute the idf of is and

72
03:10:55,520 --> 03:10:58,990
election the idf of is will be far lower 

73
03:11:00,170 --> 03:11:03,930
so idf acts like a penalty factor for

74
03:11:03,930 --> 03:11:07,570
terms which are too widely used to be considered informative 

75
03:11:10,340 --> 03:11:13,890
now that we have understood that idf is a penalty factor 

76
03:11:13,890 --> 03:11:19,800
we will multiply the tf numbers through the idf numbers giving us this 

77
03:11:22,610 --> 03:11:27,550
this is a column - wise multiplication of the tf numbers with the idf

78
03:11:27,550 --> 03:11:31,677
numbers giving us what we call the tf - idf matrix 

79
03:11:33,120 --> 03:11:40,070
therefore for each document we have a vector represented here as a row 

80
03:11:40,070 --> 03:11:44,910
so that row represents the relative importance of each term in the vocabulary 

81
03:11:44,910 --> 03:11:48,370
vocabulary means the collection of all words that appear in this collection 

82
03:11:50,160 --> 03:11:54,800
if the vocabulary has 3 million entries then this vector can get quite long 

83
03:11:56,150 --> 03:12:00,890
also if the number of document grows let just say to 1 billion 

84
03:12:00,890 --> 03:12:02,720
then it becomes a big data problem 

85
03:12:04,460 --> 03:12:07,660
now the last column after each document of vector here

86
03:12:07,660 --> 03:12:09,180
is the length of the document vector 

87
03:12:10,300 --> 03:12:15,060
which is really the square root of the sum of squares of the individual term scores

88
03:12:15,060 --> 03:12:16,060
as shown in the formula 

89
03:12:19,560 --> 03:12:22,170
to perform a search in the vector space 

90
03:12:22,170 --> 03:12:26,010
we write a query just like we type terms in google 

91
03:12:26,010 --> 03:12:29,170
here the number of terms is three 

92
03:12:29,170 --> 03:12:31,910
out of which the term new appears two times 

93
03:12:33,260 --> 03:12:37,049
in fact this is the maximum frequency out of all terms in the query 

94
03:12:38,220 --> 03:12:42,660
so we take the document vector of the query and multiply each term

95
03:12:42,660 --> 03:12:48,570
by the number of occurrences divided by two which is the maximum term frequency 

96
03:12:48,570 --> 03:12:52,090
now in this case it gives us two non - zero terms 

97
03:12:52,090 --> 03:12:54,950
0 584 and

98
03:12:54,950 --> 03:13:00,460
0 292 for new and york 

99
03:13:00,460 --> 03:13:04,726
then we compute the length of the query vector just like we did for

100
03:13:04,726 --> 03:13:07,730
the document vectors on the previous slide 

101
03:13:07,730 --> 03:13:13,443
next we will compute the similarity between the query vector and each document

102
03:13:13,443 --> 03:13:19,170
with the idea that we will measure how far the query vector is from each document 

103
03:13:21,730 --> 03:13:27,150
now there are many similar functions defined and used for different things 

104
03:13:27,150 --> 03:13:30,940
a popular similarity measure is the cosine function 

105
03:13:30,940 --> 03:13:37,200
which measures the cosine function of the angle between these two vectors 

106
03:13:37,200 --> 03:13:41,080
the mathematical formula for computing the function is given here 

107
03:13:41,080 --> 03:13:45,250
the intuition is that if the vectors are identical 

108
03:13:45,250 --> 03:13:48,330
then the angle between them is zero 

109
03:13:48,330 --> 03:13:50,230
and therefore the cosine function evaluates to one 

110
03:13:52,250 --> 03:13:54,910
as the angle increases 

111
03:13:54,910 --> 03:13:59,450
the value of the cosine function decreases to make them more dissimilar 

112
03:14:00,780 --> 03:14:04,694
the way to compute the function is to multiply the corresponding elements of

113
03:14:04,694 --> 03:14:06,170
the two vectors 

114
03:14:06,170 --> 03:14:08,850
that is the first element of one with the first

115
03:14:08,850 --> 03:14:11,340
element of the second one and so forth 

116
03:14:11,340 --> 03:14:12,790
and then sum of these products 

117
03:14:13,960 --> 03:14:19,610
here the only contributing terms are from new and

118
03:14:19,610 --> 03:14:23,230
york because these are the only two non - zero terms in the query vector 

119
03:14:24,660 --> 03:14:29,630
this sum is then divided by the product of the document length and

120
03:14:29,630 --> 03:14:32,040
the query length that we have computed earlier 

121
03:14:33,310 --> 03:14:36,650
look at the result of the distance function and you will notice that

122
03:14:36,650 --> 03:14:41,570
the document 1 is much more similar to the query than the other two 

123
03:14:42,810 --> 03:14:47,640
so while similarity scoring and document ranking process working effectively 

124
03:14:47,640 --> 03:14:50,100
the method is a little cotton dry 

125
03:14:50,100 --> 03:14:53,920
more often than not users would like a little more control

126
03:14:53,920 --> 03:14:54,910
over the ranking of terms 

127
03:14:56,270 --> 03:15:01,450
one way of accomplishing this is to put different weights on each query term 

128
03:15:01,450 --> 03:15:07,460
and in this example the query term york has a default weight of one 

129
03:15:07,460 --> 03:15:10,530
times has a weight of two 

130
03:15:10,530 --> 03:15:14,340
and post has a weight of five as specified by the user 

131
03:15:15,350 --> 03:15:17,662
so relatively speaking 

132
03:15:17,662 --> 03:15:23,765
york has a weight of 1 divided by 1 + 5 + 2 is equal to 0 125 

133
03:15:23,765 --> 03:15:29,630
times has a weight of 0 25 and post has a weight of 0 625 

134
03:15:29,630 --> 03:15:33,280
now the scoring method we showed before will change a bit 

135
03:15:33,280 --> 03:15:38,091
the query of vector and its length were exactly as computed before 

136
03:15:38,091 --> 03:15:42,777
however now each term in the query vector is further multiplied by these

137
03:15:42,777 --> 03:15:44,810
relative weights 

138
03:15:44,810 --> 03:15:48,490
in our case the term york now has a much higher rate 

139
03:15:49,500 --> 03:15:55,237
so as expected this will change the ranking of the documents and

140
03:15:55,237 --> 03:15:58,900
new york post will have the highest rank 

141
03:15:58,900 --> 03:16:04,580
now similarity search is often used for images using a vector space model 

142
03:16:04,580 --> 03:16:07,210
one can compute futures from images 

143
03:16:07,210 --> 03:16:10,180
and one common feature is a scatter histogram 

144
03:16:10,180 --> 03:16:12,000
consider the image here 

145
03:16:12,000 --> 03:16:14,820
one can create the histogram of the red green and

146
03:16:14,820 --> 03:16:20,170
blue channels where histogram is the count of pixels having a certain density value 

147
03:16:21,360 --> 03:16:26,600
this picture is mostly bright so the count of dark pixels is relatively small 

148
03:16:27,690 --> 03:16:30,746
now one can think of histograms like a vector 

149
03:16:30,746 --> 03:16:35,430
very often the pixel values will be bend before creating a vector 

150
03:16:35,430 --> 03:16:40,424
the table shown is a feature vector where the numbers for each row have been

151
03:16:40,424 --> 03:16:44,960
normalized with the size of the image to make the row sum equal to one 

152
03:16:44,960 --> 03:16:49,320
similar vectors can be computed of the image texture shapes of objects and

153
03:16:49,320 --> 03:16:50,790
any other properties 

154
03:16:50,790 --> 03:16:54,700
thus making a vector space model significant for unstructured data 

1
06:22:29,646 --> 06:22:34,170
in our experience as educators we have observed that

2
06:22:34,170 --> 06:22:38,940
learners often make the assumption that the format of the data

3
06:22:38,940 --> 06:22:43,820
is the same as the logical model of the data in the way that you operate on it 

4
06:22:43,820 --> 06:22:46,740
the goal of this very short lecture is to ensure

5
06:22:46,740 --> 06:22:49,590
that we can clearly distinguish between the two 

6
06:22:49,590 --> 06:22:54,550
so after watching this video you will be able to explain the difference between

7
06:22:54,550 --> 06:22:59,460
format which is a serialized representation of the data as opposed to

8
06:22:59,460 --> 06:23:03,010
data model which we have discussed at length in the previous months 

9
06:23:04,930 --> 06:23:09,050
perhaps the simplest example of a data format is a csv file so here is

10
06:23:09,050 --> 06:23:13,359
a snippet of a csv file from the global terrorism database we discussed earlier 

11
06:23:14,640 --> 06:23:18,520
we know that csv or common separative values means

12
06:23:18,520 --> 06:23:21,910
that the term between two commas is the value of an attribute 

13
06:23:21,910 --> 06:23:22,740
but what is this value 

14
06:23:24,200 --> 06:23:28,110
the common notion is that it a content of a single relation

15
06:23:29,280 --> 06:23:32,870
where each line is a record that a tuple and

16
06:23:32,870 --> 06:23:37,030
the iod value in the csv corresponds to the iod attribute as shown here 

17
06:23:38,340 --> 06:23:42,850
now that might very well be true but let look at a different example 

18
06:23:42,850 --> 06:23:45,440
let say this snippet here is my csv file 

19
06:23:46,790 --> 06:23:49,960
there is no difference between the previous file and this one 

20
06:23:49,960 --> 06:23:52,780
however here is how i like to see the data 

21
06:23:54,630 --> 06:23:56,600
as you can see this is a graph 

22
06:23:56,600 --> 06:24:01,190
and the data model is the same although the format is still csv 

1
12:46:29,870 --> 12:46:32,200
this is the second hands on exercise for sensor data 

2
12:46:32,200 --> 12:46:35,220
in the first we looked at static data in a text file 

3
12:46:35,220 --> 12:46:39,410
in this one we will be looking at real - time streaming measurements 

4
12:46:39,410 --> 12:46:41,675
first we will open a terminal window and

5
12:46:41,675 --> 12:46:44,770
cd into the directory containing the data and the scripts 

6
12:46:44,770 --> 12:46:47,110
next we will connect to the weather station and

7
12:46:47,110 --> 12:46:49,990
look at the real - time data as it streams in 

8
12:46:49,990 --> 12:46:54,280
after that we will look at the key to remind ourselves what the fields mean 

9
12:46:54,280 --> 12:46:57,140
and finally we will plot the data streaming from the weather station 

10
12:46:58,400 --> 12:46:59,490
let begin 

11
12:46:59,490 --> 12:47:02,830
first open a terminal window by clicking on the terminal icon 

12
12:47:02,830 --> 12:47:03,861
top of the toolbar 

13
12:47:05,498 --> 12:47:11,919
 noise let run cd downloads big - data - 2 sensor 

14
12:47:20,421 --> 12:47:23,021
you can run ls to see the name of the scripts 

15
12:47:26,772 --> 12:47:31,114
let run stream - data py to see the real - time data 

16
12:47:39,240 --> 12:47:44,860
this shows us the real - time measurements coming from the weather station 

17
12:47:44,860 --> 12:47:46,738
by looking at the time stamps 

18
12:47:46,738 --> 12:47:51,733
we can see that each measurement arrives about one second after the previous one 

19
12:47:54,078 --> 12:47:57,439
additionally we can see that r1 comes fairly often 

20
12:47:57,439 --> 12:48:01,172
whereas other measurements such as r2 are not as often 

21
12:48:06,361 --> 12:48:07,880
we can open another terminal and

22
12:48:07,880 --> 12:48:11,040
look at the key to remind ourselves what these measurements mean 

23
12:48:22,639 --> 12:48:27,875
the key is in wxt - 520 - format txt 

24
12:48:27,875 --> 12:48:32,693
we could run more wxt - 520 - format txt to view it 

25
12:48:43,792 --> 12:48:48,535
if we go back to our live data we can see that the 19th

26
12:48:48,535 --> 12:48:53,500
measurement here says ta was 22 5 degrees celsius 

27
12:48:54,930 --> 12:48:59,116
and look up here see that ta is the air temperature 

28
12:48:59,116 --> 12:49:03,737
the next measure we can see that dn was equal to 255d 

29
12:49:03,737 --> 12:49:08,340
according to our key dn is the wind direction minimum and

30
12:49:08,340 --> 12:49:10,157
the units are degrees 

31
12:49:15,417 --> 12:49:20,316
we can also plot specific measurements streaming live from the weather station 

32
12:49:23,056 --> 12:49:24,740
let plot the wind speed average 

33
12:49:26,300 --> 12:49:31,819
if we look at our key we see that the wind speed average is sm 

34
12:49:31,819 --> 12:49:37,554
so we can plot this by running stream - plot - data py sm 

35
12:49:49,501 --> 12:49:52,659
this plots the data as the weather station sends it to us 

36
12:49:56,501 --> 12:49:57,843
if we look at the x - axis 

37
12:49:57,843 --> 12:50:00,960
we can see that one measurement comes in about every second 

38
12:50:12,199 --> 12:50:15,620
we can plot other measurements by choosing different fields from the key 

39
12:50:16,650 --> 12:50:25,350
for example we can plot the air pressure by running stream - plot - data py pa 

40
12:50:25,350 --> 12:50:27,289
since pa is the air pressure 

41
12:50:43,810 --> 12:50:47,720
first thing we notice is that there only one measurement so far in the graph 

42
12:50:49,950 --> 12:50:52,710
this means that the air pressure measurements are not coming as

43
12:50:52,710 --> 12:50:54,840
fast as the wind measurements 

44
12:50:54,840 --> 12:50:56,127
in fact we only got one 

1
01:37:25,008 --> 01:37:26,736
in this hands - on activity 

2
01:37:26,736 --> 01:37:30,270
we will be looking at real time data streaming from twitter 

3
01:37:31,360 --> 01:37:33,750
first we will open a terminal window and

4
01:37:33,750 --> 01:37:37,520
cd into the directory containing python scripts to access this data 

5
01:37:38,880 --> 01:37:42,750
next we will look at the contents of tweets streaming from twitter

6
01:37:42,750 --> 01:37:44,220
containing specific words 

7
01:37:45,400 --> 01:37:48,230
finally we will plot the frequency of these streaming tweets 

8
01:37:49,500 --> 01:37:50,810
let begin 

9
01:37:50,810 --> 01:37:54,259
first click on the terminal icon at the top of the toolbar 

10
01:37:58,485 --> 01:38:03,414
let cd into the directory containing the python scripts to access the real time

11
01:38:03,414 --> 01:38:05,400
data from twitter 

12
01:38:05,400 --> 01:38:10,473
we will run cd downloads big - data - 2 json 

13
01:38:17,538 --> 01:38:20,828
we can run ls to see the files in this directory 

14
01:38:26,678 --> 01:38:32,200
the file auth should be created containing your twitter authentication information 

15
01:38:33,200 --> 01:38:36,434
you could see the separate reading for how to setup the twitter app and

16
01:38:36,434 --> 01:38:37,617
how to create this file 

17
01:38:41,023 --> 01:38:45,958
next let run the script live tweets to view tweets in real time containing

18
01:38:45,958 --> 01:38:47,093
a specific word 

19
01:38:47,093 --> 01:38:52,819
let run livetweets py president 

20
01:38:58,534 --> 01:39:01,267
this will show the real time tweets containing the word president 

21
01:39:07,171 --> 01:39:10,280
 inaudible runs in the first column you can see the time stamp of the tweet 

22
01:39:10,280 --> 01:39:13,500
in the second column you can see the text 

23
01:39:19,220 --> 01:39:21,471
when you are done hit ctrl + c 

24
01:39:26,208 --> 01:39:29,836
let run livetweet again using a different keyword that appear more

25
01:39:29,836 --> 01:39:31,260
frequently 

26
01:39:31,260 --> 01:39:35,448
let use the word time we will run livetweet time

27
01:39:49,118 --> 01:39:55,780
when we are done hit ctrl + c 

28
01:39:55,780 --> 01:39:59,900
we can plot the frequency of these tweets by running the script plot tweets 

29
01:40:02,390 --> 01:40:06,373
let run plot tweet president to see the frequency for the word president 

30
01:40:22,849 --> 01:40:26,466
as this runs we could see the frequency changes over time 

31
01:40:44,557 --> 01:40:46,960
i can see that the maximum was one 

32
01:40:48,130 --> 01:40:50,540
and a few times there were just one tweet in that second 

33
01:40:54,380 --> 01:40:56,180
when you are done looking at the graph 

34
01:40:56,180 --> 01:40:57,930
click in the terminal window and hit enter 

35
01:41:00,940 --> 01:41:03,010
now let plot the frequency for the word time 

36
01:41:04,040 --> 01:41:07,645
run plottweets py time 

37
01:41:23,468 --> 01:41:27,500
this plot shows that the word time appears a lot more frequently than president 

38
01:41:28,800 --> 01:41:34,514
we can see spikes in the frequency of 40 and a maximum of around 65 

39
01:41:34,514 --> 01:41:37,663
when you are done looking at the graph 

40
01:41:37,663 --> 01:41:41,840
click on the terminal window and press enter to quit 

1
03:19:07,000 --> 03:19:11,890
with big data streaming from different sources in varying formats models and

2
03:19:11,890 --> 03:19:16,650
speeds it is no surprise that we need to be able to ingest this data

3
03:19:16,650 --> 03:19:19,970
into a fast and scalable storage system

4
03:19:19,970 --> 03:19:24,900
that is flexible enough to serve many current and future analytical processes 

5
03:19:26,410 --> 03:19:31,730
this is when traditional data warehouses with strict data models and data

6
03:19:31,730 --> 03:19:37,160
formats do not fit the big data challenges for streaming and batch applications 

7
03:19:38,760 --> 03:19:44,700
the concept of a data lake was created in response of these data big storage and

8
03:19:44,700 --> 03:19:45,890
processing challenges 

9
03:19:47,140 --> 03:19:51,460
after this video you will be able to describe

10
03:19:51,460 --> 03:19:54,880
how data lakes enable batch processing of streaming data 

11
03:19:56,410 --> 03:20:01,390
explain the difference between schema on write and schema on read 

12
03:20:02,680 --> 03:20:06,320
organize data streams and data lakes and

13
03:20:06,320 --> 03:20:10,720
data warehouses on a spectrum of big data management and storage 

14
03:20:13,080 --> 03:20:14,080
what is a data lake 

15
03:20:15,140 --> 03:20:21,450
simply speaking a data lake is a part of a big data infrastructure

16
03:20:21,450 --> 03:20:26,270
that many streams can flow into and

17
03:20:26,270 --> 03:20:29,680
get stored for processing in their original form 

18
03:20:29,680 --> 03:20:34,140
we can think of it as a massive storage depository with huge

19
03:20:34,140 --> 03:20:39,560
processing power and ability to handle a very large number of concurrence 

20
03:20:39,560 --> 03:20:41,970
data management and analytical tasks 

21
03:20:43,370 --> 03:20:48,660
in 2010 the pentaho corporation cto james dixon

22
03:20:48,660 --> 03:20:50,510
defined a data link as follows 

23
03:20:51,930 --> 03:20:56,830
if you think of a datamart as a store of bottled water cleansed and

24
03:20:56,830 --> 03:21:00,090
packaged and structured for easy consumption 

25
03:21:00,090 --> 03:21:05,320
the data lake is a large body of water in a more natural state 

26
03:21:05,320 --> 03:21:10,940
the contents of the data lake stream in from a source to fill the lake 

27
03:21:10,940 --> 03:21:17,980
and various users of the lake can come to examine it dive in or take samples 

28
03:21:17,980 --> 03:21:20,000
a data lake works as follows 

29
03:21:21,140 --> 03:21:23,600
the data gets loaded from its source 

30
03:21:25,160 --> 03:21:28,140
stored in its native format until it is needed

31
03:21:29,330 --> 03:21:34,240
at which time the applications can freely read the data and add structure to it 

32
03:21:35,500 --> 03:21:39,250
this is what we call schema on read 

33
03:21:39,250 --> 03:21:44,070
in a traditional data warehouse the data is loaded into the warehouse

34
03:21:44,070 --> 03:21:48,670
after transforming it into a well defined and structured format 

35
03:21:48,670 --> 03:21:51,398
this is what we call schema on write 

36
03:21:51,398 --> 03:21:57,078
any application using the data needs to know this format in order to retrieve and

37
03:21:57,078 --> 03:21:58,110
use the data 

38
03:21:59,330 --> 03:22:00,990
in this approach 

39
03:22:00,990 --> 03:22:04,882
data is not loaded into the warehouse unless there is a use for it 

40
03:22:04,882 --> 03:22:10,030
however schema on read approach of data lakes ensures

41
03:22:10,030 --> 03:22:15,040
all data is stored for a potentially unknown use at a later time 

42
03:22:15,040 --> 03:22:18,110
so how is a data lake from a data warehouse 

43
03:22:19,540 --> 03:22:24,650
a traditional data warehouse stores data in a hierarchical file

44
03:22:25,820 --> 03:22:28,973
system with a well - defined structure 

45
03:22:28,973 --> 03:22:35,590
however a data lake stores data as flat files with a unique identifier 

46
03:22:35,590 --> 03:22:39,550
this often gets referred to as object storage in big data systems 

47
03:22:42,250 --> 03:22:48,330
in data lakes each data is stored as a binary large object or

48
03:22:48,330 --> 03:22:51,460
blob and is assigned a unique identifier 

49
03:22:52,460 --> 03:22:59,620
in addition each data object is tagged with a number of metadata tags 

50
03:23:00,860 --> 03:23:04,230
the data can be searched using these metadata tags

51
03:23:04,230 --> 03:23:06,400
to retrieve it when there is a need to access it 

52
03:23:07,820 --> 03:23:09,405
from a users perspective 

53
03:23:09,405 --> 03:23:14,672
metadata is stored is not a problem as long as it is accessible when needed 

54
03:23:14,672 --> 03:23:21,780
in hadoop data architectures data is loaded into hdfs and processed

55
03:23:21,780 --> 03:23:26,410
using the appropriate data management and analytical systems on commodity clusters 

56
03:23:27,860 --> 03:23:33,080
the selection of the tools is based on the nature of the problem being solved and

57
03:23:33,080 --> 03:23:34,540
the data format being accessed 

58
03:23:36,090 --> 03:23:38,920
we will talk more about the processing of data streams and

59
03:23:38,920 --> 03:23:41,820
data lakes in the next course in this specialization 

60
03:23:43,520 --> 03:23:49,250
to summarize a data lake is a storage architecture for

61
03:23:49,250 --> 03:23:51,190
big data collection and processing 

62
03:23:52,670 --> 03:23:56,060
it enables collection of all data suitable for

63
03:23:56,060 --> 03:23:59,060
analysis today and potentially in the future 

64
03:24:00,410 --> 03:24:03,450
regardless of the data source structure and

65
03:24:03,450 --> 03:24:09,120
format it supports storage of data and transforms it only when it is needed 

66
03:24:10,500 --> 03:24:15,940
a data lake ideally supports all parts of the user base to benefit from

67
03:24:15,940 --> 03:24:21,790
this architecture including business storage analytics and computing experts 

68
03:24:23,240 --> 03:24:27,310
finally and perhaps most importantly 

69
03:24:27,310 --> 03:24:31,790
data lakes are infrastructure components within a big data architecture

70
03:24:31,790 --> 03:24:35,980
that can evolve over time based on application - specific needs 

1
06:43:42,870 --> 06:43:45,473
what is a data stream 

2
06:43:45,473 --> 06:43:46,913
after this video 

3
06:43:46,913 --> 06:43:52,323
you will be able to summarize the key characteristics of a data stream 

4
06:43:52,323 --> 06:43:57,263
identify the requirements of streaming data systems and

5
06:43:57,263 --> 06:44:01,323
recognize the data streams you use in your life 

6
06:44:01,323 --> 06:44:04,262
when we talked about how big data is generated and

7
06:44:04,262 --> 06:44:07,640
the characteristics of the big data using sound waves 

8
06:44:07,640 --> 06:44:13,460
one of the challenges we mentioned was the velocity of data coming in varying rates 

9
06:44:15,560 --> 06:44:20,360
for some applications this presents the need to process data

10
06:44:20,360 --> 06:44:24,540
as it is generated or in other words as it streams 

11
06:44:25,760 --> 06:44:29,770
we call these types of applications streaming data processing applications 

12
06:44:30,840 --> 06:44:36,630
this terminology refers to a constant stream of data flowing from a source 

13
06:44:36,630 --> 06:44:41,380
for example data from a sensory machine or data from social media 

14
06:44:43,753 --> 06:44:48,149
an example application would be making data - driven marketing decisions in

15
06:44:48,149 --> 06:44:49,390
real time 

16
06:44:49,390 --> 06:44:53,790
through the use of data from real - time sales trends 

17
06:44:53,790 --> 06:44:56,670
social media analysis and sales distributions 

18
06:44:58,430 --> 06:45:04,070
another example for streaming data processing is monitoring of industrial or

19
06:45:04,070 --> 06:45:06,250
farming machinery in real time 

20
06:45:06,250 --> 06:45:11,075
for monitoring and detection of potential system failures 

21
06:45:11,075 --> 06:45:13,837
in fact any sensor network or

22
06:45:13,837 --> 06:45:19,576
internet of things environment controlled by another entity 

23
06:45:19,576 --> 06:45:23,733
or set of entities falls under this category 

24
06:45:23,733 --> 06:45:27,304
for example as you have seen in an earlier video 

25
06:45:27,304 --> 06:45:29,640
flightstats is an application 

26
06:45:29,640 --> 06:45:33,942
that processes about 60 million weekly flight events

27
06:45:33,942 --> 06:45:37,610
that come into their data acquisition system 

28
06:45:37,610 --> 06:45:41,978
and turns it into real - time intelligence for airlines and

29
06:45:41,978 --> 06:45:45,480
millions of travelers around the world daily 

30
06:45:48,520 --> 06:45:51,180
so how then do we define a data stream 

31
06:45:53,300 --> 06:45:58,268
a stream is defined as a possibly unbounded sequence of

32
06:45:58,268 --> 06:46:00,433
data items or records 

33
06:46:00,433 --> 06:46:06,880
that may or may not be related to or correlated with each other 

34
06:46:06,880 --> 06:46:11,952
each data is generally timestamped and

35
06:46:11,952 --> 06:46:15,670
in some cases geo - tagged 

36
06:46:15,670 --> 06:46:20,063
as you have seen in our examples the data can stream from many sources 

37
06:46:20,063 --> 06:46:25,403
including instruments and many internet of things application areas 

38
06:46:25,403 --> 06:46:29,480
computer programs websites or social media posts 

39
06:46:30,900 --> 06:46:35,360
streaming data sometimes get referred to as event data as

40
06:46:35,360 --> 06:46:39,940
each data item is treated as an individual event in a synchronized sequence 

41
06:46:42,200 --> 06:46:45,070
streams pose very difficult challenges for

42
06:46:45,070 --> 06:46:47,940
conventional data management architectures 

43
06:46:47,940 --> 06:46:54,860
which are built primarily on the concept of persistence static data collections 

44
06:46:54,860 --> 06:46:59,219
due to the fact that most often we have only one chance to look at and

45
06:46:59,219 --> 06:47:02,643
process streaming data before more gets piled on 

46
06:47:02,643 --> 06:47:07,597
streaming data management systems cannot be separated from real - time processing

47
06:47:07,597 --> 06:47:08,160
of data 

48
06:47:10,130 --> 06:47:11,160
managing and

49
06:47:11,160 --> 06:47:16,030
processing data in motion is a typical capability of streaming data systems 

50
06:47:17,060 --> 06:47:19,791
however the sheer size variety and

51
06:47:19,791 --> 06:47:24,305
velocity of big data adds further challenges to these systems 

52
06:47:24,305 --> 06:47:30,260
such systems are designed to manage relatively simple computations 

53
06:47:30,260 --> 06:47:33,640
such as one record at a time or

54
06:47:33,640 --> 06:47:38,070
a set of objects in a short time window of the most recent data 

55
06:47:39,460 --> 06:47:43,090
the computations are done in near - real - time 

56
06:47:43,090 --> 06:47:47,910
sometimes in memory and as independent computations 

57
06:47:49,950 --> 06:47:55,099
the processing components often subscribe to a system 

58
06:47:55,099 --> 06:47:59,074
or a stream source non - interactively 

59
06:48:00,440 --> 06:48:05,580
this means they sent nothing back to the source nor

60
06:48:05,580 --> 06:48:07,710
did they establish interaction with the source 

61
06:48:10,933 --> 06:48:16,133
the concept of dynamic steering involves dynamically changing the next steps or

62
06:48:16,133 --> 06:48:19,345
direction of an application through a continuous

63
06:48:19,345 --> 06:48:22,110
computational process using streaming 

64
06:48:23,460 --> 06:48:29,817
dynamic steering is often a part of streaming data management and processing 

65
06:48:29,817 --> 06:48:36,280
a self - driving car is a perfect example of a dynamic steering application 

66
06:48:37,350 --> 06:48:41,330
but all streaming data applications fall into this category 

67
06:48:41,330 --> 06:48:45,585
such as the online gaming example we discussed earlier in this course 

68
06:48:45,585 --> 06:48:49,893
amazon kinesis an other open - source apache

69
06:48:49,893 --> 06:48:54,993
projects like storm flink spark streaming and

70
06:48:54,993 --> 06:48:59,880
samza are examples of big data streaming systems 

71
06:49:01,650 --> 06:49:04,770
many other companies also provide streaming systems for

72
06:49:04,770 --> 06:49:07,920
big data that are frequently updated in response

73
06:49:07,920 --> 06:49:11,280
to the rapidly changing nature of these technologies 

74
06:49:13,440 --> 06:49:19,300
as a summary dynamic near - real - time streaming data management 

75
06:49:19,300 --> 06:49:24,640
processing and steering is an important part of today big data applications 

76
06:49:25,710 --> 06:49:28,590
next we will look at some of the challenges for

77
06:49:28,590 --> 06:49:30,830
streaming data management and processing 

1
13:33:12,280 --> 13:33:15,810
now that we have seen what streaming data means 

2
13:33:15,810 --> 13:33:19,690
let s look at what makes streaming data different and what some management and

3
13:33:19,690 --> 13:33:22,320
processing challenges for streaming data are 

4
13:33:23,510 --> 13:33:27,130
after this video you will be able to compare and

5
13:33:27,130 --> 13:33:29,980
contrast data in motion and data at rest 

6
13:33:31,450 --> 13:33:34,990
differentiate between streaming and batch data processing 

7
13:33:36,250 --> 13:33:42,070
and list management and processing challenges for streaming data 

8
13:33:42,070 --> 13:33:44,560
we often hear the terms data addressed and

9
13:33:44,560 --> 13:33:47,665
data in motion when talking about big data management 

10
13:33:47,665 --> 13:33:54,260
data - at - rest refers to mostly static data collected from one or

11
13:33:54,260 --> 13:34:00,186
more data sources and the analysis happens after the data is collected 

12
13:34:00,186 --> 13:34:05,920
the term data - in - motion refers to a mode

13
13:34:05,920 --> 13:34:09,210
although similar data collection methods apply 

14
13:34:09,210 --> 13:34:13,490
the data gets analyzed at the same time it is being generated 

15
13:34:14,670 --> 13:34:20,300
just like the sensor data processing in a plane or a self - driving car 

16
13:34:20,300 --> 13:34:25,610
analysis of data addressed is called batch or static processing and

17
13:34:25,610 --> 13:34:29,080
the analysis of streaming data is called stream processing 

18
13:34:30,640 --> 13:34:35,633
the run time and memory usage of most algorithms that process static data 

19
13:34:35,633 --> 13:34:39,890
is usually dependent on the data size and

20
13:34:39,890 --> 13:34:44,010
this size can easily be calculated from files or databases 

21
13:34:45,670 --> 13:34:52,200
a key property of streaming data processing is the size of the data

22
13:34:52,200 --> 13:34:57,650
is unbounded and this changes the types of algorithms that can be used 

23
13:34:59,050 --> 13:35:03,760
algorithms that require iterating or looping over the whole data set are not

24
13:35:03,760 --> 13:35:08,470
possible since with stream data you never get to the end 

25
13:35:09,730 --> 13:35:15,710
the modeling and management of streaming data should enable computations on

26
13:35:15,710 --> 13:35:20,720
one data element or a small window of group of recent data elements 

27
13:35:21,880 --> 13:35:25,820
these computations can update metrics monitor and

28
13:35:25,820 --> 13:35:29,050
plot statistics on the streaming data 

29
13:35:29,050 --> 13:35:33,120
or apply analysis techniques to the streaming data

30
13:35:33,120 --> 13:35:37,010
to learn about the dynamics of the system as a time series 

31
13:35:38,120 --> 13:35:41,820
since computations need to be completed in real time 

32
13:35:41,820 --> 13:35:46,460
the analysis tasks processing streaming data should be quicker or

33
13:35:46,460 --> 13:35:50,370
not much longer than the streaming rate of the data 

34
13:35:50,370 --> 13:35:52,200
which we define by it velocity 

35
13:35:53,460 --> 13:35:56,970
in most streaming systems the management and

36
13:35:56,970 --> 13:36:02,020
processing system subscribe to the data source but does not 

37
13:36:02,020 --> 13:36:06,310
send anything back to the stream source in terms of feedback or interactions 

38
13:36:08,070 --> 13:36:12,530
these requirements for streaming data processing are quite different than batch

39
13:36:12,530 --> 13:36:17,330
processing where the analytical steps have access to often all data and

40
13:36:17,330 --> 13:36:22,890
can take more time to complete a complex analytical task with less pressure

41
13:36:22,890 --> 13:36:27,360
on the completion time of individual data management and processing tasks 

42
13:36:29,120 --> 13:36:32,756
most organizations today use a hybrid architecture 

43
13:36:32,756 --> 13:36:37,990
sometimes get referred to as the lambda architecture for

44
13:36:37,990 --> 13:36:42,490
processing streaming and back jobs at the same time 

45
13:36:42,490 --> 13:36:49,130
in these systems streaming wheel over the real - time data is managed and

46
13:36:49,130 --> 13:36:54,230
kept until those data elements are pushed

47
13:36:54,230 --> 13:36:59,510
to a batch system and become available to access and process as batch data 

48
13:37:00,870 --> 13:37:05,770
in such systems a stream storage layer is used to enable fast

49
13:37:05,770 --> 13:37:10,591
trees of streams and ensure data ordering and consistency 

50
13:37:10,591 --> 13:37:12,398
and a processing layer for

51
13:37:12,398 --> 13:37:17,076
data is used to retrieve data from the storage layer to analyze it and

52
13:37:17,076 --> 13:37:22,003
most probably little bit to a batch data stream and notify the streaming

53
13:37:22,003 --> 13:37:27,810
storage that the data set does no longer need to be in streaming storage 

54
13:37:27,810 --> 13:37:33,200
the big data challenges we discussed were scalability data replication 

55
13:37:33,200 --> 13:37:38,490
and durability and fault tolerance arise in this type of data very significantly 

56
13:37:39,690 --> 13:37:45,260
among many there are two main challenges that needs to be overcome

57
13:37:45,260 --> 13:37:50,820
to avoid data loss and enable real time analytical tasks 

58
13:37:50,820 --> 13:37:55,950
one challenge in streaming data process is that the size and

59
13:37:55,950 --> 13:38:00,020
frequency of the mean data can significantly change over time 

60
13:38:01,330 --> 13:38:06,910
these changes can be unpredictable and may be driven by human behavior 

61
13:38:08,060 --> 13:38:13,340
for example streaming data found on social networks such as facebook and

62
13:38:13,340 --> 13:38:16,025
twitter can increase in volume during holidays 

63
13:38:16,025 --> 13:38:19,160
sports matches or major news events 

64
13:38:20,980 --> 13:38:26,910
these changes can be periodic and occur for example in the evenings or weekends 

65
13:38:28,280 --> 13:38:33,470
for example people may post messages on facebook more in the evening

66
13:38:33,470 --> 13:38:36,940
instead of during the day working hours 

67
13:38:36,940 --> 13:38:41,750
streaming data changes may also be unpredictable and sporadic 

68
13:38:41,750 --> 13:38:44,510
there can be an increase in data size and

69
13:38:44,510 --> 13:38:49,930
frequency during during major events sporting matches and things like that 

70
13:38:49,930 --> 13:38:55,950
other changes include dropping or missing data or even no data

71
13:38:55,950 --> 13:39:01,320
when there are network problems or device generating the data has hardware problems 

72
13:39:01,320 --> 13:39:03,880
as an example of streaming data fluctuation 

73
13:39:03,880 --> 13:39:05,964
consider the number of tweets per second 

74
13:39:05,964 --> 13:39:12,050
on average there are 6 000 tweets sent every second 

75
13:39:12,050 --> 13:39:16,810
however in august 2013 the world record was

76
13:39:16,810 --> 13:39:21,980
set when over 144 000 tweets were sent in a second 

77
13:39:21,980 --> 13:39:24,210
that a factor of 24 increase 

78
13:39:26,320 --> 13:39:31,250
at the end of this lesson we will ask you to focus on twitter streams for

79
13:39:31,250 --> 13:39:34,160
trending topics and any other topic 

80
13:39:35,480 --> 13:39:38,810
you will notice how the rates of tweets streaming

81
13:39:38,810 --> 13:39:42,600
changes between different times and different topics 

82
13:39:42,600 --> 13:39:49,010
to summarize streaming data must be handled differently than static data 

83
13:39:49,010 --> 13:39:54,150
unlike static data where you can determine the size streaming data is

84
13:39:54,150 --> 13:39:59,570
continually generated and you can not process it all at once 

85
13:40:01,250 --> 13:40:07,580
streaming data can unpredictably change in both size and frequency 

86
13:40:07,580 --> 13:40:09,330
this can be due to human behavior 

87
13:40:10,420 --> 13:40:12,910
finally algorithms for

88
13:40:12,910 --> 13:40:17,830
processing streaming data must be relatively fast and simple 

89
13:40:17,830 --> 13:40:20,170
since you do not know when the next data arrives 

1
03:13:33,380 --> 03:13:34,730
in a previous lecture 

2
03:13:34,730 --> 03:13:39,440
we said that new interesting solutions are emerging in the big data product space 

3
03:13:40,780 --> 03:13:44,718
while these solutions do not have the full fledged power of a dbms 

4
03:13:44,718 --> 03:13:49,800
they offer novel feature combinations that suit some application space just right 

5
03:13:50,970 --> 03:13:54,380
one of these products is aerospike 

6
03:13:54,380 --> 03:13:59,040
which calls itself a distributed nosql database and

7
03:13:59,040 --> 03:14:03,340
key value store and goes on to say that it is architected for

8
03:14:03,340 --> 03:14:07,700
the performance needs of today web scale applications 

9
03:14:07,700 --> 03:14:11,630
the diagram here is from an aerospike whitepaper 

10
03:14:12,920 --> 03:14:16,990
it shows how aerospike relates to the ecosystem for which it is designed 

11
03:14:18,340 --> 03:14:23,585
the top layer shows several applications for real time consumer facing systems 

12
03:14:23,585 --> 03:14:28,115
such as travel recommendation systems pricing engines used for

13
03:14:28,115 --> 03:14:31,455
stock market applications real time decision systems that

14
03:14:31,455 --> 03:14:35,145
analyze data to figure out whether an investment should be done and so forth 

15
03:14:36,405 --> 03:14:39,215
now all of these systems have the common need

16
03:14:39,215 --> 03:14:43,385
that large amounts of data should be accessible to them at any point of time 

17
03:14:45,090 --> 03:14:49,939
the aerospike system can interoperate with hadoop - based systems so

18
03:14:49,939 --> 03:14:54,170
spark or a legacy database or even a real time data source 

19
03:14:54,170 --> 03:14:58,761
it can exchange large volumes of data with any such source and

20
03:14:58,761 --> 03:15:03,274
serve fast lookups and queries to the applications above 

21
03:15:03,274 --> 03:15:07,420
now that translates to a very high availability robust and

22
03:15:07,420 --> 03:15:09,460
strong consistency needs 

23
03:15:11,720 --> 03:15:17,600
the figure here presents a high level architecture diagram of aerospike 

24
03:15:17,600 --> 03:15:22,010
the first item to notice here is what they call a fast bat 

25
03:15:23,410 --> 03:15:26,500
which essentially refers to the left side of the architecture 

26
03:15:28,200 --> 03:15:31,700
the client system processes transactions 

27
03:15:31,700 --> 03:15:36,730
that is data that are primarily managed in a primary index that is a key value store 

28
03:15:38,180 --> 03:15:42,950
this index stays in memory for operational purposes 

29
03:15:42,950 --> 03:15:47,430
however the server also interacts with the storage layer for persistence 

30
03:15:48,910 --> 03:15:52,290
the storage layer uses three kinds of storage systems 

31
03:15:53,770 --> 03:15:58,940
in memory with a dynamic ram or dram a regular spinning disk 

32
03:16:00,040 --> 03:16:05,190
and flash ssd which is solid state device for fast loading of data when needed 

33
03:16:06,410 --> 03:16:09,160
in fact the aerospike system

34
03:16:09,160 --> 03:16:13,210
has optimized its performance with characteristics of an ssd in mind 

35
03:16:15,320 --> 03:16:19,890
for those of who are not sure what an ssd is you can think of

36
03:16:19,890 --> 03:16:24,500
an ssd as a kind of storage device whose random read performance is much

37
03:16:24,500 --> 03:16:28,610
faster than speeding hard disk and the write performance is just a little slower 

38
03:16:30,150 --> 03:16:35,556
one vendor recently advertised its ssd has sequence share read speeds of up to

39
03:16:35,556 --> 03:16:40,980
2 500 mbps and the sequential write speeds as fast as 1 500 mbps 

40
03:16:43,435 --> 03:16:45,810
now why is this important in a big data discussion 

41
03:16:47,890 --> 03:16:52,300
when we speak of scalability grade efficiency fast transactions and so

42
03:16:52,300 --> 03:16:56,940
forth we often do not mention that part of the performance

43
03:16:56,940 --> 03:17:00,950
guarantee is governed by the combination of hardware and software 

44
03:17:02,200 --> 03:17:07,200
so the ability to offer more efficient persistent storage with fast io

45
03:17:07,200 --> 03:17:12,540
implies that while a significant amount of information can be stored on disk 

46
03:17:12,540 --> 03:17:16,714
it can be done without compromising the overall system performance for

47
03:17:16,714 --> 03:17:19,819
an environment that needs fast data loading 

48
03:17:22,490 --> 03:17:26,410
the second point of uniqueness is a secondary index there 

49
03:17:27,760 --> 03:17:32,022
aerospike built secondary index fields that are non - primary keys 

50
03:17:32,022 --> 03:17:36,192
a non - primary key is a key attribute that makes a tuple unique 

51
03:17:36,192 --> 03:17:38,560
but it has not been chosen as a primary key 

52
03:17:39,940 --> 03:17:43,980
in aerospike secondary indices are stored in main memory 

53
03:17:43,980 --> 03:17:48,770
they are built on every node in a cluster and co - located with the primary index 

54
03:17:49,960 --> 03:17:54,224
each secondary index entry contains references to records 

55
03:17:54,224 --> 03:17:56,196
which are local to the node 

56
03:17:58,240 --> 03:18:03,414
as a key value store aerospike uses standard database like

57
03:18:03,414 --> 03:18:10,450
scalar types like integer string and so forth as well as lists like reddis 

58
03:18:10,450 --> 03:18:17,340
the map type is similar to the hashtag of reddis and contains attribute value pairs 

59
03:18:17,340 --> 03:18:23,390
since it is focused on real time web application it supports geospatial data 

60
03:18:23,390 --> 03:18:27,930
like places with latitude and longitude values or regency polygons 

61
03:18:30,020 --> 03:18:35,300
this allows them to perform kv store operations for example like is

62
03:18:35,300 --> 03:18:40,640
the location of this in la jolla which is a point - in - polygon query 

63
03:18:40,640 --> 03:18:45,120
or a distance query like find hotels within three miles of my location 

64
03:18:46,700 --> 03:18:49,230
kv queries are constructed programmatically 

65
03:18:50,750 --> 03:18:57,506
now interestingly aerospike also provides a more declarative language called aql 

66
03:18:57,506 --> 03:19:02,840
aql looks very similar to sql 

67
03:19:02,840 --> 03:19:05,520
the standard query language for relational databases 

68
03:19:06,830 --> 03:19:08,360
a query like select name and

69
03:19:08,360 --> 03:19:14,460
age from user star profiles projects out the name and age values

70
03:19:14,460 --> 03:19:19,250
from the profile record set that belongs to the ding space called users 

71
03:19:20,750 --> 03:19:24,080
the language also allows advocate functions like sum and

72
03:19:24,080 --> 03:19:27,800
average and other user defined functions 

73
03:19:27,800 --> 03:19:31,379
which the system may evaluate through a map produced time operation 

74
03:19:34,160 --> 03:19:37,880
we mentioned earlier that while most medium assists today offer base

75
03:19:37,880 --> 03:19:42,850
guarantees aerospike despite being a distributor information system 

76
03:19:42,850 --> 03:19:44,740
actually offers acid guarantees 

77
03:19:45,750 --> 03:19:48,250
this is accomplished using a number of techniques 

78
03:19:50,338 --> 03:19:54,100
we will consider a few of them to give you a flavor of the mechanisms that current

79
03:19:54,100 --> 03:19:59,380
systems use to balance between large scale data management and

80
03:19:59,380 --> 03:20:03,010
transaction management in a cluster where nodes can join or leave 

81
03:20:04,800 --> 03:20:08,499
you may recall that consistency means two different things 

82
03:20:09,640 --> 03:20:13,460
one is to ensure that all constraints like domain constraints are satisfied 

83
03:20:14,940 --> 03:20:17,905
the second meaning is applied to distributor systems and

84
03:20:17,905 --> 03:20:22,320
ensures all copies of a data item in a cluster are in sync 

85
03:20:23,950 --> 03:20:27,040
for operations and single keys with replication and

86
03:20:27,040 --> 03:20:32,070
secondary indices aerospike provides immediate consistency

87
03:20:32,070 --> 03:20:35,720
using synchronous writes to replicas within the cluster 

88
03:20:36,860 --> 03:20:41,083
synchronous write means the write process will be considered

89
03:20:41,083 --> 03:20:44,420
successful only if the replica is all subdated 

90
03:20:45,768 --> 03:20:49,829
no other write is allowed on the record while the object of replica is pending 

91
03:20:51,520 --> 03:20:55,949
so what happens if there is an increase in the number of write operations due to

92
03:20:55,949 --> 03:20:57,800
an increase in ingestion rate 

93
03:20:58,890 --> 03:21:04,490
in aerospike it is possible to relax this immediate consistency condition

94
03:21:04,490 --> 03:21:08,950
by bypassing some of the consistency checks 

95
03:21:08,950 --> 03:21:13,670
but if this is done eventual consistency will still be enforced 

96
03:21:16,080 --> 03:21:21,890
durability is achieved by storing data in the flash ssd on every node and

97
03:21:21,890 --> 03:21:23,530
performing direct reads from the flash 

98
03:21:25,000 --> 03:21:28,920
now durability is also maintained through the process of replication because we have

99
03:21:28,920 --> 03:21:30,790
multiple copies of data 

100
03:21:30,790 --> 03:21:36,870
so even if one node fails the latest copy of the last data is available from one or

101
03:21:36,870 --> 03:21:39,190
more replica nodes in the same cluster 

102
03:21:39,190 --> 03:21:42,350
as well as in nodes residing in remote clusters 

103
03:21:44,550 --> 03:21:47,250
but does that just contradict the cap theorem 

104
03:21:48,525 --> 03:21:53,700
the cap theorem holds when the network is partitioned 

105
03:21:55,220 --> 03:21:58,700
that means when nodes in different parts of the network

106
03:21:58,700 --> 03:22:00,360
have different data content 

107
03:22:01,810 --> 03:22:06,750
aerospike reduces and tries to completely eliminate the situation by making

108
03:22:06,750 --> 03:22:12,250
sure that the master knows exactly where all the other nodes are 

109
03:22:12,250 --> 03:22:15,260
and the replication is happening properly even when

110
03:22:15,260 --> 03:22:17,300
the new nodes are joining the network 

1
06:35:50,850 --> 06:35:55,270
in the previous modules we talked about data variety and streaming data 

2
06:35:56,410 --> 06:36:01,310
in this module we will focus on a central issue in large scale data processing and

3
06:36:01,310 --> 06:36:07,540
management and that is when should we use hadoop or yarn style system 

4
06:36:07,540 --> 06:36:12,230
and when should we use a database system that can perform parallel operations 

5
06:36:12,230 --> 06:36:16,840
and then we will explore how the state of the art big data management systems

6
06:36:16,840 --> 06:36:19,370
address these issues of volume and variety 

7
06:36:20,930 --> 06:36:24,510
we start with the problem of high volume data and

8
06:36:24,510 --> 06:36:26,699
two contrasting approaches for handling them 

9
06:36:28,150 --> 06:36:30,590
so after this video you will be able to

10
06:36:31,650 --> 06:36:35,270
explain the various advantages of using a dbms over a file system 

11
06:36:36,560 --> 06:36:40,500
specify the differences between parallel and distributed dbms 

12
06:36:40,500 --> 06:36:45,362
and briefly describe a mapreduce - style dbms and

13
06:36:45,362 --> 06:36:49,537
its relationship with the current dbmss 

14
06:36:49,537 --> 06:36:54,005
in the early days when database systems were not around or

15
06:36:54,005 --> 06:36:59,450
just came in databases were designed as a set of application programs 

16
06:37:00,600 --> 06:37:05,390
they were written to handle data that resided in files in a file system 

17
06:37:06,450 --> 06:37:09,310
however soon this approach led to problems 

18
06:37:10,520 --> 06:37:14,560
first there are multiple file formats 

19
06:37:14,560 --> 06:37:20,370
and often there was a duplication of information in different files 

20
06:37:20,370 --> 06:37:24,940
or the files simply had inconsistent information that was very hard to

21
06:37:24,940 --> 06:37:29,150
determine especially when the data was large and the file content was complex 

22
06:37:31,550 --> 06:37:35,050
secondly there was not a uniform way to access data 

23
06:37:36,090 --> 06:37:40,120
each data access task like finding employees in a department sorted by their

24
06:37:40,120 --> 06:37:43,990
salary versus finding employees in all departments

25
06:37:43,990 --> 06:37:48,060
sorted by their start date needed to be written as a separate program 

26
06:37:49,160 --> 06:37:52,040
so people ended up writing different programs for

27
06:37:52,040 --> 06:37:54,200
data access as well as data update 

28
06:37:56,350 --> 06:37:59,890
a third problem was rooted to the enforcement of constraints 

29
06:37:59,890 --> 06:38:02,050
often called integrity constraints 

30
06:38:02,050 --> 06:38:08,000
for example to say something like every employee has exactly one job title 

31
06:38:08,000 --> 06:38:11,530
one had arrived that condition as part of an application program called 

32
06:38:12,700 --> 06:38:15,670
so if you want to change the constraint you need to look for

33
06:38:15,670 --> 06:38:17,620
the programs where such a rule is hard coded 

34
06:38:19,590 --> 06:38:23,300
the fourth problem has to do with system failures 

35
06:38:23,300 --> 06:38:27,420
supposed joe an employee becomes the leader of a group and moves in to

36
06:38:27,420 --> 06:38:32,840
the office of the old leader ben who has now become the director of the division 

37
06:38:32,840 --> 06:38:37,788
so we update joe details and move on to update ben new office for

38
06:38:37,788 --> 06:38:39,280
the system crashes 

39
06:38:39,280 --> 06:38:42,501
so the files are incompletely updated and

40
06:38:42,501 --> 06:38:46,096
there is no way to go back and start all over 

41
06:38:46,096 --> 06:38:52,138
the term atomicity means that all of the changes that we need to do for

42
06:38:52,138 --> 06:38:57,657
these promotions must happen altogether as a single unit 

43
06:38:57,657 --> 06:39:02,874
they should either fully go through or not go through at all 

44
06:39:02,874 --> 06:39:09,038
this atomicity is very difficult to handle when the data reside in one or more files 

45
06:39:11,936 --> 06:39:17,173
so a prime reason for the transition to a dbms is to alleviate these and

46
06:39:17,173 --> 06:39:18,842
other difficulties 

47
06:39:18,842 --> 06:39:22,507
if we look at the current dbms especially relational dbms 

48
06:39:22,507 --> 06:39:24,836
we will notice a number of advantages 

49
06:39:27,165 --> 06:39:30,710
dbmss offer query languages which are declarative 

50
06:39:32,060 --> 06:39:36,200
declarative means that we state what we want to retrieve

51
06:39:36,200 --> 06:39:39,060
without telling the dbms how exactly to retrieve it 

52
06:39:40,220 --> 06:39:44,682
in a relational dbms we can say find the average set of salary of

53
06:39:44,682 --> 06:39:49,800
employees in the r d division for every job title and sort from high to low 

54
06:39:49,800 --> 06:39:53,779
we do not have to tell the system how to group these records by job title or

55
06:39:53,779 --> 06:39:55,879
how to extract just the salary field 

56
06:39:58,073 --> 06:40:02,404
a typical user of a dbms who issues queries does not worry about how

57
06:40:02,404 --> 06:40:07,604
the relations are structured or whether they are located in the same machine 

58
06:40:07,604 --> 06:40:09,835
or spread across five machines 

59
06:40:09,835 --> 06:40:13,835
the goal of data independence is to isolate the users from the record

60
06:40:13,835 --> 06:40:16,993
layout so long as the logical definition of the data 

61
06:40:16,993 --> 06:40:21,083
which means the tables and their attributes are clearly specified 

62
06:40:23,195 --> 06:40:27,754
now most importantly relational dbmss have developed a very mature and

63
06:40:27,754 --> 06:40:32,389
continually improving methodology of how to answer a query efficiently 

64
06:40:32,389 --> 06:40:35,255
even when there are a large number of cables and

65
06:40:35,255 --> 06:40:38,590
the number of records exceeds hundreds of millions 

66
06:40:39,800 --> 06:40:44,880
from a 2009 account eb uses the tera data system with

67
06:40:44,880 --> 06:40:50,280
72 machines to manage approximately 2 4 terabytes of relational data 

68
06:40:51,840 --> 06:40:56,496
these systems have built powerful data structures algorithms and

69
06:40:56,496 --> 06:41:00,991
sound principles to determine how a specific array should be onset

70
06:41:00,991 --> 06:41:06,070
efficiently despite the size of the data and the complexity of the tables 

71
06:41:06,070 --> 06:41:10,790
now with any system bad things can happen 

72
06:41:10,790 --> 06:41:13,266
systems fail in the middle of an operation 

73
06:41:13,266 --> 06:41:17,489
malicious processes try to get unauthorized access to data 

74
06:41:17,489 --> 06:41:21,405
one large can often underappreciated aspect of a dbms is

75
06:41:21,405 --> 06:41:27,060
the implementation of transaction safety and failure recovery 

76
06:41:27,060 --> 06:41:28,950
now recall our discussion of atomicity 

77
06:41:30,110 --> 06:41:35,082
in databases a single logical operation on the data is called a transaction 

78
06:41:35,082 --> 06:41:39,314
for example a transfer of funds from one bank account to another 

79
06:41:39,314 --> 06:41:43,253
even involving multiple changes like debiting one account and

80
06:41:43,253 --> 06:41:46,091
crediting another is a single transaction 

81
06:41:46,091 --> 06:41:50,894
now atomicity is one of the four properties that a transaction should

82
06:41:50,894 --> 06:41:52,350
provide 

83
06:41:52,350 --> 06:41:57,740
the four properties collectively called acid are atomicity 

84
06:41:57,740 --> 06:42:01,370
consistency isolation and durability 

85
06:42:02,730 --> 06:42:07,370
consistency means any data written to the database must be

86
06:42:07,370 --> 06:42:12,301
valid according to all defined rules including constrains 

87
06:42:12,301 --> 06:42:17,747
the durability property ensures that once a transaction has been committed 

88
06:42:17,747 --> 06:42:22,709
it will remain so even in the event of power loss crashes or errors 

89
06:42:24,816 --> 06:42:28,573
the isolation property comes in the context of concurrency 

90
06:42:28,573 --> 06:42:32,920
which refers to multiple people updating a database simultaneously 

91
06:42:33,970 --> 06:42:38,640
to understand concurrency think of an airline or a railway reservation system

92
06:42:38,640 --> 06:42:42,330
where hundreds and thousands of people are buying cancelling and

93
06:42:42,330 --> 06:42:45,110
changing their reservations and tickets all at the same time 

94
06:42:46,360 --> 06:42:50,710
the dbms must be sure that a ticket should no be sold twice 

95
06:42:50,710 --> 06:42:54,920
or if one person is in the middle of buying the last ticket 

96
06:42:54,920 --> 06:42:57,840
another person does not see that ticket as available 

97
06:42:59,380 --> 06:43:03,480
these are guaranteed by the isolation property that says 

98
06:43:03,480 --> 06:43:07,340
not withstanding the number of people accessing the system at the same time 

99
06:43:07,340 --> 06:43:14,310
the transactions must happen as if they are done serially that is one after another 

100
06:43:14,310 --> 06:43:21,949
providing these capabilities is an important part of the m in dbms 

101
06:43:21,949 --> 06:43:26,390
so next we consider how traditional databases handle large data volumes 

102
06:43:27,920 --> 06:43:32,093
the classical way in which dbmss have handled the issue of large

103
06:43:32,093 --> 06:43:36,120
volumes is by created parallel and distributed databases 

104
06:43:36,120 --> 06:43:40,244
in a parallel database for example parallel oracle 

105
06:43:40,244 --> 06:43:42,357
parallel db2 or post sql xe 

106
06:43:42,357 --> 06:43:47,966
the tables are spread across multiple machines and operations like selection 

107
06:43:47,966 --> 06:43:51,853
and join use parallel algorithms to be more efficient 

108
06:43:51,853 --> 06:43:55,905
these systems also allow a user to create a replication 

109
06:43:55,905 --> 06:43:58,400
that is multiple copies of tables 

110
06:43:58,400 --> 06:44:01,420
thus introducing data redundancy so

111
06:44:01,420 --> 06:44:06,180
that failure on replica can be compensated for by using another 

112
06:44:07,740 --> 06:44:11,678
further it replicates in sync with each other and

113
06:44:11,678 --> 06:44:15,260
a query can result into any of the replicates 

114
06:44:15,260 --> 06:44:20,021
this increases the number of simultaneous that is conquer into queries

115
06:44:20,021 --> 06:44:22,332
that can be handled by the system 

116
06:44:22,332 --> 06:44:27,942
in contrast a distributed dbms which we will not discuss in detail in this course

117
06:44:27,942 --> 06:44:34,200
is a network of independently running dbmss that communicate with each other 

118
06:44:34,200 --> 06:44:39,275
in this case one component knows some part of the schema of it is neighboring

119
06:44:39,275 --> 06:44:43,670
dbms and can pass a query or part of a query to the neighbor when needed 

120
06:44:45,520 --> 06:44:49,360
so the important takeaway issue here is 

121
06:44:49,360 --> 06:44:52,890
are all of these facilities offered by a dbms important for

122
06:44:52,890 --> 06:44:55,430
the big data application that you are planning to build 

123
06:44:56,590 --> 06:44:59,131
and the answer in many cases can be negative 

124
06:44:59,131 --> 06:45:04,304
however if these issues are important then the database management

125
06:45:04,304 --> 06:45:08,726
systems may offer a viable option for a big data application 

126
06:45:11,034 --> 06:45:15,726
now let take a little more time to address an issue that often discussed in

127
06:45:15,726 --> 06:45:16,819
the big data word 

128
06:45:16,819 --> 06:45:20,300
the question is if dbmss are so powerful 

129
06:45:20,300 --> 06:45:25,178
why do we see the emergence of mapreduce - style systems 

130
06:45:25,178 --> 06:45:29,800
unfortunately the answer to this question is not straightforward 

131
06:45:31,660 --> 06:45:37,180
for a long while now dbmss have effectively used parallelism specifically

132
06:45:37,180 --> 06:45:42,030
parallel databases in addition to replication would also create partitions 

133
06:45:43,220 --> 06:45:48,017
so that different parts of a logical table can physically reside on different

134
06:45:48,017 --> 06:45:53,403
machines then different parts of a query can access the partitions in parallel and

135
06:45:53,403 --> 06:45:55,501
speed up creative performance 

136
06:45:55,501 --> 06:45:59,753
now these algorithms not only improve the operating efficiency but

137
06:45:59,753 --> 06:46:04,882
simultaneously optimize algorithms to take into account the communication cost 

138
06:46:04,882 --> 06:46:09,511
that is the time needed to exchange data between machines 

139
06:46:09,511 --> 06:46:16,390
however classical parallel dbmss did not take into account machine failure 

140
06:46:17,700 --> 06:46:23,070
and in contrast mapreduce was originally developed not for storage and

141
06:46:23,070 --> 06:46:26,850
retrieval but for distributive processing of large amounts of data 

142
06:46:27,890 --> 06:46:32,783
specifically its goal was to support complex custom computations that could

143
06:46:32,783 --> 06:46:35,569
be performed efficiently on many machines 

144
06:46:35,569 --> 06:46:38,259
so in a mapreduce or mr setting 

145
06:46:38,259 --> 06:46:42,357
the number of machines could go up to thousands 

146
06:46:42,357 --> 06:46:46,710
now since mr implementations were done over hadoop file systems 

147
06:46:46,710 --> 06:46:50,760
issues like node failure were automatically accounted for 

148
06:46:52,492 --> 06:46:57,017
so mr effectively used in complex applications like data mining or

149
06:46:57,017 --> 06:47:01,703
data clustering and these algorithms are often very complex and

150
06:47:01,703 --> 06:47:05,202
typically require problem specific techniques 

151
06:47:05,202 --> 06:47:08,554
very often these algorithms have multiple stages 

152
06:47:08,554 --> 06:47:13,891
that is the output from one processing stage is the input to the next 

153
06:47:13,891 --> 06:47:17,519
it is difficult to develop these multistage algorithms in a standard

154
06:47:17,519 --> 06:47:18,650
relational system 

155
06:47:19,990 --> 06:47:24,080
but since these were genetic operations many of them were designed to work

156
06:47:24,080 --> 06:47:27,880
with unstructured data like text and nonstandard custom data formats 

157
06:47:29,730 --> 06:47:34,139
now it now amply clear that this mixture of data management requirements

158
06:47:34,139 --> 06:47:38,818
and data processing analysis requirements have created an interesting tension in

159
06:47:38,818 --> 06:47:40,439
the data management world 

160
06:47:40,439 --> 06:47:44,030
just look at a few of these tension points 

161
06:47:44,030 --> 06:47:48,990
now dbmss perform storage and retrieval operations very efficiently 

162
06:47:50,210 --> 06:47:53,170
but first the data must be loaded into the dbms 

163
06:47:54,230 --> 06:47:55,490
so how much time does loading take 

164
06:47:56,690 --> 06:48:00,900
in one study scientists use two cvs files 

165
06:48:00,900 --> 06:48:05,850
one had 92 attributes with about 165 million tuples for

166
06:48:05,850 --> 06:48:07,890
a total size of 85 gigabytes 

167
06:48:09,190 --> 06:48:13,710
and the other had 227 attributes with 5 million tuples for

168
06:48:13,710 --> 06:48:15,390
a total size of 5 gigabytes 

169
06:48:16,850 --> 06:48:23,610
the time to load and index this data in mysql and postgresql took 15 hours each 

170
06:48:24,690 --> 06:48:29,240
in a commercial database running on three machines it took two hours 

171
06:48:29,240 --> 06:48:32,980
now there are applications like the quantities in the case we discussed

172
06:48:32,980 --> 06:48:37,180
earlier where this kind of loading time is simply not acceptable 

173
06:48:37,180 --> 06:48:39,880
because the analysis on the data must be performed

174
06:48:39,880 --> 06:48:42,070
within a given time limit after it arrival 

175
06:48:44,600 --> 06:48:47,780
a second problem faced by some application is that for

176
06:48:47,780 --> 06:48:51,140
them the dbmss offer too much functionality 

177
06:48:52,160 --> 06:48:56,460
for example think of an application that only looks at the price of an item

178
06:48:56,460 --> 06:48:58,700
if you provide it with a product name or product code 

179
06:49:00,210 --> 06:49:02,850
the number of products it serves is let say 250 million 

180
06:49:04,170 --> 06:49:07,899
this lookup operation happens only on a single table and

181
06:49:07,899 --> 06:49:11,164
does not mean anything more complex like a join 

182
06:49:11,164 --> 06:49:15,963
further consider that while there are several hundred thousand customers

183
06:49:15,963 --> 06:49:19,810
who access this data none of them really update the tables 

184
06:49:19,810 --> 06:49:24,685
so do we need a full function dbms for this read - only application 

185
06:49:24,685 --> 06:49:29,175
or can we get a simpler solution which can use a cluster of machines but

186
06:49:29,175 --> 06:49:32,275
does not provide all the wonderful guarantees that a dbms provides 

187
06:49:34,600 --> 06:49:39,940
at the other end of the spectrum there is an emerging class of optimization

188
06:49:39,940 --> 06:49:44,590
that meets all the nice transactional guarantees that a dbms provides 

189
06:49:44,590 --> 06:49:48,500
and at the same time meets the support for efficient analytical operations 

190
06:49:49,720 --> 06:49:53,741
these are often required for systems like real - time decision support 

191
06:49:53,741 --> 06:49:58,141
that will accept real - time data like customer purchases on a newly released

192
06:49:58,141 --> 06:50:01,188
product will perform some statistical analysis so

193
06:50:01,188 --> 06:50:03,365
that it can determine buying trends 

194
06:50:03,365 --> 06:50:05,969
and then decide whether in real - time 

195
06:50:05,969 --> 06:50:09,040
a discount can be offered to this customer now 

196
06:50:11,860 --> 06:50:16,368
it turns out that the combination of traditional requirements and

197
06:50:16,368 --> 06:50:20,072
new requirements is leading to new capabilities and

198
06:50:20,072 --> 06:50:23,465
products in the big data management technology 

199
06:50:23,465 --> 06:50:28,283
on the one hand dbms technologies are creating new techniques that make

200
06:50:28,283 --> 06:50:31,164
use of mapreduce - style data processing 

201
06:50:31,164 --> 06:50:34,473
many of them are being developed to run on hdfs and

202
06:50:34,473 --> 06:50:38,279
take advantage of his data replication capabilities 

203
06:50:38,279 --> 06:50:42,423
more strikingly dbmss are beginning to have a side door for

204
06:50:42,423 --> 06:50:46,405
a user to perform and mr - style operation on hdfs files and

205
06:50:46,405 --> 06:50:50,399
exchange data between the hadoop subsystem and the dbms 

206
06:50:50,399 --> 06:50:54,862
thus giving the user the flexibility to use both forms of data processing 

207
06:50:57,674 --> 06:51:00,562
it has now been recognized that a simple map and

208
06:51:00,562 --> 06:51:05,155
reduce operations are not sufficient for many data operations leading to

209
06:51:05,155 --> 06:51:10,740
a significant expansion in the number of operations in the mr ecosystems 

210
06:51:10,740 --> 06:51:14,510
for example spark has several kinds of join and

211
06:51:14,510 --> 06:51:17,350
data grouping operations in addition to map and reduce 

212
06:51:19,650 --> 06:51:22,900
sound dbmss are making use of large

213
06:51:22,900 --> 06:51:26,190
distributed memory management operations to accept streaming data 

214
06:51:27,400 --> 06:51:31,510
these systems are designed with the idea that the analysis they need to perform on

215
06:51:31,510 --> 06:51:33,400
the data are known before 

216
06:51:33,400 --> 06:51:37,860
and as new data records arrive they keep a record of the data

217
06:51:37,860 --> 06:51:41,820
in the memory long enough to finish the computation needed on that record 

218
06:51:43,310 --> 06:51:45,610
and finally computer scientists and

219
06:51:45,610 --> 06:51:48,770
data scientists are working towards new solutions

220
06:51:48,770 --> 06:51:53,770
where large scale distributed algorithms are beginning to emerge to solve different

221
06:51:53,770 --> 06:51:57,280
kinds of analytics problems like finding dense regions of a graph 

222
06:51:58,610 --> 06:52:02,210
these algorithms use a mr - style computing and

223
06:52:02,210 --> 06:52:05,880
are becoming a part of a new generation of dbms products

224
06:52:05,880 --> 06:52:08,860
that invoke these algorithms from inside the database system 

225
06:52:09,890 --> 06:52:13,720
in the next video we will take a look at some of the modern day data management

226
06:52:13,720 --> 06:52:16,030
systems that have some of these capabilities 

1
13:28:06,600 --> 13:28:11,300
as we mentioned in the last lesson there is no single irricuteous solution for

2
13:28:11,300 --> 13:28:12,200
big data problems 

3
13:28:12,200 --> 13:28:14,960
so in this lesson 

4
13:28:14,960 --> 13:28:18,900
our goal will be to explore some existing solutions in a little more depth 

5
13:28:20,780 --> 13:28:23,290
so after this lesson you will be able to 

6
13:28:24,540 --> 13:28:27,250
explain the at least five desirable characteristics of

7
13:28:27,250 --> 13:28:33,150
a big data management system explain the differences between acid and

8
13:28:33,150 --> 13:28:36,070
base and

9
13:28:36,070 --> 13:28:41,210
list examples of bdmss and describe some of their similarities and differences 

10
13:28:43,260 --> 13:28:48,890
so we start from high level suppose there were an ideal big data management system 

11
13:28:50,170 --> 13:28:52,610
what capabilities or features should such a system have 

12
13:28:54,000 --> 13:28:57,700
professor michael carey of the university of california irvine

13
13:28:57,700 --> 13:29:00,750
has described a number of characteristics 

14
13:29:00,750 --> 13:29:04,590
we will go through them and use them as an idealistic yardstick

15
13:29:04,590 --> 13:29:07,190
against which we compare existing solutions 

16
13:29:08,500 --> 13:29:14,820
first the ideal bdms would allow for a semi - structured data model 

17
13:29:14,820 --> 13:29:19,060
now that does not mean it will only support a specific format like xml 

18
13:29:20,170 --> 13:29:22,550
the operative word here is flexible 

19
13:29:23,660 --> 13:29:26,390
the flexibility can take many forms 

20
13:29:27,600 --> 13:29:31,920
one of which is the degree to which schemas should be supported by the system 

21
13:29:33,190 --> 13:29:35,810
in a perfect world it should support

22
13:29:35,810 --> 13:29:39,550
a completely traditional application which requires the development of a schema 

23
13:29:41,250 --> 13:29:45,840
at the same time it should also support applications which require no schema 

24
13:29:45,840 --> 13:29:49,420
because the data can vary in terms of its attributes and relationships 

25
13:29:51,790 --> 13:29:57,290
a different axis of flexibility is in the data types it supports 

26
13:29:57,290 --> 13:30:00,810
for example it should support operations on text and documents 

27
13:30:01,920 --> 13:30:07,010
it should also permit social media and other data that have a time component and

28
13:30:07,010 --> 13:30:10,820
need temporal operations like before after during and so on 

29
13:30:12,260 --> 13:30:15,340
similarly it should allow spacial data and

30
13:30:15,340 --> 13:30:19,530
allow operations like find all data within a five mile radius of a landmark 

31
13:30:21,960 --> 13:30:24,290
as we saw in a previous lesson 

32
13:30:24,290 --> 13:30:28,260
a big advantage of a dbms is that is provides a query language 

33
13:30:29,800 --> 13:30:34,360
there is a notion that query languages present a steep learning curve for

34
13:30:34,360 --> 13:30:37,690
data science people with no background in computer science 

35
13:30:37,690 --> 13:30:41,900
however to effectively manage large volumes of data 

36
13:30:41,900 --> 13:30:45,190
it often more convenient to use a query language and

37
13:30:45,190 --> 13:30:49,720
let the query processor automatically determine optimal ways to receive data 

38
13:30:51,270 --> 13:30:54,440
now this query language may or may not look like sql 

39
13:30:54,440 --> 13:30:59,120
which is the standard query language used by the modern relational systems but

40
13:30:59,120 --> 13:31:01,870
it should at least be equally powerful 

41
13:31:03,080 --> 13:31:08,770
now this is not an unreasonable feature given that most dbms vendors offer their

42
13:31:08,770 --> 13:31:16,120
own extension of sql of course it not enough to just have a good query language 

43
13:31:17,430 --> 13:31:22,230
today big data systems must have a parallel query engine

44
13:31:22,230 --> 13:31:24,080
which will run on multiple machines 

45
13:31:25,480 --> 13:31:29,240
the machines can be connected to a shared nothing architecture or

46
13:31:29,240 --> 13:31:32,760
shared memory architecture or a shared cluster 

47
13:31:32,760 --> 13:31:38,260
the shared nothing means two machines do not share a disk or memory 

48
13:31:38,260 --> 13:31:40,750
but this is a critical requirement for

49
13:31:40,750 --> 13:31:45,850
any bdms regardless of how complete the supported query languages is for

50
13:31:45,850 --> 13:31:50,970
efficiency sake continuing with our list the next

51
13:31:50,970 --> 13:31:55,620
capability of a bdms is often not emphasized as much as it should be 

52
13:31:56,890 --> 13:32:00,880
some applications working on top of a bdms will issue queries

53
13:32:00,880 --> 13:32:05,200
which will only have a few conditions and a few small objects to return 

54
13:32:05,200 --> 13:32:10,740
but some applications especially those generated by other software tools or

55
13:32:10,740 --> 13:32:14,380
machine learning algorithms can have many many conditions and

56
13:32:14,380 --> 13:32:15,990
can return many large objects 

57
13:32:17,090 --> 13:32:22,320
in my own work we have seen how internet bots can blast an information system

58
13:32:22,320 --> 13:32:26,230
with really large queries that can potentially choke the system 

59
13:32:26,230 --> 13:32:27,580
but that should not happen in a bdms 

60
13:32:29,810 --> 13:32:32,080
now we discussed streaming data in a previous lesson 

61
13:32:33,600 --> 13:32:37,830
in many cases a bdms will have both streaming data 

62
13:32:37,830 --> 13:32:41,590
which adds to the volume as well as to the large data that

63
13:32:41,590 --> 13:32:44,380
need to be combined with the streaming data to solve a problem 

64
13:32:45,430 --> 13:32:49,140
an example would be to combine streaming data from weather stations

65
13:32:49,140 --> 13:32:52,870
with historical data to make better predictions for wild fire 

66
13:32:55,610 --> 13:32:58,510
we have discussed the definition significance and

67
13:32:58,510 --> 13:33:01,140
importance of scalability before 

68
13:33:01,140 --> 13:33:04,130
however what a bdms needs to guarantee

69
13:33:04,130 --> 13:33:09,260
that it is designed to operate over a cluster possibly a cluster of hundred or

70
13:33:09,260 --> 13:33:13,450
thousand of machines and that it knows how to handle a failure 

71
13:33:15,040 --> 13:33:19,200
further the system should be able to handling new machines joining or

72
13:33:19,200 --> 13:33:20,829
existing machines leaving the cluster 

73
13:33:22,910 --> 13:33:27,530
finally our bdms must have data management capabilities 

74
13:33:27,530 --> 13:33:33,080
it should be easy to install restart and configure provide high availability and

75
13:33:33,080 --> 13:33:37,340
make operational management as simple as possible even when

76
13:33:37,340 --> 13:33:41,880
a bdms is declined across data centers that are possibly geographically apart

77
13:33:44,770 --> 13:33:49,580
in a prior module we discussed the acid properties of transactions and

78
13:33:49,580 --> 13:33:51,880
said that bdmss guarantee them 

79
13:33:53,250 --> 13:33:57,270
for big data systems there is too much data and

80
13:33:57,270 --> 13:34:00,540
too many updates from too many users 

81
13:34:00,540 --> 13:34:03,730
so the effort to maintain acid properties

82
13:34:03,730 --> 13:34:06,330
may lead to a significant slowdown of the system 

83
13:34:07,550 --> 13:34:11,590
now this lead to the idea that while the acid properties are still desirable 

84
13:34:12,840 --> 13:34:18,410
it might be more practical to relax the acid conditions and replace them

85
13:34:18,410 --> 13:34:23,770
with what is called the base properties beginning with basic availability 

86
13:34:25,410 --> 13:34:28,060
this states that the system

87
13:34:28,060 --> 13:34:30,600
does guarantee the availability of data in the following sense 

88
13:34:32,140 --> 13:34:36,710
if you make a request there will be a response to that request 

89
13:34:36,710 --> 13:34:40,310
but the response could still be failure to obtain data or

90
13:34:40,310 --> 13:34:43,800
the data is not inconsistent state or changing state 

91
13:34:45,370 --> 13:34:48,970
well this is not unusual because it much like waiting for

92
13:34:48,970 --> 13:34:50,540
a check to clear a bank account 

93
13:34:53,270 --> 13:34:55,580
second there is a soft state 

94
13:34:56,620 --> 13:35:01,650
which means the state of the system is very likely to change over time 

95
13:35:01,650 --> 13:35:04,030
so even during times without input 

96
13:35:04,030 --> 13:35:08,330
there may be changes going on through the system due to eventual consistency 

97
13:35:09,680 --> 13:35:12,190
thus the state of the system is always soft 

98
13:35:13,640 --> 13:35:16,080
and finally there eventual consistency 

99
13:35:17,330 --> 13:35:22,010
this means that the system will eventually become consistent

100
13:35:22,010 --> 13:35:23,770
once it stops receiving input 

101
13:35:25,870 --> 13:35:27,730
when it stops receiving input 

102
13:35:27,730 --> 13:35:32,800
the data will propagate to everywhere that it should sooner or later go to but

103
13:35:32,800 --> 13:35:36,840
in reality the system will continue to receive input 

104
13:35:36,840 --> 13:35:40,840
and it not checking the consistency of every transaction at every moment

105
13:35:40,840 --> 13:35:44,610
because there still lots of transactions to process 

106
13:35:44,610 --> 13:35:49,904
so if you make a new facebook post your friend in zambia who is supposed

107
13:35:49,904 --> 13:35:55,200
to see your update but is served by a very different data center in a different

108
13:35:55,200 --> 13:36:00,116
geographic region will certainly see it but may not see right away 

109
13:36:02,272 --> 13:36:06,066
now for those of you who have a bit of computer science background 

110
13:36:06,066 --> 13:36:10,744
we just want to mention in passing that there is actually some theoretical results

111
13:36:10,744 --> 13:36:12,260
behind this relaxation 

112
13:36:13,490 --> 13:36:17,090
the result comes from what is called the cap theorem 

113
13:36:17,090 --> 13:36:20,840
also named bauer theorem after the computer scientist eric bauer 

114
13:36:22,200 --> 13:36:27,410
he states that it is impossible for a distributed computer system

115
13:36:27,410 --> 13:36:31,420
to simultaneously provide all three of the following guarantees 

116
13:36:32,680 --> 13:36:34,300
consistency 

117
13:36:34,300 --> 13:36:38,970
it means all nodes see the same data at any time 

118
13:36:41,080 --> 13:36:46,000
availability which is a guarantee that every request receives a response

119
13:36:46,000 --> 13:36:47,948
about whether it succeeded or failed 

120
13:36:47,948 --> 13:36:52,150
and partition tolerance 

121
13:36:52,150 --> 13:36:55,270
which means the system continues to operate

122
13:36:55,270 --> 13:36:59,610
despite arbitrary partitioning due to network failures 

123
13:36:59,610 --> 13:37:03,740
now most of the big data systems available today will adhere

124
13:37:03,740 --> 13:37:08,130
to these base properties although several modern systems

125
13:37:08,130 --> 13:37:12,010
do offer the stricter acid properties or at least several of them 

126
13:37:14,180 --> 13:37:17,060
now given the idealistic background of what is desirable and

127
13:37:17,060 --> 13:37:21,060
achievable in a big data system today marketplace for

128
13:37:21,060 --> 13:37:24,100
big data related products looks somewhat like this 

129
13:37:25,680 --> 13:37:30,350
now this is matt turk depiction of big data products from a couple of years back 

130
13:37:31,750 --> 13:37:35,730
you would notice that the products are grouped into categories like no sql 

131
13:37:35,730 --> 13:37:40,340
massively parallel databases analytic systems real time systems and so forth 

132
13:37:41,430 --> 13:37:46,050
in this lesson we will do a quick tour through a few of these products

133
13:37:47,690 --> 13:37:49,470
from different areas of this landscape 

134
13:37:51,140 --> 13:37:57,070
in each case our goal will to be assess what aspects of our ideal bdms they cover 

135
13:37:57,070 --> 13:38:00,240
and whether they have obvious limitations 

136
13:38:00,240 --> 13:38:05,650
now we will not cover all features of every system but will highlight

137
13:38:05,650 --> 13:38:09,130
those aspects of the system that are relevant to our discussion on bdms 

1
03:06:15,542 --> 03:06:20,110
the first up in our rapid tour of modern systems is redis 

2
03:06:21,420 --> 03:06:25,690
redis calls itself an in - memory data structure store 

3
03:06:25,690 --> 03:06:31,540
in simple terms redis is not a full blown dbms in the sense we discussed earlier 

4
03:06:32,620 --> 03:06:37,490
it can persist data on disks and does so to save its state but

5
03:06:37,490 --> 03:06:42,840
it intended use is to optimally use memory and memory based methods

6
03:06:42,840 --> 03:06:46,840
to make a number of common data structures very fast for lots of users 

7
03:06:48,000 --> 03:06:50,649
here is a list of data structures that redis supports 

8
03:06:54,084 --> 03:06:59,410
a good way to think about them is to think of a data lookup problem 

9
03:06:59,410 --> 03:07:03,160
now in the simplest case a lookup needs a key value pair

10
03:07:03,160 --> 03:07:07,650
where the key is a string and the value is also a string 

11
03:07:07,650 --> 03:07:12,000
so for a lookup we provide the key and get back the value 

12
03:07:12,000 --> 03:07:13,550
simple right 

13
03:07:13,550 --> 03:07:14,050
let see 

14
03:07:15,760 --> 03:07:18,510
i am sure you have seen captures like this 

15
03:07:20,090 --> 03:07:24,770
these are small images used by websites to ensure that the user is a human and

16
03:07:24,770 --> 03:07:25,370
not a robot 

17
03:07:26,470 --> 03:07:30,810
the images presented to the user who is supposed to write the text he or

18
03:07:30,810 --> 03:07:33,990
she sees in the image into a text box 

19
03:07:33,990 --> 03:07:36,010
upon success the user is let in 

20
03:07:37,100 --> 03:07:41,900
to implement this one obviously needs a key value stored 

21
03:07:41,900 --> 03:07:44,500
where the key is the idea of the image 

22
03:07:44,500 --> 03:07:46,230
and the value is the desired text 

23
03:07:47,610 --> 03:07:52,900
now what if we wanted to use the image itself as the key 

24
03:07:52,900 --> 03:07:55,940
instead of an id number that you generate separately 

25
03:07:57,580 --> 03:07:59,160
the content of the image 

26
03:07:59,160 --> 03:08:04,400
which let say is a jpg file can be thought of as a binary string 

27
03:08:04,400 --> 03:08:05,706
but can it serve as a key then 

28
03:08:05,706 --> 03:08:09,050
according to the redis specification it can 

29
03:08:10,360 --> 03:08:13,050
the redis string can be binary and

30
03:08:13,050 --> 03:08:18,770
can have a size of up to 512 megabytes although its internal limit is higher 

31
03:08:19,840 --> 03:08:25,169
so small images like this can indeed be used as binary string keys 

32
03:08:27,040 --> 03:08:32,100
in some application scenarios keys may have an internal structure 

33
03:08:32,100 --> 03:08:36,090
for example product codes may have product family 

34
03:08:36,090 --> 03:08:41,040
manufacturing batch and the actual product id strung together into one id 

35
03:08:42,250 --> 03:08:46,710
the example shown here is a typical twitter style key to store the response

36
03:08:46,710 --> 03:08:48,500
to comment one two three four 

37
03:08:50,240 --> 03:08:53,980
how long do we want to keep the comment around 

38
03:08:53,980 --> 03:08:58,240
this is a standard big data issue when it comes to streaming data

39
03:08:58,240 --> 03:09:01,680
whose data values have limited utility beyond the certain period 

40
03:09:03,540 --> 03:09:05,000
one typically would not look for

41
03:09:05,000 --> 03:09:08,110
this response possibly three months after the conversation 

42
03:09:09,410 --> 03:09:13,490
one would certainly not like to keep such a value in memory for a long time 

43
03:09:13,490 --> 03:09:17,250
because the memory is being used as a cache for rapid access to current data 

44
03:09:18,510 --> 03:09:24,250
in fact redis has the ability to delete an expired key and

45
03:09:24,250 --> 03:09:27,230
can be made to call a function to generate a new key 

46
03:09:29,210 --> 03:09:32,580
an interesting side benefits to structured keys

47
03:09:32,580 --> 03:09:35,830
that it can encode a hierarchy to the structure 

48
03:09:35,830 --> 03:09:36,490
in the example 

49
03:09:36,490 --> 03:09:41,449
we show keys that represent increasingly finer subgroups of users 

50
03:09:43,290 --> 03:09:44,730
with this structure 

51
03:09:44,730 --> 03:09:49,433
a lookup on user commercial entertainment will also retrieve values

52
03:09:49,433 --> 03:09:53,390
from user commercial entertainment movie industry 

53
03:09:56,330 --> 03:10:02,390
a slightly more complex case occurs when the value is not atomic 

54
03:10:02,390 --> 03:10:07,300
but a collection object like a list which by definition is an ordered set 

55
03:10:08,720 --> 03:10:11,218
an example of such a list can come from twitter 

56
03:10:11,218 --> 03:10:15,760
that uses redis list to store timelines 

57
03:10:15,760 --> 03:10:18,860
now borrowed from a twitter presentation about their timeline architecture

58
03:10:19,920 --> 03:10:23,880
our timeline service can take a specific id and

59
03:10:23,880 --> 03:10:27,550
it quickly identifies all tweets of this user that are in the cache 

60
03:10:29,010 --> 03:10:32,310
these tweets are then populated by the content of the tweet 

61
03:10:32,310 --> 03:10:34,460
then returned as a result 

62
03:10:34,460 --> 03:10:39,000
this list can be long but the insertion and delete operations on the list

63
03:10:39,000 --> 03:10:41,970
can be performed in constant time which is in milliseconds 

64
03:10:43,220 --> 03:10:48,110
if a tweet is retweeted the ids of those tweets are also added

65
03:10:48,110 --> 03:10:51,820
to the list of the first tweet as you can see for the three cases in the figure 

66
03:10:54,099 --> 03:10:58,210
when the lists are long space saving becomes an important issue 

67
03:10:59,480 --> 03:11:03,370
so redis employs a method called ziplists which essentially

68
03:11:03,370 --> 03:11:07,700
compacts the size of the list in memory without changing the content 

69
03:11:07,700 --> 03:11:10,620
often producing significant reduction in memory used 

70
03:11:12,150 --> 03:11:14,890
of course while ziplists are very efficient for

71
03:11:14,890 --> 03:11:19,240
retrieval they are a little more complex for insertion and deletion operations 

72
03:11:21,100 --> 03:11:24,100
since redis is an open source system 

73
03:11:24,100 --> 03:11:28,070
twitter made a few innovations on the redis data structures 

74
03:11:28,070 --> 03:11:33,470
one of these innovations is that they created lists of ziplists 

75
03:11:33,470 --> 03:11:37,480
this gave them the flexibility of having constant timing insertions and

76
03:11:37,480 --> 03:11:41,970
deletions and at the same time used the compressed representation to save space 

77
03:11:43,880 --> 03:11:48,830
in 2012 the timeline service has about

78
03:11:48,830 --> 03:11:54,220
40 terabytes of main memory serving 30 million user queries per second 

79
03:11:54,220 --> 03:11:57,589
running on over 6 000 machines in one data center 

80
03:11:58,870 --> 03:12:04,280
for those interested i would like to point you to two wonderful twitter presentation

81
03:12:04,280 --> 03:12:08,200
explaining twitter use of redis among other design issue for

82
03:12:08,200 --> 03:12:09,790
a realtime data system like twitter 

83
03:12:11,740 --> 03:12:14,800
we also introduce links in the supplemental readings for this lesson 

84
03:12:18,843 --> 03:12:23,949
now the value to be looked up by the keys can actually be more complicated and

85
03:12:23,949 --> 03:12:28,260
can be records containing attribute value pairs themselves 

86
03:12:30,110 --> 03:12:31,939
redis values can be hashes

87
03:12:33,010 --> 03:12:37,970
which are essentially named containers of unique fields and their values 

88
03:12:39,010 --> 03:12:42,210
in the example the key std : 101 

89
03:12:42,210 --> 03:12:46,960
is associated with five attributed value pairs 

90
03:12:48,150 --> 03:12:51,330
the hashed attributes are stored very efficiently 

91
03:12:51,330 --> 03:12:56,570
and even when the list of attributed value pairs in a hash is really long 

92
03:12:56,570 --> 03:12:58,110
retrieval is efficient 

93
03:13:01,250 --> 03:13:03,240
horizontal scalability or

94
03:13:03,240 --> 03:13:07,710
scale out capabilities refers to the ability of a system

95
03:13:07,710 --> 03:13:12,170
to achieve scalability when the number of machines it operates on is increased 

96
03:13:13,720 --> 03:13:19,380
redis allows data partitioning through range partitioning and hash partitioning 

97
03:13:20,600 --> 03:13:26,110
rate partitioning takes a numeric key and breaks up the range of keys into bins 

98
03:13:26,110 --> 03:13:31,610
in this case by bins of 10 000 each of bin is assigned a machine 

99
03:13:33,730 --> 03:13:37,940
and ultimately a partitioning is where computing a hashing function on a key 

100
03:13:39,060 --> 03:13:41,360
suppose we have 10 machines 

101
03:13:41,360 --> 03:13:44,690
we pick the key and use the hash function to get back a number 

102
03:13:46,050 --> 03:13:50,560
we represent the number modular 10 and the result in this case 

103
03:13:50,560 --> 03:13:53,920
2 is the machine to which the record will be allocated 

104
03:13:56,979 --> 03:14:01,110
replication is accomplished in redis through master - slave mode 

105
03:14:03,350 --> 03:14:07,110
the slaves have a copy of the master node 

106
03:14:07,110 --> 03:14:08,940
and can serve read queries 

107
03:14:11,090 --> 03:14:13,850
clients write to the master node 

108
03:14:13,850 --> 03:14:16,280
and master node replicates to the slaves 

109
03:14:18,310 --> 03:14:21,090
clients read from the slaves to scale up the read performance 

110
03:14:22,190 --> 03:14:24,760
the replication processes are synchronous 

111
03:14:24,760 --> 03:14:29,040
that is that slaves do not get replicated data it locks them with each other 

112
03:14:30,360 --> 03:14:33,540
however the replication process does ensure

113
03:14:33,540 --> 03:14:35,120
that they are consistent with each other 

1
06:20:49,310 --> 06:20:52,510
the next system we will explore is vertica 

2
06:20:52,510 --> 06:20:56,810
which is the relational dbms designed to operate on top of htfs 

3
06:20:58,450 --> 06:21:01,960
it belongs to a family of dbms architectures called column stores 

4
06:21:03,320 --> 06:21:06,410
other products in the same family are ucdv 

5
06:21:06,410 --> 06:21:09,540
carrot cell xvelocity from microsoft and so forth 

6
06:21:11,070 --> 06:21:13,620
the primary difference between a row store and

7
06:21:13,620 --> 06:21:16,010
a column store is shown in the diagram here 

8
06:21:17,160 --> 06:21:20,094
logically this table has five columns 

9
06:21:20,094 --> 06:21:23,935
emp number department number 

10
06:21:23,935 --> 06:21:26,810
hire date employee last name and employee first name 

11
06:21:28,940 --> 06:21:33,760
in a row oriented design the database internally organizes the record

12
06:21:33,760 --> 06:21:34,300
two four by two 

13
06:21:36,390 --> 06:21:40,030
in a column store the data is organized column wise 

14
06:21:41,500 --> 06:21:46,100
so the nth number column is stored separately from the department id

15
06:21:46,100 --> 06:21:47,280
column and so forth 

16
06:21:48,740 --> 06:21:51,730
now suppose a query needs to find the id and

17
06:21:51,730 --> 06:21:57,100
department id of all employees who were hired after first of january 2001 

18
06:21:57,100 --> 06:22:00,990
the system only needs to look up

19
06:22:00,990 --> 06:22:04,720
the hire date column to figure out which records qualify 

20
06:22:04,720 --> 06:22:06,670
and then pick up the values of the id 

21
06:22:06,670 --> 06:22:10,450
and the department of id columns for the qualifying records 

22
06:22:10,450 --> 06:22:14,060
the other columns are not touched 

23
06:22:14,060 --> 06:22:19,270
so if a table has 30 to 50 columns very often only a few of them are needed for

24
06:22:19,270 --> 06:22:19,950
any single query 

25
06:22:21,730 --> 06:22:28,440
so for tables with 500 million rows and 40 columns a typical query is very fast 

26
06:22:28,440 --> 06:22:33,400
and uses much less memory because a full record is not used most of the time 

27
06:22:34,990 --> 06:22:39,780
my own experience is that with an application that needed a database

28
06:22:39,780 --> 06:22:41,830
with 150 billion tuples in a table 

29
06:22:42,870 --> 06:22:46,860
accounting operation took a little under three minutes to complete 

30
06:22:48,330 --> 06:22:51,890
a second advantage of the column store comes from the nature of the data 

31
06:22:53,350 --> 06:22:57,220
in a column store data in every column is sorted 

32
06:22:58,400 --> 06:23:01,900
the figure on the right shows three sorted columns of a table

33
06:23:01,900 --> 06:23:03,110
with three visible columns 

34
06:23:04,550 --> 06:23:06,940
the bottom of the first column shown here

35
06:23:06,940 --> 06:23:10,670
has many accurate sixteen transactions on the same day 

36
06:23:12,340 --> 06:23:17,170
the second column has customer ids which can be numerically close to each other 

37
06:23:17,170 --> 06:23:19,930
they are all within seventy values of the first customer 

38
06:23:21,430 --> 06:23:26,020
so in the first case we can just say that the value is one one 

39
06:23:26,020 --> 06:23:29,730
2007 and the next 16 records have this value 

40
06:23:30,800 --> 06:23:36,580
that means we do not have to store the next 16 values thus saving space 

41
06:23:38,770 --> 06:23:42,280
now this form of shortened representation is called compression 

42
06:23:43,300 --> 06:23:48,070
and this specific variety is called run - length encoding or rle 

43
06:23:49,760 --> 06:23:53,010
another form of encoding can be seen in the second column 

44
06:23:55,030 --> 06:23:58,990
here the customer ids a long integer but for

45
06:23:58,990 --> 06:24:02,460
the records shown there are nearby numbers 

46
06:24:02,460 --> 06:24:06,420
so if we pick a value in the column and just put the difference between

47
06:24:06,420 --> 06:24:10,340
this value and other values the difference will be small 

48
06:24:10,340 --> 06:24:13,380
it means we will need fewer bites to represent them 

49
06:24:15,330 --> 06:24:18,800
this one of compression is called frame - of - reference encoding 

50
06:24:20,440 --> 06:24:24,520
the lesson to remember here is that compressed data presentation

51
06:24:24,520 --> 06:24:29,760
can significantly reduce the total size of a database and if bdms 

52
06:24:29,760 --> 06:24:32,970
should use all such tricks to improve space efficiency 

53
06:24:34,730 --> 06:24:38,020
while the space efficiency and performance of vertica is impressive for

54
06:24:38,020 --> 06:24:42,490
large data one has to be careful about how to design a system with it 

55
06:24:43,620 --> 06:24:45,570
like google web table and

56
06:24:45,570 --> 06:24:50,844
apache cassandra vertica allows the declaration of column - groups 

57
06:24:52,180 --> 06:24:53,830
these are columns like first name and

58
06:24:53,830 --> 06:24:56,730
last name which are very often accessed together 

59
06:24:57,980 --> 06:25:01,040
in vertica a column group is like a mini table

60
06:25:01,040 --> 06:25:03,640
which is treated like a little row storied in the system 

61
06:25:04,750 --> 06:25:10,070
but because these groups represent activists that are frequently co accessed

62
06:25:10,070 --> 06:25:12,500
the grouping actually improves performance 

63
06:25:13,560 --> 06:25:17,660
so an application developer is better off when the nature and

64
06:25:17,660 --> 06:25:21,150
the frequency of user queries are known to some degree 

65
06:25:21,150 --> 06:25:25,190
and this knowledge is applies to designing the column groups 

66
06:25:25,190 --> 06:25:29,200
an important side effect of column structure organization of vertical

67
06:25:29,200 --> 06:25:32,390
is that the writing of data into vertica is a little slower 

68
06:25:33,590 --> 06:25:38,110
when rows are added to a table vertica initially places

69
06:25:38,110 --> 06:25:41,750
them in a row - wise data structure and

70
06:25:41,750 --> 06:25:46,029
then converts them into a column - wise data structure which is then compressed 

71
06:25:47,370 --> 06:25:50,920
this lowness can be perceptible for large uploads or updates 

72
06:25:52,140 --> 06:25:56,050
vertica belongs to a new breed of database systems that call themselves

73
06:25:56,050 --> 06:25:57,220
analytical databases 

74
06:25:58,620 --> 06:26:02,780
this means two slightly different things 

75
06:26:02,780 --> 06:26:06,110
first vertica offers many more statistical functions than in

76
06:26:06,110 --> 06:26:06,950
classical dbms 

77
06:26:08,140 --> 06:26:12,500
for example one can perform operations over a window

78
06:26:13,728 --> 06:26:20,150
to see an example consider a table of stock ticks having a time attribute 

79
06:26:20,150 --> 06:26:23,520
a stock name and a value of the stock called bid here 

80
06:26:24,570 --> 06:26:29,610
this shows that data values in the table now we would like to compute

81
06:26:29,610 --> 06:26:33,220
a moving average of the bit every 40 seconds 

82
06:26:34,510 --> 06:26:36,770
we show the query in a frame below 

83
06:26:36,770 --> 06:26:41,450
now we are not going into the details of the query just consider the blue part 

84
06:26:41,450 --> 06:26:45,930
it says the average which is the avg function in yellow 

85
06:26:45,930 --> 06:26:48,760
it must be computed on the last column which is bit 

86
06:26:49,920 --> 06:26:51,060
but this computation

87
06:26:52,200 --> 06:26:56,270
must be over the range that is computed on the timestamp call 

88
06:26:57,740 --> 06:27:03,877
so the range is defined by a 40 second row before that current row so 

89
06:27:03,877 --> 06:27:09,404
here the computation of the average advances for the stock abc 

90
06:27:09,404 --> 06:27:14,293
and for each computation the system only considers the rows whose

91
06:27:14,293 --> 06:27:18,510
timestamp is within 40 seconds before the current row 

92
06:27:19,840 --> 06:27:23,630
the table on the right shows the result of the query 

93
06:27:23,630 --> 06:27:26,130
the average value 10 12 

94
06:27:26,130 --> 06:27:29,910
is the same as the actual value because there are no other rows within 40 seconds 

95
06:27:29,910 --> 06:27:33,963
the next two result rows average over the preceding rows 

96
06:27:33,963 --> 06:27:37,530
was times r within 40 seconds of the current row 

97
06:27:38,650 --> 06:27:39,670
when we get to the blue row 

98
06:27:39,670 --> 06:27:46,020
that we notice that it occurs 1 minute 16 seconds after the previous row 

99
06:27:46,020 --> 06:27:48,950
so we cannot consider the previous sort in the computation 

100
06:27:48,950 --> 06:27:52,660
instead the result is just the value of the bid in the current row 

101
06:27:54,190 --> 06:27:58,180
the takeaway from this example is that analytical computations like

102
06:27:58,180 --> 06:28:02,260
this are happening inside the database and not in an external application 

103
06:28:03,880 --> 06:28:07,850
this brings us to the second feature of vertica as an analytical database 

104
06:28:09,200 --> 06:28:13,710
r is a well known free statistics package that used by statisticians 

105
06:28:13,710 --> 06:28:16,050
data minors predictive analytics experts 

106
06:28:16,050 --> 06:28:21,840
today r can not only read data from files 

107
06:28:21,840 --> 06:28:25,660
but it can go to an sql database and grab data to perform statistical analysis 

108
06:28:27,300 --> 06:28:30,160
over time r has evolved and

109
06:28:30,160 --> 06:28:35,830
given rights to distributed r which is a high performance platform for r 

110
06:28:37,430 --> 06:28:39,810
as expected in this distributed setting 

111
06:28:39,810 --> 06:28:43,050
the system operates in a master slave mode 

112
06:28:43,050 --> 06:28:47,650
the master node coordinates computations by sending commands to the workers 

113
06:28:47,650 --> 06:28:50,520
the worker nodes maintain data partitions and

114
06:28:50,520 --> 06:28:53,470
apply the computation functions to the data 

115
06:28:53,470 --> 06:28:57,070
just getting the data parallelly 

116
06:28:57,070 --> 06:29:00,310
the essential data structure is a distributed array 

117
06:29:00,310 --> 06:29:03,266
that is an array that is partitioned as shown here 

118
06:29:03,266 --> 06:29:06,480
now in this diagram the partitions are equal but

119
06:29:06,480 --> 06:29:08,960
in practice they may be all different sizes 

120
06:29:08,960 --> 06:29:14,320
on which one can compute a function for each of these mini - arrays 

121
06:29:14,320 --> 06:29:18,200
the bottom diagram shows a simple work flow of constructing and

122
06:29:18,200 --> 06:29:19,690
deploying a predictive model 

123
06:29:20,730 --> 06:29:22,460
the role of vertica here 

124
06:29:22,460 --> 06:29:26,620
is that it a data supplier to the worker nodes of r and a model consumer 

125
06:29:27,880 --> 06:29:31,730
the data to be analyzed is the output of the vertica query 

126
06:29:31,730 --> 06:29:36,010
which is transferred in memory through a protocol called vertica fast transfer

127
06:29:36,010 --> 06:29:37,780
through distributed r as a darray 

128
06:29:38,940 --> 06:29:40,120
when the model is created in r 

129
06:29:40,120 --> 06:29:44,900
it should come back as a code that goes through vertica as a function 

130
06:29:46,070 --> 06:29:48,870
this function can be called from inside vertica

131
06:29:48,870 --> 06:29:51,310
as if it was a user defined function 

132
06:29:52,330 --> 06:29:55,690
now in sophisticated applications the features of the data needed for

133
06:29:55,690 --> 06:30:01,670
predicted modeling will also be computed inside the dbms possibly

134
06:30:01,670 --> 06:30:04,389
using the new analytical operations of vertica that we have just shown 

135
06:30:05,770 --> 06:30:09,190
now this will make future computation much faster and

136
06:30:09,190 --> 06:30:12,930
improve the efficiency of the entire analytics process 

137
06:30:14,240 --> 06:30:19,920
going forward we believe that most dbms will want to play in the analytics field 

138
06:30:19,920 --> 06:30:21,290
will support similar functions 

1
12:51:11,390 --> 12:51:15,010
most of you have heard of mongodb as a dominant store for

2
12:51:15,010 --> 12:51:16,770
json style semi - structured data 

3
12:51:18,070 --> 12:51:20,200
mongodb is very popular and

4
12:51:20,200 --> 12:51:22,860
there are a number of excellent tutorials on it on the web 

5
12:51:24,460 --> 12:51:29,290
in this module we would like to discuss a relatively new big data management

6
12:51:29,290 --> 12:51:34,910
system for semistructured data that currently being incubated by apache 

7
12:51:34,910 --> 12:51:36,040
it called asterixdb 

8
12:51:37,260 --> 12:51:41,500
originally asterixdb was conceived by the university of california irvine 

9
12:51:42,910 --> 12:51:45,800
since it is a full fledged dbms 

10
12:51:45,800 --> 12:51:51,310
it provides acid guarantees to understand the basic design of asterixdb 

11
12:51:52,330 --> 12:51:57,140
let consider this incomplete json snippet taken from an actual tweet 

12
12:51:59,120 --> 12:52:00,900
we have seen the structure of json before 

13
12:52:02,000 --> 12:52:08,090
here we point out that entities and user the two parts

14
12:52:08,090 --> 12:52:12,910
in blue are nested that means embedded within the structure of the tweet 

15
12:52:14,640 --> 12:52:19,540
if we represent a part of the schema of this abbreviated structure in asterixdb 

16
12:52:21,010 --> 12:52:22,270
it will look like this 

17
12:52:23,890 --> 12:52:26,950
here a dataverse is like a name space for data 

18
12:52:28,610 --> 12:52:31,860
data is declared in terms of data types 

19
12:52:31,860 --> 12:52:37,030
the top type which looks like a standard data with stable declaration 

20
12:52:37,030 --> 12:52:41,030
represents the user portion of the json object that we highlighted before 

21
12:52:42,130 --> 12:52:44,520
the type below represents the message 

22
12:52:45,750 --> 12:52:48,820
now instead of nesting it like json 

23
12:52:48,820 --> 12:52:53,520
the user attribute highlighted in blue is declared to have the type

24
12:52:53,520 --> 12:52:58,240
twitterusertype thus it captures the hierarchical structure of json 

25
12:53:00,450 --> 12:53:04,560
we should also notice that the first type is declared as open 

26
12:53:06,160 --> 12:53:11,100
it means that the actual data can have more attributes than specified here 

27
12:53:12,640 --> 12:53:17,900
in contrast the tweetmessage type is declared as closed 

28
12:53:17,900 --> 12:53:22,130
meaning that the data instance must have the same attributes as in the schema 

29
12:53:24,020 --> 12:53:28,890
asterixdb can handle spatial data as given by the point data types shown in green 

30
12:53:30,380 --> 12:53:36,120
the question mark at the end of the point type says that this attribute is optional 

31
12:53:36,120 --> 12:53:38,960
that means all instances need not have it 

32
12:53:40,780 --> 12:53:46,810
finally the create dataset actually asks the system to create a dataset

33
12:53:46,810 --> 12:53:52,010
called tweetmessages whose type is the just declared quick message type 

34
12:53:53,730 --> 12:53:59,060
astrerixdb which runs on hdfs provides several options for credit support 

35
12:54:02,350 --> 12:54:07,440
first it has its own query language called the asterix query language

36
12:54:07,440 --> 12:54:10,450
which resembles the xml credit language query 

37
12:54:12,000 --> 12:54:14,340
the details of this query language are not important right now 

38
12:54:15,720 --> 12:54:20,150
we are illustrating the structure of a query just to show what it looks like 

39
12:54:21,330 --> 12:54:24,040
this particular query asks for

40
12:54:24,040 --> 12:54:29,020
all user objects from the dataset twitterusers in descending order of their

41
12:54:29,020 --> 12:54:34,010
follower count and in alphabetical order of the user preferred language 

42
12:54:35,740 --> 12:54:39,650
what is more interesting and distinctive is that asterixdb has a creative

43
12:54:39,650 --> 12:54:45,409
processing engine that can process queries in multiple languages 

44
12:54:46,750 --> 12:54:50,680
for its supported language they have developed a way to

45
12:54:50,680 --> 12:54:55,550
transfer the query into a set of low level operations like select and

46
12:54:55,550 --> 12:54:58,895
join which their query exchange can support 

47
12:54:58,895 --> 12:55:03,520
further they have determined how a record described in one of these

48
12:55:03,520 --> 12:55:07,280
languages can be transformed into an asterix 

49
12:55:07,280 --> 12:55:11,449
in this manner the support hive queries 

50
12:55:11,449 --> 12:55:15,410
which is expressed in like this 

51
12:55:15,410 --> 12:55:20,630
xquery hadoop map reduce as wall as a new language

52
12:55:20,630 --> 12:55:24,930
called sql + + which extends sql for json 

53
12:55:28,107 --> 12:55:29,781
like a typical db bdms 

54
12:55:29,781 --> 12:55:34,660
asterixdb is designed to operate on a cluster of machines 

55
12:55:34,660 --> 12:55:40,395
the basic idea not surprisingly is to use partition data parallellism 

56
12:55:40,395 --> 12:55:44,415
each data set is divided into instances of various types

57
12:55:44,415 --> 12:55:48,685
which can be decomposed to different machines by either range partitioning or

58
12:55:48,685 --> 12:55:50,565
hash partitioning like we discussed earlier 

59
12:55:52,572 --> 12:55:56,292
a runtime distributed execution engine called hyracks is used for

60
12:55:56,292 --> 12:56:00,172
partitioned parallel execution of query plans 

61
12:56:00,172 --> 12:56:03,762
for example let assume we have two relations customers and orders 

62
12:56:03,762 --> 12:56:04,522
as you can see here 

63
12:56:06,402 --> 12:56:09,249
our query is find the number of orders for

64
12:56:09,249 --> 12:56:12,946
every market segment that the customers belong to 

65
12:56:14,526 --> 12:56:19,936
now this query need a join operation between the two relations 

66
12:56:19,936 --> 12:56:25,260
using the o custkey as a foreign key of customer into orders 

67
12:56:26,450 --> 12:56:30,090
it also needs a grouping operation which for

68
12:56:30,090 --> 12:56:34,330
each market segment will pull together all the orders which will then be counted 

69
12:56:35,530 --> 12:56:38,960
you do not have to understand the details of this diagram at this point 

70
12:56:38,960 --> 12:56:42,580
we just want to point out that the different parts of the query

71
12:56:42,580 --> 12:56:47,180
that are being marked the customer filed here has two partitions

72
12:56:47,180 --> 12:56:50,760
that reside on two nodes nc one and nc two respectively 

73
12:56:52,300 --> 12:56:56,200
the orders file also has two partitions 

74
12:56:56,200 --> 12:56:58,787
but each partition is dually replicated 

75
12:57:00,567 --> 12:57:07,660
one can be accessed either of nodes nc3 or nc2 and the other on nc1 and nc5 

76
12:57:10,540 --> 12:57:15,970
hyracks will also break up the query into a number of jobs and

77
12:57:15,970 --> 12:57:19,740
then fill it out which tasks can be performed in parallel and

78
12:57:19,740 --> 12:57:22,020
which ones must be executed stage by stage 

79
12:57:23,220 --> 12:57:26,650
this whole thing will be managed by the cluster controller 

80
12:57:28,080 --> 12:57:31,930
the cluster controller is also responsible for replanning and

81
12:57:31,930 --> 12:57:36,050
reexecuting of a job if there is a failure 

82
12:57:36,050 --> 12:57:41,300
asteriskdb also has the provision

83
12:57:41,300 --> 12:57:44,990
to accept real time data from external data sources at multiple rates 

84
12:57:46,610 --> 12:57:49,160
one way is from files in a directory path 

85
12:57:50,400 --> 12:57:52,040
consider the example of tweets 

86
12:57:53,040 --> 12:57:55,480
as you have seen with the hands - on demo 

87
12:57:55,480 --> 12:58:01,350
usually people acquire tweets by accessing data through an api that twitter provides 

88
12:58:01,350 --> 12:58:04,590
very typically a certain volume of tweets lets say for

89
12:58:04,590 --> 12:58:09,010
every 5 minutes is accumulated into a json file in a specific directory 

90
12:58:09,010 --> 12:58:11,861
the next 5 minutes in another json file and so forth 

91
12:58:13,891 --> 12:58:16,205
the way to get this data into asterisks db 

92
12:58:16,205 --> 12:58:19,240
is to first create an empty data set called tweets here 

93
12:58:20,490 --> 12:58:22,420
the next task is to create a feed 

94
12:58:23,470 --> 12:58:25,830
that is an externally resource 

95
12:58:25,830 --> 12:58:30,450
one has to specify that it coming from the local file system called local fs here

96
12:58:31,640 --> 12:58:34,390
and the location of the directory the format and

97
12:58:34,390 --> 12:58:36,850
the data type it going to copy it 

98
12:58:36,850 --> 12:58:40,100
next the feed is connected to the data set and

99
12:58:40,100 --> 12:58:42,770
the system starts reading unread files from the directory 

100
12:58:44,960 --> 12:58:50,170
another way for asteriskdb to access external data is directly from an api 

101
12:58:50,170 --> 12:58:52,020
such as the twitter api 

102
12:58:52,020 --> 12:58:56,750
to do this one would create a dataset as before 

103
12:58:56,750 --> 12:59:00,460
but this time the data feed is not on the local file system 

104
12:59:01,580 --> 12:59:06,960
instead it uses the push twitter method which invokes the twitter

105
12:59:06,960 --> 12:59:10,830
client with the four authentication parameters required by the api 

106
12:59:12,360 --> 12:59:15,700
once the feed is defined it is connected to the data set as before 

1
01:50:26,986 --> 01:50:29,186
now we move to another apache product for

2
01:50:29,186 --> 01:50:31,720
large scale text data searching called solr 

3
01:50:33,100 --> 01:50:34,430
systems like solr and

4
01:50:34,430 --> 01:50:40,300
its underlying text indexing engine are typically designed for search problems 

5
01:50:40,300 --> 01:50:42,629
so they would typically be part of a search engine 

6
01:50:43,860 --> 01:50:45,410
but before we talk about solr or

7
01:50:45,410 --> 01:50:50,770
large scale text we need to first appreciate some fundamental challenges

8
01:50:50,770 --> 01:50:53,590
when it comes to storing indexing and matching text data 

9
01:50:55,690 --> 01:51:00,030
the basic challenge comes from the numerous ways in which

10
01:51:00,030 --> 01:51:04,660
a text string may vary making it hard to define what a good match is 

11
01:51:05,830 --> 01:51:07,560
let us show some of these challenges 

12
01:51:08,750 --> 01:51:13,810
remember in each case we are asking whether the strings shown on

13
01:51:13,810 --> 01:51:16,560
either side of the double tilde sign should match 

14
01:51:19,100 --> 01:51:22,541
the first issue is about spelling variations and capitalization 

15
01:51:26,049 --> 01:51:29,600
the second issue relates structured strings 

16
01:51:29,600 --> 01:51:34,080
where different parts of a string represent different kind of information 

17
01:51:34,080 --> 01:51:40,420
we have seen this before for file paths urls and like in this case product ids 

18
01:51:40,420 --> 01:51:45,081
the problem is that the searcher may not always know the structure or

19
01:51:45,081 --> 01:51:48,775
my job on misposition the internal punctuations 

20
01:51:52,014 --> 01:51:56,674
the next problem is very common with proper nouns which are represented in

21
01:51:56,674 --> 01:52:02,248
various ways including dropping a part of the string picking up only the initials 

22
01:52:02,248 --> 01:52:07,497
but consider the last variation should bh obama really match the full name 

23
01:52:10,797 --> 01:52:14,120
the next example is about frequently used synonyms 

24
01:52:15,210 --> 01:52:19,512
if the document has one of the synonyms while the query uses another 

25
01:52:19,512 --> 01:52:20,742
should they match 

26
01:52:23,679 --> 01:52:27,400
example 5 illustrates a very common problem 

27
01:52:27,400 --> 01:52:29,710
people use abbreviations all the time 

28
01:52:30,970 --> 01:52:33,160
if we look at text and social media and

29
01:52:33,160 --> 01:52:36,930
instant messaging we see a much wider variety of abbreviations 

30
01:52:38,270 --> 01:52:41,270
how well should they met with the real correct version of the term 

31
01:52:43,580 --> 01:52:46,510
now problem six is a special case of problem five 

32
01:52:48,350 --> 01:52:50,690
many long nouns are shortened

33
01:52:50,690 --> 01:52:54,440
because we take the first initial letter of each significant word 

34
01:52:55,570 --> 01:53:01,580
we say significant because we drop words like of as shown here 

35
01:53:01,580 --> 01:53:02,870
this is called initialism 

36
01:53:04,390 --> 01:53:08,490
just so you know when an initialism can be said like a real word 

37
01:53:08,490 --> 01:53:09,480
it called an acronym 

38
01:53:10,480 --> 01:53:15,620
thus ibm is an initialization but nato is an acronym 

39
01:53:17,160 --> 01:53:22,250
now problem seven and problem eight show two

40
01:53:22,250 --> 01:53:26,690
different situation where we first must decide what to do with the period sign 

41
01:53:28,100 --> 01:53:33,322
in the first case we should not find a match because students and

42
01:53:33,322 --> 01:53:37,250
american are in two different sentences punctuated by a period sign 

43
01:53:38,720 --> 01:53:42,330
but in the second case we should find a match because the period

44
01:53:42,330 --> 01:53:44,310
does not designate a sentence boundary 

45
01:53:47,055 --> 01:53:52,510
lucene the engine on which solr is built is effectively not a database 

46
01:53:52,510 --> 01:53:54,030
but a modern inverted index 

47
01:53:55,040 --> 01:53:56,020
what is an inverted index 

48
01:53:57,620 --> 01:54:01,590
let first define a vocabulary as a collection of terms 

49
01:54:01,590 --> 01:54:06,090
where a term may be a single word or it can be multiple words 

50
01:54:06,090 --> 01:54:09,030
it can a single term or it can be a collection of synonyms 

51
01:54:10,510 --> 01:54:12,730
but how would a search engine know what a term is 

52
01:54:13,795 --> 01:54:16,558
we will revisit this question in a couple of slides 

53
01:54:16,558 --> 01:54:21,390
for now let just say that if we have a corpus of documents we can

54
01:54:21,390 --> 01:54:25,690
extract most of the terms and construct a vocabulary for that collection 

55
01:54:27,500 --> 01:54:32,080
we can then define occurrence as a list

56
01:54:32,080 --> 01:54:36,270
containing all the information necessary for each term in the vocabulary 

57
01:54:37,780 --> 01:54:41,790
it would include information like which documents have the term 

58
01:54:41,790 --> 01:54:45,050
the positions in the document where the term occurs 

59
01:54:45,050 --> 01:54:49,404
we can then go on to compute the count of the term in the document and the corpus 

60
01:54:50,540 --> 01:54:54,760
referring back to a previous module in this course we can also compute

61
01:54:54,760 --> 01:54:58,370
the term frequency and inverse document frequency for the collection 

62
01:55:00,470 --> 01:55:04,250
an inverted index is essentially an index which for

63
01:55:04,250 --> 01:55:09,850
every term stores at least the id of the document where the term occurs 

64
01:55:09,850 --> 01:55:12,440
practically other computed numbers or

65
01:55:12,440 --> 01:55:16,060
properties associated with the terms will also be included in the index 

66
01:55:18,570 --> 01:55:22,820
solr is an open source enterprise search platform 

67
01:55:22,820 --> 01:55:28,190
the heart of solr is its search functionality built for full text search 

68
01:55:28,190 --> 01:55:31,110
however solr provides much more than tech search 

69
01:55:32,620 --> 01:55:38,780
it can take any structured document even csv files

70
01:55:38,780 --> 01:55:43,490
which can be broken up into fields and can index each field separately 

71
01:55:44,850 --> 01:55:48,640
full text indexes where text columns are supplemented by indexes for

72
01:55:48,640 --> 01:55:53,735
other types of data including numeric data dates geographic coordinates and

73
01:55:53,735 --> 01:55:57,390
fields where domains are limited to an emergent set of values 

74
01:55:59,190 --> 01:56:04,230
solr provides other facilities like faceted search and

75
01:56:04,230 --> 01:56:07,440
highlighting of terms that match a query 

76
01:56:07,440 --> 01:56:10,970
now if you are not familiar with the term faceted search 

77
01:56:10,970 --> 01:56:14,550
let look at the screenshot from amazon com 

78
01:56:14,550 --> 01:56:16,960
i performed a search on the string dell laptop 

79
01:56:18,260 --> 01:56:20,680
consider the highlighted part of the image 

80
01:56:22,400 --> 01:56:26,230
each laptop record carries a lot of attributes like the display size 

81
01:56:26,230 --> 01:56:28,499
the processor speed the amount of memory and so forth 

82
01:56:29,680 --> 01:56:34,480
these attributes can be put into builds like processor type has intel i5 i7 etc 

83
01:56:35,950 --> 01:56:41,098
faceted search essentially extracts the individual values of these fields and

84
01:56:41,098 --> 01:56:46,188
displays them back to the user usually with a count of the number of records 

85
01:56:46,188 --> 01:56:50,466
we see this in the upper part of the marked portion 

86
01:56:50,466 --> 01:56:55,452
which says there are 5619 laptops per 411 tablets 

87
01:56:55,452 --> 01:56:56,600
these are called facets 

88
01:56:57,750 --> 01:57:01,810
if a user clicks on a facet documents with only those values 

89
01:57:01,810 --> 01:57:04,250
that just the tablets will be presented back to the user 

90
01:57:06,380 --> 01:57:09,860
now let get back to the question of what a term is and

91
01:57:09,860 --> 01:57:11,550
how a solr system should know it 

92
01:57:13,870 --> 01:57:19,730
solr allows the system designer to specify how to parse a document 

93
01:57:19,730 --> 01:57:25,770
by instructing how to tokenize a document and how to filter it 

94
01:57:25,770 --> 01:57:30,720
tokenization is the process of breaking down the characters read 

95
01:57:30,720 --> 01:57:34,540
for example one can break the stream at white spaces and

96
01:57:34,540 --> 01:57:37,170
get all the words as tokens 

97
01:57:37,170 --> 01:57:41,803
then it can filter out the punctuation like the period the apostrophe and so

98
01:57:41,803 --> 01:57:43,631
on just to get the pure words 

99
01:57:45,984 --> 01:57:48,410
the code snippet on the right essentially achieves this 

100
01:57:49,510 --> 01:57:53,960
it uses a standard tokenizer that gets the words with immediate punctuation 

101
01:57:55,100 --> 01:57:58,730
the first filter removes the punctuations 

102
01:57:58,730 --> 01:58:02,040
the second filter turns everything into lowercase 

103
01:58:02,040 --> 01:58:05,960
and the third filter uses a synonym file to ensure that

104
01:58:05,960 --> 01:58:10,140
all the synonyms get the same token after ignoring the case 

105
01:58:10,140 --> 01:58:14,828
the last filter removes common english words like a and the 

106
01:58:16,401 --> 01:58:20,330
while a similar process would need to happen when we get the query string 

107
01:58:22,080 --> 01:58:25,245
it will also need to go through a tokenization and token filtering process 

108
01:58:27,933 --> 01:58:31,918
in the query analyzer example there are six filters within the tokenizer 

109
01:58:33,060 --> 01:58:36,130
we use a pattern tokenizer which will remove the white spaces and

110
01:58:36,130 --> 01:58:39,020
periods and semi - colon 

111
01:58:39,020 --> 01:58:43,710
the common grams filter creates tokens out of pairs of terms and

112
01:58:43,710 --> 01:58:47,470
in doing so makes sure that words in the stopword file are used 

113
01:58:47,470 --> 01:58:48,750
so if we have the string 

114
01:58:48,750 --> 01:58:53,560
the cat the term the should not be ignored in this filter 

115
01:58:55,240 --> 01:58:58,520
now the filters here are executed in order 

116
01:59:00,180 --> 01:59:03,720
after the common grams filter is already done 

117
01:59:03,720 --> 01:59:06,500
the next filter removes all the stopwords in the file 

118
01:59:08,320 --> 01:59:13,190
the fifth filter makes sure that if the queries coming from a web form 

119
01:59:13,190 --> 01:59:15,240
all the html characters are stripped off 

120
01:59:16,490 --> 01:59:21,360
finally the remaining words are stemmed so that runs and

121
01:59:21,360 --> 01:59:25,440
running in the query would match the word run in the document 

122
01:59:28,170 --> 01:59:32,935
we will end this section with a discussion on solr queries that is searches 

123
01:59:36,370 --> 01:59:40,450
we present a csv file with nine records and seven attributes 

124
01:59:42,280 --> 01:59:47,900
we issue queries against the system by posing a web query 

125
01:59:47,900 --> 01:59:51,850
in these examples we show some of the queries one can post to the system 

126
01:59:53,060 --> 01:59:56,187
this will be covered in more detail during the hands - on session 

127
01:59:57,220 --> 02:00:03,200
just notice that the q equal to is a query and the fl is what you want to back 

1
03:50:28,230 --> 03:50:33,440
listens on activity who will be performing queries assessing the postgres database 

2
03:50:33,440 --> 03:50:37,650
first will open a terminal window and start the postgres shell 

3
03:50:37,650 --> 03:50:41,430
next we will look at table and column definitions in database 

4
03:50:42,670 --> 03:50:46,000
who s in query the content of the buy - clicks table 

5
03:50:46,000 --> 03:50:51,170
and see how to do this query but filter specific rows and columns from the table 

6
03:50:51,170 --> 03:50:52,950
next we will perform average and

7
03:50:52,950 --> 03:50:55,780
some aggregation operations on a specific column 

8
03:50:57,040 --> 03:50:57,590
and finally 

9
03:50:57,590 --> 03:51:00,960
we will see how to combine two tables by joining them on a single column 

10
03:51:02,900 --> 03:51:04,130
let begin 

11
03:51:04,130 --> 03:51:07,300
first click on the terminal icon at the top of the toolbar

12
03:51:07,300 --> 03:51:08,390
to open a terminal window 

13
03:51:10,930 --> 03:51:15,347
next let start the postgres shell by running psql 

14
03:51:18,821 --> 03:51:23,410
the postgres shell allows us to enter queries and run commands for

15
03:51:23,410 --> 03:51:25,180
the postgres database 

16
03:51:25,180 --> 03:51:29,028
we can see what tables are in the database by running d 

17
03:51:33,453 --> 03:51:38,350
this says that there are three tables in the database 

18
03:51:38,350 --> 03:51:41,980
adclicks buyclicks and gameclicks 

19
03:51:41,980 --> 03:51:46,689
we could use d table name to see the definition of one of these tables 

20
03:51:47,830 --> 03:51:53,786
let look at the definition of buyclicks 

21
03:51:53,786 --> 03:51:57,084
we enter d buyclicks 

22
03:51:59,395 --> 03:52:02,090
this shows that there seven columns in the database 

23
03:52:03,400 --> 03:52:08,490
these are the column names and here the data type for each of the columns 

24
03:52:11,240 --> 03:52:13,630
now let look at the contents of the buyclicks table 

25
03:52:15,310 --> 03:52:23,718
we can query the contents by running the command select from buyclicks; 

26
03:52:23,718 --> 03:52:27,050
the select says that we want to do a query 

27
03:52:27,050 --> 03:52:30,730
the star means we want to retrieve all the columns and

28
03:52:30,730 --> 03:52:34,450
from buyclicks says which table to perform the query from 

29
03:52:35,660 --> 03:52:41,226
and finally all commands in postgres shell need to end with a semicolon 

30
03:52:41,226 --> 03:52:46,299
when we run this command we see the contents of the buyclicks table 

31
03:52:48,120 --> 03:52:51,760
column header is at the top and the contents are below 

32
03:52:53,530 --> 03:52:58,609
again hit space to scroll through the contents and hit q when we are done 

33
03:53:01,341 --> 03:53:04,250
now let view the contents of only two of the columns 

34
03:53:05,370 --> 03:53:09,889
let query only the price and the user id from the buyclicks table 

35
03:53:09,889 --> 03:53:16,180
to do this we run select price userid from buyclicks 

36
03:53:19,020 --> 03:53:22,245
this command says we want to query only the price and

37
03:53:22,245 --> 03:53:24,747
userid columns from the buyclicks table

38
03:53:27,568 --> 03:53:32,166
when we run this we only get those two columns 

39
03:53:34,416 --> 03:53:37,892
we can also perform queries that just select certain rows and

40
03:53:37,892 --> 03:53:39,400
meet a certain criteria 

41
03:53:40,570 --> 03:53:41,630
for example 

42
03:53:41,630 --> 03:53:46,744
let make a query that only shows the rows containing the price over 10 

43
03:53:47,770 --> 03:53:52,538
you can do this by running select price 

44
03:53:52,538 --> 03:53:57,736
userid from buyclicks where price 10 

45
03:53:57,736 --> 03:54:02,159
this query says we only want to see the price and userid columns from

46
03:54:02,159 --> 03:54:07,762
the buyclicks table where the value in the price column has a value greater than 10 

47
03:54:10,008 --> 03:54:16,437
we run this we see that we always have price values greater than 10 

48
03:54:18,588 --> 03:54:22,650
the sql language has a number of aggregation operations built into it 

49
03:54:24,250 --> 03:54:31,277
for example we can take the average price by running 

50
03:54:31,277 --> 03:54:36,219
select avg price from buyclicks 

51
03:54:36,219 --> 03:54:44,925
this command will show the average price from all the data in the buyclicks table 

52
03:54:44,925 --> 03:54:47,920
another aggregate operation is sum 

53
03:54:47,920 --> 03:54:52,531
we can see the total price by running 

54
03:54:52,531 --> 03:54:57,297
select sum price from buyclicks 

55
03:55:01,474 --> 03:55:06,547
we can also combine two tables by joining on a common column 

56
03:55:06,547 --> 03:55:11,050
if you recall we have three tables in our database adclicks 

57
03:55:11,050 --> 03:55:13,180
buyclicks and gameclicks 

58
03:55:14,870 --> 03:55:22,594
we look at the description the definition of adclicks buy running d adclicks 

59
03:55:26,061 --> 03:55:31,810
you can see that it also has a column called userid 

60
03:55:31,810 --> 03:55:36,030
let combine buyclicks and adclicks based on this common column 

61
03:55:37,180 --> 03:55:43,252
you can do this by running select adid 

62
03:55:43,252 --> 03:55:50,019
buyid adclicks userid from adclicks join

63
03:55:50,019 --> 03:55:58,011
buyclicks on adclicks userid = buyclicks userid

64
03:56:00,704 --> 03:56:05,278
this query says we want to just see the adid buyid and

65
03:56:05,278 --> 03:56:10,252
userid columns and we want to combine the adclicks table 

66
03:56:10,252 --> 03:56:14,729
the buyclicks table and we will be combining them on

67
03:56:14,729 --> 03:56:19,524
the userid column and it common in both those tables 

68
03:56:22,767 --> 03:56:25,809
when we run it you see just the three columns 

1
07:46:54,160 --> 07:46:57,540
next we will consider queries where two tables are used 

2
07:46:58,610 --> 07:47:00,540
let consider the query 

3
07:47:00,540 --> 07:47:05,320
find the beers liked by drinkers who frequent the great american bar 

4
07:47:07,070 --> 07:47:13,540
for this query we need the relation frequents and likes 

5
07:47:13,540 --> 07:47:16,330
now look at the scheme of these relations in the light blue box 

6
07:47:17,840 --> 07:47:20,390
they have a common attribute called drinker 

7
07:47:21,940 --> 07:47:24,740
so if we use the attribute drinker 

8
07:47:24,740 --> 07:47:27,760
we need to tell the system which one we are referring to 

9
07:47:29,980 --> 07:47:36,710
now look at the sql query the from clause in the query has these two relations 

10
07:47:36,710 --> 07:47:38,530
to handle a common attribute name issue 

11
07:47:38,530 --> 07:47:43,380
we need to give nicknames aliases to these relations 

12
07:47:43,380 --> 07:47:48,305
therefore in the from clause we say likes has the alias l and

13
07:47:48,305 --> 07:47:50,060
frequents has the alias f 

14
07:47:51,950 --> 07:47:57,290
since we want to find beers like before we use a select distinct clause for beer 

15
07:47:58,880 --> 07:48:03,740
as we saw before using select distinct avoids duplicates in the result 

16
07:48:05,300 --> 07:48:08,850
the where clause has two kinds of conditions 

17
07:48:09,870 --> 07:48:14,280
the first kind is a single table condition 

18
07:48:14,280 --> 07:48:22,170
in this case bar = the great american bar on the frequents relation 

19
07:48:23,820 --> 07:48:30,215
the second kind is a joined condition which says that the drinker attribute in

20
07:48:30,215 --> 07:48:37,090
the frequency relation is the same as the drinker attribute of the likes relation 

21
07:48:38,248 --> 07:48:42,420
we encode this in sql in the last line of the query using aliases 

22
07:48:44,140 --> 07:48:49,780
why did we not say l beer in the select clause or f bar in the first condition 

23
07:48:49,780 --> 07:48:52,410
we could have the query would have been equally right 

24
07:48:53,480 --> 07:48:58,520
but we are using a shortcut because we know that these attributes are unique

25
07:48:58,520 --> 07:48:59,680
already in the query 

26
07:49:01,570 --> 07:49:03,690
now let look at the query again 

27
07:49:03,690 --> 07:49:06,980
this time from the viewpoint of evaluating the query 

28
07:49:08,280 --> 07:49:11,080
there are many ways to evaluate the query but

29
07:49:11,080 --> 07:49:13,789
the way it most likely to be evaluated is this 

30
07:49:15,610 --> 07:49:20,110
the query will first look at the tables that have single table conditions 

31
07:49:21,260 --> 07:49:24,640
so it would perform a select operation

32
07:49:25,980 --> 07:49:30,360
on the frequents table to match the records of the condition

33
07:49:30,360 --> 07:49:33,700
that the great american bar equal to the great american bar 

34
07:49:34,950 --> 07:49:35,990
why is this strategy good 

35
07:49:37,120 --> 07:49:40,590
it because the selection operative reduces the number of triples to consider 

36
07:49:41,720 --> 07:49:45,390
thus if there are thousand triples in the relation frequents 

37
07:49:45,390 --> 07:49:47,760
maybe 60 of them matches the desired bar 

38
07:49:49,240 --> 07:49:55,010
so in the next step we have to deal with a fewer number of records than thousand 

39
07:49:56,170 --> 07:50:01,216
all right the next step will be a join with a likes relation 

40
07:50:01,216 --> 07:50:04,970
a join requires two relations in a join condition 

41
07:50:06,070 --> 07:50:07,620
the join condition comes from the query 

42
07:50:09,410 --> 07:50:13,102
the first relation shown with an underscore symbol here

43
07:50:16,008 --> 07:50:19,665
is a result of the previous operation 

44
07:50:19,665 --> 07:50:23,610
another way of saying this is that the result of

45
07:50:23,610 --> 07:50:28,470
the selection is piped into the join operation 

46
07:50:28,470 --> 07:50:31,930
that means we do not create

47
07:50:31,930 --> 07:50:35,400
an intermediate table from the result of the selection 

48
07:50:35,400 --> 07:50:41,060
the results are directly supplied to the next operator which in this case is join 

49
07:50:43,340 --> 07:50:48,017
now the result of the join operator is an intermediate structure with columns beer

50
07:50:48,017 --> 07:50:49,373
from likes relation and

51
07:50:49,373 --> 07:50:53,116
the drinker from the frequents relation that we have processed 

52
07:50:55,510 --> 07:51:00,492
this intermediate set of triples is piped to the project operation that

53
07:51:00,492 --> 07:51:02,326
picks up the beer column 

54
07:51:04,846 --> 07:51:09,216
now we need to process the distinct clause for

55
07:51:09,216 --> 07:51:14,970
deduplicate elimination which then goes to the output 

56
07:51:16,950 --> 07:51:20,614
we have already seen how the select project queries on single tables

57
07:51:20,614 --> 07:51:24,610
are evaluated when the tables are partitioned across several machines 

58
07:51:25,720 --> 07:51:29,070
we will now see how we process join queries in the same setting 

59
07:51:30,780 --> 07:51:36,546
for our case consider that the likes and

60
07:51:36,546 --> 07:51:40,470
frequents tables are on two different machines 

61
07:51:41,830 --> 07:51:43,740
in the first part of the query 

62
07:51:43,740 --> 07:51:46,699
the selection happens on the machine with the frequents table 

63
07:51:47,870 --> 07:51:52,550
the output of the query is a smaller table with the same scheme as frequents 

64
07:51:52,550 --> 07:51:55,550
that is with drinkers and bars 

65
07:51:55,550 --> 07:51:57,480
now we define an operation called semijoin 

66
07:51:59,420 --> 07:52:03,070
in which we need to move data from one machine to another 

67
07:52:05,050 --> 07:52:09,400
the goal of the semijoin operation is to reduce the cost of data movement 

68
07:52:10,480 --> 07:52:14,420
that is to move data from the machine which has

69
07:52:14,420 --> 07:52:19,196
the frequents data to the machine with the likes data 

70
07:52:19,196 --> 07:52:22,670
the cost is reduced if we ship less data 

71
07:52:22,670 --> 07:52:30,090
the way to it is to first find which data the join operation actually needs 

72
07:52:30,090 --> 07:52:35,570
clearly it needs only the drinkers column and not the bars column 

73
07:52:35,570 --> 07:52:41,070
so the drinkers column is projected out then

74
07:52:41,070 --> 07:52:44,440
just this column is transmitted to the second machine 

75
07:52:46,140 --> 07:52:51,130
finally the join is performed by looking at the values in the likes column

76
07:52:51,130 --> 07:52:54,080
that only matches the values in the shipped data 

77
07:52:56,270 --> 07:53:01,720
that means only the data from likes that matches the drinkers that are chosen 

78
07:53:03,410 --> 07:53:07,090
these are then the join results which would go to the output of the operation 

79
07:53:08,460 --> 07:53:11,680
now here you can see the semijoin operation graphically 

80
07:53:12,990 --> 07:53:16,800
the red table on the left is the output of the selection operations on the left 

81
07:53:19,520 --> 07:53:23,820
the white table on the right is the table to be joined to 

82
07:53:25,260 --> 07:53:28,120
since we need only the drinkers column 

83
07:53:28,120 --> 07:53:30,870
it is projected to create a one - column relation 

84
07:53:32,380 --> 07:53:38,490
notice that the red table has two entries for pete who frequented two bars 

85
07:53:38,490 --> 07:53:41,770
but the output of the project is condensed in the yellow table

86
07:53:41,770 --> 07:53:44,490
to just show the drinkers where pete appears only once 

87
07:53:46,170 --> 07:53:49,940
for those of you with a background in computer science 

88
07:53:49,940 --> 07:53:52,450
this can be done using a hash map like data structure 

89
07:53:54,400 --> 07:53:58,180
this one - column table is now shipped to site2 which has the likes relation 

90
07:53:59,290 --> 07:54:03,834
now at site2 the shipped relation is used to find matches from

91
07:54:03,834 --> 07:54:08,222
the drinkers column and it finds only one match called sally 

92
07:54:08,222 --> 07:54:11,960
so the corresponding result triples in this case 

93
07:54:11,960 --> 07:54:15,956
only one triple is produced at the end of this operation 

94
07:54:15,956 --> 07:54:21,414
now the original table and the matching table are shipped

95
07:54:21,414 --> 07:54:27,350
to the last of the operation to finish the join operation 

96
07:54:27,350 --> 07:54:31,120
and more efficient version of this is shown in the next slide 

97
07:54:32,120 --> 07:54:39,250
in this version the first two steps this and that are the same 

98
07:54:40,620 --> 07:54:44,938
then the result of the reduce is also shipped to site1 to find

99
07:54:44,938 --> 07:54:47,490
the matches from the red relation 

100
07:54:48,510 --> 07:54:52,060
another reduce operation is performed on site1 now

101
07:54:52,060 --> 07:54:55,020
to get the matching records on the red relation 

102
07:54:55,020 --> 07:54:58,103
finally these two reduced relations are shipped to

103
07:54:58,103 --> 07:55:01,200
the site where the final join happens 

104
07:55:01,200 --> 07:55:03,920
and all of this may seem like a lot of detail 

105
07:55:05,070 --> 07:55:07,960
let me repeat something i have said before 

106
07:55:09,600 --> 07:55:15,850
if we have a system like db2 or spark sql that implements multi - site joins it

107
07:55:15,850 --> 07:55:19,200
will perform this kind of operation under the hood you do not have to know them 

108
07:55:20,460 --> 07:55:26,020
however if we were to implement a similar operation and all that you have

109
07:55:26,020 --> 07:55:30,240
is hadoop you may end up implementing this kind of algorithm yourself 

1
15:42:23,642 --> 15:42:28,010
the queries in real life are little more complex than what we have seen before 

2
15:42:29,080 --> 15:42:31,180
so let consider a more complex query 

3
15:42:33,010 --> 15:42:39,190
let s find bars where the price of miller is the same as or

4
15:42:39,190 --> 15:42:43,600
less than what the great american bar called tgab here charges for bud 

5
15:42:45,090 --> 15:42:50,026
you may say but we do not really know what tgab charges for bud 

6
15:42:50,026 --> 15:42:56,600
that correct so we can break up the query into two parts 

7
15:42:56,600 --> 15:43:04,060
first we will find this unknown price and then we will use that price

8
15:43:04,060 --> 15:43:09,160
to find the bars that would sell miller for the same price or better price 

9
15:43:09,160 --> 15:43:14,119
now this is a classic situation where the result from the first part of

10
15:43:14,119 --> 15:43:18,326
the query should be fed as a parameter to the second query 

11
15:43:18,326 --> 15:43:21,670
now this situation is called a subquery 

12
15:43:23,510 --> 15:43:26,090
we write this in sql as shown in the slide here 

13
15:43:27,730 --> 15:43:32,730
what makes this query different is that the part where price is less than equal

14
15:43:32,730 --> 15:43:37,440
to instead of specifying a constant like 8 

15
15:43:37,440 --> 15:43:43,610
we actually place another query which computes the price of a bud at tgab 

16
15:43:46,300 --> 15:43:50,600
the shaded part is called the inner query or the subquery 

17
15:43:51,870 --> 15:43:54,750
in this case both the outer query and

18
15:43:54,750 --> 15:43:58,110
the inner query use the same relation which is sells 

19
15:43:59,820 --> 15:44:05,100
now in terms of evaluation the inner query is evaluated first 

20
15:44:05,100 --> 15:44:07,520
and the outer query uses its output 

21
15:44:08,750 --> 15:44:12,110
now while it may not be obvious at this time 

22
15:44:12,110 --> 15:44:15,930
notice that the inner query is independent of the outer query 

23
15:44:16,960 --> 15:44:18,270
in other words 

24
15:44:18,270 --> 15:44:22,930
even if we did not have the outer query we can still evaluate the inner query 

25
15:44:24,270 --> 15:44:28,550
we say in this case that the subquery is uncorrelated 

26
15:44:31,180 --> 15:44:33,260
let look at another example of a subquery 

27
15:44:34,350 --> 15:44:37,504
in this example we want to find the name and

28
15:44:37,504 --> 15:44:41,185
manufacturer of each beer that fred did not like 

29
15:44:41,185 --> 15:44:44,953
so how do we know what fred did not like 

30
15:44:44,953 --> 15:44:49,545
we do however know that the set of beers that fred likes because they

31
15:44:49,545 --> 15:44:51,925
are listed in the likes relation 

32
15:44:51,925 --> 15:44:56,324
so we need to subtract this set from the total set of beers that the company has

33
15:44:56,324 --> 15:44:57,030
recorded 

34
15:44:58,260 --> 15:45:02,180
this subtraction of sets can be performed in several ways 

35
15:45:02,180 --> 15:45:05,660
one of them is to use the not in construct 

36
15:45:07,130 --> 15:45:12,270
so the query class job is to take every name from the beers table and

37
15:45:12,270 --> 15:45:17,730
output it only if it does not appear in the set produced by the inner query 

38
15:45:19,180 --> 15:45:23,990
similar to the previous query the subquery here is also uncorrelated 

39
15:45:26,920 --> 15:45:28,640
now this is a more sophisticated query 

40
15:45:29,745 --> 15:45:33,490
the intention is to find beers that are more expensive

41
15:45:33,490 --> 15:45:35,820
than the average price of beer 

42
15:45:35,820 --> 15:45:39,250
but since beers have different prices in different bars 

43
15:45:39,250 --> 15:45:41,520
we have to find the average for every bar 

44
15:45:42,720 --> 15:45:46,340
therefore the idea is to find the average price of beer for

45
15:45:46,340 --> 15:45:51,340
every bar and then compare the price of each beer with respect to this average 

46
15:45:52,670 --> 15:45:54,580
now look at the query and the table 

47
15:45:56,265 --> 15:45:59,110
let assume we are processing the first table 

48
15:46:01,250 --> 15:46:05,134
the beer is bud and the price is 5 

49
15:46:05,134 --> 15:46:12,741
now we need to know if 5 is greater than the average price of beer sold at hgat 

50
15:46:12,741 --> 15:46:17,120
to do this we need to compute the inner query okay 

51
15:46:17,120 --> 15:46:19,410
so now let look at the fourth row 

52
15:46:19,410 --> 15:46:25,490
the price of guinness needs to be compared to that average again for hgat 

53
15:46:26,550 --> 15:46:29,869
in fact for every table processed by the outer query 

54
15:46:29,869 --> 15:46:32,980
one needs to compute the inner query for that bar 

55
15:46:34,240 --> 15:46:38,620
this makes the inner subquery correlated with the outer query 

56
15:46:39,910 --> 15:46:44,013
now a smart query processor will store the average once it computed and

57
15:46:44,013 --> 15:46:47,929
then reuse the stored value instead of computing over and over again 

58
15:46:51,376 --> 15:46:53,521
what is an aggregate query 

59
15:46:53,521 --> 15:46:58,440
let use a simple example of finding the average price of bud 

60
15:46:59,980 --> 15:47:04,100
this is like a simple select project query with the additional aspect

61
15:47:04,100 --> 15:47:08,010
that it takes a list of price values of bud from different bars and

62
15:47:08,010 --> 15:47:09,250
then computes an average 

63
15:47:10,280 --> 15:47:14,749
in the example shown the average of the five prices is 4 2 

64
15:47:14,749 --> 15:47:17,230
in other words the average function 

65
15:47:17,230 --> 15:47:22,750
the avg function takes a list of values and produces a single value 

66
15:47:22,750 --> 15:47:25,700
now there are many functions that have this behavior 

67
15:47:25,700 --> 15:47:28,090
the sum function takes a list of values and

68
15:47:28,090 --> 15:47:30,280
adds them up to produce a single value 

69
15:47:30,280 --> 15:47:33,000
the count function takes a list of list of values and

70
15:47:33,000 --> 15:47:35,490
counts the number of items in that list and so on 

71
15:47:38,020 --> 15:47:41,170
these are called aggregate functions 

72
15:47:43,160 --> 15:47:48,208
now if we wanted to count only the price values that are different that is 3 

73
15:47:48,208 --> 15:47:53,690
4 and 5 just once we can write the select clause a little differently 

74
15:47:55,350 --> 15:48:00,852
we would say that the average is over distinct values

75
15:48:00,852 --> 15:48:05,612
of price which in this case will result in 4 

76
15:48:05,612 --> 15:48:10,481
you should recognize that most analytical operations need to use statistical

77
15:48:10,481 --> 15:48:12,630
functions which are aggregates 

78
15:48:14,150 --> 15:48:16,850
so another important analytical requirement

79
15:48:16,850 --> 15:48:20,140
is computing the statistical aggregate by groups 

80
15:48:20,140 --> 15:48:24,660
for example we often compute the average salaries of employees per department 

81
15:48:25,670 --> 15:48:31,070
now back to our example here we want to find the average price paid for

82
15:48:31,070 --> 15:48:37,670
bud per drinker where we know that a drinker visits many bars 

83
15:48:37,670 --> 15:48:40,520
so the grouping variable here is drinker 

84
15:48:41,740 --> 15:48:46,900
so we have three attributes at play price which we need to aggregate 

85
15:48:46,900 --> 15:48:51,230
drinker which we need to group by and bar which is a join attribute 

86
15:48:52,660 --> 15:48:56,380
the fourth attribute namely beer is used for selection and

87
15:48:56,380 --> 15:48:59,100
does not participate in grouping 

88
15:48:59,100 --> 15:49:04,340
so after the selection we will get an intermediate relation

89
15:49:04,340 --> 15:49:07,220
containing drinker bar and price 

90
15:49:08,420 --> 15:49:12,660
with this the group by operation will create one result row for

91
15:49:12,660 --> 15:49:17,040
each drinker and place the average price over all such rows 

92
15:49:19,650 --> 15:49:21,090
now how does group by and

93
15:49:21,090 --> 15:49:23,630
aggregate computation work when the data is partitioned 

94
15:49:24,670 --> 15:49:25,570
let take the same query 

95
15:49:26,680 --> 15:49:31,071
we are looking for the average price of bud grouped by drinker 

96
15:49:33,859 --> 15:49:39,090
but this time the result of the selection are in two different machines 

97
15:49:39,090 --> 15:49:42,270
imagine that this time they are range partitioned by row numbers 

98
15:49:42,270 --> 15:49:44,170
which we have not shown to maintain clarity 

99
15:49:45,480 --> 15:49:49,374
now with the group by operation the data will get repartitioned by the grouping

100
15:49:49,374 --> 15:49:50,970
attribute that drinker 

101
15:49:53,730 --> 15:49:59,340
and then the aggregate function is computed locally 

102
15:50:00,890 --> 15:50:06,453
to accomplish this repartitioning task each machine groups its own data locally 

103
15:50:06,453 --> 15:50:11,783
determines which portions of data should be transmitted to a different machine 

104
15:50:11,783 --> 15:50:14,580
and accordingly ships it to that machine 

105
15:50:15,630 --> 15:50:18,390
now there are several variants of this general scheme

106
15:50:18,390 --> 15:50:20,680
which are even more efficient 

107
15:50:20,680 --> 15:50:24,870
now if this reminds you of the map operation you saw in your previous course 

108
15:50:24,870 --> 15:50:25,790
you are exactly right 

109
15:50:26,910 --> 15:50:32,164
this fundamental process of grouping partitioning and redistribution of data

110
15:50:32,164 --> 15:50:37,500
is inherent in data - parallel computing and implemented inside database systems 

1
07:33:01,070 --> 07:33:01,600
welcome back 

2
07:33:02,850 --> 07:33:07,705
in this video we will provide you a quick summary of the main points from

3
07:33:07,705 --> 07:33:11,303
our last course on big data modeling and management 

4
07:33:11,303 --> 07:33:15,882
if you had just completed our second course and do not need a refresher 

5
07:33:15,882 --> 07:33:18,261
you may now skip to the next lecture 

6
07:33:18,261 --> 07:33:23,311
after this video you will be able to recall why big data modeling and

7
07:33:23,311 --> 07:33:28,536
management is essential in preparing to gain insights from your data 

8
07:33:28,536 --> 07:33:31,693
summarize different kids of data models 

9
07:33:31,693 --> 07:33:37,351
describe streaming data and the different challenges it presents and explain the differences

10
07:33:37,351 --> 07:33:42,403
between a database management system and a big data management system 

11
07:33:45,510 --> 07:33:50,140
in the second course we described a data model as a specification

12
07:33:50,140 --> 07:33:54,194
that precisely characterizes the structure of the data 

13
07:33:54,194 --> 07:33:59,260
the operations on the data and the constraints that may apply on data 

14
07:34:00,360 --> 07:34:04,070
for example a data model may state that

15
07:34:04,070 --> 07:34:08,030
a data is structured like a two - dimensional array or a matrix 

16
07:34:09,760 --> 07:34:15,050
for this structure one may have a data access operation 

17
07:34:15,050 --> 07:34:20,720
which given an index of the array we use the cell of the array to refer to 

18
07:34:23,210 --> 07:34:27,820
a data model may also specify constraints on the data 

19
07:34:27,820 --> 07:34:32,720
for example while a total data set may have many arrays 

20
07:34:32,720 --> 07:34:35,640
the name of each array must be unique and

21
07:34:35,640 --> 07:34:40,420
the values of a specific array must always be greater than zero 

22
07:34:42,070 --> 07:34:47,335
database management systems handle low level data management operations 

23
07:34:47,335 --> 07:34:50,980
help organization of the data using a data model and

24
07:34:50,980 --> 07:34:54,147
provide an open programmable access to data 

25
07:34:56,975 --> 07:35:00,346
we covered a number of data models 

26
07:35:00,346 --> 07:35:03,911
we showed four models that were discussed in more details 

27
07:35:06,330 --> 07:35:09,810
the relational data to date is the most used data model 

28
07:35:10,990 --> 07:35:16,189
here data is structured like tables which are formally called relations 

29
07:35:17,260 --> 07:35:20,840
the relational data model has been implemented in traditional database

30
07:35:20,840 --> 07:35:22,120
systems 

31
07:35:22,120 --> 07:35:27,240
but they are being refreshly implemented in modern data systems over hadoop and

32
07:35:27,240 --> 07:35:30,400
spark and are getting deployed on cloud platforms 

33
07:35:31,700 --> 07:35:36,930
the second category of data gaining popularity is semi - structured data 

34
07:35:36,930 --> 07:35:41,840
which includes documents like html pages xml data and

35
07:35:41,840 --> 07:35:44,960
json data that are used by many internet applications 

36
07:35:46,470 --> 07:35:49,201
this data can have one element nested or

37
07:35:49,201 --> 07:35:54,674
embedded within another data element and hence can often be modeled as a tree 

38
07:35:57,779 --> 07:36:02,480
the third category of data models is called graph data 

39
07:36:02,480 --> 07:36:07,700
a graph is a network where nodes represent entities and

40
07:36:07,700 --> 07:36:11,540
edges represent relationships between pairs of such entities 

41
07:36:12,800 --> 07:36:18,675
for example in a social network nodes may represent users and

42
07:36:18,675 --> 07:36:22,160
edges may represent their friendship 

43
07:36:22,160 --> 07:36:26,990
the operations performed on graph data includes traversing the network so

44
07:36:26,990 --> 07:36:31,000
that one can find friend of a friend of a friend if needed 

45
07:36:33,200 --> 07:36:38,890
in contrast to the previous three models that there is a structure to the data 

46
07:36:38,890 --> 07:36:44,280
the text data is much more unstructured because an entire data item

47
07:36:44,280 --> 07:36:46,780
like a new article can be just a text string 

48
07:36:48,180 --> 07:36:52,540
however text is the primary form of data

49
07:36:52,540 --> 07:36:56,120
in information retrieval systems or search engines like google 

50
07:36:59,250 --> 07:37:04,290
we also discussed streaming data or data with velocity as a special

51
07:37:04,290 --> 07:37:09,580
class of data that continually come to the system at some data rate 

52
07:37:11,350 --> 07:37:16,308
examples can be found in data coming from road sensors that measure traffic

53
07:37:16,308 --> 07:37:20,636
patterns or stock price data from the stock exchange that may come

54
07:37:20,636 --> 07:37:24,111
in volumes from stock exchanges all over the world 

55
07:37:26,563 --> 07:37:34,090
streaming data is special because a stream is technically an infinite data source 

56
07:37:34,090 --> 07:37:37,050
and therefore we keep filling up memory and

57
07:37:37,050 --> 07:37:41,390
storage and will eventually go beyond the capacity of any system 

58
07:37:42,930 --> 07:37:46,720
streaming data therefore needs a different kind of management system 

59
07:37:47,960 --> 07:37:53,135
for this reason streaming data is processed in memory 

60
07:37:53,135 --> 07:37:56,841
in chunks which are also called windows 

61
07:37:56,841 --> 07:38:00,226
often only the necessary part of the data stream or

62
07:38:00,226 --> 07:38:04,186
the results of queries against the data stream is stored 

63
07:38:06,145 --> 07:38:12,180
a typical type of query against streaming data are alerts or notifications 

64
07:38:12,180 --> 07:38:16,933
the system notices an event like multiple stock price changing within a short time 

65
07:38:20,658 --> 07:38:23,160
streaming data is also used for prediction 

66
07:38:25,330 --> 07:38:30,287
for instance based on wind direction and temperature data streams 

67
07:38:30,287 --> 07:38:33,768
one can predict how a wildfire is going to spread 

68
07:38:36,640 --> 07:38:41,522
in the last course we also covered a number of data systems that we called

69
07:38:41,522 --> 07:38:43,590
big data management systems 

70
07:38:45,510 --> 07:38:48,890
these systems use different data models and

71
07:38:48,890 --> 07:38:54,030
have different capabilities but are characterized by some common features 

72
07:38:55,210 --> 07:38:59,430
they are also designed from the start for parallel and distributed processing 

73
07:39:00,610 --> 07:39:06,100
most of them implement data partition parallelism which if you can recall 

74
07:39:06,100 --> 07:39:10,890
refers to the process of segmenting the data into multiple machines so

75
07:39:10,890 --> 07:39:15,839
data retrieval and manipulations can be performed in parallel on these machines 

76
07:39:17,880 --> 07:39:23,360
many of these systems allow a large number of users who constantly update and

77
07:39:23,360 --> 07:39:24,510
query the system 

78
07:39:26,590 --> 07:39:31,190
some of the systems do not maintain transaction consistency with every update 

79
07:39:32,300 --> 07:39:33,224
that means 

80
07:39:33,224 --> 07:39:38,437
not all the machines may have all the updates guaranteed at every moment 

81
07:39:40,091 --> 07:39:45,460
however most of them provide a guarantee of eventual consistency 

82
07:39:45,460 --> 07:39:50,780
which means all the machines will get all updates sooner or later 

83
07:39:50,780 --> 07:39:53,920
therefore providing better accuracy and time 

84
07:39:56,610 --> 07:40:01,740
the third common characteristic of big data management systems is

85
07:40:01,740 --> 07:40:05,750
that they are often built on top of a hadoop - like platform that provides

86
07:40:05,750 --> 07:40:10,040
automatic replication and a map - reduce style processing ability 

87
07:40:11,270 --> 07:40:15,440
some of the data operations performed within these systems make use of these

88
07:40:15,440 --> 07:40:16,829
lower level capabilities 

89
07:40:19,200 --> 07:40:21,520
after this refresher on data modeling and

90
07:40:21,520 --> 07:40:25,650
management let start big data integration and processing 

1
15:13:26,188 --> 15:13:27,689
so hi 

2
15:13:27,689 --> 15:13:32,521
in the previous course we saw examples of different data models and

3
15:13:32,521 --> 15:13:36,290
talked about a few current data management systems 

4
15:13:36,290 --> 15:13:39,617
in this module we will focus on data retrieval 

5
15:13:59,163 --> 15:14:04,012
data retrieval refers to the way in which data desired

6
15:14:04,012 --> 15:14:08,980
by a user is specified and retrieved from a data store 

7
15:14:10,280 --> 15:14:14,720
note that in this course we are using the term data retrieval in two ways 

8
15:14:15,760 --> 15:14:20,430
assume that your data is stored in a data store that follows a specific data model 

9
15:14:20,430 --> 15:14:22,190
like for example the relational data model 

10
15:14:23,340 --> 15:14:27,300
by data retrieval we will refer to one 

11
15:14:27,300 --> 15:14:31,950
the way you specify how to get the desired data out of the system 

12
15:14:31,950 --> 15:14:36,910
this is called the query specification method and two 

13
15:14:36,910 --> 15:14:42,300
the internal processing that occurs within the data management system to compute or

14
15:14:42,300 --> 15:14:45,520
evaluate that specified retrieval request 

15
15:14:47,170 --> 15:14:52,780
while query specification can apply to small data stores or large data stores 

16
15:14:52,780 --> 15:14:56,600
we will keep an eye on the nature of query evaluation when the data is big 

17
15:14:58,230 --> 15:15:02,370
further we will consider how the query specification changes

18
15:15:02,370 --> 15:15:04,770
when we deal with faster streaming data 

19
15:15:07,380 --> 15:15:12,590
a query language is a language in which a retrieval request is specified 

20
15:15:14,730 --> 15:15:20,232
a query language is often called declarative which means it lets you

21
15:15:20,232 --> 15:15:25,600
specify what you want to retrieve without having to tell the system how to retrieve 

22
15:15:26,670 --> 15:15:28,660
for example you can say 

23
15:15:28,660 --> 15:15:34,550
find all data from relation employee where the salary is more than 50000 

24
15:15:34,550 --> 15:15:38,970
now you do not have to write a program which will tell the system to open a file 

25
15:15:38,970 --> 15:15:44,006
skip the first 250 bytes then in a loop pick the next 1024 bytes 

26
15:15:44,006 --> 15:15:48,340
probe into the 600th byte and read an integer and so forth 

27
15:15:49,480 --> 15:15:52,160
instead of writing such a complicated procedure 

28
15:15:52,160 --> 15:15:56,450
you just specify the data items that you need and the system does the rest 

29
15:15:57,840 --> 15:16:02,167
for example sql structured query language 

30
15:16:02,167 --> 15:16:06,810
is the most used query language for relational data 

31
15:16:06,810 --> 15:16:11,156
now in contrast to a query language a database programming

32
15:16:11,156 --> 15:16:16,076
language like oracle pl sql or postgres pgsql are high - level

33
15:16:16,076 --> 15:16:20,680
procedural programming languages that embed query operations 

34
15:16:21,870 --> 15:16:24,648
we will look at some query languages in detail and

35
15:16:24,648 --> 15:16:27,649
show examples of database programming languages 

36
15:16:30,040 --> 15:16:32,881
the first query language we will look at is sql 

37
15:16:32,881 --> 15:16:37,002
which is the ubiquitous query language when the data is structured 

38
15:16:37,002 --> 15:16:42,120
but has been extended in many ways to accommodate other types of data 

39
15:16:42,120 --> 15:16:46,329
for this course we will stick to the structured aspect of the language 

40
15:16:46,329 --> 15:16:50,939
now you should know that sql is used for classical database management systems

41
15:16:50,939 --> 15:16:55,440
like oracle as well as modern hadoop style distributed systems such as spark 

42
15:16:56,650 --> 15:16:59,220
now we will work with an illustrative example 

43
15:17:01,140 --> 15:17:04,360
first we need to define the schema of the database 

44
15:17:04,360 --> 15:17:09,640
now think of a business called the beer drinkers club that owns many bars 

45
15:17:09,640 --> 15:17:10,990
and each bar sells beer 

46
15:17:12,060 --> 15:17:16,646
our schema for this business has six relations of tables 

47
15:17:16,646 --> 15:17:21,448
the first table lists these bars the names addresses and

48
15:17:21,448 --> 15:17:23,814
the license number of the bar 

49
15:17:23,814 --> 15:17:28,469
notice that the attribute name is underlined because it is the primary key

50
15:17:28,469 --> 15:17:29,930
of the bars relation 

51
15:17:29,930 --> 15:17:33,800
recall that the primary key refers to a set of attributes 

52
15:17:33,800 --> 15:17:37,350
in this case just the name that makes a record unique 

53
15:17:39,060 --> 15:17:41,720
note that the relation bars with the attribute name

54
15:17:43,080 --> 15:17:46,241
within parenthesis is the same as the table shown on the right 

55
15:17:47,280 --> 15:17:51,147
we will use both representations as we go forward 

56
15:17:51,147 --> 15:17:56,030
the second table called beers this is the names and manufacturers of beer 

57
15:17:56,030 --> 15:18:00,266
now not every bar sells the same brands of beer and even when they do 

58
15:18:00,266 --> 15:18:02,280
they may have different prices for

59
15:18:02,280 --> 15:18:06,390
the same product because of differences in the establishment costs 

60
15:18:07,550 --> 15:18:12,960
so the sells table records which bar sells which beer at what price 

61
15:18:14,970 --> 15:18:17,268
now our business is special 

62
15:18:17,268 --> 15:18:21,358
it also keeps information about the regular member customers 

63
15:18:21,358 --> 15:18:26,750
so the drinkers relation has the name address and phone of these customers 

64
15:18:26,750 --> 15:18:31,032
well not only that it knows which member visits

65
15:18:31,032 --> 15:18:35,720
which bars and which beer each member likes 

66
15:18:35,720 --> 15:18:38,980
clearly the beer drinkers club knows its customers 

67
15:18:41,670 --> 15:18:48,490
the most basic structure of an sql query is a select - from - where clause 

68
15:18:49,500 --> 15:18:52,920
in this example we are looking for beer names that are made by heineken 

69
15:18:54,060 --> 15:18:57,160
so we need to specify our output attribute 

70
15:18:57,160 --> 15:18:59,520
in this case the name of the beer 

71
15:18:59,520 --> 15:19:05,040
the logical table which will be used to answer the query in this case beers 

72
15:19:06,370 --> 15:19:10,972
and the condition that all the desired data items should satisfy 

73
15:19:10,972 --> 15:19:15,260
namely the value of the attribute called manf is heineken 

74
15:19:16,600 --> 15:19:19,570
now there are few things to notice here 

75
15:19:19,570 --> 15:19:23,311
first the literal heineken is put within quotes 

76
15:19:23,311 --> 15:19:26,210
because it a single string literal 

77
15:19:27,470 --> 15:19:28,550
remember that in this case 

78
15:19:28,550 --> 15:19:32,830
the string is supposed to match exactly including the case 

79
15:19:34,910 --> 15:19:40,590
secondly if you go back to the data operations discussed in course two 

80
15:19:40,590 --> 15:19:45,610
you will recognize that this form of query can also be represented

81
15:19:45,610 --> 15:19:51,070
as a selection operation on the relation beers with a condition on the manf

82
15:19:51,070 --> 15:19:54,800
attribute followed by a projection operation

83
15:19:54,800 --> 15:19:58,349
that outputs the name attribute from the result of the selection operation 

84
15:19:59,680 --> 15:20:04,340
so the selection operation finds all tuples of beer for which the manufacturer

85
15:20:04,340 --> 15:20:09,710
is heineken and from those tuples it projects only the name column 

86
15:20:11,300 --> 15:20:15,580
the result of the query is a table with one single attribute called name 

87
15:20:18,250 --> 15:20:22,883
we illustrate some more features of sql using two example queries 

88
15:20:22,883 --> 15:20:26,330
the first looks for expensive beer and its price 

89
15:20:27,370 --> 15:20:30,856
let say we consider a beer to be expensive if it costs more than 15

90
15:20:30,856 --> 15:20:31,569
per bottle 

91
15:20:32,810 --> 15:20:34,200
from the schema 

92
15:20:34,200 --> 15:20:37,830
we know that the price information is available in the table called sells 

93
15:20:38,900 --> 15:20:41,470
so the from clause should use sells 

94
15:20:42,620 --> 15:20:44,490
the where clause is intuitive and

95
15:20:44,490 --> 15:20:48,200
specifies the price of the beer to be greater than 15 

96
15:20:48,200 --> 15:20:52,873
now notice that the sells relation also has a column called bar 

97
15:20:52,873 --> 15:20:59,170
now if two different bars sell the same beer at the same price 

98
15:20:59,170 --> 15:21:01,570
we will get both entries in the result 

99
15:21:01,570 --> 15:21:03,620
but that not what we want 

100
15:21:03,620 --> 15:21:07,850
now regardless of the multiplicity of bars that have the same price for

101
15:21:07,850 --> 15:21:10,340
the same beer we want the result just once 

102
15:21:12,320 --> 15:21:17,688
so this is achieved through the select distinct statement 

103
15:21:17,688 --> 15:21:23,384
which ensures that the result relation will have no duplicate 

104
15:21:23,384 --> 15:21:27,736
the second example shows the case where more than one condition

105
15:21:27,736 --> 15:21:30,045
must be specified by the result 

106
15:21:30,045 --> 15:21:33,774
in this query the business must be in san diego and

107
15:21:33,774 --> 15:21:37,764
at the same time it must be a temporary license holder 

108
15:21:37,764 --> 15:21:41,870
which means the license number should start with 32 

109
15:21:41,870 --> 15:21:47,280
as we see here these conditions are put together by the and operator 

110
15:21:49,230 --> 15:21:53,330
thus the query will pick the third record in the table

111
15:21:53,330 --> 15:21:57,560
because the first record satisfy the first condition and not the second condition 

112
15:21:58,950 --> 15:22:02,780
in a few slides we will come back to the evaluation of this type of queries

113
15:22:02,780 --> 15:22:04,625
in the context of big data 

114
15:22:04,625 --> 15:22:11,340
now remember one can also place a limit on the number of results to return 

115
15:22:11,340 --> 15:22:16,163
if our database is large and we need only five results for

116
15:22:16,163 --> 15:22:20,620
example for a sample to display we can say limit 5 

117
15:22:20,620 --> 15:22:27,706
now the exact syntax of this limit clause may vary between dbms vendors 

1
06:35:53,450 --> 06:35:57,680
now if the table of beers was large and had millions of entries 

2
06:35:58,820 --> 06:36:02,480
the table would possibly need to be split over many machines 

3
06:36:03,600 --> 06:36:07,720
another way of saying that is that the table will be partitioned

4
06:36:07,720 --> 06:36:09,960
across a number of machines 

5
06:36:09,960 --> 06:36:12,210
since a query simply performs a selection and

6
06:36:12,210 --> 06:36:15,330
projection here it can be evaluated in parallel 

7
06:36:16,860 --> 06:36:19,860
remember that name is the primary key of the table 

8
06:36:21,070 --> 06:36:23,790
one standard way of partitioning the data

9
06:36:23,790 --> 06:36:26,260
is called a range partitioning by the primary key 

10
06:36:28,210 --> 06:36:32,760
this simply means that the rows of the table are put in groups

11
06:36:32,760 --> 06:36:36,440
depending on the alphabetical order of the name value 

12
06:36:37,570 --> 06:36:41,950
so beers with names starting with e and b here are placed in machine 1 

13
06:36:41,950 --> 06:36:44,910
those starting with c and d are in machine 2 

14
06:36:44,910 --> 06:36:49,337
and if there are too many rows for entries where the name starts with h 

15
06:36:49,337 --> 06:36:51,902
maybe h is split into machines 5 and 6 

16
06:36:51,902 --> 06:36:53,260
this is shown in the sketch here 

17
06:36:54,640 --> 06:36:59,620
next we will show how queries are performed over partition tables 

18
06:36:59,620 --> 06:37:04,230
but before we do that you should know that all database management companies 

19
06:37:04,230 --> 06:37:09,090
like ibm chair data microsoft and others have a solution like this for

20
06:37:09,090 --> 06:37:12,670
large volumes of data where data partitioning is used 

21
06:37:12,670 --> 06:37:17,030
newer systems like spark and sql are naturally distributed and

22
06:37:17,030 --> 06:37:18,419
therefore offer data partitioning 

23
06:37:21,470 --> 06:37:24,620
so we show the same partition tables as we saw before 

24
06:37:24,620 --> 06:37:26,180
now we will ask two queries 

25
06:37:27,550 --> 06:37:32,340
the first query asks for all tuples as records from the beers table

26
06:37:33,500 --> 06:37:36,290
where the name of the beer starts with am 

27
06:37:37,970 --> 06:37:40,400
and the second query is exactly what we asked before 

28
06:37:43,544 --> 06:37:46,560
the first query in the sql looks like this 

29
06:37:47,730 --> 06:37:52,434
we said select from beers to mean all attributes from table beers 

30
06:37:52,434 --> 06:37:56,790
the where clause shows the syntax for a partial match query 

31
06:37:57,940 --> 06:38:01,850
in this query there are two new syntax elements 

32
06:38:01,850 --> 06:38:03,630
the first is a predicate called like 

33
06:38:05,480 --> 06:38:09,080
when we use like we are telling the query engine

34
06:38:09,080 --> 06:38:12,870
that we only have partial information about the string we want to match 

35
06:38:13,870 --> 06:38:16,870
this partly specified string is called a string pattern 

36
06:38:18,290 --> 06:38:21,320
that means there is this part of the string we know and

37
06:38:21,320 --> 06:38:22,460
a part that we do not know 

38
06:38:24,050 --> 06:38:30,300
in this case we know that our design string starts with am so we would write am 

39
06:38:30,300 --> 06:38:35,250
and then we put to refer to the part of the string that we do not know 

40
06:38:35,250 --> 06:38:39,410
putting them together we get am 

41
06:38:39,410 --> 06:38:43,671
if we wanted to find say am somewhere in the middle of the string 

42
06:38:43,671 --> 06:38:46,027
we would write the pattern as am 

43
06:38:46,027 --> 06:38:52,680
the second query is not new 

44
06:38:52,680 --> 06:38:54,580
we saw it in the last slide 

45
06:38:54,580 --> 06:38:59,490
however as we will see next evaluating the second query will be a little more

46
06:38:59,490 --> 06:39:03,930
tricky in a partition database than that we usually see for big data 

47
06:39:08,390 --> 06:39:11,780
let talk about the first query in this data partition setting 

48
06:39:11,780 --> 06:39:18,160
the question to ask is do we need to touch all partitions to answer the query 

49
06:39:18,160 --> 06:39:24,720
of course not we know that the name is a primary key for the table of beers 

50
06:39:24,720 --> 06:39:28,710
we also know that the system did arrange partitioning on the name attribute 

51
06:39:29,710 --> 06:39:34,260
this means that the evaluation process should only access machine 1

52
06:39:34,260 --> 06:39:38,370
because no other machine will have records for names starting with a 

53
06:39:39,670 --> 06:39:41,600
now this is exactly what we as humans 

54
06:39:41,600 --> 06:39:46,780
do when we look up an entry in a multivolume encyclopedia 

55
06:39:46,780 --> 06:39:50,300
we look for the starting words then figure out which specific volume would

56
06:39:50,300 --> 06:39:52,410
have that entry then pick up just that volume 

57
06:39:53,830 --> 06:39:57,330
thus so long as the system knows the partitioning strategy 

58
06:39:58,380 --> 06:40:00,310
it can make its job much more efficient 

59
06:40:01,760 --> 06:40:05,920
when a system processes thousands of queries per second 

60
06:40:05,920 --> 06:40:08,430
this kind of efficiency actually matters 

61
06:40:09,970 --> 06:40:13,785
now raised partitioning is only one of many partitioning

62
06:40:13,785 --> 06:40:16,650
schemes used in a database system okay 

63
06:40:17,790 --> 06:40:20,760
let try to answer the second query in the same partition setting 

64
06:40:21,820 --> 06:40:25,650
now the query condition is on the second attribute manf 

65
06:40:26,690 --> 06:40:28,900
now in one sense it a simpler query 

66
06:40:28,900 --> 06:40:30,392
there is no light pattern here and

67
06:40:30,392 --> 06:40:33,789
we know exactly the string that we are looking for namely the string heineken 

68
06:40:34,930 --> 06:40:38,310
however this time we really cannot get away

69
06:40:38,310 --> 06:40:42,520
by using the partitioning information because the partitioning activity is

70
06:40:42,520 --> 06:40:46,130
different from the attribute on which the query condition is applied 

71
06:40:47,300 --> 06:40:51,023
so this query will need to go to all partitions 

72
06:40:51,023 --> 06:40:56,200
technically speaking the query needs to be broadcast

73
06:40:56,200 --> 06:40:59,870
from the primary machine to all machines as shown here 

74
06:41:02,500 --> 06:41:06,320
next this broadcast query will be independently and

75
06:41:06,320 --> 06:41:10,470
in parallel execute the query on the local machine 

76
06:41:11,840 --> 06:41:15,420
then these results need to be brought back into the primary machine 

77
06:41:16,690 --> 06:41:19,760
and then they need to be unioned together 

78
06:41:19,760 --> 06:41:22,880
and only then the results can be formed and returned to the client 

79
06:41:24,130 --> 06:41:26,490
now this might seem like a lot of extra work 

80
06:41:27,510 --> 06:41:32,440
however remember the shaded part of the query is executed in parallel 

81
06:41:32,440 --> 06:41:34,830
which is the essence of dealing with large data 

82
06:41:37,350 --> 06:41:42,770
now at this point you might be thinking wait a minute what if i had 100 machines 

83
06:41:43,940 --> 06:41:46,800
and the desired data is only in 20 of them 

84
06:41:48,340 --> 06:41:52,870
should we needlessly go through all 100 machines find nothing in 80 of them and

85
06:41:52,870 --> 06:41:55,080
return 0 results from those machines 

86
06:41:55,080 --> 06:41:56,760
then why do the extra work 

87
06:41:56,760 --> 06:41:57,790
can it not be avoided 

88
06:41:59,560 --> 06:42:03,280
well to do this it would need one more piece in the solution 

89
06:42:03,280 --> 06:42:04,700
it called an index structure 

90
06:42:05,990 --> 06:42:07,290
very simply 

91
06:42:07,290 --> 06:42:11,930
an index can be thought of as a reverse table where given the value in a column 

92
06:42:11,930 --> 06:42:16,220
you would get back the records where the value appears as shown in the figure here 

93
06:42:17,810 --> 06:42:22,147
using an index speeds up query processing significantly 

94
06:42:22,147 --> 06:42:25,940
with indexes we can solve this problem in many different ways 

95
06:42:28,110 --> 06:42:32,930
the top table shows the case where each machine has its own index for

96
06:42:32,930 --> 06:42:34,400
the manf column 

97
06:42:34,400 --> 06:42:38,980
this is called a local index because the index is in every machine

98
06:42:38,980 --> 06:42:42,050
that holds the data for that table on that machine 

99
06:42:44,070 --> 06:42:46,670
in this case looking up heineken in the index 

100
06:42:46,670 --> 06:42:48,950
we would know which records would have the data 

101
06:42:50,500 --> 06:42:55,750
since the index is local the main query will indeed go to all machines but

102
06:42:55,750 --> 06:42:59,660
the lookup will be really instant and the empty results would return very quickly 

103
06:43:02,010 --> 06:43:04,410
in the second case we adopted a different solution 

104
06:43:05,610 --> 06:43:11,500
here there is an index on the main machine all on a separate index server 

105
06:43:11,500 --> 06:43:14,720
now when we place a data record in a machine 

106
06:43:14,720 --> 06:43:18,959
this index keeps an account of the machine that contains the record with that value 

107
06:43:20,240 --> 06:43:21,720
look at the second table to the right 

108
06:43:23,510 --> 06:43:28,398
given the value of heineken we know that it is only in three machines 

109
06:43:28,398 --> 06:43:32,960
and therefore we can avoid going to the other machines 

110
06:43:32,960 --> 06:43:36,840
clearly we can always use both indexing schemes 

111
06:43:36,840 --> 06:43:40,736
this will use more space but queries will be faster 

112
06:43:40,736 --> 06:43:45,717
now this gives you some of the choices you may need to make with big data 

113
06:43:45,717 --> 06:43:50,220
whether you use a parallel dbms or a distributed data solution 

1
13:19:42,850 --> 13:19:46,240
welcome to big data integration and processing 

2
13:19:46,240 --> 13:19:49,190
 welcome to course three of the big data specialization 

3
13:19:49,190 --> 13:19:50,710
i am amarnath gupta 

4
13:19:50,710 --> 13:19:52,650
 and i am ilkay altintas 

5
13:19:52,650 --> 13:19:55,590
we are really excited to work with you in this course

6
13:19:55,590 --> 13:20:00,820
to develop your understanding and skills in big data integration and processing 

7
13:20:00,820 --> 13:20:05,180
by now you might have just finished our first two courses and

8
13:20:05,180 --> 13:20:08,870
learned the basics of big data modelling and management 

9
13:20:08,870 --> 13:20:11,480
if you have not it not required 

10
13:20:11,480 --> 13:20:14,560
but for those that less background in the data modelling and

11
13:20:14,560 --> 13:20:18,680
data management areas you might find it valuable 

12
13:20:18,680 --> 13:20:23,030
 we understand that you may not have any background on data management 

13
13:20:23,030 --> 13:20:28,382
we are going to introduce query languages we will first look at sql in some detail 

14
13:20:28,382 --> 13:20:32,740
and then move to query languages for mongodb which is a semi structured

15
13:20:32,740 --> 13:20:37,260
data management system and aerospike which is a key value store 

16
13:20:38,740 --> 13:20:43,200
 we will also introduce concepts related to processing of big data as

17
13:20:43,200 --> 13:20:44,890
big data pipelines 

18
13:20:44,890 --> 13:20:49,160
we talk about data structures and transformations related to batch and

19
13:20:49,160 --> 13:20:52,230
stream processing as steps in a pipeline 

20
13:20:52,230 --> 13:20:55,100
providing us a way to talk about big data processing

21
13:20:55,100 --> 13:20:58,180
without getting into the details of the underlying technologies 

22
13:20:59,630 --> 13:21:04,710
once we have reviewed the concepts and related systems we will switch gears

23
13:21:04,710 --> 13:21:09,370
to hands on exercises with spark one of the most popular big data engines 

24
13:21:10,470 --> 13:21:15,060
we will show you examples of patch and stream processing using spark 

25
13:21:16,780 --> 13:21:20,410
 as you know for many data science applications

26
13:21:20,410 --> 13:21:25,140
one has to use many different databases and analyze the integrated data 

27
13:21:25,140 --> 13:21:31,160
in fact data integration is one leading cause leading to the bigness of data 

28
13:21:32,410 --> 13:21:35,780
we will give you a rapid exposure to information integration systems

29
13:21:35,780 --> 13:21:40,530
through use cases and point out the big data aspects one should pay attention to 

30
13:21:41,760 --> 13:21:47,620
 we are also excited to show you examples of data processing using splunk 

31
13:21:47,620 --> 13:21:51,510
our goal here is to provide you with simple hands on exercises

32
13:21:51,510 --> 13:21:53,550
that require no programming but

33
13:21:53,550 --> 13:21:59,660
show you how one can use interfaces like splunk to manage and process big data 

34
13:21:59,660 --> 13:22:01,820
we wish you a fun time learning and

35
13:22:01,820 --> 13:22:07,080
hope to hear from you in the discussions forums and learner stories as usual 

36
13:22:07,080 --> 13:22:08,880
 well happy learning and think big data

1
02:41:51,627 --> 02:41:56,359
in this video we will talk about the challenges of ingesting and

2
02:41:56,359 --> 02:42:01,178
processing big data and remind ourselves why need any paradigm and

3
02:42:01,178 --> 02:42:03,690
programming models for big data 

4
02:42:04,900 --> 02:42:09,710
after this video you will be able to summarize the requirements of programming

5
02:42:09,710 --> 02:42:12,650
models for big data and why you should care about them 

6
02:42:13,730 --> 02:42:16,800
you will also be able to explain how the challenges of big

7
02:42:16,800 --> 02:42:21,750
data related to its variety volume and velocity affects its processing 

8
02:42:25,090 --> 02:42:30,880
before we start let imagine an online gaming newscase 

9
02:42:30,880 --> 02:42:33,870
just like the one we have for catch the pink flamingo 

10
02:42:36,140 --> 02:42:41,123
you just introduced the game and users started signing up 

11
02:42:41,123 --> 02:42:44,101
you start with a traditional relational database 

12
02:42:44,101 --> 02:42:46,950
keeping track of user sessions and other events 

13
02:42:49,130 --> 02:42:54,030
your game server receives an event notification every time

14
02:42:54,030 --> 02:42:57,710
a user opens his session and makes a point in the game 

15
02:42:58,990 --> 02:43:02,690
initially everything is great your game is working and

16
02:43:02,690 --> 02:43:06,230
the database is able to handle the event streams coming into the server 

17
02:43:07,490 --> 02:43:12,960
however suddenly your game becomes highly popular a good problem to have 

18
02:43:14,890 --> 02:43:18,670
the database management system in your game server wo not be able to

19
02:43:18,670 --> 02:43:20,660
handle the load anymore 

20
02:43:20,660 --> 02:43:24,630
you start getting errors that the events cannot be inserted into the database

21
02:43:24,630 --> 02:43:26,240
at the speed they are coming in 

22
02:43:27,650 --> 02:43:34,240
you decide that you will have a buffer or a queue to process the advancing chunks 

23
02:43:34,240 --> 02:43:39,600
maybe also at the same time processing them to be organized in windows of time or

24
02:43:39,600 --> 02:43:40,260
game sessions 

25
02:43:42,390 --> 02:43:47,916
however in time as the demand goes up you will need more processing nodes and

26
02:43:47,916 --> 02:43:51,638
even more database servers that can handle the load 

27
02:43:51,638 --> 02:43:57,042
this is a typical scenario that most web sites face when confronted

28
02:43:57,042 --> 02:44:02,373
with big data issues related to volume and velocity of information 

29
02:44:02,373 --> 02:44:04,894
as this scenario demonstrates 

30
02:44:04,894 --> 02:44:09,580
solving the problem in one step might be possible initially 

31
02:44:09,580 --> 02:44:14,382
but the more reactive fixes the game developers add the system

32
02:44:14,382 --> 02:44:18,630
becomes less robust and more complicated to evolve 

33
02:44:20,880 --> 02:44:24,210
while the developers initially started with an application and

34
02:44:24,210 --> 02:44:25,360
the database to manage 

35
02:44:26,400 --> 02:44:30,614
now they have to manage a number of issues related to this

36
02:44:30,614 --> 02:44:35,920
infrastructure management just to keep up with the load on the system 

37
02:44:35,920 --> 02:44:41,282
similarly the database servers can be effected and corrupted 

38
02:44:41,282 --> 02:44:46,150
the replication and fault tolerance of them need to be handled separately 

39
02:44:47,240 --> 02:44:50,745
let start by going through these issues 

40
02:44:50,745 --> 02:44:54,300
let say one of the processing nodes went down 

41
02:44:55,470 --> 02:45:00,970
the system needs to manage and restart the processing and

42
02:45:00,970 --> 02:45:03,920
there will be potentially some data loss in the meantime 

43
02:45:05,400 --> 02:45:08,560
the system would need to check every processing node

44
02:45:08,560 --> 02:45:10,020
before it can discard data 

45
02:45:11,040 --> 02:45:17,373
each note and each database has to be replicated separately 

46
02:45:17,373 --> 02:45:24,255
batch computations that need data from multiple data servers need to access and

47
02:45:24,255 --> 02:45:31,453
maintain use of the data separately which might end up being quite slow and costly 

48
02:45:31,453 --> 02:45:35,705
big data processing techniques we will address in this course 

49
02:45:35,705 --> 02:45:40,430
will help you to reduce the management of the mentioned complexities 

50
02:45:40,430 --> 02:45:44,226
including failing servers and breaking compute nodes 

51
02:45:44,226 --> 02:45:49,840
while helping with the scalability of the management and processing infrastructure 

52
02:45:51,610 --> 02:45:56,170
we will talk about using big data systems like spark to achieve data parallel

53
02:45:56,170 --> 02:46:01,130
processing scalability for data applications on commodity clusters 

54
02:46:02,450 --> 02:46:07,080
we will use to spark runtime libraries and programming models to

55
02:46:07,080 --> 02:46:11,230
demonstrate how big data systems can be used for application management 

56
02:46:12,630 --> 02:46:17,260
to summarize what our imaginary game application needs from big data system 

57
02:46:18,830 --> 02:46:24,021
first of all there needs to be a way to use common big data operations

58
02:46:24,021 --> 02:46:28,778
to manage and split large volumes of events data streaming in 

59
02:46:28,778 --> 02:46:32,740
this means the partitioning and placement of data in and

60
02:46:32,740 --> 02:46:38,400
out of computer memory along with a model to synchronize the datasets later on 

61
02:46:39,960 --> 02:46:43,410
the access to data should be achieved in a fast way 

62
02:46:45,060 --> 02:46:47,710
the game developers need to be able to deploy

63
02:46:47,710 --> 02:46:52,630
many event processing jobs to distributed processing nodes at once 

64
02:46:52,630 --> 02:46:56,350
and these are potentially the data nodes we move the computations to 

65
02:46:57,850 --> 02:47:02,430
it should also enable reliability of the computing and

66
02:47:02,430 --> 02:47:05,190
enable fault tolerance from failures 

67
02:47:05,190 --> 02:47:08,998
this means enabling programmable replications and

68
02:47:08,998 --> 02:47:11,685
recovery of event data when needed 

69
02:47:11,685 --> 02:47:13,119
it should be easily

70
02:47:13,119 --> 02:47:18,238
scalable to a distributed set of nodes where the data gets produced 

71
02:47:18,238 --> 02:47:21,639
it should also enable scaling out 

72
02:47:21,639 --> 02:47:27,963
scaling out is simply adding new resources like distributed computers to

73
02:47:27,963 --> 02:47:33,681
process more or faster data at scale without losing performance 

74
02:47:33,681 --> 02:47:36,360
there are many data types in an online game 

75
02:47:37,360 --> 02:47:40,440
although we talked about time click events and

76
02:47:40,440 --> 02:47:45,390
scores it would be easy to imagine there are graphs of players 

77
02:47:45,390 --> 02:47:48,790
text - based chats and images that need to be processed 

78
02:47:50,390 --> 02:47:53,700
our big data system should enable processing of such

79
02:47:53,700 --> 02:47:58,500
a mixed variety of data and potentially optimize handling of

80
02:47:58,500 --> 02:48:01,870
each type separately as well as together when needed 

81
02:48:04,180 --> 02:48:08,750
in addition our system should have been able both streaming and

82
02:48:08,750 --> 02:48:14,140
batch processing enabling all the processing to be debuggable and

83
02:48:14,140 --> 02:48:16,950
extensible with minimal effort 

84
02:48:16,950 --> 02:48:21,110
that means being able to handle operations at small chunks of data

85
02:48:21,110 --> 02:48:25,450
streams with minimal delay that is what we call low latency 

86
02:48:26,810 --> 02:48:33,050
while at the same time handle processing of potentially all available data

87
02:48:33,050 --> 02:48:38,370
in batch form and all through the same system architecture 

88
02:48:40,160 --> 02:48:45,330
latency is a word that we use and hear a lot in big data processing 

89
02:48:46,370 --> 02:48:51,340
here we refer to how fast the data is being processed or simply

90
02:48:51,340 --> 02:48:58,400
the difference between production or event time and processing time of a data entry 

91
02:48:58,400 --> 02:49:02,560
in other words latency is quantification

92
02:49:02,560 --> 02:49:06,330
of the delay in the processing of the streaming data in the system 

93
02:49:08,400 --> 02:49:11,700
while some big data systems are good at it 

94
02:49:11,700 --> 02:49:16,220
hadoop for instance is not a great choice for operations that require low latency 

95
02:49:18,540 --> 02:49:21,962
let finish by remembering the real reasons for

96
02:49:21,962 --> 02:49:25,310
all these requirements of big data processing 

97
02:49:25,310 --> 02:49:30,559
making a different from processing in a traditional data architecture 

98
02:49:30,559 --> 02:49:36,148
big data has varying volume and velocity requiring the dynamic and

99
02:49:36,148 --> 02:49:39,575
scalable batch and stream processing 

100
02:49:39,575 --> 02:49:44,048
big data has a variety requiring management of data in many

101
02:49:44,048 --> 02:49:48,626
different data systems and integration of it all at scale 

1
05:31:38,250 --> 05:31:40,485
next we will describe aggregation functions 

2
05:31:41,760 --> 05:31:44,190
we have seen the first query before 

3
05:31:44,190 --> 05:31:48,460
select count simply translates to a count function 

4
05:31:49,710 --> 05:31:55,351
now we could also say db drinkers find count 

5
05:31:55,351 --> 05:31:57,880
but using count directly is more straightforward 

6
05:32:00,190 --> 05:32:04,720
now let ask to count the number of unique addresses for drinkers 

7
05:32:06,160 --> 05:32:09,740
so we do not care what the address is 

8
05:32:09,740 --> 05:32:11,210
we just care if it exists 

9
05:32:12,660 --> 05:32:18,605
this is accomplished through the exists : true expression 

10
05:32:18,605 --> 05:32:22,590
thus if an address exists for a drinker it will be counted 

11
05:32:24,740 --> 05:32:29,190
another area where we need to count is when we have an array valued attribute 

12
05:32:29,190 --> 05:32:30,040
like places 

13
05:32:31,580 --> 05:32:36,200
if we just want the number of elements in the raw list 

14
05:32:36,200 --> 05:32:42,267
we will write db country findplaces length and we will get six 

15
05:32:42,267 --> 05:32:47,538
however if we want distinct values we will use distinct instead of find and then

16
05:32:47,538 --> 05:32:52,531
use the length for counting the number of distinct elements in this case 4 

17
05:32:54,927 --> 05:33:01,178
now mongodb uses an internal machinery called the aggregation framework 

18
05:33:01,178 --> 05:33:06,208
which is modeled on the concept of data processing pipelines 

19
05:33:06,208 --> 05:33:11,840
that means documents enter a multi - stage pipeline which transforms

20
05:33:11,840 --> 05:33:17,286
the documents at each stage until it becomes an aggregated result 

21
05:33:17,286 --> 05:33:21,078
now we have seeing a similar mechanism for relational data 

22
05:33:21,078 --> 05:33:25,440
the aggregation pipelines starts by using the aggregate primitive 

23
05:33:26,680 --> 05:33:32,480
the most basic pipeline stages provides filters that operate like queries and

24
05:33:32,480 --> 05:33:36,130
the document transformations that modify the form of the output document 

25
05:33:37,310 --> 05:33:43,890
the primary filter operation is match which is followed by a query condition 

26
05:33:43,890 --> 05:33:47,401
in this case status is a 

27
05:33:47,401 --> 05:33:51,652
and expectedly the match operation produces a smaller number of documents

28
05:33:51,652 --> 05:33:53,500
to be processed at the next stage 

29
05:33:54,960 --> 05:33:59,730
this is usually followed by the group operation 

30
05:33:59,730 --> 05:34:03,040
now this operation needs to know which attributes should be grouped together 

31
05:34:04,130 --> 05:34:08,258
in the example here cust id is the grouping attribute so

32
05:34:08,258 --> 05:34:12,750
it is passed as a parameter to the group function 

33
05:34:12,750 --> 05:34:15,143
now notice the syntax 

34
05:34:15,143 --> 05:34:21,649
 id : cust id says that the grouped data will have an id attribute 

35
05:34:21,649 --> 05:34:25,573
and its value will be picked from the cust id

36
05:34:25,573 --> 05:34:30,128
attribute from the previous stage of computation 

37
05:34:30,128 --> 05:34:34,466
thus the before the cust id is telling the system that

38
05:34:34,466 --> 05:34:38,990
cust id is a known variable in the system and not a constant 

39
05:34:38,990 --> 05:34:41,639
the group operation also needs a reducer 

40
05:34:41,639 --> 05:34:47,250
which is a function that operates on an activity to produce an aggregate result 

41
05:34:47,250 --> 05:34:50,590
in this case the reduce function is sum 

42
05:34:51,980 --> 05:34:54,720
which operates on the amount attribute from the previous stage 

43
05:34:55,790 --> 05:35:00,560
like cust id we use amount to indicate that it a variable 

44
05:35:02,070 --> 05:35:04,590
as we saw in the relational case 

45
05:35:04,590 --> 05:35:09,580
data can be partitioned into chunks on the same machine or on different machines 

46
05:35:09,580 --> 05:35:12,320
these chunks are called chards 

47
05:35:12,320 --> 05:35:19,284
the aggregation pipeline of mongodb can operate on a charded collection 

48
05:35:19,284 --> 05:35:24,156
the grouping operation in mongodb can accept multiple attributes like

49
05:35:24,156 --> 05:35:25,650
the four shown here 

50
05:35:25,650 --> 05:35:30,915
also shown is a post grouping directive to sort on the basis of two attributes 

51
05:35:32,380 --> 05:35:37,150
the first is a computed count variable in ascending order 

52
05:35:37,150 --> 05:35:40,728
so the one designates the ascending order 

53
05:35:40,728 --> 05:35:42,870
the next sorting attribute is secondary 

54
05:35:42,870 --> 05:35:45,966
that means if two groups have the same value for count 

55
05:35:45,966 --> 05:35:50,110
then they will be further sorted based on the category value 

56
05:35:50,110 --> 05:35:55,860
but this time the order is descending because of the - 1 directive

57
05:35:59,207 --> 05:36:04,280
in course two we have seen solar a text search engine from apache 

58
05:36:05,410 --> 05:36:09,830
mongodb has a built in text search engine which can be invoked through the same

59
05:36:09,830 --> 05:36:11,360
aggregation framework we saw before 

60
05:36:12,360 --> 05:36:17,006
imagine that mongodb documents in this case are really text documents placed in

61
05:36:17,006 --> 05:36:18,730
a collection called articles 

62
05:36:20,270 --> 05:36:25,560
in this case the match directive of the aggregate function must be told

63
05:36:25,560 --> 05:36:28,955
it going to perform a text function on the article corpus 

64
05:36:30,320 --> 05:36:32,884
the actual text function is search 

65
05:36:32,884 --> 05:36:38,080
we set search terms like hillary democrat such that having

66
05:36:38,080 --> 05:36:44,210
either term in a document will satisfy the search requirement 

67
05:36:44,210 --> 05:36:48,260
as is the case of any text engine 

68
05:36:48,260 --> 05:36:52,410
the results of any search returns a list of documents each with a score 

69
05:36:53,600 --> 05:36:58,780
the next task is to tell mongdb to sort the results based on textscore 

70
05:37:00,050 --> 05:37:01,290
what is the meta here 

71
05:37:02,420 --> 05:37:06,532
meta stands for metadata that is additional information 

72
05:37:06,532 --> 05:37:10,860
remember that the aggregation operations are executed in a pipeline 

73
05:37:12,090 --> 05:37:15,705
any step in the pipeline can produce some extra data or

74
05:37:15,705 --> 05:37:18,780
metadata for each processed document 

75
05:37:18,780 --> 05:37:22,470
in this example the metadata produced by the search function

76
05:37:22,470 --> 05:37:25,670
is a computed attribute called textscore 

77
05:37:25,670 --> 05:37:31,627
so this directive tells the system to pick up this specific metadata attribute and

78
05:37:31,627 --> 05:37:36,828
use it to populate the score attribute which would be used for sorting 

79
05:37:36,828 --> 05:37:41,501
finally the project class does exactly what is expected 

80
05:37:41,501 --> 05:37:46,006
it tells the system to output only the title of each document and

81
05:37:46,006 --> 05:37:47,284
suppress its id 

82
05:37:51,760 --> 05:37:56,120
the last item in our discussion of mongodb is join 

83
05:37:56,120 --> 05:37:59,820
we have seen that join is a vital operation for data management operations 

84
05:38:02,010 --> 05:38:09,062
interestingly mongodb introduced this equivalent of join only in version 3 2 

85
05:38:09,062 --> 05:38:13,320
so the joining in mongodb also happens in the aggregation framework 

86
05:38:14,740 --> 05:38:17,420
there are a few ways of expressing joins in mongodb 

87
05:38:18,470 --> 05:38:23,980
we show one here that explicitly performs a join to a function called look up 

88
05:38:25,690 --> 05:38:28,445
we use an example form the mongodb documentation 

89
05:38:28,445 --> 05:38:35,380
now here are two document collections order and inventory 

90
05:38:35,380 --> 05:38:40,600
notice that the item key in orders has values abc jkl etc 

91
05:38:41,830 --> 05:38:47,100
and the sku key in the inventory has comparable values abc def etc 

92
05:38:47,100 --> 05:38:49,270
so these two are joinable by value 

93
05:38:50,550 --> 05:38:56,167
the way to specify this join one can use this query 

94
05:38:56,167 --> 05:39:01,124
the db orders aggregate declaration states that orders is

95
05:39:01,124 --> 05:39:04,311
sort of the home or local collection 

96
05:39:04,311 --> 05:39:09,592
now in the aggregate the function lookup needs to know what to look up for

97
05:39:09,592 --> 05:39:11,420
each document in orders 

98
05:39:13,050 --> 05:39:18,010
the from attribute specifies the name of the collection as inventory 

99
05:39:19,590 --> 05:39:21,410
the next two parameters are the local and

100
05:39:21,410 --> 05:39:25,591
foreign matching keys which are item and sku respectively 

101
05:39:26,976 --> 05:39:31,760
the last item as : is a construction part of

102
05:39:31,760 --> 05:39:36,380
the join operation which says how to structure the match items into the result 

103
05:39:37,780 --> 05:39:40,770
now before we show you the results let see what should match 

104
05:39:42,740 --> 05:39:47,020
the abc item in order should match the abc in sku 

105
05:39:48,120 --> 05:39:53,050
similarly the jkl item should match the jkl in sku 

106
05:39:54,260 --> 05:39:56,540
okay but there is one more twist 

107
05:39:58,020 --> 05:39:59,920
here is the actual result 

108
05:39:59,920 --> 05:40:04,090
the first two records show exactly what we expect 

109
05:40:04,090 --> 05:40:06,760
there is a new field called inventory - docs in the batching record 

110
05:40:08,640 --> 05:40:11,580
the third record however shows something interesting 

111
05:40:13,170 --> 05:40:17,250
inventory has two records shown here what do they match 

112
05:40:18,670 --> 05:40:23,890
now they match the empty document in orders because

113
05:40:23,890 --> 05:40:27,700
orders has a document whose item field is null 

114
05:40:28,980 --> 05:40:34,355
so it matches documents and inventory where the sku item is also null 

115
05:40:34,355 --> 05:40:39,290
explicitly as in document 5 or implicitly as in document 6 

116
05:40:41,801 --> 05:40:45,948
this concludes our discussion of queries in the context of mongodb 

1
11:12:22,720 --> 11:12:27,150
in this hands on activity we will be using pandas to read csv files and

2
11:12:27,150 --> 11:12:29,300
perform various queries on them 

3
11:12:29,300 --> 11:12:32,070
pandas is a data analysis library for python 

4
11:12:34,140 --> 11:12:36,730
first we will create a new jupyter python notebook 

5
11:12:38,070 --> 11:12:42,060
next we will use pandas to read a csv file into a dataframe 

6
11:12:43,375 --> 11:12:47,490
we will then view the contents of the dataframe and see how to filter rows and

7
11:12:47,490 --> 11:12:48,130
columns of it 

8
11:12:49,600 --> 11:12:53,410
next we will perform average and sum operations on the dataframe 

9
11:12:53,410 --> 11:12:58,760
and finally show how to merge two dataframes by joining on a single column 

10
11:13:01,160 --> 11:13:01,660
let begin 

11
11:13:03,420 --> 11:13:06,370
we will start by creating a new ipython notebook 

12
11:13:06,370 --> 11:13:10,570
clicking on new and selecting python 3 under notebooks 

13
11:13:13,900 --> 11:13:18,350
first we will import the pandas library by writing import pandas 

14
11:13:20,010 --> 11:13:22,710
remember that in ipython notebooks to run a command 

15
11:13:22,710 --> 11:13:24,980
we hold down the shift key and hit enter 

16
11:13:28,599 --> 11:13:34,395
next let read buyclicks csv into a pandas dataframe 

17
11:13:34,395 --> 11:13:36,730
we will put it in a variable called buyclicksdf 

18
11:13:40,135 --> 11:13:44,751
we will read it using pandas read csv 

19
11:13:44,751 --> 11:13:49,630
and we will read the buy - clicks csv file 

20
11:13:52,966 --> 11:13:56,870
we can see the contents of the file by just running the variable by itself 

21
11:14:01,948 --> 11:14:06,398
and notice that the file has many rows and then ipython truncates this 

22
11:14:06,398 --> 11:14:07,740
the dot dot dot 

23
11:14:11,227 --> 11:14:14,027
we can see only the top five rows by calling head 5 

24
11:14:15,417 --> 11:14:20,343
next let look at only two

25
11:14:20,343 --> 11:14:26,500
columns in the buyclicks data 

26
11:14:26,500 --> 11:14:28,310
let look at price and user id 

27
11:14:29,330 --> 11:14:35,680
we can do these by first entering buyclicks dataframe in the same text for

28
11:14:35,680 --> 11:14:41,040
specifying only certain columns to show is open bracket open bracket and

29
11:14:41,040 --> 11:14:42,950
then the name of the columns you want to view 

30
11:14:42,950 --> 11:14:47,700
so want your price and user id and again 

31
11:14:47,700 --> 11:14:53,395
we only want to see the first five rows so we will do head 5 

32
11:14:57,016 --> 11:15:01,350
now let query the buyclicks data for only the prices less than 3 

33
11:15:01,350 --> 11:15:06,494
first we will enter buyclicksdf one square bracket 

34
11:15:06,494 --> 11:15:11,007
to filter our particular column we enter buyclicksdf and then column name 

35
11:15:12,440 --> 11:15:16,440
now we specify the limit of the query by entering <3 

36
11:15:24,378 --> 11:15:28,080
this shows first five rows where the price is less than three 

37
11:15:30,409 --> 11:15:34,205
we can also perform aggregate operations on panda dataframes 

38
11:15:35,650 --> 11:15:43,673
we can sum all the price data by entering buyclicksdf price sum 

39
11:15:47,760 --> 11:15:51,470
another aggregate operation we can perform is looking at the average 

40
11:15:51,470 --> 11:15:53,820
let look at the average for price 

41
11:15:53,820 --> 11:15:55,540
the function is called mean 

42
11:15:56,720 --> 11:16:02,782
so once your buyclicksdf price mean 

43
11:16:07,341 --> 11:16:10,460
can also join two dataframes on a single column 

44
11:16:11,870 --> 11:16:16,620
first let read in another csv into a different dataframe 

45
11:16:16,620 --> 11:16:19,700
we will read in adclicks csv 

46
11:16:19,700 --> 11:16:26,723
so we will says adclicksdf = pandas read csv 

47
11:16:26,723 --> 11:16:32,286
we will say ad - clicks csv 

48
11:16:38,538 --> 11:16:42,870
to verify that we read this data successfully let look at the contents 

49
11:16:51,077 --> 11:16:54,063
now let combine the buyclicks dataframe and

50
11:16:54,063 --> 11:16:56,700
the adclicks data frame on the user id call 

51
11:16:58,690 --> 11:17:01,480
we will put the result in the new dataframe called mergedf 

52
11:17:01,480 --> 11:17:08,400
so we will say mergedf = adclicksdf merge

53
11:17:12,465 --> 11:17:16,820
then we need to say which dataframe we are merging with 

54
11:17:16,820 --> 11:17:21,590
buyclicksdf and the column that we are merging on 

55
11:17:21,590 --> 11:17:24,080
so we will say on = userid 

56
11:17:30,461 --> 11:17:33,681
finally we can look at the contents with this merged dataframe 

1
22:29:56,510 --> 22:30:00,380
so now from mongodb we will go to aerospike which is a key value store 

2
22:30:02,030 --> 22:30:04,120
key value stores typically offer an api 

3
22:30:05,480 --> 22:30:09,630
that is the way to access data using a programming language like python or java 

4
22:30:10,850 --> 22:30:14,620
we will take a very brief look at aerospike which offers both

5
22:30:14,620 --> 22:30:18,550
a programmatic access and a limited amount of query access to data 

6
22:30:20,550 --> 22:30:22,910
the data model of aerospike is illustrated here 

7
22:30:24,310 --> 22:30:29,960
data are organized in lean spaces which can be in memory or on flash disks 

8
22:30:31,410 --> 22:30:33,840
name spaces are top level data containers 

9
22:30:34,980 --> 22:30:40,080
the way you collect data in name spaces relates to how data is stored and managed 

10
22:30:41,270 --> 22:30:45,395
so name space contains records indexes and policies 

11
22:30:45,395 --> 22:30:50,130
now policies dictate name space behavior like how data is stored 

12
22:30:50,130 --> 22:30:52,790
whether it on ram or disk or

13
22:30:52,790 --> 22:30:57,150
how how many replicas exist for a record and when records expire 

14
22:30:59,840 --> 22:31:03,606
a name space can contain sets you can think of them as tables 

15
22:31:03,606 --> 22:31:06,470
so here there are two sets people and places 

16
22:31:06,470 --> 22:31:10,140
and a set of records which are not in any set 

17
22:31:11,750 --> 22:31:15,620
within a record data is stored in one or many bins 

18
22:31:16,980 --> 22:31:20,490
bins consist of a name and a value 

19
22:31:22,820 --> 22:31:26,970
the example written here is in java and

20
22:31:26,970 --> 22:31:29,230
you do not have to know java to follow the main point here 

21
22:31:31,068 --> 22:31:35,470
here we are creating indexes on key value data that handled by aerospike 

22
22:31:36,780 --> 22:31:38,500
this data set comes from twitter 

23
22:31:39,990 --> 22:31:45,310
each field of a tweet is extracted and put into aerospike as a key value pair 

24
22:31:45,310 --> 22:31:50,420
so we declare the namespace to be

25
22:31:50,420 --> 22:31:54,880
example and the record set to be tweet 

26
22:31:56,510 --> 22:32:02,460
the name of the index to be testindex and the name of the bin as user name 

27
22:32:04,490 --> 22:32:09,330
since this index is stored on disk an sql - like command 

28
22:32:09,330 --> 22:32:13,638
like show index shows the content of the index as you can see here 

29
22:32:13,638 --> 22:32:18,220
this routine shows

30
22:32:18,220 --> 22:32:22,419
how data can be inserted into aerospike programmatically 

31
22:32:22,419 --> 22:32:26,500
again the goal is to point out a few salient aspects of data insertion

32
22:32:26,500 --> 22:32:28,090
regardless of the syntax of the language 

33
22:32:28,090 --> 22:32:33,120
now since this is a key value store one first needs to define the key 

34
22:32:35,220 --> 22:32:40,414
this line here says that the key in the namespace call example and set call

35
22:32:40,414 --> 22:32:45,699
tweet is the value of the function getid which returns the id of a tweet 

36
22:32:48,288 --> 22:32:52,028
when the data is populated we are essentially creating bins 

37
22:32:52,028 --> 22:32:56,500
here user name is the attribute and

38
22:32:56,500 --> 22:32:59,290
the screen name obtained from the tweet is the value 

39
22:33:00,510 --> 22:33:04,440
the actual insertion happens here in the client 

40
22:33:06,200 --> 22:33:10,574
the client put statement where we need to mention the key and

41
22:33:10,574 --> 22:33:13,510
the bins we have just created 

42
22:33:13,510 --> 22:33:16,260
now why are we inserting two bins at a time 

43
22:33:16,260 --> 22:33:18,140
two bins with two ids and user name 

44
22:33:19,150 --> 22:33:22,180
this is an idiosyncrasy of the aerospike client 

45
22:33:24,810 --> 22:33:29,190
after data is inserted one can create other data using aql

46
22:33:29,190 --> 22:33:30,450
which is very much like sql 

47
22:33:31,870 --> 22:33:36,920
this screenshot shows a part of the output of a simple select star query 

48
22:33:39,680 --> 22:33:44,100
now in your hands - on exercise you will be able to play with the aerospike data 

49
22:33:45,520 --> 22:33:51,740
this is just a screenshot showing the basic query syntax of aql 

50
22:33:51,740 --> 22:33:55,110
that is aerospike query language and a few examples 

51
22:33:56,370 --> 22:33:59,960
the last two lines show a couple of interesting features of the language 

52
22:34:01,090 --> 22:34:07,200
the operation between 0 and 99 is a nicer way of stating a range query 

53
22:34:07,200 --> 22:34:09,850
which gives a lower and upper limits on a variable 

54
22:34:11,030 --> 22:34:15,550
the last line shows the operation cost 

55
22:34:15,550 --> 22:34:19,670
which transforms one type of data to another type 

56
22:34:19,670 --> 22:34:24,047
here it transforms coordinates that is latitude and longitude data 

57
22:34:24,047 --> 22:34:28,784
to a json format called geojson which is designed to represent geographic data

58
22:34:28,784 --> 22:34:30,092
in a json structure 

59
22:34:33,762 --> 22:34:37,421
we will finish our coverage of queries with a quick reference to an advanced

60
22:34:37,421 --> 22:34:39,870
topic which is beyond the scope of this course 

61
22:34:41,068 --> 22:34:45,780
now you have seen in prior courses that streaming data is complex to process

62
22:34:45,780 --> 22:34:48,310
because a stream is infinite in nature 

63
22:34:49,480 --> 22:34:52,640
now does this have any impact on query languages and evaluation 

64
22:34:53,790 --> 22:34:56,840
the answer is that it absolutely does 

65
22:34:56,840 --> 22:34:58,590
we will mention one such impact here 

66
22:35:00,150 --> 22:35:03,700
this shows a pictorial depiction of streaming data 

67
22:35:04,890 --> 22:35:09,170
the data is segmented into five pieces shown in the white boxes in the upper row 

68
22:35:10,620 --> 22:35:15,280
these can be gathered for example every few seconds or every 200 data objects 

69
22:35:16,980 --> 22:35:19,970
the query defines a window to select

70
22:35:19,970 --> 22:35:22,770
key of these data objects as a unit of processing 

71
22:35:22,770 --> 22:35:23,860
here case three 

72
22:35:25,000 --> 22:35:29,120
so three units of data are picked up in a window unit 

73
22:35:29,120 --> 22:35:32,020
to get the next window it is moved by two units 

74
22:35:33,030 --> 22:35:35,280
this is called a slide of the window 

75
22:35:36,970 --> 22:35:41,030
since the window size is three and the slide is two 

76
22:35:41,030 --> 22:35:45,130
one unit of information overlaps between the two consecutive windows 

77
22:35:46,450 --> 22:35:51,056
the lower line in this diagram shows the initialized item let ignore it 

78
22:35:51,056 --> 22:35:58,980
followed by two window sets of data records for processing 

79
22:36:00,090 --> 22:36:03,670
thus the query language therefore will have to specify a query

80
22:36:03,670 --> 22:36:09,650
that an sql query over a window which is also specified in the query 

81
22:36:09,650 --> 22:36:13,980
now in a traffic data stream example the sql statement might look like this 

82
22:36:15,330 --> 22:36:18,490
where the window size is 30 second and

83
22:36:18,490 --> 22:36:21,520
the slide is the same size as window giving output every 30 seconds 

84
22:36:22,760 --> 22:36:27,650
so streaming data results in changes in both the query language and

85
22:36:27,650 --> 22:36:29,560
the way queries are processed 

1
21:06:24,008 --> 21:06:28,900
in this hands - on activity we will be querying documents in mongodb 

2
21:06:30,020 --> 21:06:34,480
first we will start a mongodb server and then run the mongodb shell 

3
21:06:34,480 --> 21:06:39,730
we will see how to show the databases and collections in the mongodb server 

4
21:06:39,730 --> 21:06:43,520
we will look at an example document in the database and

5
21:06:43,520 --> 21:06:45,529
see how to find distinct values for a field 

6
21:06:47,360 --> 21:06:50,540
next we will search for specific field values and

7
21:06:50,540 --> 21:06:52,620
see how to filter fields returned in a query 

8
21:06:54,530 --> 21:06:58,070
finally we will search using regular expressions and operators 

9
21:07:00,903 --> 21:07:05,361
let begin first we will start the mongodb server 

10
21:07:05,361 --> 21:07:10,830
open the terminal window by clicking on the terminal icon the top of the toolbar 

11
21:07:10,830 --> 21:07:17,680
we will cd into downloads big - data - 3 - mongodb 

12
21:07:17,680 --> 21:07:23,495
we will start the mongodb server by running

13
21:07:23,495 --> 21:07:29,675
 mongodb bin mongod - - dbpath db 

14
21:07:29,675 --> 21:07:32,930
the arguments dbpath db

15
21:07:32,930 --> 21:07:37,330
specified that the database on mongodb is in the directory named db 

16
21:07:39,260 --> 21:07:40,780
run this to start the server 

17
21:07:44,530 --> 21:07:48,413
next we will start the mongodb shell so that we can perform queries 

18
21:07:48,413 --> 21:07:53,416
we will open another terminal window 

19
21:07:53,416 --> 21:08:00,417
again cd into downloads big - data - 3 - mongodb 

20
21:08:00,417 --> 21:08:05,955
we will start the shell by running mongodb bin mongo 

21
21:08:13,734 --> 21:08:19,237
we can see what databases are available in the mongodb server by running the command 

22
21:08:19,237 --> 21:08:19,933
show dbs 

23
21:08:23,521 --> 21:08:26,680
we have created the sample database with json data from twitter 

24
21:08:28,400 --> 21:08:32,400
we can use the use command to change to this database 

25
21:08:32,400 --> 21:08:34,245
we will run use sample 

26
21:08:37,078 --> 21:08:40,828
we can see the collections in this database by running show collections 

27
21:08:43,858 --> 21:08:46,656
there only one collection called users 

28
21:08:46,656 --> 21:08:50,987
so all of the queries will be using the users collection 

29
21:08:50,987 --> 21:08:55,945
let count how many documents there are in the users collection by writing

30
21:08:55,945 --> 21:08:57,349
db users count 

31
21:09:02,225 --> 21:09:07,241
we can look at one of these documents by writing db users findone 

32
21:09:15,078 --> 21:09:19,443
this document contains jason from a twitter tweet 

33
21:09:19,443 --> 21:09:22,740
you can see the field names here with their values 

34
21:09:22,740 --> 21:09:26,380
there are also nested fields under the user field 

35
21:09:26,380 --> 21:09:28,410
and each of these fields also has values 

36
21:09:29,730 --> 21:09:32,870
you can find the distinct values for

37
21:09:32,870 --> 21:09:36,320
particular field by using the distinct command 

38
21:09:36,320 --> 21:09:39,058
let find the distinct values for the username field 

39
21:09:39,058 --> 21:09:45,234
we will write db users distinct user name 

40
21:09:54,324 --> 21:09:59,193
next let find all the documents in this collection where the field username

41
21:09:59,193 --> 21:10:00,990
matches the specific value 

42
21:10:02,050 --> 21:10:05,360
the value we will search for is actionsportsjax 

43
21:10:06,730 --> 21:10:10,851
we will run the command db users find 

44
21:10:10,851 --> 21:10:13,980
and the field name is username and

45
21:10:13,980 --> 21:10:18,688
the value we are searching for is actionsportsjax 

46
21:10:28,405 --> 21:10:31,050
results of this query is compressed all in one line 

47
21:10:32,650 --> 21:10:35,401
if we append the pretty to the previous query 

48
21:10:35,401 --> 21:10:37,874
we can see the formatted output 

49
21:10:37,874 --> 21:10:40,776
so we will run the same command with append pretty 

50
21:10:47,289 --> 21:10:49,980
we can filter the fields returned from the queries 

51
21:10:51,560 --> 21:10:55,709
let perform the same query again but only show the tweet id field 

52
21:10:57,200 --> 21:11:00,090
we can do this by adding a second argument to the find command 

53
21:11:01,440 --> 21:11:06,648
so we will run the same command again 

54
21:11:06,648 --> 21:11:13,827
but add a second argument saying tweet id : 1 

55
21:11:13,827 --> 21:11:19,748
the underscore id field is a primary key used in all documents in mongodb 

56
21:11:19,748 --> 21:11:22,280
you can turn this off by adding another field to our filter 

57
21:11:24,130 --> 21:11:27,287
we will run the same command again but turn of id 

58
21:11:35,820 --> 21:11:40,550
next we use the regular expression search to find strings containing the document 

59
21:11:41,680 --> 21:11:45,640
for example if we want to find all the tweets containing the text fifa 

60
21:11:45,640 --> 21:11:49,171
we can run db users find tweet text fifa 

61
21:11:56,170 --> 21:11:59,850
there are no results in this query because this query is searching for

62
21:11:59,850 --> 21:12:01,580
tweet text equals fifa 

63
21:12:03,030 --> 21:12:07,590
that is the entire contents and the value of tweet text must be fifa 

64
21:12:08,720 --> 21:12:10,110
instead if we want to look for

65
21:12:10,110 --> 21:12:15,230
fifa anywhere in the tweet text we can do a regular expression search 

66
21:12:15,230 --> 21:12:19,750
to do this replace the double quotes with slashes 

67
21:12:19,750 --> 21:12:21,850
so we will run the same command again 

68
21:12:21,850 --> 21:12:27,573
but replacing double quotes with slashes 

69
21:12:27,573 --> 21:12:31,423
we can count how many documents are returned by this query by running the same

70
21:12:31,423 --> 21:12:33,540
command again but appending count 

71
21:12:37,325 --> 21:12:41,948
we can also search for documents in mongodb where the field values are greater

72
21:12:41,948 --> 21:12:44,020
than or less than a certain value 

73
21:12:45,170 --> 21:12:50,162
for example lets find tweet mention count greater than six 

74
21:12:50,162 --> 21:12:56,977
we will run db users find and the field name is tweet mentioned count 

75
21:12:59,670 --> 21:13:05,265
and we want to look for the value where this field is greater than six 

76
21:13:05,265 --> 21:13:11,940
so i will enter a gt : 6 

1
18:19:36,500 --> 18:19:41,870
in a prior course we looked at json as an example of semi - structured data 

2
18:19:41,870 --> 18:19:45,020
and we demonstrated that json data can be thought of as a tree 

3
18:19:46,200 --> 18:19:49,180
in this course we will focus on querying json data 

4
18:19:50,330 --> 18:19:55,340
before we start lets review the details of the json structure and

5
18:19:55,340 --> 18:19:59,180
get an initial sense of how to query this form of data 

6
18:20:00,720 --> 18:20:03,590
let consider a simple json collection and

7
18:20:03,590 --> 18:20:07,330
look at the structures substructures actually it composed of 

8
18:20:09,160 --> 18:20:12,990
the atomic element in the structure is a key value pair for example 

9
18:20:12,990 --> 18:20:18,520
name is the key and sue is the value in this case an atomic string value 

10
18:20:20,560 --> 18:20:22,178
to query a key value pair 

11
18:20:22,178 --> 18:20:27,123
we should be able perform one basic operation given the key return the value 

12
18:20:29,140 --> 18:20:35,830
now the value can also be an array an array is a list 

13
18:20:37,060 --> 18:20:41,940
so the query operations on it can either be on its position in the list or

14
18:20:41,940 --> 18:20:43,370
on the value 

15
18:20:43,370 --> 18:20:47,980
thus we should be able to ask for the second element of the array called badges 

16
18:20:47,980 --> 18:20:52,180
or we should be able to seek objects of which the key called badges

17
18:20:52,180 --> 18:20:53,270
has a value of blue 

18
18:20:55,130 --> 18:21:00,360
notice here that the document collection here is itself an array 

19
18:21:00,360 --> 18:21:04,130
within square brackets and it just two elements in it 

20
18:21:04,130 --> 18:21:08,664
the top level array does not have a key by default it called db 

21
18:21:11,820 --> 18:21:16,135
these key value peers are structured as tuples 

22
18:21:16,135 --> 18:21:19,607
often with a name in the snippet shown 

23
18:21:19,607 --> 18:21:23,644
favorites has a tuple of two key value pairs 

24
18:21:23,644 --> 18:21:28,530
now tuples can be thought of as relational records as the operations

25
18:21:28,530 --> 18:21:32,800
would include projection of an attribute and selection over a set of tables 

26
18:21:35,330 --> 18:21:38,060
on the other hand the area called points 

27
18:21:38,060 --> 18:21:42,180
has two tuples these two tuples named 

28
18:21:42,180 --> 18:21:45,880
as you will see we will address these tuples by their positions 

29
18:21:47,570 --> 18:21:51,000
finally this one has nesting 

30
18:21:51,000 --> 18:21:55,110
that means a mini structure can be embedded within another structure 

31
18:21:56,250 --> 18:21:59,220
so we need operations that will let us navigate

32
18:21:59,220 --> 18:22:02,350
from one structure to any of it embedded structures 

33
18:22:05,030 --> 18:22:10,180
now just like a basic sql query states which parts of which records from one or

34
18:22:10,180 --> 18:22:14,840
more people should be reported a mongodb query states

35
18:22:14,840 --> 18:22:18,700
which parts of which documents from a document collection should be returned 

36
18:22:20,010 --> 18:22:26,088
the primary query is expressed as a find function which contains two arguments and

37
18:22:26,088 --> 18:22:31,494
an optional qualifier there are four things to notice in this function 

38
18:22:33,142 --> 18:22:35,773
the first is the term collection 

39
18:22:35,773 --> 18:22:40,402
this tells the system which document collection to use and

40
18:22:40,402 --> 18:22:46,410
therefore is roughly similar to the from clause when restricted to one table 

41
18:22:46,410 --> 18:22:53,629
so if the name of the collection is beers the first part would say db beers find 

42
18:22:53,629 --> 18:22:58,098
the second item is a query filter which lists all conditions that

43
18:22:58,098 --> 18:23:03,159
the retrieved documents should satisfy so it like a where clause 

44
18:23:05,170 --> 18:23:09,370
now if we want to return everything then this filter is left blank 

45
18:23:10,550 --> 18:23:14,719
otherwise we will fill it in a couple of ways shown in the next few slides 

46
18:23:16,330 --> 18:23:20,652
the third term is a projection class which is essentially a list of variables

47
18:23:20,652 --> 18:23:22,550
that we want to see in the output 

48
18:23:24,520 --> 18:23:28,950
the fourth and last item sits after the find function ends and

49
18:23:28,950 --> 18:23:32,370
is separated by a dot it called a cursor modifier 

50
18:23:33,370 --> 18:23:37,760
the word cursor relates back to sql where cursor is defined

51
18:23:37,760 --> 18:23:41,120
as a block of results that is returned to the user in one chunk 

52
18:23:42,410 --> 18:23:46,357
this becomes important when the set of results is too large to be returned all

53
18:23:46,357 --> 18:23:49,256
together and the user may need to specify how much or

54
18:23:49,256 --> 18:23:51,614
what portion of results they actually want 

55
18:23:53,710 --> 18:23:56,776
so we will start out with a few queries 

56
18:23:56,776 --> 18:24:01,156
where we show how the same query can be expressed in sql and

57
18:24:01,156 --> 18:24:06,220
in mongodb the first query wants everything from beers 

58
18:24:06,220 --> 18:24:09,740
the sql query is structured on the table beers and

59
18:24:09,740 --> 18:24:12,660
the select asks to return all rows 

60
18:24:13,740 --> 18:24:18,230
in mongodb the same query is more succincted 

61
18:24:18,230 --> 18:24:20,830
since the name of the collection is already specified in

62
18:24:20,830 --> 18:24:24,650
calling the find function the body of the find function is empty 

63
18:24:25,650 --> 18:24:30,020
that means there are no query conditions and no projection clauses in it 

64
18:24:30,020 --> 18:24:34,550
the second query

65
18:24:34,550 --> 18:24:39,160
needs to return the variables beer and price for all records 

66
18:24:40,810 --> 18:24:47,120
so the find function here needs an empty query condition denoted by the open and

67
18:24:47,120 --> 18:24:52,860
closed brace symbols but the projection clauses are specifically identified 

68
18:24:52,860 --> 18:24:56,760
there is a 1 if an attribute is output and a 0 if it is not 

69
18:24:56,760 --> 18:25:01,500
as a shortcut only variables with 1 are required 

70
18:25:03,000 --> 18:25:04,204
okay so when do you use 0 

71
18:25:05,360 --> 18:25:06,870
a common situation is the following 

72
18:25:08,160 --> 18:25:12,548
every mongodb document has an identifier named id 

73
18:25:15,139 --> 18:25:19,410
by default every query will return the id of the document 

74
18:25:20,700 --> 18:25:24,150
if you do not want it to return this designated attribute 

75
18:25:24,150 --> 18:25:30,970
you should explicitly say id : 0 

76
18:25:30,970 --> 18:25:33,630
next we will add query conditions 

77
18:25:33,630 --> 18:25:37,640
that is the equivalent of the where clause in sql 

78
18:25:37,640 --> 18:25:42,490
our query number three has the query condition where name is equal to a value 

79
18:25:43,600 --> 18:25:50,020
in mongodb that equal to translate to a variable colon value form 

80
18:25:51,070 --> 18:25:56,135
notice the symbol used for a string is quotes 

81
18:25:57,987 --> 18:26:00,720
query four is more interesting for two reasons 

82
18:26:01,770 --> 18:26:05,980
first we see a way in which the distinct operation is specified 

83
18:26:07,580 --> 18:26:13,670
notice here that the primary query function is not find any more but

84
18:26:13,670 --> 18:26:15,310
a new function called distinct 

85
18:26:16,790 --> 18:26:20,990
as we will see later again in our slides 

86
18:26:20,990 --> 18:26:24,660
mongodb uses a few special query functions for some of the operations 

87
18:26:26,520 --> 18:26:30,620
so you need to know which function should be used in what context 

88
18:26:30,620 --> 18:26:32,270
when you write mogodb queries 

89
18:26:33,640 --> 18:26:38,191
secondly in this query we have a non - equality condition 

90
18:26:38,191 --> 18:26:41,153
namely the price is greater than 15 

91
18:26:43,370 --> 18:26:47,418
this example shows mongodb style of using operators in a query 

92
18:26:47,418 --> 18:26:52,090
it always variable : 

93
18:26:52,090 --> 18:26:56,470
followed by mongodb name for the operator and then the comparison value 

94
18:26:57,580 --> 18:26:59,959
so where would you find mongodb operators 

95
18:27:01,620 --> 18:27:03,890
here are some of the operators supported in mongodb 

96
18:27:05,300 --> 18:27:08,860
these operators and others are listed in the url shown at the bottom 

97
18:27:10,210 --> 18:27:12,430
the operators shown here are color coded 

98
18:27:13,470 --> 18:27:16,866
the top blue set are the comparison operators 

99
18:27:16,866 --> 18:27:22,760
we see the gt greater than operation that we used in the last slide 

100
18:27:25,100 --> 18:27:29,780
the green colored operations are array operations which we will see shortly 

101
18:27:29,780 --> 18:27:33,370
and the yellow operators at the bottom are logical operations that

102
18:27:33,370 --> 18:27:37,070
combine two conditions in different ways like the and operation we saw in sql 

103
18:27:37,070 --> 18:27:42,860
now the last operator nor is interesting 

104
18:27:42,860 --> 18:27:48,110
because it is used to specify queries when neither of two conditions must hold 

105
18:27:48,110 --> 18:27:51,960
for example find all beer whose name is neither bad nor

106
18:27:51,960 --> 18:27:55,080
is the price less than 6 per bottle 

107
18:27:55,080 --> 18:27:58,021
now i would strongly encourage you to play with these operators in

108
18:27:58,021 --> 18:27:59,098
your hands on session 

109
18:28:02,220 --> 18:28:04,980
now i am sure you remember the like query in sql 

110
18:28:06,420 --> 18:28:12,000
mongodb uses regular expressions to specify partial string matches 

111
18:28:12,000 --> 18:28:14,120
now some of you may not know what a regular expression is 

112
18:28:14,120 --> 18:28:16,660
let first use some examples 

113
18:28:18,680 --> 18:28:23,150
the first example is the same query we saw before when we are asking for

114
18:28:23,150 --> 18:28:28,514
beer manufacturers whose name has a sub string a - m in it 

115
18:28:28,514 --> 18:28:31,620
so a - m can appear anywhere within the name 

116
18:28:32,630 --> 18:28:33,550
to do this 

117
18:28:33,550 --> 18:28:38,890
the query condition first states that it is going to use a regex operation 

118
18:28:39,980 --> 18:28:45,340
and then we have to give the partial string as am 

119
18:28:45,340 --> 18:28:51,250
then it gives the directive that this match

120
18:28:51,250 --> 18:28:56,560
should be case insensitive by placing an i after the partial string 

121
18:28:56,560 --> 18:29:03,070
and if we just wanted to do names we would stop right after the find function 

122
18:29:03,070 --> 18:29:08,120
but here we also want to do a count which is a post operation

123
18:29:08,120 --> 18:29:13,140
after the find so we use count at the end of the find function 

124
18:29:14,860 --> 18:29:21,560
now what if we have the same query but we want the partial string a - m 

125
18:29:21,560 --> 18:29:24,610
to appear at the beginning of the name and

126
18:29:24,610 --> 18:29:27,070
you would like the a to really be a capital letter 

127
18:29:28,460 --> 18:29:32,540
in this case we use the caret sign to indicate

128
18:29:32,540 --> 18:29:34,950
that the partial string is at the beginning of the name 

129
18:29:36,090 --> 18:29:40,786
naturally we also drop the i at the end because the match is no longer case

130
18:29:40,786 --> 18:29:45,389
insensitive a more complex

131
18:29:45,389 --> 18:29:50,445
partial string pattern will be a case where our name starts with capital a - m 

132
18:29:50,445 --> 18:29:54,600
then has a number of characters in the middle and ends with corp 

133
18:29:55,810 --> 18:30:01,140
so for the first part the string pattern is ^ am 

134
18:30:01,140 --> 18:30:06,370
for the second part that is any character in the middle we use dot

135
18:30:06,370 --> 18:30:12,480
to represent any character and star to represent zero or more occurrences 

136
18:30:12,480 --> 18:30:15,090
for the third part we say corp but

137
18:30:15,090 --> 18:30:19,580
put a dollar at the end to say that it must appear at the end of the string 

138
18:30:20,830 --> 18:30:24,330
the regular expression pattern is a sub - language in itself and

139
18:30:24,330 --> 18:30:27,500
is supported by most programming languages today 

140
18:30:27,500 --> 18:30:31,530
we will refer you to the following url to learn more about it 

141
18:30:33,120 --> 18:30:36,270
also note an example that instead of saying find count 

142
18:30:36,270 --> 18:30:41,200
we can directly use the count function natively defined in mongodb 

143
18:30:43,680 --> 18:30:48,070
one important feature of json is that everything contain arrays as a type of

144
18:30:48,070 --> 18:30:54,390
collection objects this enables us to query arrays in multiple ways 

145
18:30:54,390 --> 18:30:58,460
one of them is to consider an array as a list and

146
18:30:58,460 --> 18:31:03,310
perform intersection operations the first query shows this 

147
18:31:03,310 --> 18:31:04,840
the data item is shown on the right 

148
18:31:06,190 --> 18:31:10,600
it has the area value attribute called tags with three entries 

149
18:31:10,600 --> 18:31:15,390
the first query asks if two specific strings belong to the array 

150
18:31:16,660 --> 18:31:21,510
in other words it wants to get the document whose tagged attribute

151
18:31:21,510 --> 18:31:23,480
intersects with the query supplied array 

152
18:31:24,520 --> 18:31:28,640
in this case there is an intersection and the document is returned 

153
18:31:30,250 --> 18:31:32,950
in the second case it is asking for

154
18:31:32,950 --> 18:31:36,570
a documents who tags attribute has no intersection 

155
18:31:37,920 --> 18:31:43,880
now notice the nin operator so there is no intersection with this list 

156
18:31:45,730 --> 18:31:49,810
so in this document there exists and intersection so nothing will be returned 

157
18:31:51,940 --> 18:31:56,560
a different kind of array query uses the positions of the list elements and

158
18:31:56,560 --> 18:31:58,680
wants to extract a portion of the array 

159
18:31:59,950 --> 18:32:03,000
this is illustrated in the third query which asks for

160
18:32:03,000 --> 18:32:06,140
the second and third items of the array 

161
18:32:06,140 --> 18:32:10,890
to encode this in mongodb we use the slice operator

162
18:32:10,890 --> 18:32:16,890
which needs two parameters the number of variable limits to skip 

163
18:32:16,890 --> 18:32:19,370
and the number of variable limits to extract after skipping 

164
18:32:20,420 --> 18:32:25,910
in this case we need to extract items two and three so the skip value is one and

165
18:32:25,910 --> 18:32:30,130
the number of items is two thus returning summer and japanese 

166
18:32:31,320 --> 18:32:35,990
now we could get the same result if we pose the query using the last statement 

167
18:32:37,080 --> 18:32:41,740
in this case the minus says that the system should count from the end and

168
18:32:42,800 --> 18:32:45,360
the true says that it should extract two elements 

169
18:32:46,990 --> 18:32:51,096
now if we omitted the minus sign it will come from the beginning and

170
18:32:51,096 --> 18:32:52,981
fetch the first two elements 

171
18:32:56,300 --> 18:33:02,570
finally we can also ask for a document who second element in tags is summer 

172
18:33:02,570 --> 18:33:08,529
in this case we use an array index tags 1 to denote the second element 

173
18:33:12,580 --> 18:33:17,232
compound statements are queries with multiple query conditions that

174
18:33:17,232 --> 18:33:20,750
are combined using logical operations 

175
18:33:20,750 --> 18:33:26,167
the query shown here has one condition which is the and are in terms of mongodb 

176
18:33:26,167 --> 18:33:28,640
the and of three different clauses 

177
18:33:29,710 --> 18:33:32,310
the last clause is the most straight forward 

178
18:33:32,310 --> 18:33:35,368
it states that the desired item should not be coors 

179
18:33:35,368 --> 18:33:42,630
the first clause is an or that is a or between two sub - conditions 

180
18:33:42,630 --> 18:33:48,622
a the prices either 3 99 or b it is 4 99 

181
18:33:48,622 --> 18:33:53,154
the second clause is also an or of two sub conditions 

182
18:33:53,154 --> 18:33:58,101
a the rating is good and b the quantity is less than 20 

183
18:33:59,474 --> 18:34:02,019
this query shows that the and and

184
18:34:02,019 --> 18:34:07,170
the or operators need a list that is an array of arguments 

185
18:34:07,170 --> 18:34:09,343
to draw a quick comparison 

186
18:34:09,343 --> 18:34:13,609
here the example of the same query imposed with sql 

187
18:34:18,250 --> 18:34:22,612
now an important feature of semi - structured data is that

188
18:34:22,612 --> 18:34:24,070
it allows nesting 

189
18:34:25,120 --> 18:34:29,750
we showed three documents here where there is an area named points 

190
18:34:29,750 --> 18:34:33,620
which in turn has two tuples with the elements points and bonus 

191
18:34:33,620 --> 18:34:39,060
let assume that these three documents are part of a collection so

192
18:34:39,060 --> 18:34:42,630
they form three items in an area called users 

193
18:34:43,800 --> 18:34:48,060
our goal is to show how we can write queries to extract data

194
18:34:48,060 --> 18:34:50,350
from these documents with nesting 

195
18:34:51,620 --> 18:34:54,300
the first query wants to find documents for

196
18:34:54,300 --> 18:34:58,860
which the value of points should be less than or equal to 80 

197
18:34:58,860 --> 18:35:01,052
now which ones 

198
18:35:01,052 --> 18:35:04,120
now points 0 

199
18:35:04,120 --> 18:35:09,016
refers to the first tuple under the outer points and

200
18:35:09,016 --> 18:35:14,800
points 0 points refers to the first element of that tuple 

201
18:35:14,800 --> 18:35:17,980
clearly only the second documents satisfies this query 

202
18:35:20,280 --> 18:35:23,150
now what happens if we have the same query but we drop the zero 

203
18:35:24,410 --> 18:35:29,260
now we are looking for points points without specifying the array index 

204
18:35:29,260 --> 18:35:31,830
this means that the points element in

205
18:35:31,830 --> 18:35:35,050
any of the tuples should have a value of 80 or less 

206
18:35:36,170 --> 18:35:40,403
so now the first and the second document will satisfy the query 

207
18:35:42,900 --> 18:35:46,340
we can put multiple conditions as seen in the third query 

208
18:35:47,490 --> 18:35:51,740
it looks for a document where the points element of a tuple should be utmost 81 

209
18:35:51,740 --> 18:35:58,440
and the bonus should be exactly 20 and clearly the second document qualifies 

210
18:35:58,440 --> 18:36:00,500
but does the third document qualify 

211
18:36:01,930 --> 18:36:07,640
in this case the first tuple satisfies points greater than 81 and

212
18:36:07,640 --> 18:36:09,870
the second tuple satisfies bonus equal to 20 

213
18:36:09,870 --> 18:36:16,190
the answer is no because the comma is treated as an implicit and

214
18:36:16,190 --> 18:36:20,280
condition within the same double as shown in the yellow braces 

215
18:36:22,320 --> 18:36:25,770
now remember that we said in course two that all semi - structured

216
18:36:25,770 --> 18:36:27,070
data can be viewed as a tree 

217
18:36:28,490 --> 18:36:32,730
now what if i pick a node of a tree and ask for all the descendents of that node 

218
18:36:33,880 --> 18:36:37,660
that would require the system to recursively get chidenodes 

219
18:36:37,660 --> 18:36:39,510
over increasing depth from the given node 

220
18:36:40,820 --> 18:36:46,141
unfortunately at this time mongodb does not support recursive search 

1
12:56:23,463 --> 12:56:28,381
a different kind of scaling problem arises when we try to answer queries over a large

2
12:56:28,381 --> 12:56:33,158
number of data sources but before we do that let see how a query is answered in

3
12:56:33,158 --> 12:56:35,570
the virtual data integration setting 

4
12:56:36,990 --> 12:56:39,819
we are going to use a toy scenario in a medical setting

5
12:56:40,890 --> 12:56:43,070
simple as we have four data sources 

6
12:56:43,070 --> 12:56:45,320
each with one table for the sake of simplicity 

7
12:56:46,570 --> 12:56:51,760
notice that two sources s1 and s3 have the same schema 

8
12:56:51,760 --> 12:56:55,380
now this is entirely possible because sources may be independent of each other 

9
12:56:56,750 --> 12:57:00,310
further there is no guarantee that they would have the same exact content 

10
12:57:01,390 --> 12:57:05,400
maybe these two sources represent clinics at different locations 

11
12:57:07,260 --> 12:57:09,860
so next we look at the target schema 

12
12:57:11,020 --> 12:57:15,630
for simplicity let consider that it not an algorithmically creative

13
12:57:15,630 --> 12:57:20,090
probabilistic mediator schema but just a manually designed schema with five tables 

14
12:57:21,670 --> 12:57:26,620
but while we assume that the target schema is fixed

15
12:57:26,620 --> 12:57:29,710
we want the possibility that we can add more sources 

16
12:57:29,710 --> 12:57:32,280
that means more clinics as the system grows 

17
12:57:34,970 --> 12:57:37,210
now i am beginning to add the schema mapping 

18
12:57:38,370 --> 12:57:42,430
now there are several techniques of specifying schema mappings 

19
12:57:42,430 --> 12:57:45,090
one of them is called local - as - view 

20
12:57:45,090 --> 12:57:50,830
this means we write the relations in each source as a view over the target schema 

21
12:57:52,590 --> 12:57:56,398
but this way of writing the query as we can see here may seem odd to you 

22
12:57:56,398 --> 12:58:00,280
it called syntax but you do not need to know it 

23
12:58:00,280 --> 12:58:06,300
just as an example the first few things the treats relation in s1 maps to 

24
12:58:06,300 --> 12:58:08,220
so you see that arrow that means maps to 

25
12:58:09,790 --> 12:58:14,030
so it maps to the query select doctor 

26
12:58:14,030 --> 12:58:19,890
chronic disease from treats patient has chronic disease 

27
12:58:19,890 --> 12:58:25,550
where treatspatient patient is equal to has chronic disease dot patient 

28
12:58:28,050 --> 12:58:29,520
we see the query here in the yellow box 

29
12:58:30,760 --> 12:58:35,200
the only thing we should notice here is that the select class on the query

30
12:58:35,200 --> 12:58:36,880
has two attributes doctor and

31
12:58:36,880 --> 12:58:42,040
chronic disease which are exactly the same attributes of the treats relation in s1 

32
12:58:44,360 --> 12:58:46,820
now let ask a query that gives the target schema 

33
12:58:47,820 --> 12:58:51,620
which doctors are responsible for discharging patients 

34
12:58:51,620 --> 12:58:53,777
which translates to the sql query shown here 

35
12:58:55,491 --> 12:59:00,344
now the problem is how to translate this query to a query that can

36
12:59:00,344 --> 12:59:02,120
be sent to the sources 

37
12:59:03,160 --> 12:59:08,985
now ideally this should be simplest query with no extra operations as shown here 

38
12:59:08,985 --> 12:59:16,020
s3 treats means treats relation in sources 3 

39
12:59:16,020 --> 12:59:18,500
now you can see the ideal answer 

40
12:59:19,600 --> 12:59:24,200
to find such an optimal query reformulation it turns out that this

41
12:59:24,200 --> 12:59:29,550
process is very complex and becomes worse as a number of sources increases 

42
12:59:31,090 --> 12:59:33,840
thus query reformulation

43
12:59:33,840 --> 12:59:38,200
becomes a significant scalability problem in a big data integration scenario 

44
12:59:42,000 --> 12:59:43,640
let look at the second use case

45
12:59:44,860 --> 12:59:48,070
public health is a significant component of our healthcare system 

46
12:59:49,150 --> 12:59:54,630
public health systems monitor detect and take action when epidemics strike 

47
12:59:54,630 --> 12:59:59,500
not so long ago we have witnessed public health concerns due to anthrax virus 

48
12:59:59,500 --> 13:00:01,240
the swine flu and the bird flu 

49
13:00:02,510 --> 13:00:07,180
but these epidemics caused a group called wadds to develop a system for

50
13:00:07,180 --> 13:00:08,070
disease surveillance 

51
13:00:09,230 --> 13:00:13,670
this system would connect all local hospitals in the washington dc area and

52
13:00:13,670 --> 13:00:16,290
is designed to exchange disease information 

53
13:00:16,290 --> 13:00:21,030
for example if a hospital lab has identified a new strain of a virus 

54
13:00:21,030 --> 13:00:23,630
other hospitals and the centers for disease control cdc 

55
13:00:23,630 --> 13:00:26,260
in the network should be able to know about it 

56
13:00:28,300 --> 13:00:33,020
it should be clear that this needs a data integration solution where the data

57
13:00:33,020 --> 13:00:38,490
sources would be the labs the data would be the lab tests medical records and

58
13:00:38,490 --> 13:00:42,170
even genetic profiles of the virus and the subjects who might be infected 

59
13:00:43,180 --> 13:00:47,630
the table here shows the different components with this architecture 

60
13:00:47,630 --> 13:00:50,590
we will just digest the necessary parts for our requirement 

61
13:00:51,620 --> 13:00:56,488
just know that rim which stands for reference information model is global

62
13:00:56,488 --> 13:01:01,370
schema that this industry has developed and expects to use as a standard 

63
13:01:04,820 --> 13:01:08,330
why we want to exchange and combine new information from different hospitals 

64
13:01:09,430 --> 13:01:11,990
every hospital is independent and

65
13:01:11,990 --> 13:01:15,190
can implement their own information system any way they see fit 

66
13:01:16,450 --> 13:01:20,080
therefore even when there are standards like hl - 7 that

67
13:01:20,080 --> 13:01:24,230
specify what kind of data a held cache system should have an exchange 

68
13:01:24,230 --> 13:01:29,520
there are considerable variations in the implementation of the standard itself 

69
13:01:29,520 --> 13:01:33,660
for example the two wide boxes show a difference in representation

70
13:01:33,660 --> 13:01:38,620
of the same kind of data this should remind you of the data variety problem 

71
13:01:39,800 --> 13:01:44,810
let say we have a patient with id 19590520 whose lab

72
13:01:46,280 --> 13:01:50,020
reports containing her plasma protein measurements are required for

73
13:01:50,020 --> 13:01:51,440
analyzing her health condition 

74
13:01:52,590 --> 13:01:56,550
the problem is that the patient went to three different clinics and

75
13:01:56,550 --> 13:01:59,990
four different labs which all implement the standards differently 

76
13:02:01,290 --> 13:02:02,540
on top of it 

77
13:02:02,540 --> 13:02:06,420
each clinic uses its own electronic medical record system

78
13:02:06,420 --> 13:02:08,230
which we have a very large amount of data 

79
13:02:09,510 --> 13:02:14,300
so the data integration system job is to transform the data from the source schema

80
13:02:14,300 --> 13:02:19,150
to the schema of the receiving system in this case the rim system 

81
13:02:20,410 --> 13:02:23,730
this is sometimes called the data exchange problem 

82
13:02:26,380 --> 13:02:30,140
informally a data exchange problem can be defined like this 

83
13:02:31,780 --> 13:02:35,300
suppose we have a given database whose relations are known 

84
13:02:36,750 --> 13:02:39,920
let us say we also know the target database schema and

85
13:02:39,920 --> 13:02:41,700
the constraints the schema will satisfy 

86
13:02:43,330 --> 13:02:47,780
further we know the desired schema mappings between the source and

87
13:02:47,780 --> 13:02:49,300
this target schema 

88
13:02:49,300 --> 13:02:55,040
what we do not know is how to populate the tuples in the target database 

89
13:02:55,040 --> 13:02:59,493
from the tuples in the socialization in such a way that both schema mappings and

90
13:02:59,493 --> 13:03:02,540
target constraints are simultaneously satisfied 

91
13:03:05,729 --> 13:03:09,854
in many domains like healthcare a significant amount of effort has been

92
13:03:09,854 --> 13:03:13,260
spend by the industry in standardizing schemas and values 

93
13:03:14,440 --> 13:03:18,320
for example loinc is a standard for medical lab observations 

94
13:03:19,720 --> 13:03:22,320
here item like systolic blood pressure or

95
13:03:22,320 --> 13:03:26,580
gene mutation are encoded in this specific way as given by this standard 

96
13:03:27,630 --> 13:03:32,060
so if we want to write that the systolic diastolic pressure of

97
13:03:32,060 --> 13:03:38,190
a individual is 132 by 90 we will not write out the string systolic blood pressure 

98
13:03:38,190 --> 13:03:40,920
but use the code for it 

99
13:03:40,920 --> 13:03:45,070
the ability to use standard code is not unique to healthcare data 

100
13:03:45,070 --> 13:03:47,919
the 50 states of the us all have two letter abbreviations 

101
13:03:49,210 --> 13:03:55,210
generalizing therefore whenever we have data such as the domain is finite and have

102
13:03:55,210 --> 13:04:00,910
a standard set of code available we give a new opportunity of handling big deal 

103
13:04:02,610 --> 13:04:06,260
mainly reducing the data size through compression 

104
13:04:08,110 --> 13:04:10,700
the compression refers to a way of

105
13:04:10,700 --> 13:04:13,920
creating an encoded representation of data 

106
13:04:13,920 --> 13:04:17,980
so that this encoder form is smaller than the original representation 

107
13:04:19,970 --> 13:04:23,420
a common encoding method is called dictionary encoding 

108
13:04:23,420 --> 13:04:26,980
consider a database with 10 million record of patient visits a lab 

109
13:04:28,250 --> 13:04:32,140
each record indicates a test and its results 

110
13:04:32,140 --> 13:04:35,140
now we show it this way like in a columnar structure

111
13:04:36,220 --> 13:04:41,040
to make the point that the data is kept in a column stored relational database

112
13:04:41,040 --> 13:04:43,430
rather than a row store relational database 

113
13:04:44,650 --> 13:04:47,380
now consider the column for test code 

114
13:04:47,380 --> 13:04:50,470
where the type of test is codified according to the standard 

115
13:04:52,470 --> 13:04:56,340
we replace a string representation of the standard by a number 

116
13:04:58,750 --> 13:05:01,540
the mapping between the original test code and

117
13:05:01,540 --> 13:05:04,870
the encoded number are also stored separately 

118
13:05:04,870 --> 13:05:06,810
now suppose there are a total of 500 tests 

119
13:05:08,240 --> 13:05:13,780
so this separate table called the dictionary here has 500 rows 

120
13:05:13,780 --> 13:05:18,500
which is clearly much smaller than ten million right 

121
13:05:18,500 --> 13:05:23,300
now 500 distinct values can be represented by encoding them in 9 bits 

122
13:05:23,300 --> 13:05:25,114
because 2 to the power of 9 is 512 

123
13:05:27,060 --> 13:05:30,750
other encoding techniques would be applied to attributes like date and patient id 

124
13:05:31,920 --> 13:05:36,890
that full large data we cannot reduce the number of total actual rules 

125
13:05:36,890 --> 13:05:39,090
so we have to store all ten million rules 

126
13:05:39,090 --> 13:05:44,893
but we can reduce the amount of space required by storing data in a column

127
13:05:44,893 --> 13:05:49,925
oriented data store and by using compression indeed modern

128
13:05:49,925 --> 13:05:56,621
systems use credit processing algorithms to operate directly on compress data 

129
13:06:00,025 --> 13:06:03,636
data compression is an important technology for big data 

130
13:06:06,555 --> 13:06:11,560
and just like is a set of qualified terms for lab tests clinical data

131
13:06:11,560 --> 13:06:16,500
also uses snomed which stands for systematized nomenclature of medicine 

132
13:06:17,770 --> 13:06:20,780
snomed is a little more than just a vocabulary 

133
13:06:21,900 --> 13:06:23,599
it does have a vocabulary of course 

134
13:06:24,620 --> 13:06:29,656
the vocabulary is the collection of medical terms in human and

135
13:06:29,656 --> 13:06:35,790
medicine to provide codes terms synonyms and definitions that cover anatomy 

136
13:06:35,790 --> 13:06:39,980
diseases findings procedures micro organisms substances etcetera 

137
13:06:40,990 --> 13:06:42,590
but it also has relationships 

138
13:06:44,060 --> 13:06:48,277
as you can see a renal cyst is related to kidney because

139
13:06:48,277 --> 13:06:51,615
kidney the finding site of a renal cyst 

140
13:06:53,890 --> 13:06:58,080
if we query against an ontology it would look like a graph grid 

141
13:06:59,150 --> 13:06:59,920
in this box 

142
13:06:59,920 --> 13:07:04,120
we are asking to find all patient findings with a benign tumor morphology 

143
13:07:05,350 --> 13:07:10,100
in terms of querying we are looking for edges of a graph where one noticed

144
13:07:10,100 --> 13:07:15,470
the concept that we need to find which is connected to a node called benign neoplasm

145
13:07:15,470 --> 13:07:20,050
that is benign tumor through an edge called associated morphology

146
13:07:21,070 --> 13:07:25,700
that applying this query against the data here produces all benign tumors

147
13:07:25,700 --> 13:07:29,970
of specific organs as you can see by the orange rounded group 

148
13:07:32,170 --> 13:07:34,410
but now that we have these terms 

149
13:07:34,410 --> 13:07:38,760
we can use these terms to search outpatient records with these terms would

150
13:07:38,760 --> 13:07:43,910
have been used so what is the essence of this used case 

151
13:07:45,030 --> 13:07:50,170
this is can shows that an in division system in a public health domain and

152
13:07:50,170 --> 13:07:53,320
in many other domains must be able to handle variety 

153
13:07:54,910 --> 13:08:01,520
in this case there a global schema called rim shown here all queries and

154
13:08:01,520 --> 13:08:06,000
analyses performed by a data analysts should be against this global schema 

155
13:08:07,230 --> 13:08:11,870
however the actual data which is generated by different medical facilities

156
13:08:11,870 --> 13:08:16,970
would need to be transformed into data in this schema 

157
13:08:16,970 --> 13:08:21,220
this would require not only format conversions but

158
13:08:21,220 --> 13:08:26,630
it would need to respect all constraints imposed by the source and by the target 

159
13:08:26,630 --> 13:08:31,520
for example a source may not distinguish between an emergency surgical procedure

160
13:08:31,520 --> 13:08:33,430
and a regular surgical procedure 

161
13:08:33,430 --> 13:08:37,429
but the target may want to put them in different tables 

162
13:08:39,850 --> 13:08:41,850
we also saw that the integration system for

163
13:08:41,850 --> 13:08:45,920
this used case would need to use quantified data but this gives

164
13:08:45,920 --> 13:08:49,970
us the opportunity to use data compression to gauge story and query efficiency 

165
13:08:51,120 --> 13:08:56,352
in terms of variety we saw how relational data like patient records xml

166
13:08:56,352 --> 13:09:01,603
data like hl7 events and graph data like ontologies are co - used 

167
13:09:03,866 --> 13:09:08,407
to support this the integration system must be able to do both model

168
13:09:08,407 --> 13:09:11,400
transformation and query transformation 

169
13:09:12,560 --> 13:09:16,260
query transformation is the process of taking a query on the target schema 

170
13:09:17,490 --> 13:09:21,540
converting it to a query against a different data model 

171
13:09:21,540 --> 13:09:27,100
for example part of an sql query against the rim may need to go to snowmad and

172
13:09:27,100 --> 13:09:30,240
hence when you to be converted to a graph query in the snowmad system 

173
13:09:31,590 --> 13:09:34,933
model transformation is a process of taking data

174
13:09:34,933 --> 13:09:38,274
represented in one model in one source system and

175
13:09:38,274 --> 13:09:43,263
converting it to an equivalent data in another model the target system 

1
02:06:04,820 --> 02:06:06,600
hello my name is victor lou 

2
02:06:06,600 --> 02:06:11,090
i am the solutions engineer of global philanthropy of daameer org 

3
02:06:11,090 --> 02:06:12,440
helping researchers academia 

4
02:06:12,440 --> 02:06:15,750
and non - profits find the insights that matter using data 

5
02:06:15,750 --> 02:06:19,280
today i am here to give you a production environment 

6
02:06:19,280 --> 02:06:21,410
personalized music recommendation project 

7
02:06:21,410 --> 02:06:25,150
that was done for ponomusic and so what is pono music 

8
02:06:27,120 --> 02:06:30,355
pono music is revolutionizing music listening by bringing back native 

9
02:06:30,355 --> 02:06:32,290
high - resolution audio 

10
02:06:32,290 --> 02:06:35,550
it the way that artists intended and recorded their music 

11
02:06:35,550 --> 02:06:40,710
neil young the iconic artist and musician is also the founder and ceo 

12
02:06:40,710 --> 02:06:44,020
pono music is the only complete high - resolution music ecosystem 

13
02:06:44,020 --> 02:06:47,950
that includes the pono player the pono music store the pono music community 

14
02:06:49,200 --> 02:06:52,030
they have been super generous in allowing us to

15
02:06:52,030 --> 02:06:55,020
take a look behind the scenes as how we built out the recommendations 

16
02:06:57,630 --> 02:07:00,540
so david meer supports their mission by providing a music recommendation

17
02:07:00,540 --> 02:07:04,900
engine that is both scalable and flexible as pono grows their user base 

18
02:07:04,900 --> 02:07:09,220
let begin by visiting pono music website as you can see 

19
02:07:09,220 --> 02:07:11,740
they have a few shelves that focus on

20
02:07:11,740 --> 02:07:15,320
various recommendations that are not tailored to each individual user 

21
02:07:15,320 --> 02:07:18,670
if you login then an additional shelf would appear

22
02:07:18,670 --> 02:07:21,690
that would deliver the recommendations that are created in data gear 

23
02:07:21,690 --> 02:07:23,029
so how does datamirror accomplish this 

24
02:07:23,029 --> 02:07:28,279
in a nutshell datamirror is a end to end antalis platform which contains ingestion 

25
02:07:28,279 --> 02:07:33,543
preparation analysis visualization and operationalization all the same platform 

26
02:07:33,543 --> 02:07:36,300
and having all the capabilities in one platform is superior to

27
02:07:36,300 --> 02:07:38,240
traditional technology stack 

28
02:07:38,240 --> 02:07:43,300
integration between disparate technologies brings a lot of unnecessary challenges 

29
02:07:43,300 --> 02:07:45,880
we take advantage of open source big data technologies 

30
02:07:45,880 --> 02:07:50,261
specifically we run natively and hadoop and spark for our back end 

31
02:07:50,261 --> 02:07:53,600
and we leverage d3js on the front end for visualizations 

32
02:07:53,600 --> 02:07:55,821
our excel - like interface makes it easy for

33
02:07:55,821 --> 02:07:59,960
folks who do not know how to code to also get in with the data modeling 

34
02:07:59,960 --> 02:08:01,720
it is very much self - service 

35
02:08:01,720 --> 02:08:04,001
in the case of pono music data mirror deployment 

36
02:08:04,001 --> 02:08:06,790
we have deployed a hadoop on amazon on web services 

37
02:08:06,790 --> 02:08:08,410
but we can also be deployed off premise if

38
02:08:08,410 --> 02:08:10,290
needed it does not have to be in the clouds 

39
02:08:10,290 --> 02:08:13,820
in fact we could work with many many different distributions of hadoop 

40
02:08:13,820 --> 02:08:16,320
we can access dean rear through any modern web browser 

41
02:08:16,320 --> 02:08:20,300
as you can see here we are using google chrome but it works on any browser right 

42
02:08:20,300 --> 02:08:22,460
so lets go ahead and take a look at the a; let log into data grip 

43
02:08:22,460 --> 02:08:25,640
so we are going to go ahead and log in here 

44
02:08:27,390 --> 02:08:32,250
so what you are seeing here is the browser tab and we can see various artifacts

45
02:08:32,250 --> 02:08:36,364
which include connections such as connections to the pull nose 

46
02:08:36,364 --> 02:08:40,434
salesforce com instance or a connection to aws s3 instance 

47
02:08:40,434 --> 02:08:42,226
because datamirror comes with so

48
02:08:42,226 --> 02:08:46,708
many prebuilt connectors which include connections to salesforce com and s3 

49
02:08:46,708 --> 02:08:50,034
we can easily grant access to those systems in order to pull or

50
02:08:50,034 --> 02:08:52,550
push data in and out of the dev 

51
02:08:52,550 --> 02:08:55,760
for example let take a quick look at the salesforce com configuration here 

52
02:08:55,760 --> 02:09:03,290
as you can see we have already configured salesforce as the connector type 

53
02:09:03,290 --> 02:09:04,530
when we hit next here 

54
02:09:04,530 --> 02:09:08,370
we will we that you can authorize a data manager to retrieve data 

55
02:09:08,370 --> 02:09:10,380
we are not going to do it because this is a production environment so

56
02:09:10,380 --> 02:09:13,300
i do not want to disrupt the pools 

57
02:09:13,300 --> 02:09:17,210
but if we were to click it it will give us a salesfloor com blogging stream 

58
02:09:17,210 --> 02:09:21,830
and so as soon as we login the datameer would have access via the o - off token 

59
02:09:21,830 --> 02:09:24,130
and so this is valid for a period of time 

60
02:09:24,130 --> 02:09:24,650
i am going to go ahead and

61
02:09:24,650 --> 02:09:27,910
hit the cancel button as this is a production environment 

62
02:09:27,910 --> 02:09:32,290
and so now we are ready to look at an import job artifact 

63
02:09:32,290 --> 02:09:35,888
so this is the sales force import job and

64
02:09:35,888 --> 02:09:40,152
as you can see we go in here and we configure it 

65
02:09:40,152 --> 02:09:45,830
we can see that it connected to the pono music smdc connection 

66
02:09:48,753 --> 02:09:53,269
i am going to go ahead and hit next here with salesforce com 

67
02:09:53,269 --> 02:09:56,079
we just have to simply define the soql 

68
02:09:56,079 --> 02:10:00,526
which is basically a querying language that based on sql 

69
02:10:00,526 --> 02:10:03,860
to locate data contained within the salesforce com objects 

70
02:10:03,860 --> 02:10:08,997
you can see the select statement here you can also see the familiar

71
02:10:08,997 --> 02:10:14,059
from statement and you can also see the where statement as well 

72
02:10:14,059 --> 02:10:16,459
so in order to protect the privacy of the users 

73
02:10:16,459 --> 02:10:20,830
datameer has the capability to obfuscate sort of columns that are sensitive 

74
02:10:20,830 --> 02:10:23,770
and so in other words we can scramble sensitive fields such as email or

75
02:10:23,770 --> 02:10:25,370
physical addresses 

76
02:10:25,370 --> 02:10:29,228
and at this point in time i am going to go ahead and cancel out and i am going to

77
02:10:29,228 --> 02:10:32,866
show the next part of this demo in an obfuscated workbook environment 

78
02:10:32,866 --> 02:10:37,813
the artifacts contained in the obfuscated folders are duplicates of the import job

79
02:10:37,813 --> 02:10:42,130
of the workbook from their production counterparts as you can see here 

80
02:10:44,360 --> 02:10:48,510
so as you can see in the import job settings we have configured

81
02:10:48,510 --> 02:10:55,320
the same connections here but for the obfuscated columns here 

82
02:10:55,320 --> 02:10:58,970
now we are obfuscating the owner email as well as the street address 

83
02:10:58,970 --> 02:10:59,690
now we hit next here 

84
02:11:03,259 --> 02:11:07,140
and so you can see various fields are being pulled here from salesforce right 

85
02:11:07,140 --> 02:11:09,130
and in particular i want to highlight the 

86
02:11:10,150 --> 02:11:13,780
the owner email that field so as you can see these are the obfuscated fields 

87
02:11:13,780 --> 02:11:17,420
similarly the actual street address that also masked as well 

88
02:11:17,420 --> 02:11:21,375
i am going to go ahead and hit cancel 

89
02:11:21,375 --> 02:11:24,780
i will go back to rather we are going to go check out the workbook environment 

90
02:11:24,780 --> 02:11:27,300
which is where we do the preparation and analysis 

91
02:11:32,465 --> 02:11:35,070
to the obfuscated workbook environment 

92
02:11:35,070 --> 02:11:38,260
as you can see we are starting with a raw source of data here 

93
02:11:38,260 --> 02:11:42,930
this was the data that was contained in the data sheet 

94
02:11:42,930 --> 02:11:47,500
take note that there a variety of icons that indicate different types of sheets 

95
02:11:47,500 --> 02:11:49,610
so you could see here this is the raw data type 

96
02:11:49,610 --> 02:11:52,360
this is a regular worksheet 

97
02:11:52,360 --> 02:11:55,400
you have a union sheet and you have a joint sheet and

98
02:11:55,400 --> 02:11:58,440
we will go into a little bit more detail as we get to them 

99
02:11:58,440 --> 02:12:01,700
so as you can see the interface is very much like excel 

100
02:12:01,700 --> 02:12:06,830
looking at the second sheet pairs and it using a function called group by 

101
02:12:06,830 --> 02:12:11,380
so the group by function is a type of aggregation function in datameer speak 

102
02:12:11,380 --> 02:12:14,640
as you can see you can build the functions by pointing and clicking 

103
02:12:14,640 --> 02:12:18,040
but you can also enter the formula to create nested functions or

104
02:12:18,040 --> 02:12:23,310
apply arithmetic operations just as a cell 

105
02:12:23,310 --> 02:12:25,770
we take all the unique combinations of albums purchased for

106
02:12:25,770 --> 02:12:28,730
each of the unique email using a group pair function 

107
02:12:29,890 --> 02:12:32,710
then we pull out each of the elements of the pairings using a list element

108
02:12:32,710 --> 02:12:34,310
function so let see here 

109
02:12:35,360 --> 02:12:38,230
if you were to build a function from scratch you can see this 

110
02:12:38,230 --> 02:12:43,950
so then you create a group by function and then direct it at a column 

111
02:12:43,950 --> 02:12:45,120
you basically pass it to argument 

112
02:12:47,890 --> 02:12:51,856
to find out the occurrence of each pairing of albums 

113
02:12:51,856 --> 02:12:56,188
we are going to go ahead and look at this lr sheet 

114
02:12:56,188 --> 02:12:59,477
and so these are we are using group by functions as well here so

115
02:12:59,477 --> 02:13:02,372
we are grouping by the first item one and then item two 

116
02:13:02,372 --> 02:13:04,088
and then we are doing a group count 

117
02:13:04,088 --> 02:13:07,430
which is counting the frequency of occurrences of those cells 

118
02:13:07,430 --> 02:13:10,350
similarly we are going to do a similar thing but

119
02:13:10,350 --> 02:13:15,096
with the item two first now and then item one and then doing a group count 

120
02:13:15,096 --> 02:13:17,780
and the reason is we do not care about the order 

121
02:13:17,780 --> 02:13:21,810
and so we repeat this on the next sheet reversing the order of the albums to make

122
02:13:21,810 --> 02:13:25,460
sure that we capture all the combinations since the order does not matter to us 

123
02:13:28,260 --> 02:13:32,887
and then we use our unit sheet function to append the second frequency

124
02:13:32,887 --> 02:13:38,140
counting sheet to the first frequency counting sheet as you can see here 

125
02:13:38,140 --> 02:13:40,962
it drag and drop of course and so we have connected them together 

126
02:13:40,962 --> 02:13:43,248
you can do multi - sheets but in this case we only have two right 

127
02:13:43,248 --> 02:13:46,260
so it okay 

128
02:13:46,260 --> 02:13:51,770
we ignore the co - occurrent set sheet as it is currently not used in the final output 

129
02:13:51,770 --> 02:13:55,010
we were kind of experimenting with other recommendation models 

130
02:13:55,010 --> 02:13:56,660
we are rebuilding those workbooks 

131
02:13:56,660 --> 02:14:00,780
so one of the really cool features for we can execute the workbook but

132
02:14:00,780 --> 02:14:03,162
deselect the storage of intermediary sheets 

133
02:14:03,162 --> 02:14:06,958
this means that we only need relevant functions are executed

134
02:14:06,958 --> 02:14:11,740
at a to the data lineage of columns that are contained in the selector sheet 

135
02:14:11,740 --> 02:14:16,620
moving on we have to create a separate sheet from the raw data 

136
02:14:16,620 --> 02:14:20,770
that counts the number of times an album occurs across all of the purchases 

137
02:14:20,770 --> 02:14:24,780
so you can see here which students will buy an album and then we do a count 

138
02:14:28,890 --> 02:14:33,768
we bring the code occurrences and the frequency together using the join

139
02:14:33,768 --> 02:14:37,599
feature as you can see it is a drag and drop interface 

140
02:14:37,599 --> 02:14:40,601
you got the obfuscated workbook each of the worksheets here and

141
02:14:40,601 --> 02:14:43,100
within each worksheets you got various columns 

142
02:14:43,100 --> 02:14:49,484
you can bring those you can drag and drop them into here to do the joins 

143
02:14:49,484 --> 02:14:54,730
you can do different types of joins you got the inner outer etc 

144
02:14:54,730 --> 02:14:57,206
you can also do multi column joins 

145
02:14:57,206 --> 02:14:58,491
you can do multi sheet joins 

146
02:14:58,491 --> 02:15:03,168
you can also select which columns you want to keep during the join 

147
02:15:03,168 --> 02:15:06,705
go ahead and cancel out here 

148
02:15:06,705 --> 02:15:10,580
datameer does not allow us to add additional functions to a joint sheet so

149
02:15:10,580 --> 02:15:13,269
we duplicate it by right clicking and then hitting the duplicate button 

150
02:15:15,820 --> 02:15:18,780
and then we create the next sheet which contains a link to columns so

151
02:15:18,780 --> 02:15:21,570
it will be linked to the rating sheet 

152
02:15:21,570 --> 02:15:26,480
this is see the link columns back to the joint sheets and then 

153
02:15:27,760 --> 02:15:32,880
we simply add a calculation where it the number of times

154
02:15:32,880 --> 02:15:36,730
the co - occurrence occurs divided by the number of times that the first album

155
02:15:36,730 --> 02:15:39,950
in the column appears throughout the data set 

156
02:15:39,950 --> 02:15:44,284
the idea is to give a high recommendation for albums that appear frequently

157
02:15:44,284 --> 02:15:48,503
together while simultaneously penalizing albums that are too common 

158
02:15:48,503 --> 02:15:52,513
and so at this point i would like to write a few more joins

159
02:15:52,513 --> 02:15:55,773
that bring together the album id the email 

160
02:15:55,773 --> 02:16:00,980
recommended album id based on the album id and email so take a look here 

161
02:16:03,310 --> 02:16:07,472
so as you can see here and then 

162
02:16:07,472 --> 02:16:12,460
before we finish we also do an anti joint by performing

163
02:16:12,460 --> 02:16:18,170
the outer left joint with the almighty in the email 

164
02:16:18,170 --> 02:16:22,830
and then filtering so this is by the way this is a double column joint 

165
02:16:24,090 --> 02:16:28,060
and then we do a filter so then we filter out all the for

166
02:16:28,060 --> 02:16:30,840
the obfuscated owner fields that are empty 

167
02:16:30,840 --> 02:16:34,192
we apply a filter to it 

168
02:16:34,192 --> 02:16:39,996
so finally we use a few group i functions to do some deep duplication 

169
02:16:39,996 --> 02:16:42,915
so we use group by functions again 

170
02:16:42,915 --> 02:16:46,637
and then so we make sure that the album functions are unique for each user and

171
02:16:46,637 --> 02:16:48,250
then we use a group max function 

172
02:16:49,780 --> 02:16:53,292
in order to preserve the highest rating for each of the unique

173
02:16:54,950 --> 02:16:59,310
recommendations the final desired output is on the recommenders sheet 

174
02:16:59,310 --> 02:17:05,710
it leverages the group top end function so you can see here group top numbers 

175
02:17:05,710 --> 02:17:09,349
so we only want to look at the to 20 recommendations for each user 

176
02:17:09,349 --> 02:17:13,759
and in addition we also create a email row number field 

177
02:17:13,759 --> 02:17:17,720
which is a key that needed for salesforce com and

178
02:17:17,720 --> 02:17:22,142
it is generated by a simple concatenation function here 

179
02:17:22,142 --> 02:17:26,642
something that note that sound is big on data governance data lineage 

180
02:17:26,642 --> 02:17:30,632
as you can see we can keep track of how all the raw data flows through

181
02:17:30,632 --> 02:17:33,650
from the raw data set all the way to the end product 

182
02:17:35,950 --> 02:17:40,651
so each workbook functions can be also exported in json format to keep

183
02:17:40,651 --> 02:17:42,605
track of revision history 

184
02:17:42,605 --> 02:17:45,470
we are not ready to operationalize things and so how do we do it 

185
02:17:45,470 --> 02:17:49,700
get our workbook configurations 

186
02:17:49,700 --> 02:17:50,860
go ahead and exit out of the workbook 

187
02:17:53,420 --> 02:17:54,710
let go back to the production environment 

188
02:18:00,340 --> 02:18:01,128
as you can see 

189
02:18:01,128 --> 02:18:05,670
the workbook recalculation retriggers when the import job is completed 

190
02:18:05,670 --> 02:18:07,820
we also only retain the latest results so

191
02:18:07,820 --> 02:18:13,440
we select purge historical results as you can see here 

192
02:18:13,440 --> 02:18:18,970
as i mentioned earlier a data mirror makes it easy to operationalize a workbook

193
02:18:18,970 --> 02:18:21,630
because we only select the final output sheets 

194
02:18:21,630 --> 02:18:25,000
and so you see everything unchecked except for the recommended sheet 

195
02:18:26,730 --> 02:18:30,420
and datameer basically will automatically determine what is a critical path

196
02:18:30,420 --> 02:18:32,990
of the intermediary sheets to produce the desired output sheet 

197
02:18:34,430 --> 02:18:37,031
once the recommendation workbook is calculated 

198
02:18:37,031 --> 02:18:39,029
then this triggers the export job run 

199
02:18:39,029 --> 02:18:42,032
i am going to go ahead and move to the export job artifact 

200
02:18:42,032 --> 02:18:44,194
exit out of this 

201
02:18:44,194 --> 02:18:46,361
go ahead and save or not save 

202
02:18:46,361 --> 02:18:51,223
we will see the export job let go ahead and configure that 

203
02:18:56,087 --> 02:18:59,127
hit next share 

204
02:18:59,127 --> 02:19:04,240
csv outputs so 

205
02:19:06,817 --> 02:19:08,557
this triggers the export job to run 

206
02:19:08,557 --> 02:19:12,600
which is how we basically push the results of the recommendations to s3 

207
02:19:12,600 --> 02:19:18,310
we elected to use s3 because we are already hosting at services and

208
02:19:18,310 --> 02:19:20,630
s3 is an affordable storage solution 

209
02:19:20,630 --> 02:19:23,695
we cannot push directly to salesforce com at the present because

210
02:19:23,695 --> 02:19:26,920
it a only connection 

211
02:19:26,920 --> 02:19:30,726
therefore we need to push data to the storage area 

212
02:19:30,726 --> 02:19:34,123
which can be scheduled for using apex 

213
02:19:34,123 --> 02:19:37,796
a general challenge for is the role limitation on each pool 

214
02:19:37,796 --> 02:19:42,386
fortunately datamirror comes with the option to push

215
02:19:42,386 --> 02:19:45,873
files that are broken into one megabyte chunks 

216
02:19:45,873 --> 02:19:50,796
so as you can see here in advanced settings 1 megabyte and

217
02:19:50,796 --> 02:19:55,140
then we set a type of consecutive numbering scheme 

218
02:19:55,140 --> 02:19:57,852
and so it dynamic naming conventions using time stamps and numbering for

219
02:19:57,852 --> 02:19:59,530
each of the chunks 

220
02:19:59,530 --> 02:20:02,830
so that sums up the current co - curds d deployment 

221
02:20:02,830 --> 02:20:07,018
as you have seen we have easily integrated data from salesforce com 

222
02:20:07,018 --> 02:20:11,272
created a recommendation model and set up a push to s3 automatically for

223
02:20:11,272 --> 02:20:13,856
easy consumption back to salesforce com 

224
02:20:13,856 --> 02:20:17,469
finally we operationalized this by triggering each of the steps sequentially

225
02:20:17,469 --> 02:20:20,297
all in sitting on top of the powerful hadoop platform 

226
02:20:20,297 --> 02:20:23,070
what is next 

227
02:20:23,070 --> 02:20:27,590
well pono music is working on implementing google analytics tracking for

228
02:20:27,590 --> 02:20:30,450
each user at the apple pages 

229
02:20:30,450 --> 02:20:33,450
so let go back to the apple music store here 

230
02:20:33,450 --> 02:20:36,620
just for an example let take a look at the train led zeppelin two album 

231
02:20:37,670 --> 02:20:40,650
as you can see all of those album ids that we were looking at earlier 

232
02:20:40,650 --> 02:20:42,390
these are actually embedded in the url 

233
02:20:44,300 --> 02:20:48,517
this means that in the near future we can actually adapt recommendations based

234
02:20:48,517 --> 02:20:53,053
on the buying behavior to recommendations based on both buying and browser behavior 

235
02:20:53,053 --> 02:20:57,313
we can look at recommendations based on genre perhaps we can try to look at most

236
02:20:57,313 --> 02:21:01,526
recent purchases or browsing behavior in the last three to six to nine months 

237
02:21:01,526 --> 02:21:05,541
or we could use album metadata such as album release date to give additional

238
02:21:05,541 --> 02:21:07,270
recommendations 

239
02:21:07,270 --> 02:21:11,698
also it is quite simple to just duplicate the recommendations workbook in

240
02:21:11,698 --> 02:21:14,409
 inaudible to try any number of these options 

241
02:21:14,409 --> 02:21:17,172
and so that means we can do a lot of ab testing 

242
02:21:17,172 --> 02:21:22,600
as we track how users react to each of the modified recommendation algorithms 

243
02:21:22,600 --> 02:21:25,170
the possibilities are literally endless 

244
02:21:25,170 --> 02:21:28,051
so anyhow thanks again for checking out how data mirror

245
02:21:28,051 --> 02:21:31,305
builds a simple code current recommendation engine for music 

246
02:21:31,305 --> 02:21:33,636
please feel free to direction questions or

247
02:21:33,636 --> 02:21:36,492
comments to me at victor liu datamirror com 

248
02:21:36,492 --> 02:21:38,220
thanks again 

1
04:27:43,380 --> 04:27:51,185
amarnath just finished overviewing querying and integrational big data 

2
04:27:51,185 --> 04:27:55,160
although fundamental understanding of these concepts are important 

3
04:27:56,300 --> 04:27:59,520
some of these tasks can be accomplished

4
04:27:59,520 --> 04:28:03,650
using graphical user interface based tools or software products 

5
04:28:04,800 --> 04:28:09,870
in this short module we introduce you to two of those tools 

6
04:28:09,870 --> 04:28:11,810
splunk and datameer 

7
04:28:13,860 --> 04:28:19,720
we have selected a few videos from the site of our sponsors splunk 

8
04:28:19,720 --> 04:28:23,900
to give you an overview of using such tools for different applications 

9
04:28:24,930 --> 04:28:27,940
you will also have a short hands - on activity on splunk 

10
04:28:29,810 --> 04:28:34,210
in addition we provide you a comprehensive video on

11
04:28:34,210 --> 04:28:38,110
using datamere in the digital music industry 

12
04:28:38,110 --> 04:28:43,150
keep in mind splunk and datamere are just two of the many

13
04:28:43,150 --> 04:28:48,613
tools in this category but they represent a huge industry 

1
08:56:45,850 --> 08:56:50,205
open xc is an open source hardware - software platform 

2
08:56:50,205 --> 08:56:55,230
we wanted to see what correlations we could actually create within this model dashboards 

3
08:56:55,230 --> 08:57:00,915
it plugs into your car and it gives you all the data that you can possibly want 

4
08:57:00,915 --> 08:57:03,510
what we are interested in is looking at how you

5
08:57:03,510 --> 08:57:06,070
can make the car much more modular and customizable 

6
08:57:06,070 --> 08:57:08,365
getting a lot of this data and afford cars

7
08:57:08,365 --> 08:57:10,750
out and letting developers do whatever they want with it 

8
08:57:10,750 --> 08:57:14,170
you could kind of like update technology as technology progresses 

9
08:57:14,170 --> 08:57:15,930
so what we are really trying to do is like catch up with

10
08:57:15,930 --> 08:57:20,060
the consumer electronics design cycle not the automotive design cycle types 

11
08:57:20,060 --> 08:57:22,265
the clock has not started yet 

12
08:57:22,265 --> 08:57:26,120
one of the tests that we did was a gas car versus an electric car 

13
08:57:26,120 --> 08:57:28,230
so we are trying to see which is faster 

14
08:57:28,230 --> 08:57:29,965
which is expecting the most energy 

15
08:57:29,965 --> 08:57:32,070
which is the most cost efficient 

16
08:57:32,070 --> 08:57:35,240
and we wanted to

17
08:57:35,240 --> 08:57:39,025
measure does not electric car driver drive different than a gas car driver 

18
08:57:39,025 --> 08:57:41,345
so actually by using the open xc data 

19
08:57:41,345 --> 08:57:43,120
and by splunking that creating links in

20
08:57:43,120 --> 08:57:47,190
dashboards will give it to match ups and correlations 

21
08:57:53,110 --> 08:57:59,965
it all comes down to how much are they mashing the gas 

22
08:57:59,965 --> 08:58:05,410
we have been internally calling that dashboard the lead foot dashboard 

23
08:58:15,700 --> 08:58:23,060
who the most efficient driver 

24
08:58:23,060 --> 08:58:26,740
there one of our drivers actually going low and

25
08:58:26,740 --> 08:58:30,085
slow and actually using into the brakes not really gnashing on the gas 

26
08:58:30,085 --> 08:58:32,635
and driving more like a normal person would 

27
08:58:32,635 --> 08:58:37,265
when you drive an electric car it a lot quicker off the line than a gas powered car 

28
08:58:37,265 --> 08:58:40,215
so we find people kind of smashing

29
08:58:40,215 --> 08:58:43,975
on the gas pedal but sort of easing up on the electric pedal 

30
08:58:43,975 --> 08:58:45,595
so i have got some data for you here 

31
08:58:45,595 --> 08:58:47,950
we see what is this i am doing 

32
08:58:47,950 --> 08:58:49,995
it says you are doing 

33
08:58:49,995 --> 08:58:50,645
there we go 

34
08:58:50,645 --> 08:58:54,570
battery car is high there you go 

35
08:58:54,570 --> 08:58:57,230
hurry up 

36
08:58:57,230 --> 08:58:59,735
there we go we do not even need because open xc we just cut it 

37
08:58:59,735 --> 08:59:01,670
these are basically all of

38
08:59:01,670 --> 08:59:05,160
the sensor readings that are being broadcast from that firefly box 

39
08:59:05,160 --> 08:59:08,455
it plugged into the diagnostics port of the car 

40
08:59:08,455 --> 08:59:11,350
it about how much power can we kind of give

41
08:59:11,350 --> 08:59:15,190
away in order to incentivize a community of makers 

42
08:59:15,190 --> 08:59:17,645
we want to be able to actually harness that and create

43
08:59:17,645 --> 08:59:20,605
an environment where they can much more quickly 

44
08:59:20,605 --> 08:59:21,990
experiment with our ideas 

45
08:59:21,990 --> 08:59:24,725
enrich the platform in general 

46
08:59:24,725 --> 08:59:29,130
make our vehicles in that sense more valuable 

1
17:56:01,180 --> 17:56:04,380
in this hands on activity we will be performing queries in splunk 

2
17:56:05,620 --> 17:56:08,980
first we will open a browser and login to splunk 

3
17:56:08,980 --> 17:56:12,590
next we will import a csv file and search its contents 

4
17:56:12,590 --> 17:56:17,030
we will see how to filter fields for specific values and

5
17:56:17,030 --> 17:56:19,532
also perform statistical calculations on the data 

6
17:56:19,532 --> 17:56:22,678
let begin 

7
17:56:22,678 --> 17:56:27,236
first open a web browser and navigate to the splunk web page 

8
17:56:27,236 --> 17:56:32,103
we will enter localhost : 8000 

9
17:56:35,793 --> 17:56:40,870
next we will log in to splunk using admin and the default password 

10
17:56:50,160 --> 17:56:54,231
next we will import a csv file into splunk 

11
17:56:54,231 --> 17:56:55,952
we will click on add data 

12
17:56:59,159 --> 17:57:04,704
upload we will click on select file 

13
17:57:07,366 --> 17:57:10,838
and we will choose the census csv file that we downloaded 

14
17:57:15,364 --> 17:57:16,583
click next 

15
17:57:21,148 --> 17:57:24,010
on the left it should say source type csv 

16
17:57:24,010 --> 17:57:29,066
if it does not click on the button and

17
17:57:29,066 --> 17:57:34,128
go down to structured and select csv 

18
17:57:34,128 --> 17:57:36,290
in this table we see a preview of the data 

19
17:57:38,050 --> 17:57:41,100
you should see the column names of the csv file at the top 

20
17:57:43,260 --> 17:57:44,360
click on next 

21
17:57:47,372 --> 17:57:48,888
review 

22
17:57:50,026 --> 17:57:51,227
and submit 

23
17:57:53,602 --> 17:57:57,797
now that the file has been imported successfully click on start searching 

24
17:58:03,511 --> 17:58:08,539
in the search box it fills in the default query 

25
17:58:08,539 --> 17:58:15,260
source = cencus csv the host name and sourcetype = csv 

26
17:58:17,161 --> 17:58:21,067
we could change these fields to search other files or

27
17:58:21,067 --> 17:58:24,894
other data types if we imported those into splunk 

28
17:58:24,894 --> 17:58:30,135
now let search census csv for particular values in the fields 

29
17:58:30,135 --> 17:58:33,478
let search for all the data where the state is california 

30
17:58:36,251 --> 17:58:39,908
we will enter stname = california 

31
17:58:45,621 --> 17:58:49,521
you will see the results down here 

32
17:58:49,521 --> 17:58:52,604
you could search for other states by using or 

33
17:58:52,604 --> 17:58:56,581
for example we can add or stname = alaska 

34
17:58:56,581 --> 17:59:00,840
this will search for state names equal to california or alaska 

35
17:59:04,450 --> 17:59:06,270
we can add conditions to our query as well 

36
17:59:06,270 --> 17:59:13,930
let search for state equals california whose population was over one million 

37
17:59:13,930 --> 17:59:20,467
we will do this by saying stname = california 

38
17:59:20,467 --> 17:59:25,925
census2010pop 1000000 

39
17:59:31,477 --> 17:59:34,900
can limit the results to one or more columns 

40
17:59:34,900 --> 17:59:41,446
you do this by adding pipe table ctyname to the end of our query 

41
17:59:46,573 --> 17:59:51,336
in spunk queries the pipe command is used to send the outputs from the first part of

42
17:59:51,336 --> 17:59:52,778
the query into the next 

43
17:59:55,139 --> 17:59:57,441
you can also show more than one column from the output 

44
17:59:57,441 --> 18:00:01,982
if we add a comma census2010pop to the end of this 

45
18:00:01,982 --> 18:00:06,145
we will see both the city name and the population 

46
18:00:06,145 --> 18:00:09,944
you can also see a visualization of this data by clicking on the visualization tab 

47
18:00:12,883 --> 18:00:17,344
at the bottom on the x - axis we see the county names and

48
18:00:17,344 --> 18:00:20,420
the y values are the population numbers 

49
18:00:23,300 --> 18:00:26,994
now let perform some statistics on this data 

50
18:00:26,994 --> 18:00:30,630
we will begin by counting the number of records where the state is california 

51
18:00:32,570 --> 18:00:38,951
you can do this by saying stname = california pipe stats count 

52
18:00:43,282 --> 18:00:48,468
now switch back to the statistics tab see the result here

53
18:00:51,360 --> 18:00:54,775
now let see the total population for california 

54
18:00:54,775 --> 18:01:00,241
you can replace count with sum cenus2010pop 

55
18:01:04,290 --> 18:01:06,923
you can also calculate the average population 

56
18:01:06,923 --> 18:01:08,781
we will replace sum with mean 

1
11:57:08,970 --> 11:57:13,280
hello my name is mitch fleichmann senior instructor here at splunk 

2
11:57:13,280 --> 11:57:15,670
today we are going to install splunk on linux 

3
11:57:17,230 --> 11:57:20,760
i am using one of the splunk education linux servers 

4
11:57:20,760 --> 11:57:21,564
let examine the environment 

5
11:57:24,146 --> 11:57:28,040
first of all notice i am logged in as the user splunker 

6
11:57:28,040 --> 11:57:31,600
as a best practice do not install splunk as the root user 

7
11:57:34,730 --> 11:57:40,498
i am currently in the opt directory where i have already downloaded

8
11:57:40,498 --> 11:57:45,426
the tarball from splunk com download for this platform 

9
11:57:47,954 --> 11:57:52,755
as a final check let check the system to make sure we have the correct operating

10
11:57:52,755 --> 11:57:53,932
system and kernel 

11
11:57:56,680 --> 11:58:03,072
this is indeed a linux machine let get confirmation that it is also 64 bit 

12
11:58:09,580 --> 11:58:11,090
so we are good to go 

13
11:58:11,090 --> 11:58:14,519
the next step is to unzip and untar the installer and

14
11:58:14,519 --> 11:58:17,729
that we can do with the gunzip and tar commands 

15
11:58:25,205 --> 11:58:30,650
when everything untucks you will notice a new sub directory created named splunk 

16
11:58:31,960 --> 11:58:35,670
and we can navigate to the splunk bin directory to start up splunk 

17
11:58:38,450 --> 11:58:43,000
couple of ways to start splunk with no switches 

18
11:58:44,330 --> 11:58:48,890
on the first startup you will be prompted to read and agree to the software license 

19
11:58:50,610 --> 11:58:54,141
or as a shortcut you can start up splunk and accept the license 

20
11:59:02,707 --> 11:59:07,315
and notice a couple of port numbers being grabbed port 8000 for

21
11:59:07,315 --> 11:59:10,902
splunk web and port 8089 for splunk management 

22
11:59:15,003 --> 11:59:16,528
the splunk daemon has started 

23
11:59:19,675 --> 11:59:21,603
splunk is generating its own keys 

24
11:59:24,707 --> 11:59:28,520
and we can see the splunk web interface the url 

25
11:59:33,050 --> 11:59:33,950
as a best practice 

26
11:59:33,950 --> 11:59:38,050
you may also want to consider automating splunk to start when the machine boots 

27
11:59:39,060 --> 11:59:44,635
that you can do as the root user 

28
11:59:44,635 --> 11:59:48,841
by issuing the splunk enable command 

29
11:59:48,841 --> 11:59:53,686
enable boot - start as the - user splunker 

30
11:59:53,686 --> 11:59:58,793
the same user that we just consult splunk with 

31
12:00:07,498 --> 12:00:10,960
so lets log in to splunk web and see how the system looks 

32
12:00:10,960 --> 12:00:12,410
and for that we will go back to the browser 

33
12:00:14,980 --> 12:00:16,230
go to the appropriate url 

34
12:00:20,340 --> 12:00:22,360
and we can see upon first login 

35
12:00:22,360 --> 12:00:27,730
you are prompted to login with the credentials admin password changeme 

36
12:00:35,180 --> 12:00:36,780
since this is the first login 

37
12:00:36,780 --> 12:00:40,170
you also coach to change your password to something more secure 

38
12:00:40,170 --> 12:00:43,218
and it highly recommended to follow this best practice as well 

39
12:00:53,435 --> 12:00:55,027
and also on first connection 

40
12:00:55,027 --> 12:00:58,715
you will also see a splash screen showing you what is new in version 6 

41
12:01:00,160 --> 12:01:01,950
in this case powerful analytics 

42
12:01:03,190 --> 12:01:05,790
some changes to the ui to make it more intuitive 

43
12:01:08,400 --> 12:01:11,919
simplified component management for cluster management 

44
12:01:11,919 --> 12:01:13,647
folder management and so on 

45
12:01:15,737 --> 12:01:17,651
also a richer developer experience 

46
12:01:21,147 --> 12:01:24,560
close down this window and you can explore the navigation options 

47
12:01:25,940 --> 12:01:27,410
and notice in the left side 

48
12:01:27,410 --> 12:01:30,470
you see a panel showing you the apps to navigate to and manage 

49
12:01:31,960 --> 12:01:36,060
and on the right side you see some panel showing you data in the system and

50
12:01:36,060 --> 12:01:37,380
various links for help 

51
12:01:39,730 --> 12:01:41,936
let go to the search and reporting app by clicking here 

52
12:01:46,527 --> 12:01:50,879
and you can see the search far up top 

53
12:01:50,879 --> 12:01:56,788
some tips on how to search and then data to search 

54
12:01:56,788 --> 12:02:00,720
since this is a fresh install there is no data to search so

55
12:02:00,720 --> 12:02:04,049
the next step is to index data and begin searching 

56
12:02:04,049 --> 12:02:04,890
good luck 

1
23:59:11,025 --> 23:59:15,660
 sound hello this is chris busheers 

2
23:59:15,660 --> 23:59:19,881
part of the splunk education team 

3
23:59:19,881 --> 23:59:24,530
in this video i will show you how install splunk onto a window server 

4
23:59:24,530 --> 23:59:27,869
first we need to get the software from the splunk com download page 

5
23:59:29,010 --> 23:59:33,710
we will need to select that the platform is 32 or 64 - bit 

6
23:59:33,710 --> 23:59:36,605
if you are not sure if your system is 32 or

7
23:59:36,605 --> 23:59:39,200
64 - bit you can check your system properties 

8
23:59:40,320 --> 23:59:45,750
as you can see this server is 64 - bit so we can install that version 

9
23:59:45,750 --> 23:59:50,720
if we saw a system type of 32 - bit we would download the 32 - bit version 

10
23:59:51,720 --> 23:59:54,020
we run the installer by double clicking on it 

11
23:59:55,220 --> 23:59:59,920
there is button to view the license agreement and a check box to accept it 

12
00:00:01,310 --> 00:00:04,800
at this point we can either install splunk with the defaults or

13
00:00:04,800 --> 00:00:06,160
customize our installation 

14
00:00:07,290 --> 00:00:11,379
let click on the customize options to see what settings can be selected 

15
00:00:12,660 --> 00:00:16,860
the first option is to change the installation location of splunk 

16
00:00:16,860 --> 00:00:20,940
we are fine with this location so we click next 

17
00:00:20,940 --> 00:00:26,100
now we must choose what account type to install splunk as local system or

18
00:00:26,100 --> 00:00:26,830
domain account 

19
00:00:28,070 --> 00:00:32,420
a local system account will allow splunk to access all data on or

20
00:00:32,420 --> 00:00:34,550
forwarded to this machine 

21
00:00:34,550 --> 00:00:37,140
a domain account will allow you to collect logs and

22
00:00:37,140 --> 00:00:42,070
metrics from remote machines as well as local and forwarded data 

23
00:00:42,070 --> 00:00:46,010
you are required to provide a domain account with the proper domain rights

24
00:00:46,010 --> 00:00:47,730
to use this type 

25
00:00:47,730 --> 00:00:51,530
local system works well for us so we click next 

26
00:00:51,530 --> 00:00:56,012
we can select to have a shortcut to splunk added and click install to continue 

27
00:00:56,012 --> 00:00:59,319
 music 

28
00:00:59,319 --> 00:01:03,750
once installed we can select to have splunk launch and click finish 

29
00:01:03,750 --> 00:01:06,720
this splunk web interface opens in our default browser 

30
00:01:07,750 --> 00:01:12,130
we enter the default user name of admin and a password of change made 

31
00:01:12,130 --> 00:01:15,400
a dialog box appears asking us to change our password 

32
00:01:15,400 --> 00:01:18,310
it is always best practice to do this 

33
00:01:18,310 --> 00:01:21,820
once logged in we are taken to the splunk launcher homepage 

34
00:01:21,820 --> 00:01:24,780
and that all it takes to get splunk installed on windows 

35
00:01:24,780 --> 00:01:26,895
now dig in and start exploring 

36
00:01:26,895 --> 00:01:32,000
 sound 

1
00:00:46,530 --> 00:00:47,180
our third and

2
00:00:47,180 --> 00:00:52,080
final case is applicable to most companies that create customer - focused products 

3
00:00:54,030 --> 00:00:58,270
they want to understand how their customers are responding to the products 

4
00:00:58,270 --> 00:01:00,800
how the product marketing efforts are performing 

5
00:01:00,800 --> 00:01:04,470
what kind of problems customers are encountering and what new features or

6
00:01:04,470 --> 00:01:07,330
feature improvements the customers are seeking and so forth 

7
00:01:08,380 --> 00:01:11,010
but how does the company get this information 

8
00:01:11,010 --> 00:01:14,060
what kind of data sources would carry this information 

9
00:01:14,060 --> 00:01:16,710
the figure show some of these sources 

10
00:01:16,710 --> 00:01:23,210
they are in focused user surveys emails sent by the customers in blogs and

11
00:01:23,210 --> 00:01:29,350
product review forums specialized groups on social media and user forums 

12
00:01:29,350 --> 00:01:35,990
in short they are on the internet or in material received through the internet 

13
00:01:35,990 --> 00:01:37,250
now how many sources are there 

14
00:01:38,830 --> 00:01:40,200
two 

15
00:01:40,200 --> 00:01:41,940
the number would vary 

16
00:01:41,940 --> 00:01:44,060
a new sites a new postings and

17
00:01:44,060 --> 00:01:45,950
new discussion threads would come up all the time 

18
00:01:47,000 --> 00:01:50,870
in all of these the goal is to identify information that

19
00:01:50,870 --> 00:01:55,130
truly relates to the companies product its features and its utility 

20
00:01:57,470 --> 00:02:00,530
to cast this as a type of big data problem 

21
00:02:00,530 --> 00:02:03,871
we look at a task that computer scientists called data fusion 

22
00:02:05,690 --> 00:02:10,840
consider a set of data sources s as we mentioned on the last slide and

23
00:02:10,840 --> 00:02:12,320
a set of data items d 

24
00:02:13,630 --> 00:02:18,420
a data item represents a particular aspect of a real world entity

25
00:02:18,420 --> 00:02:20,290
which in our case is a product of the company 

26
00:02:22,180 --> 00:02:27,860
for each data item a source can but not necessarily will provide a value 

27
00:02:27,860 --> 00:02:29,020
for example 

28
00:02:29,020 --> 00:02:34,450
the usability of an ergonomically split keyboard can have a value good 

29
00:02:35,590 --> 00:02:40,500
the value can be atomic like good or a set or a list or

30
00:02:40,500 --> 00:02:41,910
sometimes embedded in the string 

31
00:02:43,370 --> 00:02:47,270
for example the cursor sometimes freezes when using the touchpad 

32
00:02:48,410 --> 00:02:52,300
is a string which has a value about the touchpad 

33
00:02:54,400 --> 00:02:59,130
the goal of data fusion is to find the values of data items from a source 

34
00:03:01,060 --> 00:03:06,160
in many cases the system would find a unique true value of an item 

35
00:03:06,160 --> 00:03:10,580
for example the launch data of a product in europe should be the same true value

36
00:03:10,580 --> 00:03:12,530
regardless of the data source one looks at 

37
00:03:13,700 --> 00:03:17,520
in other cases we could find a value distribution of an item 

38
00:03:17,520 --> 00:03:20,960
for example the usability of our keyboard may have a value distribution 

39
00:03:22,070 --> 00:03:26,960
that with data fusion we should be able to collect the values of real world

40
00:03:26,960 --> 00:03:29,790
items from a subset of data sources 

41
00:03:29,790 --> 00:03:32,850
it is a subset because not all data sources

42
00:03:32,850 --> 00:03:34,940
will have relevant information about the data item 

43
00:03:36,440 --> 00:03:39,212
there are some other versions of what a data fusion is but for

44
00:03:39,212 --> 00:03:41,992
our purposes we will stick with this general description 

45
00:03:44,190 --> 00:03:48,856
now one obvious problem with the internet is that there are too many data

46
00:03:48,856 --> 00:03:52,530
sources at any time these lead to many difficulties 

47
00:03:53,910 --> 00:03:57,570
first it is to be understood that with too many data sources

48
00:03:57,570 --> 00:04:00,310
there will be many values for the same item 

49
00:04:01,530 --> 00:04:04,190
often these will differ and sometimes they will conflict 

50
00:04:05,580 --> 00:04:08,740
a standard technique in this case is to use a voting mechanism 

51
00:04:10,200 --> 00:04:14,820
however even a voting mechanism can be complex

52
00:04:14,820 --> 00:04:16,350
due to problems with the data source 

53
00:04:18,040 --> 00:04:21,800
one of the problems is to estimate the trustworthiness of the source 

54
00:04:23,130 --> 00:04:25,080
for each data source 

55
00:04:25,080 --> 00:04:31,260
we need to evaluate whether it reporting some basic or known facts correctly 

56
00:04:31,260 --> 00:04:34,510
if a source mentions details about a rainbow colored iphone 

57
00:04:34,510 --> 00:04:38,010
which does not exist it trustworthiness reduces

58
00:04:38,010 --> 00:04:40,960
because of the falsity of the provided value of this data item 

59
00:04:42,270 --> 00:04:46,140
accordingly a higher vote count can be assigned to a more trustworthy source 

60
00:04:47,590 --> 00:04:50,640
and then this can be used in voting 

61
00:04:52,560 --> 00:04:54,260
the second aspect is copy detection 

62
00:04:55,700 --> 00:04:59,670
detecting weather once was has copied information from another can be very

63
00:04:59,670 --> 00:05:02,490
important for detail fusion task in customer analytics 

64
00:05:03,660 --> 00:05:05,680
if a source has copied information 

65
00:05:06,750 --> 00:05:11,900
it such that discounted vote count can be assigned to a copy value and

66
00:05:11,900 --> 00:05:17,190
voting that means the copy in source will have less weight 

67
00:05:18,210 --> 00:05:23,160
now this is especially relevant when we compute value distributions because if we

68
00:05:23,160 --> 00:05:28,680
treat copies as genuine information we will statistically bias the distribution 

69
00:05:28,680 --> 00:05:33,279
now here is active research on how to detect copies how to determine bias and

70
00:05:33,279 --> 00:05:37,685
then arrive at a statistically sound estimation of value distribution 

71
00:05:37,685 --> 00:05:42,628
but to our knowledge these methods are yet to be applied to existing software for

72
00:05:42,628 --> 00:05:44,070
big data integration 

73
00:05:47,473 --> 00:05:49,240
it should be very clear by now but

74
00:05:49,240 --> 00:05:53,240
there are two kinds of big data situations when it comes to information 

75
00:05:54,400 --> 00:05:59,180
the first two uses cases that we saw requires an integration system

76
00:05:59,180 --> 00:06:03,380
to consider all sources because the application demand so 

77
00:06:04,620 --> 00:06:10,660
in contrast problems where data comes from too many redundant potentially

78
00:06:10,660 --> 00:06:15,380
unreliable sources like the internet the best results can be obtained if we have

79
00:06:15,380 --> 00:06:19,490
a way of evaluating the worthiness of sources before information integration 

80
00:06:20,660 --> 00:06:23,790
but this problem is called source selection 

81
00:06:23,790 --> 00:06:27,570
the picture on the right shows the result of a cost benefit analysis for

82
00:06:27,570 --> 00:06:29,080
data fusion 

83
00:06:29,080 --> 00:06:32,239
the x - axis indicates the number of sources used and

84
00:06:32,239 --> 00:06:36,400
the y - axis measures the proportion of true results that were returned 

85
00:06:38,090 --> 00:06:42,480
we can clearly see that the plot peaks around six - to - eight sources and

86
00:06:42,480 --> 00:06:44,990
that the efficiency falls as more sources are added 

87
00:06:46,790 --> 00:06:51,930
in a cost benefit analysis the cost must include both the human and

88
00:06:51,930 --> 00:06:53,580
the computational costs 

89
00:06:53,580 --> 00:06:57,270
while the benefit is a function of the accuracy of the fusion result 

90
00:06:57,270 --> 00:07:02,210
the technique for solving this problem comes from economics 

91
00:07:03,520 --> 00:07:06,870
assuming that cost and benefits are measure in the same unit for

92
00:07:06,870 --> 00:07:07,660
example dollars 

93
00:07:08,780 --> 00:07:11,580
they proposed to continue selecting sources

94
00:07:11,580 --> 00:07:15,740
until the marginal benefit is less than the marginal cost 

95
00:07:17,150 --> 00:07:21,210
now recent techniques were performing this computation at quite scalable 

96
00:07:21,210 --> 00:07:24,910
in one setting selecting the most beneficial sources

97
00:07:24,910 --> 00:07:28,520
from a total of one million sources took less than one hour 

98
00:07:30,900 --> 00:07:34,980
this completes our coverage of the big data integration problems 

1
00:08:17,830 --> 00:08:19,180
hello and welcome 

2
00:08:19,180 --> 00:08:24,180
my name is rene and i will be sharing with you how to create a report using pivot 

3
00:08:24,180 --> 00:08:28,920
to access the pivot interface click pivot on the navigation menu 

4
00:08:30,630 --> 00:08:34,370
the first step is to select a prebuilt data model 

5
00:08:34,370 --> 00:08:38,260
now the data model allows you to create compelling reports and

6
00:08:38,260 --> 00:08:43,090
dashboards without having to know how to write complex search queries 

7
00:08:43,090 --> 00:08:48,000
as a side note data models are typically created by a knowledge manager that

8
00:08:48,000 --> 00:08:51,200
understands their organization index data 

9
00:08:51,200 --> 00:08:55,760
grasps the search language and are familiar with lookups 

10
00:08:55,760 --> 00:08:59,640
transactions field extractions and calculated fields 

11
00:09:00,750 --> 00:09:05,260
for our example we are going to use buttercup games online sales 

12
00:09:07,200 --> 00:09:11,960
now this data model includes the online sales activities 

13
00:09:11,960 --> 00:09:15,100
it made up of nine objects 

14
00:09:15,100 --> 00:09:20,660
each object represents a specific set of events in a hierarchical structure 

15
00:09:22,290 --> 00:09:29,390
the http request would include the largest number of events in this data model 

16
00:09:29,390 --> 00:09:32,540
the next object successful request 

17
00:09:32,540 --> 00:09:37,950
would be a subset of the http request events and so on 

18
00:09:37,950 --> 00:09:42,710
let say that we want to create a report for purchases over the last seven days 

19
00:09:42,710 --> 00:09:45,560
we would select the most appropriate object 

20
00:09:45,560 --> 00:09:49,200
in this case we are going to select successful purchase 

21
00:09:50,910 --> 00:09:54,940
after you have selected the object you will notice that a new

22
00:09:54,940 --> 00:09:56,460
pivot view will display 

23
00:09:57,720 --> 00:10:02,950
if you notice the count of successful purchase is the total

24
00:10:02,950 --> 00:10:07,580
number of events for this specific object 

25
00:10:07,580 --> 00:10:09,780
now it is based off of all time 

26
00:10:11,470 --> 00:10:16,840
before you begin you should have an idea of what type of report you want to create 

27
00:10:16,840 --> 00:10:19,720
so let say that we want to create a bar chart 

28
00:10:19,720 --> 00:10:25,120
we are going to select that from the left navigation pane 

29
00:10:25,120 --> 00:10:29,375
so the third icon will allow me to click and select bar chart 

30
00:10:31,278 --> 00:10:35,130
so at this point we are ready to start building our report 

31
00:10:36,560 --> 00:10:38,310
the first selection is time range 

32
00:10:38,310 --> 00:10:41,480
let set that to the last seven days 

33
00:10:43,030 --> 00:10:47,780
the next one is filter now we do not need to define filter because we have already

34
00:10:47,780 --> 00:10:53,210
selected an object of successful purchase and that what we are reporting against 

35
00:10:54,720 --> 00:10:59,570
now for my x axis i am going to select product name 

36
00:11:00,620 --> 00:11:03,040
you will notice that i can set the label 

37
00:11:03,040 --> 00:11:05,170
i can set the sort order 

38
00:11:05,170 --> 00:11:08,091
i can even set how many maximum bars that i want 

39
00:11:09,728 --> 00:11:13,988
the next thing to define is the y axis 

40
00:11:13,988 --> 00:11:18,970
at this time it set to count of successful purchase 

41
00:11:18,970 --> 00:11:22,325
but i want the total sales by product 

42
00:11:22,325 --> 00:11:24,785
so i am going to select price and

43
00:11:24,785 --> 00:11:29,655
then you will notice that my value is already set to sum 

44
00:11:29,655 --> 00:11:31,475
so i am going to keep it as sum 

45
00:11:33,340 --> 00:11:35,030
my report is starting to look really good 

46
00:11:35,030 --> 00:11:42,370
but after reviewing it i might decide that i want to see values segmented by host 

47
00:11:42,370 --> 00:11:47,670
so let do that in order to set the segmentation

48
00:11:47,670 --> 00:11:55,290
of a specific product name i could go and set the color to the field that i want 

49
00:11:55,290 --> 00:11:56,824
so i am going to select host 

50
00:11:59,936 --> 00:12:06,490
now you will see that my total sales are segmented by host 

51
00:12:07,966 --> 00:12:09,566
so we really like this report 

52
00:12:09,566 --> 00:12:11,190
let go ahead and save it 

53
00:12:12,240 --> 00:12:14,860
so we are going to click on save as 

54
00:12:16,050 --> 00:12:18,560
select report and give it a title 

55
00:12:18,560 --> 00:12:23,810
let call it product sales by host 

56
00:12:23,810 --> 00:12:30,684
we are going to click save and then we are going to view our report 

57
00:12:33,563 --> 00:12:36,880
it was that easy to create a report through pivot 

58
00:12:38,850 --> 00:12:43,030
now let just say that we want to go and work with another pivot 

59
00:12:43,030 --> 00:12:45,810
this time we are going to just go and

60
00:12:45,810 --> 00:12:49,070
create a statistic table to view some of that data 

61
00:12:49,070 --> 00:12:51,917
let go ahead and click pivot again 

62
00:12:53,700 --> 00:12:56,090
we will select the same data model 

63
00:12:56,090 --> 00:13:00,637
buttercup games online sales and let keep to the object that

64
00:13:00,637 --> 00:13:04,855
we are familiar with right now successful purchases 

65
00:13:11,902 --> 00:13:16,920
as i mentioned i want to just simply create a statistic table 

66
00:13:17,990 --> 00:13:20,400
i want to show you how we would go about doing that 

67
00:13:21,620 --> 00:13:28,700
so the first step is to think about what are some of the row data that you want 

68
00:13:28,700 --> 00:13:35,790
let just assume that we want to view the product names within their categories 

69
00:13:35,790 --> 00:13:41,500
so let go ahead under split rows we are going to select category first 

70
00:13:42,530 --> 00:13:45,060
notice that i can set a label if i want 

71
00:13:45,060 --> 00:13:49,500
let go ahead and set it capitalized category 

72
00:13:49,500 --> 00:13:50,897
i am going to add that to my table 

73
00:13:50,897 --> 00:13:55,403
you will see that it becomes the first column 

74
00:13:55,403 --> 00:13:59,240
now i want to add another column 

75
00:13:59,240 --> 00:14:03,535
let go ahead and click the plus right next to categories 

76
00:14:03,535 --> 00:14:06,995
again we are still working with the split rows 

77
00:14:08,065 --> 00:14:13,375
i am going to select product name again if you want to set a label you can do that 

78
00:14:13,375 --> 00:14:15,955
now this is the label of our column 

79
00:14:17,175 --> 00:14:17,675
add to table 

80
00:14:21,367 --> 00:14:26,282
and i want you to notice that now we are viewing each product

81
00:14:26,282 --> 00:14:28,400
within their category 

82
00:14:29,580 --> 00:14:33,130
now at this point if you look at the statistic that is being defined 

83
00:14:33,130 --> 00:14:36,850
it the count of successful purchases 

84
00:14:36,850 --> 00:14:39,570
that is defined as column values 

85
00:14:40,940 --> 00:14:45,960
now if i want to add to this i can append and have another column 

86
00:14:45,960 --> 00:14:51,050
let go ahead and click the plus next to the count of successful purchase 

87
00:14:52,480 --> 00:14:54,300
this time i want to add the price 

88
00:14:55,520 --> 00:14:58,216
and you will notice that i have different values available 

89
00:14:58,216 --> 00:15:01,662
i could do a sum a count an average max min 

90
00:15:01,662 --> 00:15:08,274
let keep it as a sum and let add a label of total sales 

91
00:15:08,274 --> 00:15:10,795
and let add to table 

92
00:15:12,955 --> 00:15:16,675
so this is just to show you how quick it is to go and

93
00:15:16,675 --> 00:15:20,955
build a table with the statistics that you would need 

94
00:15:20,955 --> 00:15:24,977
now at this point if ever you decide i only want to view the data for

95
00:15:24,977 --> 00:15:26,260
the last seven days 

96
00:15:26,260 --> 00:15:30,590
just note that you could go back up to your filter and

97
00:15:30,590 --> 00:15:33,630
your first filter is based off of all time 

98
00:15:33,630 --> 00:15:37,870
so we could set that to the last seven days 

99
00:15:37,870 --> 00:15:40,610
so of course that the data in our table will change 

100
00:15:41,750 --> 00:15:46,030
now the last thing i want to show you here is if ever you did want to see

101
00:15:47,120 --> 00:15:54,075
the count and the total sales per host as we have defined in our previous example 

102
00:15:54,075 --> 00:15:58,615
that is where you could go to the split columns and

103
00:15:58,615 --> 00:16:01,325
define how you wish to split that information 

104
00:16:01,325 --> 00:16:04,825
so let go ahead and select host again 

105
00:16:04,825 --> 00:16:07,731
and i am just going to click on add to table 

106
00:16:12,074 --> 00:16:19,820
now i could see that the information has been split by host 

107
00:16:19,820 --> 00:16:25,460
i have www1 count of successful purchases as well as total sales 

108
00:16:25,460 --> 00:16:32,810
and then i have the split of www2 i have the count and the total sales and etc 

109
00:16:32,810 --> 00:16:37,950
so this concludes our short video hope you enjoy it 

110
00:16:37,950 --> 00:16:38,450
thank you 

1
00:24:56,860 --> 00:24:58,350
welcome 

2
00:24:58,350 --> 00:25:03,570
in this short module we will talk about information integration which refers

3
00:25:03,570 --> 00:25:08,470
to the problem of using many different information sources to accomplish a task 

4
00:25:09,780 --> 00:25:14,939
in this module we will look at the problems and solutions through a few use cases 

5
00:25:17,330 --> 00:25:22,830
so after this video you will be able to explain the data integration problem 

6
00:25:24,250 --> 00:25:29,930
define integrated views and schema mapping describe the impact of increasing

7
00:25:29,930 --> 00:25:35,306
the number of data sources appreciate the need to use data compression 

8
00:25:35,306 --> 00:25:41,910
and describe record linking data exchange and data fusion tasks 

9
00:25:45,008 --> 00:25:49,230
our first use case starts with an example given at an ibm website for

10
00:25:49,230 --> 00:25:50,760
their information integration products 

11
00:25:52,150 --> 00:25:55,580
it represents a very common scenario in today business world 

12
00:25:57,100 --> 00:26:01,040
due to the changing market dynamics companies

13
00:26:01,040 --> 00:26:04,620
are always selling off a part of their company or acquiring another company 

14
00:26:06,330 --> 00:26:10,950
as these mergers and acquisitions happen databases which were developed and

15
00:26:10,950 --> 00:26:14,900
stored separately in different companies would now need to be brought together 

16
00:26:16,680 --> 00:26:18,640
now take a minute to read this case 

17
00:26:23,134 --> 00:26:27,644
this is the case of an expanding financial services group that growing its customer

18
00:26:27,644 --> 00:26:29,260
base in different countries 

19
00:26:31,420 --> 00:26:36,980
and all they want is a single view of their entire customer base 

20
00:26:36,980 --> 00:26:41,590
in other words it does not matter which previous company originally had

21
00:26:41,590 --> 00:26:46,920
the customers suncorp - metway want to consolidate all customer

22
00:26:46,920 --> 00:26:53,130
information as if they were in one single database 

23
00:26:53,130 --> 00:26:57,223
and in reality of course they may not want to buy a huge machine and

24
00:26:57,223 --> 00:27:00,340
migrate every subsidiary company data into it 

25
00:27:01,680 --> 00:27:07,338
what they are looking to create is possibly a software solution which would make all

26
00:27:07,338 --> 00:27:13,710
customer - related data to appear as though they were together as a single database 

27
00:27:13,710 --> 00:27:17,290
this software solution is called an information integration system 

28
00:27:18,590 --> 00:27:22,920
this will help them ensure that they have a uniform set of marketing campaigns for

29
00:27:22,920 --> 00:27:24,040
all their customers 

30
00:27:26,780 --> 00:27:31,893
let try to see hypothetically of course what might be involved in creating

31
00:27:31,893 --> 00:27:36,511
this combined data and what kind of use the integrated data might result in 

32
00:27:36,511 --> 00:27:39,330
so we first create a hypothetical scenario 

33
00:27:40,552 --> 00:27:43,920
although suncorp have a large number of data sources 

34
00:27:43,920 --> 00:27:46,290
we will take a much simpler situation and

35
00:27:46,290 --> 00:27:50,610
have only two data sources from two different financial service companies 

36
00:27:52,010 --> 00:27:53,190
the first data source 

37
00:27:53,190 --> 00:27:58,090
which is an insurance company that manages it data with a relation of dbms 

38
00:27:58,090 --> 00:28:03,660
this database has nine tables where the primary object of information is a policy 

39
00:28:05,040 --> 00:28:08,090
the company offers many different types of policies

40
00:28:08,090 --> 00:28:11,380
sold to individual people by their agents 

41
00:28:11,380 --> 00:28:15,740
now as it true for all insurance companies policyholders pay their monthly

42
00:28:15,740 --> 00:28:21,130
dues and sometimes people make claims against their insurance policies 

43
00:28:21,130 --> 00:28:25,080
when they do the details of the claims are maintained in the database 

44
00:28:26,170 --> 00:28:30,220
these claims can belong to different categories and when the claims have

45
00:28:30,220 --> 00:28:34,780
paid to the claimants the transaction is recorded in the transactions table 

46
00:28:36,350 --> 00:28:38,640
as we have done several times now 

47
00:28:38,640 --> 00:28:41,930
the primary keys of the table are the underlined attributes here 

48
00:28:43,900 --> 00:28:46,000
the second company in our example is a bank 

49
00:28:47,060 --> 00:28:48,850
which also uses a relational database 

50
00:28:50,240 --> 00:28:54,500
in this bank both individuals and businesses called corporations here 

51
00:28:54,500 --> 00:28:55,140
can have accounts 

52
00:28:57,900 --> 00:29:00,050
now accounts can be of different types 

53
00:29:00,050 --> 00:29:02,900
for example a money market account is different from a savings account 

54
00:29:04,330 --> 00:29:08,910
a bank also maintains its transactions in a table which can be really large 

55
00:29:10,620 --> 00:29:15,050
but the dispute in a bank record case happens when the bank is

56
00:29:15,050 --> 00:29:19,690
charged a customer or the customer has declined responsibility of the charge 

57
00:29:19,690 --> 00:29:24,030
this can happen for example if a customer internet account was hacked or

58
00:29:24,030 --> 00:29:25,260
a debit card got stolen 

59
00:29:26,700 --> 00:29:28,800
the bank keeps a record of these anomalies and

60
00:29:28,800 --> 00:29:33,260
fraudulent events in a disputes table all right 

61
00:29:33,260 --> 00:29:37,730
let see what happens after the data from these two subsidiary companies

62
00:29:37,730 --> 00:29:38,640
are integrated 

63
00:29:41,410 --> 00:29:45,970
after the merger the company wants to do a promotional goodwill activity 

64
00:29:46,970 --> 00:29:49,490
they would like to offer a small discount to their

65
00:29:49,490 --> 00:29:54,130
insurance policyholders if they are also customers of the newly acquired bank 

66
00:29:55,350 --> 00:29:57,650
how do you identify these customers 

67
00:29:57,650 --> 00:29:58,960
let see 

68
00:29:58,960 --> 00:30:03,882
in other words we need to use the table shown on the left to create the table

69
00:30:03,882 --> 00:30:07,103
shown on the right called discount candidates 

70
00:30:07,103 --> 00:30:11,547
one is to create a yellow tables from the insurance company database and

71
00:30:11,547 --> 00:30:16,351
the blue table from the bank database and then join them to construct the table

72
00:30:16,351 --> 00:30:20,960
with a common customer id and both the policykey and bank account number 

73
00:30:22,290 --> 00:30:26,600
now this relation which is derived that is computed by querying two different

74
00:30:26,600 --> 00:30:31,260
data sources and combining their results is called an integrated view 

75
00:30:32,330 --> 00:30:37,400
it is integrated because the data is retrieved from different data sources 

76
00:30:38,610 --> 00:30:42,640
and it called a view because in database terminology

77
00:30:42,640 --> 00:30:45,680
it is a relation computed from other relations 

78
00:30:48,498 --> 00:30:51,780
to populate the integrated view discount candidates 

79
00:30:51,780 --> 00:30:54,470
we need to go through a step called schema mapping 

80
00:30:55,580 --> 00:30:59,550
the term mapping means to establish correspondence between

81
00:30:59,550 --> 00:31:03,850
the attributes of the view which is also called a target relation and

82
00:31:03,850 --> 00:31:05,120
that of the source relations 

83
00:31:06,200 --> 00:31:11,270
for example we can map the full address from individuals to the address attribute

84
00:31:11,270 --> 00:31:16,380
in discountcandidates but this would only be true for

85
00:31:16,380 --> 00:31:20,300
customers whose names and addresses match in the two databases 

86
00:31:22,040 --> 00:31:27,120
as you can see policyholders uses the full name of a customer whereas

87
00:31:27,120 --> 00:31:31,050
individuals has it broken down into first name middle initial and last name 

88
00:31:32,520 --> 00:31:37,760
on the other hand full address is a single field in individuals but

89
00:31:37,760 --> 00:31:41,397
represented in full attributes in the policyholders relation 

90
00:31:43,090 --> 00:31:46,650
the mappings of account number and policykey are more straightforward 

91
00:31:47,680 --> 00:31:50,752
well what about customer id which does not correspond to anything in the four

92
00:31:50,752 --> 00:31:52,300
input relations 

93
00:31:52,300 --> 00:31:53,959
we will come back to this later on 

94
00:31:56,019 --> 00:31:59,470
okay now we will define an integrated relation 

95
00:31:59,470 --> 00:32:01,020
how do we query 

96
00:32:01,020 --> 00:32:02,090
for example 

97
00:32:02,090 --> 00:32:06,170
how do you find the bank account number of a person whose policykey is known 

98
00:32:07,530 --> 00:32:10,020
you might think what is the problem here 

99
00:32:10,020 --> 00:32:11,590
we have a table 

100
00:32:11,590 --> 00:32:17,020
just say select account number from discount candidates where policykey

101
00:32:17,020 --> 00:32:21,680
is equal to 4 - 937528734 and we are done 

102
00:32:23,730 --> 00:32:30,310
well yes you can write this query but how the query be evaluated 

103
00:32:30,310 --> 00:32:32,820
that depends on what is called the query

104
00:32:32,820 --> 00:32:35,610
architecture of the data integration system 

105
00:32:35,610 --> 00:32:38,130
the figure on the left shows the elements of this architecture 

106
00:32:39,290 --> 00:32:42,694
we will discover it in more detail but on this slide 

107
00:32:42,694 --> 00:32:45,810
we will just describe the three axes of this cube 

108
00:32:46,830 --> 00:32:52,090
the vertical z axis specifies whether we have one data source or

109
00:32:52,090 --> 00:32:53,320
multiple data sources 

110
00:32:54,320 --> 00:32:57,390
our interest is in the case where there are multiple data sources 

111
00:32:59,218 --> 00:33:05,110
the x axis asks whether the integrated data is actually stored physically

112
00:33:05,110 --> 00:33:11,280
in some place or whether it is computed on the fly each time a query is asked 

113
00:33:11,280 --> 00:33:17,120
if it is all precomputed and stored we say that the data is materialized 

114
00:33:17,120 --> 00:33:19,940
and if it is computed on the fly we say it virtual 

115
00:33:21,480 --> 00:33:25,174
the y axis asks whether there is a single schema or

116
00:33:25,174 --> 00:33:29,319
global schema defined all over the data integrated for

117
00:33:29,319 --> 00:33:34,364
an application or whether the data stay in different computers and

118
00:33:34,364 --> 00:33:38,531
it is accessed in a peer - to - peer manner at runtime 

119
00:33:38,531 --> 00:33:43,267
thus the seemingly simple select project query will be evaluated

120
00:33:43,267 --> 00:33:48,870
depending on which part of the cube our architecture implements 

121
00:33:48,870 --> 00:33:51,880
but for now let return to our example use case 

122
00:33:54,900 --> 00:33:58,920
an obvious goal of an information integration system

123
00:33:58,920 --> 00:34:00,260
is to be complete and accurate 

124
00:34:01,770 --> 00:34:05,790
complete means no eligible record from the source should be absent in

125
00:34:05,790 --> 00:34:06,670
the target relation 

126
00:34:07,970 --> 00:34:13,668
accurate means all the entries in the integrated relation should be correct 

127
00:34:13,668 --> 00:34:18,148
now we said on the previous slide that a matching

128
00:34:18,148 --> 00:34:22,740
customer is a person who was in both databases and

129
00:34:22,740 --> 00:34:27,590
has the same name and address in the two databases 

130
00:34:27,590 --> 00:34:29,310
now let look at some example records 

131
00:34:30,390 --> 00:34:34,110
specifically consider the records marked by the three arrows 

132
00:34:35,510 --> 00:34:40,320
the two bank accounts and the policy record do not match for name or

133
00:34:40,320 --> 00:34:42,050
for address 

134
00:34:42,050 --> 00:34:44,360
so our previous method would discard them 

135
00:34:45,510 --> 00:34:47,030
but look at the records closely 

136
00:34:48,380 --> 00:34:51,070
do you think they might all belong to the same customer 

137
00:34:52,560 --> 00:34:54,740
maybe this lady has a maiden name and

138
00:34:54,740 --> 00:34:58,150
a married name and has moved from one address to another 

139
00:34:59,290 --> 00:35:02,289
maybe she changed her social security number somewhere along the way 

140
00:35:03,820 --> 00:35:06,640
so this is called a record linkage problem 

141
00:35:07,730 --> 00:35:12,510
that means we would like to ensure that the set of data records that belong to

142
00:35:12,510 --> 00:35:17,040
a single entity are recognized perhaps by clustering

143
00:35:17,040 --> 00:35:21,790
the values of different attributes or by using a set of matching rules so

144
00:35:21,790 --> 00:35:26,130
that we know how to deal with it during the integration process 

145
00:35:26,130 --> 00:35:28,600
for example we need to determine

146
00:35:28,600 --> 00:35:31,250
which of the addresses should be used in the integrated relation 

147
00:35:32,570 --> 00:35:33,650
which of the two bank accounts 

148
00:35:35,070 --> 00:35:39,800
if the answer is both accounts 102 and 103 we will need to change

149
00:35:39,800 --> 00:35:45,380
the schema of the target relation to a list instead of an atomic number

150
00:35:45,380 --> 00:35:48,290
to avoid creating multiple tuples for the same entity 

151
00:35:50,990 --> 00:35:55,380
as we saw the schema of adding process is a task of figuring out

152
00:35:55,380 --> 00:35:59,850
how elements of the schema from two sources would relate to each other and

153
00:35:59,850 --> 00:36:02,940
determining how they would map the target schema 

154
00:36:03,980 --> 00:36:08,180
you also saw that this is not really a simple process that we are trying to

155
00:36:08,180 --> 00:36:13,600
produce one integrated relation using a couple of relations from each source 

156
00:36:15,130 --> 00:36:20,250
in a big data situation there are dozens of data sources or

157
00:36:20,250 --> 00:36:26,880
more because the company growing and each source may have a few hundred tables 

158
00:36:26,880 --> 00:36:30,370
so it becomes very hard to actually solve

159
00:36:30,370 --> 00:36:35,419
this correspondence - making problem completely and accurately just because

160
00:36:35,419 --> 00:36:39,920
the number of combinations one has to go through is really really high 

161
00:36:42,120 --> 00:36:47,250
one practical way to tackle this problem is not to do a full - scale detail

162
00:36:47,250 --> 00:36:52,147
integration in the beginning but adopt what is called a pay - as - you - go model 

163
00:36:52,147 --> 00:36:55,189
the pay - as - you - go data management principle is simple 

164
00:36:56,380 --> 00:37:01,214
the system should provide some basic integration services at the outset and

165
00:37:01,214 --> 00:37:06,123
then evolve the schema mappings between the different sources on an as needed

166
00:37:06,123 --> 00:37:07,660
basis 

167
00:37:07,660 --> 00:37:12,190
so given a query the system should generate a best effort or approximate

168
00:37:12,190 --> 00:37:16,310
answers from the data sources for a perfect schema mappings do not exist 

169
00:37:17,310 --> 00:37:21,015
when it discovers a large number of sophisticated queries or

170
00:37:21,015 --> 00:37:25,375
data mining tasks over certain sources it will guide the users to make

171
00:37:25,375 --> 00:37:29,387
additional efforts to integrate these sources more precisely 

172
00:37:31,401 --> 00:37:35,270
okay so how does the first approximate schema mapping performed 

173
00:37:36,660 --> 00:37:41,920
one approach to do this is called probabilistic schema mapping 

174
00:37:41,920 --> 00:37:43,920
we will describe it in more detail next 

175
00:37:46,120 --> 00:37:47,450
in the previous step 

176
00:37:47,450 --> 00:37:53,050
we just decided to create the disk count candidates rrelation in an ad hoc way 

177
00:37:53,050 --> 00:37:54,990
and in a big data situation 

178
00:37:54,990 --> 00:37:58,920
we need to carefully determine what the integrated schema 

179
00:37:58,920 --> 00:38:03,000
also called mediated schemas should be and we should evaluate them properly 

180
00:38:04,640 --> 00:38:10,550
since our toy company is trying to create a single customer view 

181
00:38:10,550 --> 00:38:14,660
it natural to create an integrated table called customers 

182
00:38:14,660 --> 00:38:16,690
but how can we design this table 

183
00:38:16,690 --> 00:38:17,550
here are some options 

184
00:38:18,820 --> 00:38:24,487
we can create the customer table to include individuals and corporations and

185
00:38:24,487 --> 00:38:29,384
then use a flag called customer type to distinguish between them 

186
00:38:29,384 --> 00:38:34,051
now in the mediated schema then the individuals first name 

187
00:38:34,051 --> 00:38:38,191
middle initial last name policyholder name and

188
00:38:38,191 --> 00:38:43,140
corporation name would all map to customer name similarly 

189
00:38:44,390 --> 00:38:48,978
the individual full address the corporation registered address 

190
00:38:48,978 --> 00:38:52,440
and the policyholder address plus city plus state 

191
00:38:52,440 --> 00:38:56,530
plus zip would all map to customer address 

192
00:38:56,530 --> 00:39:01,010
now we can enumerate all such choices of which attributes to group together and

193
00:39:01,010 --> 00:39:04,230
map for each single attribute in the target schema 

194
00:39:05,620 --> 00:39:10,630
but no matter how you will do it it will never be a perfect fit because

195
00:39:10,630 --> 00:39:13,590
not all these combinations would go well together 

196
00:39:13,590 --> 00:39:17,160
for example should that date of birth be included in this table 

197
00:39:18,550 --> 00:39:19,870
would it make sense for corporations 

198
00:39:21,810 --> 00:39:27,310
in probabilistic mediated schema design we answer this question by

199
00:39:27,310 --> 00:39:30,410
associating probability values with each of these options 

200
00:39:33,040 --> 00:39:37,410
to compute these values we need to quantify the relationships between

201
00:39:37,410 --> 00:39:41,202
attributes by figuring out which attributes should be grouped or

202
00:39:41,202 --> 00:39:43,710
clustered together 

203
00:39:43,710 --> 00:39:47,140
now two pieces of information available in the source schemas

204
00:39:47,140 --> 00:39:50,497
can serve as evidence for attribute clustering 

205
00:39:50,497 --> 00:39:54,740
one the parallel similarity of source attributes and two 

206
00:39:54,740 --> 00:39:59,640
statistical properties of service attributes 

207
00:40:00,800 --> 00:40:05,260
the first piece of information indicates when two attributes are likely to be

208
00:40:05,260 --> 00:40:09,459
similar and is used for creating multiple mediated schemas 

209
00:40:10,680 --> 00:40:14,050
one can apply a collection of attribute matching modules

210
00:40:14,050 --> 00:40:16,650
to compute pairwise similarity 

211
00:40:16,650 --> 00:40:18,755
for example individual names and

212
00:40:18,755 --> 00:40:21,700
policyholder names are possibly quite similar 

213
00:40:22,930 --> 00:40:27,878
are individual names versus corporation names similar 

214
00:40:27,878 --> 00:40:32,283
now the similarity between two source attributes ein and ez 

215
00:40:32,283 --> 00:40:37,560
measure how closely the two attributes represent the same real world concept 

216
00:40:39,520 --> 00:40:42,230
the second piece of information indicates

217
00:40:42,230 --> 00:40:46,650
when two attributes are likely to be different and is used for

218
00:40:46,650 --> 00:40:49,599
assigning probabilities to each of the mediated schemas 

219
00:40:51,370 --> 00:40:54,770
for example date of birth and

220
00:40:54,770 --> 00:40:57,410
corporation name possibly will never co - occur together 

221
00:40:59,050 --> 00:41:02,250
but for large schemas with large data volumes 

222
00:41:02,250 --> 00:41:06,980
one can estimate these measures by taking samples from the actual database

223
00:41:06,980 --> 00:41:11,312
to come up with reasonable similarity co - occurrence scores 

224
00:41:11,312 --> 00:41:17,658
to illustrate attribute regrouping we take a significant re - simplified example 

225
00:41:17,658 --> 00:41:22,543
here we want to create customer transactions as a mediated relation

226
00:41:22,543 --> 00:41:27,180
based upon the bank transactions and insurance transactions 

227
00:41:28,530 --> 00:41:31,159
each attribute is given an abbreviation for simplicity 

228
00:41:33,030 --> 00:41:36,575
below you can see three possible mediated schemas 

229
00:41:37,770 --> 00:41:42,140
in the first one the transaction begin and end times

230
00:41:42,140 --> 00:41:47,270
from bank transactions are grouped into the same cluster as transaction date time

231
00:41:47,270 --> 00:41:51,040
from the insurance transactions because all of them are the same type 

232
00:41:52,280 --> 00:41:57,000
similarly transaction party that means who is giving or receiving money 

233
00:41:58,010 --> 00:42:01,610
and transaction description are grouped together with transaction details 

234
00:42:03,490 --> 00:42:06,130
the second schema keeps all of them separate 

235
00:42:07,320 --> 00:42:11,145
and the third candidate schema groups some of them and not others 

236
00:42:14,998 --> 00:42:18,860
now that we have multiple mediated schemas which one should we choose 

237
00:42:20,030 --> 00:42:22,990
now i am presenting here a qualitative account of the method 

238
00:42:24,280 --> 00:42:27,730
the primary goal is to look for what we can call consistency 

239
00:42:29,430 --> 00:42:33,260
a source schema is consistent with a mediated schema if

240
00:42:33,260 --> 00:42:38,070
two different attributes of the source schema do not occur in one cluster 

241
00:42:39,190 --> 00:42:43,990
and in the example med3 that means related schema three 

242
00:42:43,990 --> 00:42:49,560
is more consistent with bank transactions because unlike med1 

243
00:42:49,560 --> 00:42:53,700
it keeps tbt tet in two different clusters 

244
00:42:54,800 --> 00:43:01,090
once this is done we can count the number of consistent sources for each candidate

245
00:43:01,090 --> 00:43:05,650
mediated schema and then use this count to come up with a probability estimate 

246
00:43:06,890 --> 00:43:11,100
this estimate can then be used to choose the k best schemas 

247
00:43:12,590 --> 00:43:17,030
should one ever choose more than one just best schema 

248
00:43:17,030 --> 00:43:18,860
well that a hard question to answer in general 

249
00:43:20,280 --> 00:43:24,932
it is done when the top capability estimates are very close to each other 

1
01:08:19,025 --> 01:08:23,576
 sound the splunk platform for operational intelligence is

2
01:08:23,576 --> 01:08:29,493
a revolutionary suite of products that is unlocking unprecedented value for

3
01:08:29,493 --> 01:08:32,779
thousands of customers around the world 

4
01:08:32,779 --> 01:08:34,970
why splunk 

5
01:08:34,970 --> 01:08:37,420
it all starts with machine data 

6
01:08:37,420 --> 01:08:39,970
machine data is the big data generated by

7
01:08:39,970 --> 01:08:42,710
all the technologies that power our businesses 

8
01:08:42,710 --> 01:08:47,320
from the applications servers websites and network devices in the data center and

9
01:08:47,320 --> 01:08:50,300
the cloud to the mobile device in the palm of your hand 

10
01:08:51,340 --> 01:08:56,460
thermostats train sensors electric cars and the internet of things 

11
01:08:56,460 --> 01:08:58,412
machine data is everywhere 

12
01:08:58,412 --> 01:09:00,710
it fast - growing and complex 

13
01:09:00,710 --> 01:09:03,550
it also incredibly valuable why 

14
01:09:03,550 --> 01:09:07,630
because it contains a definitive record of all activity and behavior 

15
01:09:08,970 --> 01:09:13,380
splunk software collects and indexes this data at massive scale 

16
01:09:13,380 --> 01:09:17,340
from wherever it generated regardless of format or source 

17
01:09:17,340 --> 01:09:21,290
users can quickly and easily monitor search analyze and

18
01:09:21,290 --> 01:09:24,680
report on their data all in real time 

19
01:09:24,680 --> 01:09:26,640
machine data is different 

20
01:09:26,640 --> 01:09:30,450
it cannot be processed and analyzed using traditional methods 

21
01:09:30,450 --> 01:09:35,540
splunk software does not rely on brittle schemas and inflexible databases 

22
01:09:35,540 --> 01:09:38,660
splunk is easy to deploy easy to use and

23
01:09:38,660 --> 01:09:44,290
easy to scale whether on premises or in a public private or hybrid cloud 

24
01:09:44,290 --> 01:09:46,960
splunk is also available as a cloud service 

25
01:09:46,960 --> 01:09:50,942
and for big data environments that used hadoop for cheap app storage 

26
01:09:50,942 --> 01:09:53,522
we have hunk splunk analytics for hadoop 

27
01:09:53,522 --> 01:09:55,950
 music 

28
01:09:55,950 --> 01:09:58,843
our customers from around the world illustrate why so

29
01:09:58,843 --> 01:10:01,070
many organizations use splunk 

30
01:10:01,070 --> 01:10:05,710
intuit has standardized on splunk delivering operational visibility for

31
01:10:05,710 --> 01:10:11,060
their leading online products including quickbooks quicken and turbotax 

32
01:10:11,060 --> 01:10:14,770
intuit considers splunk one of their cornerstone technologies

33
01:10:14,770 --> 01:10:18,950
that is helping them innovate and deliver better service to their customers 

34
01:10:18,950 --> 01:10:22,210
cisco one of the world largest technology providers 

35
01:10:22,210 --> 01:10:25,940
empowers their global security team with splunk enterprise

36
01:10:25,940 --> 01:10:30,220
to gain a centralized view into end user and system activities 

37
01:10:30,220 --> 01:10:34,000
splunk has dramatically helped improve their incident detection and

38
01:10:34,000 --> 01:10:34,690
response rate 

39
01:10:36,000 --> 01:10:39,650
splunk was key to domino pizza success during the super bowl 

40
01:10:39,650 --> 01:10:43,020
by monitoring database uptime and order response 

41
01:10:43,020 --> 01:10:45,710
armed with new levels of customer understanding 

42
01:10:45,710 --> 01:10:49,490
domino was able to strengthen their online business in addition to saving

43
01:10:49,490 --> 01:10:53,960
hundreds of thousands of dollars replacing legacy technologies with splunk 

44
01:10:53,960 --> 01:10:58,350
we are thrilled to hear domino call splunk their secret sauce 

45
01:10:58,350 --> 01:11:02,200
another great splunk story includes cars and the internet of things 

46
01:11:03,810 --> 01:11:07,813
working together with the ford motor company and ford openxc platform 

47
01:11:07,813 --> 01:11:11,816
splunk delivered connected car dashboards to examine driving behavior and

48
01:11:11,816 --> 01:11:13,079
vehicle performance 

49
01:11:13,079 --> 01:11:17,529
uk - based tesco is one of the world largest retailers 

50
01:11:17,529 --> 01:11:23,650
with nearly 1 million customers and a half billion online orders per week 

51
01:11:23,650 --> 01:11:26,480
customer satisfaction is a critical metric 

52
01:11:26,480 --> 01:11:31,580
tesco deployed splunk to gain a unified view across their websites transactions 

53
01:11:31,580 --> 01:11:36,410
and business gaining valuable digital intelligence about their customers 

54
01:11:36,410 --> 01:11:41,820
splunk was founded to pursue a disruptive vision to make machine data accessible 

55
01:11:41,820 --> 01:11:44,490
usable and valuable to everyone 

56
01:11:44,490 --> 01:11:46,280
find out more about splunk and

57
01:11:46,280 --> 01:11:49,820
operational intelligence by downloading the executive summary 

58
01:11:49,820 --> 01:11:50,660
or better yet 

59
01:11:50,660 --> 01:11:55,230
experience the value firsthand by downloading splunk software for free 

60
01:11:55,230 --> 01:11:58,565
chances are someone in your organization already has 

61
01:11:58,565 --> 01:12:04,649
 sound 

1
02:20:25,630 --> 02:20:27,630
aggregations in big data pipelines 

2
02:20:30,220 --> 02:20:33,680
after this video you will be able to compare and

3
02:20:33,680 --> 02:20:38,740
select the aggregation operation that you require to solve your problem 

4
02:20:38,740 --> 02:20:44,650
explain how you can use aggregations to compact your dataset and reduce volume 

5
02:20:44,650 --> 02:20:45,890
that is in many cases 

6
02:20:47,260 --> 02:20:52,586
and design complex operations in your pipeline using a series of aggregations 

7
02:20:55,313 --> 02:21:02,189
aggregation is any operation on a data set that performs a specific transformation 

8
02:21:02,189 --> 02:21:06,979
taking all the related data elements into consideration 

9
02:21:08,060 --> 02:21:12,550
let say we have a bunch of stars which are of different colors 

10
02:21:13,570 --> 02:21:17,550
different colors denote diversity or variety in the data 

11
02:21:19,260 --> 02:21:25,610
to keep things simple we will use letter f to denote a transformation 

12
02:21:26,700 --> 02:21:27,720
in the following slides 

13
02:21:27,720 --> 02:21:33,360
we will see examples of how f can take the shape of different transformations 

14
02:21:36,550 --> 02:21:41,390
if we apply a transformation that does something using the information of

15
02:21:41,390 --> 02:21:45,430
all the stars here we are performing an aggregation 

16
02:21:47,070 --> 02:21:52,850
loosely speaking we can say that applying a transformation f 

17
02:21:52,850 --> 02:21:57,830
that takes all the elements of data as input is called aggregation 

18
02:22:01,490 --> 02:22:07,400
one of the simplest aggregations is summation over all the data elements 

19
02:22:07,400 --> 02:22:11,750
in this case let say every star counted as 1 

20
02:22:11,750 --> 02:22:15,586
summing over all the stars gives 14 

21
02:22:15,586 --> 02:22:20,119
which is the summation of 3 stars for yellow 

22
02:22:20,119 --> 02:22:24,897
5 stars for green and 6 stars for color pink 

23
02:22:27,779 --> 02:22:33,513
another aggregation that you could perform is summation of individual star colors 

24
02:22:33,513 --> 02:22:36,070
that is grouping the sums by color 

25
02:22:37,200 --> 02:22:43,360
so if each star is a 1 adding each group will result in 3 for

26
02:22:43,360 --> 02:22:48,530
yellow stars 5 for green stars and 6 for pink stars 

27
02:22:49,860 --> 02:22:50,952
in this case 

28
02:22:50,952 --> 02:22:56,611
the aggregation function f will output 3 tuples of star colors and counts 

29
02:22:59,065 --> 02:23:04,190
in a sales scenario each color could denote a different product type 

30
02:23:05,470 --> 02:23:10,320
and the number 1 could be replaced by revenue generated by a product

31
02:23:10,320 --> 02:23:12,230
in each city the product is sold 

32
02:23:13,600 --> 02:23:17,250
in fact we will keep coming back to this analogy 

33
02:23:19,420 --> 02:23:24,188
you can also perform average over items of similar kind 

34
02:23:24,188 --> 02:23:29,150
such as sums grouped by color 

35
02:23:31,060 --> 02:23:33,630
continuing the example earlier 

36
02:23:33,630 --> 02:23:38,060
you can calculate average revenue per product type using this aggregation 

37
02:23:40,530 --> 02:23:45,270
other simple yet useful aggregational operations to help you extract meaning

38
02:23:45,270 --> 02:23:52,010
from large data sets are maximum minimum and standard deviation 

39
02:23:52,010 --> 02:23:57,090
remember you can always perform aggregation as a series of operations 

40
02:23:57,090 --> 02:24:01,140
such as maximum of the sums per product 

41
02:24:01,140 --> 02:24:04,510
that is summation followed by maximum 

42
02:24:05,725 --> 02:24:09,920
if you first sum sales for each city that is for

43
02:24:09,920 --> 02:24:14,640
each product then you can take the maximum of it

44
02:24:14,640 --> 02:24:20,050
by applying maximum function to the result of the summation function 

45
02:24:20,050 --> 02:24:24,870
in this case you get the product which has maximum sales in the country 

46
02:24:27,040 --> 02:24:31,084
aggregation over boolean data sets that can have true - false or

47
02:24:31,084 --> 02:24:37,430
one - zero values could be a complex mixture of and or and not logical operations 

48
02:24:39,030 --> 02:24:43,110
a lot of problems become easy to manipulate using sets 

49
02:24:43,110 --> 02:24:47,008
because sets do not allow duplicate values 

50
02:24:47,008 --> 02:24:50,710
depending on your application this could be very useful 

51
02:24:51,800 --> 02:24:56,730
for example to count the number of products from a sales table 

52
02:24:56,730 --> 02:25:01,920
you can simply take all the sales tables and create sets

53
02:25:01,920 --> 02:25:07,130
of these products in those tables and take a union of these sets 

54
02:25:09,090 --> 02:25:15,130
to summarize by choosing the right aggregation you can generate compact and

55
02:25:15,130 --> 02:25:21,720
meaningful insights that enable faster and effective decision making in business 

56
02:25:21,720 --> 02:25:23,960
you will find that in most cases 

57
02:25:23,960 --> 02:25:27,270
aggregation results in smaller output data sets 

58
02:25:28,450 --> 02:25:33,140
hence aggregation is an important tool set to keep in pocket when dealing with

59
02:25:33,140 --> 02:25:35,669
large data sets and big data pipelines 

1
04:46:00,723 --> 04:46:03,960
big data processing pipelines : a dataflow approach 

2
04:46:05,160 --> 04:46:09,420
most big data applications are composed of a set of operations

3
04:46:09,420 --> 04:46:11,980
executed one after another as a pipeline 

4
04:46:12,980 --> 04:46:15,520
data flows through these operations 

5
04:46:15,520 --> 04:46:18,177
going through various transformations along the way 

6
04:46:18,177 --> 04:46:22,080
we also call this dataflow graphs 

7
04:46:22,080 --> 04:46:25,625
so to understand big data processing we should start by

8
04:46:25,625 --> 04:46:28,100
understanding what dataflow means 

9
04:46:29,820 --> 04:46:34,781
after this video you will be able to summarize what dataflow means and

10
04:46:34,781 --> 04:46:36,844
it role in data science 

11
04:46:36,844 --> 04:46:42,096
explain split - do - merge as a big data pipeline with examples 

12
04:46:42,096 --> 04:46:44,770
and define the term data parallel 

13
04:46:46,910 --> 04:46:51,484
let consider the hello world mapreduce example for

14
04:46:51,484 --> 04:46:55,461
wordcount which reads one or more text files and

15
04:46:55,461 --> 04:47:01,043
counts the number of occurrences of each word in these text files 

16
04:47:01,043 --> 04:47:05,483
you are by now very familiar with this example but as a reminder 

17
04:47:05,483 --> 04:47:08,891
the output will be a text file with a list of words and

18
04:47:08,891 --> 04:47:12,250
their occurrence frequencies in the input data 

19
04:47:14,451 --> 04:47:18,985
in this application the files were first split into hdfs

20
04:47:18,985 --> 04:47:23,990
cluster nodes as partitions of the same file or multiple files 

21
04:47:25,560 --> 04:47:28,680
then a map operation in this case 

22
04:47:28,680 --> 04:47:34,340
a user defined function to count words was executed on each of these nodes 

23
04:47:34,340 --> 04:47:40,460
and all the key values that were output from map were sorted based on the key 

24
04:47:40,460 --> 04:47:45,380
and the key values with the same word were moved or shuffled to the same node 

25
04:47:46,920 --> 04:47:52,130
finally the reduce operation was executed on these nodes

26
04:47:52,130 --> 04:47:55,900
to add the values for key - value pairs with the same keys 

27
04:47:57,920 --> 04:48:03,880
if you look back at this example we see that there were four distinct steps 

28
04:48:03,880 --> 04:48:09,070
namely the data split step the map step the shuffle and

29
04:48:09,070 --> 04:48:11,030
sort step and the reduce step 

30
04:48:12,310 --> 04:48:16,417
although the word count example is pretty simple it

31
04:48:16,417 --> 04:48:20,980
represents a large number of applications that these three

32
04:48:20,980 --> 04:48:25,652
steps can be applied to achieve data parallel scalability 

33
04:48:25,652 --> 04:48:31,290
we refer in general to this pattern as split - do - merge 

34
04:48:33,040 --> 04:48:39,490
in these applications data flows through a number of steps going through

35
04:48:39,490 --> 04:48:44,990
transformations with various scalability needs leading to a final product 

36
04:48:46,180 --> 04:48:48,310
the data first gets partitioned 

37
04:48:49,430 --> 04:48:54,450
the split data goes through a set of user - defined functions to do something 

38
04:48:55,840 --> 04:49:00,420
ranging from statistical operations to data joins to machine learning functions 

39
04:49:01,870 --> 04:49:06,510
depending on the application data processing needs 

40
04:49:06,510 --> 04:49:11,350
these do something operations can differ and can be chained together 

41
04:49:12,620 --> 04:49:18,130
in the end results can be combined using a merging algorithm or

42
04:49:18,130 --> 04:49:20,300
a higher - order function like reduce 

43
04:49:21,580 --> 04:49:25,520
we call the stitched - together version of these sets of steps for

44
04:49:25,520 --> 04:49:28,970
big data processing big data pipelines 

45
04:49:31,700 --> 04:49:36,180
the term pipe comes from a unix separation that

46
04:49:36,180 --> 04:49:41,490
the output of one running program gets piped into the next program as an input 

47
04:49:41,490 --> 04:49:45,870
as you might imagine one can string multiple programs together

48
04:49:45,870 --> 04:49:50,560
to make longer pipelines with various scalability needs at each step 

49
04:49:51,740 --> 04:49:54,750
however for big data processing 

50
04:49:54,750 --> 04:50:00,110
the parallelism of each step in the pipeline is mainly data parallelism 

51
04:50:00,110 --> 04:50:05,284
we can simply define data parallelism as running the same functions

52
04:50:05,284 --> 04:50:11,380
simultaneously for the elements or partitions of a dataset on multiple cores 

53
04:50:11,380 --> 04:50:15,447
for example in our word count example data

54
04:50:15,447 --> 04:50:19,720
parallelism occurs in every step of the pipeline 

55
04:50:20,770 --> 04:50:24,530
there definitely parallelization during map over the input

56
04:50:24,530 --> 04:50:28,460
as each partition gets processed as a line at a time 

57
04:50:28,460 --> 04:50:30,920
to achieve this type of data parallelism 

58
04:50:30,920 --> 04:50:35,560
we must decide on the data granularity of each parallel computation 

59
04:50:35,560 --> 04:50:36,930
in this case it is a line 

60
04:50:39,022 --> 04:50:44,560
we also see a parallel grouping of data in the shuffle and sort phase 

61
04:50:44,560 --> 04:50:48,630
this time the parallelization is over the intermediate products 

62
04:50:48,630 --> 04:50:51,050
that is the individual key - value pairs 

63
04:50:53,140 --> 04:50:56,810
and after the grouping of the intermediate products

64
04:50:56,810 --> 04:51:01,270
the reduce step gets parallelized to construct one output file 

65
04:51:02,360 --> 04:51:06,470
you have probably noticed that the data gets reduced to a smaller set

66
04:51:06,470 --> 04:51:07,060
at each step 

67
04:51:09,370 --> 04:51:11,890
although the example we have given is for

68
04:51:11,890 --> 04:51:16,020
batch processing similar techniques apply to stream processing 

69
04:51:17,060 --> 04:51:18,740
let discuss this for

70
04:51:18,740 --> 04:51:22,690
our simplified advanced stream data from an online game example 

71
04:51:23,910 --> 04:51:28,070
in this case your event gets ingested

72
04:51:28,070 --> 04:51:32,960
through a real time big data ingestion engine like kafka or flume 

73
04:51:34,380 --> 04:51:38,400
then they get passed into a streaming data platform for

74
04:51:38,400 --> 04:51:42,670
processing like samza storm or spark streaming 

75
04:51:43,750 --> 04:51:49,910
this is a valid choice for processing data one event at a time or chunking

76
04:51:49,910 --> 04:51:55,390
the data into windows or microbatches of time or other features 

77
04:51:56,940 --> 04:52:02,470
any pipeline processing of data can be applied to the streaming data here

78
04:52:02,470 --> 04:52:05,410
as we wrote in a batch - processing big data engine 

79
04:52:07,350 --> 04:52:13,034
the process stream data can then be served through a real - time view or

80
04:52:13,034 --> 04:52:15,279
a batch - processing view 

81
04:52:15,279 --> 04:52:20,380
real - time view is often subject to change as potentially delayed new data comes in 

82
04:52:21,590 --> 04:52:26,490
the storage of the data can be accomplished using h - base cassandra 

83
04:52:26,490 --> 04:52:30,660
hdfs or many other persistent storage systems 

84
04:52:32,120 --> 04:52:37,110
to summarize big data pipelines get created to process data

85
04:52:37,110 --> 04:52:41,990
through an aggregated set of steps that can be represented with the split - 

86
04:52:41,990 --> 04:52:46,370
do - merge pattern with data parallel scalability 

87
04:52:46,370 --> 04:52:49,040
this pattern can be applied to many batch and

88
04:52:49,040 --> 04:52:52,100
streaming data processing applications 

89
04:52:52,100 --> 04:52:56,958
next we will go through some processing steps in a big data pipeline in

90
04:52:56,958 --> 04:53:01,589
more detail first conceptually then practically in spark 

1
09:39:00,140 --> 09:39:04,070
now that we went through an overview of the spark ecosystem and

2
09:39:04,070 --> 09:39:08,640
the components of the spark stack it is time for us to start learning more about

3
09:39:08,640 --> 09:39:12,960
its architecture and run our first spark program in the cloud vm 

4
09:39:14,580 --> 09:39:18,710
after this video you will be able to describe how spark

5
09:39:18,710 --> 09:39:23,690
does in - memory processing using the resilient distributed dataset abstraction 

6
09:39:24,890 --> 09:39:28,740
explain the inner workings of the spark architecture and

7
09:39:28,740 --> 09:39:33,090
summarize how spark manages and executes code on clusters 

8
09:39:34,510 --> 09:39:38,531
i mentioned a few times that spark is efficient because it

9
09:39:38,531 --> 09:39:42,739
uses an abstraction called rdds for in memory processing 

10
09:39:44,704 --> 09:39:47,970
what this means might not be clear to some of you yet 

11
09:39:49,170 --> 09:39:51,660
let remember the alternative 

12
09:39:51,660 --> 09:39:56,153
in hadoop mapreduce each step also pipeline 

13
09:39:56,153 --> 09:40:01,272
reads from disk to memory performs the computations and

14
09:40:01,272 --> 09:40:05,770
writes back its output from the memory to the disk 

15
09:40:07,290 --> 09:40:12,520
however writing data to disk is a costly operation and

16
09:40:12,520 --> 09:40:16,220
this cost becomes even more with large volumes of data 

17
09:40:17,650 --> 09:40:19,010
here is an interesting fact 

18
09:40:20,320 --> 09:40:24,664
memory operations can be up to 100 000 times

19
09:40:24,664 --> 09:40:27,250
faster than disk operations in some cases 

20
09:40:28,510 --> 09:40:32,430
spark instead takes advantage of this and allows for

21
09:40:32,430 --> 09:40:36,810
immediate results of transformations in different stages of the pipeline and

22
09:40:36,810 --> 09:40:39,650
memory like map and reduce here 

23
09:40:40,980 --> 09:40:44,540
here we see that the outputs of map operations

24
09:40:44,540 --> 09:40:48,890
are shared with reduce operations without being written to the disk 

25
09:40:49,980 --> 09:40:54,900
the containers where the data gets stored in memory

26
09:40:54,900 --> 09:40:59,370
are called resilient distributed datasets or rdds for short 

27
09:41:01,210 --> 09:41:05,730
rdds are how spark distributes data and computations across

28
09:41:05,730 --> 09:41:11,180
the nodes of a commodity cluster preferably with large memory 

29
09:41:11,180 --> 09:41:13,380
thanks to this abstraction 

30
09:41:13,380 --> 09:41:17,790
spark has proven to be 100 times faster for some applications 

31
09:41:19,360 --> 09:41:22,960
let define all the words in this interesting name

32
09:41:22,960 --> 09:41:25,510
beginning with the last one datasets 

33
09:41:27,196 --> 09:41:35,790
datasets that rdd distributes comes from a batch data storage like hdfs 

34
09:41:35,790 --> 09:41:41,120
no sql databases text files or streaming data ingestion systems like cafco 

35
09:41:42,330 --> 09:41:46,760
it can even conveniently read and distribute the data from your local disc

36
09:41:46,760 --> 09:41:51,600
like text files into spark or even a hierarchy of folders 

37
09:41:53,750 --> 09:42:00,200
when spark reads data from these sources it generates rdds for them 

38
09:42:00,200 --> 09:42:06,930
the spark operations can transform rdds into other rdds like any other data 

39
09:42:06,930 --> 09:42:12,650
here it important to mention that rdds are immutable 

40
09:42:12,650 --> 09:42:16,610
this means that you cannot change them partially 

41
09:42:16,610 --> 09:42:21,970
however you can create new rdds by a series of one or many transformations 

42
09:42:23,570 --> 09:42:27,200
next let look at what distributed means in the rdd context 

43
09:42:28,480 --> 09:42:33,680
as i mentioned before rdds distribute partitioned data collections and

44
09:42:33,680 --> 09:42:39,740
computations on clusters even across a number of machines 

45
09:42:40,770 --> 09:42:43,860
for example running on the amazon cloud 

46
09:42:45,160 --> 09:42:49,880
the complexity of this operation is hidden by this very simple interface 

47
09:42:51,140 --> 09:42:56,230
computations are a diverse set of transformations of rdds like map 

48
09:42:56,230 --> 09:42:58,230
filter and join 

49
09:42:58,230 --> 09:43:04,940
and also actions on the rdds like counting and saving them persistently on disk 

50
09:43:04,940 --> 09:43:09,560
the partitioning of data can be changed dynamically to optimize spark 

51
09:43:09,560 --> 09:43:10,170
performance 

52
09:43:11,620 --> 09:43:16,420
the last element is resilient and it very important because in a large

53
09:43:16,420 --> 09:43:20,640
scale computing environment it is pretty common to have node failures 

54
09:43:21,850 --> 09:43:24,710
it very important to be able to recover from these

55
09:43:24,710 --> 09:43:27,480
situations without losing any work already done 

56
09:43:28,570 --> 09:43:33,360
for full tolerance in such situations spark tracks the history of each

57
09:43:33,360 --> 09:43:38,220
partition keeping a lineage over rdds over time 

58
09:43:38,220 --> 09:43:43,170
so every point in your calculations spark knows which are the partitions

59
09:43:43,170 --> 09:43:46,560
needed to recreate the partition in case it gets lost 

60
09:43:47,710 --> 09:43:49,480
and if that happens 

61
09:43:49,480 --> 09:43:54,730
then spark automatically figures out where it can start the recompute from and

62
09:43:54,730 --> 09:43:58,890
optimizes the amount of processing needed to recover from the failure 

63
09:44:00,410 --> 09:44:05,087
before we create our first spark program using rdds in pi spark in the cloud

64
09:44:05,087 --> 09:44:08,343
layer vm let review spark architecture 

65
09:44:11,239 --> 09:44:15,772
from a bird eye view spark has two main components 

66
09:44:15,772 --> 09:44:18,610
a driver program and worker nodes 

67
09:44:20,900 --> 09:44:24,300
the driver program is where your application starts 

68
09:44:25,900 --> 09:44:29,760
it distributes rdds on your computational cluster and

69
09:44:29,760 --> 09:44:34,435
makes sure the transformations and actions on these rdds are performed 

70
09:44:36,530 --> 09:44:41,250
driver programs create a connection to a spark cluster or

71
09:44:41,250 --> 09:44:44,950
your local spark through a spark context object 

72
09:44:46,010 --> 09:44:49,770
the default spark context in the spark shell is

73
09:44:49,770 --> 09:44:53,680
an object called sc for spark context 

74
09:44:54,980 --> 09:45:00,260
for example in the upcoming reading for creating word counts in spark 

75
09:45:00,260 --> 09:45:04,252
we will use sc as the context to

76
09:45:04,252 --> 09:45:09,210
generate rdds for a text file using the line of code shown here 

77
09:45:10,650 --> 09:45:16,340
the driver program manages a potentially large number of nodes called worker nodes 

78
09:45:17,780 --> 09:45:22,740
on a local computer we can assume that there only one worker node and

79
09:45:22,740 --> 09:45:24,590
it is where the spark operations execute 

80
09:45:26,100 --> 09:45:31,230
a worker node in spark keeps a running java virtual machine 

81
09:45:31,230 --> 09:45:35,000
called jvm commonly called the executor 

82
09:45:37,080 --> 09:45:39,913
depending on the illustration 

83
09:45:39,913 --> 09:45:44,566
executor can execute task related to mapping stages or

84
09:45:44,566 --> 09:45:49,130
reducing stages or other spark specific pipelines 

85
09:45:49,130 --> 09:45:55,642
this java virtual machine is the core that all the computation is executed 

86
09:45:55,642 --> 09:46:02,780
and this is the interface also to the rest of the big data storage systems and tools 

87
09:46:04,320 --> 09:46:09,560
for example if we ever had the hadoop file system htfs as the storage system 

88
09:46:09,560 --> 09:46:13,650
then on each worker node some of the data will be stored locally 

89
09:46:13,650 --> 09:46:18,445
as you know the most important point of this computing framework is to bring

90
09:46:18,445 --> 09:46:20,084
the computation to data 

91
09:46:20,084 --> 09:46:23,770
so spark will send some computational jobs to be executed

92
09:46:23,770 --> 09:46:29,190
on the data that are already available on the machine thanks to hdfs 

93
09:46:29,190 --> 09:46:31,930
data will be read from hdfs and

94
09:46:31,930 --> 09:46:37,580
get processed in memory the results will be stored as one rdd 

95
09:46:37,580 --> 09:46:44,012
the actual computation is running straight in the executor 

96
09:46:44,012 --> 09:46:49,214
that is the jvm that runs your scala or java codes 

97
09:46:49,214 --> 09:46:53,970
instead if you are using pyspark then there will be several python

98
09:46:53,970 --> 09:46:56,723
processes generally one for task but

99
09:46:56,723 --> 09:47:00,490
you can configure it depending on your application 

100
09:47:02,999 --> 09:47:08,770
in a real big data scenario we have many worker nodes running tasks internally 

101
09:47:09,990 --> 09:47:14,220
it is important to have a system that can automatically manage provisioning and

102
09:47:14,220 --> 09:47:15,420
restarting of these nodes 

103
09:47:16,540 --> 09:47:19,550
the cluster manager in spark has this capability 

104
09:47:20,970 --> 09:47:26,660
spark currently supports mainly three interfaces for cluster management 

105
09:47:26,660 --> 09:47:33,620
namely spark standalone cluster manager the apache mesos and hadoop yarn 

106
09:47:35,190 --> 09:47:39,441
standalone means that there a special spark process that takes care of

107
09:47:39,441 --> 09:47:41,567
restarting nodes that are failing or

108
09:47:41,567 --> 09:47:44,668
starting nodes at the beginning of the computation 

109
09:47:44,668 --> 09:47:49,276
yarn and mesos are two external research measures that can be used also for

110
09:47:49,276 --> 09:47:50,400
these purposes 

111
09:47:53,080 --> 09:47:56,400
choosing a cluster manager to fit your application and

112
09:47:56,400 --> 09:47:59,280
infrastructure can be quite confusing 

113
09:47:59,280 --> 09:48:03,020
here we give you a good article as a starting point on

114
09:48:03,020 --> 09:48:06,030
how to pick the right cluster manager for your organization 

115
09:48:07,470 --> 09:48:12,130
to summarize the spark architecture includes a driver program 

116
09:48:13,270 --> 09:48:17,918
the driver program communicates with the cluster manager for

117
09:48:17,918 --> 09:48:21,404
monitoring and provisioning of resources and

118
09:48:21,404 --> 09:48:26,783
communicates directly with worker nodes to submit and execute tasks 

119
09:48:26,783 --> 09:48:32,410
rdds get created and passed within transformations running in the executable 

120
09:48:33,900 --> 09:48:38,691
finally let see how this setup works on the cloudera vm 

121
09:48:38,691 --> 09:48:42,797
in the cloudera vm we are using spark in standalone mode and

122
09:48:42,797 --> 09:48:45,820
everything is running locally 

123
09:48:45,820 --> 09:48:52,615
so it a single machine and on the same machine we have our driver program 

124
09:48:52,615 --> 09:48:57,089
the executor jvm and our single pyspark process 

125
09:48:57,089 --> 09:49:01,771
with that we are ready to start with our first reading to install spark and

126
09:49:01,771 --> 09:49:05,946
then running our word count program using the spark environment 

1
19:28:05,430 --> 19:28:09,170
after a brief overview of some of the processing systems

2
19:28:09,170 --> 19:28:13,480
in the big data landscape it is time for us to dive deeper into spark 

3
19:28:14,820 --> 19:28:19,170
spark was initiated at uc berkeley in 2009 and

4
19:28:19,170 --> 19:28:23,840
was transferred to apache software foundation in 2013 

5
19:28:23,840 --> 19:28:28,240
since then spark has become a top level project with many users and

6
19:28:28,240 --> 19:28:29,700
contributors worldwide 

7
19:28:31,550 --> 19:28:36,530
after this video you will be able to list the main motivations for

8
19:28:36,530 --> 19:28:40,980
the development of spark draw the spark stack as a layer diagram 

9
19:28:42,320 --> 19:28:46,130
and explain the functionality of the components in the spark stack 

10
19:28:48,520 --> 19:28:53,810
as we have discussed in our earlier discussions while hadoop is great for

11
19:28:53,810 --> 19:28:57,460
batch processing using the mapreduce programming module 

12
19:28:57,460 --> 19:28:59,710
it has shortcomings in a number of ways 

13
19:29:01,110 --> 19:29:06,000
first of all since it is limited to map and reduce based transformations 

14
19:29:06,000 --> 19:29:11,020
one has to restrict their big data pipeline to map and reduce steps 

15
19:29:12,520 --> 19:29:15,680
but the number of applications can be implemented using map and

16
19:29:15,680 --> 19:29:18,970
reduce it not always possible and

17
19:29:18,970 --> 19:29:22,810
it is often not the most efficient way to express a big data pipeline 

18
19:29:24,460 --> 19:29:29,920
for example you might want to do a join operation between different data sets or

19
19:29:29,920 --> 19:29:32,430
you might want to filter or sample your data 

20
19:29:33,440 --> 19:29:36,460
or you might have a more complicated data pipeline with

21
19:29:36,460 --> 19:29:40,680
several steps including joins and group byes 

22
19:29:40,680 --> 19:29:45,280
it might have a map and reduce face but maybe another map face after that 

23
19:29:46,660 --> 19:29:52,140
these types of operations are hard or impossible to express using mapreduce and

24
19:29:52,140 --> 19:29:55,760
cannot be accommodated by the mapreduce framework in hadoop 

25
19:29:57,080 --> 19:30:00,830
another important bottleneck in hadoop mapreduce that is critical for

26
19:30:00,830 --> 19:30:05,880
performance is that mapreduce relies heavily on reading data from disc 

27
19:30:07,680 --> 19:30:12,470
this is especially a problem for iterative algorithms that require taking several

28
19:30:12,470 --> 19:30:16,800
passes through the data using a number of transformations 

29
19:30:16,800 --> 19:30:20,940
since each transformation will need to read its inputs from the disk 

30
19:30:20,940 --> 19:30:24,670
this will end up in a performance bottleneck due to io 

31
19:30:26,730 --> 19:30:29,610
most machine learning pipelines are in this category 

32
19:30:29,610 --> 19:30:32,850
making hadoop mapreduce not ideal for machine learning 

33
19:30:33,900 --> 19:30:38,070
and as i mentioned in the system overview the only programming language

34
19:30:38,070 --> 19:30:42,190
that mapreduce provides a native interface for is java 

35
19:30:42,190 --> 19:30:47,500
although it possible to run python code to implementation for it is more complex

36
19:30:47,500 --> 19:30:52,020
and not very efficient especially when you are running not with text data but

37
19:30:52,020 --> 19:30:53,320
with floating point numbers 

38
19:30:54,650 --> 19:30:58,417
the programming language issue also affects how interactive

39
19:30:58,417 --> 19:30:59,830
the environment is 

40
19:30:59,830 --> 19:31:02,490
most data scientist prefer to use scripting

41
19:31:02,490 --> 19:31:06,120
languages due to their interactive shell capabilities 

42
19:31:06,120 --> 19:31:10,670
not having such an interface in hadoop really makes it difficult to use and

43
19:31:10,670 --> 19:31:11,940
adapt my many in the field 

44
19:31:13,530 --> 19:31:17,410
in addition in the big data era having support for

45
19:31:17,410 --> 19:31:20,280
streaming data processing is a key for

46
19:31:20,280 --> 19:31:24,920
being able to run similar analysis on both real time and historical data 

47
19:31:26,380 --> 19:31:30,160
spark came out of the need to extend the mapreduce framework

48
19:31:30,160 --> 19:31:32,690
to overcome this shortcomings and

49
19:31:32,690 --> 19:31:37,080
provide an expressive cluster computing environment that can provide interactive

50
19:31:37,080 --> 19:31:41,820
querying efficient iterative analytics and streaming data processing 

51
19:31:43,140 --> 19:31:46,970
so how does apache spark provide solutions for these problems 

52
19:31:48,810 --> 19:31:52,820
spark provides a very rich and expressive programming module that

53
19:31:52,820 --> 19:31:57,690
gives you more than 20 highly efficient distributed operations or transformations 

54
19:31:58,695 --> 19:32:02,810
pipe - lining any of these steps in spark simply takes a few lines of code 

55
19:32:04,470 --> 19:32:09,040
another important feature of spark is the ability to run these computations

56
19:32:09,040 --> 19:32:09,550
in memory 

57
19:32:10,610 --> 19:32:14,350
it ability to cache and process data in memory 

58
19:32:14,350 --> 19:32:18,325
makes it significantly faster for iterative applications 

59
19:32:18,325 --> 19:32:23,665
this is proven to provide a factor of ten or even 100 speed - up

60
19:32:23,665 --> 19:32:27,805
in the performance of some algorithms especially using large data sets 

61
19:32:29,635 --> 19:32:34,625
additionally spark provides support for batch and streaming workloads at once 

62
19:32:35,790 --> 19:32:40,675
last but not least spark provides simple apis for python 

63
19:32:40,675 --> 19:32:45,570
scala java and sql programming through an interactive shell to

64
19:32:45,570 --> 19:32:50,590
accomplish analytical tasks through both external and its built - in libraries 

65
19:32:52,750 --> 19:32:56,500
the spark layer diagram also called stack 

66
19:32:56,500 --> 19:33:01,250
consists of components that build on top of the spark computational engine 

67
19:33:02,510 --> 19:33:08,860
this engine distributes and monitors tasks across the nodes of a commodity cluster 

68
19:33:09,960 --> 19:33:14,400
the components built on top of this engine are designed to interact and

69
19:33:14,400 --> 19:33:16,320
communicate through this common engine 

70
19:33:17,430 --> 19:33:21,520
any improvements to the underlying engine becomes

71
19:33:21,520 --> 19:33:25,980
an improvement in the other components thanks to such close interaction 

72
19:33:27,390 --> 19:33:32,790
this also enables building applications that span across these different

73
19:33:32,790 --> 19:33:37,950
components like querying data using spark sql and applying machine learning

74
19:33:37,950 --> 19:33:43,230
algorithms the query results using sparks machine learning library and mllib 

75
19:33:45,120 --> 19:33:49,240
the spark core is where the core capability is

76
19:33:49,240 --> 19:33:52,520
of the spark framework are implemented 

77
19:33:52,520 --> 19:33:54,010
this includes support for

78
19:33:54,010 --> 19:33:58,460
distributed scheduling memory management and full tolerance 

79
19:33:59,570 --> 19:34:03,460
interaction with different schedulers like yarn and mesos and

80
19:34:03,460 --> 19:34:08,030
various nosql storage systems like hbase also happen through spark core 

81
19:34:10,020 --> 19:34:14,068
a very important part of spark core is the apis for

82
19:34:14,068 --> 19:34:19,422
defining resilient distributed data sets or rdds for short 

83
19:34:19,422 --> 19:34:22,858
rdds are the main programming abstraction in spark 

84
19:34:22,858 --> 19:34:27,910
which carry data across many computing nodes in parallel and transform it 

85
19:34:27,910 --> 19:34:33,614
spark sql is the component of spark that provides querying structured and

86
19:34:33,614 --> 19:34:37,763
unstructured data through a common query language 

87
19:34:37,763 --> 19:34:42,830
it can connect to many data sources and provide apis to convert

88
19:34:42,830 --> 19:34:47,714
query results to rdds in python scala and java programs 

89
19:34:47,714 --> 19:34:53,780
spark streaming is where data manipulations take place in spark 

90
19:34:55,060 --> 19:35:01,120
although not a native real - time interface to datastreams spark streaming enables

91
19:35:01,120 --> 19:35:05,140
creating small aggregates of data coming from streaming data ingestion systems 

92
19:35:07,490 --> 19:35:12,000
these aggregate datasets are called micro - batches and

93
19:35:12,000 --> 19:35:15,680
they can be converted into rdbs in spark streaming for processing 

94
19:35:17,740 --> 19:35:20,740
mllib is sparks native library for

95
19:35:20,740 --> 19:35:25,240
machine learning algorithms as well as model evaluation 

96
19:35:25,240 --> 19:35:30,050
all of the functionality is potentially ported to any programming language sparks

97
19:35:30,050 --> 19:35:34,030
supports and is designed to scale out using spark 

98
19:35:35,190 --> 19:35:39,840
graphx is the graph analytics library of spark and

99
19:35:39,840 --> 19:35:45,270
enables the vertex edge data model of graphs to be converted into rdds as

100
19:35:45,270 --> 19:35:49,328
well as providing scalable implementations of graph processing algorithms 

101
19:35:51,390 --> 19:35:56,655
to summarize through these layers spark provides diverse 

102
19:35:56,655 --> 19:36:01,731
scalable interactive management and analyses of big data 

103
19:36:01,731 --> 19:36:07,166
the interactive shell enables data scientists to conduct exploratory

104
19:36:07,166 --> 19:36:12,416
analysis and create big data pipelines while also enabling the big

105
19:36:12,416 --> 19:36:17,941
data system integration engineers to scale these analytical pipelines

106
19:36:17,941 --> 19:36:22,850
across commodity computing clusters and cloud environments 

1
15:04:27,670 --> 15:04:31,470
there are many big data processing systems but

2
15:04:31,470 --> 15:04:35,260
how do we make sense of them in order to take full advantage of these systems 

3
15:04:36,320 --> 15:04:39,820
in this video we will review some of them and

4
15:04:39,820 --> 15:04:43,600
a way to categorize big data processing systems as we go through our review 

5
15:04:45,290 --> 15:04:50,330
after this video you will be able to recall the hadoop ecosystem 

6
15:04:51,490 --> 15:04:56,050
draw a layer diagram with three layers for data storage data processing and

7
15:04:56,050 --> 15:04:57,930
workflow management 

8
15:04:57,930 --> 15:05:02,800
summarize an evaluation criteria for big data processing systems and

9
15:05:02,800 --> 15:05:07,363
explain the properties of hadoop spark flink 

10
15:05:07,363 --> 15:05:11,720
beam and storm as major big data processing systems 

11
15:05:12,750 --> 15:05:15,900
in our introduction to big data course 

12
15:05:15,900 --> 15:05:19,500
we talked about a version of the layer diagram for the tools

13
15:05:19,500 --> 15:05:25,030
in the hadoop ecosystem organized vertically based on the interface 

14
15:05:25,030 --> 15:05:29,225
lower level interface is the storage and scheduling on the bottom and

15
15:05:29,225 --> 15:05:32,560
higher level languages and interactivity at the top 

16
15:05:33,830 --> 15:05:38,040
most of the tools in the hadoop ecosystem are initially built to complement

17
15:05:38,040 --> 15:05:43,410
the capabilities of hadoop for distributed filesystem management using hdfs 

18
15:05:44,440 --> 15:05:49,240
data processing using the mapreduce engine and resource scheduling and

19
15:05:49,240 --> 15:05:51,210
negotiation using the yarn engine 

20
15:05:52,310 --> 15:05:56,170
over time a number of new projects were built

21
15:05:56,170 --> 15:06:01,150
either to add to these complimentary tools or to handle

22
15:06:01,150 --> 15:06:05,750
additional types of big data management and processing not available in hadoop 

23
15:06:07,520 --> 15:06:12,250
arguably the most important change to hadoop over time

24
15:06:12,250 --> 15:06:17,210
was the separation of yarn from the mapreduce programming model

25
15:06:17,210 --> 15:06:20,510
to solely handle resource management concerns 

26
15:06:21,780 --> 15:06:26,660
this allowed for hadoop to be extensible to different programming models 

27
15:06:26,660 --> 15:06:30,950
and enabled the development of a number of processing engines for

28
15:06:30,950 --> 15:06:32,920
batch and stream processing 

29
15:06:34,250 --> 15:06:38,580
another way to look at the vast number of tools that have been added to the hadoop

30
15:06:38,580 --> 15:06:41,170
ecosystem is from the point of view of

31
15:06:41,170 --> 15:06:43,820
their functionality in the big data processing pipeline 

32
15:06:44,870 --> 15:06:50,500
simply put these associate to three distinct layers for data management and

33
15:06:50,500 --> 15:06:57,380
storage data processing and resource coordination and workflow management 

34
15:06:58,550 --> 15:07:03,760
in our second course we talked about the bottom layer

35
15:07:03,760 --> 15:07:08,770
in this diagram in detail namely the data management and storage 

36
15:07:10,600 --> 15:07:16,440
while this layer includs hadoop hdfs there are a number of other systems that

37
15:07:16,440 --> 15:07:22,250
rely on hdfs as a file system or implement their own no sql storage options 

38
15:07:24,290 --> 15:07:28,940
as big data can have a variety of structure semi structured and

39
15:07:28,940 --> 15:07:33,858
unstructured formats and gets analyzed through a variety of tools 

40
15:07:33,858 --> 15:07:38,890
many tools were introduced to fit this variety of needs 

41
15:07:38,890 --> 15:07:41,180
we call these big data management systems 

42
15:07:44,040 --> 15:07:48,630
we reviewed redis and aerospike as key value stores 

43
15:07:48,630 --> 15:07:51,670
where each data item is identified with a unique key 

44
15:07:53,150 --> 15:07:56,827
we got some practical experience with lucene and

45
15:07:56,827 --> 15:08:01,048
gephi as vector and graph data stores respectively 

46
15:08:02,800 --> 15:08:06,505
we also talked about vertica as a column store database 

47
15:08:06,505 --> 15:08:10,300
where information is stored in columns rather than rows 

48
15:08:11,880 --> 15:08:15,070
cassandra and hbase are also in this category 

49
15:08:16,270 --> 15:08:22,402
finally we introduce solr and asterisk db for managing unstructured and

50
15:08:22,402 --> 15:08:28,260
semi - structured text and mongodb as a document store 

51
15:08:28,260 --> 15:08:33,547
the processing layer is where these different varieties of data gets

52
15:08:33,547 --> 15:08:40,030
retrieved integrated and analyzed which is the primary focus of this class 

53
15:08:42,232 --> 15:08:45,061
in the integration and processing layer

54
15:08:45,061 --> 15:08:49,966
we roughly refer to the tools that are built on top of the hdfs and yarn 

55
15:08:49,966 --> 15:08:54,480
although some of them work with other storage and file systems 

56
15:08:56,180 --> 15:09:03,140
yarn is a significant enabler of many of these tools making a number of batch and

57
15:09:03,140 --> 15:09:08,111
stream processing engines like storm spark flink and being possible 

58
15:09:09,700 --> 15:09:13,340
we will revisit these processing engines and explain why we have so

59
15:09:13,340 --> 15:09:14,890
many later in this lecture 

60
15:09:15,970 --> 15:09:20,940
this layer also includes tools like hive or spark sql for

61
15:09:20,940 --> 15:09:25,120
bringing a query interface on top of the storage layer 

62
15:09:25,120 --> 15:09:30,280
pig for scripting simple big data pipelines using the mapreduce framework 

63
15:09:30,280 --> 15:09:35,009
and a number of specialized analytical libraries for machine learning and

64
15:09:35,009 --> 15:09:40,262
graph analytics like giraph as graphx of spark are examples of such libraries for

65
15:09:40,262 --> 15:09:41,551
graph processing 

66
15:09:41,551 --> 15:09:46,187
and mahout on top of the hadoop stack and mllib of spark are two options for

67
15:09:46,187 --> 15:09:47,480
machine learning 

68
15:09:48,640 --> 15:09:52,760
although we have a basic overview of graph processing and machine learning for

69
15:09:52,760 --> 15:09:57,700
big data analytics later in this course we wo not go into the details here 

70
15:09:57,700 --> 15:10:02,530
instead we will have a dedicated course on each of them

71
15:10:02,530 --> 15:10:04,670
later in this specialization 

72
15:10:04,670 --> 15:10:12,720
the third and top layer in our diagram is the coordination and management layer 

73
15:10:12,720 --> 15:10:16,890
this is where integration scheduling coordination and

74
15:10:16,890 --> 15:10:22,410
monitoring of applications across many tools in the bottom two layers take place 

75
15:10:23,440 --> 15:10:27,870
this layer is also where the results of the big data analysis gets communicated to

76
15:10:27,870 --> 15:10:33,630
other programs websites visualization tools and business intelligence tools 

77
15:10:33,630 --> 15:10:38,830
workflow management systems help to develop automated solutions that can

78
15:10:38,830 --> 15:10:43,960
manage and coordinate the process of combining data management and analytical

79
15:10:43,960 --> 15:10:49,264
tests in a big data pipeline as a configurable structured set of steps 

80
15:10:51,020 --> 15:10:55,544
the workflow driven thinking also matches this basic process of data

81
15:10:55,544 --> 15:10:57,970
science that we overviewed before 

82
15:10:59,220 --> 15:11:03,420
oozie is an example of a workflow scheduler that can interact with

83
15:11:03,420 --> 15:11:06,295
many of the tools in the integration and processing layer 

84
15:11:07,690 --> 15:11:12,390
zookeeper is the resource coordination and monitoring tool and

85
15:11:12,390 --> 15:11:16,990
manages and coordinates all these tools and middleware named after animals 

86
15:11:18,460 --> 15:11:22,429
although virtual management is my personal research area and

87
15:11:22,429 --> 15:11:24,793
i talk more about it in other venues 

88
15:11:24,793 --> 15:11:29,999
in this specialization we focus mainly on big data integration and processing 

89
15:11:29,999 --> 15:11:35,450
and we will not have a specific lecture on this layer in this course 

90
15:11:35,450 --> 15:11:39,430
we give you a reading on big data workflows after this video as

91
15:11:39,430 --> 15:11:42,750
further information and a starting point for the subject 

1
06:16:10,380 --> 06:16:14,830
in data integration and processing pipelines 

2
06:16:14,830 --> 06:16:19,910
data goes through a number of operations which can apply

3
06:16:19,910 --> 06:16:25,960
a specific function to it can work the data from one format to another 

4
06:16:25,960 --> 06:16:32,097
join data with other data sets or filter some values out of a data set 

5
06:16:33,240 --> 06:16:39,350
we generally refer to these as transformations some of which can also

6
06:16:39,350 --> 06:16:44,950
be specially named aggregations as you have seen in amarnath earlier lectures 

7
06:16:44,950 --> 06:16:51,010
in this video we will reveal some common transformation operations that we see

8
06:16:51,010 --> 06:16:56,490
in these pipelines some of which we refer to as data parallel patterns 

9
06:16:58,575 --> 06:17:03,265
after this video you will be able to list common data

10
06:17:03,265 --> 06:17:08,355
transformations within big data pipelines and design

11
06:17:08,355 --> 06:17:13,235
a conceptual data processing pipeline using the basic data transformations 

12
06:17:15,551 --> 06:17:20,770
simply speaking transformations are higher order functions or

13
06:17:20,770 --> 06:17:25,960
tools to convert your data from one form to another just like

14
06:17:25,960 --> 06:17:31,530
we would use tools at the wood shop to transform logs into furniture 

15
06:17:32,900 --> 06:17:35,630
when we look at big data pipelines used today 

16
06:17:36,650 --> 06:17:40,840
map is probably the most common transformation we find 

17
06:17:42,340 --> 06:17:47,230
the map operation is one of the basic building blocks of the big data pipeline 

18
06:17:48,760 --> 06:17:54,200
when you want to apply a process to each member of a collection 

19
06:17:54,200 --> 06:17:57,370
such as adding 10 bonus to each

20
06:17:57,370 --> 06:18:01,770
person salary on a given month a map operation comes in very handy 

21
06:18:03,520 --> 06:18:08,490
it takes your process and understand that it is required to perform

22
06:18:08,490 --> 06:18:12,370
the same operation or process to each member of the set 

23
06:18:14,610 --> 06:18:18,970
the figure on the left here shows the application

24
06:18:18,970 --> 06:18:23,396
of a map function to data depicted in grey color 

25
06:18:23,396 --> 06:18:30,800
here colors red blue and yellow are keys to identify each data set 

26
06:18:32,480 --> 06:18:39,130
as you see each data set is executed separately even for the same colored key 

27
06:18:43,010 --> 06:18:47,944
the reduce operation helps you then to collectively apply the same

28
06:18:47,944 --> 06:18:50,860
process to objects of similar nature 

29
06:18:52,680 --> 06:18:58,030
for example when you want to add your monthly spending in different categories 

30
06:18:58,030 --> 06:19:04,350
like grocery fuel and dining out the reduce operation is very useful 

31
06:19:06,500 --> 06:19:09,900
in our figure here on the top left 

32
06:19:09,900 --> 06:19:14,472
we see that data sets in grey with the same color

33
06:19:14,472 --> 06:19:19,760
are keys grouped together using a reduced function 

34
06:19:20,940 --> 06:19:24,320
reds together blues together and yellows together 

35
06:19:26,780 --> 06:19:31,579
it would be a good idea to check out the spark word count hands - on to see

36
06:19:31,579 --> 06:19:36,400
how map and reduce can be used effectively for getting things done 

37
06:19:37,560 --> 06:19:42,550
map and reduce are types of transformations that work on a single

38
06:19:42,550 --> 06:19:47,460
list of key and data pairings just like we see on the left of our figure 

39
06:19:50,460 --> 06:19:55,520
now let consider a scenario where we have two data sets identified

40
06:19:55,520 --> 06:20:01,090
by the same keys just like the two sets and colors in our diagram 

41
06:20:03,140 --> 06:20:08,120
many operations have such needs where we have to look at all the pairings of

42
06:20:08,120 --> 06:20:12,740
all key value pairs just like crossing two matrices 

43
06:20:14,900 --> 06:20:19,800
for a practical example imagine you have two teams 

44
06:20:19,800 --> 06:20:24,870
a sales team with two people and an operations team with four people 

45
06:20:26,050 --> 06:20:30,940
in an event you would want each person to meet every other person 

46
06:20:32,030 --> 06:20:35,210
in this case a cross product or

47
06:20:35,210 --> 06:20:40,200
a cartesian product becomes a good choice for organizing the event and

48
06:20:40,200 --> 06:20:44,000
sharing each pairs meeting location and travel time to them 

49
06:20:45,770 --> 06:20:51,470
in a cross or cartesian product operation each data partition gets

50
06:20:51,470 --> 06:20:57,450
paired with all other data partitions regardless of its key 

51
06:20:57,450 --> 06:21:00,370
this sometimes gets referred to as all pairs 

52
06:21:02,990 --> 06:21:08,160
now add to the cross product by just grouping together the data

53
06:21:08,160 --> 06:21:13,490
partitions with the same key just like the red data 

54
06:21:15,090 --> 06:21:17,128
and the yellow data partitions here 

55
06:21:20,005 --> 06:21:24,930
this is a typical match or join operation 

56
06:21:24,930 --> 06:21:30,190
as we see in the figure here match is very similar to the cross product 

57
06:21:30,190 --> 06:21:33,760
except that it is more selective in forming pairs 

58
06:21:34,810 --> 06:21:37,420
every pair must have something in common 

59
06:21:38,510 --> 06:21:42,920
this something in common is usually referred to as a key 

60
06:21:44,490 --> 06:21:48,330
for example each person in your operations team and

61
06:21:48,330 --> 06:21:51,510
sales team is assigned to a different product 

62
06:21:51,510 --> 06:21:56,500
you only want those people to meet who are working on the same product 

63
06:21:56,500 --> 06:22:00,490
in this case your key is product 

64
06:22:00,490 --> 06:22:03,290
and you can perform and match operation and

65
06:22:03,290 --> 06:22:07,670
send emails to those people who share a common product 

66
06:22:09,530 --> 06:22:14,756
the number of emails is likely to be less than when you performed a cartesian or

67
06:22:14,756 --> 06:22:18,980
a cross product therefore reducing the cost of the operation 

68
06:22:20,270 --> 06:22:27,170
in a match operation only the keys with data in both sets get joined 

69
06:22:28,240 --> 06:22:31,920
and become a part of the final output of the transformation 

70
06:22:34,600 --> 06:22:39,600
now let consider listing the data sets with all the keys 

71
06:22:39,600 --> 06:22:41,790
even if they do not exist in both sets 

72
06:22:43,850 --> 06:22:48,550
consider a scenario where you want to do brainstorming sessions

73
06:22:48,550 --> 06:22:51,690
of people from operations and sales and

74
06:22:51,690 --> 06:22:55,560
get people who work on the same products in the same rooms 

75
06:22:58,390 --> 06:23:00,750
a co - group operation will do this for you 

76
06:23:02,290 --> 06:23:06,160
you give it a product name as they key to work with and

77
06:23:06,160 --> 06:23:09,530
the two tables the sales team and operations team 

78
06:23:10,800 --> 06:23:15,740
the co - group will create groups which contain team members

79
06:23:15,740 --> 06:23:21,690
working on common products even if a product does not exist in one of the sets 

80
06:23:24,520 --> 06:23:28,520
the last operation we will see is the filter operation 

81
06:23:28,520 --> 06:23:31,760
filter works much like a test

82
06:23:31,760 --> 06:23:36,340
where only elements that pass a test are shown in the output 

83
06:23:38,040 --> 06:23:42,770
consider as a set that contains teams and a number of members in their teams 

84
06:23:43,890 --> 06:23:46,930
if your game requires people to pair up 

85
06:23:46,930 --> 06:23:50,570
you may want to select teams which have an even number of members 

86
06:23:51,570 --> 06:23:57,130
in this case you can create a test that only passes the teams which have

87
06:23:57,130 --> 06:24:04,690
an even number of team members shown as divided by 2 with 0 in the remainder 

88
06:24:07,608 --> 06:24:12,147
the real effectiveness of the basic transformation we saw here

89
06:24:12,147 --> 06:24:17,025
is in pipelining them in a way that helps you to solve your specific

90
06:24:17,025 --> 06:24:21,819
problem just as you would perform a series of tasks on a real block

91
06:24:21,819 --> 06:24:26,610
of wood to make a fine piece of woodwork that you can use to steer your

92
06:24:26,610 --> 06:24:30,495
ship which in this case is your business or research 

1
12:40:38,550 --> 12:40:41,512
now that we revealed all three layers 

2
12:40:41,512 --> 12:40:46,556
we are ready to come back to the integration and processing layer 

3
12:40:46,556 --> 12:40:50,652
just a simple google search for big data processing pipelines

4
12:40:50,652 --> 12:40:55,378
will bring a vast number of pipelines with large number of technologies

5
12:40:55,378 --> 12:41:00,041
that support scalable data cleaning preparation and analysis 

6
12:41:02,091 --> 12:41:06,286
how do we make sense of it all to make sure we use the right tools for

7
12:41:06,286 --> 12:41:07,540
our application 

8
12:41:08,660 --> 12:41:13,490
we will continue our lecture to review a set of evaluation criteria for

9
12:41:13,490 --> 12:41:18,750
these systems and some of the big data processing systems based on this criteria 

10
12:41:20,250 --> 12:41:25,850
depending on the resources we have access to and characteristics of our application 

11
12:41:25,850 --> 12:41:29,130
we apply several considerations to evaluate and

12
12:41:29,130 --> 12:41:30,920
pick a software stack for big data 

13
12:41:32,010 --> 12:41:37,410
of these first one we consider is the execution model 

14
12:41:37,410 --> 12:41:42,450
and the expressivity of it to support for various transformations of batch or

15
12:41:42,450 --> 12:41:45,430
streaming data or sometimes interactive computing 

16
12:41:46,920 --> 12:41:50,550
semantics of streaming including exactly once or

17
12:41:50,550 --> 12:41:55,620
at least one processing for each event or being able to keep the state of the data 

18
12:41:55,620 --> 12:41:59,680
is an important concern for this execution model 

19
12:41:59,680 --> 12:42:04,110
latency is another important criteria depending on the application 

20
12:42:05,210 --> 12:42:08,760
having a low latency system is very important for

21
12:42:08,760 --> 12:42:12,345
applications like online gaming and hazards management 

22
12:42:12,345 --> 12:42:16,340
whereas most applications are less time critical 

23
12:42:16,340 --> 12:42:21,800
like search engine indexing and would be fine with a batch processing ability 

24
12:42:22,820 --> 12:42:27,760
scalability for both small and large datasets and different

25
12:42:27,760 --> 12:42:32,070
analytical methods and algorithms is also an important evaluation criteria 

26
12:42:33,290 --> 12:42:37,430
as well as support for different programming language

27
12:42:37,430 --> 12:42:41,470
of the libraries used by the analytical tools that we have access to 

28
12:42:42,700 --> 12:42:47,030
finally while all big data tools provide fault tolerance 

29
12:42:47,030 --> 12:42:52,390
the mechanics of how the fault tolerance is handled is an important issue to consider 

30
12:42:53,790 --> 12:42:56,900
let review five of the big data processing

31
12:42:56,900 --> 12:43:01,390
engines supported by the apache foundation using this evaluation criteria 

32
12:43:03,300 --> 12:43:08,230
the mapreduce implementation of hadoop provides a batch execution

33
12:43:08,230 --> 12:43:13,230
model where the data from hdfs gets loaded into mappers before processing 

34
12:43:14,440 --> 12:43:17,380
there is no in - memory processing support 

35
12:43:17,380 --> 12:43:22,490
meaning the mappers write the data on files before the reducers can read it 

36
12:43:22,490 --> 12:43:26,250
resulting in a high - latency and less scalable execution 

37
12:43:27,960 --> 12:43:31,690
this also hinders the performance of iterative and

38
12:43:31,690 --> 12:43:36,827
interactive applications that require many steps of transformations using mapreduce 

39
12:43:39,150 --> 12:43:42,835
although the only native programming interface for

40
12:43:42,835 --> 12:43:47,870
mapreduce is in java other programming languages like python provide modules or

41
12:43:47,870 --> 12:43:52,370
libraries for hadoop mapreduce programming however with less efficiency 

42
12:43:54,180 --> 12:43:58,730
data replication is the primary method of fault tolerance 

43
12:43:58,730 --> 12:44:04,270
which in turn affects the scalability and execution speed further 

44
12:44:04,270 --> 12:44:07,560
spark was built to support iterative and

45
12:44:07,560 --> 12:44:12,730
interactive big data processing pipelines efficiently using an in - memory

46
12:44:12,730 --> 12:44:17,800
structure called resilient distributed datasets or shortly rdds 

47
12:44:19,040 --> 12:44:23,860
in addition to map and reduce operations it provides support for

48
12:44:23,860 --> 12:44:27,890
a range of transformation operations like join and filter 

49
12:44:27,890 --> 12:44:33,711
any pipeline of transformations can be applied to these rdd in - memory 

50
12:44:33,711 --> 12:44:38,829
making spark performance very high for iterative processing 

51
12:44:40,538 --> 12:44:45,480
the rdd extraction is also designed to handle fault tolerance with

52
12:44:45,480 --> 12:44:47,230
less impact on performance 

53
12:44:48,886 --> 12:44:51,170
in addition to hdfs 

54
12:44:51,170 --> 12:44:56,200
spark can read data from many storage platforms and it provides support for

55
12:44:56,200 --> 12:45:02,155
streaming data applications using a technique called micro - batching 

56
12:45:02,155 --> 12:45:08,250
its latency can be on the order of seconds depending on the batch size 

57
12:45:08,250 --> 12:45:12,030
which is relatively slower compared to native streaming platforms 

58
12:45:13,600 --> 12:45:18,320
spark has support for a number of programming languages including scala and

59
12:45:18,320 --> 12:45:22,260
python as the most popular ones as well as built - in libraries for

60
12:45:22,260 --> 12:45:24,500
graph processing and machine learning 

61
12:45:25,530 --> 12:45:29,415
although flink has very similar transformations and

62
12:45:29,415 --> 12:45:33,890
in - memory data extractions with spark it provides direct support for

63
12:45:33,890 --> 12:45:37,517
streaming data making it a lower - latency framework 

64
12:45:38,620 --> 12:45:42,720
it provides connection interfaces to streaming data ingestion engines like

65
12:45:42,720 --> 12:45:44,020
kafka and flume 

66
12:45:45,460 --> 12:45:49,340
flink supports application programming interfaces in java and

67
12:45:49,340 --> 12:45:50,966
scala just like spark 

68
12:45:50,966 --> 12:45:55,760
starting with it original version called stratosphere 

69
12:45:55,760 --> 12:46:01,010
flink had it own execution engine called nephele and had an ability

70
12:46:01,010 --> 12:46:05,670
to run both on hadoop and also separately in its own execution environment 

71
12:46:07,060 --> 12:46:11,110
in addition to map and reduce flink provides abstractions for

72
12:46:11,110 --> 12:46:15,055
other data parallel database patterns like join and group by 

73
12:46:17,210 --> 12:46:22,010
one of the biggest advantage of using flink comes from it optimizer

74
12:46:22,010 --> 12:46:25,460
to pick and apply the best pattern and execution strategy 

75
12:46:26,490 --> 12:46:30,080
there has been experiments comparing fault tolerance features of flink

76
12:46:30,080 --> 12:46:34,720
to those of sparks which conclude that sparks slightly better for spark 

77
12:46:35,840 --> 12:46:41,080
the beam system from google is a relatively new system for

78
12:46:41,080 --> 12:46:44,890
batch and stream processing with a data flow programming model 

79
12:46:46,080 --> 12:46:51,090
it initially used google own cloud data flow as an execution environment but

80
12:46:51,090 --> 12:46:55,228
spark and flink backends for it have been implemented recently 

81
12:46:55,228 --> 12:47:00,370
it a low - latency environment with high reviews on fault tolerance 

82
12:47:01,460 --> 12:47:06,456
it currently provides application programming interfaces in java and

83
12:47:06,456 --> 12:47:09,295
scala and a python sdk is in the works 

84
12:47:09,295 --> 12:47:12,532
sdk means software development kit 

85
12:47:12,532 --> 12:47:18,690
beam provides a very strong streaming and windowing framework for streaming data 

86
12:47:18,690 --> 12:47:23,417
and it is highly scalable and reliable allowing it to make trade - off

87
12:47:23,417 --> 12:47:27,690
decisions between accuracy speed and cost of processing 

88
12:47:29,710 --> 12:47:31,730
storm has been designed for

89
12:47:31,730 --> 12:47:36,960
stream processing in real time with very low - latency 

90
12:47:36,960 --> 12:47:41,930
it defined input stream interface abstractions called spouts and

91
12:47:41,930 --> 12:47:44,080
computation abstractions called bolts 

92
12:47:45,420 --> 12:47:49,700
spouts and bolts can be pipelined together using a data flow approach 

93
12:47:49,700 --> 12:47:53,430
that data gets queued until the computation acknowledges

94
12:47:53,430 --> 12:47:54,190
the receipt of it 

95
12:47:56,310 --> 12:47:59,080
a master node tracks running jobs and

96
12:47:59,080 --> 12:48:02,860
ensures all data is processed by the computations on workers 

97
12:48:04,430 --> 12:48:09,952
nathan mars the lead developer for storm built the lambda architecture

98
12:48:09,952 --> 12:48:15,744
using storm for stream processing and hadoop mapreduce for batch processing 

99
12:48:18,660 --> 12:48:23,740
the lambda architecture originally used storm for

100
12:48:23,740 --> 12:48:28,229
speed layer and hadoop and hbase for batch and

101
12:48:28,229 --> 12:48:32,490
serving layers as seen in this diagram 

102
12:48:33,840 --> 12:48:38,760
however it was later used as a more general framework that can combine

103
12:48:38,760 --> 12:48:44,990
the results of stream and batch processing executed in multiple big data systems 

104
12:48:44,990 --> 12:48:50,700
this diagram shows a generalized lambda architecture containing some of the tools

105
12:48:50,700 --> 12:48:57,930
we discussed earlier including using spark for both batch and speed layers 

106
12:48:59,540 --> 12:49:03,530
in this course we picked spark as a big data integration and

107
12:49:03,530 --> 12:49:08,600
processing environment since it supports most of our evaluation criteria 

108
12:49:08,600 --> 12:49:13,862
and this hybrid data processing architecture using built - in data querying 

109
12:49:13,862 --> 12:49:16,619
streaming and analytical libraries 

110
12:49:16,619 --> 12:49:21,586
we will continue our discussion with spark and hands - on exercises in spark 

1
01:30:00,740 --> 01:30:03,460
analytical operations in big data pipelines 

2
01:30:04,940 --> 01:30:06,730
after this video 

3
01:30:06,730 --> 01:30:12,080
you will be able to list common analytical operations within big data pipelines and

4
01:30:12,080 --> 01:30:15,650
describe sample applications for these analytical operations 

5
01:30:17,550 --> 01:30:23,200
in this lesson we will be looking at analytical operations 

6
01:30:23,200 --> 01:30:27,810
these are operations used in analytics which is the process of

7
01:30:27,810 --> 01:30:32,370
transforming data into insights for making more informed decisions 

8
01:30:33,690 --> 01:30:38,720
the purpose of analytical operations is to analyze the data to discover meaningful

9
01:30:38,720 --> 01:30:43,089
trends and patterns in order to gain insights into the problem being studied 

10
01:30:44,138 --> 01:30:46,810
the knowledge gained from these insights

11
01:30:46,810 --> 01:30:50,890
ultimately lead to more informed decisions driven by data 

12
01:30:53,150 --> 01:30:57,326
here are some common analytical operations that we will discuss in this lecture 

13
01:30:57,326 --> 01:31:01,704
classification clustering 

14
01:31:01,704 --> 01:31:05,380
path analysis and connectivity analysis 

15
01:31:06,630 --> 01:31:08,490
let start with classification 

16
01:31:09,730 --> 01:31:17,130
in classification the goal is to predict a categorical target from the input data 

17
01:31:17,130 --> 01:31:20,290
a categorical target is one with discreet values or

18
01:31:20,290 --> 01:31:22,820
categories instead of continuous values 

19
01:31:24,470 --> 01:31:27,460
for example this diagram shows

20
01:31:27,460 --> 01:31:33,260
a classification task to determine the risk associated with a loan application 

21
01:31:33,260 --> 01:31:36,801
the input consists of the loan amount 

22
01:31:36,801 --> 01:31:43,571
applicant information such as income age debts and a down payment 

23
01:31:45,202 --> 01:31:49,903
from this input data the task is to determine whether

24
01:31:49,903 --> 01:31:53,898
the loan application is low risk or high risk 

25
01:31:56,216 --> 01:31:59,852
there are many classification techniques or algorithms that can be used for

26
01:31:59,852 --> 01:32:01,480
this problem 

27
01:32:01,480 --> 01:32:06,160
we will discuss a specific one namely decision tree in the next slide 

28
01:32:08,330 --> 01:32:12,590
the decision tree algorithm is one technique for classification 

29
01:32:12,590 --> 01:32:14,000
with this technique 

30
01:32:14,000 --> 01:32:19,040
decisions to perform the classification task are modeled as a tree structure 

31
01:32:21,070 --> 01:32:25,770
for the loan risk assessment problem a simple decision tree is shown here 

32
01:32:26,960 --> 01:32:31,470
where the loan application is classified as being either low risk or

33
01:32:31,470 --> 01:32:33,930
high risk based on the loan amount 

34
01:32:33,930 --> 01:32:36,750
the applicant income and the applicant age 

35
01:32:39,090 --> 01:32:43,110
the decision tree algorithm is implemented in many machine learning tools 

36
01:32:44,310 --> 01:32:50,718
this diagram shows how to specify decision tree from input data knime 

37
01:32:50,718 --> 01:32:54,650
a graphical user - interface - based machine learning platform 

38
01:32:56,370 --> 01:33:01,400
some examples of classification are the prediction of whether cells from

39
01:33:01,400 --> 01:33:06,540
a tumor are benign or malignant categorization of

40
01:33:06,540 --> 01:33:11,630
handwritten digits as being zero one two etc up to nine 

41
01:33:13,180 --> 01:33:17,760
and determining whether a credit card transaction is legitimate or fraudulent 

42
01:33:19,940 --> 01:33:24,255
and classification of a loan application as being low - risk medium - risk or

43
01:33:24,255 --> 01:33:25,540
high - risk as you have seen 

44
01:33:27,370 --> 01:33:30,970
another common analytical operation is cluster analysis 

45
01:33:32,180 --> 01:33:34,880
in cluster analysis or clustering 

46
01:33:34,880 --> 01:33:39,320
the goal is to organize similar items in to groups of association 

47
01:33:40,880 --> 01:33:46,045
this diagram shows an example of cluster analysis in which customers are clustered

48
01:33:46,045 --> 01:33:50,510
into groups according to their preferences of movie genre 

49
01:33:52,080 --> 01:33:55,450
so customers who like sci - fi movies are grouped together 

50
01:33:57,010 --> 01:34:00,814
those who like drama movies are grouped together 

51
01:34:00,814 --> 01:34:05,337
and customers who like horror movies are grouped together 

52
01:34:05,337 --> 01:34:09,482
with this grouping new movies as well as other products 

53
01:34:09,482 --> 01:34:14,184
such as books can be offered to the right type of costumers in order to

54
01:34:14,184 --> 01:34:17,073
generate interest and increase revenue 

55
01:34:19,775 --> 01:34:24,516
a simple and commonly used algorithm for cluster analysis is k - means 

56
01:34:26,163 --> 01:34:30,670
with k - means samples are divided into k clusters 

57
01:34:30,670 --> 01:34:34,100
this clustering is done in order to minimize the variance or

58
01:34:34,100 --> 01:34:37,620
similarity between samples within the same cluster

59
01:34:37,620 --> 01:34:40,590
using some similarity measures such as distance 

60
01:34:41,690 --> 01:34:46,210
in this example k is equal to three and

61
01:34:46,210 --> 01:34:52,170
k - means divides the original data shown on the left into three clusters 

62
01:34:52,170 --> 01:34:56,620
shown as blue green and red on the chart on the right 

63
01:34:58,700 --> 01:35:02,530
the k - means clustering algorithm is implemented on many machine - learning

64
01:35:02,530 --> 01:35:03,180
platforms 

65
01:35:04,360 --> 01:35:07,550
the code here shows how to read in and

66
01:35:07,550 --> 01:35:12,190
parse input data and perform k - means clustering on the data 

67
01:35:12,190 --> 01:35:17,210
other examples of cluster analysis are grouping a company s customer base

68
01:35:17,210 --> 01:35:23,560
into distinct segments for more effective targeted marketing finding articles or

69
01:35:23,560 --> 01:35:27,240
webpages with similar topics for retrieving relevant information 

70
01:35:29,060 --> 01:35:34,844
identification of areas in the city with rates of particular types of crimes for

71
01:35:34,844 --> 01:35:39,077
effective management of law enforcement resources and

72
01:35:39,077 --> 01:35:45,055
determining different groups of weather patterns such as rainy cold or snowy 

73
01:35:47,190 --> 01:35:50,632
classification and cluster analysis are considered machine learning and

74
01:35:50,632 --> 01:35:52,620
analytical operations 

75
01:35:52,620 --> 01:35:55,790
there are also analytical operations from graph analytics 

76
01:35:55,790 --> 01:36:00,790
which is the field of analytics where the underlying data is structured as or

77
01:36:00,790 --> 01:36:02,670
can be modeled as the set of graphs 

78
01:36:04,020 --> 01:36:08,190
one analytical operation using graphs as path analysis 

79
01:36:08,190 --> 01:36:12,000
which analyzes sequences of nodes and edges in a graph 

80
01:36:13,390 --> 01:36:16,500
a common application of path analysis

81
01:36:16,500 --> 01:36:20,980
is to find routes from one location to another location 

82
01:36:20,980 --> 01:36:26,440
for example you might want to find the shortest path from your home to your work 

83
01:36:26,440 --> 01:36:31,100
this path may be different depending on conditions such as the day of the week 

84
01:36:31,100 --> 01:36:35,450
time of day traffic congestion weather and etc 

85
01:36:37,390 --> 01:36:42,530
this code shows some operations for path analysis on neo4j 

86
01:36:42,530 --> 01:36:47,033
which is a graph database system using a query language called cypher 

87
01:36:48,210 --> 01:36:53,440
the first operation finds the shortest path between specific nodes in a graph 

88
01:36:54,610 --> 01:36:58,460
the second operation finds all the shortest paths in a graph 

89
01:36:59,890 --> 01:37:03,770
connectivity analysis of graphs has to do with finding and

90
01:37:03,770 --> 01:37:06,970
tracking groups to determine interactions between entities 

91
01:37:08,110 --> 01:37:12,330
entities in highly interacting groups are more connected

92
01:37:12,330 --> 01:37:16,680
to each other than to entities of other groups in a graph 

93
01:37:17,820 --> 01:37:20,860
these groups are called communities and

94
01:37:20,860 --> 01:37:25,280
are interesting to analyze as they give insights into the degree and

95
01:37:25,280 --> 01:37:30,410
patterns of the interaction between entities and also between communities 

96
01:37:31,880 --> 01:37:38,570
some applications of connectivity analysis are to extract conversation threads 

97
01:37:38,570 --> 01:37:41,030
for example by looking at tweets and retweets 

98
01:37:42,250 --> 01:37:46,720
to find interacting groups for example to determine which users are interacting

99
01:37:46,720 --> 01:37:52,900
with each other users to find influencers for example to understand

100
01:37:52,900 --> 01:37:57,720
who are the main users leading to the conversation about a particular topic 

101
01:37:57,720 --> 01:38:00,550
or who do people pay attention to 

102
01:38:00,550 --> 01:38:04,020
this information can be used to identify the fewest number of

103
01:38:04,020 --> 01:38:05,990
people with the greatest influence 

104
01:38:05,990 --> 01:38:10,830
for example for political campaigns or marketing on social media 

105
01:38:12,360 --> 01:38:14,680
this code shows some operations for

106
01:38:14,680 --> 01:38:19,210
connectivity analysis on neo4j using the query language cypher again 

107
01:38:20,980 --> 01:38:24,660
the first operation finds the degree of all the nodes in a graph 

108
01:38:24,660 --> 01:38:29,450
and the second creates a histogram of degrees for all nodes in a graph

109
01:38:29,450 --> 01:38:34,990
to determine how connected a node in a graph is we need to look at its degree 

110
01:38:34,990 --> 01:38:38,860
the degree of a node is the number of edges connected to the node 

111
01:38:40,220 --> 01:38:44,250
a degree histogram shows the distribution of node degrees in the graph and

112
01:38:44,250 --> 01:38:49,330
is useful in comparing graphs and identifying types of users for

113
01:38:49,330 --> 01:38:54,410
example those who follow versus those who are followed in social networks 

114
01:38:55,940 --> 01:39:00,970
to summarize and add to these techniques the decision tree algorithm for

115
01:39:00,970 --> 01:39:05,470
classification and k - means algorithm for cluster analysis that we covered in this

116
01:39:05,470 --> 01:39:08,040
lecture are techniques from machine learning 

117
01:39:09,210 --> 01:39:14,650
machine learning is a field of analytics focused on the study and construction of

118
01:39:14,650 --> 01:39:19,819
computer systems that can learn from data without being explicitly programmed 

119
01:39:21,140 --> 01:39:24,540
our course on machine learning in this specialization will cover these

120
01:39:24,540 --> 01:39:28,230
algorithms in more detail along with other algorithms used for

121
01:39:28,230 --> 01:39:30,660
classification and cluster analysis 

122
01:39:30,660 --> 01:39:33,690
as well as algorithms for other machine learning tasks 

123
01:39:33,690 --> 01:39:38,360
such as regression association analysis and tools for

124
01:39:38,360 --> 01:39:41,480
implementing and executing machine learning algorithms 

125
01:39:43,380 --> 01:39:48,130
as a summary of the graph analytics the path analytics technique for finding

126
01:39:48,130 --> 01:39:52,390
the shortest path and the connectivity analysis technique for analyzing communities

127
01:39:52,390 --> 01:39:57,260
that we discussed earlier are techniques used in graph analytics 

128
01:39:57,260 --> 01:40:01,970
as explained earlier graph analytics is the field of analytics 

129
01:40:01,970 --> 01:40:06,520
where the underlying data is structured or can be modeled as a set of graphs 

130
01:40:07,760 --> 01:40:12,200
our graph analytics course in the specialization will cover these and other

131
01:40:12,200 --> 01:40:17,174
graph techniques and we will also cover tools and platforms for graph analytics 

132
01:40:17,174 --> 01:40:24,610
in summary analytic operations are used to discover meaningful

133
01:40:24,610 --> 01:40:30,115
patterns in the data in order to provide insights into the problem being studied 

134
01:40:30,115 --> 01:40:35,635
we looked at some of the examples of analytical operations for classification 

135
01:40:35,635 --> 01:40:40,355
cluster analysis path analysis and connectivity analysis in this lecture 

1
03:10:40,226 --> 03:10:41,488
in this hands - on activity 

2
03:10:41,488 --> 03:10:44,520
we will be performing word count on the complete works of shakespeare 

3
03:10:46,110 --> 03:10:50,350
first we will copy the shakespeare text into the hadoop file system 

4
03:10:50,350 --> 03:10:53,847
next we will create a new jupyter notebook and

5
03:10:53,847 --> 03:10:56,937
read the shakespeare text into a spark rdd 

6
03:10:56,937 --> 03:11:00,177
we will then perform wordcount using map and reduce 

7
03:11:00,177 --> 03:11:03,352
and write the results to hdfs and view the contents 

8
03:11:06,553 --> 03:11:08,301
let begin 

9
03:11:08,301 --> 03:11:12,300
in the intro to big data course we copy the shakespeare text into hdfs 

10
03:11:12,300 --> 03:11:15,630
let see if it still there 

11
03:11:15,630 --> 03:11:17,150
if not we can copy it now 

12
03:11:17,150 --> 03:11:21,180
click on the terminal icon at the top of the toolbar 

13
03:11:22,950 --> 03:11:28,830
now we can run hadoop fs - ls to see what is in our hadoop filesystem directory 

14
03:11:30,900 --> 03:11:33,380
there are no files in htfs so let copy it 

15
03:11:33,380 --> 03:11:40,070
if you already have words txt in your htfs directory you can skip this next step 

16
03:11:41,450 --> 03:11:45,837
cd into downloads 

17
03:11:45,837 --> 03:11:54,359
big - data - 3 spark - wordcount 

18
03:11:54,359 --> 03:11:56,450
we can do ls to see the file 

19
03:11:57,960 --> 03:12:00,390
let copy this file to htfs 

20
03:12:00,390 --> 03:12:07,221
we run hadoop fs copy from local words txt 

21
03:12:12,314 --> 03:12:16,800
we can write hadoop fs - ls again to verify that the file is there 

22
03:12:20,290 --> 03:12:21,600
now let do work count in spark 

23
03:12:23,220 --> 03:12:26,380
we will do this in an ipython notebook using jupyter server 

24
03:12:28,070 --> 03:12:30,587
look on the web browser icon the top of the toolbar 

25
03:12:34,112 --> 03:12:38,706
and go to the jupyter server url which is local host port 8889 

26
03:12:41,360 --> 03:12:43,789
next let create a new ipython notebook

27
03:12:49,685 --> 03:12:55,855
the first step is to read the words txt files in htfs into a spark rdd 

28
03:12:55,855 --> 03:12:57,572
we will call the rdd lines 

29
03:13:01,140 --> 03:13:06,128
we can read it using the spark context sc in calling the text file method 

30
03:13:11,609 --> 03:13:16,041
the argument is the url of the word set txt file and hdfs 

31
03:13:26,270 --> 03:13:30,998
let run this we can

32
03:13:30,998 --> 03:13:35,920
view the contents of this rdd by calling lines take 5 

33
03:13:40,553 --> 03:13:44,010
the argument 5 says how many lines to show of the rdd 

34
03:13:45,930 --> 03:13:50,112
next we will transform this rdd of lines into an rdd of words 

35
03:13:50,112 --> 03:13:54,755
we will say words = lines flatmap 

36
03:13:57,940 --> 03:13:59,852
lambda line : 

37
03:14:03,123 --> 03:14:07,568
line split double quote space double quote 

38
03:14:10,374 --> 03:14:17,852
this creates a new rdd called words by running flatmap over the line rdd 

39
03:14:17,852 --> 03:14:20,092
the argument is this lambda expression 

40
03:14:22,729 --> 03:14:26,310
a lambda in python is a simple way to declare a one line expression 

41
03:14:27,860 --> 03:14:30,890
in this case there one argument called line and

42
03:14:30,890 --> 03:14:34,160
we called it split method on this line and we split on spaces 

43
03:14:36,080 --> 03:14:38,720
we can run this and look at the contents of words 

44
03:14:48,720 --> 03:14:51,500
we can see that each element now is an individual word 

45
03:14:53,769 --> 03:14:56,136
next we will create tuples of these words 

46
03:14:57,907 --> 03:15:02,138
we will put them in a new rdd called tuples 

47
03:15:02,138 --> 03:15:05,405
enter tuples

48
03:15:05,405 --> 03:15:10,433
 = words map : lambda

49
03:15:10,433 --> 03:15:15,465
word; word 1 

50
03:15:20,144 --> 03:15:24,080
this creates the tuples by transforming words 

51
03:15:24,080 --> 03:15:26,370
this uses map and another lambda function 

52
03:15:26,370 --> 03:15:32,400
in this case the lambda takes one argument and returns a tuple 

53
03:15:32,400 --> 03:15:34,350
where the first value of the tuple is the word 

54
03:15:35,500 --> 03:15:37,320
the second value is the number 1 

55
03:15:37,320 --> 03:15:43,460
not that in this case we use map whereas before we used flat map 

56
03:15:45,490 --> 03:15:48,930
in this case we want a tuple for every word in the words 

57
03:15:48,930 --> 03:15:52,690
so we have a one to one mapping between inputs and outputs 

58
03:15:52,690 --> 03:15:58,064
previously while we were splitting lines into word each line had multiple words 

59
03:16:00,677 --> 03:16:01,348
in general 

60
03:16:01,348 --> 03:16:05,950
you want to use map when you have a one to one mapping between inputs and outputs 

61
03:16:05,950 --> 03:16:10,804
in flatmap you have a one to many or none mapping between inputs and

62
03:16:10,804 --> 03:16:16,607
outputs let run this and look at tuples 

63
03:16:24,043 --> 03:16:29,538
we can see that each word now has a tuple initialized with the count of 1 

64
03:16:29,538 --> 03:16:34,080
we can now count all the words by combining or reducing these tuples 

65
03:16:34,080 --> 03:16:36,190
we will put this in a new rdd called counts 

66
03:16:37,660 --> 03:16:42,039
so we will say counts equals tuples reduce by key 

67
03:16:46,602 --> 03:16:50,955
lambda a b : 

68
03:16:50,955 --> 03:16:52,125
a + b 

69
03:16:55,313 --> 03:16:59,038
in this case the lambda function takes two arguments a and be and

70
03:16:59,038 --> 03:17:01,199
will return the result of adding a and b 

71
03:17:03,876 --> 03:17:05,071
to view the result 

72
03:17:12,657 --> 03:17:16,815
you can see now that the counts for each words have been created 

73
03:17:16,815 --> 03:17:19,839
we can write this result back to hdfs 

74
03:17:19,839 --> 03:17:29,239
let say counts coalesce 1 saveastextfile 

75
03:17:29,239 --> 03:17:32,120
and then the url 

76
03:17:43,966 --> 03:17:48,675
the coalesce means we only want a single output file 

77
03:17:48,675 --> 03:17:51,172
let go back to our shell and view the results 

78
03:17:54,582 --> 03:17:57,930
we will run hadoop fs - ls to see the directory 

79
03:18:00,809 --> 03:18:03,553
and run it again to look inside the wordcount directory 

80
03:18:08,294 --> 03:18:11,602
and once more to look inside wordcount outputdir 

81
03:18:17,808 --> 03:18:24,420
as you recall the output from hadoop jobs is part - 0000 

82
03:18:24,420 --> 03:18:27,837
this is also true for spark jobs 

83
03:18:27,837 --> 03:18:30,484
let copy this file to the local file system 

84
03:18:30,484 --> 03:18:34,899
we will run hadoop fs copytolocal

85
03:18:34,899 --> 03:18:41,212
wordcount outputdir part - 00000 

86
03:18:48,530 --> 03:18:50,066
you can view the results with more 

1
06:29:30,550 --> 06:29:31,530
in this hands on activity 

2
06:29:31,530 --> 06:29:34,190
we will be using spark streaming to read weather data 

3
06:29:35,900 --> 06:29:38,470
first we open the spark streaming jupyter notebook 

4
06:29:39,610 --> 06:29:42,680
next we will look at sensor format and measurement types 

5
06:29:43,695 --> 06:29:48,110
we will then create a spark dstream of weather data read the measurements and

6
06:29:48,110 --> 06:29:49,630
create a sliding window of the data 

7
06:29:49,630 --> 06:29:55,180
we will define a function to display the maximum and minimum values in the window 

8
06:29:55,180 --> 06:29:57,770
we start to stream processing to give their results 

9
06:30:00,350 --> 06:30:04,610
before we begin this activity we need to change the virtual box settings for

10
06:30:04,610 --> 06:30:06,100
our carder virtual machine 

11
06:30:08,110 --> 06:30:11,330
start streaming needs more than one thread of execution 

12
06:30:11,330 --> 06:30:14,620
so we need to change the settings to add more than one virtual processor 

13
06:30:16,180 --> 06:30:21,930
first shut down your cloudera virtual machine and go to the virtual box manager 

14
06:30:23,960 --> 06:30:27,300
select the cloudera virtual box and click on settings 

15
06:30:29,720 --> 06:30:34,040
next click on system click on processor 

16
06:30:36,520 --> 06:30:43,444
and change the number of cpu to be two or more 

17
06:30:43,444 --> 06:30:48,910
when you are done click okay and start the machine as usual 

18
06:30:52,740 --> 06:30:53,760
let begin 

19
06:30:53,760 --> 06:30:56,280
first click on the browser icon at the top of the tool bar 

20
06:30:58,370 --> 06:31:04,560
navigate to the jupyter notebook server monitoring local host calling 8889 

21
06:31:04,560 --> 06:31:08,778
we will then go in to downloads 

22
06:31:08,778 --> 06:31:12,337
big data 3 

23
06:31:12,337 --> 06:31:15,186
spark - streaming 

24
06:31:15,186 --> 06:31:17,616
let then open spark - streaming notebook 

25
06:31:20,346 --> 06:31:24,070
this first line shows the example data we get from the weather station 

26
06:31:25,680 --> 06:31:28,680
each line has a time stamp and a set of measurements 

27
06:31:30,860 --> 06:31:34,200
each of these abbreviations is a particular type of measurement 

28
06:31:34,200 --> 06:31:35,680
followed by the actual value 

29
06:31:38,140 --> 06:31:40,328
the next cell shows the key for these measurements 

30
06:31:40,328 --> 06:31:45,190
for this hands - on we are interested in the average wind direction 

31
06:31:45,190 --> 06:31:47,059
which is abbreviated as dm 

32
06:31:49,470 --> 06:31:53,178
this next cell defines a function that parses each line of text and

33
06:31:53,178 --> 06:31:55,080
pulls out the average wind speed 

34
06:31:56,610 --> 06:31:58,860
we define it here so we do not have to type it in later 

35
06:32:00,360 --> 06:32:01,627
let run this cell 

36
06:32:04,308 --> 06:32:07,790
next let create a streaming spark context 

37
06:32:07,790 --> 06:32:09,870
first we will need to import the module 

38
06:32:09,870 --> 06:32:15,590
we will enter from pyspark streaming import streamingcontext 

39
06:32:15,590 --> 06:32:20,130
we can create a new streaming context 

40
06:32:20,130 --> 06:32:21,920
we will put in in a variable called ssc 

41
06:32:23,400 --> 06:32:28,218
we will enter ssc = streamingcontext sc 1 

42
06:32:28,218 --> 06:32:31,710
the sc is a streamingcontext 

43
06:32:31,710 --> 06:32:36,070
the 1 specifies the batch interval 1 second in this case 

44
06:32:36,070 --> 06:32:36,760
let run this 

45
06:32:39,040 --> 06:32:40,580
next we will create a dstream 

46
06:32:42,100 --> 06:32:46,100
we will import the streaming weather data over a tcp connection 

47
06:32:46,100 --> 06:32:48,099
we will put this in a dstream called lines 

48
06:32:50,940 --> 06:32:55,249
let say lines = ssc sockettextstream 

49
06:32:55,249 --> 06:33:00,661
we will enter the host name in port of the weather station 

50
06:33:00,661 --> 06:33:04,757
rtd hpwren ucsd edu for 12028 

51
06:33:04,757 --> 06:33:05,784
let run this 

52
06:33:09,924 --> 06:33:15,260
next we will create a new d - stream called vals that would hold the measurements 

53
06:33:15,260 --> 06:33:21,390
we will say vals = lines flatmap parse 

54
06:33:21,390 --> 06:33:24,020
this calls the parse function we defined above for

55
06:33:24,020 --> 06:33:26,228
each of the lines coming from the weather station 

56
06:33:26,228 --> 06:33:30,620
the resulting d - stream will have just the average wind direction values 

57
06:33:31,850 --> 06:33:32,570
we will run this 

58
06:33:36,150 --> 06:33:40,340
next we will create a window that will aggregate the d - stream values 

59
06:33:42,250 --> 06:33:46,470
we will say window = vals window 10 5 

60
06:33:46,470 --> 06:33:51,850
the first argument specifies that the length of the window should be 10 seconds 

61
06:33:51,850 --> 06:33:56,515
the second argument specifies that the window should move every 5 seconds 

62
06:33:56,515 --> 06:33:58,900
let run this 

63
06:34:00,740 --> 06:34:03,480
next we will define a function that prints the minimum and

64
06:34:03,480 --> 06:34:05,240
maximum values that we see 

65
06:34:05,240 --> 06:34:06,710
we will start by entering the definition 

66
06:34:08,440 --> 06:34:13,230
def stats this will take an rdd as an argument 

67
06:34:16,330 --> 06:34:19,478
next let print the entire contents of the rdd 

68
06:34:19,478 --> 06:34:23,736
print parenthesis rdd collect 

69
06:34:23,736 --> 06:34:28,932
this will print the entire content of the rdd 

70
06:34:28,932 --> 06:34:30,484
in a real big data application 

71
06:34:30,484 --> 06:34:33,020
this will be impractical due to the size of the data 

72
06:34:34,020 --> 06:34:36,940
however for this hands on the rdd is small and so

73
06:34:36,940 --> 06:34:39,330
we can use this to see the contents of the rdd 

74
06:34:41,720 --> 06:34:43,670
next we will print the min and max 

75
06:34:44,940 --> 06:34:46,279
before we do that however 

76
06:34:46,279 --> 06:34:50,303
we should check to make sure that the size of the rdd is greater than zero 

77
06:34:50,303 --> 06:34:53,767
we will check that rdd count is greater than 0 

78
06:34:58,285 --> 06:35:03,647
finally we will print the minid max 

79
06:35:03,647 --> 06:35:08,826
we will enter print max = min = 

80
06:35:08,826 --> 06:35:13,853
 outside of the quote we will do

81
06:35:13,853 --> 06:35:19,653
 format rdd max rdd min 

82
06:35:19,653 --> 06:35:23,286
let run this next 

83
06:35:23,286 --> 06:35:28,860
let call this function stats 

84
06:35:28,860 --> 06:35:30,944
so all the rdds in our sliding window 

85
06:35:30,944 --> 06:35:37,366
i will enter window foreachrdd stats 

86
06:35:37,366 --> 06:35:42,539
run this 

87
06:35:42,539 --> 06:35:45,110
we are now ready to start our streaming processing 

88
06:35:45,110 --> 06:35:48,880
we can do this by entering ssc start 

89
06:35:48,880 --> 06:35:50,567
we will run this to start the streaming 

90
06:35:58,214 --> 06:36:01,975
when we want to stop this streaming we will run ssc stop

91
06:36:07,028 --> 06:36:09,200
please scroll up and look at the beginning of the output 

92
06:36:12,130 --> 06:36:16,770
we will see that it printing the full window and the min and max values 

93
06:36:19,010 --> 06:36:21,610
notice that in the beginning the window is not yet filled 

94
06:36:21,610 --> 06:36:23,610
in this case there only three entries 

95
06:36:24,760 --> 06:36:28,029
we count to see that the window is moving by five measurements 

96
06:36:29,220 --> 06:36:32,510
for example the last five measurements in the second window 

97
06:36:33,540 --> 06:36:35,870
are the first five measurements in the third window 

1
13:06:04,770 --> 13:06:08,034
in this hands on activity we will be using sparksql to

2
13:06:08,034 --> 13:06:10,119
query data from an sql database 

3
13:06:11,604 --> 13:06:15,825
first we will open the sparksql jupyter notebook 

4
13:06:15,825 --> 13:06:18,080
we will connect spark to a postgres table 

5
13:06:19,740 --> 13:06:24,214
and then view the spark dataframe schema and count the rows 

6
13:06:24,214 --> 13:06:26,040
we will view the contents of the data frame 

7
13:06:27,220 --> 13:06:29,110
see how to filter rows and columns 

8
13:06:30,130 --> 13:06:33,174
and finally perform aggregate operation on a column 

9
13:06:37,636 --> 13:06:38,714
let begin 

10
13:06:38,714 --> 13:06:42,247
first click on the browser icon the top of the toolbar 

11
13:06:42,247 --> 13:06:45,062
 sound next 

12
13:06:45,062 --> 13:06:49,288
navigate to the jupyter notebook server 

13
13:06:49,288 --> 13:06:52,275
it localhost : 8889 

14
13:06:55,117 --> 13:06:58,358
go to downloads 

15
13:06:58,358 --> 13:07:02,900
big data 3 spark sql 

16
13:07:04,460 --> 13:07:06,040
to open the sparksql notebook 

17
13:07:06,040 --> 13:07:11,710
the first three cells have already been entered for you 

18
13:07:13,760 --> 13:07:20,915
first we import the sqlcontext run this 

19
13:07:20,915 --> 13:07:28,878
next we create an sqlcontext from the sparkcontext run this 

20
13:07:28,878 --> 13:07:33,676
and next we will create a spark dataframe from a postgres table 

21
13:07:36,980 --> 13:07:38,950
we used the read attribute format 

22
13:07:40,820 --> 13:07:44,020
the jdbc argument means that we are using a java database connection 

23
13:07:46,180 --> 13:07:48,000
the next line sets the url option 

24
13:07:48,000 --> 13:07:53,081
it says we are using postgres database running on the local host 

25
13:07:53,081 --> 13:07:58,287
the database name is cloudera and the username is cloudera 

26
13:07:58,287 --> 13:08:00,048
the second option db table 

27
13:08:00,048 --> 13:08:04,000
says we want our data frame to be the game clicks table 

28
13:08:04,000 --> 13:08:05,130
and finally we call load 

29
13:08:07,290 --> 13:08:08,400
let execute this 

30
13:08:10,370 --> 13:08:15,153
you can see the schema of the data frame by calling df printschema 

31
13:08:18,602 --> 13:08:21,593
this shows the name of each column along with the data type 

32
13:08:24,499 --> 13:08:28,253
we can count the rows in this df frame by calling df count 

33
13:08:35,512 --> 13:08:39,786
we can look at the first five rows by calling df show 5 

34
13:08:43,493 --> 13:08:45,362
this shows all the columns in the data frame 

35
13:08:47,240 --> 13:08:50,260
we can select specific columns by using the select method 

36
13:08:52,080 --> 13:08:56,275
let select just the user id and team level columns 

37
13:08:56,275 --> 13:09:01,860
i will enter df select userid teamlevel 

38
13:09:01,860 --> 13:09:02,641
parenthesis 

39
13:09:05,524 --> 13:09:08,610
and finally we only want to see the top five rows 

40
13:09:08,610 --> 13:09:10,312
so we will do show 5 

41
13:09:14,870 --> 13:09:19,368
we can also select rows that have a specific value 

42
13:09:19,368 --> 13:09:23,067
let look for the rows where the team level is greater than one 

43
13:09:23,067 --> 13:09:27,092
we will enter df filter 

44
13:09:27,092 --> 13:09:31,393
we will specify that we want team level greater than one by entering df 

45
13:09:31,393 --> 13:09:34,412
square bracket team level greater than one 

46
13:09:38,336 --> 13:09:42,183
and again we only want the user id and team level columns 

47
13:09:47,078 --> 13:09:49,368
and finally only the first five rows 

48
13:09:54,624 --> 13:09:59,366
we can use the group by method to aggregate a particular column 

49
13:09:59,366 --> 13:10:03,542
for example the ishit column has a value of zero or one 

50
13:10:03,542 --> 13:10:08,611
and we can use group by to count how many times each of these values occurs 

51
13:10:08,611 --> 13:10:16,068
or a df groupby ishit and we will call count to count the values

52
13:10:25,520 --> 13:10:29,552
we can also perform aggregate statistical operations on the data in a data frame 

53
13:10:31,000 --> 13:10:35,140
let compute the mean and sum values for ishit 

54
13:10:35,140 --> 13:10:40,756
first we need to import the statistical functions we will 

55
13:10:40,756 --> 13:10:45,910
run from pyspark sql functions import star 

56
13:10:45,910 --> 13:10:51,891
next we will run df select mean ishit sum ishit

57
13:10:59,375 --> 13:11:02,580
we can also join two data frames on a particular column 

58
13:11:03,980 --> 13:11:08,740
let join the existing data frame of the game clicks table with the adclicks table 

59
13:11:09,850 --> 13:11:12,410
first we need to create data frame for the adclicks table 

60
13:11:13,810 --> 13:11:14,796
let go back up 

61
13:11:19,286 --> 13:11:20,991
copy the content of this cell 

62
13:11:27,747 --> 13:11:30,795
paste it 

63
13:11:30,795 --> 13:11:35,356
we put the adclicks table the data frame called df2 

64
13:11:35,356 --> 13:11:38,462
and we will change the db table option to the adclicks 

65
13:11:42,376 --> 13:11:43,388
run it 

66
13:11:45,687 --> 13:11:48,117
let print the schema of df2 

67
13:11:56,283 --> 13:12:00,064
you can see that it also has a column called user id 

68
13:12:00,064 --> 13:12:04,024
so let join the game clicks data frame with the add

69
13:12:04,024 --> 13:12:06,643
clicks data frame on this column 

70
13:12:06,643 --> 13:12:10,229
we put the result in a new data frame called merged 

71
13:12:10,229 --> 13:12:19,490
we will say merge = df join df2 userid 

72
13:12:19,490 --> 13:12:19,990
we will run it 

73
13:12:21,820 --> 13:12:23,163
let look at the schema 

74
13:12:23,163 --> 13:12:25,193
we will call merge printschema 

75
13:12:29,763 --> 13:12:33,499
we can see that this merged data frame has the column for both game clicks and

76
13:12:33,499 --> 13:12:34,105
adclicks 

77
13:12:35,780 --> 13:12:39,537
finally we will look at the top five rows in this merged data frame 

78
13:12:39,537 --> 13:12:42,208
we will run merge show 

1
02:18:48,020 --> 02:18:51,610
we have now seen some simple transformations and

2
02:18:51,610 --> 02:18:56,900
how spark can create rdds from each other using transformations 

3
02:18:56,900 --> 02:19:01,390
we learned that transformations are evaluated after an action is performed 

4
02:19:02,810 --> 02:19:07,800
so we can simply define actions as rdd operations that trigger the evaluation of

5
02:19:07,800 --> 02:19:13,530
the transformation pipeline and return the final result to the driver program or

6
02:19:13,530 --> 02:19:15,810
save the results to a persistent storage 

7
02:19:17,660 --> 02:19:22,580
we can also call them the last step in a spark pipeline 

8
02:19:22,580 --> 02:19:24,700
let now look at a few action operations 

9
02:19:26,870 --> 02:19:32,150
after this video you will be able to explain the steps of a spark pipeline

10
02:19:32,150 --> 02:19:38,010
ending with a collect action and list four common action operations in spark 

11
02:19:40,750 --> 02:19:43,740
a very common action in spark is collect 

12
02:19:45,090 --> 02:19:49,875
in this example we can imagine that initially we are reading from hdfs 

13
02:19:51,390 --> 02:19:56,020
the rdd partitions that go through the transformation steps in our big data

14
02:19:56,020 --> 02:20:00,970
pipeline are defined as flatmap and groupbykey 

15
02:20:02,290 --> 02:20:07,625
when the final step is done the collect action is called and

16
02:20:07,625 --> 02:20:11,840
spark sends all the tasks for execution to the worker notes 

17
02:20:14,372 --> 02:20:18,940
collect will send all the resulting rdds from the workers and

18
02:20:18,940 --> 02:20:23,140
copy them to the java virtual machine on the driver program 

19
02:20:23,140 --> 02:20:27,600
and then this will be piped also to our python shell 

20
02:20:29,150 --> 02:20:34,076
while collect copies all the data another action take 

21
02:20:34,076 --> 02:20:37,720
copies the first n results of the driver 

22
02:20:39,690 --> 02:20:43,440
if the results are too large to fit in the driver memory 

23
02:20:43,440 --> 02:20:47,290
then there an opportunity to write them directly to hdfs instead 

24
02:20:49,230 --> 02:20:53,850
among many other actions reduce is probably the most famous one 

25
02:20:54,980 --> 02:20:59,300
reduce takes two elements and returns a result like sum 

26
02:20:59,300 --> 02:21:05,680
but in this case we do not have a key we just have a large area of some values 

27
02:21:05,680 --> 02:21:08,150
and we are running this function over and

28
02:21:08,150 --> 02:21:12,580
over again to reduce everything to one single value 

29
02:21:12,580 --> 02:21:15,210
for example to the global sum of everything 

30
02:21:16,905 --> 02:21:20,945
another very useful action is saveastext 

31
02:21:20,945 --> 02:21:24,585
to save the results to local disk or hdfs and

32
02:21:24,585 --> 02:21:30,485
this is very useful if the output of the power computation is pretty large 

1
04:40:18,200 --> 04:40:22,550
hello i hope you enjoyed your first programming experience with spark 

2
04:40:23,580 --> 04:40:26,890
although the words count example is simple 

3
04:40:26,890 --> 04:40:30,420
it is useful in starting to understand how to work with rdds 

4
04:40:32,040 --> 04:40:38,820
after this video you will be able to use two methods to create rdds in spark 

5
04:40:38,820 --> 04:40:44,500
explain what immutable means interpret a spark program as a pipeline

6
04:40:44,500 --> 04:40:49,860
of transformations and actions and list the steps to create a spark program 

7
04:40:52,390 --> 04:40:53,970
so let remember where we are 

8
04:40:55,540 --> 04:40:59,620
we have a driver program that defines the spark context 

9
04:41:00,700 --> 04:41:04,419
this is the entry point to your application 

10
04:41:04,419 --> 04:41:09,980
the driver converts all the data to rdds and

11
04:41:09,980 --> 04:41:15,720
everything from this point on gets managed using the rdds 

12
04:41:15,720 --> 04:41:19,320
rdds can be constructed from files or any other storage 

13
04:41:20,500 --> 04:41:24,650
they can also be constructed from data structures for collections and

14
04:41:24,650 --> 04:41:26,150
programs like lists 

15
04:41:27,940 --> 04:41:34,306
all the transformations and actions on these rdds take place either locally 

16
04:41:34,306 --> 04:41:38,560
or on the worker nodes managed by a cluster manager 

17
04:41:41,190 --> 04:41:45,560
each transformation results in a new updated version of the rdd 

18
04:41:45,560 --> 04:41:49,480
the rdds at the end get converted and

19
04:41:49,480 --> 04:41:53,319
saved in a persistent storage like hdfs or your local drive 

20
04:41:56,000 --> 04:42:02,480
as we mentioned before rdds get created in the driver program 

21
04:42:02,480 --> 04:42:04,680
the developer of the driver program 

22
04:42:04,680 --> 04:42:08,820
who in this case is you is responsible for creating them 

23
04:42:11,030 --> 04:42:14,650
you can just read in a file through your spark context or

24
04:42:14,650 --> 04:42:20,300
as we have in this example you can provide an existing collection 

25
04:42:20,300 --> 04:42:23,820
like a list to be turned into a distributed collection 

26
04:42:26,400 --> 04:42:31,130
you can also create an integer rdd using parallelize 

27
04:42:32,340 --> 04:42:34,690
and provide a number of partitions for

28
04:42:34,690 --> 04:42:39,000
distribution as we do create the numbers rdd in this line 

29
04:42:41,850 --> 04:42:45,440
here the range function in python

30
04:42:46,460 --> 04:42:49,738
will give us a list of numbers starting from 0 to 9 

31
04:42:49,738 --> 04:42:56,340
the parallelize function will create three partitions

32
04:42:56,340 --> 04:43:01,160
of the rdd to be distributed based on the parameter that was provided to it 

33
04:43:02,440 --> 04:43:07,040
spark will decide how to assign partitions to our executors and worker nodes 

34
04:43:09,110 --> 04:43:14,037
the distributed rdds can in the end be gathered into a single partition on

35
04:43:14,037 --> 04:43:17,286
the driver using the collect transformation 

36
04:43:23,478 --> 04:43:28,225
now let think of a scenario were we start processing the created rdds 

37
04:43:30,060 --> 04:43:35,252
there are two types of operations that help with processing in spark 

38
04:43:35,252 --> 04:43:38,158
namely transformations and actions 

39
04:43:40,829 --> 04:43:45,734
all partitions written in rdd go through the same transformation in

40
04:43:45,734 --> 04:43:50,820
the worker node executors when a transformation is applied to an rdd 

41
04:43:52,420 --> 04:43:56,110
spark uses lazy evaluation for transformations 

42
04:43:57,470 --> 04:44:01,310
that means they will not be immediately executed but

43
04:44:01,310 --> 04:44:03,610
instead wait for an action to be performed 

44
04:44:05,440 --> 04:44:09,890
the transformations get computed when an action is executed 

45
04:44:09,890 --> 04:44:12,810
for this reason a lot of times you will see run

46
04:44:12,810 --> 04:44:17,300
time errors showing up at the action stage and not at the transformation stages 

47
04:44:18,340 --> 04:44:20,840
it is very similar to haskell or erlang 

48
04:44:20,840 --> 04:44:23,120
if any of you are familiar with these languages 

49
04:44:25,730 --> 04:44:28,870
let put some names on these transformations 

50
04:44:28,870 --> 04:44:34,530
we can have a pipeline by converting a text file into an rdd with two partitions 

51
04:44:35,840 --> 04:44:41,440
filter some values out of it and maybe apply a map function to it 

52
04:44:41,440 --> 04:44:46,350
in the end the run the collect action on the mapped rdds

53
04:44:46,350 --> 04:44:50,620
to evaluate the results of the pipeline and convert the outputs into results 

54
04:44:51,680 --> 04:44:57,440
here filter and map are transformations and collect is the action 

55
04:44:59,340 --> 04:45:03,990
although the rdds are in memory and they are not persistent 

56
04:45:03,990 --> 04:45:07,240
we can use the cash function to make them persistent cash 

57
04:45:09,090 --> 04:45:14,684
for example in order to reuse the rdd created from a database query that could

58
04:45:14,684 --> 04:45:19,792
otherwise be costly to re - execute we can instead cache these rdds 

59
04:45:22,461 --> 04:45:25,883
we need to use caution when using the cache option 

60
04:45:25,883 --> 04:45:30,432
as it can consume too much memory and generate a bottleneck itself 

61
04:45:33,713 --> 04:45:41,070
as a part of the word count example we mapped the words rdd to generate tuples 

62
04:45:41,070 --> 04:45:45,350
we then applied reducebykey to tuples to generate counts 

63
04:45:46,470 --> 04:45:50,465
in the end we convert the number of partitions to one so

64
04:45:50,465 --> 04:45:54,120
that output is one file when written to this later 

65
04:45:54,120 --> 04:45:57,980
otherwise output will be spread over multiple files on disk 

66
04:45:59,572 --> 04:46:05,410
finally saveastextfile is an action that kickstarts the computation and

67
04:46:05,410 --> 04:46:06,120
writes to disk 

68
04:46:08,040 --> 04:46:11,700
to summarize in a typical spark program

69
04:46:11,700 --> 04:46:16,400
we create rdds from external storage or local collections like lists 

70
04:46:17,610 --> 04:46:21,130
then we apply transformations to these rdds 

71
04:46:21,130 --> 04:46:25,820
like filter map and reducebykey 

72
04:46:25,820 --> 04:46:30,800
these transformations get lazily evaluated until an action is performed 

73
04:46:31,970 --> 04:46:38,330
actions are performed both for local and parallel computation to generate results 

74
04:46:38,330 --> 04:46:42,780
next we will talk more about transformation and actions in spark 

1
09:26:59,210 --> 09:27:03,330
in the last video we talked about the programming model for

2
09:27:03,330 --> 09:27:08,930
spark where rdd get generated from external datasets and gets partitioned 

3
09:27:10,450 --> 09:27:16,710
we said rdds are immutable meaning they cannot be changed in place even partially 

4
09:27:17,740 --> 09:27:21,690
they need a transformation operation applied to them and

5
09:27:21,690 --> 09:27:23,650
get converted into a new rdd 

6
09:27:25,650 --> 09:27:30,652
this is essential for keeping track of all the processing that has been

7
09:27:30,652 --> 09:27:36,094
applied to our dataset providing the ability to keep a linear chain of rdds 

8
09:27:36,094 --> 09:27:42,360
in addition as a part of a big data pipeline we start with an rdd 

9
09:27:42,360 --> 09:27:46,842
and through several transformation steps many other rdds as

10
09:27:46,842 --> 09:27:51,840
intermediate products get executed until we get to our final result 

11
09:27:53,070 --> 09:27:56,910
we also mention that an important feature of spark

12
09:27:56,910 --> 09:28:00,060
is that all these transformation are lazy 

13
09:28:01,820 --> 09:28:05,820
this means they do not execute immediately when applied to an rdd 

14
09:28:07,040 --> 09:28:11,670
so when we apply a transformation nothing happens right away 

15
09:28:11,670 --> 09:28:15,850
we are basically preparing our big data pipeline to be executed later 

16
09:28:16,990 --> 09:28:21,936
when we are done defining all the transformations and perform an action 

17
09:28:21,936 --> 09:28:26,961
spark will take care of finding the best way to execute this computation and

18
09:28:26,961 --> 09:28:30,729
then start all the necessary tasks in our worker nodes 

19
09:28:30,729 --> 09:28:35,108
in this video we will explain some common transformation in spark 

20
09:28:36,985 --> 09:28:41,537
after this video you will be able to explain the difference between

21
09:28:41,537 --> 09:28:45,001
a narrow transformation and wide transformation 

22
09:28:45,001 --> 09:28:49,748
describe map flatmap filter and coalesce as narrow

23
09:28:49,748 --> 09:28:54,404
transformations and list two wide transformations 

24
09:28:57,240 --> 09:29:02,330
let take at look at probably the simplest transformation which is a map 

25
09:29:04,280 --> 09:29:08,160
by now you are well versed in home networks 

26
09:29:08,160 --> 09:29:14,240
it applies the function to each partition or element of an rdd 

27
09:29:14,240 --> 09:29:17,690
this is a one to one transformation 

28
09:29:17,690 --> 09:29:22,599
it is also in the category of element - wise transformations since it transforms

29
09:29:22,599 --> 09:29:24,847
every element of an rdd separately 

30
09:29:28,805 --> 09:29:32,945
the code example in the blue box here applies a function

31
09:29:32,945 --> 09:29:36,645
called lower to all the elements in a text rdd 

32
09:29:37,900 --> 09:29:38,960
the lower function

33
09:29:40,180 --> 09:29:43,810
turns all the characters in a line to lower case letters 

34
09:29:44,880 --> 09:29:49,650
so the input is one line of text with any kind of

35
09:29:49,650 --> 09:29:54,360
capitalization and the outfit is going to be the same line all lower case 

36
09:29:56,130 --> 09:30:01,160
in this example we have two worker nodes drawn as orange boxes 

37
09:30:02,710 --> 09:30:06,328
the black boxes are partitions of our dataset 

38
09:30:06,328 --> 09:30:09,842
we work by partition and not by element 

39
09:30:09,842 --> 09:30:15,036
as you would remember this it is the difference between spark and mapreduce 

40
09:30:17,488 --> 09:30:22,418
the partition is just a chunk of our data with some number of elements in it and

41
09:30:22,418 --> 09:30:26,964
the map function gets applied to all elements in that partition in each

42
09:30:26,964 --> 09:30:28,440
worker node locally 

43
09:30:29,850 --> 09:30:33,748
each node applies the map function to the data or

44
09:30:33,748 --> 09:30:37,555
rdd partition they received independently 

45
09:30:37,555 --> 09:30:41,560
let look at a few more in element - wise transformation category 

46
09:30:44,251 --> 09:30:47,190
flatmap is very similar to map 

47
09:30:48,290 --> 09:30:53,573
however instead of returning an individual element for each map 

48
09:30:53,573 --> 09:30:59,323
it returns an rdd with an aggregate of all the results for all the elements 

49
09:30:59,323 --> 09:31:01,888
in the example in the blue box 

50
09:31:01,888 --> 09:31:06,132
the split words fuction takes a line as an input 

51
09:31:06,132 --> 09:31:11,970
which is one element and it output is each word as a single element 

52
09:31:11,970 --> 09:31:16,120
so it splits a line to words 

53
09:31:17,760 --> 09:31:19,570
the same thing gets done for each line 

54
09:31:21,180 --> 09:31:24,130
when the output for all the lines is flattened 

55
09:31:24,130 --> 09:31:27,520
we get a simple one - dimensional list of words 

56
09:31:29,220 --> 09:31:34,480
so we will get all the words in all the lines in just one list 

57
09:31:36,070 --> 09:31:41,703
depending on the line length the output partitions might be of different sizes 

58
09:31:41,703 --> 09:31:45,463
detected here by the height of each black box being different 

59
09:31:47,977 --> 09:31:52,750
in spark terms map and flatmap are narrow transformations 

60
09:31:54,040 --> 09:31:59,390
narrow transformation refers to the processing where the processing

61
09:31:59,390 --> 09:32:04,827
logic depends only on data that is already residing in the partition and

62
09:32:04,827 --> 09:32:07,466
data shuffling is not necessary 

63
09:32:10,080 --> 09:32:14,030
another very important transformation is filter 

64
09:32:14,030 --> 09:32:19,957
often we are interested just in a subset of our data or

65
09:32:19,957 --> 09:32:23,118
we want to get rid of bad data 

66
09:32:23,118 --> 09:32:28,105
filter transformation takes the function take executes on each element

67
09:32:28,105 --> 09:32:29,575
of a rdd partition and

68
09:32:29,575 --> 09:32:34,740
returns only the elements that the transformation element returns true 

69
09:32:37,219 --> 09:32:42,281
the example code in the blue box here applies a filter

70
09:32:42,281 --> 09:32:48,014
function that filters out words that start with the letter a 

71
09:32:48,014 --> 09:32:52,130
the function starts with a takes the input word 

72
09:32:52,130 --> 09:32:58,080
then transforms it to lowercase and then checks if the word starts with a 

73
09:33:00,920 --> 09:33:06,640
so the output of this operation will be a list with only words that start with a 

74
09:33:08,870 --> 09:33:10,670
this is another narrow transformation 

75
09:33:12,270 --> 09:33:15,930
so it only gets executed locally

76
09:33:15,930 --> 09:33:20,070
without the need to shuffle any rdd partitions across the word kernels 

77
09:33:22,320 --> 09:33:28,486
the output of filter depends on the input and the filter functions 

78
09:33:28,486 --> 09:33:29,663
in some cases 

79
09:33:29,663 --> 09:33:35,003
even if you started with even rdd partitions within the worker nodes 

80
09:33:35,003 --> 09:33:41,247
the rdd size can significantly vary across the workers after a filter operation 

81
09:33:41,247 --> 09:33:46,588
then this happens is a pretty good idea to join some of those partitions

82
09:33:46,588 --> 09:33:51,772
to increase performance and even out processing across clusters 

83
09:33:51,772 --> 09:33:58,399
this transformation is called coalesce 

84
09:33:58,399 --> 09:34:04,060
coalesce simply helps with balancing the data partition numbers and sizes 

85
09:34:05,220 --> 09:34:10,148
when you have significantly reduced your initial data after some filters and

86
09:34:10,148 --> 09:34:11,817
other transformations 

87
09:34:11,817 --> 09:34:16,311
having a large number of partitions might not be very useful anymore 

88
09:34:16,311 --> 09:34:17,063
in this case 

89
09:34:17,063 --> 09:34:21,521
you can use coalesce to reduce the number of partitions to a more manageable number 

90
09:34:24,421 --> 09:34:29,941
until now we talked about narrow transformations that happen in a worker

91
09:34:29,941 --> 09:34:34,947
node locally without having to transfer data through the network 

92
09:34:34,947 --> 09:34:38,572
now let start talking about wide transformations 

93
09:34:41,048 --> 09:34:43,693
let remember our word count example 

94
09:34:43,693 --> 09:34:50,420
as a part of the word count example we map the words rdd could generate tuples 

95
09:34:50,420 --> 09:34:56,110
the output of map is a key value pair list where the key is the word and

96
09:34:56,110 --> 09:34:57,540
the value is always one 

97
09:34:59,420 --> 09:35:05,390
we then apply it reducebykey to tuples to generate counts 

98
09:35:05,390 --> 09:35:08,360
which simply sums the values for each key or word 

99
09:35:09,670 --> 09:35:16,555
let imagine for a second that we use groupbykey instead of reducebykey 

100
09:35:16,555 --> 09:35:21,453
we will come back to reducebykey in just a little bit 

101
09:35:21,453 --> 09:35:24,774
remember mapped outputs tuples 

102
09:35:24,774 --> 09:35:29,920
which is a list of key value pairs in the forms of word one 

103
09:35:31,670 --> 09:35:37,146
at each worker node we will have tuples that have the same word as key 

104
09:35:37,146 --> 09:35:43,213
in this example apple as the key and 1 as the count and 2 worker nodes 

105
09:35:45,720 --> 09:35:50,367
trying to group together all the counts of a word across worker nodes

106
09:35:50,367 --> 09:35:53,690
requires shuffling of data between these nodes 

107
09:35:54,800 --> 09:35:58,466
just like we do for the word apple here 

108
09:35:58,466 --> 09:36:03,616
groupbykey is the transformation that helps us combine values with

109
09:36:03,616 --> 09:36:09,596
the same key into a list without applying a special user define function to it 

110
09:36:09,596 --> 09:36:15,435
as you see on the right the result of a groupbykey transformation on all

111
09:36:15,435 --> 09:36:21,292
the map outputs by the word apple is the key ends up in a list with all ones 

112
09:36:24,741 --> 09:36:30,109
if you instead apply the function to list like summing up all the values 

113
09:36:30,109 --> 09:36:33,542
then we could have had the word count results 

114
09:36:33,542 --> 09:36:34,715
in this case 2 

115
09:36:34,715 --> 09:36:41,346
if we instead applied a function to the list like summing up all the values 

116
09:36:41,346 --> 09:36:45,468
then we would have had the word count results 

117
09:36:45,468 --> 09:36:49,655
if we need to apply such functions to a group of values related to a key like

118
09:36:49,655 --> 09:36:52,140
this we use the reducebykey operation 

119
09:36:54,220 --> 09:36:59,670
reducebykey helps us to combine the value using a reduce function 

120
09:36:59,670 --> 09:37:02,580
which in the word count case is a simple summation 

121
09:37:04,420 --> 09:37:08,420
in groupbykey and reducebykey transformations 

122
09:37:09,650 --> 09:37:13,860
we observe the behavior that require shuffling of the data across work nodes 

123
09:37:15,350 --> 09:37:18,830
we call such transformations wide transformations 

124
09:37:20,340 --> 09:37:25,440
in wide transformation operations processing depends on data

125
09:37:25,440 --> 09:37:30,940
residing in multiple partitions distributed across worker nodes and this

126
09:37:30,940 --> 09:37:35,430
requires data shuffling over the network to bring related datasets together 

127
09:37:37,220 --> 09:37:42,630
as a summary we have listed a small number of transformations in spark with

128
09:37:42,630 --> 09:37:47,240
some examples and distinguished between them as narrow and wide transformations 

129
09:37:48,520 --> 09:37:53,743
although this is a good start i advise you to go through the list

130
09:37:53,743 --> 09:37:59,760
provided at the link shown here after you complete this beginner course 

131
09:37:59,760 --> 09:38:04,662
read about the rest of the transformations in spark before you start programming in

132
09:38:04,662 --> 09:38:07,192
spark and have fun with transformations 

1
19:05:07,170 --> 19:05:10,580
lastly we will introduce you to spark graphx 

2
19:05:12,970 --> 19:05:18,021
after this video you will be able to describe graphx is 

3
19:05:18,021 --> 19:05:22,970
explain how vertices and edges are stored in graphx and

4
19:05:22,970 --> 19:05:26,702
describe how pregel works at a high level 

5
19:05:29,295 --> 19:05:34,875
graphx is apache spark application programming interface for

6
19:05:34,875 --> 19:05:38,570
graphs and graph - parallel computation 

7
19:05:38,570 --> 19:05:42,001
graphx uses a property graph model 

8
19:05:42,001 --> 19:05:45,420
this means both nodes 

9
19:05:45,420 --> 19:05:49,500
and edges in a graph can have attributes and values 

10
19:05:51,860 --> 19:05:57,955
in graphx the node properties are stored in a vertex table and

11
19:05:57,955 --> 19:06:02,330
edge properties are stored in an edge table 

12
19:06:03,930 --> 19:06:09,429
the connectivity information that is which edge connects which nodes 

13
19:06:09,429 --> 19:06:13,501
is stored separately from the node and edge properties 

14
19:06:16,696 --> 19:06:21,400
graphx is built on special rdds for vertices and edges 

15
19:06:22,890 --> 19:06:28,432
vertexrdd represents a set of vertices 

16
19:06:28,432 --> 19:06:33,823
all of which have an attribute called a 

17
19:06:33,823 --> 19:06:38,865
the edgerdd here extends this basic edge storing by

18
19:06:38,865 --> 19:06:45,326
the edges in columnar format on each partition for performance 

19
19:06:45,326 --> 19:06:51,740
note that vertexid are defined to be unique by design 

20
19:06:52,780 --> 19:06:57,257
the edge class is an object with a source vertex and

21
19:06:57,257 --> 19:07:01,209
destination vertex and an edge attribute 

22
19:07:03,257 --> 19:07:09,511
in addition to the vortex and edge views of the property graph 

23
19:07:09,511 --> 19:07:12,584
graphx also has triplet view 

24
19:07:12,584 --> 19:07:19,350
the triplet view logically joins vortex and edge properties 

25
19:07:22,003 --> 19:07:26,131
graphx has an operator that can execute operations

26
19:07:26,131 --> 19:07:29,880
from the pregel library for graph analytics 

27
19:07:31,420 --> 19:07:37,000
this pregel operator executes in a series of super steps

28
19:07:37,000 --> 19:07:40,700
which defines a messaging protocol for vertices 

29
19:07:42,620 --> 19:07:44,824
we will revisit graph analytics and

30
19:07:44,824 --> 19:07:48,946
using graphx in more detail in course five of the specialization 

31
19:07:50,934 --> 19:07:56,070
in summary spark can be used for graph parallel computations 

32
19:07:57,300 --> 19:08:03,740
graphx uses special rdds for storing vertex and edge information 

33
19:08:05,170 --> 19:08:09,165
and the pregel operator works in a series of super steps 

1
14:13:15,580 --> 14:13:18,435
now we will introduce you to spark mllib 

2
14:13:20,080 --> 14:13:25,020
after this video you will be able to describe what mllib is 

3
14:13:25,020 --> 14:13:29,865
list the main categories of techniques available in mllib 

4
14:13:29,865 --> 14:13:34,530
and explain code segments containing mllib algorithms 

5
14:13:34,530 --> 14:13:40,750
mllib is a scalable machine learning library that runs on top of spark core 

6
14:13:40,750 --> 14:13:45,370
it provides distributed implementations of commonly used machine learning

7
14:13:45,370 --> 14:13:46,750
algorithms and utilities 

8
14:13:48,070 --> 14:13:54,260
as with spark core mllib has apis for scala java python and r 

9
14:13:56,520 --> 14:13:59,120
mllib offers many algorithms and

10
14:13:59,120 --> 14:14:02,170
techniques commonly used in a machine learning process 

11
14:14:03,170 --> 14:14:05,710
the main categories are machine learning 

12
14:14:05,710 --> 14:14:09,860
statistics and some common utility tools for common techniques 

13
14:14:11,120 --> 14:14:12,970
as the name suggests 

14
14:14:12,970 --> 14:14:16,380
many machine learning algorithms are available in mllib 

15
14:14:17,760 --> 14:14:22,440
these are algorithms to build models for classification regression and

16
14:14:22,440 --> 14:14:22,990
clustering 

17
14:14:24,140 --> 14:14:28,110
there are also techniques for evaluating the resulting models 

18
14:14:28,110 --> 14:14:31,740
for example you can compute the values for

19
14:14:31,740 --> 14:14:36,560
a receiver of creating characteristic that we call an roc curve 

20
14:14:36,560 --> 14:14:39,350
a common statistical technique for

21
14:14:39,350 --> 14:14:42,550
plotting the performance of a binary classifier 

22
14:14:44,180 --> 14:14:48,040
statistical functions are also provided in mllib 

23
14:14:48,040 --> 14:14:52,655
examples are summary statistics means standard deviation etc 

24
14:14:53,850 --> 14:14:57,440
correlations and methods to sample a dataset 

25
14:14:59,420 --> 14:15:04,360
mllib also has techniques commonly used in the machine learning process 

26
14:15:04,360 --> 14:15:07,040
such as dimensionality reduction and

27
14:15:07,040 --> 14:15:10,630
feature transformation methods for preprocessing the data 

28
14:15:11,690 --> 14:15:15,550
in short spark mllib offers

29
14:15:15,550 --> 14:15:19,220
many techniques often used in a machine learning pipeline 

30
14:15:21,280 --> 14:15:27,410
let take a look at an example to compute summary statistics using mllib 

31
14:15:27,410 --> 14:15:31,920
note that we will use the spark pipe of api similar to the ones used for

32
14:15:31,920 --> 14:15:33,520
our other examples in this course 

33
14:15:34,980 --> 14:15:39,220
here is the code segment to compute summary statistics for

34
14:15:39,220 --> 14:15:43,180
a data set consisting of columns of numbers 

35
14:15:43,180 --> 14:15:47,370
lines of code are in white and the comments are in orange 

36
14:15:47,370 --> 14:15:52,759
the first line imports statistics functions from the stat module 

37
14:15:52,759 --> 14:15:58,710
the second line creates an rdd of vectors with the data 

38
14:15:58,710 --> 14:16:02,500
you can think of each vector as a column in a data matrix 

39
14:16:03,800 --> 14:16:07,670
the next line denoted with three invokes

40
14:16:07,670 --> 14:16:12,210
the column stats function to compute summary statistics for each column 

41
14:16:13,240 --> 14:16:18,820
the last three lines show by four print out the mean 

42
14:16:18,820 --> 14:16:22,480
variance and number of non - zero entries for each column 

43
14:16:24,070 --> 14:16:28,640
as you can see from this example computing the summary statistics for

44
14:16:28,640 --> 14:16:31,700
a data set is very straightforward using a mllib 

45
14:16:34,410 --> 14:16:36,490
here is another example 

46
14:16:36,490 --> 14:16:40,600
although we will go through the ratio learning details in our next course 

47
14:16:40,600 --> 14:16:44,260
here we give you a hint of how to use two ratio learning techniques 

48
14:16:44,260 --> 14:16:46,920
one for classification and one for clustering 

49
14:16:48,270 --> 14:16:54,130
this code segment shows the six steps to build a decisiontree for classification 

50
14:16:54,130 --> 14:16:57,138
the first line imports the decisiontree module 

51
14:16:57,138 --> 14:17:00,510
the second line imports mlutils module 

52
14:17:02,190 --> 14:17:07,600
the next line fails the decisiontree to classify the data for two classes 

53
14:17:07,600 --> 14:17:12,570
then the model is printed out and finally the model is saved in a file 

54
14:17:15,580 --> 14:17:19,490
here is another mllib example this time for clustering 

55
14:17:19,490 --> 14:17:24,480
this code segment shows the 5 - step code to build a k - means clustering 

56
14:17:26,420 --> 14:17:31,520
the first line imports the k - means module the second line

57
14:17:31,520 --> 14:17:36,040
imports an array module from numpy 

58
14:17:36,040 --> 14:17:41,048
the next two lines read in the data and parses it using space as the limiter 

59
14:17:41,048 --> 14:17:45,100
then the k - means model

60
14:17:45,100 --> 14:17:50,270
is built by dividing the parseddata into three clusters 

61
14:17:50,270 --> 14:17:53,923
finally the cluster centers are printed out for each 

62
14:17:57,259 --> 14:18:01,790
in summary mllib is spark machine learning library 

63
14:18:03,240 --> 14:18:04,710
it provides algorithms and

64
14:18:04,710 --> 14:18:07,880
techniques that are implemented using distributors processing 

65
14:18:08,970 --> 14:18:11,327
the main categories of algorithms and

66
14:18:11,327 --> 14:18:15,968
techniques available in machine learning library are machine learning 

67
14:18:15,968 --> 14:18:20,324
statistics and utility functions for the machine learning process 

1
04:31:36,072 --> 04:31:38,356
over the next couple of videos 

2
04:31:38,356 --> 04:31:43,020
we will introduce you to the basic components of the spark stack 

3
04:31:44,660 --> 04:31:47,440
in this lecture we start with spark sql 

4
04:31:49,000 --> 04:31:54,610
after this video you will be able to process structured data using spark sql

5
04:31:54,610 --> 04:31:59,140
module and explain the numerous benefits of spark sql 

6
04:32:02,240 --> 04:32:08,150
spark sql is the component of spark that enables querying structured and

7
04:32:08,150 --> 04:32:11,510
unstructured data through a common query language 

8
04:32:12,630 --> 04:32:17,720
it can connect to many data sources and provides apis to convert

9
04:32:17,720 --> 04:32:22,430
the query results to rdds in python scala and java programs 

10
04:32:24,670 --> 04:32:27,991
spark sql gives a mechanism for

11
04:32:27,991 --> 04:32:32,467
sql users to deploy sql queries on spark 

12
04:32:35,047 --> 04:32:39,806
spark sql enables business intelligence tools to connect to

13
04:32:39,806 --> 04:32:44,854
spark using standard connection protocols like jdbc and odbc 

14
04:32:47,379 --> 04:32:52,346
spark sql also provides apis to convert the query data into dataframes

15
04:32:52,346 --> 04:32:54,290
to hold distributed data 

16
04:32:55,410 --> 04:33:00,757
dataframes are organized as named columns and basically look like tables 

17
04:33:04,248 --> 04:33:10,320
the first step to run any sql spark is to create a sqlcontext 

18
04:33:12,580 --> 04:33:17,360
once you have an sqlcontext you want to leverage it

19
04:33:17,360 --> 04:33:22,200
to create a dataframe so you can deploy complex operations on the data set 

20
04:33:23,200 --> 04:33:26,480
dataframes can be created from existing rdds 

21
04:33:26,480 --> 04:33:29,270
hive tables or many other data sources 

22
04:33:31,910 --> 04:33:39,020
a file can be read and converted into a dataframe using a single command 

23
04:33:40,610 --> 04:33:48,440
the show function here will display the dataframe in your spark show 

24
04:33:48,440 --> 04:33:52,950
rdds can be converted to dataframes but require a little more work 

25
04:33:53,980 --> 04:33:57,130
first you will have to convert each line into a row 

26
04:33:58,410 --> 04:34:02,640
once your data is in a dataframe you can perform all sorts of

27
04:34:02,640 --> 04:34:07,640
transformation operations on it as shown here including show 

28
04:34:07,640 --> 04:34:11,590
printschema select filter and groupby 

29
04:34:13,850 --> 04:34:18,910
to summarize spark sql lets you run relational queries on spark 

30
04:34:20,500 --> 04:34:25,110
it also lets you connect to a variety of databases and

31
04:34:25,110 --> 04:34:28,750
deploy business intelligence tools over spark 

32
04:34:28,750 --> 04:34:32,707
we will go through some of this functionality in one of the readings and

33
04:34:32,707 --> 04:34:34,795
in the upcoming hands - on session 

1
09:06:09,630 --> 09:06:13,040
next we will talk about spark streaming 

2
09:06:14,650 --> 09:06:20,474
after this video you will be able to summarize how spark reads streaming data 

3
09:06:20,474 --> 09:06:25,240
list several sources of streaming data supported by spark and

4
09:06:25,240 --> 09:06:28,077
describe spark sliding windows 

5
09:06:28,077 --> 09:06:33,778
spark streaming provides scalable processing for

6
09:06:33,778 --> 09:06:38,954
real - time data and runs on top of spark core 

7
09:06:40,816 --> 09:06:44,319
continuous data streams are converted or

8
09:06:44,319 --> 09:06:50,130
grouped into discrete rdds which can then be processed in parallel 

9
09:06:51,620 --> 09:06:55,357
spark streaming provides apis for scala 

10
09:06:55,357 --> 09:06:59,620
java and python like other spark products 

11
09:07:03,370 --> 09:07:08,820
spark streaming can read data from many different

12
09:07:08,820 --> 09:07:14,278
types of resources including kafka and flume 

13
09:07:14,278 --> 09:07:20,778
kafka is a high throughput published subscribed messaging system 

14
09:07:20,778 --> 09:07:25,044
and flume collects and aggregates log data 

15
09:07:25,044 --> 09:07:30,125
spark streaming can also read from batch

16
09:07:30,125 --> 09:07:34,609
input data sources such as hdfs 

17
09:07:34,609 --> 09:07:39,720
s3 and many other non sql databases 

18
09:07:39,720 --> 09:07:47,368
additionally spark streaming can read directly from twitter raw tcp sockets 

19
09:07:47,368 --> 09:07:53,306
and many other data sources that are real - time data providers 

20
09:07:56,358 --> 09:07:57,280
so how does it all work 

21
09:07:58,320 --> 09:08:03,910
here we show you a flow of transformations and actions

22
09:08:03,910 --> 09:08:09,131
which you will try in the upcoming reading and hands - on exercises on spark streaming 

23
09:08:09,131 --> 09:08:13,102
spark streaming reads

24
09:08:13,102 --> 09:08:18,760
streaming data and converts it into micro batches

25
09:08:18,760 --> 09:08:24,350
which we call dstreams which is short for discretized stream 

26
09:08:27,378 --> 09:08:32,736
in this example a 10 second stream gets converted

27
09:08:32,736 --> 09:08:37,980
into five rdds using a batch length of 2 seconds 

28
09:08:39,800 --> 09:08:44,920
similar to other rdds transformations such as map reduce 

29
09:08:44,920 --> 09:08:47,515
and filter can be applied to dstreams 

30
09:08:47,515 --> 09:08:53,720
dstreams can be aggregated

31
09:08:53,720 --> 09:09:00,310
into windows allowing you to apply computations on sliding window of data 

32
09:09:01,860 --> 09:09:06,165
in this example the window size is 4 seconds and

33
09:09:06,165 --> 09:09:09,510
the sliding interval is 2 seconds 

34
09:09:12,826 --> 09:09:17,594
in summary spark streaming is spark library to work with

35
09:09:17,594 --> 09:09:20,260
streaming data in near real time 

36
09:09:22,375 --> 09:09:27,210
dstreams can be used just like any other rdd and

37
09:09:27,210 --> 09:09:30,940
can go through the same transformation as batch datasets 

38
09:09:32,560 --> 09:09:39,907
dstreams can create a sliding window to perform calculations on a window of time 

1
18:15:49,012 --> 18:15:52,776
now that we know what machine learning is and have seen some examples of it 

2
18:15:52,776 --> 18:15:56,010
let talk about how we do machine learning 

3
18:15:56,010 --> 18:16:00,015
in this lecture we will get an overview of the main categories of machine learning

4
18:16:00,015 --> 18:16:00,625
techniques 

5
18:16:02,265 --> 18:16:05,315
after this video you will be able to

6
18:16:05,315 --> 18:16:08,805
describe the main categories of machine learning techniques and

7
18:16:08,805 --> 18:16:12,469
summarize how supervised learning differs from unsupervised learning 

8
18:16:14,710 --> 18:16:17,260
there are different categories of machine learning techniques for

9
18:16:17,260 --> 18:16:19,150
different types of problems 

10
18:16:19,150 --> 18:16:21,240
the main categories are listed here 

11
18:16:21,240 --> 18:16:25,630
they are classification regression cluster analysis and

12
18:16:25,630 --> 18:16:27,450
association analysis 

13
18:16:27,450 --> 18:16:30,000
we will cover each one in detail in the following slides 

14
18:16:31,730 --> 18:16:36,830
in classification the goal is to predict the category of the input data 

15
18:16:36,830 --> 18:16:41,840
an example of this is predicting the weather as being sunny rainy windy or

16
18:16:41,840 --> 18:16:43,080
cloudy 

17
18:16:43,080 --> 18:16:47,260
the input data in this case would be sensor data specifying the temperature 

18
18:16:47,260 --> 18:16:52,840
relative humidity atmospheric pressure wind speed wind direction etc 

19
18:16:52,840 --> 18:16:57,502
the target or what you are trying to predict would be the different weather

20
18:16:57,502 --> 18:17:01,670
categories sunny windy rainy and cloudy 

21
18:17:01,670 --> 18:17:06,580
another example is to classify a tumor as either benign or malignant 

22
18:17:06,580 --> 18:17:09,460
in this case the classification is referred to as

23
18:17:09,460 --> 18:17:13,040
binary classification since there are only two categories 

24
18:17:13,040 --> 18:17:15,400
but you can have many categories as well 

25
18:17:15,400 --> 18:17:17,170
as the weather prediction problem shown here 

26
18:17:18,390 --> 18:17:22,080
another example is to identify hand written digits as being in one of ten

27
18:17:22,080 --> 18:17:23,950
categories zero to nine 

28
18:17:25,970 --> 18:17:30,280
some more examples of classification are classifying a tumor from a medical

29
18:17:30,280 --> 18:17:32,710
image as being benign or malignant 

30
18:17:32,710 --> 18:17:35,430
predicting whether it will rain the next day 

31
18:17:35,430 --> 18:17:40,410
determining if a loan application is high - risk medium - risk or low - risk 

32
18:17:40,410 --> 18:17:42,860
identifying the sentiment of a tweet or

33
18:17:42,860 --> 18:17:45,990
review as being positive negative or neutral 

34
18:17:47,930 --> 18:17:51,870
when your model has to predict a numeric value instead of a category 

35
18:17:51,870 --> 18:17:54,380
then the task becomes a regression problem 

36
18:17:56,040 --> 18:17:59,920
an example of regression is to predict the price of a stock 

37
18:17:59,920 --> 18:18:03,320
the stock price is a numeric value not a category 

38
18:18:03,320 --> 18:18:06,700
so this is a regression task instead of a classification task 

39
18:18:07,710 --> 18:18:10,480
if you were to predict whether the stock price will rise or

40
18:18:10,480 --> 18:18:13,620
fall then that would be a classification problem 

41
18:18:13,620 --> 18:18:16,340
but if you are predicting the actual price of the stock 

42
18:18:16,340 --> 18:18:17,620
then that is a regression problem 

43
18:18:18,740 --> 18:18:22,340
that is the main difference between classification and regression 

44
18:18:22,340 --> 18:18:25,250
in classification you are predicting a category and

45
18:18:25,250 --> 18:18:27,520
in regression you are predicting a numeric value 

46
18:18:29,630 --> 18:18:34,000
some other examples of regression are estimating the demand of a product

47
18:18:34,000 --> 18:18:36,570
based on time or season of the year 

48
18:18:36,570 --> 18:18:39,130
predicting a score on a test 

49
18:18:39,130 --> 18:18:42,210
determining the likelihood of how effective a drug will be for

50
18:18:42,210 --> 18:18:47,270
a particular patient predicting the amount of rain for a region 

51
18:18:47,270 --> 18:18:48,550
in cluster analysis 

52
18:18:48,550 --> 18:18:53,275
the goal is to organize similar items in your data set into groups 

53
18:18:53,275 --> 18:18:57,120
a very common application of cluster analysis is referred to as

54
18:18:57,120 --> 18:18:59,160
customer segmentation 

55
18:18:59,160 --> 18:19:03,227
this means that you are separating your customer base into different groups or

56
18:19:03,227 --> 18:19:05,054
segments based on customer types 

57
18:19:05,054 --> 18:19:09,833
for example it would be very beneficial to segment your customers into seniors 

58
18:19:09,833 --> 18:19:12,230
adults and teenagers 

59
18:19:12,230 --> 18:19:14,544
these groups have different likes and dislikes and

60
18:19:14,544 --> 18:19:16,383
have different purchasing behaviors 

61
18:19:16,383 --> 18:19:20,569
by segmenting your customers to different groups you can more effectively provide

62
18:19:20,569 --> 18:19:24,652
marketing adds targeted for each groups particular interests 

63
18:19:24,652 --> 18:19:27,750
note that cluster analysis is also referred to as clustering 

64
18:19:29,860 --> 18:19:33,810
some other examples of cluster analysis are identifying areas of

65
18:19:33,810 --> 18:19:38,840
similar topography such as desert region grassy areas mountains etc 

66
18:19:38,840 --> 18:19:42,610
categorizing different types of tissues from medical images 

67
18:19:42,610 --> 18:19:47,080
determining different groups of weather patterns such as snowy dry monsoon 

68
18:19:48,160 --> 18:19:51,810
and discovering hot spots for different types of crime from police reports 

69
18:19:53,960 --> 18:19:57,770
the goal in association analysis is to come up with a set of rules to

70
18:19:57,770 --> 18:20:01,230
capture associations between items or events 

71
18:20:01,230 --> 18:20:05,520
the rules are used to determine when items or events occur together 

72
18:20:05,520 --> 18:20:09,500
a common application of association analysis is known as market

73
18:20:09,500 --> 18:20:11,300
basket analysis 

74
18:20:11,300 --> 18:20:15,020
which is used to understand customer purchasing behavior 

75
18:20:15,020 --> 18:20:19,000
for example association analysis can reveal that banking customers who have

76
18:20:19,000 --> 18:20:23,330
cds or certificates of deposits also tend to be interested in

77
18:20:23,330 --> 18:20:26,970
other investment vehicles such as money market accounts 

78
18:20:26,970 --> 18:20:29,840
this information can be used for cross selling 

79
18:20:29,840 --> 18:20:34,020
if you advertise money market accounts to your customers with cds they are likely to

80
18:20:34,020 --> 18:20:35,390
open such an account 

81
18:20:37,750 --> 18:20:41,700
according to data mining folklore a supermarket chain used association

82
18:20:41,700 --> 18:20:46,410
analysis to discover a connection between two seemingly unrelated products 

83
18:20:46,410 --> 18:20:49,130
they discovered that many customers who go to the store

84
18:20:49,130 --> 18:20:53,150
late on sunday night to buy diapers also tend to buy beer 

85
18:20:53,150 --> 18:20:55,470
this information was then used to place beer and

86
18:20:55,470 --> 18:20:59,620
diapers close together and they saw a jump in sales of both items 

87
18:20:59,620 --> 18:21:01,630
this is the famous diaper beer connection 

88
18:21:03,920 --> 18:21:08,600
some other applications of association analysis are recommending similar

89
18:21:08,600 --> 18:21:13,180
items based on the purchasing behavior or browsing histories of customers 

90
18:21:14,180 --> 18:21:17,720
finding items that are often purchased together such as garden hose and

91
18:21:17,720 --> 18:21:19,080
potting soil and

92
18:21:19,080 --> 18:21:22,910
offer sales on these related items at the same time to drive sales of both items 

93
18:21:23,980 --> 18:21:26,970
identifying web pages that are often accessed together so

94
18:21:26,970 --> 18:21:31,070
that you can more efficiently offer up these related web pages at the same time 

95
18:21:32,944 --> 18:21:36,890
we have now looked at the different categories of machine learning techniques 

96
18:21:36,890 --> 18:21:41,240
they are classification regression cluster analysis and

97
18:21:41,240 --> 18:21:42,280
association analysis 

98
18:21:42,280 --> 18:21:45,753
we have also seen some examples of each category

99
18:21:47,941 --> 18:21:51,764
there is also another categorization of machine learning techniques and

100
18:21:51,764 --> 18:21:54,520
that is supervised versus unsupervised approaches 

101
18:21:55,840 --> 18:21:57,900
in supervised approaches the target 

102
18:21:57,900 --> 18:22:00,830
which is what the model is predicting is provided 

103
18:22:00,830 --> 18:22:05,750
this is referred to as having labeled data because the target is labeled for

104
18:22:05,750 --> 18:22:07,457
every sample that you have in your data set 

105
18:22:08,550 --> 18:22:12,520
referring back to our example of predicting a weather category of sunny 

106
18:22:12,520 --> 18:22:14,940
windy rainy or cloudy 

107
18:22:14,940 --> 18:22:19,550
every sample in the data set is labeled as being one of these four categories 

108
18:22:19,550 --> 18:22:24,910
so the data is labeled and predicting the weather categories is a supervised task 

109
18:22:24,910 --> 18:22:28,740
in general classification and regression are supervised approaches 

110
18:22:30,680 --> 18:22:33,200
in unsupervised approaches on the other hand 

111
18:22:33,200 --> 18:22:37,440
the target that the model is predicting is unknown or unavailable 

112
18:22:37,440 --> 18:22:40,320
this means that you have unlabeled data 

113
18:22:40,320 --> 18:22:44,130
going back to our cluster analysis example of segmenting customers into

114
18:22:44,130 --> 18:22:45,400
different groups 

115
18:22:45,400 --> 18:22:49,180
the samples in your data are not labeled with the correct group 

116
18:22:49,180 --> 18:22:53,250
instead the segmentation is performed using a clustering technique to group

117
18:22:53,250 --> 18:22:56,660
items based on characteristics that they have in common 

118
18:22:56,660 --> 18:22:58,560
thus the data is unlabeled and

119
18:22:58,560 --> 18:23:03,630
the task of grouping customers into different segments is an unsupervised one 

120
18:23:03,630 --> 18:23:05,606
in general cluster analysis and

121
18:23:05,606 --> 18:23:08,751
association analysis are unsupervised approaches 

122
18:23:10,962 --> 18:23:11,534
in summary 

123
18:23:11,534 --> 18:23:16,000
in this lecture we looked at the different categories of machine learning techniques 

124
18:23:16,000 --> 18:23:22,210
we discussed classification regression cluster analysis and association analysis 

125
18:23:22,210 --> 18:23:24,370
we also defined what supervised and

126
18:23:24,370 --> 18:23:26,590
unsupervised approaches are in machine learning 

1
12:39:14,490 --> 12:39:17,360
in this lecture we will review each of the steps in

2
12:39:17,360 --> 12:39:19,440
the machine learning process in greater detail 

3
12:39:20,930 --> 12:39:25,360
after this video you will be able to explain the goals of each step in

4
12:39:25,360 --> 12:39:30,989
the machine learning process and list key activities in each step in the process 

5
12:39:32,910 --> 12:39:36,840
this is the machine learning process that we saw in the last lecture 

6
12:39:36,840 --> 12:39:40,560
in this lecture we will cover each step in more detail 

7
12:39:40,560 --> 12:39:42,640
we will describe the goals of each step and

8
12:39:42,640 --> 12:39:44,600
the key activities performed in each step 

9
12:39:46,730 --> 12:39:50,870
the first step in the data science process is to acquire the data 

10
12:39:50,870 --> 12:39:52,910
the goal of the step is to identify and

11
12:39:52,910 --> 12:39:55,270
obtain all data related to the problem at hand 

12
12:39:56,980 --> 12:40:00,800
first we need to identify all related data and the sources 

13
12:40:00,800 --> 12:40:05,330
keep in mind that data can come from different sources such as files 

14
12:40:05,330 --> 12:40:08,940
databases the internet mobile devices 

15
12:40:08,940 --> 12:40:12,290
so remember to include all data related to the problem you are addressing 

16
12:40:13,520 --> 12:40:16,520
after you have identified your data and data sources 

17
12:40:16,520 --> 12:40:21,360
the next step is to collect the data and integrate data from the different sources 

18
12:40:21,360 --> 12:40:25,570
this may require conversion as data can come in different formats 

19
12:40:25,570 --> 12:40:28,246
and it may also require aligning the data 

20
12:40:28,246 --> 12:40:32,520
as data from different sources may have different time or spacial resolutions 

21
12:40:34,660 --> 12:40:39,410
once you ve collected and integrated your data you now have a coherent data set for

22
12:40:39,410 --> 12:40:40,050
your analysis 

23
12:40:41,920 --> 12:40:46,400
the next step after acquiring data is to prepare it to make it suitable for

24
12:40:46,400 --> 12:40:47,690
analysis 

25
12:40:47,690 --> 12:40:52,290
there are two parts to this step explore data and preprocess data 

26
12:40:52,290 --> 12:40:54,410
we will discuss data exploration first 

27
12:40:56,040 --> 12:41:00,670
in data exploration you want to do some preliminary investigation

28
12:41:00,670 --> 12:41:05,680
in order to gain a better understanding of the specific characteristics of your data 

29
12:41:05,680 --> 12:41:08,779
this in turn will guide the rest of the process 

30
12:41:08,779 --> 12:41:11,640
with data exploration you will want to look for

31
12:41:11,640 --> 12:41:16,250
things like correlations general trends outliers etc 

32
12:41:18,080 --> 12:41:20,930
correlations provide information about the relationship

33
12:41:20,930 --> 12:41:22,280
between variables in your data 

34
12:41:23,990 --> 12:41:28,710
trends in your data will reveal if the variable is moving in a certain direction 

35
12:41:28,710 --> 12:41:31,480
such as transaction volume increasing throughout the year 

36
12:41:33,120 --> 12:41:36,660
outliers indicate potential problems with the data or

37
12:41:36,660 --> 12:41:40,850
may indicate an interesting data point that needs further examination 

38
12:41:40,850 --> 12:41:42,680
without this data exploration activity 

39
12:41:42,680 --> 12:41:45,370
you will not be able to use your data effectively 

40
12:41:47,350 --> 12:41:51,680
one way to explore your data is to calculate summary statistics

41
12:41:51,680 --> 12:41:53,290
to numerically describe the data 

42
12:41:55,180 --> 12:41:59,570
summary statistics are quantities that capture various characteristics

43
12:41:59,570 --> 12:42:03,390
of a set of values with a single number or a small set of numbers 

44
12:42:03,390 --> 12:42:08,400
some basic summary statistics that you should compute for your data set are mean 

45
12:42:08,400 --> 12:42:13,360
median mode range and standard deviation 

46
12:42:13,360 --> 12:42:16,660
mean and median are measures of the location of a set of values 

47
12:42:16,660 --> 12:42:20,440
mode is the value that occurs most frequently in your data set and

48
12:42:20,440 --> 12:42:24,630
range and standard deviation are measures of spread in your data 

49
12:42:24,630 --> 12:42:29,060
looking at these measures will give you an idea of the nature of your data 

50
12:42:29,060 --> 12:42:32,130
they can tell you if there something wrong with your data 

51
12:42:32,130 --> 12:42:36,100
for example if the range of the values for age in your data

52
12:42:36,100 --> 12:42:40,020
includes negative numbers or a number much greater than a hundred 

53
12:42:40,020 --> 12:42:42,830
there something suspicious in the data that needs to be examined 

54
12:42:44,760 --> 12:42:47,230
visualization techniques also provide quick and

55
12:42:47,230 --> 12:42:49,570
effective ways to explore your data 

56
12:42:49,570 --> 12:42:52,760
some examples are a histogram 

57
12:42:52,760 --> 12:42:56,570
such as the plot shown here shows the distribution of the data and

58
12:42:56,570 --> 12:43:00,269
can show skewness or unusual dispersion in outliers 

59
12:43:01,960 --> 12:43:06,380
a line plot like the one in the lower left can be used to look at trends in

60
12:43:06,380 --> 12:43:09,080
the data such as the change in the price of a stock 

61
12:43:10,750 --> 12:43:13,816
a heat map can give you an idea of where the hot spots are 

62
12:43:15,485 --> 12:43:20,315
a scatter plot effectively shows correlation between two variables 

63
12:43:20,315 --> 12:43:23,585
overall there are many types of plots to visualize data 

64
12:43:23,585 --> 12:43:26,525
they are very useful in helping you understand the data you have 

65
12:43:28,745 --> 12:43:32,155
the second part of the prepare step is preprocess 

66
12:43:32,155 --> 12:43:36,755
so after we have explored the data we need to preprocess the data to prepare it for

67
12:43:36,755 --> 12:43:37,405
analysis 

68
12:43:39,150 --> 12:43:42,950
the goal here is to create the data that will be used for analysis 

69
12:43:42,950 --> 12:43:46,280
the main activities on this part are to clean the data 

70
12:43:46,280 --> 12:43:50,070
select the appropriate variables to use and transform the data as needed 

71
12:43:51,855 --> 12:43:54,375
a very important part of data preparation is to

72
12:43:54,375 --> 12:43:56,975
clean the data to address quality issues 

73
12:43:56,975 --> 12:43:58,875
real world data is nothing 

74
12:43:58,875 --> 12:44:04,275
there are many examples of quality issues with data from real applications including

75
12:44:04,275 --> 12:44:09,065
missing values such as income in a survey duplicate data 

76
12:44:09,065 --> 12:44:12,615
such as two different records for the same customer with different addresses 

77
12:44:14,020 --> 12:44:17,670
inconsistent or invalid data such as a six digit zip code 

78
12:44:19,130 --> 12:44:22,110
noise in the collection of data that distorts the true values 

79
12:44:23,915 --> 12:44:28,180
outliers such as a number much larger than 100 for someone age 

80
12:44:28,180 --> 12:44:29,700
it is essential to detect and

81
12:44:29,700 --> 12:44:33,230
address these issues that can negatively affect the quality of the data 

82
12:44:35,740 --> 12:44:39,620
other activities in data preprocessing can be broadly categorized as

83
12:44:39,620 --> 12:44:43,140
feature selection and feature transformation 

84
12:44:43,140 --> 12:44:46,410
feature selection refers to choosing the set of features to

85
12:44:46,410 --> 12:44:49,500
use that is appropriate for the application 

86
12:44:49,500 --> 12:44:52,820
feature selection can involve removing redundant or

87
12:44:52,820 --> 12:44:57,420
irrelevant features combining features or creating new features 

88
12:44:58,610 --> 12:45:00,520
during the exploring data step 

89
12:45:00,520 --> 12:45:03,820
you may have discovered that two features are very correlated 

90
12:45:03,820 --> 12:45:06,460
in that case one of these features can be removed

91
12:45:06,460 --> 12:45:09,630
without negatively effecting the analysis results 

92
12:45:09,630 --> 12:45:12,110
for example the purchase price of a product and

93
12:45:12,110 --> 12:45:15,830
the amount of sales tax pain are very likely to be correlated 

94
12:45:15,830 --> 12:45:19,860
eliminating the sales tax amount then will be beneficial 

95
12:45:19,860 --> 12:45:24,330
removing redundant or irrelevant features will make the subsequent analysis simpler 

96
12:45:25,490 --> 12:45:28,870
you may also want to combine features or create new ones 

97
12:45:28,870 --> 12:45:31,580
for example adding the applicants education level

98
12:45:31,580 --> 12:45:34,449
as a feature to a loan approval application would make sense 

99
12:45:35,450 --> 12:45:38,700
there are also algorithms to automatically determine the most relevant

100
12:45:38,700 --> 12:45:40,940
features based on various mathematical properties 

101
12:45:43,280 --> 12:45:47,510
feature transformation maps the data from one format to another 

102
12:45:47,510 --> 12:45:49,805
various transformation operations exist 

103
12:45:49,805 --> 12:45:54,470
for example scaling maps the data values to a specified

104
12:45:54,470 --> 12:45:58,630
range to prevent any one feature from dominating the analysis results 

105
12:46:00,280 --> 12:46:04,900
filtering or aggregation can be used to reduce noise and variability in the data 

106
12:46:06,700 --> 12:46:11,160
dimensionality reduction maps the data to a smaller subset of dimensions

107
12:46:11,160 --> 12:46:14,020
to simplify the subsequent analysis 

108
12:46:14,020 --> 12:46:18,497
we will discuss techniques to prepare data in more detail later in this course 

109
12:46:20,000 --> 12:46:23,237
after preparing the data to address data quality issues and

110
12:46:23,237 --> 12:46:26,020
preprocess it to get it in the appropriate format 

111
12:46:26,020 --> 12:46:30,680
the next step in the machine learning process is to analyze the data 

112
12:46:30,680 --> 12:46:33,840
the goals of the staff are to build a machine learning model 

113
12:46:33,840 --> 12:46:37,960
to analyze the data and to evaluate the results that you get from the model 

114
12:46:40,140 --> 12:46:45,110
the analyze steps starts with this determining the type of problem you have 

115
12:46:45,110 --> 12:46:48,990
you begin by selecting appropriate machine learning techniques to analyze the data 

116
12:46:50,760 --> 12:46:53,350
then you construct the model using the data that you have prepared 

117
12:46:54,940 --> 12:46:59,220
once the model is built you will want to apply it to new data samples

118
12:46:59,220 --> 12:47:01,450
to evaluate how well the model performs 

119
12:47:02,800 --> 12:47:06,125
thus data analysis involves selecting the appropriate technique for

120
12:47:06,125 --> 12:47:10,375
your problem building the model then evaluating the results 

121
12:47:10,375 --> 12:47:11,855
as there are a different types of problems 

122
12:47:11,855 --> 12:47:14,855
there are also different types of analysis techniques 

123
12:47:14,855 --> 12:47:16,097
we will cover algorithms for

124
12:47:16,097 --> 12:47:19,785
building machine learning models in a much more detail in week three of this course 

125
12:47:21,890 --> 12:47:25,260
the next step in the machine learning process is reporting results from your

126
12:47:25,260 --> 12:47:25,865
analysis 

127
12:47:25,865 --> 12:47:30,020
in reporting your results it is important to communicate your insights and

128
12:47:30,020 --> 12:47:32,170
make a case for what actions should follow 

129
12:47:34,215 --> 12:47:35,545
in reporting your results 

130
12:47:35,545 --> 12:47:39,945
you will want to think about what to present as well as how to present 

131
12:47:39,945 --> 12:47:43,912
in deciding what to present you should consider what the main results are 

132
12:47:43,912 --> 12:47:46,537
what insights were gained from your analysis and

133
12:47:46,537 --> 12:47:49,725
what added value do these insights bring to the application 

134
12:47:50,725 --> 12:47:54,517
keep in mind that even negative results are valuable lessons learned and

135
12:47:54,517 --> 12:47:57,730
suggest further avenues for additional analysis 

136
12:47:57,730 --> 12:48:00,150
remember that all findings must be presented so

137
12:48:00,150 --> 12:48:02,840
that informs decisions can be made for next steps 

138
12:48:03,910 --> 12:48:05,780
in deciding how to present 

139
12:48:05,780 --> 12:48:09,460
remember that visualization is an important tool in presenting your results 

140
12:48:11,055 --> 12:48:14,285
plots and summary statistics discussing the explore step

141
12:48:14,285 --> 12:48:16,625
can be used effectively here as well 

142
12:48:16,625 --> 12:48:20,095
you should also have tables with details from your analysis as backup 

143
12:48:20,095 --> 12:48:22,695
if someone wants to take a deeper dive into the results 

144
12:48:23,915 --> 12:48:27,785
in summary you want to report your findings by presenting your results and

145
12:48:27,785 --> 12:48:30,815
the value added with graphs using visualization tools 

146
12:48:33,160 --> 12:48:35,710
the final step in the machine loading process

147
12:48:35,710 --> 12:48:39,600
is to determine what action should be taken based on the insights gained 

148
12:48:41,370 --> 12:48:44,930
what action should be taken based on the results of your analysis 

149
12:48:44,930 --> 12:48:48,110
should you market certain products to a specific customer segment to

150
12:48:48,110 --> 12:48:49,630
increase sales 

151
12:48:49,630 --> 12:48:52,730
what inefficiency is can be removed from your process 

152
12:48:52,730 --> 12:48:55,580
what incentives would be effective in attracting new customers 

153
12:48:57,450 --> 12:49:00,180
once a specific action has been determined 

154
12:49:00,180 --> 12:49:02,090
the next step is to implement the action 

155
12:49:03,100 --> 12:49:08,320
things to consider here include how can the action be added to your application 

156
12:49:08,320 --> 12:49:10,020
how will end users be affected 

157
12:49:11,950 --> 12:49:16,030
assessing the impact of the implemented action is then necessary to evaluate

158
12:49:16,030 --> 12:49:17,810
the benefits gained 

159
12:49:17,810 --> 12:49:20,820
the results of this assessment determine next steps 

160
12:49:20,820 --> 12:49:24,960
which could suggest additional analysis or further opportunities 

161
12:49:24,960 --> 12:49:28,049
which would begin another cycle of the machine learning process 

162
12:49:30,100 --> 12:49:33,960
in summary we have looked at the steps in the machine learning process in detail 

163
12:49:35,130 --> 12:49:39,470
the goals or main activities of each step were discussed in this lecture 

164
12:49:39,470 --> 12:49:42,570
remember that this is an iterative process 

165
12:49:42,570 --> 12:49:46,450
in which each step can be repeated more than once and findings from

166
12:49:46,450 --> 12:49:50,750
each step may require a previous step to be repeated with new information

1
01:29:04,010 --> 01:29:06,490
hello everyone you have probably heard of machine

2
01:29:06,490 --> 01:29:10,250
learning before enough to grab your attention and bring you to this class 

3
01:29:10,250 --> 01:29:13,250
however you might be wondering what machine learning is 

4
01:29:13,250 --> 01:29:14,920
we will talk about that in this lecture and

5
01:29:14,920 --> 01:29:18,119
discuss how we can see machine learning in action in our day - to - day life 

6
01:29:20,280 --> 01:29:24,890
after this video you will be able to explain what machine learning is 

7
01:29:24,890 --> 01:29:28,440
and list three applications of machine learning encountered in everyday life 

8
01:29:30,690 --> 01:29:35,170
since this course is about machine learning lets define what that means 

9
01:29:35,170 --> 01:29:38,470
we hear this term a lot these days used in many contexts 

10
01:29:38,470 --> 01:29:42,240
so it good to start out with a solid definition of what machine learning means 

11
01:29:43,800 --> 01:29:48,200
machine learning is the field of study that focuses on computer systems that can

12
01:29:48,200 --> 01:29:49,850
learn from data 

13
01:29:49,850 --> 01:29:54,360
that is the system often called models can learn to perform a specific

14
01:29:54,360 --> 01:29:58,600
task by analyzing lots of examples for a particular problem 

15
01:29:58,600 --> 01:30:02,550
for example a machine learning model can learn to recognize

16
01:30:02,550 --> 01:30:06,600
an image of a cat by being shown lots and lots of images of cats 

17
01:30:08,010 --> 01:30:10,710
this notion of learning from data means that a machine learning

18
01:30:10,710 --> 01:30:15,960
model can learn a specific task without being explicitly programmed 

19
01:30:15,960 --> 01:30:19,400
in other words the machine learning model is not given the step

20
01:30:19,400 --> 01:30:22,610
by step instructions on how to recognize the image of a cat 

21
01:30:23,970 --> 01:30:26,910
instead the model learns what features are important

22
01:30:26,910 --> 01:30:32,030
in determining whether it picture contains a cat from the data that has analyzed 

23
01:30:32,030 --> 01:30:35,120
because the model learns to perform this task from data

24
01:30:35,120 --> 01:30:39,250
it good to know that the amount and quality of data available for

25
01:30:39,250 --> 01:30:43,110
building the model are important factors in how well the model learns the task 

26
01:30:44,520 --> 01:30:47,110
because machine learning models can learn from data

27
01:30:47,110 --> 01:30:50,420
that can be used to discover hidden patterns and trends in the data 

28
01:30:52,160 --> 01:30:56,235
these patterns and trends lead to valuable insights into the data 

29
01:30:56,235 --> 01:31:00,265
thus the use of machine learning allows for data driven decisions to be made for

30
01:31:00,265 --> 01:31:02,095
a particular problem 

31
01:31:02,095 --> 01:31:05,745
so to summarize the field of machine learning focuses on the study and

32
01:31:05,745 --> 01:31:09,185
construction of computer systems that can learn from data

33
01:31:09,185 --> 01:31:11,775
without being explicitly programmed 

34
01:31:11,775 --> 01:31:13,025
machine learning algorithms and

35
01:31:13,025 --> 01:31:16,355
techniques are used to build models to discover hidden patterns and

36
01:31:16,355 --> 01:31:19,980
trends in the data allowing for data - driven decisions to be made 

37
01:31:22,180 --> 01:31:25,840
you may have heard that machine learning is an inter - disciplinary field 

38
01:31:25,840 --> 01:31:27,410
this is very true 

39
01:31:27,410 --> 01:31:31,740
machine learning combines concepts and methods from many disciplines including

40
01:31:31,740 --> 01:31:36,830
math statistics computer science artificial intelligence and optimization 

41
01:31:37,930 --> 01:31:39,940
in applying machine learning to a problem 

42
01:31:39,940 --> 01:31:44,060
domain knowledge is essential to the success of end results 

43
01:31:44,060 --> 01:31:47,520
by domain knowledge we mean an understanding of the application or

44
01:31:47,520 --> 01:31:48,150
business domain 

45
01:31:49,160 --> 01:31:53,070
knowledge about the application the data related to the application and

46
01:31:53,070 --> 01:31:56,860
how the outcomes will be used are crucial to driving the process of building

47
01:31:56,860 --> 01:31:58,540
the machine learning model 

48
01:31:58,540 --> 01:32:02,593
so domain knowledge is also an integral part of a machine learning solution 

49
01:32:04,278 --> 01:32:07,713
machine learning has been used in many different learning application many of

50
01:32:07,713 --> 01:32:11,410
which you will probably encounter in your daily life perhaps without realizing it 

51
01:32:13,430 --> 01:32:17,030
one application of machine learning that you likely used this past weekend or

52
01:32:17,030 --> 01:32:20,650
even just today is credit card fraud detection 

53
01:32:20,650 --> 01:32:24,340
every time you use your credit card the current purchase is analyzed

54
01:32:24,340 --> 01:32:28,230
against your history of credit card transactions to determine if the current

55
01:32:28,230 --> 01:32:33,070
purchase is a legitimate transaction or a potentially fraudulent one 

56
01:32:33,070 --> 01:32:36,840
if the purchase is very different from your past purchases such as for

57
01:32:36,840 --> 01:32:40,480
a big ticket item in a category that you had never shown an interest in or

58
01:32:40,480 --> 01:32:43,630
when the point of sales location is from another country 

59
01:32:43,630 --> 01:32:45,860
then it will be flagged as a suspicious activity 

60
01:32:46,920 --> 01:32:49,100
in that case the transaction may be denied 

61
01:32:49,100 --> 01:32:51,990
or you may get a call from your credit card company to confirm that

62
01:32:51,990 --> 01:32:54,760
the purchase was indeed made by you 

63
01:32:54,760 --> 01:32:57,560
this is a very common use of machine learning that is encountered in

64
01:32:57,560 --> 01:32:58,160
everyday life 

65
01:33:00,120 --> 01:33:03,690
another example application of machine learning encountered in daily life is

66
01:33:03,690 --> 01:33:06,140
handwritten digit recognition 

67
01:33:06,140 --> 01:33:10,890
when you deposit a hand - written check into an atm a machine learning process is used

68
01:33:10,890 --> 01:33:15,660
to read the numbers written on the check to determine the amount of the deposit 

69
01:33:15,660 --> 01:33:19,420
handwritten digits are trickier to decipher than typed digits

70
01:33:19,420 --> 01:33:22,150
due to the many variations in people handwriting 

71
01:33:23,490 --> 01:33:27,130
a machine learning system can sift through the different variations to find similar

72
01:33:27,130 --> 01:33:30,770
patterns to distinguish a one from a nine for example 

73
01:33:32,820 --> 01:33:35,930
recommendations in what sites is another example application of

74
01:33:35,930 --> 01:33:39,460
machine learning that most people have experienced first hand 

75
01:33:39,460 --> 01:33:44,160
after you buy an item on a website you will often get a list of related items 

76
01:33:44,160 --> 01:33:47,100
often this will be displayed as customers who bought this

77
01:33:47,100 --> 01:33:50,380
item also bought these items or you may also like 

78
01:33:51,630 --> 01:33:55,130
these related items have been associated with the item you purchased by

79
01:33:55,130 --> 01:33:56,650
a machine learning model and

80
01:33:56,650 --> 01:34:00,380
are now being shown to you since you may also be interested in them 

81
01:34:00,380 --> 01:34:03,160
this is a common application of machine learning used often in

82
01:34:03,160 --> 01:34:04,040
sales and marketing 

83
01:34:04,040 --> 01:34:09,600
here are some other examples of where machine learning has been used 

84
01:34:09,600 --> 01:34:12,360
like targeted ads on mobile devices 

85
01:34:12,360 --> 01:34:15,490
sentiment analysis of social media data 

86
01:34:15,490 --> 01:34:20,570
climate monitoring to detect seasonal patterns crime pattern detection and

87
01:34:20,570 --> 01:34:23,910
a healthiness analysis of drugs among many other applications 

88
01:34:25,000 --> 01:34:26,580
as you can see from this short list 

89
01:34:26,580 --> 01:34:30,420
machine learning has been used in various applications including science 

90
01:34:30,420 --> 01:34:34,910
medicine retail law enforcement education and many others 

91
01:34:36,850 --> 01:34:41,190
let take a few minutes to discuss the different terms which refer to this field 

92
01:34:41,190 --> 01:34:45,290
the term we are using for this course is machine learning but you may have heard

93
01:34:45,290 --> 01:34:50,080
other terms such as data mining predictive analytics and data slangs 

94
01:34:50,080 --> 01:34:52,480
so what is the difference between these different terms 

95
01:34:54,120 --> 01:34:57,740
as we have discussed machine learning has its roots since statistics 

96
01:34:57,740 --> 01:35:01,590
artificial intelligence and computer science among other fields 

97
01:35:01,590 --> 01:35:04,100
machine learning encompasses the algorithms and

98
01:35:04,100 --> 01:35:05,990
techniques used to learn from data 

99
01:35:07,700 --> 01:35:11,560
the term data mining became popular around the time that the use

100
01:35:11,560 --> 01:35:13,820
databases became common place 

101
01:35:13,820 --> 01:35:17,980
so data mining was used to refer to activities related to finding patterns in

102
01:35:17,980 --> 01:35:20,780
databases and data warehouses 

103
01:35:20,780 --> 01:35:24,290
there are some practical data management aspects to data mining related to

104
01:35:24,290 --> 01:35:26,380
accessing data from databases 

105
01:35:26,380 --> 01:35:29,460
but the process of finding patterns in data is similar and

106
01:35:29,460 --> 01:35:32,320
can use the same algorithms and techniques as machine learning 

107
01:35:34,640 --> 01:35:40,020
predictive analytics refers to analyzing data in order to predict future outcomes 

108
01:35:40,020 --> 01:35:44,090
this term is usually used in the business context to describe activities such as

109
01:35:44,090 --> 01:35:48,540
sales forecasting or predicting the purchasing behavior of a customer 

110
01:35:48,540 --> 01:35:52,060
but again the techniques used to make these predictions are the same techniques

111
01:35:52,060 --> 01:35:52,790
from machine learning 

112
01:35:55,050 --> 01:35:58,205
data science is a new term that is used to describe processing and

113
01:35:58,205 --> 01:36:00,860
analyzing data to extract meaning 

114
01:36:00,860 --> 01:36:03,890
again machine learning techniques can also be used here 

115
01:36:03,890 --> 01:36:07,260
because the term data science became popular at the same time that big

116
01:36:07,260 --> 01:36:12,670
data began appearing data science usually refers to extracting meaning from big data

117
01:36:12,670 --> 01:36:16,380
and so includes approaches for collecting storing and managing big data 

118
01:36:17,420 --> 01:36:19,370
these terms evolved at different times and

119
01:36:19,370 --> 01:36:22,050
may have encompassed different sets of activities 

120
01:36:22,050 --> 01:36:25,115
but there have always been more similarities than differences between

121
01:36:25,115 --> 01:36:26,610
them 

122
01:36:26,610 --> 01:36:28,720
now they are often used interchangeably and

123
01:36:28,720 --> 01:36:31,370
have come to mean essentially the same thing 

124
01:36:31,370 --> 01:36:36,127
the process of extracting valuable insight from data the core algorithms and

125
01:36:36,127 --> 01:36:39,891
techniques for doing this do not change with different terms 

126
01:36:41,577 --> 01:36:45,544
to summarize in this lecture we have discussed what machine learning is and

127
01:36:45,544 --> 01:36:47,210
how it is being used 

128
01:36:47,210 --> 01:36:51,590
machine learning models learn from data to perform a task without being explicitly

129
01:36:51,590 --> 01:36:52,730
programmed 

130
01:36:52,730 --> 01:36:55,650
they are used to discover patterns and trends in the data 

131
01:36:55,650 --> 01:37:00,060
and a lab for data driven decisions to be made for the problems being studied 

132
01:37:00,060 --> 01:37:04,550
we also discuss in this lecture examples of how machine learning is being used 

133
01:37:04,550 --> 01:37:07,840
we see how the example applications that machine learning can be applied

134
01:37:07,840 --> 01:37:09,320
to many different areas 

1
03:06:13,130 --> 03:06:15,610
what are the steps in the machine learning process 

2
03:06:15,610 --> 03:06:17,180
we will discuss those in this lecture 

3
03:06:18,980 --> 03:06:23,500
after this video you will be able to identify the steps in the machine

4
03:06:23,500 --> 03:06:27,390
learning process and discuss why the machine learning process is iterative 

5
03:06:29,510 --> 03:06:33,400
this diagram illustrates the steps in the machine learning process 

6
03:06:33,400 --> 03:06:36,800
in this lecture we will cover an overview of these steps 

7
03:06:36,800 --> 03:06:39,640
in the next lecture we will cover each step in more detail 

8
03:06:41,890 --> 03:06:45,040
it should be kept in mind that all of these steps need to be carried out with

9
03:06:45,040 --> 03:06:47,130
a clear purpose in mind 

10
03:06:47,130 --> 03:06:51,440
that is the problem or opportunity that is being addressed must be defined with

11
03:06:51,440 --> 03:06:54,220
clearly stated goals and objectives 

12
03:06:54,220 --> 03:06:55,150
for example 

13
03:06:55,150 --> 03:06:59,590
the purpose of a project may be to study customer purchasing behavior to come up

14
03:06:59,590 --> 03:07:04,850
with a more effective marketing strategy in order to increase sales revenue 

15
03:07:04,850 --> 03:07:08,420
the purpose behind the project will drive the machine learning process 

16
03:07:10,190 --> 03:07:12,690
the first step in the machine learning process is to get

17
03:07:12,690 --> 03:07:16,020
all available data related to the problem at hand 

18
03:07:16,020 --> 03:07:20,450
here we need to identify all data sources collect the data and

19
03:07:20,450 --> 03:07:22,800
finally integrate data from these multiple sources 

20
03:07:24,590 --> 03:07:28,070
the next step in the machine learning process is to prepare the data 

21
03:07:29,230 --> 03:07:33,920
this step is further divided into two parts explore data and pre - process data 

22
03:07:35,740 --> 03:07:39,550
the first part of data preparation involves preliminary exploration

23
03:07:39,550 --> 03:07:43,300
of the data to understand the nature of the data that we have to work with 

24
03:07:44,320 --> 03:07:48,780
things we want to understand about the data are its characteristics format 

25
03:07:48,780 --> 03:07:49,300
and quality 

26
03:07:50,600 --> 03:07:54,310
a good understanding of the data leads to a more informed analysis and

27
03:07:54,310 --> 03:07:55,650
a more successful outcome 

28
03:07:57,580 --> 03:08:01,140
once we know more about the data through exploratory analysis 

29
03:08:01,140 --> 03:08:05,240
the next part is pre - processing of the data for analysis 

30
03:08:05,240 --> 03:08:09,500
this includes cleaning data selecting the variables to use and

31
03:08:09,500 --> 03:08:13,140
transforming data to make the data more suitable for analysis in the next step 

32
03:08:14,990 --> 03:08:18,705
the prepared data then would be passed on to the analysis step 

33
03:08:18,705 --> 03:08:22,280
this step involves selecting the analytical techniques to use 

34
03:08:22,280 --> 03:08:25,350
building a model using the data and assessing the results 

35
03:08:26,980 --> 03:08:30,930
step four in the machine learning process it to communicate results 

36
03:08:30,930 --> 03:08:35,490
this includes evaluating the results with respect to the goal set for the project 

37
03:08:35,490 --> 03:08:38,360
presenting the results in a easy to understand way and

38
03:08:38,360 --> 03:08:40,058
communicating the results to others 

39
03:08:40,058 --> 03:08:44,320
the last step is to apply the results 

40
03:08:44,320 --> 03:08:47,550
this brings us back to the purpose of the project 

41
03:08:47,550 --> 03:08:52,185
how can the insights from our analysis be used to provide effective marketing

42
03:08:52,185 --> 03:08:53,907
to increase sales revenue 

43
03:08:53,907 --> 03:08:57,039
determining actions from insights gained

44
03:08:57,039 --> 03:09:00,524
from analysis is the main focus of the act step 

45
03:09:00,524 --> 03:09:04,480
note that the machine learning process is a very iterative one 

46
03:09:04,480 --> 03:09:07,960
findings from one step may require a previous step to be repeated

47
03:09:07,960 --> 03:09:09,650
with new information 

48
03:09:09,650 --> 03:09:14,170
for example during the prepare step we may find some data quality issues that may

49
03:09:14,170 --> 03:09:18,760
require us to go back to the acquire step to address some issues with data

50
03:09:18,760 --> 03:09:22,550
collection or to get additional data that we did not include in the first go around 

51
03:09:23,680 --> 03:09:26,810
each step may also require several iterations 

52
03:09:26,810 --> 03:09:31,260
for example it is common to try different analysis techniques in the analyze step

53
03:09:31,260 --> 03:09:33,470
in order to get reasonable results from the model 

54
03:09:34,490 --> 03:09:38,540
so it is important to recognize that this is a highly iterative process and

55
03:09:38,540 --> 03:09:39,320
not a linear one 

1
06:15:51,008 --> 06:15:56,620
now that maya has explained the basics of machine learning 

2
06:15:56,620 --> 06:16:01,660
let remember how big data influences analytical applications 

3
06:16:01,660 --> 06:16:06,020
and how we can take advantage of the existing big data tools and

4
06:16:06,020 --> 06:16:08,490
techniques in machine learning 

5
06:16:08,490 --> 06:16:11,960
how can machine learning algorithms be scaled up

6
06:16:11,960 --> 06:16:15,010
to process large volumes of data 

7
06:16:15,010 --> 06:16:16,120
let talk about that now 

8
06:16:18,100 --> 06:16:21,395
after this video you will be able to

9
06:16:21,395 --> 06:16:25,665
explain how machine learning techniques can scale up to big data 

10
06:16:26,685 --> 06:16:31,855
and discuss the role of distributed computing platforms like hadoop and

11
06:16:31,855 --> 06:16:34,485
spark in applying machine learning to big data 

12
06:16:35,855 --> 06:16:39,605
with the massive amounts of data that need to be processed for

13
06:16:39,605 --> 06:16:43,775
applications such as drug effectiveness analysis 

14
06:16:43,775 --> 06:16:48,200
climate monitoring and website recommendations to name a few 

15
06:16:49,420 --> 06:16:54,570
we need to be able to add scalability to machine learning techniques 

16
06:16:54,570 --> 06:16:56,600
how do we apply machine learning at scale 

17
06:16:57,790 --> 06:17:02,770
one way is to scale up by adding more memory processors 

18
06:17:02,770 --> 06:17:08,400
and storage to our system so that it can store and process more data 

19
06:17:08,400 --> 06:17:10,110
this is not the big data approach 

20
06:17:11,270 --> 06:17:16,182
specialized hardware such as graphical processing units or

21
06:17:16,182 --> 06:17:20,536
gpus for short can also be added to speed up the miracle

22
06:17:20,536 --> 06:17:24,730
operations common in machine learning algorithms 

23
06:17:24,730 --> 06:17:28,300
although this is a good approach this is also not the big data approach 

24
06:17:29,650 --> 06:17:35,510
as we learned in our introductory course one problem with this approach

25
06:17:35,510 --> 06:17:39,720
is that larger specialized hardware can be very costly 

26
06:17:41,180 --> 06:17:46,590
another problem is that we eventually will hit a limit 

27
06:17:46,590 --> 06:17:49,830
there only so much hardware you can add to a machine 

28
06:17:50,990 --> 06:17:54,100
an alternative approach is to scale out 

29
06:17:55,860 --> 06:18:01,260
this means using many local commodity distribution systems together 

30
06:18:02,310 --> 06:18:07,160
data is distributed over these systems to gain processing speed up 

31
06:18:09,030 --> 06:18:16,870
as shown in this illustration the idea is to divide the data into smaller subsets 

32
06:18:16,870 --> 06:18:22,900
the same processing is applied to each subset or map and the results

33
06:18:22,900 --> 06:18:27,660
are merged at the end to come up with the overall results for the original dataset 

34
06:18:28,770 --> 06:18:34,700
let consider an example where we want to apply the same operation

35
06:18:34,700 --> 06:18:39,420
to all the samples in a dataset of n samples 

36
06:18:39,420 --> 06:18:42,640
in this case n is four 

37
06:18:42,640 --> 06:18:47,150
if it takes t time units to perform this

38
06:18:47,150 --> 06:18:52,460
operation on each sample then with sequential processing

39
06:18:52,460 --> 06:18:57,350
the time to apply that operation to all samples is n times t 

40
06:18:58,470 --> 06:19:01,582
if we have a cluster of four processors 

41
06:19:01,582 --> 06:19:05,890
we can distribute the data across the four processors 

42
06:19:07,770 --> 06:19:14,730
each process performs the operation on the dataset subset of n over four samples 

43
06:19:16,230 --> 06:19:21,990
processing of the four subsets of the data is done in parallel 

44
06:19:21,990 --> 06:19:25,390
that is the subsets are processed at the same time 

45
06:19:27,010 --> 06:19:33,870
the processing time for the distributed approach is approximately n over 4 times t

46
06:19:35,010 --> 06:19:41,700
plus any overhead required to merge the subset results and maybe shuffle them 

47
06:19:43,100 --> 06:19:48,050
this is a speedup of nearly four times over the sequential approach 

48
06:19:50,040 --> 06:19:54,021
on a distributed computing platform such as spark or hadoop 

49
06:19:54,021 --> 06:19:59,610
scalable machine learning algorithms use the same scale out approach 

50
06:19:59,610 --> 06:20:03,620
data is distributed across different processors 

51
06:20:03,620 --> 06:20:08,690
which operate on the data subsets in parallel using map reduce and

52
06:20:08,690 --> 06:20:11,230
other distributed parallel transformations 

53
06:20:12,480 --> 06:20:13,420
this allows for

54
06:20:13,420 --> 06:20:17,050
machine learning techniques to be applied to large volumes of data 

55
06:20:18,230 --> 06:20:24,300
in this course we will use spark and its scalable machine learning library 

56
06:20:24,300 --> 06:20:29,830
mlf to show you how machine learning can be applied to big data 

57
06:20:29,830 --> 06:20:30,640
and do not forget 

58
06:20:30,640 --> 06:20:35,890
this is the processing of the machine learning on where the data resides 

59
06:20:35,890 --> 06:20:38,480
and we call this the big data approach 

60
06:20:39,770 --> 06:20:45,020
however you can also imagine a scenario where you also

61
06:20:45,020 --> 06:20:50,070
update the machine learning algorithms to scale up 

62
06:20:50,070 --> 06:20:54,389
so you can paralyze the machine learning algorithms themselves 

63
06:20:54,389 --> 06:20:58,644
and also use processing of big data together with this approach 

1
12:36:49,910 --> 12:36:54,618
in this video we will provide you with a quick summary of the main points

2
12:36:54,618 --> 12:36:58,644
from our first three courses to recall what you have learned 

3
12:36:58,644 --> 12:37:02,520
if you have just completed our third course and do not need a refresher 

4
12:37:02,520 --> 12:37:04,470
you might skip to the next lecture 

5
12:37:07,140 --> 12:37:12,325
we started our first course explaining how a new torrent of big data

6
12:37:12,325 --> 12:37:17,974
combined with cloud computing capabilities to process data anytime and

7
12:37:17,974 --> 12:37:22,803
anywhere has been at the core of the launch of the big data era 

8
12:37:22,803 --> 12:37:26,995
such capabilities enable or present opportunities for

9
12:37:26,995 --> 12:37:32,704
many dynamic data - driven applications including energy management 

10
12:37:32,704 --> 12:37:37,620
smart cities precision medicine and smart manufacturing 

11
12:37:38,620 --> 12:37:43,163
these applications are increasingly more data - driven dynamic and

12
12:37:43,163 --> 12:37:47,470
heterogeneous in terms of their technology needs 

13
12:37:47,470 --> 12:37:50,490
they are also more process - driven and

14
12:37:50,490 --> 12:37:55,130
need to be tackled using a collaborative approach by a team that

15
12:37:55,130 --> 12:37:59,400
puts value on accountability and reproducibility of the results 

16
12:38:00,840 --> 12:38:06,250
overall by modeling managing integrating diverse

17
12:38:06,250 --> 12:38:11,260
data streams we add value to our big data and

18
12:38:11,260 --> 12:38:15,030
improve our business even more before we start analyzing it 

19
12:38:16,640 --> 12:38:18,080
a part of modeling and

20
12:38:18,080 --> 12:38:23,740
managing big data is focusing on the dimensions of the scalability and

21
12:38:23,740 --> 12:38:28,080
considering the challenges associated with these dimensions to pick the right tools 

22
12:38:29,990 --> 12:38:34,859
we also talked about characteristics of big data referring to some vs

23
12:38:34,859 --> 12:38:40,360
like volume variety velocity veracity and valence 

24
12:38:41,500 --> 12:38:47,727
each week presents a challenging dimension of big data 

25
12:38:47,727 --> 12:38:55,010
namely size complexity speed quality and connectedness 

26
12:38:55,010 --> 12:38:57,530
we also added a sixth v 

27
12:38:57,530 --> 12:39:01,920
value referring to the real reason we are interested in big data 

28
12:39:03,550 --> 12:39:07,660
to turn it into an advantage in the context of a problem using data science

29
12:39:07,660 --> 12:39:10,890
techniques big data needs to be analyzed 

30
12:39:12,470 --> 12:39:19,530
we explained a five steps process for data science that includes data acquisition 

31
12:39:19,530 --> 12:39:23,500
modeling management integration and analysis 

32
12:39:24,620 --> 12:39:27,370
the influence of big data pushes for

33
12:39:27,370 --> 12:39:31,710
alternative scalability approaches at each step of the process 

34
12:39:33,820 --> 12:39:39,310
if we just focus on the scalability challenges related to the three vs 

35
12:39:39,310 --> 12:39:42,720
we can say big data has varying volume and

36
12:39:42,720 --> 12:39:48,020
velocity requiring dynamic and scalable batch and stream processing 

37
12:39:49,110 --> 12:39:54,715
big data has variety requiring management of data in many different data systems 

38
12:39:54,715 --> 12:39:56,790
and integration of it at scale 

39
12:39:58,630 --> 12:40:01,470
in our introduction to the big data course 

40
12:40:01,470 --> 12:40:05,590
we talked about the version of a layer diagram for the tools in the hadoop

41
12:40:05,590 --> 12:40:09,950
ecosystem organized vertically based on the interface 

42
12:40:11,520 --> 12:40:16,210
low level interfaces for storage and scheduling on the bottom

43
12:40:17,890 --> 12:40:21,610
and high level languages and interactivity at the top 

44
12:40:22,820 --> 12:40:27,691
most of the tools in the hadoop ecosystem were initially built to compliment

45
12:40:27,691 --> 12:40:32,880
the capabilities of hadoop for distributed file system management using hdfs 

46
12:40:34,060 --> 12:40:37,490
data processing using the mapreduce engine and

47
12:40:37,490 --> 12:40:41,810
resource scheduling and negotiation using the yarn engine 

48
12:40:43,320 --> 12:40:46,580
over time a number of new projects were built 

49
12:40:46,580 --> 12:40:52,010
either to add to these complementary tools or to handle additional types of

50
12:40:52,010 --> 12:40:57,640
big data management and processing not available in hadoop just like spark 

51
12:40:59,260 --> 12:41:04,430
arguably the most important change to hadoop over time

52
12:41:04,430 --> 12:41:08,960
was the separation of yarn from the mapreduce programming model

53
12:41:08,960 --> 12:41:11,590
to solely handle resource management concerns 

54
12:41:13,060 --> 12:41:18,850
this allowed for hadoop to be extensible to different programming models and enable

55
12:41:18,850 --> 12:41:23,640
the development of a number of processing engines for batch and stream processing 

56
12:41:25,140 --> 12:41:29,500
another way to look at the vast number of tools that have been added to the hadoop

57
12:41:29,500 --> 12:41:33,420
ecosystem is from the point of view of their functionality

58
12:41:33,420 --> 12:41:34,990
in the big data processing pipeline 

59
12:41:36,090 --> 12:41:41,680
simply put these are associated with three distinct layers for

60
12:41:41,680 --> 12:41:46,660
data management and storage for data processing and

61
12:41:46,660 --> 12:41:49,780
for resource coordination and workflow management 

62
12:41:51,360 --> 12:41:57,122
in our second course we talked in detail about the bottom layer in this diagram 

63
12:41:57,122 --> 12:41:59,810
namely data management and storage 

64
12:42:01,920 --> 12:42:07,890
while this layer includes hadoop hdfs there are a number of other systems

65
12:42:07,890 --> 12:42:14,680
that rely on hdfs as a file system or implement their own no - sql storage option 

66
12:42:14,680 --> 12:42:19,255
as big data can have a variety of structured semi - structured and

67
12:42:19,255 --> 12:42:24,730
unstructured formats and gets analyzed through a variety of tools 

68
12:42:24,730 --> 12:42:27,880
many tools were introduced to fit this variety of needs 

69
12:42:29,210 --> 12:42:32,040
we call these big data management systems 

70
12:42:33,510 --> 12:42:37,690
we reviewed redis and aerospike as

71
12:42:37,690 --> 12:42:42,180
key value stores where each data item is identified with a unique key 

72
12:42:44,670 --> 12:42:49,200
we also got some practical experience with lucene and

73
12:42:49,200 --> 12:42:53,842
gephi as vector and graph - stores respectively 

74
12:42:55,310 --> 12:42:59,660
we also talked about vertica as a column - store database where

75
12:42:59,660 --> 12:43:03,570
information is stored in columns rather than rows 

76
12:43:05,660 --> 12:43:09,998
cassandra and hbase are also in this category 

77
12:43:09,998 --> 12:43:16,749
finally we introduced solr and asterisk db for managing unstructured and

78
12:43:16,749 --> 12:43:21,822
semi - structured text and mongodb as a document store 

79
12:43:24,591 --> 12:43:29,444
the processing layer is where all these different types

80
12:43:29,444 --> 12:43:34,091
of data get retrieved integrated and analyzed 

81
12:43:34,091 --> 12:43:38,343
which was the primary focus of our third course 

82
12:43:38,343 --> 12:43:41,451
in the integration and processing layer 

83
12:43:41,451 --> 12:43:46,283
we roughly refer to the tools that are built on top of htfs and yarn 

84
12:43:46,283 --> 12:43:50,965
although some of them were with other storage and file systems 

85
12:43:52,675 --> 12:43:59,064
yarn is a significant enable of many of these tools making a number of batch and

86
12:43:59,064 --> 12:44:05,277
stream processing engines like storm spark flink and beam possible 

87
12:44:05,277 --> 12:44:09,373
this layer also includes tools like hive and spark sql for

88
12:44:09,373 --> 12:44:13,894
bringing a query interface on top of the storage layer pig for

89
12:44:13,894 --> 12:44:19,014
scripting simple big data pipelines using the mapreduce framework and

90
12:44:19,014 --> 12:44:22,512
a number of specialized analytical libraries 

91
12:44:22,512 --> 12:44:25,775
formation learning and graph analytics 

92
12:44:25,775 --> 12:44:32,538
giraph and graphx of spark are examples of such libraries for graph processing 

93
12:44:32,538 --> 12:44:35,448
mahout on top of the hadoop stack and

94
12:44:35,448 --> 12:44:39,820
mllib of spark are two options for machine learning 

95
12:44:40,970 --> 12:44:44,665
although we had a basic overview of graph processing and

96
12:44:44,665 --> 12:44:48,991
machine learning for big data analytics earlier in our second and

97
12:44:48,991 --> 12:44:52,860
third courses we have not gone into the details there 

98
12:44:52,860 --> 12:44:58,576
in this course we will use spark mllib as one of our two main tools 

99
12:44:58,576 --> 12:45:04,692
providing a deeper introduction to the machine learning library of spark 

100
12:45:04,692 --> 12:45:11,075
the third and top layer in our diagram is the coordination and management layer 

101
12:45:11,075 --> 12:45:14,888
this is where integrations scheduling coordination and

102
12:45:14,888 --> 12:45:19,840
monitoring of applications across many tools in the bottom two layers take place 

103
12:45:20,890 --> 12:45:25,940
this layer is also where the results of the big data analysis get communicated

104
12:45:25,940 --> 12:45:31,030
to other programs websites visualization tools and business intelligence tools 

105
12:45:32,740 --> 12:45:37,905
workflow management systems help to develop automated solutions that

106
12:45:37,905 --> 12:45:42,835
can manage and coordinate the process of combining data management and

107
12:45:42,835 --> 12:45:49,165
analytical tasks in a big data pipeline as a configurable structured set of steps 

108
12:45:50,275 --> 12:45:53,275
workflow driven thinking also matches

109
12:45:53,275 --> 12:45:57,810
this basic process of data science that we overviewed before 

110
12:45:57,810 --> 12:46:01,510
oozie is an example workflow scheduler that can interact with

111
12:46:01,510 --> 12:46:04,620
many of the tools in the integration and processing layer 

112
12:46:05,740 --> 12:46:10,364
zookeeper is the resource coordination tool which monitors and

113
12:46:10,364 --> 12:46:15,680
manages and coordinates all these tools and named after animal 

114
12:46:17,510 --> 12:46:21,950
now that we have reviewed all three layers we are ready to come back to

115
12:46:21,950 --> 12:46:27,790
the integration and processing layer but now in the context of machine learning 

116
12:46:27,790 --> 12:46:31,060
in which we will use machine learning techniques to

117
12:46:31,060 --> 12:46:35,840
apply to our five step data science process and analyze big data 

118
12:46:37,570 --> 12:46:43,370
just a simple google search for big data processing pipelines will bring

119
12:46:43,370 --> 12:46:47,950
a vast number of pipelines with a large number of technologies

120
12:46:47,950 --> 12:46:52,520
that support scalable data cleaning preparation and analysis 

121
12:46:54,080 --> 12:46:58,650
how do we make sense of all of it to make sure we use the right tools for

122
12:46:58,650 --> 12:46:59,240
our application 

123
12:47:00,320 --> 12:47:03,580
how do we pick the right pre - processing and

124
12:47:03,580 --> 12:47:06,780
machine learning techniques to start doing predictive modeling 

125
12:47:08,450 --> 12:47:13,190
over the next few weeks dr mei will walk you through some of the most

126
12:47:13,190 --> 12:47:18,515
fundamental machine learning techniques along with introductory hands

127
12:47:18,515 --> 12:47:24,102
on exercises we designed for you to ease you into the world of machine learning 

128
12:47:24,102 --> 12:47:25,418
let get started 

1
01:24:15,340 --> 01:24:18,970
let go through an overview of the tools we will be using in this course 

2
01:24:20,210 --> 01:24:24,890
after this video you will be able to describe what knime is 

3
01:24:24,890 --> 01:24:30,450
describe what spark mllib is and contrast knime and mllib as machine learning tools 

4
01:24:32,240 --> 01:24:35,940
the machine learning tools that we will be using in this course are knime and

5
01:24:35,940 --> 01:24:37,410
spark mllib 

6
01:24:37,410 --> 01:24:39,870
these are both open source tools 

7
01:24:39,870 --> 01:24:42,936
this lecture will introduce these tools to you 

8
01:24:42,936 --> 01:24:45,780
you will need to use them for the hands on activities in this course 

9
01:24:47,880 --> 01:24:53,390
knime analytics is a platform for data analytics reporting and visualization 

10
01:24:53,390 --> 01:24:58,650
the knime platform uses a graphical user interface based approach with drag and

11
01:24:58,650 --> 01:25:02,890
drop features to facilitate constructing an analytic solution 

12
01:25:02,890 --> 01:25:05,630
the basic components in knime are referred to as nodes 

13
01:25:06,690 --> 01:25:09,770
each node provides a specific functionalities 

14
01:25:09,770 --> 01:25:11,880
such as reading in a file 

15
01:25:11,880 --> 01:25:15,900
creating a specific type of machine running model and generating a plot 

16
01:25:17,080 --> 01:25:20,790
nodes can be connected to create machine running workflows or pipelines 

17
01:25:22,130 --> 01:25:25,510
knime stands for konstanz information miner 

18
01:25:25,510 --> 01:25:29,610
the konstanz is for the university of konstanz in germany 

19
01:25:29,610 --> 01:25:32,885
and note that the k in knime is silent in the pronunciation 

20
01:25:35,187 --> 01:25:38,875
in knime you assemble the steps that need to be performed in a machine

21
01:25:38,875 --> 01:25:42,190
learning process by connecting nodes to create a workflow 

22
01:25:43,200 --> 01:25:47,070
to create a workflow the user chooses the appropriate nodes

23
01:25:47,070 --> 01:25:51,750
from the node repository and assembles them into a workflow 

24
01:25:51,750 --> 01:25:55,039
the workflow can then be executed in the knime work bench 

25
01:25:57,200 --> 01:26:01,340
a node implements a specific operation in a workflow 

26
01:26:01,340 --> 01:26:03,880
in this screenshot we see two nodes 

27
01:26:03,880 --> 01:26:07,380
the file reader node is used to read data from a text file or

28
01:26:07,380 --> 01:26:09,820
a url or a web address 

29
01:26:09,820 --> 01:26:13,370
the decision tree learner node builds a decision tree model 

30
01:26:14,470 --> 01:26:20,226
each node can have input and output ports and can be connected to other nodes 

31
01:26:20,226 --> 01:26:23,535
when a node is executed it takes data from its input port 

32
01:26:23,535 --> 01:26:29,650
performs some operations on the data and writes the results to the output port 

33
01:26:29,650 --> 01:26:32,950
data is transferred between nodes that are connected 

34
01:26:32,950 --> 01:26:37,130
a node can be configured by opening up its configuration dialog 

35
01:26:37,130 --> 01:26:39,329
this is where the parameters for the node can be set 

36
01:26:41,718 --> 01:26:45,878
the node repository is where you will find all the nodes available in your knime

37
01:26:45,878 --> 01:26:47,730
installation 

38
01:26:47,730 --> 01:26:50,800
the nodes are organized by categories 

39
01:26:50,800 --> 01:26:55,260
knime provides an array of nodes to perform operations for data access 

40
01:26:55,260 --> 01:26:59,610
data manipulation analysis visualization and reporting 

41
01:27:01,840 --> 01:27:06,691
as you can see knime provides a visual approach to machine learning 

42
01:27:06,691 --> 01:27:11,043
it gui - based drag - and - drop approach provides an easy way to create and

43
01:27:11,043 --> 01:27:13,160
execute a machine learning workflow 

44
01:27:14,320 --> 01:27:18,530
the open source version of knime however is limited in how large of a dataset

45
01:27:18,530 --> 01:27:19,230
it can handle 

46
01:27:20,290 --> 01:27:24,100
there are commercial extensions to knime to manage large datasets and

47
01:27:24,100 --> 01:27:28,390
offer other extra functionalities but these extensions are not open source 

48
01:27:30,150 --> 01:27:32,456
now let talk about spark mllib 

49
01:27:32,456 --> 01:27:35,407
you have worked with spark before if you took the third course in this

50
01:27:35,407 --> 01:27:36,710
specialization on big data 

51
01:27:37,980 --> 01:27:40,210
spark is a distributed computing platform 

52
01:27:41,210 --> 01:27:44,900
mllib is a scalable machine learning library that runs on top of spark 

53
01:27:46,070 --> 01:27:50,770
it provides distributed implementations of commonly used machine learning algorithms

54
01:27:50,770 --> 01:27:51,469
and utilities 

55
01:27:52,470 --> 01:27:55,502
the ml and mllib of course stands for machine learning 

56
01:27:57,801 --> 01:28:03,120
to implement machine learning operations in spark mllib you need to write code 

57
01:28:03,120 --> 01:28:06,740
so mllib is not a gui - based approach 

58
01:28:06,740 --> 01:28:10,360
this segment of code reads and parses data from a file 

59
01:28:10,360 --> 01:28:14,937
then builds a decision tree classification model 

60
01:28:14,937 --> 01:28:19,480
mllib as with the base spark core provides an application programming

61
01:28:19,480 --> 01:28:25,570
interface or api for java python scala and r 

62
01:28:25,570 --> 01:28:29,174
this means that you can write code in these programming languages to execute

63
01:28:29,174 --> 01:28:31,098
the base operations provided in spark 

64
01:28:33,082 --> 01:28:36,640
spark mllib runs on a distributed platform 

65
01:28:36,640 --> 01:28:38,550
it provides machine learning algorithms and

66
01:28:38,550 --> 01:28:42,560
techniques that are implemented using distributed processing 

67
01:28:42,560 --> 01:28:46,470
so mllib is used for processing and analyzing large datasets 

68
01:28:47,500 --> 01:28:51,610
and as we have discussed writing code is required to implement operations in mllib 

69
01:28:53,680 --> 01:28:58,576
in summary knime is a gui - based machine learning tool while spark mllib provides

70
01:28:58,576 --> 01:29:04,040
a programming - based scalable platform for processing very large datasets 

71
01:29:04,040 --> 01:29:08,270
you will be using both spark mllib and knime throughout this course 

72
01:29:08,270 --> 01:29:12,420
we have readings and hands - on exercises to help you get familiar with these popular

73
01:29:12,420 --> 01:29:15,040
open source tools for machine learning 

74
01:29:15,040 --> 01:29:18,360
i think you will find it very informative and fun to work with these tools 

1
02:53:33,335 --> 02:53:37,239
welcome to course four of the big data specialization 

2
02:53:37,239 --> 02:53:42,926
i am ilkay altintas for the new learners i am the chief data science officer

3
02:53:42,926 --> 02:53:49,059
at the san diego supercomputer center at the university of california san diego 

4
02:53:49,059 --> 02:53:53,317
i feel honored to teach you the basics of big data modeling 

5
02:53:53,317 --> 02:53:57,170
management and analysis in this specialization 

6
02:53:57,170 --> 02:54:01,544
and to work with dr mai nguyen on this class 

7
02:54:01,544 --> 02:54:05,270
 and i am mai nguyen while most of you might be familiar with ilkay 

8
02:54:05,270 --> 02:54:06,915
i am a new face 

9
02:54:06,915 --> 02:54:11,010
i am excited to be here to teach you what i love doing machine learning 

10
02:54:11,010 --> 02:54:14,330
i received my phd in computer science with a focus on machine learning

11
02:54:14,330 --> 02:54:17,240
from the university of california in san diego 

12
02:54:17,240 --> 02:54:19,390
since then i have worked as a data scientist 

13
02:54:19,390 --> 02:54:22,550
and instructor of machine learning in various venues 

14
02:54:22,550 --> 02:54:25,580
i am the lead for data analytics at sdfc 

15
02:54:25,580 --> 02:54:28,770
in this role i work on data science projects doing research

16
02:54:28,770 --> 02:54:31,480
on scalability on machine learning methods to big data 

17
02:54:32,870 --> 02:54:37,970
 we are really happy to have you in this course to develop your understanding and

18
02:54:37,970 --> 02:54:40,060
skills in machine learning 

19
02:54:40,060 --> 02:54:43,060
 and give you an introductory level experience

20
02:54:43,060 --> 02:54:45,500
with application of machine learning to big data 

21
02:54:46,650 --> 02:54:51,410
by now you might have just finished our first three courses and

22
02:54:51,410 --> 02:54:58,250
learned the basics of big data modeling management integration and processing 

23
02:54:58,250 --> 02:55:00,990
if you have not it not required 

24
02:55:00,990 --> 02:55:04,320
but for those with less background in big data management and

25
02:55:04,320 --> 02:55:06,650
systems you might find it valuable 

26
02:55:07,950 --> 02:55:10,590
 we understand that you may not even have heard anything on

27
02:55:10,590 --> 02:55:12,090
machine learning yet 

28
02:55:12,090 --> 02:55:16,230
that why we will start by discussing what machine learning is 

29
02:55:16,230 --> 02:55:20,330
describing some sample applications and presenting the typical process

30
02:55:20,330 --> 02:55:24,500
of a machine learning project to give you a sense of what machine learning is 

31
02:55:24,500 --> 02:55:27,210
then we will delve into some commonly used machine learning

32
02:55:27,210 --> 02:55:29,310
techniques like classification and clustering 

33
02:55:30,480 --> 02:55:35,630
 we are also going to show you how to explore your data prepare it for

34
02:55:35,630 --> 02:55:40,900
analysis and evaluate the results you get with your machine learning model 

35
02:55:42,020 --> 02:55:46,440
these are all necessary steps for a successful machine learning solution 

36
02:55:48,060 --> 02:55:50,980
 as you know for many data science applications

37
02:55:50,980 --> 02:55:55,210
one has to use many different tools and methods to analyze data 

38
02:55:55,210 --> 02:55:58,000
in fact keeping up with the rapid development of new tools is

39
02:55:58,000 --> 02:56:01,190
one of the challenges of today big data environment 

40
02:56:01,190 --> 02:56:04,870
in this course we will introduce you to two different types of machine learning

41
02:56:04,870 --> 02:56:07,470
tools namely nime and spark mnl 

42
02:56:08,520 --> 02:56:13,320
nime is a graphical user interface based tool that requires no programming 

43
02:56:13,320 --> 02:56:16,370
and as representative of a set of tools used in visual

44
02:56:16,370 --> 02:56:18,810
workflow approach to machine learning 

45
02:56:18,810 --> 02:56:21,784
you will have hand on practice with nime as you go through the exercises in

46
02:56:21,784 --> 02:56:22,380
this course 

47
02:56:23,740 --> 02:56:28,260
we are also excited to show you examples of data processing

48
02:56:28,260 --> 02:56:32,229
using sparks machine learning library and owlweb 

49
02:56:32,229 --> 02:56:34,770
our goal here is to provide you

50
02:56:34,770 --> 02:56:39,840
with simple hands - on exercises that require introductory level programming 

51
02:56:39,840 --> 02:56:43,930
to inspire you on how big data machine learning tools can be operated 

52
02:56:45,230 --> 02:56:47,390
we wish you a fun time learning and

53
02:56:47,390 --> 02:56:52,420
hope to hear from you in the discussion forums and learner stories as always 

54
02:56:53,530 --> 02:56:56,590
 we have suggested time estimates each week for the course 

55
02:56:56,590 --> 02:56:59,990
but feel free to take the course at a faster or slower pace 

56
02:56:59,990 --> 02:57:02,920
and do not forget to connect to other learners through the forums to

57
02:57:02,920 --> 02:57:05,000
enhance your learning experience 

58
02:57:05,000 --> 02:57:05,800
 happy learning 

59
02:57:05,800 --> 02:57:06,711
 happy learning 

1
05:50:38,790 --> 05:50:41,940
in the last lecture we discussed data quality issues 

2
05:50:41,940 --> 05:50:46,340
we will now discuss some common techniques for addressing those quality issues 

3
05:50:46,340 --> 05:50:51,600
after this video you will be able to define what imputation means 

4
05:50:51,600 --> 05:50:54,570
illustrate three ways to handle missing values and

5
05:50:54,570 --> 05:50:59,270
describe the role of domain knowledge in addressing data quality issues 

6
05:50:59,270 --> 05:51:03,150
as we discussed in the last lecture real world data is messy 

7
05:51:03,150 --> 05:51:06,776
some data quality issues that you can find in your data are missing values 

8
05:51:06,776 --> 05:51:12,420
duplicate data invalid data noise and outliers 

9
05:51:12,420 --> 05:51:15,840
you will need to clean your data if you want to perform any meaningful analysis

10
05:51:15,840 --> 05:51:16,390
on that data 

11
05:51:17,960 --> 05:51:20,880
recall that missing data occurs when you do not have a value for

12
05:51:20,880 --> 05:51:23,380
certain variables in some samples 

13
05:51:23,380 --> 05:51:27,050
a simple way to handle missing data is to simply drop any samples with missing

14
05:51:27,050 --> 05:51:28,220
values or nas 

15
05:51:29,460 --> 05:51:32,320
all machine learning tools provide a mechanism or command for

16
05:51:32,320 --> 05:51:35,200
filtering out rows with any missing values 

17
05:51:35,200 --> 05:51:38,360
the advantage of this approach is that it is very simple 

18
05:51:38,360 --> 05:51:42,770
the caveat is that you are removing data when you filter out examples 

19
05:51:42,770 --> 05:51:46,510
if the number of samples dropped is large then you end up losing a lot of your data 

20
05:51:47,830 --> 05:51:50,970
an alternative to dropping samples with missing data is to

21
05:51:50,970 --> 05:51:52,810
impute the missing values 

22
05:51:52,810 --> 05:51:56,720
imputing means to replace the missing values with some reasonable values 

23
05:51:57,900 --> 05:52:02,000
the advantage of this approach is that you are making use of all your data 

24
05:52:02,000 --> 05:52:05,310
oc course imputing is more complicated than simply dropping samples 

25
05:52:06,650 --> 05:52:09,000
there are several ways to impute missing values 

26
05:52:09,000 --> 05:52:12,640
one strategy is to replace the missing values with the mean or

27
05:52:12,640 --> 05:52:15,100
median value of the variable 

28
05:52:15,100 --> 05:52:20,490
for example a missing value for years of employment can be replaced by the mean or

29
05:52:20,490 --> 05:52:25,120
median value for years of employment for all current employees 

30
05:52:25,120 --> 05:52:28,290
another approach is to use the most frequent value

31
05:52:28,290 --> 05:52:30,070
in place of the missing value 

32
05:52:30,070 --> 05:52:34,230
for example the most frequently recorded age of customers

33
05:52:34,230 --> 05:52:39,110
associated with the specific item can be used if that value is missing 

34
05:52:39,110 --> 05:52:43,160
alternatively a sensible value can be derived as a replacement for

35
05:52:43,160 --> 05:52:44,510
a missing value 

36
05:52:44,510 --> 05:52:47,980
for example a missing value for income can be set to zero for

37
05:52:47,980 --> 05:52:50,340
customers less then 18 years old or

38
05:52:50,340 --> 05:52:55,120
it can be replaced with an average value based on occupation and location 

39
05:52:55,120 --> 05:52:58,110
note that this approach requires knowledge about the application and

40
05:52:58,110 --> 05:53:02,130
the variable with missing values in order to make reasonable choices

41
05:53:02,130 --> 05:53:05,140
about what valuables would be sensible to replace the missing values 

42
05:53:06,660 --> 05:53:11,390
in the case of duplicate data one approach is to delete the older record 

43
05:53:11,390 --> 05:53:14,500
another approach is to merge duplicate records 

44
05:53:14,500 --> 05:53:19,330
this often requires a way to determine how to resolve conflicting values 

45
05:53:19,330 --> 05:53:23,720
for example in the case of multiple addresses for the same customer 

46
05:53:23,720 --> 05:53:28,360
some logic for determining similarities between addresses might be necessary 

47
05:53:28,360 --> 05:53:31,130
for example st period is the same as street 

48
05:53:32,740 --> 05:53:37,180
to address invalid data consulting another data source may be necessary 

49
05:53:37,180 --> 05:53:40,910
for example an invalid zip code can be corrected

50
05:53:40,910 --> 05:53:44,810
by looking up the correct zip code based on city and state 

51
05:53:44,810 --> 05:53:49,200
a best estimate for a reasonable value can also be used as a replacement 

52
05:53:49,200 --> 05:53:52,460
for example for a missing age value for an employee 

53
05:53:52,460 --> 05:53:56,980
a reasonable value can be estimated based on the employee length of employment 

54
05:53:58,850 --> 05:54:01,750
noise that distorts the data values can be addressed by

55
05:54:01,750 --> 05:54:04,010
filtering out the source of the noise 

56
05:54:04,010 --> 05:54:08,390
for example filtering out the frequency of a constant background noise

57
05:54:08,390 --> 05:54:11,680
will remove that noise component from a recording 

58
05:54:11,680 --> 05:54:14,670
this filtering must be done with care however 

59
05:54:14,670 --> 05:54:18,290
as it can also remove some components of the true data in the process 

60
05:54:19,760 --> 05:54:23,005
outliers can be detected through the use of summary statistics and

61
05:54:23,005 --> 05:54:24,760
plots of the data 

62
05:54:24,760 --> 05:54:27,940
outliers can significantly skew the distribution of your data and

63
05:54:27,940 --> 05:54:30,430
thus the results of your analysis 

64
05:54:30,430 --> 05:54:33,240
in cases where outliers are not the focus of your analysis 

65
05:54:33,240 --> 05:54:37,100
you will want to remove these outlier samples from your data set 

66
05:54:37,100 --> 05:54:39,710
for example when a thermostat malfunctions and

67
05:54:39,710 --> 05:54:43,070
causes values to fluctuate wildly or to be much higher or

68
05:54:43,070 --> 05:54:46,760
lower than normal these samples should be filtered out 

69
05:54:46,760 --> 05:54:51,140
in some applications however outliers are exactly what you are looking for 

70
05:54:51,140 --> 05:54:54,100
so when you detect outliers you do not want to throw them out 

71
05:54:54,100 --> 05:54:57,110
instead you want to examine them more closely 

72
05:54:57,110 --> 05:55:01,610
a classic example of this is in fraud detection where outliers represent

73
05:55:01,610 --> 05:55:05,410
potential fraudulent use and those samples should be analyzed closely 

74
05:55:06,700 --> 05:55:09,750
in order to address data quality issues effectively

75
05:55:09,750 --> 05:55:12,640
knowledge about the application is crucial 

76
05:55:12,640 --> 05:55:15,360
things such as how the data was collected 

77
05:55:15,360 --> 05:55:20,980
the user population the intended use of the application etc are important 

78
05:55:20,980 --> 05:55:25,060
this domain knowledge is essential to making informed decisions on how

79
05:55:25,060 --> 05:55:28,936
to best impute missing values how to handle duplicate records and

80
05:55:28,936 --> 05:55:32,688
invalid data and what to do about noise and outliers in your data 

1
11:46:10,163 --> 11:46:14,662
this activity we will be exploring weather data in spark 

2
11:46:14,662 --> 11:46:19,936
first we will load weather data from a csv file into a spark dataframe 

3
11:46:19,936 --> 11:46:25,259
next we will examine the columns and schema of the dataframe 

4
11:46:25,259 --> 11:46:30,957
we will then view the summary statistics and drop rows with missing values 

5
11:46:30,957 --> 11:46:33,762
finally we will compute the correlation between two columns 

6
11:46:36,918 --> 11:46:38,698
let begin 

7
11:46:38,698 --> 11:46:42,088
first we will create a new jupyter python notebook 

8
11:46:42,088 --> 11:46:46,741
you do this by clicking on new and choosing python 3 

9
11:46:50,522 --> 11:46:54,058
next we will import sql context 

10
11:46:54,058 --> 11:46:58,007
this is done by entering from

11
11:46:58,007 --> 11:47:02,960
pyspark sql import sqlcontext 

12
11:47:02,960 --> 11:47:05,320
next we will create an instance of the sqlcontext 

13
11:47:06,320 --> 11:47:13,510
we will enter sqlcontext = sqlcontext sc 

14
11:47:13,510 --> 11:47:16,490
now let read our weather data into a dataframe 

15
11:47:16,490 --> 11:47:22,177
we will call the dataframe df and we will read it using the sqlcontext 

16
11:47:22,177 --> 11:47:27,027
we will enter sqlcontext read load 

17
11:47:27,027 --> 11:47:30,808
the first argument is the url of the file 

18
11:47:30,808 --> 11:47:38,681
that file : home cloudera downloads big - data - - 

19
11:47:38,681 --> 11:47:44,313
4 daily weather csv 

20
11:47:44,313 --> 11:47:49,269
the second argument specifies the format of how to read the file 

21
11:47:49,269 --> 11:47:54,246
in this case we are going to use the spark csv package from databricks to load

22
11:47:54,246 --> 11:47:56,748
the csv directly into the dataframe 

23
11:47:56,748 --> 11:48:01,202
we need to use this because the cloudera image only has spark 1 

24
11:48:01,202 --> 11:48:04,628
in spark 2 and later this package is included so

25
11:48:04,628 --> 11:48:07,166
we do not have to use this argument 

26
11:48:07,166 --> 11:48:11,159
we will enter format = and

27
11:48:11,159 --> 11:48:19,510
the name of the package is com databricks spark csv 

28
11:48:19,510 --> 11:48:23,745
the next argument specifies that the first line in the csv file is the header 

29
11:48:23,745 --> 11:48:29,060
header = true 

30
11:48:29,060 --> 11:48:34,503
the last argument tells spark to try to infer the schema

31
11:48:34,503 --> 11:48:39,366
from the csv header inferschema = true 

32
11:48:39,366 --> 11:48:41,020
run this 

33
11:48:41,020 --> 11:48:43,240
now let look at our dataframe 

34
11:48:43,240 --> 11:48:48,873
we can run a df columns to see the names of all the columns 

35
11:48:48,873 --> 11:48:52,633
we can also run df printschema to see the schema of the dataframe 

36
11:48:56,360 --> 11:48:59,983
next let look at the summary statistics for the data 

37
11:48:59,983 --> 11:49:02,645
we can do this using the describe method 

38
11:49:02,645 --> 11:49:08,459
we will run df describe show 

39
11:49:12,491 --> 11:49:16,739
this shows summary statistics for all the columns in the dataframe 

40
11:49:16,739 --> 11:49:20,020
there a lot of information here so lets just choose one column 

41
11:49:20,020 --> 11:49:23,490
let look at air pressure at 9 am 

42
11:49:23,490 --> 11:49:28,867
we can see the summary statistics for air pressure 9 am by running

43
11:49:28,867 --> 11:49:33,868
df describe air pressure 9am show 

44
11:49:38,531 --> 11:49:44,860
there are five statistics in this output the count the number of rows 

45
11:49:44,860 --> 11:49:50,300
the mean the standard deviation and the min and max values 

46
11:49:50,300 --> 11:49:56,872
we can see the total number of columns in the dataframe by running len df columns 

47
11:50:00,056 --> 11:50:06,214
we can also see the total number of rows in the dataframe by writing df count 

48
11:50:06,214 --> 11:50:10,502
this says that there are 1 095 rows in the dataframe 

49
11:50:10,502 --> 11:50:15,393
however the summary statistics for air pressure 9am 

50
11:50:15,393 --> 11:50:17,889
we see the count is 1 092 

51
11:50:19,800 --> 11:50:22,882
summary statistics do not include rows of missing values 

52
11:50:22,882 --> 11:50:27,587
this means that there are three rows in the air pressure 9am column that have

53
11:50:27,587 --> 11:50:28,630
missing values 

54
11:50:28,630 --> 11:50:31,066
we can drop these missing values 

55
11:50:31,066 --> 11:50:34,670
let create a new dataframe where we have dropped these missing values 

56
11:50:36,370 --> 11:50:39,572
we will call the new dataframe df2 

57
11:50:39,572 --> 11:50:44,773
to drop the missing values we will enter

58
11:50:44,773 --> 11:50:52,656
df na drop subset = air pressure 9am 

59
11:50:52,656 --> 11:50:57,779
we can then count the total number of rows in the new dataframe df2 count 

60
11:51:00,551 --> 11:51:05,208
we see this value agrees with our earlier value of 1092 

61
11:51:05,208 --> 11:51:10,772
next let compute the correlation between two columns in the dataframe 

62
11:51:10,772 --> 11:51:15,485
we will compute the correlation between rain accumulation and rain duration 

63
11:51:15,485 --> 11:51:21,144
to do this we will enter df2 stat corr and

64
11:51:21,144 --> 11:51:25,245
then the names of the two columns 

65
11:51:25,245 --> 11:51:32,182
rain accumulation 9am and rain duration 9am 

1
23:37:43,100 --> 23:37:47,060
visualizing your data is a very effective way to explore your data 

2
23:37:47,060 --> 23:37:50,780
we will look at different ways to visualize your data in this lecture 

3
23:37:50,780 --> 23:37:51,950
after this video 

4
23:37:51,950 --> 23:37:57,150
you will be able to discuss how plots can be useful in exploring data 

5
23:37:57,150 --> 23:38:01,669
describe how you would use a scatter plot and summarize what a boxplot shows 

6
23:38:02,840 --> 23:38:06,390
visualizing data that is looking at data graphically 

7
23:38:06,390 --> 23:38:08,800
is a great way to explore your data set 

8
23:38:08,800 --> 23:38:12,505
data visualization is a nice complement to using summary statistics for

9
23:38:12,505 --> 23:38:13,422
exploring data 

10
23:38:13,422 --> 23:38:17,358
we will cover several ways to visualize your data in this lecture 

11
23:38:17,358 --> 23:38:22,080
there are several types of plots that you can use to visualize your data 

12
23:38:22,080 --> 23:38:29,350
we will go over histogram line plot scatter plot bar plot and box plot 

13
23:38:29,350 --> 23:38:32,550
these are the most commonly used plots but there are many others as well 

14
23:38:33,750 --> 23:38:37,930
a histogram is used to display the distribution of a variable 

15
23:38:37,930 --> 23:38:42,250
the range of values for the variable is divided into the number of bins and

16
23:38:42,250 --> 23:38:45,610
the number of values that fall into each bin is counted 

17
23:38:45,610 --> 23:38:47,660
which determines the height of each bin 

18
23:38:49,410 --> 23:38:53,600
a histogram can reveal many things about a variable in your data for

19
23:38:53,600 --> 23:38:58,200
example you can usually determine the central tendency of a variable 

20
23:38:58,200 --> 23:39:01,060
that is where the majority of the values lie 

21
23:39:01,060 --> 23:39:03,989
you can also see the most frequent value of values for that variable 

22
23:39:05,280 --> 23:39:09,940
a histogram also shows whether the values for that variable are skewed and

23
23:39:09,940 --> 23:39:12,920
whether the skewness is to the left towards smaller values or

24
23:39:12,920 --> 23:39:14,810
to the right towards larger values 

25
23:39:15,860 --> 23:39:19,340
you can also pick outliers in the histogram as shown on the bottom plot 

26
23:39:20,820 --> 23:39:24,460
a line plot shows how data values change over time 

27
23:39:24,460 --> 23:39:28,910
the values of a variable or variables are shown on the y axis and

28
23:39:28,910 --> 23:39:31,780
the x axis shows the motion of time 

29
23:39:31,780 --> 23:39:34,810
the resulting line displays the data values over time 

30
23:39:36,070 --> 23:39:39,780
a line plot can show patterns in your variables 

31
23:39:39,780 --> 23:39:44,840
for example a cyclical pattern can be detected as in this plot 

32
23:39:44,840 --> 23:39:48,720
where the values start high then decrease and go back up again 

33
23:39:49,870 --> 23:39:54,060
trends can also be detected as shown in the upper - right plot

34
23:39:54,060 --> 23:39:58,550
where the values fluctuate but show a general upward trend over time 

35
23:39:59,630 --> 23:40:04,030
it is also easy to compare how multiple variables change over time on a single

36
23:40:04,030 --> 23:40:06,930
line plot as displayed in the center bottom plot 

37
23:40:08,500 --> 23:40:13,360
a scatter plot is a great way to visualize the relationship between two variables 

38
23:40:13,360 --> 23:40:15,830
one variable is on the x axis 

39
23:40:15,830 --> 23:40:19,530
the other variable is on the y axis each sample is

40
23:40:19,530 --> 23:40:24,010
a product using the values of the 2 variables aspects and y coordinates 

41
23:40:24,010 --> 23:40:28,590
the resulting plot shows how one variable changes as the other is changed 

42
23:40:29,870 --> 23:40:34,220
a scatter plot can be used to display the correlation between 2 variables 

43
23:40:34,220 --> 23:40:38,090
for example 2 variables such as the high temperature of the day and

44
23:40:38,090 --> 23:40:39,240
the low temperature of the day 

45
23:40:39,240 --> 23:40:41,920
can have a positive correlation as shown in this plot 

46
23:40:43,080 --> 23:40:47,200
a positive correlation means that as the value of one variable increases 

47
23:40:47,200 --> 23:40:52,205
the value of the other variable also increases by a similar amount 

48
23:40:52,205 --> 23:40:55,745
the upper right scatter plot shows a negative correlation between two

49
23:40:55,745 --> 23:40:57,015
variables 

50
23:40:57,015 --> 23:41:00,285
this means that as the value of one variable increases 

51
23:41:00,285 --> 23:41:04,730
there is a corresponding decrease in the other variable two variables

52
23:41:04,730 --> 23:41:09,440
can also have a non - linear correlation as shown in the lower left plot 

53
23:41:09,440 --> 23:41:12,040
this means that a change in one variable will not

54
23:41:12,040 --> 23:41:15,550
always correspond to the same change in the other variable 

55
23:41:15,550 --> 23:41:19,010
this is indicated by the curve in the scatter plot as opposed to something

56
23:41:19,010 --> 23:41:21,640
closer to a straight line for linear correlation 

57
23:41:22,750 --> 23:41:25,890
there can also be no correlation between two variables 

58
23:41:25,890 --> 23:41:28,910
in this case you will see something like randomly placed dots as

59
23:41:28,910 --> 23:41:31,730
displayed in the lower right plot indicating no

60
23:41:31,730 --> 23:41:35,250
relationship between how the two variables change with respect to each other 

61
23:41:36,580 --> 23:41:40,980
a bar plot is used to show the distribution of categorical variables 

62
23:41:40,980 --> 23:41:44,980
recall that a histogram is also used to look at the distribution of the values

63
23:41:44,980 --> 23:41:46,450
of the variable 

64
23:41:46,450 --> 23:41:49,860
the difference is that in general a histogram is used for

65
23:41:49,860 --> 23:41:54,990
numeric variables whereas a bar plot is used for categorical variables 

66
23:41:54,990 --> 23:41:59,850
in a bar chart the different categories of a categorical variable is shown along

67
23:41:59,850 --> 23:42:05,870
the x - axis and the count of instances for each category is displayed on the y - axis 

68
23:42:05,870 --> 23:42:09,670
this is an effective way to compare the different categories 

69
23:42:09,670 --> 23:42:12,700
for example the most frequent category can be easily determined 

70
23:42:14,090 --> 23:42:18,790
a bar plot is also a great way to compare two categorical variables 

71
23:42:18,790 --> 23:42:22,630
for example this plot compares two categorical variables 

72
23:42:22,630 --> 23:42:27,270
one in blue and the other in orange each with three different categories 

73
23:42:27,270 --> 23:42:29,638
here you can see that for the first category 

74
23:42:29,638 --> 23:42:31,757
the blue variable has the higher count 

75
23:42:31,757 --> 23:42:35,949
while the orange variable has a higher count for the second and third category 

76
23:42:35,949 --> 23:42:39,642
this type of bar plot is called a grouped bar chart 

77
23:42:39,642 --> 23:42:42,480
and the different variables of products side by side 

78
23:42:43,830 --> 23:42:47,580
a different kind of comparison can be performed using a stacked bar chart

79
23:42:47,580 --> 23:42:50,040
as seen in a lower right quad 

80
23:42:50,040 --> 23:42:53,460
here the accounts for the two variables are stacked on top of each other for

81
23:42:53,460 --> 23:42:54,160
each category 

82
23:42:55,290 --> 23:42:58,700
with this bar chart you can determine that the combined count for

83
23:42:58,700 --> 23:43:03,830
the first category is about equal to the combine count for the second category 

84
23:43:03,830 --> 23:43:07,000
while the compliant count for the third category is much larger 

85
23:43:08,140 --> 23:43:13,140
a box plot is another plot that shows the distribution of a numeric variable 

86
23:43:13,140 --> 23:43:16,510
it shows the distribution in a different format than the histogram however 

87
23:43:17,870 --> 23:43:20,990
this is how a box plot displays the distribution of values for

88
23:43:20,990 --> 23:43:25,530
a variable the gray portion in the figure is the box part 

89
23:43:25,530 --> 23:43:30,110
the lower and upper boundaries of the box represent the 25th and

90
23:43:30,110 --> 23:43:32,830
75th percentiles respectively 

91
23:43:32,830 --> 23:43:36,480
this means that the box represents the middle 50 of the data 

92
23:43:36,480 --> 23:43:41,430
the median is the 50th percentile meaning that 50 of

93
23:43:41,430 --> 23:43:46,560
the data is greater than its value and 50 of the data is less than this value 

94
23:43:47,600 --> 23:43:52,540
the top and bottom lines are the whiskers and represent the 10th and

95
23:43:52,540 --> 23:43:54,890
90th percentiles respectively 

96
23:43:54,890 --> 23:43:59,210
so 80 of the data are in the region indicated by the upper extreme and

97
23:43:59,210 --> 23:44:00,710
lower extreme 

98
23:44:00,710 --> 23:44:03,790
any data values outside of this region are outliers and

99
23:44:03,790 --> 23:44:06,080
are indicated as single point on the box plot 

100
23:44:07,410 --> 23:44:10,600
note that there are different variations of the box plot 

101
23:44:10,600 --> 23:44:14,520
with the whiskers representing different types of extreme values 

102
23:44:14,520 --> 23:44:18,700
box plots provide a compact way to show how variables are distributed so

103
23:44:18,700 --> 23:44:21,660
they are often used to compare variables 

104
23:44:21,660 --> 23:44:22,920
the box plot on the left for

105
23:44:22,920 --> 23:44:26,540
example compares the base salary for two different roles 

106
23:44:26,540 --> 23:44:30,720
this plot can quickly provide information regarding the median value the range and

107
23:44:30,720 --> 23:44:33,950
the spread of the two different variables 

108
23:44:33,950 --> 23:44:36,200
we can quickly see that the median salary for

109
23:44:36,200 --> 23:44:39,160
the marketing role is higher than the research role 

110
23:44:40,170 --> 23:44:43,210
we can also see that the variation or spread of the values for

111
23:44:43,210 --> 23:44:48,240
marketing is greater than for research due to the larger area of the purple box 

112
23:44:49,870 --> 23:44:53,380
a box plot can also show you if the distribution of the data values is

113
23:44:53,380 --> 23:44:57,740
symmetrical positively skewed or negatively skewed 

114
23:44:57,740 --> 23:45:01,480
here we see that a box plot can also be displayed on its side 

115
23:45:01,480 --> 23:45:04,890
a symmetric distribution is indicated if the line in the box

116
23:45:04,890 --> 23:45:09,090
which specifies the median is in the center of the box 

117
23:45:09,090 --> 23:45:12,430
a negative skew is indicated when the median is to the right

118
23:45:12,430 --> 23:45:14,030
of the center of the box 

119
23:45:14,030 --> 23:45:17,470
this means that there are more values that are less than the median

120
23:45:17,470 --> 23:45:19,010
than there are values greater than the median 

121
23:45:20,140 --> 23:45:24,310
similarly a positive skew is indicated when the median is to the left

122
23:45:24,310 --> 23:45:25,270
of the center of the box 

123
23:45:26,455 --> 23:45:29,370
to summarize data visualization provides a quick and

124
23:45:29,370 --> 23:45:31,910
intuitive way to examine your data 

125
23:45:31,910 --> 23:45:34,680
data visualization should be used in conjunction with summary

126
23:45:34,680 --> 23:45:38,420
statistics that we discussed in the last lecture to explore data 

127
23:45:38,420 --> 23:45:42,210
the different types of plots that we have covered in this lecture will also be very

128
23:45:42,210 --> 23:45:45,960
helpful in communicating your results throughout your machine learning project 

1
23:23:27,950 --> 23:23:31,820
let look at how we can use summary statistics to explore data in more detail 

2
23:23:33,320 --> 23:23:38,740
after this video you will be able to define what a summary statistic is 

3
23:23:38,740 --> 23:23:41,610
list three common summary statistics and

4
23:23:41,610 --> 23:23:44,930
explain how summary statistics are useful in exploring data 

5
23:23:46,830 --> 23:23:51,096
summary statistics are quantities that describe a set of data values 

6
23:23:51,096 --> 23:23:56,750
summary statistics provide a simple and quick way to summarize a dataset 

7
23:23:56,750 --> 23:24:00,270
we will discuss three main categories of summary statistics 

8
23:24:00,270 --> 23:24:05,650
measures of location or centrality measures of spread and measures of shape 

9
23:24:07,680 --> 23:24:11,970
measures of location are summary statistics that describe the central or

10
23:24:11,970 --> 23:24:14,670
typical value in your dataset 

11
23:24:14,670 --> 23:24:18,700
these statistics give a sense of the middle or center of the dataset 

12
23:24:19,730 --> 23:24:23,870
examples of these are mean median and mode 

13
23:24:23,870 --> 23:24:27,720
the mean is just the average of the values in a dataset 

14
23:24:27,720 --> 23:24:32,198
the median is the value in the middle if you sorted the values in your dataset 

15
23:24:32,198 --> 23:24:37,300
in a sorted list half of the values will be less than the median and

16
23:24:37,300 --> 23:24:38,930
half will be greater than the median 

17
23:24:40,200 --> 23:24:42,700
if the number of data values is even 

18
23:24:42,700 --> 23:24:45,609
then the median is the mean of the two middle values 

19
23:24:46,620 --> 23:24:50,550
the mode is a value that is repeated more often than any other value 

20
23:24:52,660 --> 23:24:56,290
in this example we have a dataset with ten values 

21
23:24:56,290 --> 23:24:57,667
for this dataset 

22
23:24:57,667 --> 23:25:03,010
the mean is 51 1 which is the number of all the values divided by 10 

23
23:25:03,010 --> 23:25:10,380
the median is 46 if you sort these numbers the middle numbers are 42 and 50 

24
23:25:10,380 --> 23:25:14,151
the average of these two numbers is 46 

25
23:25:14,151 --> 23:25:18,054
there are two modes for the this dataset 42 and 78 

26
23:25:18,054 --> 23:25:22,705
since each occurs twice more than any other value in the dataset 

27
23:25:24,480 --> 23:25:29,187
measures of spread describe how dispersed or varied your dataset is 

28
23:25:29,187 --> 23:25:33,764
common measures of spread are minimum maximum range 

29
23:25:33,764 --> 23:25:37,450
standard deviation and variance 

30
23:25:37,450 --> 23:25:40,440
minimum and maximum are of course the smallest and

31
23:25:40,440 --> 23:25:43,740
largest values in your dataset respectively 

32
23:25:43,740 --> 23:25:48,070
the range is simply the difference between the maximum and minimum and

33
23:25:48,070 --> 23:25:51,020
tells you how spread out your data is 

34
23:25:51,020 --> 23:25:54,785
standard deviation describes the amount of variation in your dataset 

35
23:25:55,920 --> 23:25:59,620
a low standard deviation value means that the samples in your dataset

36
23:25:59,620 --> 23:26:01,630
tend to be close to the mean 

37
23:26:01,630 --> 23:26:06,050
and a high standard deviation value means that the data samples are spread out 

38
23:26:06,050 --> 23:26:09,480
variance is closely related to standard deviation 

39
23:26:09,480 --> 23:26:13,530
in fact the variance is the square of the standard deviation 

40
23:26:13,530 --> 23:26:18,900
so it also indicates how spread out the data samples are from the mean 

41
23:26:18,900 --> 23:26:23,780
for the same dataset the range is 66 which is a difference between the largest

42
23:26:23,780 --> 23:26:27,964
number which is 87 and the smallest number which is 21 

43
23:26:27,964 --> 23:26:31,620
the variance is 548 767 

44
23:26:31,620 --> 23:26:36,770
you can calculate this using a calculator or a spreadsheet 

45
23:26:36,770 --> 23:26:42,030
and the standard deviation is 23 426 which is the square root of the variance 

46
23:26:43,680 --> 23:26:48,470
measures of shape describe the shape of the distribution of a set of values 

47
23:26:48,470 --> 23:26:51,740
common members of shape are skewness and kurtosis 

48
23:26:52,790 --> 23:26:57,150
skewness indicates whether the data values are asymmetrically distributed 

49
23:26:58,150 --> 23:27:02,020
a skewness value of around zero indicates that the data distribution

50
23:27:02,020 --> 23:27:05,760
is approximately normal as shown in the middle figure in the top diagram 

51
23:27:06,830 --> 23:27:11,280
a negative skewness value indicates that the distribution is skewed to the left 

52
23:27:11,280 --> 23:27:14,590
as indicated in the left figure in the top diagram 

53
23:27:14,590 --> 23:27:17,200
a positive skewness value on the other hand

54
23:27:17,200 --> 23:27:20,230
indicates that the data distribution is skewed to the right 

55
23:27:21,340 --> 23:27:25,430
kurtosis measures the tailedness of the data distribution or

56
23:27:25,430 --> 23:27:29,170
how heavy or fat the tails of the distribution are 

57
23:27:29,170 --> 23:27:34,040
a high kurtosis value describes a distribution with longer and fatter tails

58
23:27:34,040 --> 23:27:39,075
and a higher and sharper central peak indicating the presence of outliers 

59
23:27:39,075 --> 23:27:41,692
a low kurtosis value on the other hand 

60
23:27:41,692 --> 23:27:46,376
describes a distribution with shorter and lighter tails and lower and

61
23:27:46,376 --> 23:27:50,280
broader central peak suggesting the lack of outliers 

62
23:27:52,140 --> 23:27:58,740
in our age example the skewness is about 0 3 indicating a slight positive skew 

63
23:27:58,740 --> 23:28:04,210
and the kurtosis is - 1 2 indicating a distribution with a low and

64
23:28:04,210 --> 23:28:06,940
broad central peak and shorter and lighter tails 

65
23:28:08,080 --> 23:28:13,210
measures of dependence determine if any relationship exists between variables 

66
23:28:13,210 --> 23:28:16,560
pairwise correlation is a commonly used measure of dependence 

67
23:28:17,850 --> 23:28:22,480
this is a table that shows pairwise correlation for a set of variables 

68
23:28:22,480 --> 23:28:25,430
note that correlation applies only to numerical variables 

69
23:28:26,470 --> 23:28:31,517
correlations is between zero and one with zero indicating no correlation 

70
23:28:31,517 --> 23:28:34,430
and one indicating a one to one correlation 

71
23:28:34,430 --> 23:28:37,268
so a correlation of 0 89 is very strong and

72
23:28:37,268 --> 23:28:42,240
this is expected since a person height and weight should be very correlated 

73
23:28:43,780 --> 23:28:48,480
the summary statistics we just covered are useful for numerical variables 

74
23:28:48,480 --> 23:28:52,900
for categorical variables we want to look at statistics that describe the number of

75
23:28:52,900 --> 23:28:56,290
categories and the frequency of each category 

76
23:28:56,290 --> 23:28:58,400
this is done using a contingency table 

77
23:28:59,940 --> 23:29:02,860
here an example that shows a distribution of people pets and

78
23:29:02,860 --> 23:29:04,140
their colors 

79
23:29:04,140 --> 23:29:09,150
we can see the most common pet is a dog and least common a fish 

80
23:29:09,150 --> 23:29:12,948
similarly black is the most common color and orange the least common 

81
23:29:12,948 --> 23:29:18,240
the contingency table also shows the distribution between the categories 

82
23:29:18,240 --> 23:29:23,750
for example only fish are orange while most of the brown pets are dogs 

83
23:29:23,750 --> 23:29:26,810
in addition to looking at the traditional summary statistics for

84
23:29:26,810 --> 23:29:32,100
numerical variables and category count for categorical variables 

85
23:29:32,100 --> 23:29:33,370
for machine learning problems 

86
23:29:33,370 --> 23:29:37,450
we also want to examine some additional statistics to quickly validate the data 

87
23:29:38,760 --> 23:29:42,050
one of the first things to check is the number of rows and

88
23:29:42,050 --> 23:29:44,660
the number of columns in your dataset 

89
23:29:44,660 --> 23:29:48,080
does the number of rows match the expected number of samples 

90
23:29:48,080 --> 23:29:52,110
does the number of columns match the expected number of variables 

91
23:29:52,110 --> 23:29:54,000
these should be very quick and easy checks 

92
23:29:55,490 --> 23:29:59,770
another easy data validation check is to look at the values in the first and

93
23:29:59,770 --> 23:30:03,170
last few samples in your dataset to see if they are reasonable 

94
23:30:04,500 --> 23:30:08,780
for example do the temperature values looks to be in the right units of measure 

95
23:30:10,180 --> 23:30:12,514
do the values for rainfall look correct or

96
23:30:12,514 --> 23:30:15,053
are there some values that look out of place 

97
23:30:15,053 --> 23:30:18,447
are the data types for your variables correct for example 

98
23:30:18,447 --> 23:30:22,130
is the date field captured as dates or timestamp 

99
23:30:22,130 --> 23:30:24,926
or is it capture as a string or numerical value 

100
23:30:24,926 --> 23:30:28,770
these will have consequences in how these fields should be processed 

101
23:30:30,210 --> 23:30:33,490
another important step is to check for missing values 

102
23:30:33,490 --> 23:30:37,280
you need to determine the number of samples with missing values 

103
23:30:37,280 --> 23:30:40,060
you also need to determine if there are any variables

104
23:30:40,060 --> 23:30:41,964
with a high percentage of missing values 

105
23:30:43,155 --> 23:30:46,585
handling missing values is a very important step in data preparation

106
23:30:46,585 --> 23:30:48,260
which we will cover in the next module 

107
23:30:48,260 --> 23:30:52,875
having this information will be very helpful in determining how missing values

108
23:30:52,875 --> 23:30:54,915
should be handled in data preparation 

109
23:30:56,270 --> 23:31:00,190
we covered several types of summary statistics useful for exploring data and

110
23:31:00,190 --> 23:31:01,590
machine learning 

111
23:31:01,590 --> 23:31:06,020
the statistics provide useful information about your dataset and should be

112
23:31:06,020 --> 23:31:09,480
thoroughly examine if you want to get a better understanding of your data 

1
22:54:37,066 --> 22:54:41,080
now that we are familiar with some commonly use terms to describe data 

2
22:54:41,080 --> 22:54:44,010
let look at what data exploration is and why it important 

3
22:54:45,180 --> 22:54:49,610
after this video you will be able to explain why data

4
22:54:49,610 --> 22:54:54,760
exploration is necessary articulate the objectives of data exploration 

5
22:54:54,760 --> 22:54:56,920
list the categories of techniques for exploring data 

6
22:54:58,520 --> 22:55:01,160
data exploration means doing some preliminary

7
22:55:01,160 --> 22:55:03,300
investigation of your data set 

8
22:55:03,300 --> 22:55:07,860
the goal is to gain a better understanding of the data that you have to work with 

9
22:55:07,860 --> 22:55:12,120
if you understand the characteristics of your data you can make optimal use of it

10
22:55:12,120 --> 22:55:15,320
in whatever subsequent processing and analysis you do with the data 

11
22:55:16,690 --> 22:55:21,730
note that data exploration is also called exploratory data analysis or

12
22:55:21,730 --> 22:55:22,470
eda for short 

13
22:55:23,810 --> 22:55:25,780
how do you go about exploring data 

14
22:55:25,780 --> 22:55:29,150
there are two main categories of techniques to explore your data 

15
22:55:29,150 --> 22:55:33,130
one based on summary statistics and the other based on visualization methods 

16
22:55:34,570 --> 22:55:38,580
summary statistics provide important information that summarizes a set

17
22:55:38,580 --> 22:55:40,120
of data values 

18
22:55:40,120 --> 22:55:42,050
there are many such statistics 

19
22:55:42,050 --> 22:55:44,470
many of them you have probably heard of before 

20
22:55:44,470 --> 22:55:50,560
such as mean median and standard deviation 

21
22:55:50,560 --> 22:55:54,602
these are some very commonly used summary statistics 

22
22:55:54,602 --> 22:55:58,780
a summary statistic provides a single quantity that summarizes some aspects of

23
22:55:58,780 --> 22:56:00,090
the dataset 

24
22:56:00,090 --> 22:56:04,200
for example the mean is a single value that describes the average value

25
22:56:04,200 --> 22:56:07,630
of the dataset no matter how large that dataset is 

26
22:56:07,630 --> 22:56:12,140
you can think of the mean as an indicator of where your dataset is centrally located

27
22:56:12,140 --> 22:56:15,720
on a number line thus summary statistics provide a simple and

28
22:56:15,720 --> 22:56:17,285
quick way to summarize a dataset 

29
22:56:18,710 --> 22:56:23,000
data visualization techniques allow you to look at your data graphically 

30
22:56:23,000 --> 22:56:26,540
there are several types of plots that you can use to visualize your data 

31
22:56:26,540 --> 22:56:31,790
some examples are histogram line plot and scatter plot 

32
22:56:33,290 --> 22:56:37,590
each type of plot serves a different purpose we will cover the use of plots to

33
22:56:37,590 --> 22:56:39,710
visualize your data in an upcoming lecture 

34
22:56:41,190 --> 22:56:44,110
what should you look for when exploring your data 

35
22:56:44,110 --> 22:56:46,910
you use statistics and visual methods to summarize and

36
22:56:46,910 --> 22:56:50,240
describe your dataset and some of the things you will want to look for

37
22:56:50,240 --> 22:56:53,430
are correlations general trends and outliers 

38
22:56:54,900 --> 22:56:57,940
correlations provide information about the relation took between

39
22:56:57,940 --> 22:56:59,760
variables in your data 

40
22:56:59,760 --> 22:57:01,220
by looking at correlations 

41
22:57:01,220 --> 22:57:05,200
you may be able to determine that two variables are very correlated 

42
22:57:05,200 --> 22:57:09,500
this means they provide the same or similar information about your data 

43
22:57:09,500 --> 22:57:13,440
since this contain redundant information this suggest that you may want to

44
22:57:13,440 --> 22:57:16,220
remove one of the variables to make the analysis simpler 

45
22:57:17,780 --> 22:57:21,690
trends in your data will reveal characteristics in your data 

46
22:57:21,690 --> 22:57:25,940
for example you can see where the majority of the data values lie 

47
22:57:25,940 --> 22:57:27,220
whether your data is skilled or

48
22:57:27,220 --> 22:57:31,585
not what the most frequent value or values are in a date set etc 

49
22:57:32,960 --> 22:57:36,660
looking at trends in your data can also reveal that a variable is moving in

50
22:57:36,660 --> 22:57:41,380
a certain direction such as sales revenue increasing or decreasing over the years 

51
22:57:42,820 --> 22:57:44,954
calculating the minimum the maximum and

52
22:57:44,954 --> 22:57:48,910
range of the data values are basic steps in exploring your data 

53
22:57:48,910 --> 22:57:52,450
determining outliers is a also very important 

54
22:57:52,450 --> 22:57:55,340
outliers indicate potential problems with the data and

55
22:57:55,340 --> 22:57:57,560
may need to be eliminated in some applications 

56
22:57:58,660 --> 22:57:59,830
in other applications 

57
22:57:59,830 --> 22:58:04,560
outliers represent interesting data points that should be looked at more closely 

58
22:58:04,560 --> 22:58:07,680
in either case outliers usually require further examination 

59
22:58:08,850 --> 22:58:12,600
in summary what you get by exploring your data is a better understanding of

60
22:58:12,600 --> 22:58:16,826
the complexity of the data so you can work with it more effectively 

61
22:58:16,826 --> 22:58:20,560
better understanding in turn will guide the rest of the process and

62
22:58:20,560 --> 22:58:23,200
lead to more informed analysis 

63
22:58:23,200 --> 22:58:24,450
summary statistics and

64
22:58:24,450 --> 22:58:28,190
visualization techniques are essential in exploring your data 

65
22:58:28,190 --> 22:58:31,330
this should be used together to examined a dataset 

66
22:58:31,330 --> 22:58:32,523
in the next two lectures 

67
22:58:32,523 --> 22:58:35,895
we will look at a specific methods that you can apply to explore your data 

1
21:53:12,040 --> 21:53:15,230
in this lesson you will learn how to prepare data for analysis 

2
21:53:16,500 --> 21:53:20,950
after this video you will be able to articulate the importance of data

3
21:53:20,950 --> 21:53:25,480
preparation define the objectives of data preparation and

4
21:53:25,480 --> 21:53:27,310
list some activities in preparing data 

5
21:53:29,170 --> 21:53:33,040
the raw data that you get directly from your sources is rarely in the format

6
21:53:33,040 --> 21:53:35,790
that you can use to perform analysis on 

7
21:53:35,790 --> 21:53:40,070
the goal of data preparation is to create the data that will be used for analysis 

8
21:53:41,600 --> 21:53:47,240
this means cleaning the data and putting the data in the right format for analysis 

9
21:53:47,240 --> 21:53:51,470
the latter generally involves selecting the appropriate features to use and

10
21:53:51,470 --> 21:53:53,070
transforming the data as needed 

11
21:53:54,520 --> 21:53:57,950
the data that you have acquired will likely have many problems 

12
21:53:57,950 --> 21:54:01,940
an important part of data preparation is to clean the data that you have to work

13
21:54:01,940 --> 21:54:08,030
with to address what are referred to as data quality issues 

14
21:54:08,030 --> 21:54:12,049
there are many types of data quality issues 

15
21:54:12,049 --> 21:54:16,176
including missing values duplicate data 

16
21:54:16,176 --> 21:54:21,180
inconsistent or invalid data noise and outliers 

17
21:54:21,180 --> 21:54:25,310
the problems listed here can negatively affect the quality of the data and

18
21:54:25,310 --> 21:54:28,430
compromise the analysis process and results 

19
21:54:28,430 --> 21:54:32,139
so it is very important to detect and address these data quality issues 

20
21:54:33,370 --> 21:54:38,260
some techniques to address data quality issues include removing data records

21
21:54:38,260 --> 21:54:42,250
with missing values merging duplicate records 

22
21:54:42,250 --> 21:54:47,430
generating a best or at most reasonable estimate for invalid values 

23
21:54:47,430 --> 21:54:51,859
we will discuss these techniques in more detail in the next lecture 

24
21:54:51,859 --> 21:54:56,135
since the goal of the step of data preparation is cleaning the data 

25
21:54:56,135 --> 21:54:59,520
it is referred to as data cleaning or data cleansing 

26
21:55:01,030 --> 21:55:04,360
after the data has been cleaned another goal of data preparation is to

27
21:55:04,360 --> 21:55:07,720
get the data into the format needed for the analysis 

28
21:55:07,720 --> 21:55:12,709
this step is referred to by many names data munching data wrangling and

29
21:55:12,709 --> 21:55:14,220
data preprocessing 

30
21:55:15,930 --> 21:55:19,910
the two broad categories of data wrangling are feature selection and

31
21:55:19,910 --> 21:55:20,940
feature transformation 

32
21:55:22,040 --> 21:55:24,770
feature selection involves deciding on which features

33
21:55:24,770 --> 21:55:27,150
to use from the existing ones available in your data 

34
21:55:28,160 --> 21:55:31,860
features can be removed added or combined 

35
21:55:31,860 --> 21:55:34,770
feature transformation involves changing the format of the data

36
21:55:34,770 --> 21:55:40,610
in some way to reduce noise or variability or make the data easier to analyze 

37
21:55:40,610 --> 21:55:43,450
two common feature transformations are scaling the data so

38
21:55:43,450 --> 21:55:48,260
that all features have the same value range and reducing the dimensionality 

39
21:55:48,260 --> 21:55:51,490
which is effectively the number of features of the data 

40
21:55:51,490 --> 21:55:53,750
we will discuss these techniques later in this lesson 

41
21:55:55,440 --> 21:55:59,390
data preparation is a very important part of the machine learning process 

42
21:55:59,390 --> 21:56:02,840
it can be a tedious process but it is a crucial step 

43
21:56:02,840 --> 21:56:05,750
if you do not spend the time and effort to create good data for

44
21:56:05,750 --> 21:56:08,230
the analysis you will not get good results 

45
21:56:08,230 --> 21:56:12,200
no matter how sophisticated the analysis technique you are using is 

46
21:56:12,200 --> 21:56:15,530
always remember garbage in garbage out 

47
21:56:15,530 --> 21:56:19,189
so take the time to prepare your data if you want good analysis results 

1
19:49:31,220 --> 19:49:35,360
a very important part of data preparation is to assess the quality of your data 

2
19:49:35,360 --> 19:49:39,120
we will look at some common data quality issues in this lecture 

3
19:49:39,120 --> 19:49:44,660
after this video you will be able to describe three data quality issues 

4
19:49:44,660 --> 19:49:47,680
name three reasons for poor data quality and

5
19:49:47,680 --> 19:49:51,060
explain why data quality issues need to be addressed 

6
19:49:51,060 --> 19:49:53,820
real world data is often very messy so

7
19:49:53,820 --> 19:49:57,150
it a given fact that you will need to clean your data by identifying and

8
19:49:57,150 --> 19:50:00,660
addressing many issues that affect the quality of your data 

9
19:50:00,660 --> 19:50:03,130
let take a closer look at what these data quality issues are 

10
19:50:04,330 --> 19:50:07,770
a very common data quality issue is missing data 

11
19:50:07,770 --> 19:50:11,410
recall that a sample in your dataset typically contains several variables or

12
19:50:11,410 --> 19:50:15,170
features like name age and income 

13
19:50:15,170 --> 19:50:18,960
for some samples some of these variables may not have a value 

14
19:50:18,960 --> 19:50:23,420
these are referred to as missing values in the data 

15
19:50:23,420 --> 19:50:27,650
missing values are also referred to as n a for not available 

16
19:50:27,650 --> 19:50:30,810
so you will see n a and missing values used interchangeably 

17
19:50:31,910 --> 19:50:35,462
you may have missing values in your data if you have an optional field in your

18
19:50:35,462 --> 19:50:36,500
data set 

19
19:50:36,500 --> 19:50:41,471
for example the field age is often an optional field on a survey 

20
19:50:41,471 --> 19:50:45,590
also many people may choose not to provide a response for income 

21
19:50:45,590 --> 19:50:50,470
and so you will end up with missing values for the variable income in your data set 

22
19:50:50,470 --> 19:50:54,390
in some cases a variable may not be applicable to all cases 

23
19:50:54,390 --> 19:50:59,120
for example income may not be applicable to people who are retired or

24
19:50:59,120 --> 19:51:01,430
unemployed or to children 

25
19:51:01,430 --> 19:51:04,350
so you will not have an entry for income in all of your samples 

26
19:51:05,490 --> 19:51:09,930
you can also have missing values due to a data collecting device that malfunctions 

27
19:51:09,930 --> 19:51:13,240
a network problem that affects how the data was transmitted or

28
19:51:13,240 --> 19:51:17,120
something else that goes wrong during the data collection process itself or

29
19:51:17,120 --> 19:51:19,930
the process of transmitting the data or storing the data 

30
19:51:21,360 --> 19:51:25,680
duplicate data occurs when your data set has data objects that are duplicates or

31
19:51:25,680 --> 19:51:27,030
new duplicates of one another 

32
19:51:28,280 --> 19:51:31,090
an example of this is when there are two different records for

33
19:51:31,090 --> 19:51:33,470
the same customer with different addresses 

34
19:51:34,860 --> 19:51:36,590
this can come about for example 

35
19:51:36,590 --> 19:51:40,510
if a customer address has changed but the second address was simply

36
19:51:40,510 --> 19:51:45,160
added to this customer records instead of used to update the first address 

37
19:51:45,160 --> 19:51:48,220
duplicate data can occur when merging data from multiple sources 

38
19:51:49,470 --> 19:51:50,260
invalid or

39
19:51:50,260 --> 19:51:54,720
inconsistent data occurs when you have an impossible value for a variable 

40
19:51:54,720 --> 19:51:59,810
some common examples are when you have a six digit zip code the letters ab for

41
19:51:59,810 --> 19:52:03,570
state abbreviations or a negative numbers for age 

42
19:52:03,570 --> 19:52:06,460
these invalid data values can come about when there is a data

43
19:52:06,460 --> 19:52:08,860
entry error during data collection 

44
19:52:08,860 --> 19:52:11,460
for example if you allow people to type in their zip code and

45
19:52:11,460 --> 19:52:15,650
someone accidentally includes an extra digit to their five digit zip code 

46
19:52:15,650 --> 19:52:17,915
then you will end up with an invalid six digit zipcode 

47
19:52:19,240 --> 19:52:22,710
noise refers to anything that can distort your data 

48
19:52:22,710 --> 19:52:25,780
noise can be introduced during the data collection process or

49
19:52:25,780 --> 19:52:27,910
data transmission process 

50
19:52:27,910 --> 19:52:31,590
an example is buzzing in the background when an audio message is recorded

51
19:52:31,590 --> 19:52:34,920
due to background noise or a faulty microphone 

52
19:52:34,920 --> 19:52:38,470
another example is an overly bright image due to an incorrect light

53
19:52:38,470 --> 19:52:39,290
exposure setting 

54
19:52:40,690 --> 19:52:44,150
an outlier is a data sample with values that are considerably different

55
19:52:44,150 --> 19:52:46,550
than the rest of the other data samples in a data set 

56
19:52:47,670 --> 19:52:51,110
an example scenario that can create outliers is when there a sense of

57
19:52:51,110 --> 19:52:56,650
failure that causes values being recorded to be much higher or lower than normal 

58
19:52:56,650 --> 19:53:00,130
in this case you want to remove the outliers from your data 

59
19:53:00,130 --> 19:53:03,580
in other applications however such as fraud detection 

60
19:53:03,580 --> 19:53:08,080
outliers are the important samples that should be examined more closely 

61
19:53:08,080 --> 19:53:11,460
so depending on the application outliers may need to be removed or

62
19:53:11,460 --> 19:53:13,270
be kept for further analysis 

63
19:53:14,400 --> 19:53:17,000
if you simply ignore these data quality issues 

64
19:53:17,000 --> 19:53:21,300
any analysis that is performed will produce misleading results 

65
19:53:21,300 --> 19:53:24,220
in addition some implementations of analysis techniques

66
19:53:24,220 --> 19:53:27,030
cannot handle some of these problems such as missing values 

67
19:53:28,160 --> 19:53:31,231
so problems that we have discussed in this lecture need to be

68
19:53:31,231 --> 19:53:34,312
addressed before any meaningful analysis can be performed 

69
19:53:34,312 --> 19:53:35,813
we will discuss some techniques for

70
19:53:35,813 --> 19:53:37,970
handling data quality issues in the next lecture 

1
15:43:07,790 --> 15:43:10,560
if you have been in a conversation on machine learning 

2
15:43:10,560 --> 15:43:14,680
you have probably heard terms like feature sample and variable 

3
15:43:14,680 --> 15:43:16,950
we will be defining some of those terms in this lecture 

4
15:43:18,930 --> 15:43:23,580
after this video you will be able to describe what a feature is and

5
15:43:23,580 --> 15:43:25,740
how it relates to a sample 

6
15:43:25,740 --> 15:43:28,480
name some alternative terms for feature 

7
15:43:28,480 --> 15:43:32,100
summarize how a categorical feature differs from a numerical feature 

8
15:43:35,170 --> 15:43:37,300
before we delve into the methods for processing and

9
15:43:37,300 --> 15:43:41,690
analyzing data let first start with defining some term used to describe data 

10
15:43:41,690 --> 15:43:43,610
starting with sample and variable 

11
15:43:45,100 --> 15:43:49,240
a sample is an instance or example of an entity in your data 

12
15:43:49,240 --> 15:43:52,150
this is typically a row in your dataset 

13
15:43:52,150 --> 15:43:55,380
this figure shows part of a dataset of values related to weather 

14
15:43:56,670 --> 15:44:01,950
each row is a sample representing weather data for particular day 

15
15:44:01,950 --> 15:44:06,370
the table in the figure shows four samples of weather data each for different day 

16
15:44:07,960 --> 15:44:12,410
in this table each sample has five values associated with it 

17
15:44:12,410 --> 15:44:16,430
these values are different information pieces about the sample

18
15:44:16,430 --> 15:44:20,200
such as the sample id sample date 

19
15:44:20,200 --> 15:44:24,660
minimum temperature maximum temperature and rainfall on that day 

20
15:44:25,820 --> 15:44:28,630
we call these different values variables of the sample 

21
15:44:31,020 --> 15:44:33,920
there are many names for sample and variable 

22
15:44:33,920 --> 15:44:38,600
some other terms for sample that you might hear in a machine learning

23
15:44:38,600 --> 15:44:44,470
context include record example row instance and observation 

24
15:44:44,470 --> 15:44:47,600
it is helpful to realize that all of these terms mean the same thing in

25
15:44:47,600 --> 15:44:48,830
machine learning 

26
15:44:48,830 --> 15:44:53,360
that is they all refer to a specific example of an entity in your dataset 

27
15:44:54,970 --> 15:45:00,400
there are also many names for the term variable such as feature column 

28
15:45:00,400 --> 15:45:02,450
dimension attribute and field 

29
15:45:02,450 --> 15:45:06,330
all of these terms refer to specific characteristics for

30
15:45:06,330 --> 15:45:07,660
each sample in your dataset 

31
15:45:09,680 --> 15:45:12,540
an important point to emphasize about variable is that 

32
15:45:12,540 --> 15:45:15,380
they are additional values with a data type 

33
15:45:15,380 --> 15:45:18,550
each variable has a data type associated with it 

34
15:45:18,550 --> 15:45:21,630
the most common data types are numeric and categorical 

35
15:45:22,770 --> 15:45:26,080
there are other data types as well such as string and date but

36
15:45:26,080 --> 15:45:30,260
we will focus on two of the more common data types numeric and categorical 

37
15:45:32,380 --> 15:45:37,720
as the name implies numeric variables are variables that take on number values 

38
15:45:37,720 --> 15:45:42,320
numeric variables can be measured and their values can be sorted in some way 

39
15:45:42,320 --> 15:45:46,120
note that a numeric variable can take on just integer values or

40
15:45:46,120 --> 15:45:47,950
be continuous valued 

41
15:45:47,950 --> 15:45:51,600
it can also have just positive numbers negative numbers or both 

42
15:45:52,990 --> 15:45:55,750
let go over some examples of various numeric variables 

43
15:45:56,830 --> 15:46:01,570
a person height is a positive continuous valued number 

44
15:46:01,570 --> 15:46:07,220
the score in an exam is a positive number that range between zero and a 100 

45
15:46:07,220 --> 15:46:11,280
the number of transactions per hour is a positive integer 

46
15:46:11,280 --> 15:46:15,030
whereas the change in a stock price can be either positive or negative 

47
15:46:17,170 --> 15:46:20,650
a variable with labels names or categories for

48
15:46:20,650 --> 15:46:24,070
values instead of numbers are called categorical variables 

49
15:46:25,120 --> 15:46:29,250
for example a variable that describes the color of an item such as the color of

50
15:46:29,250 --> 15:46:35,948
a car can have values such as red silver blue white and black 

51
15:46:35,948 --> 15:46:39,140
these are non - numeric values that describes some quality or

52
15:46:39,140 --> 15:46:40,650
characteristic of an entity 

53
15:46:41,830 --> 15:46:46,670
these values can be thought of as names or labels that can be sorted into categories 

54
15:46:46,670 --> 15:46:51,450
therefore categorical variables are also referred to as qualitative variables or

55
15:46:51,450 --> 15:46:52,650
nominal variables 

56
15:46:54,360 --> 15:46:59,340
some examples of categorical variables are gender marital status 

57
15:46:59,340 --> 15:47:02,640
type of customer for example teenager adult senior 

58
15:47:03,650 --> 15:47:08,310
product categories for example electronics kitchen bathroom and

59
15:47:08,310 --> 15:47:09,200
color of an item 

60
15:47:11,110 --> 15:47:15,790
to summarize a sample is an instance or example of an entity in your data 

61
15:47:15,790 --> 15:47:20,190
a variable captures a specific characteristic of each entity 

62
15:47:20,190 --> 15:47:23,800
so a sample has many variables to describe it 

63
15:47:23,800 --> 15:47:26,786
data from real applications are often multidimensional 

64
15:47:26,786 --> 15:47:31,240
meaning that there are many dimensions or variables describing each sample 

65
15:47:31,240 --> 15:47:34,430
each variable has a data type associated with it 

66
15:47:34,430 --> 15:47:38,280
the most common data types are numeric and categorical 

67
15:47:38,280 --> 15:47:41,633
note that there are many terms to describe these data related concepts 

1
07:30:48,790 --> 07:30:53,070
the data used in machine learning processes often have many variables 

2
07:30:53,070 --> 07:30:56,413
this is what we call highly dimensional data 

3
07:30:56,413 --> 07:31:00,815
most of these dimensions may or may not matter in the context of our application

4
07:31:00,815 --> 07:31:02,760
with the questions we are asking 

5
07:31:02,760 --> 07:31:07,288
reducing such high dimensions to a more manageable set of related and

6
07:31:07,288 --> 07:31:12,222
useful variables improves the performance and accuracy of our analysis 

7
07:31:12,222 --> 07:31:18,365
after this video you will be able to explain what dimensionality reduction is 

8
07:31:18,365 --> 07:31:22,639
discuss the benefits of dimensionality reduction and

9
07:31:22,639 --> 07:31:25,678
describe how pca transforms your data 

10
07:31:25,678 --> 07:31:30,465
the number of features or variables you have in your data set determines

11
07:31:30,465 --> 07:31:34,391
the number of dimensions or dimensionality of your data 

12
07:31:34,391 --> 07:31:38,857
if your dataset has two features than it is two dimensional data 

13
07:31:38,857 --> 07:31:43,454
if it has three features than it has three features and so on 

14
07:31:43,454 --> 07:31:48,090
you want to use as many features as possible to capture the characteristics of

15
07:31:48,090 --> 07:31:49,016
your data but

16
07:31:49,016 --> 07:31:53,105
you also do not want the dimension audio of your data to be too high 

17
07:31:53,105 --> 07:31:58,844
as the dimensionality increases the problem spaces you are looking at increases

18
07:31:58,844 --> 07:32:04,197
requiring substantially more instances to adequately sample of that space 

19
07:32:04,197 --> 07:32:06,805
so as the dimensionality increases 

20
07:32:06,805 --> 07:32:10,640
the space that you are looking at grows exponentially 

21
07:32:10,640 --> 07:32:14,460
as the space grows data becomes increasingly sparse 

22
07:32:14,460 --> 07:32:18,782
in this diagram we see how the problem space grows as

23
07:32:18,782 --> 07:32:23,011
the dimensionality increases from 1 to 2 to 3 

24
07:32:23,011 --> 07:32:27,915
in the left plot we have a one dimensional space partitioned into four

25
07:32:27,915 --> 07:32:31,150
regions each with size of 5 units 

26
07:32:31,150 --> 07:32:36,249
the middle plot shows a two dimensional space with 5x5 regions 

27
07:32:36,249 --> 07:32:40,429
the number of regions has now going from 4 to 16 

28
07:32:40,429 --> 07:32:46,911
in the third plot the problem space is three dimensional with 5x5x5 regions 

29
07:32:46,911 --> 07:32:51,171
the number of regions increased even more to 64 

30
07:32:51,171 --> 07:32:56,364
we see that as the number of dimensions increases the number of regions

31
07:32:56,364 --> 07:33:01,403
increases exponentially and the data becomes increasingly sparse 

32
07:33:01,403 --> 07:33:07,230
with a small dataset relative to the problem space analysis results degrade 

33
07:33:07,230 --> 07:33:11,927
in addition certain calculations used in analysis become much more difficult to

34
07:33:11,927 --> 07:33:14,018
define and calculate effectively 

35
07:33:14,018 --> 07:33:17,998
for example distances between samples are harder to compare since all samples

36
07:33:17,998 --> 07:33:20,330
are far away from each other 

37
07:33:20,330 --> 07:33:24,784
all of these challenges represent the difficulty of dealing with high

38
07:33:24,784 --> 07:33:29,172
dimensional data and as referred to as the curse of dimensionality 

39
07:33:29,172 --> 07:33:31,632
to avoid the curse of dimensionality 

40
07:33:31,632 --> 07:33:34,918
you want to reduce the dimensionality of your data 

41
07:33:34,918 --> 07:33:39,095
this means finding a smaller subset of features that can effectively capture

42
07:33:39,095 --> 07:33:41,030
the characteristics of your data 

43
07:33:42,050 --> 07:33:46,350
recall from the lecture on feature selection part of data preparation is to

44
07:33:46,350 --> 07:33:47,996
select the features to use 

45
07:33:47,996 --> 07:33:53,081
for example you can a feature that is very correlated with another feature 

46
07:33:53,081 --> 07:33:56,021
using feature selection techniques to select assessitive

47
07:33:56,021 --> 07:33:58,850
features is one approach to dimensionality reduction 

48
07:34:00,220 --> 07:34:04,160
another approach to dimensionality reduction is to mathematically determine

49
07:34:04,160 --> 07:34:08,230
the most important dimension to keep and ignore the rest 

50
07:34:08,230 --> 07:34:11,260
the idea is to find a smallest subset of dimensions that

51
07:34:11,260 --> 07:34:13,670
capture the most variation in your data 

52
07:34:14,850 --> 07:34:19,734
this reduces the dimensions of the data while eliminating the relevant features

53
07:34:19,734 --> 07:34:22,263
making the subsequent analysis simple 

54
07:34:22,263 --> 07:34:27,188
a technique commonly use to find the subset of most important dimensions is

55
07:34:27,188 --> 07:34:31,028
called principal component analysis or pca for short 

56
07:34:31,028 --> 07:34:35,997
the goal of pca is to map the data from the original high dimensional space

57
07:34:35,997 --> 07:34:40,722
to a lower dimensional space that captures as much of the variation in

58
07:34:40,722 --> 07:34:42,282
the data as possible 

59
07:34:42,282 --> 07:34:43,444
in other words 

60
07:34:43,444 --> 07:34:48,760
pca aims to find the most useful subset of dimensions to summarize the data 

61
07:34:50,380 --> 07:34:52,330
this plot illustrates the idea behind pca 

62
07:34:53,560 --> 07:34:58,200
here we have data samples in a two dimensional space that is defined

63
07:34:58,200 --> 07:35:00,044
by the x axis and the y axis 

64
07:35:00,044 --> 07:35:05,168
you can see that most of the variation in the data lies along the red diagonal line 

65
07:35:05,168 --> 07:35:09,527
this means that the dat samples are best differentiated along this dimension

66
07:35:09,527 --> 07:35:14,730
because they are spread out not clumped together along this dimension 

67
07:35:14,730 --> 07:35:19,577
this dimension indicated by the red line is the first principle component

68
07:35:19,577 --> 07:35:21,538
labelled as pc1 in the part 

69
07:35:21,538 --> 07:35:27,130
it captures the large amount of variance along a single dimension in data 

70
07:35:27,130 --> 07:35:32,305
pc1 indicated by the red line does not correspond to either axis 

71
07:35:32,305 --> 07:35:36,450
the next principle component is determined by looking in the direction that is

72
07:35:36,450 --> 07:35:40,972
orthogonal in other words perpendicular to the first principle component which

73
07:35:40,972 --> 07:35:44,800
captures the next largest amount of variance in the data 

74
07:35:44,800 --> 07:35:48,042
this is the second principal component pc2 and

75
07:35:48,042 --> 07:35:51,212
it indicated by the green line in the plot 

76
07:35:51,212 --> 07:35:56,198
this process can be repeated to find as many principal components as desired 

77
07:35:56,198 --> 07:36:00,726
note that the principal components do not align with either the x - axis or

78
07:36:00,726 --> 07:36:01,610
the y - axis 

79
07:36:01,610 --> 07:36:05,720
and that they are orthogonal in other words perpendicular to each other 

80
07:36:05,720 --> 07:36:07,489
this is what pca does 

81
07:36:07,489 --> 07:36:10,693
it finds the underlined dimensions the principal

82
07:36:10,693 --> 07:36:15,223
components that capture as much of the variation in the data as possible 

83
07:36:15,223 --> 07:36:19,853
these principal components form a new coordinates system to transform

84
07:36:19,853 --> 07:36:24,567
the data to instead of the conventional dimensions like x y and z 

85
07:36:24,567 --> 07:36:28,320
so how does pca help with dimensionality reduction 

86
07:36:28,320 --> 07:36:31,817
let look again in this plot with the first principle component 

87
07:36:31,817 --> 07:36:35,846
since the first principle component captures most of the variations in

88
07:36:35,846 --> 07:36:40,342
the data the original data sample can be mapped to this dimension indicated by

89
07:36:40,342 --> 07:36:43,045
the red line with minimum loss of information 

90
07:36:43,045 --> 07:36:46,645
in this case then we map a two - dimensional dataset to

91
07:36:46,645 --> 07:36:51,487
a one - dimensional space while keeping as much information as possible 

92
07:36:51,487 --> 07:36:54,990
here are some main points about principal components analysis 

93
07:36:54,990 --> 07:36:58,147
pca finds a new coordinate system for your data 

94
07:36:58,147 --> 07:37:02,151
such that the first coordinate defined by the first principal

95
07:37:02,151 --> 07:37:05,855
component captures the greatest variance in your data 

96
07:37:05,855 --> 07:37:10,341
the second coordinate defined by the second principal component captures

97
07:37:10,341 --> 07:37:13,271
the second greatest variance in a data etc 

98
07:37:13,271 --> 07:37:18,098
the first few principle components that capture most of the variance in a data

99
07:37:18,098 --> 07:37:22,120
can be used to define a lower - dimensional space for your data 

100
07:37:22,120 --> 07:37:26,513
pca can be a very useful technique for dimensionality reduction 

101
07:37:26,513 --> 07:37:30,284
especially when working with high - dimensional data 

102
07:37:30,284 --> 07:37:34,032
while pca is a useful technique for reducing the dimensionality of your

103
07:37:34,032 --> 07:37:36,718
data which can help with the downstream analysis 

104
07:37:36,718 --> 07:37:41,715
it can also make the resulting analysis models more difficult to interpret 

105
07:37:41,715 --> 07:37:46,965
the original features in your data set have specific meanings such as income 

106
07:37:46,965 --> 07:37:48,865
age and occupation 

107
07:37:48,865 --> 07:37:53,792
by mapping the data to a new coordinate system defined by principal components 

108
07:37:53,792 --> 07:37:58,364
the dimensions in your transformed data no longer have natural meanings 

109
07:37:58,364 --> 07:38:02,136
this should be kept in mind when using pca for dimensionality reduction 

1
15:08:51,740 --> 15:08:55,720
in this activity we will use knime to perform data exploration of weather data 

2
15:08:57,890 --> 15:09:00,000
first we will create a new knime workflow 

3
15:09:01,140 --> 15:09:02,560
we will then input the weather data 

4
15:09:02,560 --> 15:09:07,740
next we will create a histogram of air temperature 

5
15:09:07,740 --> 15:09:11,250
create a scatter plot between two variables 

6
15:09:11,250 --> 15:09:15,930
create a bar chart to show the distribution of a categorical variable 

7
15:09:15,930 --> 15:09:19,700
and finally create a box plot to compare two different distributions 

8
15:09:21,493 --> 15:09:24,350
let begin 

9
15:09:24,350 --> 15:09:26,660
first we will create a new workflow in knime 

10
15:09:28,100 --> 15:09:30,000
to do this we go to the file menu 

11
15:09:31,277 --> 15:09:38,890
select new select new knime workflow and click on next 

12
15:09:40,410 --> 15:09:42,720
then we type the name of the workflow we want to create 

13
15:09:43,730 --> 15:09:45,380
we will call this one plots 

14
15:09:46,850 --> 15:09:47,710
click on finish 

15
15:09:49,969 --> 15:09:54,690
next we want to load weather data into knime 

16
15:09:54,690 --> 15:09:56,710
to do this we will use a file reader node 

17
15:09:57,720 --> 15:10:01,890
to add this node go to the bottom left node repository 

18
15:10:01,890 --> 15:10:04,220
and in the box type in file reader 

19
15:10:05,298 --> 15:10:09,670
drag file reader to the canvas 

20
15:10:09,670 --> 15:10:13,890
next double click on file reader to configure it with the weather data 

21
15:10:14,930 --> 15:10:15,900
click on the browse button 

22
15:10:17,879 --> 15:10:21,360
we will choose the weather file 

23
15:10:21,360 --> 15:10:24,084
that daily weather csv 

24
15:10:25,794 --> 15:10:28,875
we can see preview of the data at the bottom half

25
15:10:28,875 --> 15:10:31,410
of the file reader configure dialog 

26
15:10:31,410 --> 15:10:35,270
you can see values for each column in the csv file 

27
15:10:35,270 --> 15:10:38,440
click ok to close the dialog 

28
15:10:39,520 --> 15:10:43,510
next we want to create a histogram of air temperature 

29
15:10:43,510 --> 15:10:46,410
we will add the histogram node to the workflow 

30
15:10:46,410 --> 15:10:50,230
again we go to the node repository and type in histogram 

31
15:10:51,640 --> 15:10:54,170
we will drag and drop histogram to the workflow and

32
15:10:56,640 --> 15:10:58,700
we will connect file reader to histogram 

33
15:10:59,700 --> 15:11:03,327
we will click and hold on output to file reader and

34
15:11:03,327 --> 15:11:06,610
drag to the input of histogram and release 

35
15:11:08,530 --> 15:11:12,520
before we configure the histogram node we need to run the file reader node 

36
15:11:12,520 --> 15:11:16,662
we can do that by selecting the file reader and

37
15:11:16,662 --> 15:11:20,913
either clicking on the green arrow at the top or

38
15:11:20,913 --> 15:11:24,952
right clicking and choosing execute here 

39
15:11:27,466 --> 15:11:32,508
once we have done that double - click on histogram 

40
15:11:32,508 --> 15:11:38,176
we will select error temp 9am as the biding column and

41
15:11:38,176 --> 15:11:44,520
also add error temp 9am to the aggregation column 

42
15:11:44,520 --> 15:11:47,570
we will leave the default number of bins as ten 

43
15:11:47,570 --> 15:11:49,020
click on ok 

44
15:11:51,110 --> 15:11:52,070
now we will run the workflow 

45
15:11:52,070 --> 15:11:57,060
you can click on the two arrow green button at the top to run all the nodes 

46
15:11:58,339 --> 15:12:01,420
now let view the histogram 

47
15:12:01,420 --> 15:12:03,690
right click on the histogram node 

48
15:12:03,690 --> 15:12:07,453
and choose view histogram view 

49
15:12:07,453 --> 15:12:11,530
the x - axis shows the value for each bin in the histogram 

50
15:12:11,530 --> 15:12:14,850
and the y - axis is the count or frequency 

51
15:12:16,080 --> 15:12:20,000
we can see the most frequent values are between 60 and 73 

52
15:12:20,000 --> 15:12:25,119
on the right we also see that there some with missing values 

53
15:12:26,710 --> 15:12:28,250
next close the window 

54
15:12:29,580 --> 15:12:33,550
now let create a scatter plot to show the relationship between two variables 

55
15:12:34,650 --> 15:12:37,430
first we will add the scatter plot node to the work flow 

56
15:12:40,128 --> 15:12:44,870
we will connect the the output of file reader to the input of scatter plot 

57
15:12:46,244 --> 15:12:51,700
execute the workflow right - click on

58
15:12:51,700 --> 15:12:57,650
scatter plot and choose view scatter plot 

59
15:12:57,650 --> 15:13:04,560
we will click on the column selection and choose air temp 9am for the x column 

60
15:13:04,560 --> 15:13:07,990
and for the y column choose relative humidity 9 am 

61
15:13:07,990 --> 15:13:10,630
in this plot 

62
15:13:10,630 --> 15:13:14,650
we can see a negative correlation between temperature and humidity 

63
15:13:14,650 --> 15:13:17,650
as the temperature increases the humidity goes down 

64
15:13:19,520 --> 15:13:20,480
we will close this window 

65
15:13:22,490 --> 15:13:27,890
next we will create a bar chart to show the distribution of a categorical variable 

66
15:13:27,890 --> 15:13:30,120
we will visualize the wind direction at 9 am 

67
15:13:30,120 --> 15:13:35,290
we will begin by creating the categorical variable by using the numeric binner node 

68
15:13:36,310 --> 15:13:38,400
let add numeric binner to the work flow 

69
15:13:41,011 --> 15:13:46,510
we will connect the output of file reader to the input of numeric binner 

70
15:13:47,675 --> 15:13:51,528
double - click on numeric binner to configure it 

71
15:13:51,528 --> 15:13:55,799
select max wind direction 9am and

72
15:13:55,799 --> 15:13:59,950
click on add five times to add five bins 

73
15:14:01,220 --> 15:14:03,050
now let give a name for each of these bins 

74
15:14:04,510 --> 15:14:07,180
select a bin and now choose a name 

75
15:14:07,180 --> 15:14:09,920
the first will be for the north direction 

76
15:14:09,920 --> 15:14:12,938
so we will call that 1 - n 

77
15:14:12,938 --> 15:14:19,214
next 2 - e for east 3 - s for

78
15:14:19,214 --> 15:14:24,444
south 4 - w for west and

79
15:14:24,444 --> 15:14:28,630
finally 1 - n for north again 

80
15:14:29,650 --> 15:14:33,340
now we need to specify the values for each of these bins 

81
15:14:33,340 --> 15:14:38,010
we will select the first one and set the endpoint to be 45 degrees 

82
15:14:38,010 --> 15:14:43,820
next choose east and set that maximum to be 135 

83
15:14:45,330 --> 15:14:49,360
select the third one and choose the max to be 225 

84
15:14:49,360 --> 15:14:55,110
finally select the fourth one and select the max to be 315 

85
15:14:55,110 --> 15:14:58,510
next click on a pin new column 

86
15:14:58,510 --> 15:15:04,368
set the name for categorical variable to be categorical underscore max underscore 

87
15:15:04,368 --> 15:15:08,531
wind direction 9am 

88
15:15:10,419 --> 15:15:13,740
click ok 

89
15:15:13,740 --> 15:15:15,880
now add a histogram note to the work flow 

90
15:15:20,111 --> 15:15:24,455
connect the output of numeric binner to histogram 

91
15:15:24,455 --> 15:15:28,630
double - click on histogram to configure it 

92
15:15:28,630 --> 15:15:34,908
make sure they binning column is categorical max wind direction 9am 

93
15:15:34,908 --> 15:15:37,160
and also there are no aggregation columns 

94
15:15:38,800 --> 15:15:42,140
click ok and execute the work flow 

95
15:15:44,066 --> 15:15:45,840
let view the chart 

96
15:15:46,880 --> 15:15:51,170
right click on histogram and choose view histogram view 

97
15:15:53,470 --> 15:15:57,720
this tells us that most of the wind comes from the east and the south 

98
15:15:59,070 --> 15:16:00,620
and not many measurements from the north 

99
15:16:02,220 --> 15:16:03,650
next close this 

100
15:16:05,490 --> 15:16:09,252
now lets create a box plot to compare two different distributions 

101
15:16:09,252 --> 15:16:12,380
we w will examine the distribution air pressure for

102
15:16:12,380 --> 15:16:16,610
low humidity days versus normal or high humidity days 

103
15:16:16,610 --> 15:16:20,810
to do this we will first need to creat a categorical variable for humidity 

104
15:16:20,810 --> 15:16:23,800
lets add another numeric binner actor to the workflow 

105
15:16:26,599 --> 15:16:30,190
connect the output a file reader to this actor 

106
15:16:32,090 --> 15:16:34,500
double click to configure 

107
15:16:34,500 --> 15:16:36,830
select relative humidity 9 am 

108
15:16:36,830 --> 15:16:39,830
and click on add twice to add two bins 

109
15:16:41,820 --> 15:16:46,350
select the first bin and we will call that humidity low 

110
15:16:46,350 --> 15:16:53,750
select the second bin and we will call that humidity not low 

111
15:16:53,750 --> 15:16:56,859
select the first bin and

112
15:16:56,859 --> 15:17:01,390
we will set the maximum value to 25 

113
15:17:01,390 --> 15:17:02,900
check append new column 

114
15:17:04,290 --> 15:17:11,130
and set the name to low humidity day click on ok 

115
15:17:11,130 --> 15:17:14,110
next add the conditional box plot node to the workflow 

116
15:17:18,604 --> 15:17:23,310
right click on conditional box plot to configure it 

117
15:17:23,310 --> 15:17:27,891
make sure the nominal column is low humidity day and

118
15:17:27,891 --> 15:17:31,100
the numeric column is air pressure 9am 

119
15:17:31,100 --> 15:17:32,010
click ok 

120
15:17:32,010 --> 15:17:36,260
run the workflow 

121
15:17:38,760 --> 15:17:43,410
right click on conditional box plot and choose few additional box plot 

122
15:17:45,210 --> 15:17:49,870
and the x axis we see the two values or a categorical variable humidity low and

123
15:17:49,870 --> 15:17:51,650
humidity not low 

124
15:17:51,650 --> 15:17:54,000
the y axis shows the air pressure values 

125
15:17:55,370 --> 15:17:59,300
i can see that on average lower humidity means higher air pressure 

126
15:18:01,792 --> 15:18:03,750
close this 

127
15:18:03,750 --> 15:18:07,330
finally save the workflow when we are done by clicking on the disk

128
15:18:07,330 --> 15:18:08,630
at the top left of the toolbar 

1
06:26:59,190 --> 06:27:02,590
in addition to cleaning your data to address data quality issues 

2
06:27:02,590 --> 06:27:07,470
data preparation also includes selecting features to use for analysis 

3
06:27:07,470 --> 06:27:12,980
after this video you will be able to explain what feature selection involves 

4
06:27:12,980 --> 06:27:16,880
discuss the goal of feature selection and list three approaches for

5
06:27:16,880 --> 06:27:18,940
selecting features 

6
06:27:18,940 --> 06:27:21,720
feature selection refers to choosing the set of features to

7
06:27:21,720 --> 06:27:25,000
use that is appropriate for the subsequent analysis 

8
06:27:25,000 --> 06:27:28,930
the goal of feature selection is to come up with the smallest set of features

9
06:27:28,930 --> 06:27:33,030
that best captures the characteristics of the problem being addressed 

10
06:27:33,030 --> 06:27:37,030
the smaller the number of features used the simpler the analysis will be 

11
06:27:37,030 --> 06:27:37,980
but of course 

12
06:27:37,980 --> 06:27:42,430
the set of features used must include all features relevant to the problem 

13
06:27:42,430 --> 06:27:45,100
so there must be a balance between expressiveness and

14
06:27:45,100 --> 06:27:46,830
compactness of the feature set 

15
06:27:46,830 --> 06:27:51,560
there are several methods to consider in selecting features 

16
06:27:51,560 --> 06:27:53,010
new features can be added 

17
06:27:53,010 --> 06:27:58,430
some features can be removed features can be re - coded or features can be combined 

18
06:27:58,430 --> 06:28:02,060
all these operations affect the final set of features that will be used for

19
06:28:02,060 --> 06:28:03,270
analysis 

20
06:28:03,270 --> 06:28:05,790
of course some features can be kept as is as well 

21
06:28:07,210 --> 06:28:10,070
new features can be derived from existing features 

22
06:28:10,070 --> 06:28:15,200
for example a new feature to specify whether a student is in state or

23
06:28:15,200 --> 06:28:19,700
out of state can be added based on the student state of residence 

24
06:28:19,700 --> 06:28:22,490
for an application such as college admissions 

25
06:28:22,490 --> 06:28:26,300
this new feature represents an important aspect of an application and so

26
06:28:26,300 --> 06:28:28,450
would be very helpful as a separate feature 

27
06:28:29,480 --> 06:28:33,510
another example is adding a feature to indicate the color of a vehicle 

28
06:28:33,510 --> 06:28:36,870
which can play an important role in an auto insurance application 

29
06:28:37,940 --> 06:28:41,000
features can also be removed candidates for

30
06:28:41,000 --> 06:28:43,950
removal are features that are very correlated 

31
06:28:43,950 --> 06:28:46,630
during data exploration you may have discovered that

32
06:28:46,630 --> 06:28:51,820
two features are very correlated that is they change in very similar ways 

33
06:28:51,820 --> 06:28:53,990
for example the purchase price of a product and

34
06:28:53,990 --> 06:28:58,480
the amount of sales tax paid are likely to be very correlated 

35
06:28:58,480 --> 06:29:02,250
the higher the purchase price the higher the sales tax 

36
06:29:02,250 --> 06:29:05,110
in this case you might want to drop one of these features 

37
06:29:05,110 --> 06:29:08,340
since these features have essentially duplicate information 

38
06:29:08,340 --> 06:29:11,280
and keeping both features makes the feature set larger and

39
06:29:11,280 --> 06:29:14,770
the analysis unnecessarily more complex 

40
06:29:14,770 --> 06:29:17,810
features with a high percentage of missing values may also be good

41
06:29:17,810 --> 06:29:19,640
candidates for removal 

42
06:29:19,640 --> 06:29:20,740
the validity and

43
06:29:20,740 --> 06:29:24,360
usefulness of features with a lot of missing values are in question 

44
06:29:24,360 --> 06:29:27,810
so removing may not result in any loss of information 

45
06:29:27,810 --> 06:29:30,350
again these features would have been discovered during the data

46
06:29:30,350 --> 06:29:31,090
exploration step 

47
06:29:32,240 --> 06:29:35,880
irrelevant features should also be removed from the data set 

48
06:29:35,880 --> 06:29:39,550
irrelevant features are those that contain no information that is useful for

49
06:29:39,550 --> 06:29:41,320
the analysis task 

50
06:29:41,320 --> 06:29:44,778
an example of this is employee id in predicting income 

51
06:29:44,778 --> 06:29:49,770
other fields used simply for identification such as row number 

52
06:29:49,770 --> 06:29:52,850
person id etc are good candidates for removal 

53
06:29:53,940 --> 06:29:57,990
features can also be combined if the new feature presents important information

54
06:29:57,990 --> 06:30:01,570
that is not represented by looking at the original features individually 

55
06:30:03,160 --> 06:30:07,640
for example bmi which is body mass index is an indicator of

56
06:30:07,640 --> 06:30:11,290
whether a person is underweight average weight or overweight 

57
06:30:12,290 --> 06:30:15,890
this is an important feature to have for a weight loss application 

58
06:30:15,890 --> 06:30:20,270
it represents information about how much a person weighs relative to their height

59
06:30:20,270 --> 06:30:24,230
that is not available by looking at just the person height or weight alone 

60
06:30:25,840 --> 06:30:29,480
a feature can be re - coded as appropriate for the application 

61
06:30:29,480 --> 06:30:33,550
a common example of this is when you want to turn a continuous feature in to

62
06:30:33,550 --> 06:30:35,420
a categorical one 

63
06:30:35,420 --> 06:30:40,324
for example for a marketing application you might want to re - code customer age

64
06:30:40,324 --> 06:30:46,910
into customer categories such as teenager young adult adult and senior citizen 

65
06:30:46,910 --> 06:30:53,310
so you would map ages 13 to 19 to teenager ages 20 to 25 to young adult 

66
06:30:53,310 --> 06:30:58,370
26 to 55 as adult and over 55 as senior 

67
06:30:59,590 --> 06:31:03,640
for some applications you may want to make use of finery features 

68
06:31:03,640 --> 06:31:07,550
as an example you might want a feature to capture whether a customer tends to buy

69
06:31:07,550 --> 06:31:09,810
expensive items or not 

70
06:31:09,810 --> 06:31:14,100
in this case you would want a feature that maps to one for a customer with

71
06:31:14,100 --> 06:31:18,484
an average purchase price over a certain amount and maps to zero otherwise 

72
06:31:18,484 --> 06:31:22,960
re - coding features can also result in breaking one feature

73
06:31:22,960 --> 06:31:24,920
into multiple features 

74
06:31:24,920 --> 06:31:28,730
a common example of this is to separate an address feature

75
06:31:28,730 --> 06:31:34,580
into its constituent parts street address city state and zip code 

76
06:31:34,580 --> 06:31:37,280
this way you can more easily group records by state for

77
06:31:37,280 --> 06:31:40,130
example to provide a state by state analysis 

78
06:31:41,550 --> 06:31:46,280
future selection aims to select the smallest set of features to best

79
06:31:46,280 --> 06:31:49,100
capture the characteristics of the data for your application 

80
06:31:50,390 --> 06:31:54,310
know from the examples represented that domain knowledge once again place

81
06:31:54,310 --> 06:31:58,400
a key role in choosing the appropriate features to use 

82
06:31:58,400 --> 06:32:02,934
good understanding of the application is essential in deciding which features to

83
06:32:02,934 --> 06:32:04,190
add drop or modify 

84
06:32:05,350 --> 06:32:09,256
it should also be noted that feature selection can be referred to as feature

85
06:32:09,256 --> 06:32:13,596
engineering since what you are doing here is to engineer the best feature set for

86
06:32:13,596 --> 06:32:14,661
your application 

1
12:59:13,850 --> 12:59:16,330
let now discuss what feature transformation is 

2
12:59:17,640 --> 12:59:21,340
after this video you will be able to articulate the purpose of

3
12:59:21,340 --> 12:59:26,170
feature transformation list three feature transformation operations and

4
12:59:26,170 --> 12:59:28,180
discuss when scaling is important 

5
12:59:29,670 --> 12:59:31,390
in addition to feature selection 

6
12:59:31,390 --> 12:59:35,690
data pre - processing can also include feature transformation 

7
12:59:35,690 --> 12:59:38,730
feature transformation involves mapping a set of values for

8
12:59:38,730 --> 12:59:42,860
the feature to a new set of values to make the representation of the data more

9
12:59:42,860 --> 12:59:46,800
suitable or easier to process for the downstream analysis 

10
12:59:48,760 --> 12:59:52,010
a common feature transformation operation is scaling 

11
12:59:52,010 --> 12:59:54,200
this involves changing the range of values for

12
12:59:54,200 --> 12:59:58,570
a feature of features to another specified range 

13
12:59:58,570 --> 13:00:01,840
this is done to avoid allowing features with large values to

14
13:00:01,840 --> 13:00:03,280
dominate the analysis results 

15
13:00:04,300 --> 13:00:08,030
for example if your dataset has both width and height as features 

16
13:00:08,030 --> 13:00:11,910
the magnitude of the weight values which are in pounds will be much

17
13:00:11,910 --> 13:00:15,910
larger than the magnitude of the height values which are in feet and inches 

18
13:00:15,910 --> 13:00:18,950
so scaling both features to a common value range

19
13:00:18,950 --> 13:00:21,910
will make the contributions from both weight and height equal 

20
13:00:23,350 --> 13:00:28,230
one way to perform scaling is to map all values of a feature to a specific range

21
13:00:28,230 --> 13:00:30,690
such as between zero and one 

22
13:00:30,690 --> 13:00:33,342
for example let say you have a feature for

23
13:00:33,342 --> 13:00:36,750
income that ranges from 30 000 to 100 000 

24
13:00:36,750 --> 13:00:41,420
and you have another feature for years of employment that ranges from 0 to 50 

25
13:00:41,420 --> 13:00:44,500
these features have very different scales 

26
13:00:44,500 --> 13:00:47,840
if you want both features to have equal weighting when you compare the data

27
13:00:47,840 --> 13:00:53,170
samples then you can scale the range of both features to be between 0 and 1 

28
13:00:53,170 --> 13:00:56,870
that way the income feature which is on much largest scale than the years

29
13:00:56,870 --> 13:01:00,940
of employment feature will not dominate the compares result 

30
13:01:00,940 --> 13:01:05,350
alternatively scaling can be perform by transforming the features such that

31
13:01:05,350 --> 13:01:09,420
the results have zero mean and unit standard deviation 

32
13:01:09,420 --> 13:01:12,730
the steps to perform the scaling is to first calculate the mean and

33
13:01:12,730 --> 13:01:15,930
standard deviation values for the feature to be scaled 

34
13:01:15,930 --> 13:01:18,560
then for each value for this feature 

35
13:01:18,560 --> 13:01:23,510
subtract the mean value from that value and divide by the standard deviation 

36
13:01:23,510 --> 13:01:27,290
the transformed feature will end up with a mean value of zero and

37
13:01:27,290 --> 13:01:28,680
standard deviation of one 

38
13:01:29,720 --> 13:01:32,420
this effectively removes the units of the features and

39
13:01:32,420 --> 13:01:37,370
converts each future value to number of standard deviations from the mean 

40
13:01:38,540 --> 13:01:42,720
this scaling method is used when the min and max values are known 

41
13:01:42,720 --> 13:01:46,760
this is also useful when there are outliers which will skew the calculation

42
13:01:46,760 --> 13:01:50,380
for the range as the max value is determined by the furthest outlier 

43
13:01:51,400 --> 13:01:55,980
this scaling operation is often referred to as zero - normalization or

44
13:01:55,980 --> 13:01:57,030
as standardization 

45
13:01:58,390 --> 13:02:01,110
filtering is another feature transformation operation 

46
13:02:02,250 --> 13:02:07,450
this is commonly applied to time series data such as speech or audio signals 

47
13:02:07,450 --> 13:02:10,520
a low pass filter can be used to filter out noise 

48
13:02:10,520 --> 13:02:14,910
which usually manifests as the high frequency component in the signal 

49
13:02:14,910 --> 13:02:19,090
a low pass filter removes components above a certain frequency

50
13:02:19,090 --> 13:02:21,410
allowing the rest to pass through unaltered 

51
13:02:22,750 --> 13:02:26,310
filtering can also be used to remove noise in images 

52
13:02:26,310 --> 13:02:31,530
noise in an image is random variation in intensity or color in the pixels 

53
13:02:31,530 --> 13:02:35,590
for example a noise can cause an image to appear grainy 

54
13:02:35,590 --> 13:02:40,180
a mean or median filter can be used to replace the pixel value with the mean or

55
13:02:40,180 --> 13:02:42,240
median value of its neighboring pixels 

56
13:02:43,270 --> 13:02:45,730
this has the effect of smoothing out the image 

57
13:02:45,730 --> 13:02:48,390
removing the noise that causes the graininess in the image 

58
13:02:49,830 --> 13:02:53,911
aggregation combines values for a feature in order to summarize data or

59
13:02:53,911 --> 13:02:55,210
reduce variability 

60
13:02:56,340 --> 13:03:00,850
aggregation combines values for a feature in order to summarize the data or

61
13:03:00,850 --> 13:03:02,970
to reduce variability 

62
13:03:02,970 --> 13:03:07,660
aggregation is done by summing or averaging data values at a higher level 

63
13:03:07,660 --> 13:03:11,720
for example hourly values can be aggregated to the daily level 

64
13:03:11,720 --> 13:03:16,255
or values within a city region can be aggregated to a state level 

65
13:03:16,255 --> 13:03:19,010
these plots show the results of aggregation 

66
13:03:19,010 --> 13:03:23,060
the left plot shows the wind speed values in miles per hour

67
13:03:23,060 --> 13:03:26,730
averaged every 10 minutes over a period of seven days 

68
13:03:26,730 --> 13:03:31,110
notice that there a lot of variability that is the values fluctuate a lot 

69
13:03:32,130 --> 13:03:35,730
the right plot shows wind speed values averaged every hour so

70
13:03:35,730 --> 13:03:38,920
every 60 minutes instead of every 10 minutes 

71
13:03:38,920 --> 13:03:43,340
notice that the line is much smoother here because the values are aggregated at

72
13:03:43,340 --> 13:03:44,750
a higher time scale level 

73
13:03:45,750 --> 13:03:48,900
so aggregation can have the effect of removing noise

74
13:03:48,900 --> 13:03:52,160
to provide a clear representation of the structure of your data 

75
13:03:53,240 --> 13:03:56,260
another example is tracking stock prices 

76
13:03:56,260 --> 13:03:59,620
hourly deviations of a stock may be difficult to track but 

77
13:03:59,620 --> 13:04:04,900
aggregated daily changes may better reveal any upward or downward trend of the stock 

78
13:04:06,280 --> 13:04:10,330
in summary feature transformation involves mapping the set of values for

79
13:04:10,330 --> 13:04:14,600
a feature to a new set of values to make the representation of the data

80
13:04:14,600 --> 13:04:18,180
more suitable for the downstream analysis 

81
13:04:18,180 --> 13:04:22,090
feature transformation should be used with cautions since they change the nature of

82
13:04:22,090 --> 13:04:28,030
the data and unintentionally remove some important characteristics of the data 

83
13:04:28,030 --> 13:04:31,757
so it important to look at the effects of the transformation you are applying

84
13:04:31,757 --> 13:04:34,897
to your data to make certain that it has the intended consequences 

1
02:03:46,000 --> 02:03:50,871
in this activity we will be using knime to handle missing data 

2
02:03:50,871 --> 02:03:55,045
first we will create new workflow and import our weather data 

3
02:03:55,045 --> 02:04:01,571
next we will remove missing values for specific measurement in the data 

4
02:04:01,571 --> 02:04:04,813
we will then impute missing values with the mean 

5
02:04:04,813 --> 02:04:07,554
and finally remove all the rows with missing values 

6
02:04:10,655 --> 02:04:14,145
let begin first let create a new workflow in knime 

7
02:04:20,723 --> 02:04:22,558
we will name it handling missing values 

8
02:04:28,252 --> 02:04:33,453
next let import the weather data using the file reader node 

9
02:04:33,453 --> 02:04:35,421
we will add the file reader node to the canvas 

10
02:04:44,553 --> 02:04:48,060
we will configure file reader to read daily weather csv 

11
02:04:54,409 --> 02:04:59,280
next we will connect the histogram node to file reader 

12
02:05:06,553 --> 02:05:10,909
before we configure the histogram node we need to run the file reader node 

13
02:05:10,909 --> 02:05:14,331
we can do this by clicking on file reader and

14
02:05:14,331 --> 02:05:17,677
clicking on the green arrow in the toolbar 

15
02:05:21,224 --> 02:05:24,259
next double - click on histogram to configure it 

16
02:05:24,259 --> 02:05:29,561
we will set both the binning column and aggregation columns to air temp 9am 

17
02:05:38,003 --> 02:05:40,259
next we will add a missing value node 

18
02:05:48,931 --> 02:05:52,257
we will connect this to the file reader node 

19
02:05:52,257 --> 02:05:55,992
double click on missing value to configuring it 

20
02:05:55,992 --> 02:05:59,945
go to column settings 

21
02:05:59,945 --> 02:06:07,466
select air temp 9am and click on add 

22
02:06:07,466 --> 02:06:09,584
we will then choose remove row 

23
02:06:12,982 --> 02:06:18,298
this will remove the measurements of air temp 9am that had missing values 

24
02:06:18,298 --> 02:06:18,948
click ok 

25
02:06:21,838 --> 02:06:27,106
next we will add another histogram node to the output of missing value 

26
02:06:27,106 --> 02:06:31,162
we can do this easily by copying and pasting the existing histogram node 

27
02:06:40,404 --> 02:06:43,144
now let run the workflow by clicking on the double green arrows 

28
02:06:46,966 --> 02:06:50,491
let view the output in the histogram nodes before and

29
02:06:50,491 --> 02:06:52,592
after we remove missing values 

30
02:06:52,592 --> 02:06:56,158
in this histogram view we can see that there are missing values 

31
02:06:56,158 --> 02:06:58,985
if we go and look at the second histogram node 

32
02:06:58,985 --> 02:07:01,752
the one after we removed the missing values 

33
02:07:04,766 --> 02:07:06,958
we can see that there are no missing values in this chart 

34
02:07:09,678 --> 02:07:14,748
next instead of removing missing values let impute the values to the mean 

35
02:07:14,748 --> 02:07:18,042
we could do this by configuring the missing value node 

36
02:07:20,865 --> 02:07:25,454
and instead of remove row change that to mean 

37
02:07:25,454 --> 02:07:33,050
click ok and rerun the work flow 

38
02:07:33,050 --> 02:07:36,906
we could see the difference by comparing the graphs and the two histogram nodes 

39
02:07:42,182 --> 02:07:43,700
if we go to visualization settings 

40
02:07:46,403 --> 02:07:50,412
and change labels to all elements we will see the number of elements in each band 

41
02:07:57,379 --> 02:08:00,929
in the histogram before we removed the missing values 

42
02:08:00,929 --> 02:08:04,486
we can see the fifth column has 216 measurements in it 

43
02:08:04,486 --> 02:08:11,952
where as the fifth column after we imputed the missing values has 221 values in it 

44
02:08:11,952 --> 02:08:12,680
let close these 

45
02:08:17,608 --> 02:08:22,756
next let remove all the rows that have missing values in the data 

46
02:08:22,756 --> 02:08:25,073
we can do this by double clicking on missing value 

47
02:08:28,578 --> 02:08:30,770
removing air temp 9am 

48
02:08:32,918 --> 02:08:34,116
clicking on the default tab 

49
02:08:36,728 --> 02:08:39,892
changing this to remove row 

50
02:08:39,892 --> 02:08:45,224
i will click ok now we rerun the workflow 

51
02:08:45,224 --> 02:08:49,470
again we will look at the histograms before and after we remove the missing values 

52
02:09:01,128 --> 02:09:06,170
again we will go to visualization settings and change the labels to all elements so

53
02:09:06,170 --> 02:09:08,878
we can see the number of elements in each bin 

54
02:09:13,213 --> 02:09:16,490
we can see the number of elements in each bin in the different histograms

55
02:09:16,490 --> 02:09:17,149
has changed 

1
04:13:03,910 --> 04:13:08,280
in this activity we will see how to handle missing values in spark 

2
04:13:08,280 --> 04:13:11,320
first we will load weather data into a spark dataframe 

3
04:13:12,560 --> 04:13:17,092
we will then examine the summary statistics for air temperature remove

4
04:13:17,092 --> 04:13:22,072
the rows with missing values and finally impute missing values with the mean 

5
04:13:25,037 --> 04:13:26,002
let begin 

6
04:13:26,002 --> 04:13:30,900
first we will open the notebook called handling missing values 

7
04:13:31,920 --> 04:13:34,910
the first cell creates an sqlcontext and

8
04:13:34,910 --> 04:13:37,720
then loads the weather data csv into a data frame 

9
04:13:39,170 --> 04:13:44,060
let execute this we can view the summary statistics for

10
04:13:44,060 --> 04:13:50,570
the dataframe by running df describe show 

11
04:13:50,570 --> 04:13:53,238
let just look at the summary statistics for the air temperature 

12
04:13:53,238 --> 04:13:54,913
we will run

13
04:13:54,913 --> 04:14:03,852
df describe air score 9am show 

14
04:14:06,730 --> 04:14:09,590
this says that there are 1090 rows 

15
04:14:10,790 --> 04:14:14,490
this does not include the rows of missing values for the air temperature 

16
04:14:15,630 --> 04:14:20,381
we can count the total number of rows in the dataframe by running df count 

17
04:14:22,027 --> 04:14:26,026
since there are 1095 total rows in the dataframe but

18
04:14:26,026 --> 04:14:28,801
only 1090 in the air temp column 

19
04:14:28,801 --> 04:14:33,630
that means there are five rows in air temp that have missing values 

20
04:14:35,240 --> 04:14:39,029
next let remove all the rows in the dataframe that have missing values 

21
04:14:40,060 --> 04:14:43,220
we will put these in a new data frame called removealldf 

22
04:14:44,700 --> 04:14:49,476
to drop the missing values we will run df na drop 

23
04:14:51,971 --> 04:14:53,984
let look at the summary statistics for

24
04:14:53,984 --> 04:14:57,160
air temp 9am with the missing values dropped 

25
04:14:57,160 --> 04:15:03,384
we will run removealldf describe air temp 9am show 

26
04:15:06,051 --> 04:15:09,146
we can see that the mean and standard deviation values

27
04:15:09,146 --> 04:15:13,840
are close to the original values before we removed the rows with missing values 

28
04:15:15,650 --> 04:15:19,784
additionally the count of the number of rows is 1064 

29
04:15:21,380 --> 04:15:25,768
we can verify that this is the total number of rows in the new dataframe by

30
04:15:25,768 --> 04:15:27,758
running removealldf count 

31
04:15:32,389 --> 04:15:35,075
next let impute the missing values with the mean 

32
04:15:35,075 --> 04:15:37,314
instead of removing them from the dataframe 

33
04:15:38,920 --> 04:15:42,712
first we will need to load the average function from pyspark 

34
04:15:44,060 --> 04:15:53,030
we will do this by running from pyspark sql functions import avg 

35
04:15:53,030 --> 04:15:57,689
next we will create a copy of the dataframe in which we will input the missing values 

36
04:15:58,910 --> 04:16:01,590
we will call the new dataframe imputedf 

37
04:16:05,000 --> 04:16:09,349
to impute the missing values we will iterate through each column of

38
04:16:09,349 --> 04:16:14,317
the original dataframe first computing the mean value for that column and

39
04:16:14,317 --> 04:16:18,913
then replacing the missing values in that column with the mean value 

40
04:16:18,913 --> 04:16:23,145
to move through the columns in the data frame 

41
04:16:23,145 --> 04:16:27,704
we will enter for x in imputedf columns : next 

42
04:16:27,704 --> 04:16:32,061
we will compute the mean value for that column 

43
04:16:35,105 --> 04:16:39,296
we will use the data frame in which we removed all the missing values 

44
04:16:39,296 --> 04:16:42,630
we will call the agg function to compute an aggregate 

45
04:16:42,630 --> 04:16:45,900
and the argument that we give it is avg 

46
04:16:45,900 --> 04:16:51,830
the argument avg is x which is the column we are trying to compute the average of 

47
04:16:53,730 --> 04:16:57,010
the agg function returns to dataframe and

48
04:16:57,010 --> 04:17:00,220
we want to get the first row of that data frame 

49
04:17:00,220 --> 04:17:05,280
we can do this by calling first and then you get the first value in

50
04:17:05,280 --> 04:17:10,420
this row or say 0 

51
04:17:10,420 --> 04:17:13,410
next let print the column name in mean value 

52
04:17:14,890 --> 04:17:19,561
print x meanvalue 

53
04:17:22,522 --> 04:17:26,202
now let update our new dataframe 

54
04:17:26,202 --> 04:17:31,156
replacing the missing values with the mean value 

55
04:17:31,156 --> 04:17:40,242
imputedf = imputedf na fill meanvalue x 

56
04:17:41,460 --> 04:17:42,135
let run this 

57
04:17:47,273 --> 04:17:49,344
we can see that the mean value for

58
04:17:49,344 --> 04:17:54,260
air temp 9am matches the mean value computed in the summary statistics of

59
04:17:54,260 --> 04:17:57,820
the data frame where the missing values were removed 

60
04:17:59,710 --> 04:18:03,390
finally let print the imputed data summary statistics 

61
04:18:03,390 --> 04:18:07,450
first we will show the summary statistics for the original dataframe 

62
04:18:07,450 --> 04:18:10,318
and then the summary statistics for the imputed dataframe 

63
04:18:10,318 --> 04:18:17,813
we will enter df describe air temp 9am show and

64
04:18:17,813 --> 04:18:25,062
imputedf describe air temp 9am sho - w 

65
04:18:25,062 --> 04:18:27,997
run this 

66
04:18:27,997 --> 04:18:32,869
we can see that the number of rows in the imputed dataframe is larger

67
04:18:32,869 --> 04:18:36,612
than the number of rows in the original dataframe 

68
04:18:37,875 --> 04:18:41,500
there were five rows in the original dataframe with missing values 

69
04:18:41,500 --> 04:18:43,210
and these have now been replaced with the mean 

1
08:31:46,760 --> 08:31:50,340
let talk about what it means to build a classification model and

2
08:31:50,340 --> 08:31:52,990
how building a model differs from applying a model 

3
08:31:54,080 --> 08:31:55,290
after this video 

4
08:31:55,290 --> 08:32:00,170
you will be able to discuss what building a classification model means 

5
08:32:00,170 --> 08:32:03,511
explain the difference between building and applying a model 

6
08:32:03,511 --> 08:32:06,910
and summarize why the parameters of a model needs to be adjusted 

7
08:32:08,170 --> 08:32:11,150
a machine learning model is a mathematical model 

8
08:32:11,150 --> 08:32:15,560
in the general sense this means that the model has parameters and uses

9
08:32:15,560 --> 08:32:19,630
equations to determine the relationship between its inputs and outputs 

10
08:32:20,640 --> 08:32:25,730
the parameters are used by the model to modify the inputs to generate the outputs 

11
08:32:25,730 --> 08:32:28,930
the model adjusts its parameters in order to correct or

12
08:32:28,930 --> 08:32:31,770
refine this input output relationship 

13
08:32:33,030 --> 08:32:35,630
here an example of a simple model 

14
08:32:35,630 --> 08:32:38,150
this mathematical model represents a line 

15
08:32:38,150 --> 08:32:44,554
y is the output x is the input m determines the slope of the line and

16
08:32:44,554 --> 08:32:51,080
b determines the y - intercept or where the line crosses the y - axis 

17
08:32:51,080 --> 08:32:53,720
m and b are the model parameters 

18
08:32:53,720 --> 08:32:55,722
given a specific value for

19
08:32:55,722 --> 08:33:00,368
x the model uses as parameters along with x to determine y 

20
08:33:00,368 --> 08:33:05,168
by adjusting the values for the parameters m and b 

21
08:33:05,168 --> 08:33:10,760
the model can adjust how the input x matched to the output y 

22
08:33:12,010 --> 08:33:15,262
here we see how the output y changes for

23
08:33:15,262 --> 08:33:19,844
the same value of input x when parameter b changes 

24
08:33:19,844 --> 08:33:25,480
recall that b is the y - intercept or where the line crosses the y - axis 

25
08:33:26,520 --> 08:33:34,048
the value of b is + 1 for the red line and - 1 for the blue line 

26
08:33:34,048 --> 08:33:37,996
for the input x = 1 the value of y is 3 for

27
08:33:37,996 --> 08:33:42,175
the red line as indicated by the red arrow 

28
08:33:42,175 --> 08:33:48,024
for the blue line when the parameter b changes from + 1 to - 1 

29
08:33:48,024 --> 08:33:53,670
for x = 1 the value of y is 1 as indicated by the blue arrow 

30
08:33:54,700 --> 08:33:58,710
so we see that with just a simple change in one model parameter 

31
08:33:58,710 --> 08:34:01,074
the input to output mapping changes 

32
08:34:02,585 --> 08:34:05,495
a machine learning model works in a similar way 

33
08:34:05,495 --> 08:34:08,285
it maps input values to output values 

34
08:34:08,285 --> 08:34:11,640
and it adjusts the parameters in order to correct or

35
08:34:11,640 --> 08:34:14,145
refine this input - output mapping 

36
08:34:15,215 --> 08:34:18,665
the parameters of a machine learning model are adjusted or

37
08:34:18,665 --> 08:34:22,185
estimated from the data using a learning algorithm 

38
08:34:23,300 --> 08:34:27,240
this in essence is what is involved in building a model 

39
08:34:27,240 --> 08:34:32,869
this process is referred to by many terms such as model building 

40
08:34:32,869 --> 08:34:37,326
model creation model training and model fitting 

41
08:34:37,326 --> 08:34:38,803
in building a model 

42
08:34:38,803 --> 08:34:43,736
we want to adjust the parameters in order to reduce the model error 

43
08:34:43,736 --> 08:34:48,424
in the case of supervised tasks such as classification this means getting

44
08:34:48,424 --> 08:34:53,340
the model outputs to match the targets or desired outputs as much as possible 

45
08:34:54,580 --> 08:34:58,600
since the classification task is to predict the correct category or

46
08:34:58,600 --> 08:35:01,650
class given the input variables 

47
08:35:01,650 --> 08:35:06,150
you can think of the classification problem visually as carving out the input

48
08:35:06,150 --> 08:35:10,910
space as regions corresponding to the different class labels 

49
08:35:10,910 --> 08:35:15,160
in this diagram for example the classification model needs to form

50
08:35:15,160 --> 08:35:20,350
the boundaries to define the regions separating red triangles

51
08:35:20,350 --> 08:35:24,580
from blue diamonds from green circles from yellow squares 

52
08:35:25,670 --> 08:35:31,180
in this example if a sample falls within the region in the upper right corner 

53
08:35:31,180 --> 08:35:33,160
it will be classified as a blue diamond 

54
08:35:34,280 --> 08:35:39,215
classification decisions are based on these regions and the regions are defined

55
08:35:39,215 --> 08:35:43,316
by the boundaries as indicated by the dashed lines in the diagram 

56
08:35:43,316 --> 08:35:46,349
so these boundaries are referred to as decision boundaries 

57
08:35:47,520 --> 08:35:52,720
building a classification then means using the data to adjust the model parameters

58
08:35:52,720 --> 08:35:56,680
in order to form decision boundaries to separate the target classes 

59
08:35:57,915 --> 08:36:01,770
note that the term classifier is often used to mean classification model 

60
08:36:03,480 --> 08:36:06,650
in general building a classification model 

61
08:36:06,650 --> 08:36:10,190
as well as other machine learning models involves two phases 

62
08:36:11,870 --> 08:36:16,515
the first is the training phase in which the model is constructed and

63
08:36:16,515 --> 08:36:20,190
its parameters adjusted using as what referred to as training data 

64
08:36:21,350 --> 08:36:25,020
training data is the data set used to train or create a model 

65
08:36:26,500 --> 08:36:28,750
the second is the testing phase 

66
08:36:28,750 --> 08:36:31,984
this is where the learned model is applied to new data 

67
08:36:31,984 --> 08:36:34,250
that is data not used in training the model 

68
08:36:35,790 --> 08:36:37,570
here another way to look at the two phases 

69
08:36:39,010 --> 08:36:43,130
in a training phase the learning algorithm uses the training data

70
08:36:43,130 --> 08:36:46,070
to adjust the model parameters to minimize errors 

71
08:36:47,090 --> 08:36:49,700
at the end of the training phase you get the trained model 

72
08:36:51,370 --> 08:36:56,270
in the testing phase the trained model is applied to test data 

73
08:36:56,270 --> 08:37:02,220
test data is separate from training data and is previously unseen by the model 

74
08:37:02,220 --> 08:37:05,810
the model is then evaluated on how it performs on the test data 

75
08:37:06,930 --> 08:37:10,607
the goal in building a classifier model is to have the model

76
08:37:10,607 --> 08:37:13,696
perform well on training as well as test data 

77
08:37:13,696 --> 08:37:16,611
we will discuss in more detail the use of training and

78
08:37:16,611 --> 08:37:21,470
test data sets in the next module when we discuss model evaluation 

79
08:37:21,470 --> 08:37:26,330
to adjust a model parameters we need to apply a learning algorithm 

80
08:37:26,330 --> 08:37:30,460
we will discuss the specific algorithms to build a classification model in

81
08:37:30,460 --> 08:37:31,510
the next few lectures 

1
17:09:18,070 --> 17:09:21,650
in this video we will outline some commonly used algorithms for

2
17:09:21,650 --> 17:09:24,260
building a classification model 

3
17:09:24,260 --> 17:09:28,540
after this video you will be able to describe the goal of

4
17:09:28,540 --> 17:09:33,260
a classification algorithm and name some common algorithms for classification 

5
17:09:34,630 --> 17:09:39,140
recall that a classification task is to predict the category from the input

6
17:09:39,140 --> 17:09:40,580
variables 

7
17:09:40,580 --> 17:09:46,570
a classification model processes the input data it receives and provides an output 

8
17:09:46,570 --> 17:09:50,860
since classification is a supervised task a target or

9
17:09:50,860 --> 17:09:54,710
desired output is provided for each sample 

10
17:09:54,710 --> 17:09:59,640
the goal is to get the model outputs to match the targets as much as possible 

11
17:10:01,040 --> 17:10:04,350
a classification model adjusts its parameters

12
17:10:04,350 --> 17:10:07,610
to get its outputs to match the targets 

13
17:10:07,610 --> 17:10:11,500
to adjust a model parameters a learning algorithm is applied 

14
17:10:12,580 --> 17:10:16,080
this occurs in a training phase when the model is constructed 

15
17:10:17,570 --> 17:10:20,950
there are many algorithms to build a classification model 

16
17:10:20,950 --> 17:10:25,514
in this course we will cover the algorithms listed here 

17
17:10:25,514 --> 17:10:30,371
knn or k nearest neighbors decision tree and naive bayes 

18
17:10:30,371 --> 17:10:34,870
knn stands for k nearest neighbors 

19
17:10:34,870 --> 17:10:39,490
this technique relies on the notion that samples with similar characteristics 

20
17:10:39,490 --> 17:10:45,440
that is samples with similar values for input likely belong to the same class 

21
17:10:45,440 --> 17:10:48,150
so classification of a sample is dependent

22
17:10:48,150 --> 17:10:50,650
on the target values of the neighboring points 

23
17:10:52,410 --> 17:10:56,560
another classification technique is referred to as decision tree 

24
17:10:56,560 --> 17:11:01,270
a decision tree is a classification model that uses a treelike structure

25
17:11:01,270 --> 17:11:04,398
to represent multiple decision paths 

26
17:11:04,398 --> 17:11:09,010
traversing each path leads to a different way to classify an input sample 

27
17:11:10,450 --> 17:11:14,770
a naive bayes model uses a probabilistic approach to classification 

28
17:11:15,870 --> 17:11:19,920
baye theorem is used to capture the relationship between the input data and

29
17:11:19,920 --> 17:11:20,830
the output class 

30
17:11:21,880 --> 17:11:26,130
simply put the baye theorem compares the probability of an event

31
17:11:26,130 --> 17:11:28,660
in the presence of another event 

32
17:11:28,660 --> 17:11:33,150
we see here the probability of a if b is present 

33
17:11:33,150 --> 17:11:37,770
for example probability of having a fire if the weather is hot 

34
17:11:37,770 --> 17:11:41,100
you can imagine event b depending on more than one variable 

35
17:11:41,100 --> 17:11:43,416
for example weather is hot and windy 

36
17:11:43,416 --> 17:11:51,060
we will cover knn decision tree and naive bayes in detail in the next few lectures 

37
17:11:51,060 --> 17:11:55,150
there are many other classification techniques but we will focus on these

38
17:11:55,150 --> 17:11:58,970
since they are fundamental algorithms that are commonly used and

39
17:11:58,970 --> 17:12:01,940
form the basis of other algorithms for classification 

1
10:21:19,720 --> 10:21:23,366
in this activity we will perform classification in spark is in

2
10:21:23,366 --> 10:21:24,355
decision tree 

3
10:21:24,355 --> 10:21:28,889
first we will load weather data into dataframe and

4
10:21:28,889 --> 10:21:31,578
drop unused and missing data 

5
10:21:31,578 --> 10:21:34,686
we will then create a categorical variable for

6
10:21:34,686 --> 10:21:39,100
low humidity days and aggregate features used to make predictions 

7
10:21:39,100 --> 10:21:43,110
we will then split our data into training and test sets 

8
10:21:44,400 --> 10:21:46,330
and then create and train the decision tree 

9
10:21:47,730 --> 10:21:52,414
finally we will save our predictions to a csv file 

10
10:21:52,414 --> 10:21:56,635
let begin first we will open the notebook called classification 

11
10:21:56,635 --> 10:22:04,344
the first cell contains the classes we need to load to run this exercise 

12
10:22:04,344 --> 10:22:05,122
let run this 

13
10:22:08,152 --> 10:22:13,540
next we create sql context and load the weather data csv into a data frame 

14
10:22:13,540 --> 10:22:16,570
the second cell also prints all the columns in this data frame 

15
10:22:17,770 --> 10:22:18,270
run this 

16
10:22:23,731 --> 10:22:26,923
the third cell defines the columns in the weather data we will use for

17
10:22:26,923 --> 10:22:28,440
the decision tree classifier 

18
10:22:29,460 --> 10:22:29,960
let run it 

19
10:22:32,570 --> 10:22:34,710
we will now use the column name number 

20
10:22:34,710 --> 10:22:41,030
so let drop that from the data frame df = df drop number 

21
10:22:42,960 --> 10:22:49,422
now let revolve the rows with missing data df = df na drop 

22
10:22:51,486 --> 10:22:58,129
now let print the number of rows and columns in our resulting data frame 

23
10:22:58,129 --> 10:23:01,664
df count len df columns 

24
10:23:05,994 --> 10:23:10,800
next let create a categorical variable to denote if the humidity is low 

25
10:23:12,070 --> 10:23:17,793
we will enter binarizer = binarizer 

26
10:23:17,793 --> 10:23:21,663
the first argument specifies a threshold value for the variable 

27
10:23:21,663 --> 10:23:27,598
we want the categorical variable to be 1 if the humidity is greater than 25 

28
10:23:27,598 --> 10:23:33,555
so we will enter a threshold = 24 9999 

29
10:23:33,555 --> 10:23:38,820
the next argument specifies the column to use to create the categorical variable 

30
10:23:38,820 --> 10:23:44,409
we will input inputcol = relative humidity 3pm 

31
10:23:44,409 --> 10:23:51,610
the final argument specifies the new column name outputcol = label 

32
10:23:51,610 --> 10:23:54,206
now let create a new data frame with this categorical variable 

33
10:23:54,206 --> 10:24:00,123
binarizereddf = binarizer transform df 

34
10:24:00,123 --> 10:24:01,702
let run this 

35
10:24:03,787 --> 10:24:06,774
let look at the first four rows in this new data frame 

36
10:24:06,774 --> 10:24:12,519
we will run binarizeddf select are lative humidity 3pm 

37
10:24:12,519 --> 10:24:14,918
 label show 4 

38
10:24:18,238 --> 10:24:24,630
the relative humidity in the first row is greater than 25 and the label is 1 

39
10:24:24,630 --> 10:24:28,720
the relative humidity in the second third and fourth rows are less than 25 

40
10:24:28,720 --> 10:24:32,060
and the label is 0 

41
10:24:32,060 --> 10:24:38,014
next let aggregate the features we will use to make predictions into a single col 

42
10:24:38,014 --> 10:24:40,517
assembler = vectorassember 

43
10:24:40,517 --> 10:24:44,248
the first argument is a list of the columns to be aggregated 

44
10:24:44,248 --> 10:24:49,946
inputcols = featurecolumns and the second argument is the name

45
10:24:49,946 --> 10:24:56,876
of the new column containing the aggregated features outputcol = features 

46
10:24:56,876 --> 10:25:00,836
we can create the new data frame by running

47
10:25:00,836 --> 10:25:05,829
assembled = assembler transform binarizeddf 

48
10:25:05,829 --> 10:25:07,141
let run this 

49
10:25:09,707 --> 10:25:13,410
next we will split our data set into two parts one for

50
10:25:13,410 --> 10:25:15,972
training data and one for test data 

51
10:25:15,972 --> 10:25:21,198
you can do this by entering training

52
10:25:21,198 --> 10:25:29,693
data testdata = assembled randomsplit 0 8 

53
10:25:29,693 --> 10:25:34,602
0 2 seed = 13234 

54
10:25:34,602 --> 10:25:39,049
we can see the size of the two sets by running count 

55
10:25:39,049 --> 10:25:43,818
trainingdata count testdata count 

56
10:25:46,419 --> 10:25:48,996
next let create and train the decision tree 

57
10:25:48,996 --> 10:25:53,490
we will enter dt = decisiontreeclassifier 

58
10:25:54,510 --> 10:25:59,763
the first argument is the column we are trying to predict labelcol = label 

59
10:25:59,763 --> 10:26:04,036
the second argument is the name of the column containing your

60
10:26:04,036 --> 10:26:07,989
aggregated features featurescol = features 

61
10:26:09,310 --> 10:26:11,880
the third argument is the stopping criteria for

62
10:26:11,880 --> 10:26:16,700
tree induction based on the maximum depth of the tree maxdepth = 5 

63
10:26:16,700 --> 10:26:22,135
the fourth argument is the stopping criteria for tree induction based

64
10:26:22,135 --> 10:26:27,954
on the minimum number of samples in a node mininstancespernode = 20 

65
10:26:27,954 --> 10:26:32,942
and finally the last argument specifies the impurity measure

66
10:26:32,942 --> 10:26:38,127
used to split the nodes impurity = gini let run this 

67
10:26:40,507 --> 10:26:43,665
next we can create a model by training the decision tree 

68
10:26:43,665 --> 10:26:46,463
we will do this by executing it in a pipeline 

69
10:26:46,463 --> 10:26:54,020
we will enter pipeline = pipeline stages = dt 

70
10:26:54,020 --> 10:26:58,719
we will create them all by putting a training data model = 

71
10:26:58,719 --> 10:27:03,740
pipeline fit trainingdata 

72
10:27:03,740 --> 10:27:10,695
let run this now we can make predictions using our test data 

73
10:27:10,695 --> 10:27:16,520
we will enter predictions = model transform testdata 

74
10:27:16,520 --> 10:27:20,824
you can look at the first 10 rows of the prediction by running 

75
10:27:20,824 --> 10:27:25,619
predictions select prediction label show 10 

76
10:27:28,232 --> 10:27:31,611
you can see in the first ten rows the prediction matches the label 

77
10:27:31,611 --> 10:27:34,595
now let save our predictions to a csv file 

78
10:27:34,595 --> 10:27:38,190
in the next spark hands - on activity we will evaluate the accuracy 

79
10:27:38,190 --> 10:27:43,861
you can save it by running predictions select prediction 

80
10:27:43,861 --> 10:27:48,753
 label write save path = file file : 

81
10:27:48,753 --> 10:27:54,631
home cloudera downloads big - data - 4 predic - tions csv 

82
10:27:54,631 --> 10:27:58,365
we will specify the format to use spark csv 

83
10:27:58,365 --> 10:28:04,101
format = com databricks spark csv 

84
10:28:04,101 --> 10:28:07,735
finally we will enter header = true 

85
10:28:07,735 --> 10:28:10,823
run this to save the predictions to a csv file 

1
20:49:29,640 --> 20:49:32,850
in this activity we will be performing classification in knime 

2
20:49:33,900 --> 20:49:37,830
first we will create a new workflow and import our weather data 

3
20:49:37,830 --> 20:49:42,990
next we will remove the missing data and then create a categorical value for

4
20:49:42,990 --> 20:49:44,260
the humidity measurements 

5
20:49:45,540 --> 20:49:49,490
we will then examine summary statistics of the data before and

6
20:49:49,490 --> 20:49:54,710
after the missing data was removed and finally build a decision tree workflow 

7
20:49:58,203 --> 20:49:59,570
let begin 

8
20:49:59,570 --> 20:50:01,450
first let create a new workflow 

9
20:50:05,330 --> 20:50:06,990
we will call it classification 

10
20:50:09,650 --> 20:50:14,010
next we will import the daily weather data using the file reader node 

11
20:50:17,495 --> 20:50:24,429
configure the file reader node to use daily weather csv 

12
20:50:24,429 --> 20:50:29,250
next we will add a missing value node 

13
20:50:29,250 --> 20:50:32,020
to remove the missing values in the daily weather data 

14
20:50:37,872 --> 20:50:41,480
we will configure missing value to remove all the missing values 

15
20:50:49,112 --> 20:50:54,072
next we will use a numeric binner node to create a categorical variable for

16
20:50:54,072 --> 20:50:55,595
the humidity at 3pm 

17
20:50:57,090 --> 20:51:00,530
we will add the numeric binner node we will connect it to missing value 

18
20:51:00,530 --> 20:51:06,170
we will select the relative humidity 3pm column 

19
20:51:07,320 --> 20:51:13,144
we will create two bins the first bin we will call humidity low 

20
20:51:17,696 --> 20:51:19,450
and that value will go up to 25 

21
20:51:19,450 --> 20:51:24,885
the second bin we will 

22
20:51:24,885 --> 20:51:31,411
call humidity not low 

23
20:51:31,411 --> 20:51:34,274
we will check the append new column and

24
20:51:34,274 --> 20:51:38,045
we will call the new column low humidity day 

25
20:51:47,286 --> 20:51:51,435
next we will examine some summary statistics both before and

26
20:51:51,435 --> 20:51:54,940
after we have removed the missing values 

27
20:51:54,940 --> 20:51:57,649
we will add the statistics node to the canvas 

28
20:52:02,914 --> 20:52:06,130
we will connect the first one to the output of the file reader actor 

29
20:52:07,740 --> 20:52:10,590
we will add another one to the output of numeric binner 

30
20:52:15,550 --> 20:52:19,540
we will change the name of the first one to be before filtering 

31
20:52:23,630 --> 20:52:26,660
and change the name of the second one to after filtering 

32
20:52:30,440 --> 20:52:31,650
lets configure the first one 

33
20:52:33,650 --> 20:52:38,114
we will change the maximum number of possible values per column to 1500 

34
20:52:40,400 --> 20:52:42,090
we will also add all the columns 

35
20:52:46,130 --> 20:52:48,670
let configure the second statistics now 

36
20:52:48,670 --> 20:52:52,210
again we will change the maximum number of possible values per column to 1500 and

37
20:52:52,210 --> 20:52:53,950
we will add all the columns 

38
20:52:57,165 --> 20:52:58,360
now let run our workflow 

39
20:53:02,983 --> 20:53:06,680
let view both statistic nodes to compare the outputs 

40
20:53:10,971 --> 20:53:15,240
in the before filtering statistics we can see that there are missing values 

41
20:53:17,770 --> 20:53:22,519
however there are no missing values in the after filtering statistics 

42
20:53:26,890 --> 20:53:30,050
we can also compare the summary statistics for different measurements 

43
20:53:30,050 --> 20:53:32,450
to see that they are similar values 

44
20:53:32,450 --> 20:53:35,810
for example let take a look at air temp 9am 

45
20:53:46,505 --> 20:53:49,065
we could see that most of the statistics are the same 

46
20:53:50,435 --> 20:53:52,325
as well as the distribution of values 

47
20:53:54,365 --> 20:53:58,210
in the after filtering statistics click on the nominal tab 

48
20:53:58,210 --> 20:54:03,610
and we will look at the low humidity day categorical variable we created 

49
20:54:11,759 --> 20:54:15,719
we can see that the samples are equally distributed between the two values 

50
20:54:19,140 --> 20:54:21,290
let close these statistics views 

51
20:54:22,740 --> 20:54:24,720
next let add a column filter node 

52
20:54:32,100 --> 20:54:35,441
we will connect this to the output of numeric binner 

53
20:54:35,441 --> 20:54:37,390
double - click to configure the node 

54
20:54:37,390 --> 20:54:43,141
and we will exclude relative humidity 9am and

55
20:54:43,141 --> 20:54:48,770
relative humidity 3pm columns click ok 

56
20:54:48,770 --> 20:54:52,329
next we will add a color manager node to the canvas 

57
20:54:56,940 --> 20:55:01,850
will connect this to the output of column filter double - click to configure it 

58
20:55:03,350 --> 20:55:10,450
we will make sure the humidly low is red and humidity not low is blue 

59
20:55:10,450 --> 20:55:12,317
click ok to close 

60
20:55:12,317 --> 20:55:17,710
next we will add the partitioning node to the campus 

61
20:55:22,380 --> 20:55:25,800
we will connect this to the output of color manager 

62
20:55:25,800 --> 20:55:26,990
double - click to configure 

63
20:55:26,990 --> 20:55:31,060
we want to split the data into two partitions 

64
20:55:31,060 --> 20:55:33,920
the first partition should have 80 of the data 

65
20:55:33,920 --> 20:55:35,890
the second partition should have 20 

66
20:55:35,890 --> 20:55:40,560
to do this click on relative and change this to 80 

67
20:55:40,560 --> 20:55:44,870
we will make sure draw randomly is selected 

68
20:55:47,030 --> 20:55:53,390
and check use random seed and change the seed to 12345 

69
20:55:53,390 --> 20:55:56,700
normally we would not specific the random seed 

70
20:55:56,700 --> 20:56:01,040
however since we want repeatable results we use a specific seed value here 

71
20:56:02,260 --> 20:56:03,520
click ok to continue 

72
20:56:04,830 --> 20:56:07,770
next we will add a decision tree learner to the work flow 

73
20:56:10,790 --> 20:56:13,950
we will connect this to the top output of the partitioning node 

74
20:56:15,248 --> 20:56:18,580
double - click to configure and

75
20:56:18,580 --> 20:56:23,090
change the min number of records per node to 20 

76
20:56:23,090 --> 20:56:24,480
click ok 

77
20:56:26,330 --> 20:56:31,410
next we will add a decision tree predictor node to the canvas and we will connect

78
20:56:31,410 --> 20:56:36,310
the bottom output of partitioning to the bottom input of the predictor 

79
20:56:37,660 --> 20:56:41,760
next we will connect the output of decision tree learner

80
20:56:41,760 --> 20:56:44,420
to the top input of decision tree predictor 

81
20:56:45,530 --> 20:56:47,190
now let execute our workflow 

82
20:56:48,250 --> 20:56:51,000
we can view the resulting classification rules

83
20:56:51,000 --> 20:56:53,190
by right clicking on decision tree predictor 

84
20:56:55,130 --> 20:57:00,040
and choosing either view : decision tree view or view : decision tree view simple 

85
20:57:01,800 --> 20:57:02,610
let do the first one 

86
20:57:03,650 --> 20:57:07,100
this shows the first two splits in our decision tree 

87
20:57:07,100 --> 20:57:09,300
we can expand each branch by clicking on the plus 

88
20:57:09,300 --> 20:57:11,343
let close this 

89
20:57:11,343 --> 20:57:15,910
now let look at the simplified view 

90
20:57:17,980 --> 20:57:21,160
again we can expand the branches to see the splits in the decision tree 

91
20:57:22,910 --> 20:57:27,250
close this finally let save this workflow 

92
20:57:28,310 --> 20:57:33,150
we will analyze the results of the decision tree model in the next knime hands on 

93
20:57:33,150 --> 20:57:35,388
save the workflow by clicking on the disk icon 

1
17:47:04,160 --> 17:47:08,410
welcome back we already discussing classification models and techniques for

2
17:47:08,410 --> 17:47:10,030
next few lectures 

3
17:47:10,030 --> 17:47:12,450
let first define what the classification task is 

4
17:47:13,670 --> 17:47:18,720
after this video you will be able to define what classification is 

5
17:47:18,720 --> 17:47:23,040
explain whether classification is supervised or unsupervised and

6
17:47:23,040 --> 17:47:27,660
describe how binomial classification differs from multinomial classification 

7
17:47:29,250 --> 17:47:32,780
classification is one type of machine learning problems 

8
17:47:32,780 --> 17:47:36,340
in the classification problem the input data is presented to the machine learning

9
17:47:36,340 --> 17:47:42,620
model and the task is to predict the target corresponding to the input data 

10
17:47:42,620 --> 17:47:47,530
the target is a categorical variable so the classification task is

11
17:47:47,530 --> 17:47:52,850
to predict the category or label of the target given the input data 

12
17:47:52,850 --> 17:47:56,870
for example the classification problem illustrated in this image

13
17:47:56,870 --> 17:47:59,660
is to predict the type of weather 

14
17:47:59,660 --> 17:48:04,540
the target that the model has to predict is the weather and the possible values for

15
17:48:04,540 --> 17:48:09,490
weather in this case is sunny windy rainy or cloudy 

16
17:48:10,560 --> 17:48:15,127
the input data can consist of measurements like temperature relative humidity 

17
17:48:15,127 --> 17:48:19,380
atmospheric pressure wind speed wind direction etc 

18
17:48:20,570 --> 17:48:25,640
so given specific values for temperature relative humidity atmospheric pressure 

19
17:48:25,640 --> 17:48:30,570
etc the task for the model is to predict if the weather will be sunny 

20
17:48:30,570 --> 17:48:33,370
windy rainy or cloudy for the day 

21
17:48:35,230 --> 17:48:39,530
this is what the data set might look like for the weather classification problem 

22
17:48:39,530 --> 17:48:44,540
each row is a sample with input variables temperature humidity and

23
17:48:44,540 --> 17:48:46,980
pressure and target variable weather 

24
17:48:48,010 --> 17:48:51,580
each row has specific values for the input variables and

25
17:48:51,580 --> 17:48:53,970
a corresponding value for the target variable 

26
17:48:55,000 --> 17:48:59,580
the classification task is to predict the value of the target variable

27
17:48:59,580 --> 17:49:01,389
from the values of the input variables 

28
17:49:03,090 --> 17:49:06,950
since a target is provided we have labeled data and so

29
17:49:06,950 --> 17:49:10,470
classification is a supervised task 

30
17:49:10,470 --> 17:49:13,680
recall that in a supervised task the target or

31
17:49:13,680 --> 17:49:17,170
desired output for each sample is given 

32
17:49:17,170 --> 17:49:21,440
note that the target variable goes by many names such as target 

33
17:49:21,440 --> 17:49:26,170
label output class variable category and class 

34
17:49:28,040 --> 17:49:33,050
a classification problem can be binary or multi - class with binary

35
17:49:33,050 --> 17:49:39,520
classification the target variable has two possible values for example yes and no 

36
17:49:39,520 --> 17:49:43,500
with multi - class classification the target variable has more than

37
17:49:43,500 --> 17:49:45,320
two possible values 

38
17:49:45,320 --> 17:49:49,425
for example the target can be short medium and tall 

39
17:49:49,425 --> 17:49:54,805
multi - class classification is also referred to multinomial or

40
17:49:54,805 --> 17:49:56,200
multi - label classification 

41
17:49:57,230 --> 17:50:01,600
remember though that the target is always a categorical variable in classification 

42
17:50:03,270 --> 17:50:07,270
some examples of binary classification are predicting

43
17:50:07,270 --> 17:50:12,380
whether it will rain tomorrow or not here there are two possible outcomes 

44
17:50:12,380 --> 17:50:15,450
yes it will rain tomorrow or no it will not rain tomorrow 

45
17:50:16,890 --> 17:50:21,360
identifying whether a credit card transaction is legitimate or fraudulent 

46
17:50:21,360 --> 17:50:25,660
again there are only two possible values for the target legitimate or fraudulent 

47
17:50:26,910 --> 17:50:30,780
some examples of multi - class classification include

48
17:50:30,780 --> 17:50:34,610
predicting what type of product that a customer will buy 

49
17:50:34,610 --> 17:50:37,540
the possible values for the target variables would be product

50
17:50:37,540 --> 17:50:41,960
categories such as kitchen electronics clothes etc 

51
17:50:41,960 --> 17:50:45,580
there is more than one category of products so

52
17:50:45,580 --> 17:50:48,210
this is a multi - class classification problem 

53
17:50:49,430 --> 17:50:54,530
another example is categorizing a tweet as having a positive negative or

54
17:50:54,530 --> 17:50:58,090
neutral sentiment again the number of possible values for

55
17:50:58,090 --> 17:50:59,830
the target is more than two here 

56
17:50:59,830 --> 17:51:04,250
so this is also a multi - class classification task

57
17:51:04,250 --> 17:51:06,330
to summarize in classification 

58
17:51:06,330 --> 17:51:10,770
the model has to predict the category corresponding to the input data 

59
17:51:10,770 --> 17:51:15,870
since the target is provided for each sample classification is a supervised

60
17:51:15,870 --> 17:51:20,519
task the target variable is always categorical in classification 

1
11:38:24,120 --> 11:38:25,080
in this lecture 

2
11:38:25,080 --> 11:38:30,490
we will look at the decision tree model a popular method used for classification 

3
11:38:30,490 --> 11:38:35,500
after this video you will be able to explain how a decision tree is used for

4
11:38:35,500 --> 11:38:37,120
classification 

5
11:38:37,120 --> 11:38:41,650
describe the process of constructing a decision tree for classification 

6
11:38:41,650 --> 11:38:45,520
and interpret how a decision tree comes up with a classification decision 

7
11:38:46,530 --> 11:38:50,520
the idea behind decision trees for classification is to split the data

8
11:38:50,520 --> 11:38:56,040
into subsets where each subset belongs to only one class 

9
11:38:56,040 --> 11:39:00,900
this is accomplished by dividing the input space into pure regions 

10
11:39:00,900 --> 11:39:04,210
that is regions with samples from only one class 

11
11:39:05,420 --> 11:39:09,740
with real data completely pure subsets may not be possible 

12
11:39:09,740 --> 11:39:15,006
so the goal is to divide the data into subsets that are as pure as possible 

13
11:39:15,006 --> 11:39:20,320
that is each subset contains as many samples as possible from a single class 

14
11:39:21,450 --> 11:39:26,050
graphically this is equivalent to dividing the input space into regions that are as

15
11:39:26,050 --> 11:39:27,940
pure as possible 

16
11:39:27,940 --> 11:39:32,760
boundaries separating these regions are called decision boundaries 

17
11:39:32,760 --> 11:39:36,450
and the decision tree model makes classification decisions

18
11:39:36,450 --> 11:39:38,600
based on these decision boundaries 

19
11:39:39,760 --> 11:39:46,040
a decision tree is a hierarchical structure with nodes and directed edges 

20
11:39:46,040 --> 11:39:48,560
the node at the top is called the root node 

21
11:39:49,600 --> 11:39:51,870
the nodes at the bottom are called the leaf nodes 

22
11:39:53,000 --> 11:39:58,810
nodes that are neither the root node or the leaf nodes are called internal nodes 

23
11:39:58,810 --> 11:40:01,530
the root and internal nodes have test conditions 

24
11:40:02,580 --> 11:40:05,740
each leaf node has a class label associated with it 

25
11:40:06,770 --> 11:40:11,250
a classification decision is made by traversing the decision tree

26
11:40:11,250 --> 11:40:12,280
starting with the root node 

27
11:40:13,620 --> 11:40:17,470
at each node the answer to the test condition determines which

28
11:40:17,470 --> 11:40:19,740
branch to traverse to 

29
11:40:19,740 --> 11:40:23,330
when a leaf node is reached the category at the leaf node

30
11:40:23,330 --> 11:40:25,595
determines the classification decision 

31
11:40:26,740 --> 11:40:32,590
the depth of a node is the number of edges from the root node to that node 

32
11:40:32,590 --> 11:40:34,130
the depth of the root node is 0 

33
11:40:34,130 --> 11:40:38,620
the depth of a decision tree is the number of edges

34
11:40:38,620 --> 11:40:42,700
in the longest path from the root node to the leaf node 

35
11:40:42,700 --> 11:40:46,280
the size of a decision tree is the number of nodes in the tree 

36
11:40:47,460 --> 11:40:50,160
this is an example of a decision tree 

37
11:40:50,160 --> 11:40:55,290
it can be used to classify an animal as a mammal or not a mammal 

38
11:40:55,290 --> 11:40:59,530
according to this decision tree if an animal is warm - blooded 

39
11:40:59,530 --> 11:41:04,570
gives live birth and is a vertebrate then it is a mammal 

40
11:41:04,570 --> 11:41:08,000
if an animal does not have all of these three characteristics 

41
11:41:08,000 --> 11:41:09,350
then it is not a mammal 

42
11:41:10,520 --> 11:41:14,790
a decision tree is built by starting with all samples at a single node 

43
11:41:14,790 --> 11:41:16,400
the root node 

44
11:41:16,400 --> 11:41:21,170
additional nodes are added when the data is split into subsets 

45
11:41:21,170 --> 11:41:25,449
at a high level constructing a decision tree consists of the following steps 

46
11:41:26,860 --> 11:41:28,650
start with all samples and a node 

47
11:41:30,180 --> 11:41:34,850
partition the samples into subsets based in the input variables 

48
11:41:34,850 --> 11:41:39,250
the goal is to create subsets of records that are purest that is

49
11:41:39,250 --> 11:41:43,990
each subset contains as many samples as possible belonging to just one class 

50
11:41:45,260 --> 11:41:49,557
another way to say this is that the subsets should be as homogeneous or

51
11:41:49,557 --> 11:41:50,900
as pure as possible 

52
11:41:52,120 --> 11:41:56,230
repeatedly partition data into successively purer subsets

53
11:41:56,230 --> 11:41:58,460
until some stopping criterion is satisfied 

54
11:41:59,990 --> 11:42:00,930
an algorithm for

55
11:42:00,930 --> 11:42:05,720
constructing a decision tree model is referred to as an induction algorithm 

56
11:42:05,720 --> 11:42:09,410
so you may hear the term tree induction used to describe the process of

57
11:42:09,410 --> 11:42:10,690
building a decision tree 

58
11:42:12,330 --> 11:42:15,150
note that at each split the induction algorithm

59
11:42:15,150 --> 11:42:19,820
only considers the best way to split that particular portion of the data 

60
11:42:19,820 --> 11:42:22,660
this is referred to as a greedy approach 

61
11:42:22,660 --> 11:42:26,800
greedy algorithms solve a subset of the problem at a time and

62
11:42:26,800 --> 11:42:31,510
as a necessary approach when solving the entire problem is not feasible 

63
11:42:31,510 --> 11:42:34,020
this is the case with decision trees 

64
11:42:34,020 --> 11:42:38,090
it is not feasible to determine the best tree given a data set so

65
11:42:38,090 --> 11:42:41,460
the tree has to be built in piecemeal fashion

66
11:42:41,460 --> 11:42:44,978
by determining the best way to split the current node at each step 

67
11:42:44,978 --> 11:42:49,485
and combining these decisions together to form the final decision tree 

68
11:42:49,485 --> 11:42:54,600
in constructing a decision tree how is the data partitioned 

69
11:42:54,600 --> 11:42:57,520
how does a decision tree determine the best way to split

70
11:42:57,520 --> 11:42:59,920
the set of samples at a node 

71
11:42:59,920 --> 11:43:04,530
again the goal is to partition data at a node into subsets that are as

72
11:43:04,530 --> 11:43:05,700
pure as possible 

73
11:43:06,760 --> 11:43:08,110
in this example 

74
11:43:08,110 --> 11:43:12,640
the partition shown on the right results in more homogeneous subsets 

75
11:43:12,640 --> 11:43:17,620
since these subsets contain more samples belonging to a single class

76
11:43:17,620 --> 11:43:20,950
than the resulting subsets shown on the left 

77
11:43:20,950 --> 11:43:24,800
so the partition on the right results in purer subsets and

78
11:43:24,800 --> 11:43:26,180
is the preferred partition 

79
11:43:27,440 --> 11:43:31,650
therefore we need a way to measure the purity of a split

80
11:43:31,650 --> 11:43:35,910
in order to compare different ways to partition a set of data 

81
11:43:35,910 --> 11:43:40,980
it turns out that it works out better mathematically if we measure the impurity

82
11:43:40,980 --> 11:43:42,820
rather than the purity of a split 

83
11:43:43,830 --> 11:43:46,370
so the impurity measure of a node

84
11:43:46,370 --> 11:43:49,730
specifies how mixed the resulting subsets are 

85
11:43:49,730 --> 11:43:53,580
since we want the resulting subsets to have homogeneous class labels 

86
11:43:53,580 --> 11:43:59,230
not mixed class labels we want the split that minimizes the impurity measure 

87
11:44:00,720 --> 11:44:02,610
a common impurity measure used for

88
11:44:02,610 --> 11:44:04,900
determining the best split is the gini index 

89
11:44:05,900 --> 11:44:10,440
the lower the gini index the higher the purity of the split 

90
11:44:10,440 --> 11:44:15,470
so the decision tree will select the split that minimizes the gini index 

91
11:44:15,470 --> 11:44:19,710
besides the gini index other impurity measures include entropy or

92
11:44:19,710 --> 11:44:22,571
information gain and misclassification rate 

93
11:44:24,140 --> 11:44:27,750
the other factor in determining the best way to partition a node

94
11:44:27,750 --> 11:44:30,500
is which variable to split on 

95
11:44:30,500 --> 11:44:35,910
the decision tree will test all variables to determine the best way to split a node

96
11:44:35,910 --> 11:44:39,860
using a purity measure such as the gini index to compare the various

97
11:44:39,860 --> 11:44:40,709
possibilities 

98
11:44:41,910 --> 11:44:46,880
recall that the tree induction algorithm repeatedly splits nodes to get more and

99
11:44:46,880 --> 11:44:48,890
more homogeneous subsets 

100
11:44:48,890 --> 11:44:50,970
so when does this process stop 

101
11:44:50,970 --> 11:44:53,150
when does the algorithm stop growing the tree 

102
11:44:54,150 --> 11:44:58,670
there several criteria that can be used to determine when a node should no longer

103
11:44:58,670 --> 11:44:59,935
be split into subsets 

104
11:45:01,830 --> 11:45:04,730
the induction algorithm can stop expanding a node when

105
11:45:04,730 --> 11:45:08,500
all samples in the node have the same class label 

106
11:45:08,500 --> 11:45:12,370
this means that this set of data is as pure as possible and

107
11:45:12,370 --> 11:45:16,030
further splitting will not result in any better partition of the data 

108
11:45:17,230 --> 11:45:21,890
since getting completely pure subsets is difficult to achieve with real data 

109
11:45:21,890 --> 11:45:24,620
this stopping criterion can be modified 

110
11:45:24,620 --> 11:45:28,961
to when a certain percentage of the samples in the node say 90 for

111
11:45:28,961 --> 11:45:31,400
example have the same class labels 

112
11:45:32,750 --> 11:45:36,710
the algorithm can stop expanding a node when the number of samples in the node

113
11:45:36,710 --> 11:45:38,940
falls below a certain minimum value 

114
11:45:39,940 --> 11:45:42,920
a this point the number of samples is too small to make

115
11:45:42,920 --> 11:45:46,119
much difference in the classification results with the further splitting 

116
11:45:47,570 --> 11:45:51,860
the induction algorithm can stop expanding a node when the improvement in impurity

117
11:45:51,860 --> 11:45:56,540
measure is too small to make much of a difference in classification results 

118
11:45:58,060 --> 11:46:02,980
the algorithm can stop expanding a node when the maximum tree depth is reached 

119
11:46:02,980 --> 11:46:06,240
this is to control the complexity of the resulting tree 

120
11:46:08,060 --> 11:46:11,590
there can be other criteria that can be used to determine when tree induction

121
11:46:11,590 --> 11:46:12,090
should stop 

122
11:46:13,720 --> 11:46:18,140
let take a look at an example to illustrate the tree induction process 

123
11:46:18,140 --> 11:46:22,720
let say that we want to classify loan applicants as being likely to repay a loan

124
11:46:22,720 --> 11:46:26,090
or not likely to repay a loan based on their income and

125
11:46:26,090 --> 11:46:27,750
amount of debt they already have 

126
11:46:29,090 --> 11:46:30,360
building a decision tree for

127
11:46:30,360 --> 11:46:33,490
this classification problem could proceed as follows 

128
11:46:33,490 --> 11:46:37,230
consider the input space of this problem as shown in the left figure 

129
11:46:37,230 --> 11:46:41,990
one way to split this data set into homogeneous

130
11:46:41,990 --> 11:46:46,380
subsets is to consider the decision boundary where income equals t1 

131
11:46:47,800 --> 11:46:51,410
to the right of this decision boundary are mostly red samples and

132
11:46:51,410 --> 11:46:53,140
to the left are mostly blue samples 

133
11:46:54,340 --> 11:46:56,950
the subsets are not completely homogeneous but

134
11:46:56,950 --> 11:47:01,100
that is the best way to split the original data set based on the variable income 

135
11:47:03,120 --> 11:47:07,380
this decision boundary is represented in the decision tree by the condition

136
11:47:07,380 --> 11:47:11,326
income is greater than t1 at the root node 

137
11:47:11,326 --> 11:47:14,894
this is the condition used to split the original data set 

138
11:47:14,894 --> 11:47:18,350
samples with income greater than the threshold value

139
11:47:18,350 --> 11:47:22,800
of t1 are placed in the right subset and samples with income less than or

140
11:47:22,800 --> 11:47:27,950
equal to t1 are placed in the left subset just as shown in the right diagram 

141
11:47:29,480 --> 11:47:31,748
the right subset is now labeled as red 

142
11:47:31,748 --> 11:47:35,203
meaning that the loan applicant is likely to be paid alone 

143
11:47:36,688 --> 11:47:41,980
the second step then is to determine how to split the region outlined in red 

144
11:47:41,980 --> 11:47:47,000
as shown in the left diagram in input space the best way to split this data

145
11:47:47,000 --> 11:47:50,940
is specified by the second decision boundary with debt equals t2 

146
11:47:52,520 --> 11:47:55,580
this is represented in the decision tree on the right

147
11:47:55,580 --> 11:47:59,760
with the addition of the node with condition debt greater than t2 

148
11:48:01,050 --> 11:48:04,930
samples with the value of debt greater than t2 are shown in the region

149
11:48:04,930 --> 11:48:07,010
above the decision boundary 

150
11:48:07,010 --> 11:48:12,370
this region contains all blue samples and so the corresponding node is labeled blue 

151
11:48:12,370 --> 11:48:15,700
meaning that the loan applicant is not likely to repay the loan 

152
11:48:17,170 --> 11:48:21,180
the third and final split looks at how to split the region outlined in red

153
11:48:21,180 --> 11:48:22,010
in the left diagram 

154
11:48:23,210 --> 11:48:27,930
the best split is specified by the boundary with income equals t3 

155
11:48:27,930 --> 11:48:31,190
this splits the red region into two pure subsets 

156
11:48:32,250 --> 11:48:36,330
this split is represented in the decision tree by adding a node with condition 

157
11:48:36,330 --> 11:48:38,629
income is greater than t3 

158
11:48:38,629 --> 11:48:42,210
the left resulting node is labeled blue and

159
11:48:42,210 --> 11:48:44,980
the right resulting node is labeled red 

160
11:48:44,980 --> 11:48:49,200
corresponding to the resulting subsets within the red border in the left diagram 

161
11:48:50,780 --> 11:48:53,810
we end with the final decision tree on the right 

162
11:48:53,810 --> 11:48:58,400
which implements the decision boundaries shown as dash lines in the left diagram 

163
11:48:59,630 --> 11:49:03,280
these decision boundaries partition the data set as shown 

164
11:49:03,280 --> 11:49:08,200
the label for each region is determined by the label of the majority of the samples 

165
11:49:08,200 --> 11:49:12,090
these labels are reflected in the leaf nodes of the decision tree 

166
11:49:12,090 --> 11:49:12,910
shown on the right 

167
11:49:14,500 --> 11:49:18,850
you may have noticed that the decision boundaries of a decision tree are parallel

168
11:49:18,850 --> 11:49:25,110
to the axes formed by the variables this is referred to as being rectilinear 

169
11:49:25,110 --> 11:49:29,410
the boundaries are rectilinear because each split considers only

170
11:49:29,410 --> 11:49:31,370
a single variable 

171
11:49:31,370 --> 11:49:35,185
there are variance of the tree induction algorithm that consider more than one

172
11:49:35,185 --> 11:49:37,560
attribute when splitting a note 

173
11:49:37,560 --> 11:49:42,550
however each split has to consider all combinations of combined variables and

174
11:49:42,550 --> 11:49:47,000
so such induction algorithms are much more computationally expensive 

175
11:49:48,310 --> 11:49:51,650
there are a few important things to note about the decision tree classifier 

176
11:49:53,160 --> 11:49:56,970
the resulting tree is often simple to understand and interpret 

177
11:49:56,970 --> 11:50:01,500
this is one of the biggest advantages of decision trees for classification 

178
11:50:01,500 --> 11:50:04,550
it is often possible to look at the resulting tree to see

179
11:50:04,550 --> 11:50:08,570
which variables are important to the classification problem and

180
11:50:08,570 --> 11:50:11,840
understand how the classification is performed 

181
11:50:11,840 --> 11:50:15,590
for this reason many people will start out with the decision tree classifier

182
11:50:15,590 --> 11:50:18,690
to get a feel for the classification problem 

183
11:50:18,690 --> 11:50:21,730
even if they end up using more sophisticated models later on 

184
11:50:23,730 --> 11:50:26,480
the tree induction algorithm as described in this lesson

185
11:50:26,480 --> 11:50:29,990
is relatively computationally inexpensive 

186
11:50:29,990 --> 11:50:33,940
so training a decision tree for classification can be relatively fast 

187
11:50:35,985 --> 11:50:40,495
the greedy approach used by tree induction algorithm determines the best way to split

188
11:50:40,495 --> 11:50:42,905
the portion of the data at a node but

189
11:50:42,905 --> 11:50:46,823
does not guarantee the best solution overall for the entire data set 

190
11:50:49,000 --> 11:50:51,490
decision boundaries are rectilinear 

191
11:50:51,490 --> 11:50:55,550
this can limit the expressiveness of the resulting model which means

192
11:50:55,550 --> 11:50:59,480
that it may not be able to solve complicated classification problems that

193
11:50:59,480 --> 11:51:02,400
require more complex decision boundaries to be formed 

194
11:51:04,060 --> 11:51:09,280
in summary the decision tree classifier uses a tree like structure to specify

195
11:51:09,280 --> 11:51:14,600
a series of conditions that are tested to determine the class label for a sample 

196
11:51:15,740 --> 11:51:20,230
the decision tree is constructed by repeatedly splitting a data partition

197
11:51:20,230 --> 11:51:23,860
into successively more homogeneous subsets 

198
11:51:23,860 --> 11:51:26,680
the resulting tree can often be easy to interpret 

1
23:29:49,681 --> 23:29:54,059
we will start with a very simple classification technique called k - nearest

2
23:29:54,059 --> 23:29:54,807
neighbors 

3
23:29:54,807 --> 23:29:59,451
after this video you will be able to describe how knn is used for

4
23:29:59,451 --> 23:30:01,710
classification 

5
23:30:01,710 --> 23:30:06,995
discuss the assumption behind knn and explain what the k stands for in knn 

6
23:30:06,995 --> 23:30:10,995
knn stands for k - nearest neighbors 

7
23:30:10,995 --> 23:30:14,924
this is one of the simplest techniques to build a classification model 

8
23:30:14,924 --> 23:30:19,623
the basic idea is to classify a sample based on its neighbors 

9
23:30:19,623 --> 23:30:24,774
so when you get a new sample as shown by the green circle in the figure the class

10
23:30:24,774 --> 23:30:29,944
label for that sample is determined by looking at the labels of its neighbors 

11
23:30:29,944 --> 23:30:33,030
knn relies on the notion of the so - called duck test 

12
23:30:33,030 --> 23:30:37,724
that is if it looks like a duck swims like a duck and quacks like a duck 

13
23:30:37,724 --> 23:30:40,430
then it probably is a duck 

14
23:30:40,430 --> 23:30:42,260
in the classification context 

15
23:30:42,260 --> 23:30:48,320
this means that samples with similar input values likely belong to the same class 

16
23:30:48,320 --> 23:30:52,640
so samples with similar input values should be labeled with the same

17
23:30:52,640 --> 23:30:54,390
target label 

18
23:30:54,390 --> 23:30:57,820
this means that classification of a sample is dependent

19
23:30:57,820 --> 23:31:01,250
on the target labels of the neighboring points 

20
23:31:01,250 --> 23:31:04,140
in more detail then this is how knn works 

21
23:31:04,140 --> 23:31:06,084
given a new sample look for

22
23:31:06,084 --> 23:31:10,710
the samples in the training data that are closest to the new sample 

23
23:31:10,710 --> 23:31:12,740
these are the neighbors 

24
23:31:12,740 --> 23:31:17,550
use the labels of this neighboring points to determine the label for the new sample 

25
23:31:18,590 --> 23:31:21,410
this figure illustrate how knn works 

26
23:31:21,410 --> 23:31:25,240
the problem here is to determine if a sample should be classified as

27
23:31:25,240 --> 23:31:26,940
a blue square or red triangle 

28
23:31:27,990 --> 23:31:29,980
the green circle is the new sample 

29
23:31:29,980 --> 23:31:34,780
to determine a class label for this new sample look at its closest neighbors 

30
23:31:34,780 --> 23:31:38,230
these neighbors are the samples within the dashed circle 

31
23:31:39,270 --> 23:31:42,180
two blue squares and one red triangle 

32
23:31:42,180 --> 23:31:45,590
the class labels of the neighboring samples determine the label for

33
23:31:45,590 --> 23:31:46,290
the new sample 

34
23:31:47,690 --> 23:31:52,480
the value of k determines the number of nearest neighbor to consider 

35
23:31:52,480 --> 23:31:57,160
so if k equals 1 then only the closest neighbor is examined

36
23:31:57,160 --> 23:32:01,030
to determine the class of the new sample as shown in the left figure 

37
23:32:02,170 --> 23:32:03,197
if k equals 2 

38
23:32:03,197 --> 23:32:08,182
then the 2 nearest neighbors are considered as seen in the middle figure 

39
23:32:08,182 --> 23:32:12,815
if k equal 3 then the 3 nearest neighbors are considered as

40
23:32:12,815 --> 23:32:15,101
in the right figure and so on 

41
23:32:15,101 --> 23:32:19,311
if k equal 1 and only 1 neighbor is used then the label for

42
23:32:19,311 --> 23:32:23,030
the new sample is simpler the label of the neighbor 

43
23:32:23,030 --> 23:32:26,090
this is shown in the left figure 

44
23:32:26,090 --> 23:32:28,610
the label of the new sample is then a 

45
23:32:28,610 --> 23:32:31,510
since that is the label of its one nearest neighbor 

46
23:32:32,630 --> 23:32:37,270
when multiple neighbors are considered then a voting scheme is used 

47
23:32:37,270 --> 23:32:41,290
majority of vote is commonly used so the label associated with

48
23:32:41,290 --> 23:32:46,670
the majority of the neighbors is used as the label of the new sample 

49
23:32:46,670 --> 23:32:49,730
this is what we see in the right figure 

50
23:32:49,730 --> 23:32:53,090
with k equals 3 3 nearest neighbors are considered 

51
23:32:54,398 --> 23:32:57,830
with two neighbors labeled as a and one as b 

52
23:32:57,830 --> 23:33:01,560
the majority of vote determines that the new sample should be labeled as a 

53
23:33:02,680 --> 23:33:07,168
in case of a tie which could be possible if the value of k is even 

54
23:33:07,168 --> 23:33:10,154
then some tight breaking rule is needed 

55
23:33:10,154 --> 23:33:13,853
for example the label of the closer neighbor is used or

56
23:33:13,853 --> 23:33:17,250
the label is chosen randomly among the neighbors 

57
23:33:17,250 --> 23:33:19,480
this is seen in the middle figure 

58
23:33:20,550 --> 23:33:24,970
with two nearest neighbors and each with a different class label the label for

59
23:33:24,970 --> 23:33:27,640
the new sample is randomly chosen here to be b 

60
23:33:29,000 --> 23:33:33,230
with knn some measure of similarity is needed to determine how

61
23:33:33,230 --> 23:33:36,030
close two samples are together 

62
23:33:36,030 --> 23:33:39,690
this is necessary to determine which samples are the nearest neighbors 

63
23:33:40,690 --> 23:33:44,650
distance measures such as distance are commonly used 

64
23:33:44,650 --> 23:33:48,480
other distance measures that can be used include manhattan and hemming distance 

65
23:33:49,640 --> 23:33:54,671
to summarize knn is a very simple classification technique 

66
23:33:54,671 --> 23:33:57,970
note that there is no separate training phase 

67
23:33:57,970 --> 23:34:02,490
there is no separate part where a model is constructed and its parameter is adjusted 

68
23:34:02,490 --> 23:34:05,970
this is unlike most other classification algorithms 

69
23:34:07,510 --> 23:34:11,230
knn can generate complex decision boundaries allowing for

70
23:34:11,230 --> 23:34:13,980
complex classification decisions to be made 

71
23:34:15,400 --> 23:34:17,410
it can be susceptible to noise however 

72
23:34:17,410 --> 23:34:21,910
because classification decisions are made using only information about

73
23:34:21,910 --> 23:34:24,735
a few neighboring points instead of the entire dataset 

74
23:34:26,570 --> 23:34:30,540
knn can be slow however since the distance between a new sample and

75
23:34:30,540 --> 23:34:34,630
all sample points in the data must be calculated in order to determine

76
23:34:34,630 --> 23:34:38,796
the k - nearest neighbors 

1
23:04:27,920 --> 23:04:32,020
in this lecture we will discuss the naive bayes classifier 

2
23:04:32,020 --> 23:04:36,680
after this video you will be able to discuss how a naive bayes model

3
23:04:36,680 --> 23:04:41,550
works fro classification define the components of bayes rule and

4
23:04:41,550 --> 23:04:44,349
explain what the naive means in naive bayes 

5
23:04:45,570 --> 23:04:51,500
a naive bayes classification model uses a probabilistic approach to classification 

6
23:04:51,500 --> 23:04:55,190
what this means is that the relationships between the input features and

7
23:04:55,190 --> 23:04:58,980
the class is expressed as probabilities 

8
23:04:58,980 --> 23:05:03,390
so given the input features for a sample the probability for

9
23:05:03,390 --> 23:05:05,880
each class is estimated 

10
23:05:05,880 --> 23:05:10,980
the class with the highest probability then determines the label for the sample 

11
23:05:12,800 --> 23:05:17,250
in addition to using a probabilistic framework for classification 

12
23:05:17,250 --> 23:05:22,060
the naive bayes classifier also uses what is known as bayes theorem 

13
23:05:22,060 --> 23:05:25,930
the application of bayes theorem makes estimating the probabilities easier 

14
23:05:27,100 --> 23:05:31,850
in addition naive bayes assumes that the input features are statistically

15
23:05:31,850 --> 23:05:34,400
independent of one another 

16
23:05:34,400 --> 23:05:37,070
this means that for a given class 

17
23:05:37,070 --> 23:05:42,060
the value of one feature does not affect the value of any other feature 

18
23:05:42,060 --> 23:05:46,060
this independence assumption is an oversimplified one that does not always

19
23:05:46,060 --> 23:05:50,330
hold true and so is considered a naive assumption 

20
23:05:50,330 --> 23:05:52,510
the naive independence assumption and

21
23:05:52,510 --> 23:05:55,830
the use of bayes theorem gives this classification model its name 

22
23:05:56,910 --> 23:05:58,320
we will cover bayes theorem and

23
23:05:58,320 --> 23:06:01,120
the independence assumption in more detail in this lecture 

24
23:06:02,580 --> 23:06:05,200
before we look at naive bayes in more detail 

25
23:06:05,200 --> 23:06:07,630
let first start with some background on probability 

26
23:06:08,800 --> 23:06:12,970
probability is the measure of how likely an event is 

27
23:06:12,970 --> 23:06:18,060
the probability of an event a occurring is denoted p and in parenthesis a 

28
23:06:19,330 --> 23:06:24,710
it is calculated by dividing the number of ways event a can occur 

29
23:06:24,710 --> 23:06:26,769
by the total number of possible outcomes 

30
23:06:28,370 --> 23:06:34,170
for example what is the probability of rolling a die and getting six 

31
23:06:34,170 --> 23:06:37,890
when you roll a die you can get a number from one to six so

32
23:06:37,890 --> 23:06:40,060
the number of possible outcomes is six 

33
23:06:41,330 --> 23:06:44,310
the number of ways fro getting six is one 

34
23:06:44,310 --> 23:06:48,850
since the way you can get six is if the die shows six when it stops rolling 

35
23:06:49,850 --> 23:06:54,800
that means that the probability of getting the number six when you roll a die

36
23:06:54,800 --> 23:06:57,080
is one over six 

37
23:06:57,080 --> 23:07:01,790
this is denoted p of six and that equal to one over six and

38
23:07:01,790 --> 23:07:05,670
is read as probability of six is one over six 

39
23:07:07,440 --> 23:07:12,020
there also a joint probability the joint probability specifies

40
23:07:12,020 --> 23:07:16,930
the probability of event a and event b occurring together 

41
23:07:18,090 --> 23:07:23,620
in this diagram the probability of event a occurring is shown as the blue circle

42
23:07:24,650 --> 23:07:28,710
and the probability of event b occurring is shown as the green circle 

43
23:07:29,770 --> 23:07:34,630
then the joint probability that is the probability of a and

44
23:07:34,630 --> 23:07:39,340
b occurring together is shown as the overlap of these two circles 

45
23:07:40,470 --> 23:07:45,635
the joint probability of a and b is denoted 

46
23:07:45,635 --> 23:07:48,610
p a b for

47
23:07:48,610 --> 23:07:53,860
an example of joint probability let consider rolling 2 dice together 

48
23:07:53,860 --> 23:07:58,090
what is the probability in getting 2 sixes or a six from each die 

49
23:07:59,250 --> 23:08:04,020
if the two events are independent then the joint probability is simply the result

50
23:08:04,020 --> 23:08:07,880
of multiplying the probabilities of the individual events together 

51
23:08:09,000 --> 23:08:12,690
in this case then we have the probability of rolling a six for

52
23:08:12,690 --> 23:08:18,040
each die is one over six so the joint probability is one over 36 

53
23:08:18,040 --> 23:08:23,480
this leads us to conditional probability 

54
23:08:23,480 --> 23:08:28,250
the conditional probability is the probability of event a occurring

55
23:08:28,250 --> 23:08:31,090
given that event b has already occurred 

56
23:08:32,150 --> 23:08:38,570
another way to say this is that event a is conditioned on event b 

57
23:08:38,570 --> 23:08:43,170
the conditional probability is the noted p and in parentheses a 

58
23:08:43,170 --> 23:08:48,390
vertical line b and is read as probability of a given b 

59
23:08:49,730 --> 23:08:54,510
this diagram gives a graphical definition of conditional probability 

60
23:08:54,510 --> 23:09:00,030
as before the blue circle is the probability of event a occurring 

61
23:09:00,030 --> 23:09:04,260
the green circle is the probability of event b occurring 

62
23:09:04,260 --> 23:09:07,610
the overlap is a joint probability of a and b 

63
23:09:08,730 --> 23:09:13,500
the conditional probability p a given b then is

64
23:09:13,500 --> 23:09:18,870
calculated as the join probability divided by the probability of b 

65
23:09:20,040 --> 23:09:23,980
the conditional probability is an important concept in classification

66
23:09:23,980 --> 23:09:25,800
as we will see later 

67
23:09:25,800 --> 23:09:30,020
it provides the means to specify the probability of a class label 

68
23:09:30,020 --> 23:09:31,240
given the input values 

69
23:09:32,960 --> 23:09:39,390
the relationship between conditional probabilities p of b given a and

70
23:09:39,390 --> 23:09:44,295
p of a given b can be expressed through bayes theorem 

71
23:09:44,295 --> 23:09:48,811
this theorem is named after a reverend named thomas bayes who lived in the 1700s 

72
23:09:48,811 --> 23:09:53,691
it is a way to look at how the probability of a hypothesis is affected by

73
23:09:53,691 --> 23:09:56,170
new evidence gathered from data 

74
23:09:56,170 --> 23:10:01,925
bayes theorem expresses the relationship between probability of b

75
23:10:01,925 --> 23:10:07,285
given a and probability of a given b as shown in this equation 

76
23:10:07,285 --> 23:10:13,033
bayes theorem is also known as bayes rule or bayes law 

77
23:10:13,033 --> 23:10:16,750
now that we have reviewed some background on probability

78
23:10:16,750 --> 23:10:19,610
let see how all this relates to the classification problem 

79
23:10:20,840 --> 23:10:25,249
with the probabilistic framework the classification task is defined as follows 

80
23:10:26,530 --> 23:10:30,930
capital x is the set of values for the input features in the sample 

81
23:10:32,090 --> 23:10:36,780
given a sample with features x predict the corresponding class c 

82
23:10:37,860 --> 23:10:42,930
another way to state this is what is the class label associated with

83
23:10:42,930 --> 23:10:48,550
the feature vector x or how should the feature vector x be classified 

84
23:10:49,910 --> 23:10:54,470
to find the class label c we need to calculate the conditional probability of

85
23:10:54,470 --> 23:10:58,540
class c given x for all classes and

86
23:10:58,540 --> 23:11:00,990
select a class with the highest probability 

87
23:11:02,450 --> 23:11:04,010
so for classification 

88
23:11:04,010 --> 23:11:10,460
we want to find the value of c that maximizes the probability of c given x 

89
23:11:10,460 --> 23:11:14,140
the problem is that it is difficult to estimate this probability 

90
23:11:14,140 --> 23:11:19,530
because we would need to enumerate every possible combination of feature values and

91
23:11:19,530 --> 23:11:21,460
to know the conditional probability 

92
23:11:21,460 --> 23:11:26,570
for each class given every possible feature combination and

93
23:11:26,570 --> 23:11:29,330
here is where bayes theorem comes into play 

94
23:11:29,330 --> 23:11:33,710
the classification problem can be reformulated using bayes theorem

95
23:11:33,710 --> 23:11:35,876
to simplify the classification problem 

96
23:11:35,876 --> 23:11:43,060
specifically using bayes theorem the probability of c given x can be expressed

97
23:11:43,060 --> 23:11:47,600
using other probability quantities which can be estimated from the data 

98
23:11:49,030 --> 23:11:53,010
here bayes theorem again but some additional terms defined 

99
23:11:53,010 --> 23:11:58,870
probability of c i x is referred to as the posterior probability since

100
23:11:58,870 --> 23:12:04,930
it is the probability of the class label being c after observing input features x 

101
23:12:06,400 --> 23:12:10,850
probability of x given c is the probability of

102
23:12:10,850 --> 23:12:15,450
observing input features x given that c is the class label 

103
23:12:16,540 --> 23:12:21,480
this is the class conditional probability since it is conditioned on the class 

104
23:12:22,790 --> 23:12:27,440
probability of c is the probability of the class label being c 

105
23:12:28,470 --> 23:12:33,560
this is the probability of each class prior to observing any input data 

106
23:12:33,560 --> 23:12:36,350
and so is referred to as the prior probability 

107
23:12:37,810 --> 23:12:41,240
the probability of x is the probability

108
23:12:41,240 --> 23:12:45,479
of observing input features x regardless of what the class label is 

109
23:12:47,070 --> 23:12:51,617
so for classification we want to calculate the posterior

110
23:12:51,617 --> 23:12:54,939
probability p c x for each class c 

111
23:12:54,939 --> 23:13:01,343
from bayes theorem p c x is related to

112
23:13:01,343 --> 23:13:07,250
the p x c p c and probability of x 

113
23:13:08,790 --> 23:13:12,680
probability of x does not depend on the class c therefore 

114
23:13:12,680 --> 23:13:16,040
it is a constant value given the input x 

115
23:13:16,040 --> 23:13:19,940
since it the same value for all classes the probability

116
23:13:19,940 --> 23:13:24,610
of x can be removed from the calculation of probability of c given x 

117
23:13:25,610 --> 23:13:30,910
what is left then are probability of x given c and the probability of c 

118
23:13:32,630 --> 23:13:36,826
so estimating the probability of c given x boils down to

119
23:13:36,826 --> 23:13:41,583
estimating the probability of x given c and probability of c 

120
23:13:41,583 --> 23:13:45,397
the nice thing is that probability of x given c and

121
23:13:45,397 --> 23:13:49,220
probability of c can be estimated from the data 

122
23:13:50,390 --> 23:13:55,200
so now we have a way to calculate probably of c given x which is what we need for

123
23:13:55,200 --> 23:13:55,940
classification 

124
23:13:57,150 --> 23:13:58,720
to the estimate the probability of c

125
23:13:59,780 --> 23:14:04,450
which is the probability of the class of c before observing any input data 

126
23:14:04,450 --> 23:14:08,690
we simply calculate the fraction of samples with that class label

127
23:14:08,690 --> 23:14:09,910
c in the training data 

128
23:14:11,170 --> 23:14:15,650
for this example there are four samples labeled as green circles out of 10

129
23:14:15,650 --> 23:14:22,740
samples so probability of green circle is 4 out of 10 or 0 4 

130
23:14:22,740 --> 23:14:28,650
similarly the fraction of samples labeled as red triangles is 6 out of 10 or 0 6 

131
23:14:28,650 --> 23:14:33,500
so estimating the prior probabilities is a simple count

132
23:14:33,500 --> 23:14:36,660
of number of samples with each class label

133
23:14:36,660 --> 23:14:39,970
divided by the total number of samples in the training data center 

134
23:14:41,410 --> 23:14:46,000
in estimating probability of x given c which is the probability

135
23:14:46,000 --> 23:14:50,520
of observing feature factor x given that the class is c 

136
23:14:50,520 --> 23:14:53,760
we can use the independent assumption to simplify the problem 

137
23:14:54,790 --> 23:14:58,820
the independence assumption of the naive bayes classifier assumes

138
23:14:58,820 --> 23:15:03,620
that each feature x sub i in the featured vector x

139
23:15:03,620 --> 23:15:08,410
is conditionally independent of every other feature given the class c 

140
23:15:09,520 --> 23:15:14,600
this means that we only need to estimate the probability of x

141
23:15:14,600 --> 23:15:18,360
sub i given c instead of having to

142
23:15:18,360 --> 23:15:23,030
estimate the probability of the entire feature x given c 

143
23:15:23,030 --> 23:15:25,070
for every combination of values for

144
23:15:25,070 --> 23:15:30,900
the features in x then we would simply multiply these individual probabilities

145
23:15:30,900 --> 23:15:35,630
together to get the probability of the entire feature vector x 

146
23:15:35,630 --> 23:15:41,660
given the class c to estimate the probability of x sub i 

147
23:15:41,660 --> 23:15:46,210
given c we count up the number of times a particular input

148
23:15:46,210 --> 23:15:50,820
value is observed for the class c in the training data 

149
23:15:50,820 --> 23:15:55,880
for example the number of times that we see the value of yes for the future home

150
23:15:55,880 --> 23:16:02,380
owner when the class label is no it three as indicated by the green arrows 

151
23:16:02,380 --> 23:16:07,980
this is divided by the number of samples with no as the class label which is seven 

152
23:16:07,980 --> 23:16:10,160
this fraction three out of seven 

153
23:16:10,160 --> 23:16:14,910
is the probability that home owner is yes given that the class is no 

154
23:16:17,090 --> 23:16:20,420
similarly the samples with the value of single for

155
23:16:20,420 --> 23:16:25,710
the feature marital status when it crosses yes are indicated by the red arrows 

156
23:16:25,710 --> 23:16:29,050
and the probability that marital status is single 

157
23:16:29,050 --> 23:16:34,520
given that the class label is yes is 2 3 or 0 67 

158
23:16:36,050 --> 23:16:41,360
some things to know about the naive bayes classification model are it is a fast and

159
23:16:41,360 --> 23:16:42,840
simple algorithm 

160
23:16:42,840 --> 23:16:46,940
the algorithm boils down to calculating counts for probabilities and

161
23:16:46,940 --> 23:16:51,930
performing some multiplication so it is very simple to implement 

162
23:16:51,930 --> 23:16:55,000
and the probabilities that are needed can be calculated with a single

163
23:16:55,000 --> 23:16:57,180
scan of the data set and stored in a table 

164
23:16:58,610 --> 23:17:02,610
either two processing of the data is not necessary as with many other

165
23:17:02,610 --> 23:17:03,500
machine learning algorithms 

166
23:17:04,540 --> 23:17:09,150
so model building and testing of both task it scales well 

167
23:17:09,150 --> 23:17:11,970
due today independent assumption the probability for

168
23:17:11,970 --> 23:17:15,000
each feature can be independently estimated 

169
23:17:15,000 --> 23:17:19,120
these means that featured probability is can be calculated in parallel 

170
23:17:19,120 --> 23:17:23,270
this also means that the data set size does not have to grow exponentially

171
23:17:23,270 --> 23:17:24,630
with a number of features 

172
23:17:25,780 --> 23:17:29,130
this avoids the many problems associated with the curse of dimensionality 

173
23:17:30,480 --> 23:17:33,480
this also means that you do not need a lot of data to build the model 

174
23:17:34,700 --> 23:17:38,410
the number of parameters scales linearly with the number of features 

175
23:17:39,740 --> 23:17:43,700
the independence assumption does not hold true in many cases 

176
23:17:43,700 --> 23:17:47,850
in practice however the naive bayes classifier still tends to perform very

177
23:17:47,850 --> 23:17:51,070
well this is because even though naive bayes may

178
23:17:51,070 --> 23:17:55,400
not provide good estimates of the correct class probabilities 

179
23:17:55,400 --> 23:17:59,870
as long as the correct class is more probable than any other class 

180
23:17:59,870 --> 23:18:02,420
the correct classification results will be reached 

181
23:18:03,920 --> 23:18:07,940
the independence assumption also prevents the naive base classifier to model

182
23:18:07,940 --> 23:18:12,810
interactions between features which limits its classification power 

183
23:18:12,810 --> 23:18:17,040
the increased risk of smoking in a history of cancer would not be captured 

184
23:18:17,040 --> 23:18:17,560
for example 

185
23:18:19,510 --> 23:18:23,230
the naive bays classifier has been applied to many real world problems

186
23:18:23,230 --> 23:18:27,620
including spam filtering document classification and sentiment analysis 

187
23:18:28,650 --> 23:18:33,530
to summarize the naive bayes classifier uses a probabilistic framework for

188
23:18:33,530 --> 23:18:35,120
classification 

189
23:18:35,120 --> 23:18:38,916
it applies bayes theorem and the feature independence assumption 

190
23:18:38,916 --> 23:18:43,570
to simplify the problem of estimating probabilities for the classification task 

1
22:23:11,130 --> 22:23:14,720
in addition to the evaluation matrix covered n the last lecture 

2
22:23:14,720 --> 22:23:18,170
the performance of a classification model can also be evaluated

3
22:23:18,170 --> 22:23:20,430
using a confusion matrix 

4
22:23:20,430 --> 22:23:22,990
we will introduce the confusion matrix in this lecture 

5
22:23:24,180 --> 22:23:26,990
after this video you will be able to 

6
22:23:26,990 --> 22:23:31,410
describe how a confusion matrix can be used to evaluate a classifier 

7
22:23:31,410 --> 22:23:34,240
interpret the confusion matrix of a model 

8
22:23:34,240 --> 22:23:37,160
and relate accuracy to values in a confusion matrix 

9
22:23:38,430 --> 22:23:44,080
let use our example again of predicting whether a given animal is a mammal or not 

10
22:23:44,080 --> 22:23:48,280
recall that this is a binary classification task with the class label

11
22:23:48,280 --> 22:23:52,849
being either yes indicating mammal or no indicating non - mammal 

12
22:23:54,360 --> 22:23:57,190
now let review the different types of errors that you can get with

13
22:23:57,190 --> 22:23:58,810
classification 

14
22:23:58,810 --> 22:24:00,650
if the true label is yes and

15
22:24:00,650 --> 22:24:05,760
the predicted label is yes then this is a true positive abbreviated as tp 

16
22:24:06,850 --> 22:24:10,450
this is a case where the label is correctly predicted as positive 

17
22:24:11,500 --> 22:24:13,430
if the true label is no and

18
22:24:13,430 --> 22:24:18,280
the predicted label is no then this is a true negative abbreviated as tn 

19
22:24:19,350 --> 22:24:22,710
this is the case where the label is correctly predicted as negative 

20
22:24:23,900 --> 22:24:25,970
if the true label is no and

21
22:24:25,970 --> 22:24:30,960
the predicted label is yes then this is a false positive abbreviated as fp 

22
22:24:32,240 --> 22:24:36,910
this is the case where the label is incorrectly predicted as positive

23
22:24:36,910 --> 22:24:37,830
when it should be negative 

24
22:24:39,110 --> 22:24:42,850
if the true label is yes and the predicted label is no 

25
22:24:42,850 --> 22:24:46,180
then this is a false negative abbreviated as fn 

26
22:24:47,300 --> 22:24:51,350
this is the case where the label is incorrectly predicted as negative 

27
22:24:51,350 --> 22:24:52,390
when it should be positive 

28
22:24:54,390 --> 22:24:57,910
a confusion matrix can be used to summarize the different types of

29
22:24:57,910 --> 22:24:59,050
classification errors 

30
22:25:00,100 --> 22:25:04,710
the true positive cell corresponds to the samples that are correctly predicted as

31
22:25:04,710 --> 22:25:06,180
positive by the model 

32
22:25:07,290 --> 22:25:11,460
the true negative cell corresponds to the samples that are correctly predicted

33
22:25:11,460 --> 22:25:12,030
as negative 

34
22:25:13,070 --> 22:25:17,240
the false positive cell corresponds to samples that are incorrectly predicted

35
22:25:17,240 --> 22:25:17,940
as positive 

36
22:25:18,950 --> 22:25:23,310
the false negative cell corresponds to samples that are incorrectly predicted

37
22:25:23,310 --> 22:25:23,810
as negative 

38
22:25:25,110 --> 22:25:29,280
each cell has the count or percentage of samples with each type of errors 

39
22:25:30,940 --> 22:25:35,520
let look at an example to see how a confusion matrix is filled in 

40
22:25:35,520 --> 22:25:39,750
the table on the lists true labels along with the models prediction for

41
22:25:39,750 --> 22:25:41,910
a data set of ten samples 

42
22:25:41,910 --> 22:25:44,610
we will summarize this results in a confusion matrix 

43
22:25:46,100 --> 22:25:49,140
first let figure out the number of true positives 

44
22:25:49,140 --> 22:25:52,860
we call that a true positive occurrence when the output is correctly predicted

45
22:25:52,860 --> 22:25:54,320
as positive 

46
22:25:54,320 --> 22:25:59,140
in other words the true label is yes and the model prediction is also yes 

47
22:25:59,140 --> 22:26:04,680
in this example there are three true positives as indicated by the red arrows 

48
22:26:04,680 --> 22:26:08,390
we enter three and the true positive cell in the confusion matrix 

49
22:26:09,660 --> 22:26:12,310
now let look at the true negatives 

50
22:26:12,310 --> 22:26:16,950
a true negative occurs when the output is correctly predicted as negative 

51
22:26:16,950 --> 22:26:22,150
in other words the true label is no and the models prediction is also no 

52
22:26:22,150 --> 22:26:23,160
in this example 

53
22:26:23,160 --> 22:26:27,360
there are four true negatives as indicated by the green arrows 

54
22:26:27,360 --> 22:26:31,050
we enter four in the true negative cell in the confusion matrix 

55
22:26:32,800 --> 22:26:34,720
what about false negatives 

56
22:26:34,720 --> 22:26:39,490
a false negative occurs when the output is incorrectly predicted as negative 

57
22:26:39,490 --> 22:26:41,390
when it should be positive 

58
22:26:41,390 --> 22:26:44,890
that is the true label is yes and the model prediction is no 

59
22:26:45,890 --> 22:26:46,690
in this example 

60
22:26:46,690 --> 22:26:50,990
there are two false negatives as indicated by the purple arrows 

61
22:26:50,990 --> 22:26:54,530
we enter two in the false negative cell in the confusion matrix 

62
22:26:55,970 --> 22:26:59,030
finally we need to look at false positives 

63
22:26:59,030 --> 22:27:00,730
a false positive occurs 

64
22:27:00,730 --> 22:27:05,760
when the input is incorrectly predicted as positive when it should be negative 

65
22:27:05,760 --> 22:27:10,380
that is the true label is no and models prediction is yes 

66
22:27:10,380 --> 22:27:14,020
in this example there is one false positive as indicated by the yellow arrow 

67
22:27:15,300 --> 22:27:18,870
we enter one in the false positive cell in the confusion matrix 

68
22:27:20,210 --> 22:27:23,460
this is our complete confusion matrix for this example 

69
22:27:23,460 --> 22:27:26,630
we see that the sum of the numbers in the cells add up to ten 

70
22:27:26,630 --> 22:27:28,820
which is the number of samples in our dataset 

71
22:27:30,650 --> 22:27:33,320
note that the diagonal values true positives and

72
22:27:33,320 --> 22:27:37,150
true negatives are samples with correct predictions 

73
22:27:37,150 --> 22:27:40,010
in our example these values sum up to 7 

74
22:27:40,010 --> 22:27:44,340
meaning that 7 out of 10 samples were correctly predicted by the model 

75
22:27:45,370 --> 22:27:49,910
the higher the sum of the diagonal values the better the performance of the model 

76
22:27:51,430 --> 22:27:56,050
the off diagonal values capture the misclassified samples 

77
22:27:56,050 --> 22:27:59,040
where the model predictions do not match the true labels 

78
22:28:00,070 --> 22:28:05,242
in this example these values indicate that there were three misclassifications 

79
22:28:05,242 --> 22:28:06,410
smaller values for

80
22:28:06,410 --> 22:28:11,130
the off diagonal cells in the confusion matrix indicate better model performance 

81
22:28:13,320 --> 22:28:16,230
note that the diagonal values true positives and

82
22:28:16,230 --> 22:28:20,080
true negatives are samples with correct predictions 

83
22:28:20,080 --> 22:28:23,970
in our example these values sum up to 7 meaning

84
22:28:23,970 --> 22:28:28,030
that 7 out of 10 samples were correctly predicted by the model 

85
22:28:28,030 --> 22:28:32,350
the higher the sum of the diagonal values the better the performance of the model 

86
22:28:33,460 --> 22:28:37,910
you may have noticed that the diagonal values are related to accuracy rate 

87
22:28:37,910 --> 22:28:41,390
recall that accuracy is defined as the sum of two positives and

88
22:28:41,390 --> 22:28:44,610
true negatives divided by all samples 

89
22:28:44,610 --> 22:28:46,352
the sum of two positives and

90
22:28:46,352 --> 22:28:50,839
two negatives is the sum of the diagonal values in a confusion matrix 

91
22:28:50,839 --> 22:28:54,516
the sum of the diagonal values in a confusion matrix 

92
22:28:54,516 --> 22:28:59,187
divided by the total number of samples gives you the accuracy rate 

93
22:28:59,187 --> 22:29:04,200
similarly the off diagonal values are related to the error rate 

94
22:29:04,200 --> 22:29:08,320
recall that the error rate is the opposite of the accuracy rate 

95
22:29:08,320 --> 22:29:12,222
the sum of the off diagonal values in a confusion matrix 

96
22:29:12,222 --> 22:29:16,783
divided by the total number of samples gives you the error rate 

97
22:29:16,783 --> 22:29:20,399
looking at the values in the confusion matrix can help you understand

98
22:29:20,399 --> 22:29:23,529
the kind of misclassifications that your model is making 

99
22:29:23,529 --> 22:29:26,937
a high value for this cell indicated by the yellow arrow 

100
22:29:26,937 --> 22:29:32,250
means that classifying the positive class is problematic for the model 

101
22:29:32,250 --> 22:29:36,460
a high value for the cell indicated by the orange arrow on the other hand 

102
22:29:36,460 --> 22:29:40,260
means that classifying the negative class is problematic for the model 

103
22:29:41,340 --> 22:29:44,230
in summary the confusion matrix is a table

104
22:29:44,230 --> 22:29:47,980
used to summarize the different types of errors for a classifier 

105
22:29:47,980 --> 22:29:51,990
the values in a confusion matrix can be used to evaluate the performance of

106
22:29:51,990 --> 22:29:57,470
a classifier and are related to evaluation metrics such as accuracy and error rates 

107
22:29:57,470 --> 22:30:01,020
they also indicate what other types of misclassifications the model is making 

108
22:30:02,040 --> 22:30:06,334
note that in some implementations of a confusion matrix the true and

109
22:30:06,334 --> 22:30:08,460
predictive labels are switched 

110
22:30:08,460 --> 22:30:13,025
be sure to review the documentation for the software you are using to generate

111
22:30:13,025 --> 22:30:16,565
a confusion matrix to understand what each cell specifies 

1
20:53:27,780 --> 20:53:31,020
in this activity we will be evaluating the decision tree model

2
20:53:31,020 --> 20:53:34,788
we created in the knime classification hands - on 

3
20:53:34,788 --> 20:53:38,630
first we will create a confusion matrix to determine the accuracy

4
20:53:38,630 --> 20:53:39,690
of the decision tree model 

5
20:53:41,030 --> 20:53:42,920
next we will use highlighting and

6
20:53:42,920 --> 20:53:45,820
a scatter plot to analyze the classification errors 

7
20:53:48,606 --> 20:53:50,969
let begin 

8
20:53:50,969 --> 20:53:55,281
first let open the classification workflow that we built in the previous

9
20:53:55,281 --> 20:53:56,033
hands - on 

10
20:53:56,033 --> 20:54:00,252
in the top - left of knime is a knime explorer 

11
20:54:00,252 --> 20:54:02,780
double - click on the classification under local 

12
20:54:05,914 --> 20:54:09,802
next we will create a confusion matrix to analyze the accuracy

13
20:54:09,802 --> 20:54:11,530
of our decision tree model 

14
20:54:13,040 --> 20:54:16,380
to do this we will add the scorer node to the canvas 

15
20:54:23,249 --> 20:54:28,340
connect the output of decision tree predictor to the input of scorer 

16
20:54:29,340 --> 20:54:30,480
double - click on scorer 

17
20:54:32,500 --> 20:54:34,635
we will use the default values so click ok 

18
20:54:37,580 --> 20:54:39,280
next run the workflow 

19
20:54:40,420 --> 20:54:41,810
now let view the confusion matrix 

20
20:54:46,998 --> 20:54:49,570
i can see the accuracy as 80 282 

21
20:54:49,570 --> 20:54:56,651
at the top i can see that it accurately predicted 76 values for

22
20:54:56,651 --> 20:55:02,205
humidity low and 95 for humidity not low 

23
20:55:02,205 --> 20:55:08,035
it inaccurately predicted 24 values for humidty low and

24
20:55:08,035 --> 20:55:14,242
18 for humidty not low for an error of 19 718 

25
20:55:14,242 --> 20:55:18,225
close this 

26
20:55:18,225 --> 20:55:21,875
next we use an interactive table to look at the values that were incorrectly

27
20:55:21,875 --> 20:55:22,500
predicted 

28
20:55:23,730 --> 20:55:25,990
we will add the interactive table to the canvas 

29
20:55:34,767 --> 20:55:39,872
we will connect this to the output of decision tree predictor 

30
20:55:39,872 --> 20:55:42,870
run the workflow and view the table 

31
20:55:49,262 --> 20:55:53,810
the right two columns have the real value for low humidity and the prediction 

32
20:55:54,965 --> 20:55:58,150
for some of these rows we can tell that the prediction was not correct 

33
20:55:59,700 --> 20:56:05,119
for example in row ten and in row 17 

34
20:56:09,800 --> 20:56:13,707
let leave the table view open and go back to the workflow 

35
20:56:13,707 --> 20:56:17,415
next we will add the scatter plot nodes to the workflow 

36
20:56:26,185 --> 20:56:28,790
connect this to the output of decision tree predictor 

37
20:56:32,169 --> 20:56:35,900
execute the workflow and view the scatter plot 

38
20:56:39,413 --> 20:56:43,840
we will select row 17 and choose hilite selected 

39
20:56:45,720 --> 20:56:46,780
go back to the scatter plot 

40
20:56:46,780 --> 20:56:51,310
and we see this particular value is highlighted in the plot 

41
20:56:52,930 --> 20:56:54,960
we could choose another row from the table and

42
20:56:54,960 --> 20:56:57,570
highlight it again to see its place in the plot 

43
20:56:58,730 --> 20:56:59,730
let choose row ten 

44
20:57:02,669 --> 20:57:06,083
we can do this for values that were incorrectly predicted 

45
20:57:06,083 --> 20:57:08,620
to find any patterns for further analysis 

1
17:50:34,990 --> 17:50:38,970
in this activity we will use spark to evaluate our decision tree 

2
17:50:38,970 --> 17:50:41,820
first we load the classification predictions 

3
17:50:41,820 --> 17:50:44,170
created during the last spark hands on activity 

4
17:50:45,180 --> 17:50:48,320
we then compute the accuracy of these predictions 

5
17:50:48,320 --> 17:50:50,000
and then generate a confusion matrix 

6
17:50:52,050 --> 17:50:53,480
let begin 

7
17:50:53,480 --> 17:50:56,065
first let open the model evaluation notebook 

8
17:50:58,630 --> 17:51:02,320
next let execute the first cell to load the classes 

9
17:51:03,460 --> 17:51:06,740
then execute the second cell to load the predictions we saved

10
17:51:06,740 --> 17:51:07,950
during the previous hands on 

11
17:51:09,760 --> 17:51:12,580
we could then complete the accuracy of these predictions

12
17:51:12,580 --> 17:51:15,539
by using a multi - class classification evaluator 

13
17:51:16,590 --> 17:51:24,088
let enter evaluator = multiclassclassificationevaluator labelco - 

14
17:51:24,088 --> 17:51:29,124
l = label predictioncol = prediction and

15
17:51:29,124 --> 17:51:34,505
finally metricname = precision and execute this 

16
17:51:38,253 --> 17:51:42,605
we can then compute the accuracy by calling evaluate on the evaluator 

17
17:51:42,605 --> 17:51:47,384
we will enter evaluator evaluate predictions 

18
17:51:49,877 --> 17:51:52,924
this says that the accuracy is about 81 

19
17:51:53,940 --> 17:51:58,095
next let use multi - class metrics to compute a confusion matrix 

20
17:51:58,095 --> 17:52:02,485
multi - class metrics takes an rdd of numbers and

21
17:52:02,485 --> 17:52:05,759
our data is currently in a data frame 

22
17:52:05,759 --> 17:52:10,457
we can access the rdd of the underlying data frame by using the rdd

23
17:52:10,457 --> 17:52:12,549
attribute of predictions 

24
17:52:12,549 --> 17:52:18,184
if we look at predictions rdd take 2 we see the rdd

25
17:52:18,184 --> 17:52:24,520
is the rdd of rows or each row has a prediction and label 

26
17:52:24,520 --> 17:52:28,660
however multi - class metrics wants an rdd of numbers 

27
17:52:28,660 --> 17:52:30,046
we could do this using a map 

28
17:52:30,046 --> 17:52:35,860
we will enter predictions rdd map and we will use a key word 

29
17:52:35,860 --> 17:52:40,010
tuple and we will look at the first two elements in this rdd 

30
17:52:40,010 --> 17:52:40,510
run this 

31
17:52:43,137 --> 17:52:45,740
we can see now this rdd is just numbers 

32
17:52:46,810 --> 17:52:51,058
now we will create a new instance of a multiclass metrics using this rdd 

33
17:52:51,058 --> 17:52:58,275
metrics = multiclassmetrics predrictions rdd map tu - 

34
17:52:58,275 --> 17:52:59,162
ple 

35
17:53:02,149 --> 17:53:06,667
we can then display the confusion matrix by running

36
17:53:06,667 --> 17:53:12,780
metrics confusionmatrix toarray trans - pose 

37
17:53:12,780 --> 17:53:19,674
we can see these results are similar to the confusion matrix in nine 

1
11:43:54,000 --> 11:43:58,150
generalization and overfitting are very important concepts in machine learning 

2
11:43:58,150 --> 11:44:00,390
we will cover them in the next three lectures 

3
11:44:01,690 --> 11:44:06,660
after this video you will be able to define what overfitting is 

4
11:44:06,660 --> 11:44:10,690
describe how overfitting is related to generalization and

5
11:44:10,690 --> 11:44:12,799
explain why overfitting should be avoided 

6
11:44:14,140 --> 11:44:18,860
before we look at generalization and overfitting let first define some terms

7
11:44:18,860 --> 11:44:21,710
that we will need to know to discuss errors in classification 

8
11:44:23,140 --> 11:44:28,400
recall that a machine learning model maps the input it receives to an output 

9
11:44:28,400 --> 11:44:32,760
for a classification model the model output is the predicted class label for

10
11:44:32,760 --> 11:44:36,580
the input variables and the true class label is the target 

11
11:44:38,210 --> 11:44:41,640
then if the classifier predicts the correct classes label for

12
11:44:41,640 --> 11:44:44,610
a sample that is a success 

13
11:44:44,610 --> 11:44:48,370
if the predicted class label is different from the true class label 

14
11:44:48,370 --> 11:44:49,530
then that is an error 

15
11:44:50,880 --> 11:44:55,950
the error rate then is the percentage of errors made over the entire data set 

16
11:44:55,950 --> 11:45:00,330
that is it is the number of errors divided by the total number of samples

17
11:45:00,330 --> 11:45:00,950
in a data set 

18
11:45:01,980 --> 11:45:06,260
error rate is also known as misclassification rate or simply error 

19
11:45:07,690 --> 11:45:12,160
in our lesson on classification we discuss that there is a training phase in which

20
11:45:12,160 --> 11:45:17,910
the model is built and a testing phase in which the model is applied to new data 

21
11:45:17,910 --> 11:45:23,070
the model is built using training data and evaluated on test data 

22
11:45:23,070 --> 11:45:26,760
the training and test data are two different data sets 

23
11:45:26,760 --> 11:45:30,810
the goal in building a machine learning model is to have the model perform well

24
11:45:30,810 --> 11:45:34,450
on training as well as test data 

25
11:45:34,450 --> 11:45:39,290
error rate or simply error on the training data is refered to as training

26
11:45:39,290 --> 11:45:43,960
error and the error on test data is referred to as test error 

27
11:45:45,030 --> 11:45:49,120
the error on the test data is an indication of how well the classifier

28
11:45:49,120 --> 11:45:50,560
will perform on new data 

29
11:45:51,690 --> 11:45:54,210
this is known as generalization 

30
11:45:54,210 --> 11:45:58,180
generalization refers to how well your model performs on new data 

31
11:45:58,180 --> 11:46:00,240
that is data not used to train the model 

32
11:46:01,260 --> 11:46:04,530
you want your model to generalize well to new data 

33
11:46:04,530 --> 11:46:08,200
if your model generalizes well then it will perform well on data sets that

34
11:46:08,200 --> 11:46:11,210
are similar in structure to the training data but

35
11:46:11,210 --> 11:46:14,330
does not contain exactly the same samples as in the training set 

36
11:46:15,700 --> 11:46:20,280
since the test error indicates how well your model generalizes to new data 

37
11:46:20,280 --> 11:46:23,330
note that the test error is also called generalization error 

38
11:46:24,780 --> 11:46:28,280
a related concept to generalization is overfitting 

39
11:46:28,280 --> 11:46:30,810
if your model has very low training error but

40
11:46:30,810 --> 11:46:34,290
high generalization error then it is overfitting 

41
11:46:34,290 --> 11:46:38,680
this means that the model has learned to model the noise in the training data 

42
11:46:38,680 --> 11:46:41,250
instead of learning the underlying structure of the data 

43
11:46:42,530 --> 11:46:45,180
these plots illustrate what happens when a model overfits 

44
11:46:46,190 --> 11:46:48,470
training samples are shown as points and

45
11:46:48,470 --> 11:46:53,340
the input to output mapping that the model has learned is indicated as a curve 

46
11:46:53,340 --> 11:46:57,260
the plot on the left shows that the model has learned the underlying structure of

47
11:46:57,260 --> 11:47:02,530
the data as the curve follows the trend of the sample point as well 

48
11:47:02,530 --> 11:47:03,580
the plot on the right 

49
11:47:03,580 --> 11:47:07,640
however shows that the model has learned to model the noise in a data set 

50
11:47:08,640 --> 11:47:11,640
the model tries to capture every sample point 

51
11:47:11,640 --> 11:47:14,210
instead of the general trend of the samples together 

52
11:47:15,220 --> 11:47:16,220
the training error and

53
11:47:16,220 --> 11:47:19,440
the generalization error are plotted together during model training 

54
11:47:21,400 --> 11:47:25,000
what is the connection between overfitting and generalization 

55
11:47:25,000 --> 11:47:28,820
a model that overfits will not generalize well to new data 

56
11:47:28,820 --> 11:47:32,525
so the model will do well on just the data it was trained on but

57
11:47:32,525 --> 11:47:35,410
given a new data set it will perform poorly 

58
11:47:36,420 --> 11:47:39,650
a classifier that performs well on just the training data set

59
11:47:39,650 --> 11:47:41,270
will not be very useful 

60
11:47:41,270 --> 11:47:45,370
so it is essential that the goal of good generalization performance is kept in mind

61
11:47:45,370 --> 11:47:46,170
when building a model 

62
11:47:47,890 --> 11:47:50,380
a problem related to overfitting is underfitting 

63
11:47:51,890 --> 11:47:56,540
overfitting occurs when the model is fitting to the noise in the training data 

64
11:47:56,540 --> 11:47:59,770
this results in low training error and high test error 

65
11:48:01,330 --> 11:48:02,830
underfitting on the other hand 

66
11:48:02,830 --> 11:48:06,940
occurs when the model has not learned the structure of the data 

67
11:48:06,940 --> 11:48:09,930
this results in high training error and high test error 

68
11:48:11,590 --> 11:48:13,164
both are undesirable 

69
11:48:13,164 --> 11:48:17,658
since both mean that the model will not generalize well to new data 

70
11:48:17,658 --> 11:48:21,903
overfitting generally occurs when a model is too complex that is 

71
11:48:21,903 --> 11:48:26,306
it has too many parameters relative to the number of training samples 

72
11:48:26,306 --> 11:48:30,911
so to avoid overfitting the model needs to be kept as simple as possible and yet

73
11:48:30,911 --> 11:48:35,400
still solve the input output mapping for the given data set 

74
11:48:35,400 --> 11:48:36,880
we will discuss methods for

75
11:48:36,880 --> 11:48:39,470
avoiding overfitting in the next couple of lectures 

76
11:48:40,940 --> 11:48:46,220
in summary overfitting is when your model has learned the noise in the training data

77
11:48:46,220 --> 11:48:48,870
instead of the underlying structure of the data 

78
11:48:48,870 --> 11:48:53,353
you want to avoid overfitting so that your model will generalize well to new data 

1
23:32:46,970 --> 23:32:49,490
how do you evaluate your model performance 

2
23:32:49,490 --> 23:32:52,870
in this lecture we will look at different metrics that can be used to

3
23:32:52,870 --> 23:32:55,360
evaluate the performance of your classification model 

4
23:32:56,470 --> 23:32:59,280
after this video you will be able to

5
23:32:59,280 --> 23:33:03,480
discuss how performance metrics can be used to evaluate models 

6
23:33:03,480 --> 23:33:08,270
name three model evaluation metrics and explain why accuracy may be misleading 

7
23:33:10,110 --> 23:33:14,090
for the classification task an error occurs when the model prediction of

8
23:33:14,090 --> 23:33:18,370
the class label is different from the true class label 

9
23:33:18,370 --> 23:33:22,330
we can also define the different types of errors in classification depending on

10
23:33:22,330 --> 23:33:23,920
the predicted and true labels 

11
23:33:24,930 --> 23:33:29,535
let take the case with the task is to predict whether a given animal is

12
23:33:29,535 --> 23:33:30,627
a mammal or not 

13
23:33:30,627 --> 23:33:36,596
this is a binary classification task with the class label being either yes 

14
23:33:36,596 --> 23:33:40,830
indicating mammal or no indicating non - mammal 

15
23:33:40,830 --> 23:33:43,450
then the different types of errors are as follows 

16
23:33:44,600 --> 23:33:49,693
if the true label is yes and the predicted label is yes 

17
23:33:49,693 --> 23:33:54,367
then this is a true positive abbreviated as tp 

18
23:33:54,367 --> 23:33:58,380
this is the case where the label is correctly predicted as positive 

19
23:34:00,270 --> 23:34:04,500
if the true label is no and the predicted label is no 

20
23:34:04,500 --> 23:34:08,640
then this is a true negative abbreviated as tn 

21
23:34:10,080 --> 23:34:13,760
this is the case where the label is correctly predicted as negative 

22
23:34:15,910 --> 23:34:20,120
if the true label is no and the predicted label is yes 

23
23:34:20,120 --> 23:34:25,380
then this is a false positive abbreviated as fp 

24
23:34:25,380 --> 23:34:29,800
this is the case with the label is incorrectly predicted as positive 

25
23:34:29,800 --> 23:34:30,660
when it should be negative 

26
23:34:32,770 --> 23:34:34,720
if the true label is yes and

27
23:34:34,720 --> 23:34:39,830
the predicted label is no then this is a false negative abbreviated as fn 

28
23:34:40,950 --> 23:34:45,380
this is the case where the label is incorrectly predicted as negative 

29
23:34:45,380 --> 23:34:46,630
when it should be positive 

30
23:34:47,990 --> 23:34:52,400
these definitions can take a while to sink in so feel free to hit the pause button

31
23:34:52,400 --> 23:34:55,080
and replay button several times here to review this part 

32
23:34:57,110 --> 23:35:00,940
these four different types of errors are used in calculating many evaluation

33
23:35:00,940 --> 23:35:03,110
metrics for classifiers 

34
23:35:03,110 --> 23:35:07,270
the most commonly used evaluation metric is the accuracy rate or

35
23:35:07,270 --> 23:35:09,410
accuracy for short 

36
23:35:09,410 --> 23:35:12,880
for classication accuracy is calculated as the number of

37
23:35:12,880 --> 23:35:16,160
correct predictions divided by the total number of predictions 

38
23:35:17,300 --> 23:35:20,770
note that the number of correct predictions is the sum of the true

39
23:35:20,770 --> 23:35:25,660
positives and the true negatives since the true and predicted labels match for

40
23:35:25,660 --> 23:35:27,160
those cases 

41
23:35:27,160 --> 23:35:30,640
the accuracy rate is an intuitive way to measure the performance

42
23:35:30,640 --> 23:35:31,780
of a classification model 

43
23:35:33,410 --> 23:35:37,540
model performance can also be expressed in terms of error rate 

44
23:35:37,540 --> 23:35:40,140
error rate is the opposite of accuracy rate 

45
23:35:41,420 --> 23:35:46,290
let look at an example to see how accuracy and error rates are calculated 

46
23:35:46,290 --> 23:35:50,660
the table on the left lists the true label along with the model prediction

47
23:35:50,660 --> 23:35:52,760
for a data set of ten samples 

48
23:35:54,130 --> 23:35:57,345
first let s figure out the number of true positives 

49
23:35:57,345 --> 23:36:01,930
recall that a true positive occurs when the output is correctly predicted as

50
23:36:01,930 --> 23:36:03,250
positive 

51
23:36:03,250 --> 23:36:07,820
in other words the true label is yes and the model prediction is yes 

52
23:36:07,820 --> 23:36:12,298
in this example there are three true positives as indicated by the red arrows 

53
23:36:12,298 --> 23:36:16,690
so tp = 3 remember that value as we will need it later 

54
23:36:18,870 --> 23:36:22,000
now let figure out the number of true negatives 

55
23:36:22,000 --> 23:36:26,860
a true negative occurs when the output is correctly predicted as negative 

56
23:36:26,860 --> 23:36:30,940
in other words the true label is no and the model prediction is no 

57
23:36:30,940 --> 23:36:34,719
in this example there are four true negatives as indicated by the green

58
23:36:34,719 --> 23:36:35,239
arrows 

59
23:36:35,239 --> 23:36:39,160
so tn = 4 we will need to remember this value as well 

60
23:36:41,330 --> 23:36:46,490
now we use the values for tp and tn to calculate the accuracy rate 

61
23:36:46,490 --> 23:36:52,660
using the equation for accuracy rate we plug in three for tp and four for tn 

62
23:36:52,660 --> 23:36:56,270
we get seven correct predictions for the numerator 

63
23:36:56,270 --> 23:37:01,480
the denominator is simply the total number of samples in our data set which is ten 

64
23:37:01,480 --> 23:37:04,201
so the accuracy rate for

65
23:37:04,201 --> 23:37:10,063
example is 7 out of 10 which is 0 7 or 70 

66
23:37:10,063 --> 23:37:13,890
the error rate is the exact opposite of the accuracy rate 

67
23:37:13,890 --> 23:37:19,120
to calculate the error rate we simply subtract the accuracy rate from 1 

68
23:37:19,120 --> 23:37:24,555
for our example that is 1 - 0 7 which is 0 3 

69
23:37:24,555 --> 23:37:29,771
so the error rate for this example is 0 3 or 30 

70
23:37:29,771 --> 23:37:33,440
there a limitation with accuracy and

71
23:37:33,440 --> 23:37:36,370
error rates when you have a class imbalance problem 

72
23:37:37,740 --> 23:37:41,060
this is when there are very few samples of the class of interest and

73
23:37:41,060 --> 23:37:42,710
the majority are negative examples 

74
23:37:43,870 --> 23:37:48,540
an example of this is identifying if a tumor is cancerous or not 

75
23:37:48,540 --> 23:37:52,820
what is of interest is identifying samples with cancerous tumors but

76
23:37:52,820 --> 23:37:57,200
these positive cases where the tumor is cancerous are very rare 

77
23:37:57,200 --> 23:38:01,110
so you end up with a very small fraction of positive samples and

78
23:38:01,110 --> 23:38:03,150
most of the samples are negative 

79
23:38:03,150 --> 23:38:05,310
thus the name class imbalance problem 

80
23:38:06,880 --> 23:38:10,260
what could be the problem with using accuracy for a class imbalance problem 

81
23:38:11,590 --> 23:38:16,130
consider the situation where only 3 of the cases are cancerous tumors 

82
23:38:17,400 --> 23:38:21,480
if the classification model always predicts non - cancer 

83
23:38:21,480 --> 23:38:25,090
it will have an accuracy rate of 97 

84
23:38:25,090 --> 23:38:29,910
since 97 of the samples will have non - cancerous tumors 

85
23:38:29,910 --> 23:38:35,370
but note that in this case the model fails to detect any cancer cases at all 

86
23:38:35,370 --> 23:38:38,600
so the accuracy rate is very misleading here 

87
23:38:38,600 --> 23:38:41,430
you may think that your model is performing very well with such

88
23:38:41,430 --> 23:38:42,740
a high accuracy rate 

89
23:38:42,740 --> 23:38:48,190
but in fact it cannot identify any of the cases in the class of interest 

90
23:38:48,190 --> 23:38:51,860
in these cases we need evaluation metrics that can capture how

91
23:38:51,860 --> 23:38:55,680
well the model classifies positive versus negative classes 

92
23:38:57,290 --> 23:39:00,942
a pair of evaluations metrics that are commonly used when there is a class

93
23:39:00,942 --> 23:39:04,220
imbalance are precision and recall 

94
23:39:04,220 --> 23:39:08,950
precision is defined as the number of true positives divided by the sum of

95
23:39:08,950 --> 23:39:11,640
true positives and false positives 

96
23:39:11,640 --> 23:39:15,670
in other words it is the number of true positives divided by the total number

97
23:39:15,670 --> 23:39:18,730
of samples predicted as being positive 

98
23:39:20,060 --> 23:39:24,430
recall is defined as the number of true positives divided by the sum of

99
23:39:24,430 --> 23:39:26,860
true positives and false negatives 

100
23:39:26,860 --> 23:39:31,440
it is the number of true positives divided by the total number of samples 

101
23:39:31,440 --> 23:39:33,379
actually belonging to the true class 

102
23:39:34,895 --> 23:39:38,050
here an illustration that shows precision and recall 

103
23:39:38,050 --> 23:39:43,320
the selected elements indicated by the green half circle are the true positives 

104
23:39:43,320 --> 23:39:47,240
that is samples predicted as positive and are actually positive 

105
23:39:48,670 --> 23:39:51,830
the relevant elements indicated by the green half circle and

106
23:39:51,830 --> 23:39:57,140
the green half rectangle are the true positives plus the false negatives 

107
23:39:57,140 --> 23:40:01,520
that is samples that are actually positive but some are correctly predicted

108
23:40:01,520 --> 23:40:04,990
as positive and some are incorrectly predicted as negative 

109
23:40:05,990 --> 23:40:10,140
recall then is the number of samples correctly predicted as positive 

110
23:40:10,140 --> 23:40:13,250
divided by all samples that are actually positive 

111
23:40:15,140 --> 23:40:18,200
the entire circle indicated by the green half circle and

112
23:40:18,200 --> 23:40:23,000
the pink half circle are the true positives plus the false positives 

113
23:40:23,000 --> 23:40:25,710
that is samples that were predicted as positive

114
23:40:25,710 --> 23:40:28,950
although some were actually positive and some were actually negative 

115
23:40:30,175 --> 23:40:34,530
then precision is the number of samples correctly predicted as positive 

116
23:40:34,530 --> 23:40:38,670
divided by the number of all samples predicted as positive 

117
23:40:40,230 --> 23:40:43,790
precision is considered a measure of exactness because it calculates

118
23:40:43,790 --> 23:40:46,710
the percentage of samples predicted as positive 

119
23:40:46,710 --> 23:40:48,420
which are actually in a positive class 

120
23:40:49,460 --> 23:40:53,270
recall is considered a measure of completeness because it calculates

121
23:40:53,270 --> 23:40:56,940
the percentage of positive samples that the model correctly identified 

122
23:40:58,720 --> 23:41:01,950
there is a trade off between precision and recall 

123
23:41:01,950 --> 23:41:06,762
a perfect precision score of one for a class c means that every sample

124
23:41:06,762 --> 23:41:11,506
predicted as belonging to class c does indeed belong to class c 

125
23:41:11,506 --> 23:41:15,674
but this says nothing about the number of samples from class c that were predicted

126
23:41:15,674 --> 23:41:16,480
incorrectly 

127
23:41:17,550 --> 23:41:19,790
a perfect recall score of one for

128
23:41:19,790 --> 23:41:24,710
a class c means that every sample from class c was correctly labeled 

129
23:41:24,710 --> 23:41:27,870
but this does not say anything about how many other samples were

130
23:41:27,870 --> 23:41:30,850
incorrectly labeled as belonging to class c 

131
23:41:30,850 --> 23:41:32,640
so they are used together 

132
23:41:32,640 --> 23:41:37,070
for example precision values can be compared for a fixed value of recall or

133
23:41:37,070 --> 23:41:38,360
vice versa 

134
23:41:38,360 --> 23:41:42,140
the goal for classification is to maximize both precision and recall 

135
23:41:43,870 --> 23:41:49,060
precision and recall can be combined into a single metric called the f - measure 

136
23:41:49,060 --> 23:41:52,830
the equation for that is 2 times the product of precision and

137
23:41:52,830 --> 23:41:55,630
recall divided by their sum 

138
23:41:55,630 --> 23:41:58,350
there are different versions of the f - measure 

139
23:41:58,350 --> 23:41:59,910
the equation on this side is for

140
23:41:59,910 --> 23:42:04,860
the f1 measure which is the most commonly used variant of the f measure 

141
23:42:04,860 --> 23:42:08,990
with the f1 measure precision and recall are equally weighted 

142
23:42:08,990 --> 23:42:12,245
the f2 measure weights recall higher than precision 

143
23:42:12,245 --> 23:42:17,410
and the f0 5 measure weights precision higher than recall 

144
23:42:18,410 --> 23:42:21,560
the value for the f1 measure ranges from zero to one 

145
23:42:21,560 --> 23:42:24,720
with higher values giving better classification performance 

146
23:42:26,160 --> 23:42:28,580
in summary there are several metrics for

147
23:42:28,580 --> 23:42:31,990
evaluating the performance of a classification model 

148
23:42:31,990 --> 23:42:35,170
they are defined in terms of the types of errors you can get in

149
23:42:35,170 --> 23:42:36,390
a classification problem 

150
23:42:37,450 --> 23:42:42,348
we covered some of the most commonly used evaluation metrics in this lecture 

151
23:42:42,348 --> 23:42:46,960
namely accuracy and error rates precision and recall and f1 measure 

1
23:15:32,810 --> 23:15:37,170
in this lecture we will discuss how overfitting occurs with decision trees and

2
23:15:37,170 --> 23:15:38,210
how it can be avoided 

3
23:15:39,300 --> 23:15:40,160
after this video 

4
23:15:40,160 --> 23:15:46,120
you will be able to discuss overfitting in the context of decision tree models 

5
23:15:46,120 --> 23:15:49,496
explain how overfitting is addressed in decision tree induction 

6
23:15:49,496 --> 23:15:53,370
define pre - pruning and post - pruning 

7
23:15:53,370 --> 23:15:57,260
in our lecture on decision trees we discussed that during the construction of

8
23:15:57,260 --> 23:16:02,830
a decision tree also referred to as tree induction the tree repeatedly

9
23:16:02,830 --> 23:16:07,000
splits the data in a node in order to get successively paired subsets of data 

10
23:16:08,320 --> 23:16:12,810
note that a decision tree classifier can potentially expand its nodes until it can

11
23:16:12,810 --> 23:16:16,230
perfectly classify samples in the training data 

12
23:16:16,230 --> 23:16:20,280
but if the tree grows nodes to fit the noise in the training data 

13
23:16:20,280 --> 23:16:23,370
then it will not classify a new sample well 

14
23:16:23,370 --> 23:16:27,450
this is because the tree has partitioned the input space according to the noise in

15
23:16:27,450 --> 23:16:31,410
the data instead of to the true structure of a data 

16
23:16:31,410 --> 23:16:32,825
in other words it has overfit 

17
23:16:34,710 --> 23:16:37,580
how can overfitting be avoided in decision trees 

18
23:16:37,580 --> 23:16:39,220
there are two ways 

19
23:16:39,220 --> 23:16:42,510
one is to stop growing the tree before the tree is fully grown

20
23:16:42,510 --> 23:16:44,590
to perfectly fit the training data 

21
23:16:44,590 --> 23:16:47,890
this is referred to as pre - pruning 

22
23:16:47,890 --> 23:16:51,770
the other way to avoid overfitting in decision trees is to grow the tree to its

23
23:16:51,770 --> 23:16:56,860
maximum size and then prune the tree back by removing parts of the tree 

24
23:16:56,860 --> 23:17:00,020
this is referred to as post - pruning 

25
23:17:00,020 --> 23:17:04,300
in general overfitting occurs because the model is too complex 

26
23:17:04,300 --> 23:17:06,060
for a decision tree model 

27
23:17:06,060 --> 23:17:10,250
model complexity is determined by the number of nodes in the tree 

28
23:17:10,250 --> 23:17:15,300
addressing overfitting in decision trees means controlling the number of nodes 

29
23:17:15,300 --> 23:17:18,920
both methods of pruning control the growth of the tree and consequently 

30
23:17:18,920 --> 23:17:20,880
the complexity of the resulting model 

31
23:17:22,380 --> 23:17:27,270
with pre - pruning the idea is to stop tree induction before a fully grown tree is

32
23:17:27,270 --> 23:17:30,540
built that perfectly fits the training data 

33
23:17:30,540 --> 23:17:35,320
to do this restrictive stopping conditions for growing nodes must be used 

34
23:17:35,320 --> 23:17:40,121
for example a nose stops expanding if the number of samples in the node is less

35
23:17:40,121 --> 23:17:42,940
than some minimum threshold 

36
23:17:42,940 --> 23:17:46,954
another example is to stop expanding a note if the improvement in the impurity

37
23:17:46,954 --> 23:17:49,190
measure falls below a certain threshold 

38
23:17:50,480 --> 23:17:54,440
in post - pruning the tree is grown to its maximum size 

39
23:17:54,440 --> 23:17:59,280
then the tree is pruned by removing nodes using a bottom up approach 

40
23:17:59,280 --> 23:18:03,250
that is the tree is trimmed starting with the leaf nodes 

41
23:18:03,250 --> 23:18:06,900
the pruning is done by replacing a subtree with a leaf node if

42
23:18:06,900 --> 23:18:10,140
this improves the generalization error or if there is no

43
23:18:10,140 --> 23:18:13,580
change to the generalization error with this replacement 

44
23:18:13,580 --> 23:18:17,590
in other words if removing a subtree does not have a negative effect

45
23:18:17,590 --> 23:18:21,240
on the generalization error then the nodes in that subtree only

46
23:18:21,240 --> 23:18:24,730
add to the complexity of the tree and not to its overall performance 

47
23:18:24,730 --> 23:18:26,058
so those nodes should be removed 

48
23:18:26,058 --> 23:18:31,290
in practice post - pruning tends to give better results 

49
23:18:31,290 --> 23:18:35,250
this is because pruning decisions are based on information from the full tree 

50
23:18:35,250 --> 23:18:40,510
pre - pruning on the other hand may stop the tree growing process prematurely 

51
23:18:40,510 --> 23:18:44,951
however post - pruning is more computationally expensive since the tree

52
23:18:44,951 --> 23:18:47,040
has to be expanded to its full size 

53
23:18:48,490 --> 23:18:53,720
in summary to address overfitting in decision trees tree pruning is used 

54
23:18:53,720 --> 23:18:58,030
there are two pruning methods pre - pruning and post - pruning 

55
23:18:58,030 --> 23:19:00,880
both methods control the complexity of the tree model 

1
22:34:32,980 --> 22:34:36,400
in this lecture we will discuss what a validation set is and

2
22:34:36,400 --> 22:34:39,610
how it relates to overfitting and model performance evaluation 

3
22:34:41,150 --> 22:34:45,300
after this video you will be able to describe how validation sets can be

4
22:34:45,300 --> 22:34:47,450
used to avoid overfitting 

5
22:34:47,450 --> 22:34:51,940
articulate how training validation and test sets are used 

6
22:34:51,940 --> 22:34:54,539
and list three ways that validation can be performed 

7
22:34:55,920 --> 22:34:57,600
in our lesson on classification 

8
22:34:57,600 --> 22:35:01,570
we discussed that there is a training phase in which the model is built and

9
22:35:01,570 --> 22:35:05,690
a testing phase in which the model is applied to new data 

10
22:35:05,690 --> 22:35:10,580
the model is built using training data and evaluated on test data 

11
22:35:10,580 --> 22:35:14,290
the training and test data are two different datasets 

12
22:35:14,290 --> 22:35:18,490
the goal in building a machine learning model is to have the model perform well

13
22:35:18,490 --> 22:35:23,110
on the training set as well as generalize well on new data in the test set 

14
22:35:24,750 --> 22:35:29,690
recall that a model that overfits does not generalize well to new data 

15
22:35:29,690 --> 22:35:35,630
recall also that overfitting generally occurs when a model is too complex 

16
22:35:35,630 --> 22:35:39,110
so to have a model with good generalization performance 

17
22:35:39,110 --> 22:35:43,550
model training has to stop before the model gets too complex 

18
22:35:43,550 --> 22:35:45,170
how do you determine when this should occur 

19
22:35:46,730 --> 22:35:51,184
a validation set can be used to guide the training process to avoid overfitting and

20
22:35:51,184 --> 22:35:54,450
deliver good generalization performance 

21
22:35:54,450 --> 22:35:58,550
we have discussed having a training set and a separate test set 

22
22:35:58,550 --> 22:36:01,000
the training set is used to build a model and

23
22:36:01,000 --> 22:36:03,900
the test set is used to see how the model performs a new data 

24
22:36:04,960 --> 22:36:09,450
now we want to further divide up the training data into a training set and

25
22:36:09,450 --> 22:36:11,200
a validation set 

26
22:36:11,200 --> 22:36:14,680
the training set is used to train the model as before and

27
22:36:14,680 --> 22:36:18,780
the validation set is used to determine when to stop training the model

28
22:36:18,780 --> 22:36:22,800
to avoid overfitting in order to get the best generalization performance 

29
22:36:24,060 --> 22:36:27,010
the idea is to look at the errors on both training set and

30
22:36:27,010 --> 22:36:30,500
validation set during model training as shown here 

31
22:36:30,500 --> 22:36:34,670
the orange solid line on the plot is the training error and

32
22:36:34,670 --> 22:36:37,960
the green line is the validation error 

33
22:36:37,960 --> 22:36:41,760
we see that as model building progresses along the x - axis 

34
22:36:41,760 --> 22:36:43,890
the number of nodes increases 

35
22:36:43,890 --> 22:36:47,280
that is the complexity of the model increases 

36
22:36:47,280 --> 22:36:53,200
we can see that as the model complexity increases the training error decreases 

37
22:36:53,200 --> 22:36:57,140
on the other hand the validation error initially decreases but

38
22:36:57,140 --> 22:36:58,370
then starts to increase 

39
22:36:59,900 --> 22:37:04,960
when the validation error increases this indicates that the model is overfitting 

40
22:37:04,960 --> 22:37:07,800
resulting in decreased generalization performance 

41
22:37:09,640 --> 22:37:12,950
this can be used to determine when to stop training 

42
22:37:12,950 --> 22:37:17,390
where validation error starts to increase is when you get the best generalization

43
22:37:17,390 --> 22:37:20,340
performance so training should stop there 

44
22:37:20,340 --> 22:37:24,050
this method of using a validation set to determine when to stop training

45
22:37:24,050 --> 22:37:26,610
is referred to as model selection

46
22:37:26,610 --> 22:37:30,610
since you are selecting one from many of varying complexities 

47
22:37:30,610 --> 22:37:34,831
note that this was illustrated for a decision tree classifier but

48
22:37:34,831 --> 22:37:39,135
the same method can be applied to any type of machine learning model 

49
22:37:39,135 --> 22:37:44,510
there are several ways to create and use the validation set to avoid overfitting 

50
22:37:44,510 --> 22:37:48,482
the different methods are holdout method 

51
22:37:48,482 --> 22:37:53,313
random subsampling k - fold cross - validation 

52
22:37:53,313 --> 22:37:57,300
and leave - one - out cross - validation 

53
22:37:57,300 --> 22:38:01,050
the first way to use a validation set is the holdout method 

54
22:38:01,050 --> 22:38:03,900
this describes the scenario that we have been discussing 

55
22:38:03,900 --> 22:38:08,270
where part of the training data is reserved as a validation set 

56
22:38:08,270 --> 22:38:10,300
the validation set is then the holdout set 

57
22:38:11,300 --> 22:38:14,950
errors on the training set and the holdout set are calculated at each step

58
22:38:14,950 --> 22:38:19,080
during model training and plotted together as we have seen before 

59
22:38:19,080 --> 22:38:22,950
and the lowest error on the holdout set is when training should stop 

60
22:38:22,950 --> 22:38:26,240
this is the just the process that we have described here before 

61
22:38:26,240 --> 22:38:28,670
there some limitations to the holdout method however 

62
22:38:29,710 --> 22:38:33,970
first since some samples are reserved for the holdout validation set 

63
22:38:33,970 --> 22:38:37,580
the training set now has less data than it originally started out with 

64
22:38:38,770 --> 22:38:43,810
secondly if the training and holdout sets do not have the same data distributions 

65
22:38:43,810 --> 22:38:46,140
then the results will be misleading 

66
22:38:46,140 --> 22:38:50,750
for example if the training data has many more samples of one class and

67
22:38:50,750 --> 22:38:54,540
the holdout dataset has many more samples of another class 

68
22:38:54,540 --> 22:38:59,100
the next method for using a validation set is repeated holdout 

69
22:38:59,100 --> 22:39:00,260
as the name implies 

70
22:39:00,260 --> 22:39:04,220
this is essentially repeating the holdout method several times 

71
22:39:04,220 --> 22:39:09,170
with each iteration samples are randomly selected from the original training data

72
22:39:09,170 --> 22:39:12,040
to create the holdout validation set 

73
22:39:12,040 --> 22:39:15,740
this is repeated several times with different training and validation sets 

74
22:39:16,810 --> 22:39:20,010
then the iterates on the holdout set for the different iterations

75
22:39:20,010 --> 22:39:25,150
are averaged together to get the overall iterate for model selection 

76
22:39:25,150 --> 22:39:29,420
a potential problem with repeated holdout is that you could end up with some samples

77
22:39:29,420 --> 22:39:31,570
being used more than others for training 

78
22:39:32,640 --> 22:39:37,470
since a sample can be used for either testing or training any number of times 

79
22:39:37,470 --> 22:39:42,050
some samples may be put in the training set more times than other samples 

80
22:39:42,050 --> 22:39:46,250
so you might end up with some samples being overrepresented while

81
22:39:46,250 --> 22:39:49,430
other samples are underrepresented in training or testing 

82
22:39:51,520 --> 22:39:56,154
a way to improve on the repeated holdout method is use cross - validation 

83
22:39:56,154 --> 22:39:58,628
cross - validation works as follows 

84
22:39:58,628 --> 22:40:03,440
segment the data into k number of disjoint partitions 

85
22:40:03,440 --> 22:40:08,420
during each iteration one partition is used as the validation set 

86
22:40:08,420 --> 22:40:10,690
repeat the process k times 

87
22:40:10,690 --> 22:40:14,230
each time using a different partition for validation 

88
22:40:14,230 --> 22:40:17,880
so each partition is used for validation exactly once 

89
22:40:17,880 --> 22:40:20,230
this is illustrated in this figure 

90
22:40:20,230 --> 22:40:24,520
in the fist iteration the first partition specified in green is used for

91
22:40:24,520 --> 22:40:25,960
validation 

92
22:40:25,960 --> 22:40:28,950
in the second iteration the second partition is used for

93
22:40:28,950 --> 22:40:30,160
validation and so on 

94
22:40:31,300 --> 22:40:35,920
the overall validation error is calculated by averaging the validation errors for

95
22:40:35,920 --> 22:40:36,980
all k iterations 

96
22:40:38,040 --> 22:40:42,490
the model with the smallest average validation error then is selected 

97
22:40:42,490 --> 22:40:47,530
the process we just described is referred to as k - fold cross - validation 

98
22:40:47,530 --> 22:40:51,390
this is a very commonly used approach to model selection in practice 

99
22:40:52,440 --> 22:40:57,405
this approach gives you a more structured way to divide available data up between

100
22:40:57,405 --> 22:41:02,371
training and validation datasets and provides a way to overcome the variability

101
22:41:02,371 --> 22:41:07,134
in performance that you can get when using a single partitioning of the data 

102
22:41:07,134 --> 22:41:11,401
leave - one - out cross - validation is a special case of k - fold

103
22:41:11,401 --> 22:41:16,220
cross - validation where k equals n where n is the size of your dataset 

104
22:41:17,260 --> 22:41:21,860
here for each iteration the validation set has exactly one sample 

105
22:41:21,860 --> 22:41:25,240
so the model is trained to using n minus one samples and

106
22:41:25,240 --> 22:41:27,920
is validated on the remaining sample 

107
22:41:27,920 --> 22:41:33,600
the rest of the process works the same way as regular k - fold cross - validation 

108
22:41:33,600 --> 22:41:38,391
note that cross - validation is often abbreviated cv and leave - one - out

109
22:41:38,391 --> 22:41:43,340
cross - validation is in abbreviated l - o - o - c - v and pronounced loocv 

110
22:41:45,550 --> 22:41:49,990
we have described several ways to use a validation set to address overfitting 

111
22:41:49,990 --> 22:41:53,750
error on the validation set is used to determine when to stop training so

112
22:41:53,750 --> 22:41:55,350
that the model does not overfit 

113
22:41:56,400 --> 22:42:00,440
note that the validation error that comes out of this process can also be used

114
22:42:00,440 --> 22:42:03,720
to estimate generalization performance of the model 

115
22:42:03,720 --> 22:42:04,780
in other words 

116
22:42:04,780 --> 22:42:09,420
the error on the validation set provides an estimate of the error on the test set 

117
22:42:10,890 --> 22:42:12,570
with the addition of the validation set 

118
22:42:12,570 --> 22:42:16,460
you really need three distinct datasets when you build a model 

119
22:42:16,460 --> 22:42:17,660
let review these datasets 

120
22:42:19,080 --> 22:42:23,560
the training dataset is used to train the model that is to adjust the parameters of

121
22:42:23,560 --> 22:42:25,990
the model to learn the input to output mapping 

122
22:42:27,640 --> 22:42:31,710
the validation dataset is used to determine when training should stop

123
22:42:31,710 --> 22:42:33,370
in order to avoid overfitting 

124
22:42:35,110 --> 22:42:39,490
the test data set is used to evaluate the performance of the model on new data 

125
22:42:41,640 --> 22:42:46,320
note that the test data set should never ever be used in any way to create or

126
22:42:46,320 --> 22:42:47,940
tune the model 

127
22:42:47,940 --> 22:42:49,070
it should not be used for

128
22:42:49,070 --> 22:42:53,010
example in a cross - validation process to determine when to stop training 

129
22:42:54,020 --> 22:42:58,680
the test dataset must always remain independent from model training and

130
22:42:58,680 --> 22:43:03,980
remain untouched until the very end when all training has been completed 

131
22:43:03,980 --> 22:43:08,467
note that in sampling the original dataset to create the training validation and

132
22:43:08,467 --> 22:43:12,909
test sets all datasets must contain the same distribution of the target classes 

133
22:43:12,909 --> 22:43:18,362
for example if in the original dataset 70 of the samples belong to one class and

134
22:43:18,362 --> 22:43:23,297
30 to the other class then this same distribution should approximately

135
22:43:23,297 --> 22:43:27,427
be present in each of the training validation and test sets 

136
22:43:27,427 --> 22:43:30,713
otherwise analysis results will be misleading 

137
22:43:30,713 --> 22:43:33,667
to summarize we have discuss the need for

138
22:43:33,667 --> 22:43:36,900
three different datasets in building model 

139
22:43:36,900 --> 22:43:41,588
a training set to train the model a validation set to determine when to stop

140
22:43:41,588 --> 22:43:44,920
training and a test to evaluate performance on new data 

141
22:43:46,210 --> 22:43:49,800
we learned how a validation set can be used to avoid overfitting and

142
22:43:49,800 --> 22:43:54,600
in the process provide an estimate of generalization performance 

143
22:43:54,600 --> 22:43:56,982
and we covered different ways to create and

144
22:43:56,982 --> 22:44:00,236
use a validation set such as k - fold cross - validation 

1
21:18:32,930 --> 21:18:36,150
now that we have seen what association analysis is 

2
21:18:36,150 --> 21:18:39,460
let go over the association analysis process in more detail 

3
21:18:40,930 --> 21:18:45,970
after this video you will be able to define the terms support and

4
21:18:45,970 --> 21:18:50,790
confidence describe the steps in association analysis and

5
21:18:50,790 --> 21:18:54,330
explain how association rules are formed from item sets 

6
21:18:55,540 --> 21:18:58,240
let review the set in association analysis 

7
21:18:58,240 --> 21:19:03,920
they are create item the sets then identify the frequent item sets 

8
21:19:03,920 --> 21:19:05,230
finally generate the rules 

9
21:19:06,240 --> 21:19:09,010
we will continue with this example dataset 

10
21:19:09,010 --> 21:19:11,530
there are five transactions in the dataset 

11
21:19:11,530 --> 21:19:14,650
each with a set of items purchased together 

12
21:19:14,650 --> 21:19:18,770
the goal is to come up with rules describing associations between items 

13
21:19:20,280 --> 21:19:22,390
the first step is to create item sets 

14
21:19:23,450 --> 21:19:26,780
item sets have different sizes which need to be created 

15
21:19:26,780 --> 21:19:28,220
we will color code the items so

16
21:19:28,220 --> 21:19:31,350
that each one is easier to pick out from the transactions table 

17
21:19:32,880 --> 21:19:37,250
we start out with just 1 - item sets that is sets with just one item 

18
21:19:38,260 --> 21:19:41,530
the left table is the dataset of transactions 

19
21:19:41,530 --> 21:19:45,920
the right table contains the 1 - item sets that can be created from this dataset 

20
21:19:47,170 --> 21:19:51,940
as each item set is created we also need to keep track of the frequency at which

21
21:19:51,940 --> 21:19:53,910
these item set occurs in the dataset 

22
21:19:55,275 --> 21:19:59,130
this is referred to support for the item set and

23
21:19:59,130 --> 21:20:04,580
is calculated by dividing the number of times the item set occurs in the dataset

24
21:20:04,580 --> 21:20:07,250
by the total number of transactions 

25
21:20:07,250 --> 21:20:11,050
this is what is in the support column in the right table 

26
21:20:11,050 --> 21:20:14,586
for example eggs the last item in the right table occurs

27
21:20:14,586 --> 21:20:18,502
just occurs just once in the dataset in the transaction two 

28
21:20:18,502 --> 21:20:23,838
so if support is 1 5 or one fifth the item set with

29
21:20:23,838 --> 21:20:31,330
diaper occurs in all transactions so if support is 5 5 or 1 

30
21:20:31,330 --> 21:20:35,410
the support for each item set will be used to identify frequent item sets

31
21:20:35,410 --> 21:20:39,930
in the next step specifically the support issues to prune or

32
21:20:39,930 --> 21:20:42,990
remove item sets that do not occur frequently 

33
21:20:44,605 --> 21:20:47,825
the support of each item set will be used to identify

34
21:20:47,825 --> 21:20:50,155
frequent item sets in the next step 

35
21:20:50,155 --> 21:20:52,905
specifically the support is used to prune or

36
21:20:52,905 --> 21:20:56,705
remove item sets that do not occur frequently 

37
21:20:56,705 --> 21:21:00,525
for example the minimum support threshold is set to 3 5 

38
21:21:00,525 --> 21:21:05,271
so looking at the 1 - item sets table we can remove any item set with

39
21:21:05,271 --> 21:21:07,656
the support of less than 3 5 

40
21:21:07,656 --> 21:21:12,348
these item sets are highlighted in pink they will be removed before the sets for

41
21:21:12,348 --> 21:21:14,360
two items are created 

42
21:21:14,360 --> 21:21:20,300
the final one item sets are then the item sets with bread milk beer and diaper 

43
21:21:21,770 --> 21:21:25,640
we only consider items that were in the one item sets that were not pruned 

44
21:21:26,740 --> 21:21:28,990
the two item sets are shown in the right table 

45
21:21:30,000 --> 21:21:32,370
we again need to keep track of the support for

46
21:21:32,370 --> 21:21:35,100
these item sets just as we did with the one item sets 

47
21:21:36,180 --> 21:21:38,730
for example for the last item set with beer and

48
21:21:38,730 --> 21:21:42,280
diaper we see by looking at the left table that beer and

49
21:21:42,280 --> 21:21:48,000
diaper occur together three times in transactions two three and four 

50
21:21:48,000 --> 21:21:50,700
so with support is 3 5 

51
21:21:50,700 --> 21:21:53,870
again we need to prune item sets with low support 

52
21:21:53,870 --> 21:21:57,450
the ones highlighted in pink in the two item sets table 

53
21:21:57,450 --> 21:22:01,800
those would be the item set with bread and beer and the item set with milk and beer 

54
21:22:02,830 --> 21:22:05,120
the remaining two items that end 

55
21:22:05,120 --> 21:22:08,150
one item such or then use to create the three item sets 

56
21:22:09,560 --> 21:22:11,460
let look now at creating three item sets 

57
21:22:12,750 --> 21:22:17,190
the only three item sets that has a support value greater than minimum support

58
21:22:17,190 --> 21:22:19,320
is the one shown in the right table 

59
21:22:19,320 --> 21:22:22,240
namely the items start with bread milk and diaper 

60
21:22:23,940 --> 21:22:29,130
the second step in association analysis is to identify the frequent item sets 

61
21:22:29,130 --> 21:22:31,860
but note that the process that we just described for

62
21:22:31,860 --> 21:22:35,530
creating item sets already identifies frequent item sets 

63
21:22:36,530 --> 21:22:39,880
a frequent item set is one whose support is greater than or

64
21:22:39,880 --> 21:22:42,590
equal to the minimum support 

65
21:22:42,590 --> 21:22:47,580
so by keeping track of the support of each item set as it is being created and

66
21:22:47,580 --> 21:22:50,020
removing item sets with low support 

67
21:22:50,020 --> 21:22:52,570
we are already identifying frequent item sets 

68
21:22:53,650 --> 21:22:58,740
for our example the frequent one two and three item sets are shown here 

69
21:23:00,230 --> 21:23:03,940
now that we identified the frequent item sets the last step is to

70
21:23:03,940 --> 21:23:07,650
generate the rules to capture associations that we see in the data 

71
21:23:09,050 --> 21:23:13,627
let first define some terms we will need to discuss association rules 

72
21:23:13,627 --> 21:23:17,690
the format of an association rule is shown at the top 

73
21:23:17,690 --> 21:23:23,910
it written as x arrow y and is read as if x then y 

74
21:23:23,910 --> 21:23:27,190
the x part is called the antecedent and

75
21:23:27,190 --> 21:23:29,770
the y part is called the consequent of the rule 

76
21:23:30,870 --> 21:23:32,520
x and y are item sets 

77
21:23:33,560 --> 21:23:36,920
an important term in rule generation is the rule confidence 

78
21:23:38,120 --> 21:23:40,680
this is to find as a support for x and

79
21:23:40,680 --> 21:23:46,080
y together divided by the support for x only 

80
21:23:46,080 --> 21:23:48,680
so rule confidence calculates the frequency of

81
21:23:48,680 --> 21:23:50,710
instances to which the rule applies 

82
21:23:52,210 --> 21:23:56,830
recall that the support for x is the frequency of item set x and

83
21:23:56,830 --> 21:24:00,470
is defined as the number of transactions containing items in x

84
21:24:00,470 --> 21:24:03,595
divided by the total number of transactions 

85
21:24:03,595 --> 21:24:06,875
the rule confidence measures how frequently items in

86
21:24:06,875 --> 21:24:10,585
y appear in the transaction that contain x 

87
21:24:10,585 --> 21:24:15,365
in other words the confidence measures the reliability of the rule by determining

88
21:24:15,365 --> 21:24:20,635
how often if x and y is found to be true in the data 

89
21:24:20,635 --> 21:24:22,835
how is rule confidence used in rule generation 

90
21:24:24,230 --> 21:24:29,160
association rules are generated from the frequent item sets created from the data 

91
21:24:29,160 --> 21:24:33,070
each item in an item set can be used as a part of the antecedent or

92
21:24:33,070 --> 21:24:35,200
consequent of the rule 

93
21:24:35,200 --> 21:24:38,970
and you can have many ways to combine items to form the antecedent and

94
21:24:38,970 --> 21:24:39,490
consequent 

95
21:24:40,810 --> 21:24:44,210
so if we just simply generate rules from each frequent item set 

96
21:24:44,210 --> 21:24:46,390
we would end up with lots and lots of rules 

97
21:24:47,390 --> 21:24:52,680
each item set with k items can generate 2 to the k - 2 rules 

98
21:24:52,680 --> 21:24:54,330
that a lot of rules 

99
21:24:54,330 --> 21:24:57,240
and the majority of those rules would not be found in the data 

100
21:24:58,330 --> 21:25:00,070
this is where rule confidence comes in 

101
21:25:01,380 --> 21:25:05,140
we can use rule confidence to constrain the number of rules to keep 

102
21:25:06,190 --> 21:25:10,140
specifically a minimum confidence threshold is set and

103
21:25:10,140 --> 21:25:12,810
only rules with confidence greater than or

104
21:25:12,810 --> 21:25:17,410
equal to the minimum confidence are significant and only those will be kept 

105
21:25:18,850 --> 21:25:21,320
let look at how this works with our example dataset 

106
21:25:22,520 --> 21:25:27,150
we call that only one three item set was created from the transactions 

107
21:25:27,150 --> 21:25:30,380
that three items that contains items bread milk and

108
21:25:30,380 --> 21:25:31,630
diaper as shown at the top 

109
21:25:32,730 --> 21:25:37,000
with these three item set let see how we can generate rules from it and

110
21:25:37,000 --> 21:25:39,670
determine which rules to keep and which one to prune 

111
21:25:41,150 --> 21:25:44,507
let set the minimum confidence to 0 95 

112
21:25:44,507 --> 21:25:47,350
and here again is the definition for confidence 

113
21:25:48,890 --> 21:25:53,250
for candidate rule if bread and milk then diaper 

114
21:25:53,250 --> 21:25:56,940
we can calculate it confidence as follows the support for

115
21:25:56,940 --> 21:26:01,790
both antecedent and consequent is the number of times we see bread milk and

116
21:26:01,790 --> 21:26:07,570
diaper together in the data divided by the total number of transactions 

117
21:26:07,570 --> 21:26:12,240
items bread milk and diaper appear together in transaction 1 4 and

118
21:26:12,240 --> 21:26:15,720
5 so the support is 3 5 

119
21:26:15,720 --> 21:26:19,570
the support for just the antecedent is the number of times we see bread and

120
21:26:19,570 --> 21:26:22,690
milk together divided by the total number of transactions 

121
21:26:23,770 --> 21:26:28,778
items bread and milk appear together also in transactions 1 4 and 5 

122
21:26:28,778 --> 21:26:31,880
so the support is 3 5 

123
21:26:31,880 --> 21:26:36,119
the confidence of this rule is then 1 or 100 

124
21:26:36,119 --> 21:26:39,743
this means that the rule is correct 100 

125
21:26:39,743 --> 21:26:43,540
every time bread and milk are bought together diaper is bought as well 

126
21:26:44,780 --> 21:26:46,780
for candidate rule if bread and

127
21:26:46,780 --> 21:26:51,170
diaper than milk we calculate its confidence the same way 

128
21:26:51,170 --> 21:26:55,780
the support for bread diaper and milk is 3 5 as before 

129
21:26:55,780 --> 21:27:01,920
items bread and diaper are paired together in transactions 1 2 4 and 5 

130
21:27:01,920 --> 21:27:06,230
so the support for the items set with bread and milk is 4 5 

131
21:27:06,230 --> 21:27:11,324
then the confidence with this rule is 0 75 or 75 

132
21:27:11,324 --> 21:27:14,845
since the minimum confidence is 0 95 or 95 

133
21:27:14,845 --> 21:27:19,710
the first rule is kept and the second rule is removed from consideration 

134
21:27:21,430 --> 21:27:24,670
there are several algorithms for association analysis 

135
21:27:24,670 --> 21:27:28,820
each uses a different set of methods to make frequent items set creation and

136
21:27:28,820 --> 21:27:30,970
rule generation efficient 

137
21:27:30,970 --> 21:27:35,080
the more popular algorithms are apriori fp growth and eclat 

138
21:27:36,560 --> 21:27:41,410
as a summary we just looked at the steps in association analysis in more detail 

139
21:27:41,410 --> 21:27:46,055
we saw how items sets can be created from a dataset how frequent items sets can be

140
21:27:46,055 --> 21:27:50,770
identified and how association rules can be created from frequent item sets and

141
21:27:50,770 --> 21:27:52,633
pruned using rule confidence 

1
18:46:24,950 --> 18:46:28,990
we have looked at classification regression and cluster analysis 

2
18:46:28,990 --> 18:46:33,280
let now discuss association analysis as a machine learning task 

3
18:46:33,280 --> 18:46:39,100
after this video you will be able to explain what association analysis entails 

4
18:46:39,100 --> 18:46:44,259
list some applications of association analysis define what an item set is 

5
18:46:45,490 --> 18:46:49,600
in association analysis the goal is to come up with a set of rules to capture

6
18:46:49,600 --> 18:46:53,070
associations between items or events 

7
18:46:53,070 --> 18:46:57,870
the rules are used to determine when items or events occur together 

8
18:46:57,870 --> 18:47:02,110
you may remember seeing these images earlier in the course where we introduced

9
18:47:02,110 --> 18:47:05,730
the different categories of machine learning tasks and techniques 

10
18:47:05,730 --> 18:47:09,580
but do you remember what the association is between these items 

11
18:47:09,580 --> 18:47:12,810
well let recap that story in case you do not remember 

12
18:47:12,810 --> 18:47:14,380
the story goes like this 

13
18:47:14,380 --> 18:47:18,100
a supermarket chain used association analysis to discover

14
18:47:18,100 --> 18:47:22,050
a connection between two seemingly unrelated products 

15
18:47:22,050 --> 18:47:25,200
they discovered that many customers who go to the store

16
18:47:25,200 --> 18:47:29,860
late on sunday night to buy diapers also tend to buy beer 

17
18:47:29,860 --> 18:47:33,634
this information was then used to place beer and diapers close together 

18
18:47:33,634 --> 18:47:36,780
and they saw a jump in sales of both items 

19
18:47:36,780 --> 18:47:39,620
this illustrates that you can uncover unexpected and

20
18:47:39,620 --> 18:47:43,040
useful relationships with association analysis 

21
18:47:43,040 --> 18:47:47,530
this diaper and beer story has become part of the data mining folklore 

22
18:47:47,530 --> 18:47:51,650
it unclear how much of it true but it has become the prime example of what you

23
18:47:51,650 --> 18:47:55,600
can discover with association analysis and machine learning in general 

24
18:47:56,990 --> 18:48:01,511
a common application of association analysis is referred to as

25
18:48:01,511 --> 18:48:03,398
market basket analysis 

26
18:48:03,398 --> 18:48:07,550
this is used to understand the purchasing behavior of customers 

27
18:48:07,550 --> 18:48:11,190
the idea is that you are looking into the shopping basket of customers

28
18:48:11,190 --> 18:48:12,650
when they are at the market and

29
18:48:12,650 --> 18:48:17,150
analyzing that data to understand what items are purchased together 

30
18:48:17,150 --> 18:48:21,090
this information can be used to place related items together or

31
18:48:21,090 --> 18:48:24,290
to have sales on items that are often purchased together 

32
18:48:25,310 --> 18:48:29,210
another application of association analysis is to recommend items that

33
18:48:29,210 --> 18:48:34,620
a customer may be interested in based on their purchasing or browsing history 

34
18:48:34,620 --> 18:48:37,720
this is very commonly used on companies websites

35
18:48:37,720 --> 18:48:39,379
to get customers to buy more items 

36
18:48:40,470 --> 18:48:42,610
there are medical applications as well 

37
18:48:42,610 --> 18:48:47,430
analysis of patients and treatments may reveal associations to identify effective

38
18:48:47,430 --> 18:48:51,080
treatments for patients with certain medical histories 

39
18:48:51,080 --> 18:48:54,710
this diagram illustrates how association analysis works 

40
18:48:54,710 --> 18:48:58,090
the data set is a collection of transactions 

41
18:48:58,090 --> 18:49:00,519
each transaction contains one or more items 

42
18:49:01,530 --> 18:49:04,490
this is referred to as the item set 

43
18:49:04,490 --> 18:49:06,070
from the given items sets 

44
18:49:06,070 --> 18:49:10,250
generate association rules that capture which item tend occur together 

45
18:49:11,390 --> 18:49:15,520
in our example the data set consists of five transactions 

46
18:49:15,520 --> 18:49:20,518
in the first transaction the items are diaper bread and milk 

47
18:49:20,518 --> 18:49:25,900
the second transaction has items bread diaper beer and eggs and so on 

48
18:49:27,258 --> 18:49:31,290
rules that could be generated from this data set are shown at the bottom 

49
18:49:31,290 --> 18:49:34,090
for example the first rule states that if bread and

50
18:49:34,090 --> 18:49:37,910
milk are bought together then diaper is also bought 

51
18:49:37,910 --> 18:49:41,870
the second rule state that if milk is bought then bread is also bought 

52
18:49:43,420 --> 18:49:46,690
the association analysis process consist of the following steps 

53
18:49:48,060 --> 18:49:51,140
the first step is to create item sets 

54
18:49:51,140 --> 18:49:55,690
item sets are generated for sets with one item two items three items and so on 

55
18:49:57,200 --> 18:50:00,300
then frequent item sets are identified 

56
18:50:00,300 --> 18:50:04,380
frequent item sets are those that occur at least a minimum number of times 

57
18:50:05,590 --> 18:50:09,580
from the frequent item sets association rules are generated 

58
18:50:09,580 --> 18:50:12,310
we will take a more detailed look at these steps in the next lecture 

59
18:50:13,660 --> 18:50:16,800
some things to note about association analysis 

60
18:50:16,800 --> 18:50:21,480
like cluster analysis each transaction does not have a label to specify which

61
18:50:21,480 --> 18:50:23,760
item set or rule it belongs to 

62
18:50:23,760 --> 18:50:27,340
so association analysis is an unsupervised task 

63
18:50:28,540 --> 18:50:31,510
you may end up with many rules at the end of the analysis 

64
18:50:31,510 --> 18:50:34,540
but whether those rules are interesting useful 

65
18:50:34,540 --> 18:50:39,370
or applicable requires interpretation using domain knowledge of the application 

66
18:50:40,540 --> 18:50:41,720
in addition 

67
18:50:41,720 --> 18:50:46,200
the association analysis process will not tell you how to apply the rules 

68
18:50:46,200 --> 18:50:49,999
this also requires knowledge of the application 

69
18:50:49,999 --> 18:50:54,245
so as with cluster analysis interpretation and analysis are required

70
18:50:54,245 --> 18:50:58,780
to make sense of the resulting rules that you get from association analysis 

71
18:50:59,970 --> 18:51:01,020
in summary 

72
18:51:01,020 --> 18:51:06,240
association analysis finds rules to capture associations between items 

73
18:51:06,240 --> 18:51:10,670
the association rules have intuitive appeal because they are in the form of

74
18:51:10,670 --> 18:51:14,420
if this then that which is easy to understand 

75
18:51:14,420 --> 18:51:18,150
the results of association analysis require analysis and

76
18:51:18,150 --> 18:51:22,430
interpretation using domain knowledge to determine the usefulness and

77
18:51:22,430 --> 18:51:24,020
applicability of the resulting rules 

78
18:51:25,310 --> 18:51:28,756
next we will cover the steps in the association analysis process in

79
18:51:28,756 --> 18:51:29,501
more detail 

1
13:37:53,990 --> 13:37:58,240
in this activity we will use spark to perform cluster analysis 

2
13:37:58,240 --> 13:38:00,320
first we will load the minute weather data 

3
13:38:01,600 --> 13:38:05,479
next we will remove the unused and missing data and

4
13:38:05,479 --> 13:38:08,752
then scale the data so that the mean is zero 

5
13:38:08,752 --> 13:38:11,931
we will then create an elbow plot a subset of the data 

6
13:38:11,931 --> 13:38:15,430
to determine the optimal number of clusters 

7
13:38:15,430 --> 13:38:19,280
and then cluster the full data set using k - means 

8
13:38:19,280 --> 13:38:23,390
finally we will generate parallel plots to analyze the individual clusters 

9
13:38:25,000 --> 13:38:25,530
let begin 

10
13:38:26,600 --> 13:38:28,560
first let open the clustering notebook 

11
13:38:31,650 --> 13:38:34,259
execute the first cell to load the libraries 

12
13:38:36,090 --> 13:38:38,220
execute the second cell to load the data set 

13
13:38:40,630 --> 13:38:44,870
this data set contains weather station measurements that were taken every minute 

14
13:38:44,870 --> 13:38:46,980
so there a lot of measurements 

15
13:38:46,980 --> 13:38:53,705
we can count how many rows there are in the data frame by running df count 

16
13:38:54,757 --> 13:38:58,860
this says that there are over 1 5 million rows in the data frame 

17
13:38:58,860 --> 13:39:03,090
clustering this much data on a single cloudera vm can take a lot of time 

18
13:39:03,090 --> 13:39:05,680
so let only work with one - tenth of the data 

19
13:39:05,680 --> 13:39:08,340
let subset the data into the new data frame 

20
13:39:08,340 --> 13:39:13,124
we will enter filtereddf

21
13:39:13,124 --> 13:39:18,820
 = df filter df rowid

22
13:39:18,820 --> 13:39:22,924
 10 = = 0 

23
13:39:22,924 --> 13:39:31,182
we then count the rows of the new data frame by calling filtereddf count 

24
13:39:33,997 --> 13:39:38,130
the new data frame has one - tenth as many rows as the original data set 

25
13:39:39,520 --> 13:39:41,888
let compute the summary statistics using describe 

26
13:39:41,888 --> 13:39:47,613
filtereddf describe and to display it nicely 

27
13:39:47,613 --> 13:39:52,303
we will enter topandas transpose 

28
13:39:52,303 --> 13:39:52,803
run this 

29
13:40:00,604 --> 13:40:03,895
these weather measurements were taken during the period of a long drought 

30
13:40:05,090 --> 13:40:08,680
as we can see from the mean values of the two rain measurements 

31
13:40:08,680 --> 13:40:12,820
rain accumulation the rain duration the measurements are close to zero 

32
13:40:14,600 --> 13:40:17,504
let count how many rain values are equal to 0 

33
13:40:17,504 --> 13:40:25,198
filterdf filter filterdf rain accumulation = = 

34
13:40:25,198 --> 13:40:28,431
0 0 count 

35
13:40:31,436 --> 13:40:33,890
now let do rain duration 

36
13:40:33,890 --> 13:40:40,994
filtereddf filter filtereddf rain duration = = 

37
13:40:40,994 --> 13:40:44,563
0 0 count 

38
13:40:44,563 --> 13:40:50,053
we can see from these counts that most in these two measurements are zero 

39
13:40:50,053 --> 13:40:53,527
so let remove them from our data frame 

40
13:40:53,527 --> 13:40:59,719
workingdf = filterdf drop rain accumulation drop - 

41
13:40:59,719 --> 13:41:01,710
rain duration 

42
13:41:01,710 --> 13:41:04,710
and we will also drop the column called hpwren timestamp 

43
13:41:04,710 --> 13:41:06,502
since we will not use it 

44
13:41:06,502 --> 13:41:09,099
so drop hpwren timestamp 

45
13:41:13,422 --> 13:41:18,608
next let drop rows with missing values and count how many rows were dropped 

46
13:41:18,608 --> 13:41:23,085
before = workingdf count workingdf = 

47
13:41:23,085 --> 13:41:29,055
workingdf na drop after = workingdf count 

48
13:41:29,055 --> 13:41:35,147
and finally we will print the difference before - after 

49
13:41:39,410 --> 13:41:42,000
so only 46 rows had missing values and were dropped 

50
13:41:43,320 --> 13:41:47,190
next let scale the data so that each feature will have a value of 0 for

51
13:41:47,190 --> 13:41:51,010
the mean and a value of 1 for the standard deviation 

52
13:41:51,010 --> 13:41:54,500
first we need to combine the columns into a single vector column 

53
13:41:54,500 --> 13:41:59,366
we can look at the existing columns by entering workingdf columns 

54
13:41:59,366 --> 13:42:03,510
we do not want to include rowid since it is a row number 

55
13:42:03,510 --> 13:42:06,430
additionally the minimum wind measurements have a high correlation

56
13:42:06,430 --> 13:42:09,510
to the average wind measurements so we will not include this either 

57
13:42:10,560 --> 13:42:13,230
let create an array of the columns we want to combine and

58
13:42:13,230 --> 13:42:16,798
then use vector assembler to create the vector column 

59
13:42:16,798 --> 13:42:21,760
featuresused = 

60
13:42:21,760 --> 13:42:24,439
now let copy and paste the columns we want to use 

61
13:42:43,974 --> 13:42:50,085
assembler = vectorassembler inputcols = featuresused 

62
13:42:50,085 --> 13:42:54,358
outputcol = features unscaled 

63
13:42:54,358 --> 13:43:00,534
and finally assembled = assembler transform workingdf 

64
13:43:00,534 --> 13:43:02,111
run this 

65
13:43:02,111 --> 13:43:08,021
now we will use standard scaler to scale the data 

66
13:43:08,021 --> 13:43:14,255
scaler = standard scaler inputcol = features unscaler 

67
13:43:14,255 --> 13:43:18,706
outputcol = features withstd = true 

68
13:43:18,706 --> 13:43:24,828
withmean = true scalermodel = scaler fit assembled 

69
13:43:24,828 --> 13:43:30,297
scaleddata = scalermodel transform assembled 

70
13:43:36,150 --> 13:43:39,642
next we will create an elbow plot to determine the value k for

71
13:43:39,642 --> 13:43:41,400
the number of clusters 

72
13:43:41,400 --> 13:43:45,440
to create the elbow plot we will calculate the within cluster 

73
13:43:45,440 --> 13:43:49,120
sum of squared error or wsse for different values of k 

74
13:43:49,120 --> 13:43:53,800
so since some valves running k - means many times let do it for

75
13:43:53,800 --> 13:43:56,330
a smaller subset of the data since it will be faster 

76
13:43:58,350 --> 13:44:01,280
first let choose the data to work with 

77
13:44:01,280 --> 13:44:03,286
let subset the data 

78
13:44:03,286 --> 13:44:08,347
so scaleddata = scaleddata select features rowid 

79
13:44:08,347 --> 13:44:15,407
elbowset = scaleddata filter scaleddata rowid

80
13:44:15,407 --> 13:44:21,173
 3 = = 0 select features 

81
13:44:21,173 --> 13:44:25,907
and finally we will call the persist method on elbowset 

82
13:44:25,907 --> 13:44:29,736
persist will keep it in memory and make the calculations faster 

83
13:44:29,736 --> 13:44:33,925
run this 

84
13:44:33,925 --> 13:44:39,072
now let compute the wsse values for different values of k 

85
13:44:39,072 --> 13:44:42,080
we will do for k 2 to 30 

86
13:44:42,080 --> 13:44:46,693
clusters equals range 2 31 

87
13:44:46,693 --> 13:44:55,271
wsselist = utils elbow elbowset clusters 

88
13:44:55,271 --> 13:45:02,800
run this this will print out the value of wsse for each value of k 

89
13:45:02,800 --> 13:45:06,410
as you can tell this will take some time to run let skip to the end 

90
13:45:09,570 --> 13:45:12,158
now let display the plot of our data 

91
13:45:12,158 --> 13:45:22,159
utils elbow plot wsselist clusters 

92
13:45:22,159 --> 13:45:29,614
the x axis is k the number of clusters and the y axis is the wsse value 

93
13:45:29,614 --> 13:45:33,190
you can see that the graph flattens out between 10 and 15 for k 

94
13:45:34,320 --> 13:45:38,620
so let choose k equals 12 as the midpoint for our number of clusters 

95
13:45:40,788 --> 13:45:45,040
we will now cluster the data into 12 clusters using k - means 

96
13:45:45,040 --> 13:45:47,285
first let select the data we want to cluster 

97
13:45:47,285 --> 13:45:54,726
scaleddatafeat = scaleddata select features 

98
13:45:54,726 --> 13:45:58,142
scaleddatafeat persist 

99
13:46:00,496 --> 13:46:03,394
now i will perform the clustering using kmeans 

100
13:46:03,394 --> 13:46:08,688
kmeans = kmeans k = 12 seed = 1 

101
13:46:08,688 --> 13:46:14,752
model = kmeans fit scaleddatafeat and finally 

102
13:46:14,752 --> 13:46:20,582
transformed = model transform scaledatafeat 

103
13:46:20,582 --> 13:46:22,198
run this 

104
13:46:22,198 --> 13:46:26,702
we can now see the centre measurement of each

105
13:46:26,702 --> 13:46:31,459
cluster by calling model clustercenters 

106
13:46:35,924 --> 13:46:40,080
it is difficult to compare the cluster centers just by looking at these numbers 

107
13:46:40,080 --> 13:46:45,319
so let use parallel plots to visualize them 

108
13:46:45,319 --> 13:46:51,095
p = utils pd centers featuresused 

109
13:46:51,095 --> 13:46:55,000
model clustercenters 

110
13:46:56,810 --> 13:46:57,850
let show the clusters for

111
13:46:57,850 --> 13:47:01,390
dry days where the weather samples have low relative humidity 

112
13:47:02,670 --> 13:47:12,670
utils parallel plot p p are lative humidi - ty 

113
13:47:13,714 --> 13:47:17,674
< - 0 5 p 

114
13:47:21,719 --> 13:47:24,920
the x axis of this chart shows the different measurement types 

115
13:47:25,990 --> 13:47:29,730
the y values show the standard deviations with zero being the mean 

116
13:47:31,330 --> 13:47:35,330
each line is a different cluster and there are five clusters in this graph 

117
13:47:37,010 --> 13:47:40,130
we can see that they all have a low relative humidity 

118
13:47:40,130 --> 13:47:45,060
notice that cluster four the red one has a high average wind speed and

119
13:47:45,060 --> 13:47:46,430
a high maximum wind speed 

120
13:47:48,400 --> 13:47:51,850
additionally it has a low average wind direction 

121
13:47:51,850 --> 13:47:55,380
which means it was coming from the north and northeast directions 

122
13:47:55,380 --> 13:47:58,480
so this cluster probably represents santa ana conditions 

123
13:48:00,770 --> 13:48:03,615
next let show the plot for warm days 

124
13:48:03,615 --> 13:48:06,867
the weather samples with high air temperature 

125
13:48:06,867 --> 13:48:14,877
utils parallel plot p p air temp 

126
13:48:14,877 --> 13:48:17,490
0 5 p 

127
13:48:23,292 --> 13:48:27,283
other clusters in this plot have air temperature greater than 0 5 standard

128
13:48:27,283 --> 13:48:28,920
deviations away from the mean 

129
13:48:29,930 --> 13:48:33,000
however they have different values for the other features 

130
13:48:33,000 --> 13:48:35,530
now let show the clusters for cool days 

131
13:48:35,530 --> 13:48:39,430
weather samples with high relative humidity and low air temperatures 

132
13:48:40,650 --> 13:48:50,650
utils parallel plot p p are lative humidi - ty 

133
13:48:52,966 --> 13:49:00,129
 0 5 p air temp 

134
13:49:00,129 --> 13:49:04,387
< 0 5 p 

135
13:49:04,387 --> 13:49:09,222
all the clusters in this plot have relative humidity greater than 0 5

136
13:49:09,222 --> 13:49:14,608
standard deviations and air temp less than 0 5 standard deviations 

137
13:49:14,608 --> 13:49:17,680
these clusters represent cool temperature with high humidity and

138
13:49:17,680 --> 13:49:20,220
possibly rainy weather patterns 

139
13:49:20,220 --> 13:49:22,980
so far we have seen all the clusters except two 

140
13:49:22,980 --> 13:49:27,012
since it is not falling to any other categories let plot this cluster 

141
13:49:27,012 --> 13:49:36,795
utils parallel plot p iloc 2 

142
13:49:36,795 --> 13:49:37,527
p 

143
13:49:40,556 --> 13:49:42,892
cluster two captures days with mild weather 

1
03:27:36,000 --> 03:27:39,770
in addition to classification and regression machine learning tasks and

2
03:27:39,770 --> 03:27:44,720
techniques can also fall into another category known as cluster analysis 

3
03:27:44,720 --> 03:27:50,460
after this video you will be able to articulate the goal of cluster analysis 

4
03:27:50,460 --> 03:27:55,310
discuss whether cluster analysis is supervised or unsupervised and

5
03:27:55,310 --> 03:27:57,530
list some ways that cluster results can be applied 

6
03:27:59,110 --> 03:28:00,150
in cluster analysis 

7
03:28:00,150 --> 03:28:05,870
the goal is to organize similar items in your data set into groups or clusters 

8
03:28:05,870 --> 03:28:11,550
by segmenting your data into clusters you can analyze each cluster more carefully 

9
03:28:11,550 --> 03:28:14,790
note that cluster analysis is also referred to as clustering 

10
03:28:16,480 --> 03:28:21,220
a very common application of cluster analysis that we have discussed before is

11
03:28:21,220 --> 03:28:25,720
to divide your customer base into segments based on their purchasing histories 

12
03:28:25,720 --> 03:28:29,180
for example you can segment customers into those who have purchased science

13
03:28:29,180 --> 03:28:33,960
fiction books and videos versus those who tend to buy nonfiction books 

14
03:28:33,960 --> 03:28:37,240
versus those who have bought many children books 

15
03:28:37,240 --> 03:28:41,860
this way you can provide more targeted suggestions to each different group 

16
03:28:41,860 --> 03:28:46,620
some other examples of cluster analysis are characterizing different weather

17
03:28:46,620 --> 03:28:50,620
patterns for a region grouping the latest news articles into

18
03:28:50,620 --> 03:28:54,450
topics to identify the trending topics of the day and

19
03:28:54,450 --> 03:28:58,650
discovering hot spots for different types of crime from police reports

20
03:28:58,650 --> 03:29:01,690
in order to provide sufficient police presence for problem areas 

21
03:29:03,550 --> 03:29:07,100
cluster analysis divides all the samples in a data set into groups 

22
03:29:08,340 --> 03:29:11,620
in this diagram we see that the red green and

23
03:29:11,620 --> 03:29:13,550
purple data points are clustered together 

24
03:29:14,570 --> 03:29:18,630
which group a sample is placed in is based on some measure of similarity 

25
03:29:20,060 --> 03:29:25,070
the goal of cluster analysis is to segment data so that differences between samples

26
03:29:25,070 --> 03:29:30,240
in the same cluster are minimized as shown by the yellow arrow and differences

27
03:29:30,240 --> 03:29:35,280
between samples of different clusters are maximized as shown by the orange arrow 

28
03:29:35,280 --> 03:29:38,190
visually you can think of this as getting samples in

29
03:29:38,190 --> 03:29:41,750
each cluster to be as close together as possible and

30
03:29:41,750 --> 03:29:45,540
the samples from different clusters to be as far apart as possible 

31
03:29:46,810 --> 03:29:49,380
cluster analysis requires some sort of metric to

32
03:29:49,380 --> 03:29:52,425
measure similarity between two samples 

33
03:29:52,425 --> 03:29:57,490
some common similarity measures are euclidean distance which is the distance

34
03:29:57,490 --> 03:30:01,325
along a straight line between two points a and b as shown in this plot 

35
03:30:02,390 --> 03:30:06,920
manhattan distance which is calculated on a strictly horizontal and

36
03:30:06,920 --> 03:30:10,100
vertical path as shown in the right plot 

37
03:30:10,100 --> 03:30:14,932
to go from point a to point b you can only step along either the x - axis or

38
03:30:14,932 --> 03:30:18,490
the y - axis in a two - dimensional case 

39
03:30:18,490 --> 03:30:22,390
so the path to calculate the manhattan distance consists of segments

40
03:30:22,390 --> 03:30:27,240
along the axes instead of along a diagonal path as with euclidean distance 

41
03:30:28,580 --> 03:30:33,010
cosine similarity measures the cosine of the angle between points a and

42
03:30:33,010 --> 03:30:35,460
b as shown in the bottom plot 

43
03:30:35,460 --> 03:30:40,400
since distance measures such as euclidean distance are often used to measure

44
03:30:40,400 --> 03:30:44,048
similarity between samples in clustering algorithms 

45
03:30:44,048 --> 03:30:48,152
note that it may be necessary to normalize the input variables so

46
03:30:48,152 --> 03:30:51,889
that no one value dominates the similarity calculation 

47
03:30:51,889 --> 03:30:53,071
we discussed scaling and

48
03:30:53,071 --> 03:30:57,250
normalizing variables in the lecture on feature transformation 

49
03:30:57,250 --> 03:31:00,500
normalizing is one method to scale variables 

50
03:31:00,500 --> 03:31:05,560
essentially scaling the input variables puts the variables on the same scale so

51
03:31:05,560 --> 03:31:08,840
that all variables have equal weighting in the calculation

52
03:31:08,840 --> 03:31:10,889
to determine similarity between samples 

53
03:31:11,930 --> 03:31:16,110
scaling is necessary when you have variables that have very different scales 

54
03:31:16,110 --> 03:31:18,040
such as weight and height 

55
03:31:18,040 --> 03:31:22,250
the magnitude of the height values which are in feet and inches 

56
03:31:22,250 --> 03:31:26,910
will be much smaller than the magnitude of the weight values which are in pounds 

57
03:31:26,910 --> 03:31:30,500
so scaling both variables to a common value range will

58
03:31:30,500 --> 03:31:33,130
make the contributions from both weight and height equal 

59
03:31:34,410 --> 03:31:36,710
here are some things to note about cluster analysis 

60
03:31:37,720 --> 03:31:41,220
first unlike classification or regression in general 

61
03:31:41,220 --> 03:31:44,500
cluster analysis is an unsupervised task 

62
03:31:44,500 --> 03:31:48,590
this means that there is no target label for any sample in the data set 

63
03:31:50,020 --> 03:31:53,660
in general there is no correct clustering results 

64
03:31:53,660 --> 03:31:56,260
the best set of clusters is highly dependent

65
03:31:56,260 --> 03:31:59,230
on how the resulting clusters will be used 

66
03:31:59,230 --> 03:32:03,070
there are numerical measures to compare two different clusters but

67
03:32:03,070 --> 03:32:06,000
since there are no labels to determine whether a sample has

68
03:32:06,000 --> 03:32:09,190
been correctly clustered there is no ground truth

69
03:32:09,190 --> 03:32:13,520
to determine if a set of clustering results are truly correct or incorrect 

70
03:32:15,120 --> 03:32:17,190
clusters do not come with labels 

71
03:32:17,190 --> 03:32:20,410
you may end up with five different clusters at the end of a cluster

72
03:32:20,410 --> 03:32:24,630
analysis process but you do not know what each cluster represents 

73
03:32:24,630 --> 03:32:27,410
only by analyzing the samples in each cluster

74
03:32:27,410 --> 03:32:31,450
can you come out with reasonable labels for your clusters 

75
03:32:31,450 --> 03:32:35,150
given all this it is important to keep in mind that interpretation and

76
03:32:35,150 --> 03:32:40,400
analysis of the clusters are required to make sense of and

77
03:32:40,400 --> 03:32:42,869
make use of the results of cluster analysis 

78
03:32:44,330 --> 03:32:48,040
there are several ways that the results of cluster analysis can be used 

79
03:32:48,040 --> 03:32:51,570
the most obvious is data segmentation and the benefits that come from that 

80
03:32:52,570 --> 03:32:56,230
if you segment your customer base into different types of readers 

81
03:32:56,230 --> 03:33:00,280
the resulting insights can be used to provide more effective marketing

82
03:33:00,280 --> 03:33:03,540
to the different customer groups based on their preferences 

83
03:33:03,540 --> 03:33:07,840
for example analyzing each segment separately can provide valuable insights

84
03:33:07,840 --> 03:33:12,570
into each group likes dislikes and purchasing behavior just like we see

85
03:33:12,570 --> 03:33:15,730
science fiction non - fiction and children books preferences here 

86
03:33:17,450 --> 03:33:20,910
clusters can also be used to classify new data samples 

87
03:33:20,910 --> 03:33:23,030
when a new sample is received 

88
03:33:23,030 --> 03:33:27,520
like the orange sample here compute the similarity measure between it and

89
03:33:27,520 --> 03:33:32,160
the centers of all clusters and assign a new sample to the closest cluster 

90
03:33:32,160 --> 03:33:33,850
the label of that cluster 

91
03:33:33,850 --> 03:33:39,020
manually determined through analysis is then used to classify the new sample 

92
03:33:39,020 --> 03:33:43,770
in our book buyers preferences example a new customer can be classified as being

93
03:33:43,770 --> 03:33:47,950
either a science fiction non - fiction or children books customer

94
03:33:47,950 --> 03:33:51,000
depending on which cluster the new customer is most similar to 

95
03:33:52,530 --> 03:33:54,880
once cluster labels have been determined 

96
03:33:54,880 --> 03:33:59,660
samples in each cluster can be used as labeled data for a classification task 

97
03:33:59,660 --> 03:34:03,090
the samples would be the input to the classification model 

98
03:34:03,090 --> 03:34:06,750
and the cluster label would be the target class for each sample 

99
03:34:06,750 --> 03:34:10,840
this process can be used to provide much needed labeled data for classification 

100
03:34:12,240 --> 03:34:17,260
yet another use of cluster results is as a basis for anomaly detection 

101
03:34:17,260 --> 03:34:19,470
if a sample is very far away or

102
03:34:19,470 --> 03:34:24,490
very different from any of the cluster centers like the yellow sample here 

103
03:34:24,490 --> 03:34:28,720
then that sample is a cluster outlier and can be flagged as an anomaly 

104
03:34:29,740 --> 03:34:33,060
however these anomalies require further analysis 

105
03:34:33,060 --> 03:34:37,130
depending on the application these anomalies can be considered noise and

106
03:34:37,130 --> 03:34:39,470
should be removed from the data set 

107
03:34:39,470 --> 03:34:43,580
an example of this would be a sample with a value of 150 for age 

108
03:34:44,840 --> 03:34:49,610
for other cases these anomalous cases should be studied more carefully 

109
03:34:49,610 --> 03:34:53,290
examples of this are in a credit card fraud detection or 

110
03:34:53,290 --> 03:34:56,280
a network intrusion detection application 

111
03:34:56,280 --> 03:35:00,810
in these applications examples outside of the norm are the interesting cases

112
03:35:00,810 --> 03:35:04,840
that should be looked at to determine if they represent potential problems 

113
03:35:05,880 --> 03:35:11,440
to summarize cluster analysis is used to organize similar data items into groups or

114
03:35:11,440 --> 03:35:12,990
clusters 

115
03:35:12,990 --> 03:35:16,830
analyzing the resulting clusters often leads to useful insights

116
03:35:16,830 --> 03:35:19,400
about the characteristics of each group 

117
03:35:19,400 --> 03:35:22,240
as well as the underlying structure of the entire data set 

118
03:35:23,530 --> 03:35:25,400
clusters require analysis and

119
03:35:25,400 --> 03:35:28,140
interpretation to make sense of the results 

120
03:35:28,140 --> 03:35:33,580
since there are no labels associated with samples or clusters in a clustering task 

121
03:35:33,580 --> 03:35:37,109
in the next lecture we will discuss a specific algorithm for cluster analysis 

1
07:03:12,050 --> 07:03:16,100
k - means clustering is a simple yet effective algorithm for

2
07:03:16,100 --> 07:03:18,820
cluster analysis that is commonly used in practice 

3
07:03:19,920 --> 07:03:20,720
after this video 

4
07:03:20,720 --> 07:03:25,410
you will be able to describe the steps in the k - means algorithm 

5
07:03:25,410 --> 07:03:30,229
explain what the k stands for in k - means and define what a cluster centroid is 

6
07:03:31,300 --> 07:03:36,640
recall a cluster analysis divides samples in a data set into groups or clusters 

7
07:03:36,640 --> 07:03:41,590
the idea is to group similar items in the same cluster where similar is defined

8
07:03:41,590 --> 07:03:45,340
by some metric that measures similarity between data samples 

9
07:03:45,340 --> 07:03:48,760
so the goal of cluster analysis is to divide data sample

10
07:03:48,760 --> 07:03:53,260
such that sample within a cluster are as close together as possible 

11
07:03:53,260 --> 07:03:58,387
and samples from different clusters are as far apart as possible 

12
07:03:58,387 --> 07:04:02,440
k - means is a classic algorithm used for cluster analysis 

13
07:04:02,440 --> 07:04:03,810
the algorithm is very simple 

14
07:04:05,040 --> 07:04:08,015
the first step is to select k initial centroids 

15
07:04:09,200 --> 07:04:13,540
a centroid is simply the center of a cluster as you see in the diagram here 

16
07:04:15,180 --> 07:04:19,950
next assign each sample in a dataset to the closest centroid 

17
07:04:19,950 --> 07:04:22,920
this means you calculate the distance between the sample and

18
07:04:22,920 --> 07:04:28,790
each cluster center and assign a sample to the cluster with the closest centroid 

19
07:04:28,790 --> 07:04:30,140
then you calculate the mean 

20
07:04:30,140 --> 07:04:33,630
or average of each cluster to determine a new centroid 

21
07:04:35,165 --> 07:04:39,330
these two steps are then repeated until some stopping criterion are reached 

22
07:04:40,450 --> 07:04:44,346
here an illustration of how k - means works 

23
07:04:44,346 --> 07:04:46,460
 a shows the original data set with some samples 

24
07:04:48,070 --> 07:04:50,990
and b illustrates centroids initially selected 

25
07:04:52,385 --> 07:04:54,820
 c shows the first iteration 

26
07:04:54,820 --> 07:04:58,352
here samples are assigned to the closest centroid 

27
07:04:58,352 --> 07:05:01,050
in d the centroids are recalculated 

28
07:05:02,625 --> 07:05:04,560
 e shows the second iteration 

29
07:05:04,560 --> 07:05:07,540
samples are assigned to the closer centroid 

30
07:05:07,540 --> 07:05:11,040
note that some samples changed their cluster assignments 

31
07:05:11,040 --> 07:05:14,480
and in f the centroids are recalculated again 

32
07:05:14,480 --> 07:05:15,630
cluster assignments and

33
07:05:15,630 --> 07:05:20,730
centroid calculations are repeated until some stopping criteria is reached 

34
07:05:20,730 --> 07:05:22,940
and you get your final clusters as shown in f 

35
07:05:24,700 --> 07:05:26,770
how are the initial centroids selected 

36
07:05:26,770 --> 07:05:30,050
the issue is that the final cluster results are sensitive

37
07:05:30,050 --> 07:05:31,500
to initial centroids 

38
07:05:31,500 --> 07:05:34,990
this means that cluster results with one set of initial centroids can be

39
07:05:34,990 --> 07:05:39,270
very different from results with another set of initial centroids 

40
07:05:39,270 --> 07:05:42,145
there are many approaches to selecting the initial centroids for

41
07:05:42,145 --> 07:05:45,660
k - means varying in levels of sophistication 

42
07:05:45,660 --> 07:05:50,620
the easiest and most widely used approach is to apply k - means several times

43
07:05:50,620 --> 07:05:55,510
with different initial centroids randomly chosen to cluster you dataset 

44
07:05:55,510 --> 07:05:59,020
and then select the centroids that give the best clustering results 

45
07:06:00,130 --> 07:06:02,970
to evaluate the cluster results an error measure 

46
07:06:02,970 --> 07:06:07,870
known as the within cluster sum of squared error can be used 

47
07:06:07,870 --> 07:06:10,450
the error associated with a sample

48
07:06:10,450 --> 07:06:15,720
within a cluster is the distance between the sample and the cluster centroid 

49
07:06:15,720 --> 07:06:19,840
the squared error of the sample then is the squared of that distance 

50
07:06:21,020 --> 07:06:24,160
we sum up all the squared errors for all samples for

51
07:06:24,160 --> 07:06:27,430
a cluster to the get the squared error for that cluster 

52
07:06:28,590 --> 07:06:33,130
we then do the same thing for all clusters to get the final calculation for

53
07:06:33,130 --> 07:06:36,590
the within - cluster sum of squared error for

54
07:06:36,590 --> 07:06:39,490
all clusters in the results of a cluster analysis run 

55
07:06:40,810 --> 07:06:45,920
given two clustering results the one with the smaller within - cluster sum of squared

56
07:06:45,920 --> 07:06:51,400
error or wsse for short provides the better solution numerically 

57
07:06:51,400 --> 07:06:54,770
however as we have discussed before there is no ground truth

58
07:06:54,770 --> 07:06:59,120
to mathematically determine which set of clusters is more correct than the other 

59
07:07:00,160 --> 07:07:04,561
in addition note that increasing the number of clusters 

60
07:07:04,561 --> 07:07:08,977
that is increasing the value for k always reduces wsse 

61
07:07:08,977 --> 07:07:12,510
so wsse should be used with caution 

62
07:07:12,510 --> 07:07:16,770
it only makes sense to use wsse to compare two sets of clusters

63
07:07:16,770 --> 07:07:20,880
with the same value for k and generate it from the same dataset 

64
07:07:22,060 --> 07:07:25,410
also the set of clusters with the smallest wsse may not

65
07:07:25,410 --> 07:07:28,730
always be the best solution for the application at hand 

66
07:07:28,730 --> 07:07:30,150
again interpretation and

67
07:07:30,150 --> 07:07:34,430
domain knowledge about what the cluster should represent and how they will be used

68
07:07:34,430 --> 07:07:38,710
are crucial in determining which cluster results are best 

69
07:07:38,710 --> 07:07:42,710
now that there are several metrics that are used to evaluate cluster results as well 

70
07:07:43,950 --> 07:07:48,223
choosing the optimal value for k is always a big question in using k - means 

71
07:07:48,223 --> 07:07:51,185
there are several methods to determine the value for k 

72
07:07:51,185 --> 07:07:53,140
we will discuss a few here 

73
07:07:54,310 --> 07:07:57,880
visualization techniques can be used to determine the dataset to see if

74
07:07:57,880 --> 07:08:00,720
there are natural groupings of the samples 

75
07:08:00,720 --> 07:08:01,700
scatter plots and

76
07:08:01,700 --> 07:08:05,450
the use of dimensionality reduction are useful here to visualize the data 

77
07:08:06,850 --> 07:08:09,540
a good value for k is application - dependent 

78
07:08:09,540 --> 07:08:14,260
so domain knowledge of the application can drive the selection for the value of k 

79
07:08:14,260 --> 07:08:15,030
for example 

80
07:08:15,030 --> 07:08:18,940
if you want to cluster the types of products customers are purchasing 

81
07:08:18,940 --> 07:08:23,850
a natural choice for k might be the number of product categories that you offer 

82
07:08:23,850 --> 07:08:28,510
or k might be selected to represent the geographical locations of respondents to

83
07:08:28,510 --> 07:08:29,670
a survey 

84
07:08:29,670 --> 07:08:31,150
in which case a good value for

85
07:08:31,150 --> 07:08:34,655
k would be the number of regions your interested in analyzing 

86
07:08:36,210 --> 07:08:40,330
there are also data - driven method for determining the value of k 

87
07:08:40,330 --> 07:08:42,140
these methods calculate symmetric for

88
07:08:42,140 --> 07:08:46,570
different values of k to determine the best selections of k 

89
07:08:46,570 --> 07:08:48,540
one such method is the elbow method 

90
07:08:49,790 --> 07:08:53,890
the elbow method for determining the value of k is shown on this plot 

91
07:08:53,890 --> 07:08:56,140
as we saw in the previous slide 

92
07:08:56,140 --> 07:09:00,690
wsse or within - cluster sum of squared error measures how much

93
07:09:00,690 --> 07:09:05,750
data samples deviate from their respective centroids in a set of clustering results 

94
07:09:05,750 --> 07:09:10,810
if we plot wsse for different values for k we can see how this

95
07:09:10,810 --> 07:09:16,170
error measure changes as a value of k changes as seen in the plot 

96
07:09:16,170 --> 07:09:22,010
the bend in this error curve indicates a drop in gain by adding more clusters 

97
07:09:22,010 --> 07:09:26,270
so this elbow in the curve provides a suggestion for a good value of k 

98
07:09:27,320 --> 07:09:31,540
note that the elbow can not always be unambiguously determined especially for

99
07:09:31,540 --> 07:09:33,170
complex data 

100
07:09:33,170 --> 07:09:36,630
and in many cases the error curve will not have a clear suggestion for

101
07:09:36,630 --> 07:09:39,270
one value but for multiple values 

102
07:09:39,270 --> 07:09:42,810
this can be used as a guideline for the range of values to try for k 

103
07:09:44,490 --> 07:09:46,650
we have discussed choosing the initial centroids and

104
07:09:46,650 --> 07:09:50,370
looked at ways to select a value for k the number of clusters 

105
07:09:50,370 --> 07:09:52,440
let now look at when to stop 

106
07:09:52,440 --> 07:09:54,763
how do you know when to stop iterating when using k - means 

107
07:09:54,763 --> 07:10:00,890
one obviously stopping criterion is when there are no changes to the centroids 

108
07:10:00,890 --> 07:10:04,370
this means that no samples would change cluster assignments 

109
07:10:04,370 --> 07:10:08,140
and recalculating the centroids will not result in any changes 

110
07:10:08,140 --> 07:10:11,410
so additional iterations will not bring about any more changes

111
07:10:11,410 --> 07:10:12,320
to the cluster results 

112
07:10:13,510 --> 07:10:18,350
the stopping criterion can be relaxed to the second stopping criterion listed here 

113
07:10:18,350 --> 07:10:21,390
which is when the number of sample changing clusters is below

114
07:10:21,390 --> 07:10:25,180
a certain threshold say 1 for example 

115
07:10:25,180 --> 07:10:28,750
at this point the clusters are changing by only a few samples 

116
07:10:28,750 --> 07:10:32,820
resulting in only minimal changes to the final cluster results 

117
07:10:32,820 --> 07:10:34,350
so the algorithm can be stopped here 

118
07:10:35,880 --> 07:10:39,370
at the end of k - means we have a set of clusters each with a centroid 

119
07:10:40,370 --> 07:10:44,860
each centroid is the mean of the samples assigned to that cluster 

120
07:10:44,860 --> 07:10:49,500
you can think of the centroid as a representative sample for that cluster 

121
07:10:49,500 --> 07:10:51,860
so to interpret the cluster analysis results 

122
07:10:51,860 --> 07:10:54,580
we can examine the cluster centroids 

123
07:10:54,580 --> 07:10:58,720
comparing the values of the variables between the centroids will reveal

124
07:10:58,720 --> 07:11:01,480
how different or alike clusters are and

125
07:11:01,480 --> 07:11:04,800
provide insights into what each cluster represents 

126
07:11:04,800 --> 07:11:09,259
for example if the value for age is different for different customer clusters 

127
07:11:09,259 --> 07:11:12,361
this indicates that the clusters are encoding different

128
07:11:12,361 --> 07:11:15,213
customer segments by age among other variables 

129
07:11:15,213 --> 07:11:20,390
in summary k - means is a classic algorithm for performing cluster analysis 

130
07:11:20,390 --> 07:11:23,570
it is an algorithm that is simple to understand and implement and

131
07:11:23,570 --> 07:11:25,330
is also efficient 

132
07:11:25,330 --> 07:11:29,158
the value of k the number of clusters must be specified 

133
07:11:29,158 --> 07:11:32,070
and final clusters are sensitive to initial centroids 

1
14:14:45,260 --> 14:14:48,590
linear regression is a very common algorithm to build regression models 

2
14:14:50,000 --> 14:14:55,150
after this video you will be able to describe how linear regression works 

3
14:14:55,150 --> 14:15:00,120
discuss how least squares is used in linear regression define simple and

4
14:15:00,120 --> 14:15:01,260
multiple linear regression 

5
14:15:03,070 --> 14:15:08,440
a linear regression model captures the relationship between a numerical output

6
14:15:08,440 --> 14:15:10,440
and the input variables 

7
14:15:10,440 --> 14:15:14,520
the relationship is modeled as a linear relationship hence the linear

8
14:15:14,520 --> 14:15:15,410
in linear regression 

9
14:15:16,620 --> 14:15:21,250
to see how linear regression works let take a look at an example from the iris

10
14:15:21,250 --> 14:15:25,792
flower dataset which is a commonly used dataset for machine learning 

11
14:15:25,792 --> 14:15:30,060
this dataset has samples of different species of iris flowers

12
14:15:30,060 --> 14:15:33,630
along with measurements such as petal width and petal length 

13
14:15:33,630 --> 14:15:38,600
here we have a plot with petal width measurements in centimeters on the x axis 

14
14:15:38,600 --> 14:15:41,610
and petal length measurements on the y axis 

15
14:15:41,610 --> 14:15:45,589
let say that we want to predict petal length based on petal width 

16
14:15:46,680 --> 14:15:50,240
then the regression task is this given a measurement for

17
14:15:50,240 --> 14:15:52,350
petal width predict the petal length 

18
14:15:53,620 --> 14:15:57,650
we can build a linear regression model to capture this linear relationship between

19
14:15:57,650 --> 14:16:01,330
the input petal width and the output petal length 

20
14:16:01,330 --> 14:16:05,510
the linear relationship for this samples is shown as the red line on the plot 

21
14:16:06,910 --> 14:16:10,880
from this example we see that linear regression works by finding the best

22
14:16:10,880 --> 14:16:13,740
fitting straight line through the samples 

23
14:16:13,740 --> 14:16:15,300
this is called the regression line 

24
14:16:16,450 --> 14:16:19,360
in the simple case with just one input variable 

25
14:16:19,360 --> 14:16:22,000
the regression line is simply a line 

26
14:16:22,000 --> 14:16:27,290
the equation for a line is y = mx + b 

27
14:16:27,290 --> 14:16:31,530
where m determines the slope of the line and

28
14:16:31,530 --> 14:16:35,480
b is the y intercept or where the line crosses the y axis 

29
14:16:36,790 --> 14:16:38,960
m and b are the parameters of the model 

30
14:16:40,130 --> 14:16:43,850
training a linear regression model means adjusting these parameters to

31
14:16:43,850 --> 14:16:46,770
fit the regression line to the samples 

32
14:16:46,770 --> 14:16:49,890
the regression line can be determined using what is referred to

33
14:16:49,890 --> 14:16:51,380
as the least squares method 

34
14:16:52,540 --> 14:16:56,100
this plot illustrates how the least squares method works 

35
14:16:56,100 --> 14:16:57,980
the yellow dots are the data samples 

36
14:16:59,320 --> 14:17:01,690
the red line is the regression line 

37
14:17:01,690 --> 14:17:04,150
the straight line that goes through the samples 

38
14:17:04,150 --> 14:17:08,210
this line represents the model prediction of the output given the input 

39
14:17:09,380 --> 14:17:14,010
each green line indicates the distance of each sample from the regression line 

40
14:17:14,010 --> 14:17:17,630
so the green line represents the error between the prediction 

41
14:17:17,630 --> 14:17:21,870
which is the value of the red regression line and the actual value of the sample 

42
14:17:22,990 --> 14:17:27,080
the square of this distance is referred to as the residual

43
14:17:27,080 --> 14:17:29,390
associated with that sample 

44
14:17:29,390 --> 14:17:31,970
the least squares method finds the regression line

45
14:17:31,970 --> 14:17:36,430
that makes the sum of the residuals as small as possible 

46
14:17:36,430 --> 14:17:39,920
in other words we want to find the line that minimizes the sum

47
14:17:39,920 --> 14:17:41,800
of the squared errors of prediction 

48
14:17:43,040 --> 14:17:47,160
the goal of linear regression then is to find the best fitting straight line

49
14:17:47,160 --> 14:17:49,490
through the samples using the least squares method 

50
14:17:50,820 --> 14:17:54,550
once the regression model is built we can use it to make predictions 

51
14:17:54,550 --> 14:17:58,200
for example given a measurement of 1 5 centimeters for

52
14:17:58,200 --> 14:18:02,920
petal width the model will predict a value of 4 5 centimeters for

53
14:18:02,920 --> 14:18:06,250
petal length base on the regression line that it has constructed 

54
14:18:07,450 --> 14:18:11,808
in linear regression if there is only one input variable then the task is

55
14:18:11,808 --> 14:18:14,350
referred to as simple linear regression 

56
14:18:15,490 --> 14:18:18,210
in cases with more than one input variables 

57
14:18:18,210 --> 14:18:21,850
then it is referred to as multiple linear regression 

58
14:18:21,850 --> 14:18:25,640
to summarize linear regression captures the linear relationship

59
14:18:25,640 --> 14:18:29,328
between a numerical output and the input variables 

60
14:18:29,328 --> 14:18:33,240
the least squares method can be used to build a linear regression model

61
14:18:33,240 --> 14:18:35,780
by finding the best fitting line through the samples 

1
04:33:19,880 --> 04:33:23,540
we studied the machine learning process applied techniques to explore and

2
04:33:23,540 --> 04:33:27,910
prepare data discussed the different categories of machine learning tasks 

3
04:33:27,910 --> 04:33:30,200
looked at metrics and methods for evaluating a model 

4
04:33:30,200 --> 04:33:35,020
learned how to use scalable machine learning algorithms for big data problems 

5
04:33:35,020 --> 04:33:38,469
and worked with two widely used tools to construct the machine learning models 

6
04:33:39,650 --> 04:33:43,040
i hope that the lectures along with the hands - on activities have given you

7
04:33:43,040 --> 04:33:46,680
a sound and practical introduction to machine learning tools and techniques 

8
04:33:46,680 --> 04:33:51,010
i also hope that the course piqued your interest in the exiting and

9
04:33:51,010 --> 04:33:53,960
rapidly developing field of machine learning for big data 

10
04:33:55,110 --> 04:33:57,630
keep in mind that the best way to learn machine learning

11
04:33:57,630 --> 04:33:59,600
is to do machine learning 

12
04:33:59,600 --> 04:34:04,470
so i encourage you to go out and find a problem or data set that interest you 

13
04:34:04,470 --> 04:34:08,800
apply the techniques you have learned in this course to it and start analyzing 

14
04:34:08,800 --> 04:34:12,110
thank you for your time and effort on this course happy machine learning 

1
09:07:32,260 --> 09:07:35,660
if you recall we have previously discussed that the main categories of

2
09:07:35,660 --> 09:07:40,170
machine learning tasks are classification regression 

3
09:07:40,170 --> 09:07:43,560
cluster analysis and association analysis 

4
09:07:43,560 --> 09:07:46,100
we have discussed classification in detail 

5
09:07:46,100 --> 09:07:49,190
now let look at the other categories starting with regression 

6
09:07:50,840 --> 09:07:55,880
after this video you will be able to define what regression is 

7
09:07:55,880 --> 09:07:59,870
explain the difference between regression and classification and

8
09:07:59,870 --> 09:08:01,630
name some applications of regression 

9
09:08:03,140 --> 09:08:07,410
before we talk about regression let review classification 

10
09:08:07,410 --> 09:08:11,630
in a classification problem the input data is presented to the machine learning

11
09:08:11,630 --> 09:08:17,450
model and the task is to predict the target corresponding to the input data 

12
09:08:17,450 --> 09:08:20,960
the target is a categorical variable 

13
09:08:20,960 --> 09:08:25,132
so the classification task is to predict the category or label of

14
09:08:25,132 --> 09:08:28,450
the target given the input data 

15
09:08:28,450 --> 09:08:32,870
the classification example shown here is one we have seen before 

16
09:08:32,870 --> 09:08:37,950
the input variables are measurements such as temperature relative humidity 

17
09:08:37,950 --> 09:08:41,790
atmospheric pressure wind speed wind direction etc 

18
09:08:41,790 --> 09:08:42,500
the task for

19
09:08:42,500 --> 09:08:47,590
the model is to predict the weather category associated with the input data 

20
09:08:47,590 --> 09:08:48,730
the possible values for

21
09:08:48,730 --> 09:08:53,180
the weather category is sunny windy rainy or cloudy 

22
09:08:53,180 --> 09:08:57,265
since we are predicting the category this is a classification task 

23
09:08:57,265 --> 09:09:02,080
with that context in mind let now discuss regression 

24
09:09:02,080 --> 09:09:06,220
when the model has to predict a numeric value instead of a category 

25
09:09:06,220 --> 09:09:09,410
then the task becomes a regression problem 

26
09:09:09,410 --> 09:09:13,170
an example of regression is to predict the price of a stock 

27
09:09:13,170 --> 09:09:16,415
the stock price is a numeric value not a category 

28
09:09:16,415 --> 09:09:20,455
so this is a regression task instead of a classification task 

29
09:09:20,455 --> 09:09:24,755
note that if you were to predict not the actual price of the stock but whether

30
09:09:24,755 --> 09:09:29,505
the stock price will go up or go down then that would be a classification task 

31
09:09:29,505 --> 09:09:33,085
that is the main difference between classification and regression 

32
09:09:33,085 --> 09:09:36,120
in classification you are predicting a category and

33
09:09:36,120 --> 09:09:38,270
in regression you are predicting a numeric value 

34
09:09:39,720 --> 09:09:42,810
here are some examples where regression can be used 

35
09:09:42,810 --> 09:09:47,610
forecast the high temperature for the next day estimate the average housing price

36
09:09:47,610 --> 09:09:52,640
for a particular region determine the demand for a new product a new book for

37
09:09:52,640 --> 09:09:57,980
example based on similar existing products predict the power usage for

38
09:09:57,980 --> 09:09:59,160
a particular power grid 

39
09:10:00,890 --> 09:10:02,780
this is what the data set might look like for

40
09:10:02,780 --> 09:10:06,190
the regression task of predicting tomorrow high temperature 

41
09:10:06,190 --> 09:10:08,340
the input variables could be the high temperature for

42
09:10:08,340 --> 09:10:11,940
today the low temperature for today and the month 

43
09:10:11,940 --> 09:10:15,270
and the target is the high temperature for tomorrow 

44
09:10:15,270 --> 09:10:19,220
the model has to predict this target value for each sample 

45
09:10:19,220 --> 09:10:23,250
recall that in a supervised task the target is provided 

46
09:10:23,250 --> 09:10:28,070
well for an unsupervised task the target is not available or not known 

47
09:10:28,070 --> 09:10:31,540
since the target label is provided for each sample here 

48
09:10:31,540 --> 09:10:35,370
the regression task is a supervised one similar to classification 

49
09:10:36,900 --> 09:10:41,580
as with classification building a regression model also involve two phases 

50
09:10:41,580 --> 09:10:44,330
a training phase in which the model is built and

51
09:10:44,330 --> 09:10:47,410
a testing phase in which the model is applied to new data 

52
09:10:48,590 --> 09:10:52,550
the model is built using training data and evaluated on test data 

53
09:10:53,690 --> 09:10:58,011
similar to classification the goal in building a regression model is also to

54
09:10:58,011 --> 09:11:02,340
have a model perform well on training data as well as generalize to new data 

55
09:11:03,850 --> 09:11:07,190
the use of three different datasets that we have previously discussed

56
09:11:07,190 --> 09:11:09,470
also apply to regression 

57
09:11:09,470 --> 09:11:12,650
recall that the three datasets are used as follows 

58
09:11:12,650 --> 09:11:17,130
the training dataset is used to train the model that is to adjust the parameters of

59
09:11:17,130 --> 09:11:19,408
the model to learn the input to output mapping 

60
09:11:19,408 --> 09:11:24,380
the validation dataset is used to determine when training should stop

61
09:11:24,380 --> 09:11:26,700
in order to avoid over fitting 

62
09:11:26,700 --> 09:11:32,320
and the test dataset is used to evaluate the performance of the model on new data 

63
09:11:32,320 --> 09:11:36,900
in summary in regression the model needs to predict the numeric value

64
09:11:36,900 --> 09:11:38,970
corresponding to the input data 

65
09:11:38,970 --> 09:11:44,160
since a target is provided for each sample regression is a supervised task 

66
09:11:44,160 --> 09:11:47,990
the target is always a numerical variable in regression 

67
09:11:47,990 --> 09:11:49,042
in the next lecture 

68
09:11:49,042 --> 09:11:52,323
we will discuss a specific algorithm to build a regression model 

1
18:19:24,060 --> 18:19:25,330
hello 

2
18:19:25,330 --> 18:19:28,750
welcome to the graph analytics module in the big data specialization 

3
18:19:30,180 --> 18:19:34,250
i am amarnath gupta a research scientist at the san diego supercomputer center 

4
18:19:35,730 --> 18:19:37,080
what do i do research on 

5
18:19:38,150 --> 18:19:43,380
well a number of different areas all generally related to data engineering 

6
18:19:44,570 --> 18:19:48,740
but the area i am recently very excited about has to do with graphs 

7
18:19:50,160 --> 18:19:53,650
now graphs or networks has many people call them 

8
18:19:54,650 --> 18:20:00,270
are about studying relationships and relationship patterns on objects 

9
18:20:02,570 --> 18:20:08,420
i became interested in graphs when i was a graduate student and

10
18:20:08,420 --> 18:20:14,930
the professor in our artificial intelligence class showed us how a part of

11
18:20:14,930 --> 18:20:21,240
human knowledge can be represented as a kind of graphs called semantic networks 

12
18:20:23,560 --> 18:20:28,030
she showed us how even very simple things like

13
18:20:28,030 --> 18:20:32,840
relationships somewhere in a family can be represented and viewed as graphs 

14
18:20:33,960 --> 18:20:36,121
represent your knowledge huh 

15
18:20:36,121 --> 18:20:38,030
now that got me really excited 

16
18:20:39,160 --> 18:20:45,042
someday i thought this will be a really interesting topic to research on 

17
18:20:46,730 --> 18:20:52,660
what i did not realize then is that in just a few years graphs and

18
18:20:52,660 --> 18:20:58,787
the need to model and analyze them would become so dominant in both academia and

19
18:20:58,787 --> 18:21:03,600
industry that graphs will be found everywhere today 

20
18:21:04,980 --> 18:21:09,520
now we look at facebook linkedin twitter and many 

21
18:21:09,520 --> 18:21:14,930
many more companies that are thriving in the market with data

22
18:21:14,930 --> 18:21:19,450
that are represented modeled and processed as graphs 

23
18:21:21,150 --> 18:21:24,210
now even the entire world wide web if you think about it 

24
18:21:24,210 --> 18:21:26,140
is a giant graph that people analyze 

25
18:21:27,320 --> 18:21:29,710
and that brings us to this course 

26
18:21:29,710 --> 18:21:34,400
now in this course i will introduce you to the wonderful world of graph analytics

27
18:21:34,400 --> 18:21:37,700
specifically i would like to show how different

28
18:21:37,700 --> 18:21:42,070
kinds of real world data science problems can be viewed and modeled as graphs 

29
18:21:43,110 --> 18:21:47,856
and how the process of solving them can apply analytical

30
18:21:47,856 --> 18:21:52,710
techniques that used graph based methods that is algorithms 

31
18:21:55,180 --> 18:21:56,870
this course would have four models 

32
18:21:58,000 --> 18:22:03,820
in model one we will introduce graphs and different applications that use graphs 

33
18:22:06,280 --> 18:22:11,880
in model two we will cover a number of common techniques 

34
18:22:11,880 --> 18:22:16,000
mathematical and algorithm techniques that are used in graph analytics 

35
18:22:17,190 --> 18:22:20,264
in module three we will look at a graph database 

36
18:22:20,264 --> 18:22:23,288
and through some sort of a hands on guidance 

37
18:22:23,288 --> 18:22:27,840
we will show you how to store and query graph data with the database 

38
18:22:28,870 --> 18:22:33,800
in module four we will cover some strategies of handling very large

39
18:22:33,800 --> 18:22:38,855
graphs and discuss how existing tools that are currently used by

40
18:22:38,855 --> 18:22:44,625
the community are actually prints 

41
18:22:44,625 --> 18:22:46,585
thank you for joining this course and

42
18:22:46,585 --> 18:22:50,885
i sincerely hope that you will find it both exciting and useful 

43
18:22:52,265 --> 18:22:52,884
happy learning 

1
12:42:15,660 --> 12:42:19,950
as we saw in these four examples what graphs are used for

2
12:42:19,950 --> 12:42:22,920
are kind of different but they all show

3
12:42:22,920 --> 12:42:26,275
different viewpoints from which you can use graphs for your analysis 

4
12:42:26,275 --> 12:42:31,380
since this course focuses on graph analytics 

5
12:42:31,380 --> 12:42:34,150
i would like to briefly recap what the term means 

6
12:42:35,430 --> 12:42:40,790
analytics is the ability to discover meaningful patterns and

7
12:42:40,790 --> 12:42:44,972
interesting insights into data using mathematical properties of data 

8
12:42:46,150 --> 12:42:50,260
it covers the process of computing with mathematical properties and

9
12:42:50,260 --> 12:42:53,460
accessing the data itself efficiently 

10
12:42:53,460 --> 12:42:55,970
further it involves the ability to represent and

11
12:42:55,970 --> 12:42:59,910
work with domain knowledge as we saw in use case two with biology 

12
12:42:59,910 --> 12:43:04,220
finally analytics often involves statistical modeling techniques for

13
12:43:04,220 --> 12:43:07,180
drawing inferences and making predictions on data 

14
12:43:08,510 --> 12:43:13,110
with analytics we should be able to achieve the goals shown here 

15
12:43:14,900 --> 12:43:15,977
take a minute to read 

16
12:43:25,544 --> 12:43:30,180
therefore graph analytics is a special case of analytics where

17
12:43:30,180 --> 12:43:33,985
the underlying data can be modeled as a set of graphs 

1
01:25:48,890 --> 01:25:53,257
so in this video we will going to talk about graph analytics

2
01:25:53,257 --> 01:25:57,820
within the context of this big data specialization 

3
01:26:00,080 --> 01:26:04,330
so in the previous courses you know about the three important v of big data 

4
01:26:05,410 --> 01:26:09,580
so the three well - known v are volume velocity and variety 

5
01:26:10,940 --> 01:26:15,440
we will also talk about a lesser - known v which is called valence 

6
01:26:17,170 --> 01:26:21,976
okay what we want to talk about is 

7
01:26:21,976 --> 01:26:27,881
what impact these things have on graph data 

8
01:26:27,881 --> 01:26:32,693
so for volume let take a dataset like

9
01:26:32,693 --> 01:26:37,238
the load network of the united states 

10
01:26:37,238 --> 01:26:39,770
well that a pretty large graph 

11
01:26:39,770 --> 01:26:43,870
so when we say volume i mean that the size of the graph

12
01:26:43,870 --> 01:26:47,790
is much larger than what you might have

13
01:26:48,960 --> 01:26:53,400
in the memory of a reasonable computer or real computing infrastructure 

14
01:26:55,130 --> 01:27:00,462
now we will see what impact the size

15
01:27:00,462 --> 01:27:06,490
of the graph has on analytic operations 

16
01:27:06,490 --> 01:27:09,497
what we mean by velocity when it comes to graphs 

17
01:27:09,497 --> 01:27:10,620
well think of facebook again 

18
01:27:11,860 --> 01:27:13,680
so these little graphs are updates 

19
01:27:14,700 --> 01:27:22,190
so you write a post then like somebody else post and make a comment 

20
01:27:22,190 --> 01:27:23,774
that a bunch of updates 

21
01:27:23,774 --> 01:27:25,810
that comes and adds to your graph 

22
01:27:27,110 --> 01:27:31,440
well then ten minutes later you do something similar and

23
01:27:31,440 --> 01:27:33,000
that also comes and adds to the graph 

24
01:27:33,000 --> 01:27:36,060
then your friend does the same thing it adds to your graph 

25
01:27:36,060 --> 01:27:41,530
so as time goes by you are sending more edges to your graph 

26
01:27:43,060 --> 01:27:47,240
and the speed at which you are doing this for

27
01:27:47,240 --> 01:27:50,870
at least like facebook can be really really high 

28
01:27:50,870 --> 01:27:53,590
so the rate of update in facebook is really high 

29
01:27:54,910 --> 01:28:01,997
this is what is called streaming edges into graphs 

30
01:28:01,997 --> 01:28:07,314
and there can be multiple streams for various reasons 

31
01:28:07,314 --> 01:28:08,515
what do we mean by variety 

32
01:28:08,515 --> 01:28:16,790
for graphs it means that the graph is collecting data from various places 

33
01:28:16,790 --> 01:28:21,002
and all these different places are giving different kinds of information to

34
01:28:21,002 --> 01:28:21,671
the graph 

35
01:28:21,671 --> 01:28:24,905
so in the end the graph has more non - uniform and

36
01:28:24,905 --> 01:28:29,217
complex information potentially coming from multiple sources 

37
01:28:29,217 --> 01:28:32,210
that what we mean by variety when we refer to graphs 

38
01:28:33,690 --> 01:28:36,930
that picture there by the way is different kinds of protein interactions 

39
01:28:40,600 --> 01:28:43,850
the next one the less - known one is valence 

40
01:28:45,510 --> 01:28:50,578
now if you remember your chemistry this comes from valence electrons 

41
01:28:50,578 --> 01:28:54,470
which are electrons in an atom which are used for bonding 

42
01:28:54,470 --> 01:28:57,790
the other electrons are called core electrons 

43
01:28:57,790 --> 01:29:02,274
so the idea is if we increase the valence of the graphs 

44
01:29:02,274 --> 01:29:06,183
you increase the connectiveness of the graph 

45
01:29:06,183 --> 01:29:07,901
how we will see 

46
01:29:10,920 --> 01:29:15,894
now graph size clearly impacts analytics 

47
01:29:15,894 --> 01:29:23,041
why a it takes more space but more importantly it increases the algorithmic

48
01:29:23,041 --> 01:29:28,190
complexity of any operation that you want it to on the graph 

49
01:29:28,190 --> 01:29:33,344
now we will see an example of that but what happens as

50
01:29:33,344 --> 01:29:38,948
a result is that the data - to - analysis time becomes high 

51
01:29:38,948 --> 01:29:42,240
so i put in some data and i wanted to do this analysis 

52
01:29:42,240 --> 01:29:47,120
but there is so much data that my analysis takes way longer than it should 

53
01:29:48,620 --> 01:29:53,343
let give a simple example an example we have seen before 

54
01:29:53,343 --> 01:29:58,373
remember we had this little graph from our biological example where we

55
01:29:58,373 --> 01:30:04,488
were asking find a simple path between alzheimer disease and colorectal cancer 

56
01:30:04,488 --> 01:30:06,925
and in this case the result is obvious 

57
01:30:10,524 --> 01:30:13,589
now let pause and ask 

58
01:30:13,589 --> 01:30:17,248
there are two nodes that i mentioned in this case 

59
01:30:17,248 --> 01:30:21,170
my colorectal cancer and alzheimer disease nodes 

60
01:30:22,800 --> 01:30:27,940
and we are asking is there a simple path connecting them 

61
01:30:30,290 --> 01:30:32,470
this is called a decision problem 

62
01:30:32,470 --> 01:30:39,988
i give you a data and i am asking does such a simple path exist or not exist 

63
01:30:39,988 --> 01:30:43,124
but this is actually a very hard decision problem 

64
01:30:43,124 --> 01:30:47,964
and the computer scientists will tell you that this is a very complicated problem

65
01:30:47,964 --> 01:30:50,321
because it has a very high complexity 

66
01:30:50,321 --> 01:30:52,340
let ask another question 

67
01:30:52,340 --> 01:30:55,070
well how many simple paths now i want to count 

68
01:30:55,070 --> 01:30:57,430
how many simple paths exist between these two nodes 

69
01:30:58,810 --> 01:31:01,580
indeed it is another hard computing problem 

70
01:31:03,370 --> 01:31:07,750
and if you really want to know the size of the result 

71
01:31:07,750 --> 01:31:12,780
in the worst case is exponential in the number of nodes 

72
01:31:12,780 --> 01:31:15,930
so if we increase the number of nodes and edges if we increase the size of

73
01:31:15,930 --> 01:31:23,490
the graph such a seemingly simple question can take a very very very long time 

74
01:31:25,180 --> 01:31:28,890
so that it almost practically impossible to compute it for

75
01:31:28,890 --> 01:31:32,910
a really large graph if we have no other information supporting 

76
01:31:32,910 --> 01:31:35,008
that the worst case 

77
01:31:35,008 --> 01:31:40,441
but when we say algorithmic complexity increases that what we mean 

78
01:31:43,692 --> 01:31:48,894
let talk about velocity and i said our favorite example is facebook 

79
01:31:48,894 --> 01:31:53,096
so we are adding a bunch of updates which means we are adding a bunch of edges 

80
01:31:53,096 --> 01:31:59,490
we are streaming the edges into the data and we want to compute a metric 

81
01:31:59,490 --> 01:32:03,791
we want to see what is the shortest distance

82
01:32:03,791 --> 01:32:08,697
between person a and person b or item a and item b 

83
01:32:08,697 --> 01:32:13,155
or i want to know that facebook has communities 

84
01:32:13,155 --> 01:32:14,690
twitter has communities like we saw 

85
01:32:16,220 --> 01:32:20,540
and how many people out there in these communities and

86
01:32:20,540 --> 01:32:23,600
how many such communities are there like a facebook group 

87
01:32:25,560 --> 01:32:30,180
now if you want to compute this metric and you get this

88
01:32:30,180 --> 01:32:34,449
edges very fast it is very difficult to know when you have the answer 

89
01:32:35,480 --> 01:32:43,450
because you are going to get an increasing number of edges in the system 

90
01:32:44,450 --> 01:32:49,660
and you keep computing this metric that you want to find the answer for and

91
01:32:49,660 --> 01:32:55,040
it will turn out that your continuous stream does not fit in memory 

92
01:32:55,040 --> 01:32:59,820
because your memory is limited compared to the amount of

93
01:33:01,930 --> 01:33:05,680
edges or edge updates you are streaming into the system 

94
01:33:07,010 --> 01:33:11,221
so that what is happened when you have high velocity information 

95
01:33:11,221 --> 01:33:16,584
very soon your memory runs out and you want to compute

96
01:33:16,584 --> 01:33:21,290
your answer right now from the data that you have 

97
01:33:24,110 --> 01:33:27,640
okay let look at variety also known as heterogeneity 

98
01:33:29,460 --> 01:33:32,881
there are two aspects of heterogeneity 

99
01:33:32,881 --> 01:33:38,803
one we have already mentioned graph data is often created through integration 

100
01:33:38,803 --> 01:33:41,449
like we saw in the case of the biology 

101
01:33:44,446 --> 01:33:50,530
and therefore the variety comes because the nature of data is very different 

102
01:33:52,510 --> 01:33:55,800
also they may not be all the same kind of data 

103
01:33:56,890 --> 01:34:01,340
for example the data may come from a relational database 

104
01:34:02,440 --> 01:34:04,580
it may come from an xml database 

105
01:34:04,580 --> 01:34:06,900
it may come from another graph 

106
01:34:06,900 --> 01:34:08,130
it may come from a document 

107
01:34:09,470 --> 01:34:14,910
it may even come from complex things like social networks 

108
01:34:14,910 --> 01:34:19,890
like citation networks between papers or patents between interaction networks 

109
01:34:21,070 --> 01:34:24,300
between web entities which are connected through links 

110
01:34:25,780 --> 01:34:31,360
and from human knowledge that has been represented as graphs through ontologists 

111
01:34:31,360 --> 01:34:37,476
so all of these graphs the nodes and the edges do not mean the same thing 

112
01:34:37,476 --> 01:34:42,083
and somehow in there you need to capture what it means to have

113
01:34:42,083 --> 01:34:46,971
an edge because that will determine what you can do with the edge 

114
01:34:46,971 --> 01:34:51,490
a simple example in an ontology 

115
01:34:51,490 --> 01:34:58,280
is something that says a is a b and b is a c so a is a c 

116
01:34:59,730 --> 01:35:06,430
the a is a c is an inference that you do given the other two relationships 

117
01:35:06,430 --> 01:35:09,133
what would be an example 

118
01:35:09,133 --> 01:35:15,570
my pet is a dog and the dog is a mammal therefore my pet is a mammal 

119
01:35:16,770 --> 01:35:20,320
you want to do inferences for some edges likes is a 

120
01:35:21,480 --> 01:35:24,520
now you need to know this 

121
01:35:24,520 --> 01:35:28,140
you do not do this with the biology example where you are looking at genes and

122
01:35:28,140 --> 01:35:33,447
proteins because that operation does not make sense when you have genes and

123
01:35:33,447 --> 01:35:34,230
proteins 

124
01:35:34,230 --> 01:35:38,720
so therefore every graph may have a different semantics 

125
01:35:38,720 --> 01:35:41,880
and what happens with variety is the number of sub - semantics and

126
01:35:41,880 --> 01:35:44,670
the number of valid operations that you can do 

127
01:35:44,670 --> 01:35:46,730
that changes and that becomes more complex 

128
01:35:48,395 --> 01:35:52,720
now valence i said is about connectedness 

129
01:35:52,720 --> 01:35:55,760
it is also about interdependency among data 

130
01:35:55,760 --> 01:36:00,844
so if i have a higher valence which means i have more data elements that

131
01:36:00,844 --> 01:36:06,040
are more strongly related and these relationships can be exploited 

132
01:36:08,820 --> 01:36:13,437
in most cases the part where valence becomes important 

133
01:36:13,437 --> 01:36:19,806
is that it increases over time which means parts of the graph becomes denser 

134
01:36:19,806 --> 01:36:24,264
and the average distance between node pairs decreases 

135
01:36:24,264 --> 01:36:27,210
let me show you here is my gmail 

136
01:36:29,080 --> 01:36:37,140
and i have plotted my gmail graphs from 2006 to about two months back 

137
01:36:38,950 --> 01:36:42,810
when i first started using it i had these users 

138
01:36:42,810 --> 01:36:47,140
a very few users and they are not really related 

139
01:36:48,170 --> 01:36:50,220
now with time more and

140
01:36:50,220 --> 01:36:54,200
more people started communicating with me through gmail 

141
01:36:55,290 --> 01:36:59,618
and more and more of these people were also talking amongst themselves and

142
01:36:59,618 --> 01:37:01,240
copying me and responding to me 

143
01:37:02,410 --> 01:37:08,130
by the end you would see that you can find dense groups

144
01:37:08,130 --> 01:37:12,290
within my gmail because the information and

145
01:37:12,290 --> 01:37:19,020
the connectedness between people have evolved and become more dense over time 

146
01:37:19,020 --> 01:37:24,620
this is the phenomenon of valence and this is very important to study because

147
01:37:24,620 --> 01:37:31,120
you want to study things like what parts of the graph have become more dense 

148
01:37:31,120 --> 01:37:32,801
and why have they become more dense 

149
01:37:32,801 --> 01:37:34,490
maybe something was going on 

150
01:37:34,490 --> 01:37:37,010
maybe there was an event that brought these people together 

151
01:37:37,010 --> 01:37:40,550
and you want to analyze that and find that out from your graph analytics 

152
01:37:42,090 --> 01:37:47,067
that why you want to understand the effect of valence 

153
01:37:47,067 --> 01:37:52,361
you also want to understand what do i do if the graph becomes very very dense

154
01:37:52,361 --> 01:37:57,840
in a place so that finding a path through that dense space becomes very hard 

155
01:37:59,830 --> 01:38:05,532
you will see in a later video that when this happens the computer system 

156
01:38:05,532 --> 01:38:11,597
that is trying to process these graphs in a parallel and distributed way has to do

157
01:38:11,597 --> 01:38:17,507
something special to handle these increasing density in parts of the graph 

1
03:04:06,190 --> 03:04:11,640
so before we go into graph analytics the first question we want to ask is 

2
03:04:11,640 --> 03:04:12,168
what is a graph 

3
03:04:12,168 --> 03:04:15,332
let see 

4
03:04:15,332 --> 03:04:20,360
this is a plot of the sales of some items against time and

5
03:04:20,360 --> 03:04:25,190
it gives you a nice visual representation of the data but is it a graph 

6
03:04:26,470 --> 03:04:28,760
while you think about it let try another one 

7
03:04:30,790 --> 03:04:34,170
this is another common visual representation of data 

8
03:04:34,170 --> 03:04:36,450
if you go to google and type pie graph 

9
03:04:36,450 --> 03:04:39,750
you will see many results that look like this 

10
03:04:39,750 --> 03:04:40,580
but is this a graph 

11
03:04:41,750 --> 03:04:43,950
in our research this is not a graph 

12
03:04:45,260 --> 03:04:46,700
we call it a chart 

13
03:04:48,420 --> 03:04:51,190
yes you guessed it right this time 

14
03:04:51,190 --> 03:04:53,070
this too is a chart and not a graph 

15
03:04:54,760 --> 03:04:57,460
so why do people call them graphs then 

16
03:04:58,630 --> 03:05:01,460
in a sense they are abbreviating 

17
03:05:01,460 --> 03:05:04,519
a chart depicts what is called a graph of a function 

18
03:05:05,580 --> 03:05:06,150
let me explain 

19
03:05:07,300 --> 03:05:08,970
look at the two column table on the right 

20
03:05:10,000 --> 03:05:13,170
the first column has information about product category

21
03:05:13,170 --> 03:05:15,940
with values like furniture office supplies and technology 

22
03:05:17,120 --> 03:05:22,170
the second column represents another set of values called total containing 1724 

23
03:05:22,170 --> 03:05:26,570
4610 and 2065 

24
03:05:26,570 --> 03:05:30,320
now we can define our mapping option 

25
03:05:30,320 --> 03:05:34,180
which means a correspondence from product category to total 

26
03:05:35,190 --> 03:05:39,070
so we map furniture to the value 1 724 

27
03:05:39,070 --> 03:05:42,440
office supplies to the value 4 610 and so forth 

28
03:05:42,440 --> 03:05:44,480
if we visually portray this 

29
03:05:44,480 --> 03:05:47,820
we can represent it like a bar chart or a pie chart 

30
03:05:47,820 --> 03:05:52,560
this is why both the previous diagram and this diagram are charts and not graphs 

31
03:05:55,190 --> 03:05:59,990
graph analytics has its basis in a brand of mathematics called graph theory 

32
03:05:59,990 --> 03:06:00,830
what is more interesting 

33
03:06:00,830 --> 03:06:05,640
however is that graph theory was born out of a very practical problem 

34
03:06:07,040 --> 03:06:11,900
the problem started in a very old city in prussia which is now in russia

35
03:06:12,960 --> 03:06:13,995
called konigsberg 

36
03:06:15,890 --> 03:06:20,528
even if we look at in in google maps it would sort of look like this 

37
03:06:20,528 --> 03:06:25,135
one of the interesting features of konigsberg is that it has two islands and

38
03:06:25,135 --> 03:06:28,074
these islands are connected by seven bridges 

39
03:06:28,074 --> 03:06:34,624
back in 1736 the city wanted to create a walkway and the criteria was 

40
03:06:34,624 --> 03:06:40,083
this walkway would traverse all seven bridges such that somebody

41
03:06:40,083 --> 03:06:46,556
wanting to go from one part of the city to another can cross a bridge only once 

42
03:06:46,556 --> 03:06:51,120
now this is sort of an urban planning problem right 

43
03:06:51,120 --> 03:06:54,520
well in fact it required a mathematician to solve the problem 

44
03:06:55,850 --> 03:06:59,440
the mathematician named euler shown on the left looked at this and

45
03:06:59,440 --> 03:07:03,260
figured out that you really cannot create such a walk 

46
03:07:03,260 --> 03:07:04,510
why 

47
03:07:04,510 --> 03:07:08,030
he said it cannot be done because there are an odd number of bridges and

48
03:07:08,030 --> 03:07:09,870
proved it mathematically 

49
03:07:09,870 --> 03:07:13,120
from this problem the whole field called graph theory emerged 

50
03:07:14,670 --> 03:07:19,600
on the right you can see edsger dijkstra a well - known computer scientist who has

51
03:07:19,600 --> 03:07:24,330
developed graph algorithms one of which we will study later in the course 

52
03:07:24,330 --> 03:07:29,120
his work has had far further impact on both the theoretical computer science and

53
03:07:29,120 --> 03:07:30,040
practical applications 

54
03:07:31,760 --> 03:07:34,130
what is the difference between the mathematics view and

55
03:07:34,130 --> 03:07:35,140
the computer science view 

56
03:07:36,190 --> 03:07:38,170
let try to define the mathematical view of graphs 

57
03:07:39,490 --> 03:07:41,970
we start with a set of vertices 

58
03:07:41,970 --> 03:07:46,595
here we have a set of six nodes or vertices 

59
03:07:46,595 --> 03:07:49,060
now i will add another set 

60
03:07:49,060 --> 03:07:50,810
i will call this the set of edges 

61
03:07:52,020 --> 03:07:54,720
in our diagram there are only four edges but

62
03:07:54,720 --> 03:07:56,560
there something special about these edges 

63
03:07:57,990 --> 03:08:02,720
each edge is just not an ordinary atomic object 

64
03:08:02,720 --> 03:08:08,390
an edge like e1 actually is one term from v and then a second term from v 

65
03:08:09,510 --> 03:08:13,260
so this edge e1 goes from v1 to v5 

66
03:08:14,650 --> 03:08:17,230
pictorially we can draw and arrow from v1 to v5 

67
03:08:18,800 --> 03:08:22,270
now if i had said v5 to v1 the arrow would be reversed 

68
03:08:23,370 --> 03:08:24,870
so what do we have so far 

69
03:08:24,870 --> 03:08:28,070
we have a set of vertices and a set of edges 

70
03:08:28,070 --> 03:08:30,550
that is the mathematical definition of a graph 

71
03:08:32,900 --> 03:08:35,050
what about the computer scientist definition 

72
03:08:36,410 --> 03:08:40,760
of course a computer scientist needs to adhere to the mathematical definition but

73
03:08:40,760 --> 03:08:44,120
they want to represent and manipulate the same information 

74
03:08:44,120 --> 03:08:46,380
so they need a data structure 

75
03:08:46,380 --> 03:08:49,220
in other words something they can operate on 

76
03:08:49,220 --> 03:08:50,720
so what kind of operations would they do 

77
03:08:51,808 --> 03:08:52,650
well with a graph 

78
03:08:52,650 --> 03:08:57,340
they can say add edge or add a new vertex find the nearest neighbors of

79
03:08:57,340 --> 03:09:01,100
the vertex where the term neighbor refers to the nodes connected to the vertex 

80
03:09:03,320 --> 03:09:07,590
we said a computer scientist needs to represent graphs using data structures 

81
03:09:07,590 --> 03:09:09,400
here is one that you should recognize 

82
03:09:09,400 --> 03:09:12,360
it a matrix called the adjacency matrix of a graph 

83
03:09:13,390 --> 03:09:16,560
both the rows and columns of this matrix represent notes 

84
03:09:18,080 --> 03:09:23,718
if i go from v1 one along the row i see that there one at v3 

85
03:09:23,718 --> 03:09:27,597
which means there is an edge from v1 to v3 

86
03:09:27,597 --> 03:09:32,543
similarly there is another from v1 to v5 

87
03:09:32,543 --> 03:09:34,563
let look at some operation 

88
03:09:34,563 --> 03:09:38,580
let say i first want to add an edge from v3 to v2 

89
03:09:38,580 --> 03:09:40,298
what should i do 

90
03:09:40,298 --> 03:09:43,831
i will start from the row v3 

91
03:09:43,831 --> 03:09:49,493
go up to the column v2 and add a 1 in that set 

92
03:09:49,493 --> 03:09:53,030
so i have added an edge which is an operation on the matrix 

93
03:09:54,890 --> 03:09:55,700
another operation 

94
03:09:57,020 --> 03:09:58,540
i want to get the neighbors a v3 

95
03:09:59,600 --> 03:10:01,180
this will be a little more complicated 

96
03:10:03,020 --> 03:10:06,090
i look at the row v3 and paint that row and

97
03:10:06,090 --> 03:10:08,930
i will also look at the column v3 and paint that column 

98
03:10:10,370 --> 03:10:16,430
if we go down the row of v3 we will get v2 so v2 is neighbor 

99
03:10:16,430 --> 03:10:21,150
and if we go down the column of v3 we will get v1 and v6 

100
03:10:22,420 --> 03:10:25,930
so v1 and v6 have alternators 

101
03:10:25,930 --> 03:10:26,540
what is the difference 

102
03:10:29,250 --> 03:10:35,070
since the matrix has the from along the rows and the tos along the columns 

103
03:10:35,070 --> 03:10:38,950
following the row v3 gives us the edges outgoing from v3 

104
03:10:39,960 --> 03:10:43,910
and following the column v3 gives us the edges coming into v3 

105
03:10:46,590 --> 03:10:49,340
is this the only representation of the graph 

106
03:10:49,340 --> 03:10:50,830
no it not 

107
03:10:50,830 --> 03:10:51,390
here is another one 

108
03:10:52,490 --> 03:10:56,300
in this representation there are two kinds of data objects 

109
03:10:56,300 --> 03:11:00,800
no data which are the blue rectangles and edge data which are the triangles 

110
03:11:02,080 --> 03:11:06,000
to get a sense of this representation look at the note v1 and

111
03:11:06,000 --> 03:11:08,490
follow the top yellow line 

112
03:11:08,490 --> 03:11:10,490
it will reach v3 

113
03:11:10,490 --> 03:11:15,330
so this link structure directly captures the graph diagram we created before 

114
03:11:15,330 --> 03:11:20,150
now start from v1 again and follow the blue link and

115
03:11:20,150 --> 03:11:23,450
it will reach e1 and then v3 

116
03:11:23,450 --> 03:11:27,630
so that tells you that e1 is an edge object between v1 and v3 

117
03:11:29,530 --> 03:11:35,020
next let stand on the edge triangle e1 and follow the red

118
03:11:35,020 --> 03:11:40,450
dashed line to get to e2 which is the next edge from the same starting node 

119
03:11:41,570 --> 03:11:43,790
is this possibly too complex 

120
03:11:43,790 --> 03:11:45,200
yes it is 

121
03:11:45,200 --> 03:11:50,390
however as we will see down the road many graph database systems are using this kind

122
03:11:50,390 --> 03:11:55,050
of data structure internally so that the database operations become more efficient 

1
06:16:00,860 --> 06:16:03,220
in the first example of graph analytics for

2
06:16:03,220 --> 06:16:06,340
big data we will consider the social media called twitter 

3
06:16:07,480 --> 06:16:09,320
and graphs representing tweets 

4
06:16:11,160 --> 06:16:14,600
what types of objects and relationships does a tweet have 

5
06:16:14,600 --> 06:16:19,240
well in fact tweets consist of many of the same elements and

6
06:16:19,240 --> 06:16:20,890
relationships as the facebook posts 

7
06:16:22,910 --> 06:16:26,380
tweets have users they contain text 

8
06:16:26,380 --> 06:16:31,780
tweets can point to one another or to url they have hashtags they can

9
06:16:31,780 --> 06:16:36,720
include reference to various other types of media but what about relationships 

10
06:16:38,120 --> 06:16:42,120
well much like facebook relationships and

11
06:16:42,120 --> 06:16:46,660
tweets are more reflective of what the users do with tweets 

12
06:16:46,660 --> 06:16:50,360
for example people create tweets they respond to tweets 

13
06:16:50,360 --> 06:16:52,030
they mention other users and so forth 

14
06:16:53,300 --> 06:16:55,850
let look at these relationships in detail 

15
06:16:57,290 --> 06:17:02,600
in this graph all of the light blue notes are tweets and

16
06:17:02,600 --> 06:17:05,090
all of the purple notes are users 

17
06:17:05,090 --> 06:17:07,930
when a specific user creates a specific tweet 

18
06:17:07,930 --> 06:17:10,240
an entity is created in the graph as a result 

19
06:17:11,280 --> 06:17:13,580
we will come back to this graph in the next few slides 

20
06:17:14,580 --> 06:17:17,980
so what we want to do with it 

21
06:17:17,980 --> 06:17:19,820
well to be very specific 

22
06:17:19,820 --> 06:17:23,480
this graph was created from tweets of a bunch of online gamers 

23
06:17:24,700 --> 06:17:28,220
in this case we were monitoring the tweets of all people

24
06:17:28,220 --> 06:17:30,112
who played particular online video game 

25
06:17:31,205 --> 06:17:34,415
typically gamers are very excited about their video games 

26
06:17:34,415 --> 06:17:37,295
they are exited about their characters of the video games and they want to

27
06:17:37,295 --> 06:17:41,695
discuss the video games they are excited about when new versions are released 

28
06:17:43,325 --> 06:17:46,195
so what are the data science questions here 

29
06:17:46,195 --> 06:17:50,420
people who look at these things such as behavioral psychologists want to know 

30
06:17:50,420 --> 06:17:56,420
if you are a war game user or if you play any other violent game where

31
06:17:56,420 --> 06:18:01,830
there is a lot of fighting online do these people also show violent behavior 

32
06:18:01,830 --> 06:18:03,110
maybe they do maybe they do not 

33
06:18:04,290 --> 06:18:08,430
they also want to know whether as a result of looking at somebody tweets and

34
06:18:08,430 --> 06:18:13,610
following them over time they can tell if the person is addicted to the game or not 

35
06:18:13,610 --> 06:18:15,190
so why use graphs for that 

36
06:18:16,960 --> 06:18:19,850
one of the things you could do is what we showed here 

37
06:18:20,860 --> 06:18:25,210
from the graphs you can extract certain elements like conversations 

38
06:18:27,530 --> 06:18:32,320
if you look at it you will see all of them have somebody posting something 

39
06:18:32,320 --> 06:18:37,100
somebody responding to the post and the first person responding to the response 

40
06:18:37,100 --> 06:18:40,390
they are retweeting and they are responding and so forth 

41
06:18:40,390 --> 06:18:44,478
you would see a conversation chain going on which can be short or long 

42
06:18:44,478 --> 06:18:47,940
like you see and other people would join in 

43
06:18:47,940 --> 06:18:50,140
we do not know if it violent or not but

44
06:18:50,140 --> 06:18:52,909
at least there is a lively conversation going on about something 

45
06:18:54,730 --> 06:18:59,870
using graphs we could find some meaningful parts of a graph

46
06:18:59,870 --> 06:19:03,600
that we can further analyze using techniques like text analytics 

47
06:19:05,130 --> 06:19:07,870
another kind of thing people would like to identify 

48
06:19:07,870 --> 06:19:11,170
is which players are interacting with which other players 

49
06:19:11,170 --> 06:19:12,580
who are these people 

50
06:19:12,580 --> 06:19:14,550
do they form a close community 

51
06:19:14,550 --> 06:19:16,470
can anybody join in 

52
06:19:16,470 --> 06:19:19,060
are there groups what are the groups 

53
06:19:19,060 --> 06:19:21,870
if you find a group who are the most influential users 

54
06:19:23,710 --> 06:19:27,740
who are the people that everybody refers to listens to and so on 

55
06:19:27,740 --> 06:19:31,989
graph analytics is used to answer all these questions about a conversation

56
06:19:31,989 --> 06:19:35,467
that going on live in social media stream 

57
06:19:35,467 --> 06:19:37,681
let look at our next example 

1
12:35:38,160 --> 12:35:40,230
our second use case is about biology 

2
12:35:41,610 --> 12:35:43,790
interactions arise naturally in biology 

3
12:35:45,410 --> 12:35:50,120
genes produce proteins proteins regulate the functions of other proteins 

4
12:35:51,400 --> 12:35:53,970
cells transmit signals to other cells 

5
12:35:55,215 --> 12:35:58,610
these functional genes lead to pathological conditions that

6
12:35:58,610 --> 12:36:01,980
present themselves as observable phenotypes 

7
12:36:01,980 --> 12:36:05,520
some of these interactions that are observed and measured by experiment lists 

8
12:36:07,200 --> 12:36:10,770
another class of graph relations represent human knowledge 

9
12:36:10,770 --> 12:36:15,390
for example an edge may denote an anatomical association such as

10
12:36:15,390 --> 12:36:19,986
the nucleuses inside a cell part of the cerebral cortex is part of the brain 

11
12:36:19,986 --> 12:36:24,820
similarly one can encode different

12
12:36:24,820 --> 12:36:28,950
classifications of entities like both humans and dogs and mammals 

13
12:36:30,470 --> 12:36:35,980
researchers also find these relationships from literature

14
12:36:35,980 --> 12:36:39,890
or by computational techniques like bioinformatics algorithms 

15
12:36:40,900 --> 12:36:45,120
there are many bioinformatics algorithms that find statistical correlations between

16
12:36:45,120 --> 12:36:45,880
genes and proteins 

17
12:36:46,880 --> 12:36:52,070
many biological networks are created by creating edges between entities because

18
12:36:52,070 --> 12:36:56,140
they are strongly correlated based on measurements like gene expression level 

19
12:36:58,110 --> 12:37:02,280
yet a third category of computed relationships

20
12:37:02,280 --> 12:37:06,090
associates terms by mining scientific literature 

21
12:37:06,090 --> 12:37:10,320
if two entities are co - mentioned in scientific articles very often 

22
12:37:10,320 --> 12:37:14,100
then there is a likelihood that these two entities are related 

23
12:37:14,100 --> 12:37:18,910
in this case the edge between them does not depict a specific relationship but

24
12:37:18,910 --> 12:37:20,670
the fact they are associated 

25
12:37:22,150 --> 12:37:28,360
all these interactions can be assembled into networks of graphs where the nodes

26
12:37:28,360 --> 12:37:33,800
are biological entities and edges represent different categories of

27
12:37:33,800 --> 12:37:39,012
molecular interactions associations between diseases and 

28
12:37:39,012 --> 12:37:42,400
now there are two issues i would like you to understand 

29
12:37:42,400 --> 12:37:48,150
first graphs like the one we saw are assembled 

30
12:37:48,150 --> 12:37:54,160
that is integrated from many data sources produced by many independent science

31
12:37:54,160 --> 12:38:00,600
groups who have different research goals use different scientific techniques 

32
12:38:00,600 --> 12:38:05,120
but the underlying biological entities are common across many groups 

33
12:38:06,180 --> 12:38:10,690
it is the commonality that helps us stitch together these logic graphs 

34
12:38:10,690 --> 12:38:16,950
second as more results get linked and integrated the size of the networks grow 

35
12:38:16,950 --> 12:38:20,370
leading to big data problems and the need for big data technology 

36
12:38:21,700 --> 12:38:24,050
now let ask the same question 

37
12:38:24,050 --> 12:38:24,660
why graphs 

38
12:38:26,180 --> 12:38:31,060
in bio medicine people are always trying to discover new science for

39
12:38:31,060 --> 12:38:34,810
instance they want to discover unknown relationships 

40
12:38:34,810 --> 12:38:40,600
for example they can take two diseases very different diseases colorectal

41
12:38:40,600 --> 12:38:46,048
cancer and alzheimers disease and they might want to ask are they related 

42
12:38:46,048 --> 12:38:48,210
if so can you connect the dots and

43
12:38:48,210 --> 12:38:52,270
find out what the intervening network between them is 

44
12:38:52,270 --> 12:38:56,310
it turns out as you can see there are several genes that directly or

45
12:38:56,310 --> 12:38:58,080
indirectly connect these two diseases 

46
12:38:59,180 --> 12:39:03,726
thus we can use path finding techniques like this to discover previously

47
12:39:03,726 --> 12:39:05,939
unknown connections in a network 

48
12:39:07,280 --> 12:39:08,380
in addition 

49
12:39:08,380 --> 12:39:13,570
researchers sometimes think to explore the networks to help their discovery process 

50
12:39:13,570 --> 12:39:17,510
i have collaborated with a group that is using graph exploration techniques

51
12:39:17,510 --> 12:39:21,570
to figure out how phenotypes of undiagnosed diseases all fit together 

1
01:14:59,610 --> 01:15:03,600
our third use case is about the human information network 

2
01:15:03,600 --> 01:15:05,870
that means a personal information network 

3
01:15:06,950 --> 01:15:09,680
the graphic you see there is my linkedin network 

4
01:15:11,030 --> 01:15:13,060
everybody who is on my linkedin is a node 

5
01:15:14,070 --> 01:15:17,500
and if they know each other there an edge between them 

6
01:15:17,500 --> 01:15:20,880
you can possibly see some groups or clusters there 

7
01:15:20,880 --> 01:15:24,240
one analytical question we will explore in module two

8
01:15:24,240 --> 01:15:26,440
is how to discover these groups automatically 

9
01:15:28,120 --> 01:15:32,310
now of course with linkedin we only have professional information 

10
01:15:32,310 --> 01:15:35,130
but it is prudent to ask whether

11
01:15:35,130 --> 01:15:40,340
one can integrate the professional network with other forms of personal information 

12
01:15:40,340 --> 01:15:45,440
this would be other social media data like my friendship network from facebook or

13
01:15:45,440 --> 01:15:51,230
google or it could be information like my outlook email network 

14
01:15:51,230 --> 01:15:56,060
one can add even more information and put that interpersonal relationships 

15
01:15:56,060 --> 01:16:01,490
for example professor norman who is on my contact list is my director 

16
01:16:01,490 --> 01:16:03,860
and if we add my calendar to this 

17
01:16:03,860 --> 01:16:09,980
one can find events that i will be attending and people that i will be meeting 

18
01:16:09,980 --> 01:16:12,431
taking it even further for

19
01:16:12,431 --> 01:16:17,949
special applications you might actually include financial and

20
01:16:17,949 --> 01:16:22,956
business transactions or performance in activities 

21
01:16:22,956 --> 01:16:27,671
or fitness information or the location from my gps 

22
01:16:29,276 --> 01:16:34,360
now if you add all of these questions we should ask what we do with these things 

23
01:16:35,420 --> 01:16:35,920
let see 

24
01:16:37,920 --> 01:16:40,910
one use of this kind of information is matching 

25
01:16:42,180 --> 01:16:46,960
for example matching people with jobs is not just a matter of screening

26
01:16:46,960 --> 01:16:50,840
series based on job descriptions or evaluating their job experience 

27
01:16:51,880 --> 01:16:55,890
to recruit for high level positions like the board of directors of a company 

28
01:16:57,020 --> 01:17:01,210
you need to evaluate the network of the candidate to determine their relationships

29
01:17:01,210 --> 01:17:03,850
with important organizations groups and individuals 

30
01:17:05,020 --> 01:17:07,900
for some tasks like choosing a surgeon 

31
01:17:07,900 --> 01:17:10,230
you might also want inspect their social media ratings 

32
01:17:12,130 --> 01:17:14,830
in other classic applications we might want to look for

33
01:17:14,830 --> 01:17:17,630
people who can influence a human network 

34
01:17:17,630 --> 01:17:20,960
suppose we are in an election campaign team and

35
01:17:20,960 --> 01:17:25,620
need our message to get to as many people as possible in a city 

36
01:17:25,620 --> 01:17:30,800
we can not go door to door ourselves we need to go to some people and

37
01:17:30,800 --> 01:17:33,360
these people will reach others on our behalf 

38
01:17:34,360 --> 01:17:39,150
graph analytic techniques may help identify the fewest number of people

39
01:17:39,150 --> 01:17:43,610
who will have maximal reach into most of the potential voters in the city 

40
01:17:45,310 --> 01:17:48,250
a third category of application is threat detection 

41
01:17:50,200 --> 01:17:54,076
this network is created by groups who study news 

42
01:17:54,076 --> 01:17:57,390
for example they collect information about all

43
01:17:57,390 --> 01:18:01,670
the militant groups in different countries and all the reported acts of terrorism 

44
01:18:03,260 --> 01:18:05,460
this creates a network as you can see 

45
01:18:06,590 --> 01:18:08,266
we show only a part of the network 

46
01:18:08,266 --> 01:18:12,203
in this graph view the green notes are groups and

47
01:18:12,203 --> 01:18:15,406
the pink notes are deemed individuals 

48
01:18:16,730 --> 01:18:18,920
it should be clear that some of these individuals and

49
01:18:18,920 --> 01:18:22,050
groups are more closely associated than others 

50
01:18:23,100 --> 01:18:27,057
discovering and tracking these groups their current activities 

51
01:18:27,057 --> 01:18:31,685
together with an analysis of details of the events they are associated with might

52
01:18:31,685 --> 01:18:34,904
help analysts to get deeper insight into the networks and

53
01:18:34,904 --> 01:18:37,604
perhaps anticipate their future activities 

1
02:33:36,880 --> 02:33:40,350
our fourth and last use case has to do with smart cities 

2
02:33:41,380 --> 02:33:45,820
now a city is a geographically bounded space and

3
02:33:45,820 --> 02:33:50,630
contains many different networks operating within the same spatial domain 

4
02:33:50,630 --> 02:33:52,140
what kind of networks 

5
02:33:52,140 --> 02:33:52,640
let see 

6
02:33:53,820 --> 02:33:55,820
it has transportation networks 

7
02:33:57,320 --> 02:34:02,870
water and sewage networks power transmission networks 

8
02:34:02,870 --> 02:34:03,990
your ip broadband networks 

9
02:34:05,960 --> 02:34:08,670
some of these networks have multiple subtypes for

10
02:34:08,670 --> 02:34:13,520
example transportation networks include the bus networks the subway network 

11
02:34:13,520 --> 02:34:16,700
the surface street network and the railway network and so on 

12
02:34:17,760 --> 02:34:22,380
these networks form a physical infrastructure and therefore

13
02:34:22,380 --> 02:34:26,500
can be represented as graphs where each node has a geographic coordinate 

14
02:34:27,750 --> 02:34:32,400
but some of these networks can also be thought of as a commodity flow network 

15
02:34:33,550 --> 02:34:36,560
people flow through transportation networks 

16
02:34:36,560 --> 02:34:39,330
sewage material flows through a sewage network and so on 

17
02:34:41,090 --> 02:34:42,710
for many of these networks 

18
02:34:42,710 --> 02:34:47,100
a city planner would like to make sure that they cover the entire city 

19
02:34:47,100 --> 02:34:51,920
that commute time is optimized traffic congestions are well planned 

20
02:34:51,920 --> 02:34:56,380
to accomplish this they would need to create what is called a network model 

21
02:34:56,380 --> 02:35:01,210
for example urban planners develop city traffic model servicing 

22
02:35:01,210 --> 02:35:04,320
a traffic model will use both the geographic layout and

23
02:35:04,320 --> 02:35:07,080
connectivity of the network along with the flow parameters like

24
02:35:07,080 --> 02:35:09,610
the number of commuters getting on board at any station 

25
02:35:11,110 --> 02:35:15,170
well if we are planning to create a smart hub 

26
02:35:15,170 --> 02:35:18,900
we need to make sure that all the right things happen at the same place 

27
02:35:19,960 --> 02:35:23,700
people who come out of a metro station find nearby businesses 

28
02:35:23,700 --> 02:35:25,630
they find nearby facilities 

29
02:35:25,630 --> 02:35:28,060
those facilities should have broadband network for

30
02:35:28,060 --> 02:35:30,433
people who are going to go on their mobile phones 

31
02:35:30,433 --> 02:35:34,550
the same places need to have a water supply network and

32
02:35:34,550 --> 02:35:37,560
you have to plan it in such a way that all these networks

33
02:35:37,560 --> 02:35:40,750
who exist within a certain distance of each other 

34
02:35:40,750 --> 02:35:42,910
and all the facilities can be planned accordingly 

35
02:35:44,140 --> 02:35:46,120
beyond normal operations 

36
02:35:46,120 --> 02:35:50,010
we also need to model what would happen if the network gets disrupted 

37
02:35:50,010 --> 02:35:52,970
what are the kinds of congestion or traffic that might disturb the network 

38
02:35:53,970 --> 02:35:59,000
therefore these graphs are no longer just structures but they represent things like

39
02:35:59,000 --> 02:36:03,140
congestions things like people behavior and materials behavior over the network 

40
02:36:04,400 --> 02:36:07,030
one should also compute energy use patterns for

41
02:36:07,030 --> 02:36:11,240
the busy parts of the network in order to figure out how the network structure or

42
02:36:11,240 --> 02:36:15,370
the network flow can be altered to enable energy optimal operations 

43
02:36:16,790 --> 02:36:19,130
as we saw in these four examples 

44
02:36:19,130 --> 02:36:22,700
what graphs are used for are kind of different but

45
02:36:22,700 --> 02:36:27,380
they all show different view points from which you can use graphs for analysis 

46
02:36:28,430 --> 02:36:32,510
so this course focuses on graph analytics 

47
02:36:32,510 --> 02:36:35,220
i would like to briefly recap what the term means 

48
02:36:36,620 --> 02:36:41,900
analytics is the ability to discover meaningful patterns and

49
02:36:41,900 --> 02:36:47,280
interesting insights into data using mathematical properties of data 

50
02:36:47,280 --> 02:36:51,410
it covers a process of computing with mathematical properties and

51
02:36:51,410 --> 02:36:54,590
accessing the data itself efficiently 

52
02:36:54,590 --> 02:36:57,110
further it involves the ability to represent and

53
02:36:57,110 --> 02:37:01,060
work with domain knowledge as we saw in use case two with biology 

54
02:37:01,060 --> 02:37:05,350
finally analytics often involves statistical modeling techniques for

55
02:37:05,350 --> 02:37:08,310
drawing inferences and making predictions on data 

56
02:37:09,630 --> 02:37:14,195
with analytics we should be able to achieve the goals shown here 

57
02:37:30,099 --> 02:37:33,375
therefore graph analytics is a special piece of

58
02:37:33,375 --> 02:37:37,980
analytics where the underlying data can be modeled as a set of graphs 

1
05:11:13,460 --> 05:11:16,750
in the previous video we learned what a graph is 

2
05:11:17,770 --> 05:11:20,550
we gained insight into both the mathematician view and

3
05:11:20,550 --> 05:11:21,700
the computer scientist view 

4
05:11:22,890 --> 05:11:26,310
now we want to look at the big picture of graph analytics 

5
05:11:27,390 --> 05:11:30,210
why are we doing what we are doing and

6
05:11:30,210 --> 05:11:33,090
why do i need a graph representation on graph analytics 

7
05:11:34,760 --> 05:11:39,730
let start with a graph network which you may already be familiar with facebook 

8
05:11:40,780 --> 05:11:42,770
here is my facebook page 

9
05:11:42,770 --> 05:11:45,820
let look at it and consider some of the elements it contains 

10
05:11:48,470 --> 05:11:50,340
there is a primary user and that me 

11
05:11:52,100 --> 05:11:52,860
there are my friends 

12
05:11:54,490 --> 05:11:59,310
and typically there are some posts some of which may have media in them 

13
05:11:59,310 --> 05:12:00,800
such as video 

14
05:12:00,800 --> 05:12:02,380
what else does it have 

15
05:12:02,380 --> 05:12:05,619
well let consider what is inside a post 

16
05:12:05,619 --> 05:12:08,281
the post contains text 

17
05:12:08,281 --> 05:12:10,434
it has tags 

18
05:12:10,434 --> 05:12:14,700
i tag some people some people comment on my post 

19
05:12:15,770 --> 05:12:17,240
if somebody commenting 

20
05:12:17,240 --> 05:12:22,330
then that means there must be a commenter who is also a user in the facebook 

21
05:12:23,640 --> 05:12:27,340
other people like my post or

22
05:12:27,340 --> 05:12:32,550
they like a comment and they also respond to some of the comments 

23
05:12:33,820 --> 05:12:36,940
many posts have locations associated with them as well 

24
05:12:38,780 --> 05:12:41,380
when we consider all of these together do you see the graph 

25
05:12:41,380 --> 05:12:42,910
well if not we will show you 

26
05:12:44,300 --> 05:12:48,300
here we see very much the same types of information but now they are in a graph 

27
05:12:49,320 --> 05:12:52,300
first notice me on the far left 

28
05:12:52,300 --> 05:12:56,878
if you look carefully at this graph you will also notice there are relations labeled

29
05:12:56,878 --> 05:13:01,830
friend - of and created post and parts of the post and everything that you saw

30
05:13:01,830 --> 05:13:07,400
before that are now organized in terms of objects and relationship speaking objects 

31
05:13:08,580 --> 05:13:13,250
the objects here on the post comments on the post replies to the comments and so

32
05:13:13,250 --> 05:13:17,880
forth and the relationships also include tagged in or refers to 

33
05:13:17,880 --> 05:13:20,330
so this is sort of the big picture right 

34
05:13:22,590 --> 05:13:27,380
next we will look at 4 specific use cases in four different disciplines 

35
05:13:28,480 --> 05:13:30,110
the first one will be on social media 

36
05:13:32,010 --> 05:13:36,020
the second one will be on biology where we will look at genes and diseases 

37
05:13:37,700 --> 05:13:42,440
the third one will be human information network which means personal information 

38
05:13:43,840 --> 05:13:45,750
and the fourth one will be on smart cities 

39
05:13:46,950 --> 05:13:48,790
let go through these examples one by one 

1
10:25:01,020 --> 10:25:03,990
we will go over the logic of dijkstra algorithm without writing code 

2
10:25:05,460 --> 10:25:06,940
if you are an advanced student and

3
10:25:06,940 --> 10:25:09,350
know the algorithm you can skip to the next lecture 

4
10:25:11,010 --> 10:25:14,840
the basic plan is to start from node i and

5
10:25:14,840 --> 10:25:17,210
progressively traverse a sequence of notes 

6
10:25:18,580 --> 10:25:22,450
when the method attempts to choose the next node to traverse to 

7
10:25:22,450 --> 10:25:27,850
it chooses a node for which the total rate of the path to that node is the lowest 

8
10:25:29,120 --> 10:25:32,130
in the beginning the algorithm is on the start node i 

9
10:25:33,530 --> 10:25:36,311
the distance from i to i is 0 

10
10:25:36,311 --> 10:25:38,677
and the distance to all other nodes is infinity 

11
10:25:38,677 --> 10:25:40,820
because the system does not know them yet 

12
10:25:42,560 --> 10:25:45,921
a second table called a priority queue 

13
10:25:45,921 --> 10:25:51,255
is currently exactly the same as the distance row of the first array 

14
10:25:52,330 --> 10:25:55,350
the system starts with processing node i the source node 

15
10:25:56,350 --> 10:26:01,050
that means it finds the nodes that can reach from i f and j 

16
10:26:02,270 --> 10:26:07,121
note that the respective total weights that is 5 for f 

17
10:26:07,121 --> 10:26:11,882
and 15 for j to get to these nodes in the distance row 

18
10:26:11,882 --> 10:26:13,520
then it marks i as done 

19
10:26:14,710 --> 10:26:18,330
we have made the node i agree because the node is processed 

20
10:26:20,080 --> 10:26:26,640
next it looks at row d to find the least distance which is 5 

21
10:26:28,560 --> 10:26:30,470
the corresponding vertex is f 

22
10:26:31,870 --> 10:26:36,170
so next the method traverses to f 

23
10:26:36,170 --> 10:26:41,086
now the algorithm on node f and has determined that out of its possible

24
10:26:41,086 --> 10:26:46,040
destinations e g and j j is the least expensive 

25
10:26:46,040 --> 10:26:52,081
the total path that is the weight of the path to j is 10 

26
10:26:52,081 --> 10:26:56,320
5 from i to f plus 5 from f to j 

27
10:26:56,320 --> 10:27:00,440
this diagram shows that the priority is now shorter because it has

28
10:27:00,440 --> 10:27:03,510
popped out the already processed load i 

29
10:27:03,510 --> 10:27:08,260
at step three we are processing node j but face the following situation 

30
10:27:09,450 --> 10:27:12,454
we can go back to f from j but

31
10:27:12,454 --> 10:27:17,227
that will cost 10 plus 15 that is 25 

32
10:27:17,227 --> 10:27:22,470
25 is worse than the cost of the current path to f which is 5 directly from i 

33
10:27:23,680 --> 10:27:28,220
thus we do not go from f to j and do not update the distance shown 

34
10:27:28,220 --> 10:27:34,775
the other option is to go from j to g which incurs a cost of 10 plus 5 15 

35
10:27:34,775 --> 10:27:39,559
hm this does not improve the current cost to reach g through f 

36
10:27:39,559 --> 10:27:42,710
which is now at 15 already 

37
10:27:42,710 --> 10:27:46,720
therefore we do not update the distance for g 

38
10:27:46,720 --> 10:27:50,880
so at this point we see that while j is processed 

39
10:27:50,880 --> 10:27:54,230
it had no impact on the traversal process 

40
10:27:54,230 --> 10:27:58,916
we consider the distance row again and find that the next node to expand is g 

41
10:27:58,916 --> 10:28:00,630
which is reached through f 

42
10:28:01,630 --> 10:28:04,531
continuing as before g processed 

43
10:28:04,531 --> 10:28:09,191
it opens up the possibility of diverging to c at a cost of 35 

44
10:28:09,191 --> 10:28:12,430
or to d at the cost of 25 

45
10:28:12,430 --> 10:28:17,254
but wait we have an issue there are two competing nodes 

46
10:28:17,254 --> 10:28:23,760
e coming form f or d coming from g that are both expansion candidates 

47
10:28:23,760 --> 10:28:24,705
at this point 

48
10:28:24,705 --> 10:28:29,516
the algorithm can make a random choice because there is no other information 

49
10:28:29,516 --> 10:28:32,640
let say we make an arbitrary choice we expand to e next 

50
10:28:33,650 --> 10:28:37,240
in an optional video we will see

51
10:28:37,240 --> 10:28:42,140
how we can use the additional information to make a more informed decision 

52
10:28:42,140 --> 10:28:43,930
after expanding to e 

53
10:28:43,930 --> 10:28:47,700
we can find that we have already reached the node b so we are done 

54
10:28:49,000 --> 10:28:54,080
the other choice that is to go to d from g costs less than the path through b but

55
10:28:54,080 --> 10:28:56,690
it does not matter now because the destination is reached 

56
10:28:57,960 --> 10:29:03,390
just for the sake of completeness if we did let the algorithm continue to operate 

57
10:29:03,390 --> 10:29:06,440
it will terminate when all reachable nodes are reached 

58
10:29:07,480 --> 10:29:11,310
we say all reachable nodes become some nodes like h 

59
10:29:12,850 --> 10:29:15,220
are not reachable because it has no incoming edge 

60
10:29:16,220 --> 10:29:20,060
such a node as we said before is called the root node of the graph 

61
10:29:20,060 --> 10:29:23,400
in general a graph can have more than one root node 

62
10:29:25,040 --> 10:29:29,824
now that we have reached our destination we need to construct the shortest path 

63
10:29:29,824 --> 10:29:35,630
we start by taking the destination b and find its predecessor e 

64
10:29:37,350 --> 10:29:43,069
then we find the node e and check its predecessor which is f 

65
10:29:43,069 --> 10:29:46,347
finally we find the predecessor of f to obtain i 

66
10:29:46,347 --> 10:29:48,645
which is a source node for the task 

67
10:29:49,880 --> 10:29:52,610
so these nodes can then be stretched together in reverse 

68
10:29:52,610 --> 10:29:57,570
thus building i to f to e to b which is highlighted in the film 

69
10:29:58,590 --> 10:30:03,380
so how well does this algorithm work for big graphs 

70
10:30:03,380 --> 10:30:06,020
actually not very well 

71
10:30:06,020 --> 10:30:08,940
we often measure the performance of an algorithm

72
10:30:08,940 --> 10:30:10,710
in terms of the size of the data 

1
20:55:10,350 --> 20:55:15,040
so we saw that the dijkstra algorithm has a very high worst case complexity 

2
20:55:16,370 --> 20:55:19,770
despite the high complexity of the algorithm there are several

3
20:55:19,770 --> 20:55:23,590
practical improvements that will enhance the performance of the method 

4
20:55:23,590 --> 20:55:27,310
one of them is bi - directional dijkstra algorithm 

5
20:55:27,310 --> 20:55:31,340
the idea is very simple we will go forward from the source now and

6
20:55:31,340 --> 20:55:36,960
backward from the target node and stop when the two expanding frontiers meet 

7
20:55:36,960 --> 20:55:39,840
we will briefly illustrate the process without going deep into

8
20:55:39,840 --> 20:55:41,690
the details of every step 

9
20:55:41,690 --> 20:55:44,890
so the technique starts just like the regular method 

10
20:55:44,890 --> 20:55:48,810
the control moves from i to f at a cost of 5 

11
20:55:48,810 --> 20:55:50,500
but then it switches and

12
20:55:50,500 --> 20:55:55,240
starts from the target and moves backward along the edges 

13
20:55:55,240 --> 20:55:57,950
so from a it comes to d or c 

14
20:55:58,960 --> 20:56:03,460
we will chose d because an ad is a least weight part 

15
20:56:03,460 --> 20:56:09,730
now the forward step is performed again and we traverse from f to g 

16
20:56:09,730 --> 20:56:13,060
we are skipping the expansion to j because as we saw before 

17
20:56:13,060 --> 20:56:15,040
it does not contribute to the path 

18
20:56:15,040 --> 20:56:18,510
so the total rate of the ifg path is 15 

19
20:56:18,510 --> 20:56:23,420
the backward process then reaches g through d 

20
20:56:23,420 --> 20:56:26,000
the cost of the ifg is 15 

21
20:56:26,000 --> 20:56:28,930
and the cost of adg is also 15 

22
20:56:28,930 --> 20:56:32,650
we stop because a common node is reached 

23
20:56:32,650 --> 20:56:35,570
we need to ensure that the weight of the forward path and

24
20:56:35,570 --> 20:56:40,810
that of the reverse path are added and the total combined rate is minimal 

25
20:56:40,810 --> 20:56:44,701
at this point we can concatenate the partial paths

26
20:56:44,701 --> 20:56:48,777
to construct the full shortest path which is ifgda 

27
20:56:48,777 --> 20:56:53,773
now one point to remember is that the length of the smallest weight path

28
20:56:53,773 --> 20:56:56,744
can be longer than the shortest hop path 

29
20:56:56,744 --> 20:57:02,444
here the fca path has 2 hops but a weight of 20 

30
20:57:02,444 --> 20:57:07,366
but the weight of the 3 hop path fgda is 15 

31
20:57:07,366 --> 20:57:09,060
so just remember that 

1
17:52:19,580 --> 17:52:22,750
the final type of analytics we will discuss in this module

2
17:52:22,750 --> 17:52:24,425
is called centrality analysis 

3
17:52:25,850 --> 17:52:31,010
we call something central if it in the middle of a larger entity 

4
17:52:31,010 --> 17:52:35,370
we also call something central if it important and vital 

5
17:52:36,680 --> 17:52:42,770
for graphs we will identify important notes by looking at how central they are 

6
17:52:42,770 --> 17:52:43,270
in the graph 

7
17:52:46,280 --> 17:52:50,673
look at the the six node graph you do not have to know a lot

8
17:52:50,673 --> 17:52:55,345
to figure out that the orange node is pretty important why 

9
17:52:55,345 --> 17:52:58,290
well one reason could be it the most vulnerable node 

10
17:52:58,290 --> 17:53:00,240
if you remove it the graph will fall apart 

11
17:53:00,240 --> 17:53:03,210
because this is clearly one way to look at the centrality of the node 

12
17:53:04,630 --> 17:53:05,650
but it not the only way 

13
17:53:07,270 --> 17:53:09,440
another way of looking at the orange node 

14
17:53:09,440 --> 17:53:12,880
is that it can reach all other nodes quicker than any other node 

15
17:53:14,390 --> 17:53:17,570
so this is the idea behind influences 

16
17:53:17,570 --> 17:53:21,480
people in a social network who are connected enough to reach out and

17
17:53:21,480 --> 17:53:25,810
possibly influence a lot more people than others will be able to 

18
17:53:27,340 --> 17:53:29,820
if the little graph represents a transportation network 

19
17:53:30,820 --> 17:53:34,890
and you were asked to build a restaurant somewhere around this network 

20
17:53:34,890 --> 17:53:37,780
you will possibly choose an area near the central node 

21
17:53:37,780 --> 17:53:42,250
because more traffic will flow through this node than other nodes 

22
17:53:42,250 --> 17:53:44,830
and some of them will get out at the station and

23
17:53:44,830 --> 17:53:46,010
bring business to your restaurant 

24
17:53:47,250 --> 17:53:49,310
we can give many more examples 

25
17:53:49,310 --> 17:53:53,810
in biological networks house - keeping genes are genes needed for

26
17:53:53,810 --> 17:53:57,050
the maintenance of basic cellular function and

27
17:53:57,050 --> 17:54:01,060
are expressed in all cells of an organism under normal and

28
17:54:01,060 --> 17:54:05,780
abnormal conditions these genes are central because they are vital and

29
17:54:05,780 --> 17:54:08,300
they are connected to many other nodes in the biological network 

30
17:54:09,440 --> 17:54:13,330
so much so that they need to be taken out of the network so

31
17:54:13,330 --> 17:54:15,930
that the rest of the network can be studied 

32
17:54:15,930 --> 17:54:20,390
out of all these examples we will focus on a kind of problem

33
17:54:20,390 --> 17:54:23,480
researchers have called the key player problem 

34
17:54:24,740 --> 17:54:26,380
the problem comes in two flavors 

35
17:54:27,460 --> 17:54:30,177
now take a little time to read the two examples 

36
17:54:42,255 --> 17:54:44,970
the first is sort of a negative piece 

37
17:54:46,250 --> 17:54:50,660
we have a network and we are trying to find a small subset of people

38
17:54:50,660 --> 17:54:55,250
who removal will maximally disrupt the network 

39
17:54:56,260 --> 17:54:59,610
the key operative word here is maximally 

40
17:54:59,610 --> 17:55:05,360
so if there is a node which removal breaks the network into two parts 

41
17:55:05,360 --> 17:55:09,300
it is not what we want if there another two nodes

42
17:55:09,300 --> 17:55:12,440
whose removal will break the network into ten parts 

43
17:55:12,440 --> 17:55:17,270
the second case however is a lot more conventional 

44
17:55:17,270 --> 17:55:22,000
the goal is to find a small set of nodes with maximal combined reachability

45
17:55:22,000 --> 17:55:23,250
to other nodes 

46
17:55:23,250 --> 17:55:28,000
that means taken together these nodes should reach almost all other nodes 

47
17:55:30,010 --> 17:55:33,180
you should know that people use two terms to

48
17:55:33,180 --> 17:55:35,940
characterize a general concept of network centrality 

49
17:55:37,340 --> 17:55:39,920
the first one centrality is about a node 

50
17:55:41,290 --> 17:55:45,450
it measures how central the node is with respect to the network 

51
17:55:46,450 --> 17:55:50,070
so the orange node in the left graph has high centrality and

52
17:55:50,070 --> 17:55:51,550
the blue nodes have low centrality 

53
17:55:53,140 --> 17:55:58,670
the second term centralization is about the network 

54
17:55:58,670 --> 17:56:00,620
now look at the graph on the right 

55
17:56:00,620 --> 17:56:05,830
where the dark orange node is still very central for the light orange node and

56
17:56:05,830 --> 17:56:09,770
between the other nodes and therefore has reasonable centrality 

57
17:56:11,050 --> 17:56:14,170
as more nodes start having higher centrality 

58
17:56:14,170 --> 17:56:17,090
the centralization of the network drops

59
17:56:17,090 --> 17:56:21,330
because there is less variation in the centrality values of the network 

60
17:56:22,940 --> 17:56:28,332
so for each type of centrality that we discuss next we can compute the network

61
17:56:28,332 --> 17:56:33,812
centralization by considering the sum of the difference between the maximum

62
17:56:33,812 --> 17:56:39,393
centrality and the centrality of the node divided by the maximum centrality 

63
17:56:39,393 --> 17:56:42,310
there are thirty different measures of centrality 

64
17:56:43,640 --> 17:56:46,190
we will consider only a few of them and

65
17:56:46,190 --> 17:56:49,645
explore the conceptual principles supported here 

66
17:56:49,645 --> 17:56:53,080
the first and the most intuitive measure is degree centrality 

67
17:56:53,080 --> 17:56:58,180
quick measures the degree of a node divided by the possible edges it

68
17:56:58,180 --> 17:57:03,100
could have if it connected to each of the other n - 1 nodes in the graph 

69
17:57:04,110 --> 17:57:04,960
now we have seen this before 

70
17:57:06,010 --> 17:57:09,850
this measure gives us a sense of the hubness of the node 

71
17:57:09,850 --> 17:57:14,630
the higher the number is the more hub - like the node is 

72
17:57:14,630 --> 17:57:17,640
thinking of our second key player problem 

73
17:57:17,640 --> 17:57:21,880
one way to approach this is to find a hub - like measure for

74
17:57:21,880 --> 17:57:24,660
a group with multiple nodes 

75
17:57:24,660 --> 17:57:27,690
instead of measuring the degree centrality of individual nodes 

76
17:57:29,310 --> 17:57:34,520
the measure simply counts the number of edges coming into the group as a whole

77
17:57:34,520 --> 17:57:36,430
compared to the number of outsiders 

78
17:57:37,620 --> 17:57:44,460
in our example two red nodes here together connect to all blue nodes 

79
17:57:45,830 --> 17:57:50,230
although there is just one node neighboring both of them 

80
17:57:50,230 --> 17:57:53,950
closeness centrality takes a different approach to the centrality problem 

81
17:57:55,670 --> 17:58:00,222
it acts the shortest distance of a node to all other nodes and

82
17:58:00,222 --> 17:58:02,232
divides it by a minus one 

83
17:58:02,232 --> 17:58:07,575
so in our graph a node like i which is on the periphery of the graph 

84
17:58:07,575 --> 17:58:11,199
will be quite far from all nodes in general and

85
17:58:11,199 --> 17:58:15,930
therefore will have a very high closeness centrality value 

86
17:58:15,930 --> 17:58:22,070
on the other hand nodes like f c and h are much closer to all other nodes 

87
17:58:23,340 --> 17:58:26,580
know that we define this measure in terms of shortest paths 

88
17:58:28,810 --> 17:58:34,060
so if a moving object like information flows to the shortest path

89
17:58:34,060 --> 17:58:39,810
in the network f is more likely to receive them earlier than other nodes 

90
17:58:39,810 --> 17:58:42,710
and therefore can process into other nodes more quickly 

91
17:58:44,190 --> 17:58:45,590
therefore 

92
17:58:45,590 --> 17:58:50,300
if we look to inject a new piece of information into the network with the idea

93
17:58:50,300 --> 17:58:55,486
that it should read every other node quickly i should possibly inject it at f 

94
17:58:55,486 --> 17:59:02,940
recall however that node information flow is two shortest routes 

95
17:59:02,940 --> 17:59:08,370
an example is gossip that tends to travel through centrality nodes 

96
17:59:08,370 --> 17:59:12,430
closeness centrality does not work well for these types of information nodes 

97
17:59:13,490 --> 17:59:17,410
another very popular centrality measure is called betweenness centrality 

98
17:59:18,640 --> 17:59:25,430
for any node it measures the fraction of shortest paths flowing through that node 

99
17:59:25,430 --> 17:59:27,730
compared to the number of shortest paths in the graph 

100
17:59:29,590 --> 17:59:35,065
since b is at one end its between a centrality is 0 

101
17:59:35,065 --> 17:59:36,935
but let look at a 

102
17:59:36,935 --> 17:59:39,010
a is in the path from b to e 

103
17:59:40,320 --> 17:59:40,880
so is d 

104
17:59:42,060 --> 17:59:46,440
therefore a score for the b to e path is 0 5 

105
17:59:46,440 --> 17:59:54,280
similarly its score for the c to e path is also 0 5 making the total score 1 

106
17:59:54,280 --> 17:59:59,623
e is in the path from a to d for there is one more

107
17:59:59,623 --> 18:00:05,651
path from a to do through c so e score is 0 5 

108
18:00:05,651 --> 18:00:08,840
now can you verify why c score is 3 5 

109
18:00:08,840 --> 18:00:12,275
betweenness centrality is typically used for

110
18:00:12,275 --> 18:00:16,440
problem where a commodity is flowing through the network 

111
18:00:16,440 --> 18:00:18,840
as in the case of closeness centrality 

112
18:00:20,080 --> 18:00:24,573
any quantity that does not flow in shortest path channels like infection or

113
18:00:24,573 --> 18:00:28,872
rumor on the internet does not work well with betweeness centrality 

1
11:52:48,070 --> 11:52:51,570
this lesson on graph analytics is about identifying and

2
11:52:51,570 --> 11:52:54,290
tracking groups of interacting entities in a network 

3
11:52:55,900 --> 11:52:57,740
we call these groups communities 

4
11:52:59,550 --> 11:53:03,060
let try to provide a more concrete definition of communities in a network 

5
11:53:04,410 --> 11:53:06,800
we multiply the definition by this diagram 

6
11:53:06,800 --> 11:53:09,820
showing a research study on the santa fe institute 

7
11:53:09,820 --> 11:53:14,680
a theoretical research institute located in santa fe new mexico us 

8
11:53:14,680 --> 11:53:18,850
they performed multidisciplinary studies on fundamental principles

9
11:53:18,850 --> 11:53:21,220
of complex adaptive systems 

10
11:53:21,220 --> 11:53:24,110
the nodes in the graph are researchers 

11
11:53:24,110 --> 11:53:26,640
and an edge exists between two researchers 

12
11:53:26,640 --> 11:53:27,830
if they collaborate with each other 

13
11:53:29,070 --> 11:53:34,020
as you can see there are distinct groups among researchers 

14
11:53:34,020 --> 11:53:36,300
a mathematically college researcher 

15
11:53:36,300 --> 11:53:40,090
does not collaborate with researchers who work on structure of the rna 

16
11:53:41,370 --> 11:53:45,450
so the graph is essentially a set of separate groups 

17
11:53:45,450 --> 11:53:49,570
thinly connected through a handful of cross - disciplinary researchers 

18
11:53:49,570 --> 11:53:51,000
who collaborate across groups 

19
11:53:52,290 --> 11:53:55,320
these groups are then the communities in this collaboration graph 

20
11:53:56,590 --> 11:53:57,280
what does this tell us 

21
11:53:58,650 --> 11:54:03,260
it tells us that communities are highly interacting clusters in a graph 

22
11:54:03,260 --> 11:54:07,690
that is they form pockets of denser subgraphs that are more

23
11:54:07,690 --> 11:54:11,400
connected to each other than to members of the other clusters 

24
11:54:12,540 --> 11:54:17,020
communities of humans or otherwise are interesting things to study 

25
11:54:17,020 --> 11:54:20,740
because it gives us an insight into the interaction patterns 

26
11:54:20,740 --> 11:54:22,280
and how they change with time 

27
11:54:23,340 --> 11:54:26,280
here are some analytics questions about communities 

28
11:54:27,310 --> 11:54:29,740
we have divided them into three categories 

29
11:54:31,080 --> 11:54:33,990
analytics questions that do not depend on time are called static 

30
11:54:35,030 --> 11:54:39,290
here we ask questions about the composition of the community 

31
11:54:39,290 --> 11:54:41,970
how tight - knit members are connected and so forth 

32
11:54:43,030 --> 11:54:49,470
in the second category we involve the formation and evolution of the community 

33
11:54:50,740 --> 11:54:53,720
communities can form temporal for example 

34
11:54:53,720 --> 11:54:56,720
around an event like a school shooting 

35
11:54:56,720 --> 11:54:59,620
or some communities despite the comings and

36
11:54:59,620 --> 11:55:03,750
goings of members sustain themselves well 

37
11:55:03,750 --> 11:55:09,040
a facebook group a political party fans of a music band 

38
11:55:09,040 --> 11:55:12,515
are likely to continue over time and hence are non - transient 

39
11:55:14,070 --> 11:55:17,350
one can also be interested in the history of formation of a community 

40
11:55:17,350 --> 11:55:18,100
like a criminal network 

41
11:55:19,230 --> 11:55:21,410
the third category is about predictions 

42
11:55:22,780 --> 11:55:26,740
analysts would like to predict how a community would grow 

43
11:55:26,740 --> 11:55:28,828
whether it composition of members might change 

44
11:55:28,828 --> 11:55:33,300
or whether there are emerging power shifts within the community 

45
11:55:34,920 --> 11:55:38,490
now before we ask these questions however 

46
11:55:38,490 --> 11:55:41,620
we need to first identify communities in a large network 

47
11:55:42,900 --> 11:55:47,790
to find communities we need to formalize the idea that there are more

48
11:55:47,790 --> 11:55:52,520
connections within the community and fewer connections between two communities 

49
11:55:53,770 --> 11:55:58,180
one way to achieve this is to think of dividing the degree of a node 

50
11:55:58,180 --> 11:56:02,430
into an internal and an external component 

51
11:56:02,430 --> 11:56:07,310
the internal component is the count of edges within community 

52
11:56:07,310 --> 11:56:10,680
and the external degree is the count of edges outside the community 

53
11:56:12,480 --> 11:56:13,110
an example 

54
11:56:13,110 --> 11:56:17,450
will be to consider that my community is where i live that is san diego 

55
11:56:17,450 --> 11:56:22,280
and then count the number of my friends within san diego versus outside san diego 

56
11:56:23,660 --> 11:56:28,560
in the figure the highlighted node has four internal connections and

57
11:56:28,560 --> 11:56:29,640
one external connection 

58
11:56:31,730 --> 11:56:35,310
the next step would be to think of the internal degree and

59
11:56:35,310 --> 11:56:37,660
the external degree of an entire cluster 

60
11:56:38,890 --> 11:56:43,080
we can sum up the internal degrees of all nodes in a cluster 

61
11:56:43,080 --> 11:56:46,480
and call it the internal degree of the whole cluster 

62
11:56:46,480 --> 11:56:49,089
and similarly sum the external degrees of the nodes 

63
11:56:49,089 --> 11:56:51,320
to compute the external degree of the cluster 

64
11:56:52,460 --> 11:56:55,360
now we can define intra - cluster

65
11:56:55,360 --> 11:56:59,810
density to be the ratio of the number of internal edges of the cluster 

66
11:56:59,810 --> 11:57:03,250
divided by the number of possible connections inside the box 

67
11:57:04,670 --> 11:57:07,350
the denominator is n cues 2 

68
11:57:07,350 --> 11:57:11,190
which is the number of pairwise combination of nodes within the cluster 

69
11:57:11,190 --> 11:57:12,490
we call this delta int 

70
11:57:13,780 --> 11:57:17,750
similarly inter - cluster density delta x is the number

71
11:57:17,750 --> 11:57:22,040
of inter - cluster edges divided by the possible pairings between nc 

72
11:57:23,120 --> 11:57:27,070
a node in the cluster to n - nc the number of nodes outside the cluster 

73
11:57:28,970 --> 11:57:34,500
there are two kinds of methods used for finding communities in the network 

74
11:57:34,500 --> 11:57:39,320
one of them focuses on local properties that is 

75
11:57:39,320 --> 11:57:43,560
properties for which one only looks at a node and its neighbor 

76
11:57:44,720 --> 11:57:48,110
for the most ideal community in a network is a subgraph 

77
11:57:48,110 --> 11:57:51,099
where every node is connected to every other node in the subgraph 

78
11:57:52,230 --> 11:57:53,840
such a structure is called a clique 

79
11:57:55,570 --> 11:57:58,740
to find a perfect community structure as a clique 

80
11:57:58,740 --> 11:58:03,560
one can try to find the largest clique within a graph return cell 

81
11:58:04,580 --> 11:58:06,670
that is a computationally challenging problem 

82
11:58:08,260 --> 11:58:12,780
it much simpler to find cliques if we know the value of k 

83
11:58:13,780 --> 11:58:17,312
that means the number of members in the clique 

84
11:58:17,312 --> 11:58:20,000
we are going to show a simple version of this in model three 

85
11:58:21,710 --> 11:58:25,230
the more general problem has been solved by complex algorithms 

86
11:58:25,230 --> 11:58:26,990
that are beyond the scope of this course 

87
11:58:28,950 --> 11:58:34,620
in the real world perfect cliques larger than three or four are harder to find 

88
11:58:34,620 --> 11:58:36,450
so we need to relax the definition of it 

89
11:58:37,750 --> 11:58:40,680
now there are two types of relaxations 

90
11:58:40,680 --> 11:58:44,250
those based on distance and those based on density 

91
11:58:45,950 --> 11:58:51,970
two distance based definitions are n - clique and n - plan 

92
11:58:51,970 --> 11:58:55,660
we will illustrate this over a friendship graph shown here 

93
11:58:55,660 --> 11:58:59,050
n - clique is a subgraph 

94
11:58:59,050 --> 11:59:03,990
such that the distance between each node pair in that subgraph is n or less 

95
11:59:03,990 --> 11:59:09,790
by this definition holly paul and gary form a two clique 

96
11:59:11,100 --> 11:59:14,090
that a little awkward is not it 

97
11:59:14,090 --> 11:59:15,120
yeah 

98
11:59:15,120 --> 11:59:17,550
they are within two distance of each other 

99
11:59:17,550 --> 11:59:20,600
but the two clique does not include intermediate nodes that

100
11:59:20,600 --> 11:59:21,480
connect the member nodes 

101
11:59:22,760 --> 11:59:26,690
so mike is not in the two clique 

102
11:59:27,890 --> 11:59:31,570
this situation is corrected in n - clan 

103
11:59:31,570 --> 11:59:36,280
where to belong to the n - clan the shortest part between any members 

104
11:59:36,280 --> 11:59:38,438
without involving outsiders is n or less 

105
11:59:38,438 --> 11:59:43,250
now holly mike 

106
11:59:43,250 --> 11:59:48,030
bill don harry and gary form a two - clan 

107
11:59:48,030 --> 11:59:52,660
clearly this group is more cohesive than the two - clique we saw before 

108
11:59:54,760 --> 11:59:58,490
n - clique and n - clan are distance based measures for

109
11:59:58,490 --> 12:00:00,455
finding cohesive groups of communities 

110
12:00:00,455 --> 12:00:06,270
k - core is a density based method for finding communities 

111
12:00:06,270 --> 12:00:09,117
let look at the dark orange subgraph 

112
12:00:09,117 --> 12:00:14,410
every node is connected to at least three other nodes within the subgraph 

113
12:00:14,410 --> 12:00:15,867
they form a 3 - core 

114
12:00:15,867 --> 12:00:20,110
let include the medium light orange nodes in the subgraph now 

115
12:00:21,720 --> 12:00:26,380
each node is connected to at least two other members within the subgraph 

116
12:00:26,380 --> 12:00:27,495
they form a 2 - core 

117
12:00:29,400 --> 12:00:36,894
relaxing further we can add the light orange nodes and the graph as a 1 - core 

1
23:53:23,910 --> 23:53:26,480
we have already defined the degree of the node

2
23:53:26,480 --> 23:53:28,965
as the number of edges connected to it 

3
23:53:28,965 --> 23:53:31,815
thus specifying if a node is more connected than another 

4
23:53:33,210 --> 23:53:36,760
looking closer we can separate out

5
23:53:36,760 --> 23:53:41,450
that two components denotes degrees into in degree and out 

6
23:53:41,450 --> 23:53:46,150
which are the counts of the incident and the outgoing edges of a node respectively 

7
23:53:47,570 --> 23:53:50,970
in the example graph g has an indegree and

8
23:53:50,970 --> 23:53:56,860
outdegree of three making the total degree equal to six 

9
23:53:56,860 --> 23:54:01,680
we first construct this degree table for each node 

10
23:54:01,680 --> 23:54:05,460
it a simple procedure where we count the number of nodes with degree 

11
23:54:05,460 --> 23:54:06,960
0 1 2 and so forth 

12
23:54:06,960 --> 23:54:14,100
the degree versus count table is a degree histogram of the graph 

13
23:54:14,100 --> 23:54:19,380
we can compare two graphs by computing the vector distance between them 

14
23:54:19,380 --> 23:54:22,625
one simplistic measure is just the euclidean distance 

15
23:54:22,625 --> 23:54:27,331
for our case the degree histogram based on comparisons of

16
23:54:27,331 --> 23:54:31,390
the histograms find the graphs to be very similar 

17
23:54:31,390 --> 23:54:35,036
the more sophisticated methods are available but

18
23:54:35,036 --> 23:54:37,822
are outside the scope of this course 

19
23:54:49,919 --> 23:54:53,280
we can also compute histograms of just the in degree or

20
23:54:53,280 --> 23:54:55,380
just the out degree of the graph 

21
23:54:56,495 --> 23:55:02,210
but perhaps more interesting is the joint two dimensional histogram of the graph 

22
23:55:02,210 --> 23:55:06,970
the colorful histogram of the graph can be interpreted here as follows 

23
23:55:06,970 --> 23:55:12,330
the graph has a maximum in degree of three and a maximum out degree of three 

24
23:55:12,330 --> 23:55:15,644
this creates a two - dimensional histogram with four times four

25
23:55:15,644 --> 23:55:17,740
equal to 16 different joined values 

26
23:55:19,020 --> 23:55:22,700
the actual value for any combination is computed from the graph and

27
23:55:22,700 --> 23:55:23,740
color coded in the ratings 

28
23:55:24,900 --> 23:55:28,890
for example there is no node with in degree 0 and out degree 0 

29
23:55:28,890 --> 23:55:32,930
so the lower - left square of the graph has value zero and color - coded blue 

30
23:55:34,540 --> 23:55:38,580
on the other hand there are two nodes with in - degree three and out - degree three 

31
23:55:38,580 --> 23:55:43,510
thus the top - right corner has the value twp which is 20 of the nodes 

32
23:55:44,810 --> 23:55:48,310
color coded as light green 

33
23:55:48,310 --> 23:55:50,630
the 2d histogram provides an interesting insight 

34
23:55:52,160 --> 23:55:55,940
the nodes with more incident edges than outgoing edges

35
23:55:57,080 --> 23:56:00,740
represent entities that take in more than they put out 

36
23:56:01,880 --> 23:56:07,090
in a social networking setting they represent members who are listeners 

37
23:56:08,450 --> 23:56:11,910
they receive a lot of posts but send much fewer posts 

38
23:56:13,360 --> 23:56:16,510
on the opposite side of the spectrum 

39
23:56:16,510 --> 23:56:22,242
there are talkers whose out - degrees exceed their in - degrees 

40
23:56:23,710 --> 23:56:29,173
the entities that are in between we have both large values of in - degree and

41
23:56:29,173 --> 23:56:30,340
out - degree 

42
23:56:30,340 --> 23:56:32,460
these are communicators 

43
23:56:32,460 --> 23:56:36,180
in this graph there seems to be more talkers than listeners 

44
23:56:36,180 --> 23:56:37,850
not surprising though 

45
23:56:37,850 --> 23:56:41,236
my social media friends show similar statistics 

1
23:50:04,008 --> 23:50:07,640
in this video we will explore

2
23:50:07,640 --> 23:50:12,040
another fundamental property of graphs called connectivity 

3
23:50:12,040 --> 23:50:16,350
we list two important kinds of graph analytic questions

4
23:50:16,350 --> 23:50:17,610
that are based on connectivity 

5
23:50:18,770 --> 23:50:22,380
in the first case we are asking about the robustness of the network 

6
23:50:23,440 --> 23:50:26,520
suppose we have a computer network with many servers and

7
23:50:26,520 --> 23:50:28,330
a hacker is trying to bring the system down 

8
23:50:29,670 --> 23:50:34,190
you set a small set of servers that the hacker can target to disrupt the network 

9
23:50:35,690 --> 23:50:39,750
what we mean by disrupt is to disconnect a part of the network from the rest 

10
23:50:41,262 --> 23:50:45,110
a similar problem may occur in the power distribution network 

11
23:50:45,110 --> 23:50:50,100
in that case an attacker may be able to attack one or two central components so

12
23:50:50,100 --> 23:50:52,700
that large portions of the network loses power 

13
23:50:54,250 --> 23:50:58,117
i have a geneticist colleague at tech graduate institute who studied

14
23:50:58,117 --> 23:51:00,460
the robustness of biological networks 

15
23:51:01,850 --> 23:51:06,990
he told me that many biological networks have built in redundancy so

16
23:51:06,990 --> 23:51:13,030
that even if you disrupt one important gene there are other genes in the network

17
23:51:13,030 --> 23:51:16,580
which will support the biological function possibly through other routes 

18
23:51:17,760 --> 23:51:21,940
therefore a network is robust if removing one or

19
23:51:21,940 --> 23:51:25,610
more edges or nodes still keeps it connected 

20
23:51:27,380 --> 23:51:31,720
the second category is about network comparison in terms of their

21
23:51:31,720 --> 23:51:32,630
overall connectivity 

22
23:51:34,090 --> 23:51:37,060
the two graphs shown here are very different in their structure 

23
23:51:38,730 --> 23:51:41,920
what are some parameters by which we can compare them 

24
23:51:41,920 --> 23:51:47,540
that to talk about connectivity we must first define the concept of connectivity 

25
23:51:47,540 --> 23:51:48,200
very simple 

26
23:51:49,330 --> 23:51:54,120
a graph is connected if we can reach any node from any other node 

27
23:51:56,150 --> 23:52:01,243
let look at the crypto graph in the picture clearly we cannot

28
23:52:01,243 --> 23:52:06,714
reach from all nodes to all nodes here because this graph is not connected 

29
23:52:06,714 --> 23:52:12,850
however we can identify four parts of the graph that are themselves connected 

30
23:52:13,920 --> 23:52:18,300
these islands of connected parts are called components or

31
23:52:18,300 --> 23:52:19,790
connected components of a graph 

32
23:52:21,198 --> 23:52:25,420
for directed graphs we need to be a little more specific about connectivity 

33
23:52:26,450 --> 23:52:31,200
we say that the directed graph is strongly connected if we follow the direction of

34
23:52:31,200 --> 23:52:35,690
the edges and still reach every node from every other node 

35
23:52:37,170 --> 23:52:42,640
a weaker form of connectivity is if we do not care about the direction of the arrows

36
23:52:42,640 --> 23:52:45,790
and can reach every node from every other node 

37
23:52:47,010 --> 23:52:51,430
another way of saying it is that the undirected version of the graph

38
23:52:51,430 --> 23:52:55,760
is connected this is called weak connected 

39
23:52:55,760 --> 23:52:59,300
look at the graph that we have seen so many times in this course 

40
23:52:59,300 --> 23:53:01,994
is it strongly connected or weakly connected 

41
23:53:09,749 --> 23:53:14,070
this leads to some big graph challenge 

42
23:53:14,070 --> 23:53:15,750
given an arbitrarily large graph 

43
23:53:16,960 --> 23:53:20,050
can i find the connected components of the graph efficiently 

44
23:53:21,230 --> 23:53:24,280
second given an arbitrarily large graph 

45
23:53:24,280 --> 23:53:26,970
can we find its sub graphs that are strongly connected 

46
23:53:28,080 --> 23:53:30,649
we will touch upon these questions in module four 

1
23:43:34,840 --> 23:43:36,630
so there are two ways to break a graph 

2
23:43:38,320 --> 23:43:42,720
identifying the smallest node set which if removed will disconnect the graph 

3
23:43:44,000 --> 23:43:50,617
here if we remove f d and

4
23:43:50,617 --> 23:43:55,140
h or h f g we have disconnected the graph 

5
23:43:56,220 --> 23:44:01,640
so the separating set is either h f and g or h f and d 

6
23:44:02,870 --> 23:44:04,475
so the connectivity is three 

7
23:44:04,475 --> 23:44:10,726
similarly removing c - f d - g d - f and

8
23:44:10,726 --> 23:44:17,590
h - e edges will also disconnect the graph 

9
23:44:17,590 --> 23:44:19,730
therefore the edge connectivity is four 

10
23:44:20,800 --> 23:44:23,900
so this is how network robustness is defined 

11
23:44:24,950 --> 23:44:27,790
now let ask is this network robust 

12
23:44:28,790 --> 23:44:32,530
suppose the attacker removed node f 

13
23:44:32,530 --> 23:44:33,220
why remove f 

14
23:44:34,410 --> 23:44:38,430
because f is the most connected node 

15
23:44:38,430 --> 23:44:40,620
that is f has the highest degree 

16
23:44:41,780 --> 23:44:45,220
if we remove f five paths are disrupted 

17
23:44:45,220 --> 23:44:49,550
for example there is no way to go from c to g or

18
23:44:49,550 --> 23:44:53,190
from i to e because these paths went through f 

19
23:44:54,340 --> 23:44:55,270
let try this exercise 

20
23:44:56,350 --> 23:44:59,800
suppose the attacker has already removed f 

21
23:44:59,800 --> 23:45:03,610
and would like to cause more damage to the rest of the network 

22
23:45:03,610 --> 23:45:05,340
which node should be targeted next 

23
23:45:06,490 --> 23:45:11,330
the answer is c because c is the next most connected node 

24
23:45:12,600 --> 23:45:16,637
higher degree nodes make the network more vulnerable 

1
23:28:50,752 --> 23:28:55,460
so far the centrality of a node is defined using the degree and

2
23:28:55,460 --> 23:28:58,250
the shortest distance to other nodes 

3
23:28:58,250 --> 23:28:59,830
now we introduce a different idea 

4
23:29:01,050 --> 23:29:04,480
we would like to say that if you are important and

5
23:29:04,480 --> 23:29:07,350
i am connected to you then i must be somewhat important too 

6
23:29:08,540 --> 23:29:11,510
in other words my centrality

7
23:29:11,510 --> 23:29:16,030
is proportional to the combined centrality values of my neighbors 

8
23:29:16,030 --> 23:29:20,720
now if we write that down mathematically it would look like the top formula 

9
23:29:20,720 --> 23:29:24,690
the centrality of vi is the sum of its neighbors 

10
23:29:24,690 --> 23:29:27,140
now we can write that as an equation

11
23:29:27,140 --> 23:29:29,300
where lambda is a proportionality constant 

12
23:29:30,710 --> 23:29:35,040
the resulting equation looks exactly like the eigenvector equation we have

13
23:29:35,040 --> 23:29:36,210
seen before 

14
23:29:36,210 --> 23:29:39,790
now again you really do not have to understand how it works 

15
23:29:39,790 --> 23:29:43,010
it fine to know that if we solve that equation 

16
23:29:43,010 --> 23:29:45,390
we will get the eigen values lambda 

17
23:29:46,690 --> 23:29:49,130
now let take the largest lambda and

18
23:29:49,130 --> 23:29:54,130
find the corresponding eigenvector which will give you the centrality of each note 

19
23:29:55,680 --> 23:29:58,390
notice the difference between the degree centrality and

20
23:29:58,390 --> 23:30:00,790
the eigenvector centrality in the same graph 

21
23:30:02,290 --> 23:30:05,600
the yellow node in the middle has a low degree centrality

22
23:30:05,600 --> 23:30:07,540
compared to its neighbors 

23
23:30:07,540 --> 23:30:13,470
however with the eigenvector centrality the node becomes comparably more important

24
23:30:13,470 --> 23:30:17,880
because the neighbor centrality status boost some of the centrality of this node 

25
23:30:19,260 --> 23:30:22,790
in contrast consider the second highlighted note 

26
23:30:22,790 --> 23:30:27,350
it had the same on normalize degree centrality as the previous node 

27
23:30:28,520 --> 23:30:32,610
but because the neighbors are low centrality nodes 

28
23:30:32,610 --> 23:30:35,440
the eigenvector centrality goes down 

29
23:30:35,440 --> 23:30:41,730
so this is the intended consequence of the eigenvector centrality measure 

30
23:30:41,730 --> 23:30:47,060
speaking of consequences eigenvector centrality essentially says

31
23:30:47,060 --> 23:30:49,810
if you know the right people your importance will go up 

32
23:30:51,090 --> 23:30:52,760
well that kind of risky proposition 

33
23:30:53,880 --> 23:30:59,090
here is me in a social network and let say i am connected to

34
23:30:59,090 --> 23:31:03,250
this somewhat dubious character and i think it does not really matter 

35
23:31:04,430 --> 23:31:09,490
what i do not know is that my connection has it own set of connections and

36
23:31:09,490 --> 23:31:10,160
look at who they are 

37
23:31:11,480 --> 23:31:15,740
so on the one hand these shady characters that are now connected to

38
23:31:15,740 --> 23:31:20,020
indirectly does raise my eigenvector centrality but

39
23:31:20,020 --> 23:31:24,000
it also has quite a damaging effect on my reputation 

40
23:31:24,000 --> 23:31:27,230
my ev centrality almost makes me look like a suspect 

41
23:31:28,230 --> 23:31:28,950
now think about that 

42
23:31:30,320 --> 23:31:34,090
now brin and page the founders of google

43
23:31:34,090 --> 23:31:37,260
had an interesting way to think about the eigenvector centrality 

44
23:31:38,660 --> 23:31:40,750
they thought about a server 

45
23:31:40,750 --> 23:31:43,410
well no not that kind of server 

46
23:31:43,410 --> 23:31:44,770
this kind of server 

47
23:31:44,770 --> 23:31:46,860
the kind that surfs the web 

48
23:31:46,860 --> 23:31:49,855
but this is a special web surfer called a random surfer 

49
23:31:51,050 --> 23:31:56,050
and here is what he does he picks a web page and looks at the links 

50
23:31:58,040 --> 23:32:03,320
then he chooses a random link goes to that page and does the same thing again 

51
23:32:05,000 --> 23:32:09,600
except sometimes when you kind of get bored and goes to totally new page 

52
23:32:11,220 --> 23:32:13,510
how often does he do this random jump 

53
23:32:14,720 --> 23:32:19,250
let say there always sort of a 15 chance that he will 

54
23:32:19,250 --> 23:32:22,490
or more generally with the probability of 1 minus alpha 

55
23:32:24,040 --> 23:32:27,210
so page and brin idea was to figure out

56
23:32:27,210 --> 23:32:31,780
that this surfer will visit a page with a high chance if the page is central 

57
23:32:33,060 --> 23:32:34,390
they came up with a measure for

58
23:32:34,390 --> 23:32:39,010
this stationary probability of a page being visited by the random surfer 

59
23:32:40,160 --> 23:32:42,820
they did not call it centrality they called it pagerank 

60
23:32:44,330 --> 23:32:47,114
let see a youtube video to understand how pagerank behaves 

61
23:34:05,194 --> 23:34:09,427
okay now we are talking about the world wide web which is a huge graph 

62
23:34:09,427 --> 23:34:12,565
how do we of such a graph 

63
23:34:13,575 --> 23:34:18,550
the answer is iteratively using a method called power iteration 

64
23:34:19,710 --> 23:34:23,440
this method can be used because we are looking for the largest eigenvalue 

65
23:34:23,440 --> 23:34:24,070
let me show you 

66
23:34:25,390 --> 23:34:26,360
let take a small graph 

67
23:34:27,990 --> 23:34:31,730
let initialize the still unknown page rank as zero for all nodes 

68
23:34:33,020 --> 23:34:38,930
now page rank a is a 0 15 chance that i was at a already 

69
23:34:38,930 --> 23:34:43,740
and 0 85 chance that i come to a from b or i come to a from c 

70
23:34:45,270 --> 23:34:48,370
however at this point the page rank of b and c are at zero 

71
23:34:49,700 --> 23:34:53,134
so in the first iteration page rank of a is 0 15 

72
23:34:54,670 --> 23:35:00,210
now for b i can only come to b from a but we cannot claim all of pagerank of a 

73
23:35:00,210 --> 23:35:04,901
because there is always a half chance that the surfer will come to b

74
23:35:04,901 --> 23:35:08,255
from a because he could also get to here from c 

75
23:35:08,255 --> 23:35:14,042
this plus the 0 15 chance that the surfer is already at b 

76
23:35:14,042 --> 23:35:16,733
makes bs pagerank 0 21 

77
23:35:16,733 --> 23:35:20,221
now after doing a few rounds of this computation between 50 and 100 iteration 

78
23:35:20,221 --> 23:35:20,760
let say 

79
23:35:21,820 --> 23:35:22,830
the values will converge 

80
23:35:23,830 --> 23:35:27,570
now this computation has been demonstrated to perform well

81
23:35:27,570 --> 23:35:28,740
in the mapreduce framework 

82
23:35:30,020 --> 23:35:34,100
in module four we will talk about another way of computing this metric 

1
23:04:27,940 --> 23:04:30,930
welcome back to the second module of the course 

2
23:04:32,000 --> 23:04:35,480
in this module we will cover a number of basic principles and

3
23:04:35,480 --> 23:04:36,780
techniques of graph analytics 

4
23:04:38,340 --> 23:04:42,400
as we mentioned in the last module the goal of graph analytics

5
23:04:42,400 --> 23:04:47,200
is to utilize the mathematical properties of data and provide

6
23:04:47,200 --> 23:04:51,710
efficient algorithmic solutions for large and complex graph structure problems 

7
23:04:52,970 --> 23:04:57,870
as i said in this module we will learn a number of basic graph analytic techniques 

8
23:04:59,080 --> 23:05:02,580
after this module you will be able to identify the right

9
23:05:02,580 --> 23:05:06,160
class of techniques to apply for a graph analytics problem 

10
23:05:06,160 --> 23:05:10,630
to be more specific in this module we will consider

11
23:05:10,630 --> 23:05:14,380
the mathematical and algorithmic aspects and not so

12
23:05:14,380 --> 23:05:17,050
much the computing frameworks to implement these methods 

13
23:05:18,590 --> 23:05:23,210
in modules 3 and 4 we will look at two different kinds of computing

14
23:05:23,210 --> 23:05:27,190
platforms that are used for implementing the techniques discussed in this module 

15
23:05:28,330 --> 23:05:30,110
here is the lesson plan for the module 

16
23:05:31,320 --> 23:05:34,800
first we will discuss a few basic terms and

17
23:05:34,800 --> 23:05:37,850
their definition which we will use for the rest of the course 

18
23:05:40,030 --> 23:05:42,980
of course they are not the only terms and concepts we will learn 

19
23:05:44,200 --> 23:05:47,520
as we go through each technique we will add more terms and

20
23:05:47,520 --> 23:05:49,026
definitions in our vocabulary 

21
23:05:49,026 --> 23:05:53,838
now following these definitions we will consider four categories of

22
23:05:53,838 --> 23:05:56,060
graph analytic procedures 

23
23:05:56,060 --> 23:06:01,300
the first called path analytics is centered around the analytic techniques

24
23:06:01,300 --> 23:06:04,250
where the primary objective involves traversing to the nodes and

25
23:06:04,250 --> 23:06:05,170
edges of the ground 

26
23:06:06,820 --> 23:06:09,260
the second analytic technique inquires and

27
23:06:09,260 --> 23:06:12,010
explores the connectivity pattern of the gaps 

28
23:06:12,010 --> 23:06:16,160
where the term connectivity pattern refers to the structure and

29
23:06:16,160 --> 23:06:18,510
organizations of the edges of the graph 

30
23:06:20,070 --> 23:06:23,230
the third analytics category involves the discovery and

31
23:06:23,230 --> 23:06:27,820
behavior of communities which are closely interacting entities in a network 

32
23:06:28,970 --> 23:06:34,210
the fourth category termed centrality analytics detects and characterizes

33
23:06:34,210 --> 23:06:38,670
significant nodes of a network with respect to a specific analysis problem 

34
23:06:39,750 --> 23:06:43,570
of course there are many more types of graph analytic techniques that we will cover

35
23:06:43,570 --> 23:06:44,710
in the course 

36
23:06:44,710 --> 23:06:48,000
we will provide some additional reading material for those who are interested 

37
23:06:48,000 --> 23:06:51,590
but we start by recapitulating our definition of graphs

38
23:06:51,590 --> 23:06:56,225
as a collection of vertices and edges which represent ordered pairs of nodes 

39
23:06:57,580 --> 23:07:00,880
while this mathematical definition is indeed correct 

40
23:07:00,880 --> 23:07:05,430
in practice it needs to be extended to give you other information elements 

41
23:07:07,040 --> 23:07:08,540
let us consider a single tweet 

42
23:07:10,090 --> 23:07:15,040
as we have mentioned previously a tweet is a complex information output

43
23:07:15,040 --> 23:07:17,970
because it is a graph in itself with several nodes and edges 

44
23:07:19,120 --> 23:07:21,870
but over and about the structure a tweet

45
23:07:21,870 --> 23:07:25,880
actually contains much more information and code inside the nodes and edges 

46
23:07:28,320 --> 23:07:31,560
first there several kinds of nodes in a tweet 

47
23:07:32,680 --> 23:07:36,810
for example it has a tweet node a user node a media node 

48
23:07:36,810 --> 23:07:39,335
a url node a hashtag node and so forth 

49
23:07:40,750 --> 23:07:46,115
this assignment of kinds or labels to nodes is often called node typing 

50
23:07:47,710 --> 23:07:51,352
every graph application will have its own set of types 

51
23:07:51,352 --> 23:07:56,350
and it will assign one or more types to a node but it is not mandatory for

52
23:07:56,350 --> 23:07:57,840
an application to use node types 

53
23:08:00,140 --> 23:08:05,280
mathematically we can extend our original definition with two more elements 

54
23:08:05,280 --> 23:08:10,760
the set of node types and the mapping function that assigns types to nodes 

55
23:08:10,760 --> 23:08:13,360
that means it associates a type to every node 

56
23:08:14,430 --> 23:08:20,950
however not all nodes need to have a type but in many applications they do 

57
23:08:23,370 --> 23:08:27,250
in addition to types a node also has attributes and values 

58
23:08:28,680 --> 23:08:29,910
in our tweet example 

59
23:08:31,090 --> 23:08:35,740
text is the name of an attribute that refers to the textual body of the tweet

60
23:08:35,740 --> 23:08:40,910
whose value is a character string written by the author of the tweet 

61
23:08:41,980 --> 23:08:44,620
for a specific kind of data like a tweet 

62
23:08:44,620 --> 23:08:47,770
one has a fixed set of attributes as decided by twitter 

63
23:08:48,970 --> 23:08:51,800
this collection of attributes is called a node schema 

64
23:08:53,080 --> 23:08:54,480
for a general graph 

65
23:08:54,480 --> 23:08:57,700
a node schema may have an arbitrary number of attribute value pairs 

66
23:08:58,810 --> 23:09:03,330
we will revisit this in module 3 when we discuss graphing the models 

67
23:09:04,690 --> 23:09:09,290
similarly at edge of a graph we have an edge type also called an edge label 

68
23:09:10,390 --> 23:09:11,508
also just like a node 

69
23:09:11,508 --> 23:09:16,026
an edge may have an edge schema consisting of attribute value pierce 

70
23:09:16,026 --> 23:09:21,590
here interaction type is an attribute in our biological

71
23:09:21,590 --> 23:09:25,260
network that describes the modality of interaction between a pair of genes 

72
23:09:27,550 --> 23:09:30,100
for the specific edge we have highlighted 

73
23:09:30,100 --> 23:09:33,470
the genes interact through biochemical activity 

74
23:09:33,470 --> 23:09:36,040
because they are party to some biochemical process 

75
23:09:38,830 --> 23:09:42,130
clearly there are different kinds of interaction between these

76
23:09:42,130 --> 23:09:42,910
genes or proteins 

77
23:09:43,930 --> 23:09:48,684
that means an attribute called interaction type can have a set of

78
23:09:48,684 --> 23:09:52,570
possible values like physical genetic and so on 

79
23:09:54,410 --> 23:09:59,055
this set of possible values is called the domain of the attribute 

80
23:10:01,850 --> 23:10:04,670
putting these elements back into our mathematical model 

81
23:10:04,670 --> 23:10:09,040
we get a more concrete specification of what a real live graph would contain 

82
23:10:10,340 --> 23:10:14,450
we have already discussed edge types as well as node and edge properties 

83
23:10:16,450 --> 23:10:18,090
take a minute to look through this again 

84
23:10:19,910 --> 23:10:25,880
whenever you consider an application that needs graph analytics the first task

85
23:10:25,880 --> 23:10:29,800
should be to determine the informational model of the graph your application needs 

86
23:10:30,850 --> 23:10:34,630
it always a good exercise to document the information model 

87
23:10:34,630 --> 23:10:37,280
in terms of the elements described on the slide 

88
23:10:39,365 --> 23:10:41,890
let see a little more on the topic of edge properties 

89
23:10:43,468 --> 23:10:47,650
many application encode different kids of numeric knowledge

90
23:10:47,650 --> 23:10:50,370
into edges of a graph in the form of edge points 

91
23:10:51,850 --> 23:10:55,050
if we do not put weights in an adjacency metrics 

92
23:10:55,050 --> 23:11:00,290
an edge is just represented by placing a one in the appropriate cell 

93
23:11:00,290 --> 23:11:02,800
however if we do use a weight 

94
23:11:02,800 --> 23:11:07,070
the weight value can be placed in the adjacency matrix to facilitate

95
23:11:07,070 --> 23:11:09,939
down stream computation as we will show in the next lesson 

96
23:11:11,330 --> 23:11:12,130
what do the weights mean 

97
23:11:13,440 --> 23:11:15,920
that depends on the application 

98
23:11:15,920 --> 23:11:16,720
let see some examples 

99
23:11:18,530 --> 23:11:24,440
the most obvious example is a road map where the nodes are road intersections and

100
23:11:24,440 --> 23:11:26,960
the edges represent stretches of the street or

101
23:11:26,960 --> 23:11:28,380
highway between these intersections 

102
23:11:29,520 --> 23:11:33,200
the edge weight can represent the distance of a particular segment of the road 

103
23:11:36,070 --> 23:11:40,500
in a personal communication network for example an email network 

104
23:11:40,500 --> 23:11:46,520
we can count the average number of emails per week sent from john to jill and

105
23:11:46,520 --> 23:11:49,700
use it as a proxy for the strength of their connection 

106
23:11:49,700 --> 23:11:54,049
so more emails means a stronger connection 

107
23:11:54,049 --> 23:11:58,276
in a biological network one often has to assess whether an interaction that

108
23:11:58,276 --> 23:12:02,699
can occur is actually likely to occur given the concentration of the reactants 

109
23:12:02,699 --> 23:12:07,330
the chemical environment at the site of the reaction and so forth 

110
23:12:07,330 --> 23:12:11,520
this is represented as a weight that designates the likelihood of interaction 

111
23:12:12,930 --> 23:12:15,610
finally consider a knowledge network

112
23:12:15,610 --> 23:12:19,830
where nodes represent entities like people places and events 

113
23:12:19,830 --> 23:12:24,920
and edges represent relationships like a person visited a place or

114
23:12:24,920 --> 23:12:28,460
movie actor tom is dating movie actress kim 

115
23:12:28,460 --> 23:12:31,780
now this kind of information may be important for some news media 

116
23:12:31,780 --> 23:12:36,716
however if the information does not come from an authentic source 

117
23:12:36,716 --> 23:12:41,200
itself it is more prudent to put a certainty value on it 

118
23:12:41,200 --> 23:12:44,150
this certainty value may be treated as a weight on the edge 

119
23:12:45,530 --> 23:12:48,280
moving on from the information model of the graph 

120
23:12:48,280 --> 23:12:52,200
the structure of the graph often contains valuable insights to a graph 

121
23:12:53,270 --> 23:12:56,060
many of the graph analytic techniques we will discuss in this section

122
23:12:57,130 --> 23:13:00,338
will consider these structural properties of graphs 

123
23:13:00,338 --> 23:13:07,130
one such structure is a loop which is an edge from a node to itself 

124
23:13:07,130 --> 23:13:08,570
in the example here 

125
23:13:08,570 --> 23:13:12,410
you can see that a protein can interact with another protein of the same kind 

126
23:13:13,440 --> 23:13:15,420
many other examples abound 

127
23:13:15,420 --> 23:13:17,997
people send emails to themselves 

128
23:13:17,997 --> 23:13:20,630
a road segment circles back to the same intersection 

129
23:13:21,800 --> 23:13:24,814
a website has a link to its own url 

130
23:13:24,814 --> 23:13:29,578
the existence of loops and the nodes that have such loops can be very informative

131
23:13:29,578 --> 23:13:33,790
in some applications and can be problematic for other applications 

132
23:13:35,160 --> 23:13:38,910
another structure property of a graph is the occurrence of multiple

133
23:13:38,910 --> 23:13:41,230
edges between the same node pair 

134
23:13:41,230 --> 23:13:45,170
the graphs with this feature are called multi - graphs 

135
23:13:45,170 --> 23:13:49,330
in this example the two map kinase genes have five edges between them 

136
23:13:51,030 --> 23:13:52,396
why multiple edges 

137
23:13:52,396 --> 23:13:59,130
it because each edge has a different information content 

138
23:13:59,130 --> 23:13:59,760
in this case 

139
23:13:59,760 --> 23:14:04,100
these two genes can have five different types of interactions between them 

140
23:14:04,100 --> 23:14:08,540
where each interaction has a different value for the attribute interaction type 

141
23:14:09,830 --> 23:14:12,500
we see this all the time in human networks too 

142
23:14:12,500 --> 23:14:17,080
a person can be my spouse a co - performer in music and

143
23:14:17,080 --> 23:14:19,050
my financial adviser all at the same time 

144
23:14:20,140 --> 23:14:24,100
many analytics algorithms are not natively designed for

145
23:14:24,100 --> 23:14:29,040
multigraphs and often need a little customization to handle them 

146
23:14:29,040 --> 23:14:33,130
we will mention some of these customizations as we go forward and walk

147
23:14:33,130 --> 23:14:37,791
through the different kinds of analytics applications preformed on graphs 

1
22:19:01,600 --> 22:19:03,180
so we talked about local properties 

2
22:19:04,300 --> 22:19:05,160
in this lecture 

3
22:19:05,160 --> 22:19:08,860
we will cover a global property based method of inaudible finding 

4
22:19:10,610 --> 22:19:14,000
the specific property we focus on is called modularity 

5
22:19:15,230 --> 22:19:17,990
it tries to estimate the quality of clusters of

6
22:19:17,990 --> 22:19:18,855
communities in the inaudible 

7
22:19:20,270 --> 22:19:21,680
the intuition is as follows 

8
22:19:22,890 --> 22:19:25,255
if we consider the edges in a group and

9
22:19:25,255 --> 22:19:29,528
try to see whether it different from what you would see if the edges

10
22:19:29,528 --> 22:19:33,588
were assigned randomly with some probability distribution 

11
22:19:33,588 --> 22:19:37,730
if there is a community there will be more edges than would happen at random 

12
22:19:38,850 --> 22:19:43,692
if there is no community in some part of the graph the number of edges in

13
22:19:43,692 --> 22:19:48,637
that part will either be close to the random case or even lower than that 

14
22:19:48,637 --> 22:19:54,136
the modularity measure thus estimates the quality of the clusters in

15
22:19:54,136 --> 22:20:01,070
the graph by evaluating this difference of the actual minus the random edge fraction 

16
22:20:02,260 --> 22:20:04,930
so this is the mathematical formulation of what i just described 

17
22:20:06,290 --> 22:20:10,045
the adjacency matrix a gives us the actual edges 

18
22:20:11,740 --> 22:20:16,547
the pij provides a probability of a random edge 

19
22:20:16,547 --> 22:20:20,500
the m in the denominator gives us the fractional edges 

20
22:20:22,110 --> 22:20:26,654
and the delta function task is to evaluate if i and

21
22:20:26,654 --> 22:20:29,451
j should be in the same cluster 

22
22:20:29,451 --> 22:20:34,502
if they are the contribution will be added to q 

23
22:20:34,502 --> 22:20:39,449
the quality metric which is multi - layered 

24
22:20:40,590 --> 22:20:44,599
well we have not defined what the probability model looks like 

25
22:20:44,599 --> 22:20:47,790
there are many ways to figure out what pij should look like 

26
22:20:49,360 --> 22:20:54,170
one simple model says that the chance that there is an edge between nodes i and

27
22:20:54,170 --> 22:21:01,240
j is proportional to the degree of node i times the degree of node j 

28
22:21:01,240 --> 22:21:03,510
that means if nodes i and

29
22:21:03,510 --> 22:21:07,890
j are already well connected there is a high chance that they share an edge 

30
22:21:08,940 --> 22:21:11,520
so if you are a mathematical person you might be thinking okay 

31
22:21:12,690 --> 22:21:17,500
let find clusters in the graph so that q is maximum and then we are done 

32
22:21:18,960 --> 22:21:24,094
well sadly maximizing q is very hard 

33
22:21:24,094 --> 22:21:27,050
so we need to find an approximate solution 

34
22:21:29,490 --> 22:21:32,180
so we will illustrate a very

35
22:21:32,180 --> 22:21:36,660
popular method of finding this modularity based community detection 

36
22:21:38,290 --> 22:21:41,500
well the best way to describe this is through a youtube video 

37
22:21:42,560 --> 22:21:43,060
that the url 

38
22:21:44,600 --> 22:21:48,758
now the next slide and the following slide 

39
22:21:48,758 --> 22:21:54,195
describes it but i think it better to explain the method

40
22:21:54,195 --> 22:21:58,686
through screenshots off the video as it happens 

41
22:22:00,748 --> 22:22:05,273
we will show you some snapshots of this video 

42
22:22:05,273 --> 22:22:08,630
there are 309 nodes in this graph 

43
22:22:09,670 --> 22:22:12,720
initially they all have different colors 

44
22:22:12,720 --> 22:22:14,580
that is they belong to different communities 

45
22:22:16,270 --> 22:22:23,328
this screenshot shows the graph at iteration 144 of the algorithm 

46
22:22:23,328 --> 22:22:27,586
the number of communities now is 286 

47
22:22:27,586 --> 22:22:34,050
the chart on the right side plots time on the x - axis and modularity on the y - axis 

48
22:22:34,050 --> 22:22:36,670
as you see the modularity on the rights 

49
22:22:37,750 --> 22:22:40,730
roughly at each iteration 

50
22:22:41,800 --> 22:22:46,910
the system is trying to change the color of a node to that of its neighbors 

51
22:22:46,910 --> 22:22:51,593
but it actually changes the color only if the modularity value of

52
22:22:51,593 --> 22:22:55,765
the whole graph changes as a result of that color change 

53
22:22:55,765 --> 22:23:01,107
after a few more iterations the number of communities has become 241 

54
22:23:01,107 --> 22:23:06,990
the three arrows show some parts of the graph where the nodes have changed colors 

55
22:23:06,990 --> 22:23:10,578
modularity is on the rise 

56
22:23:10,578 --> 22:23:16,115
after 1 437 iterations the modularity of the graph is still going up 

57
22:23:16,115 --> 22:23:18,610
now there are 113 communities 

58
22:23:20,060 --> 22:23:24,220
the errors show some new areas where the neighboring nodes have the same color 

59
22:23:25,580 --> 22:23:30,690
at 1 842 iterations the modulary gains slows down 

60
22:23:30,690 --> 22:23:35,992
meanwhile the number of communities have reduced to 75 

61
22:23:35,992 --> 22:23:41,540
at 4 179 iterations the modularity growth has started becoming flat 

62
22:23:42,980 --> 22:23:48,657
but in the meantime the number of colors that is communities has reduced to 48 

63
22:23:48,657 --> 22:23:53,556
at around 5 196 iterations the algorithm decides that there is not enough

64
22:23:53,556 --> 22:23:56,041
reduction in the number of communities 

65
22:23:56,041 --> 22:24:00,240
which is reduced to only 45 in the last 1 000 iterations or so 

66
22:24:01,730 --> 22:24:06,440
now it collapses each community that is clustered to a single node and

67
22:24:06,440 --> 22:24:09,180
creates a cluster to cluster edges 

68
22:24:10,250 --> 22:24:15,140
the orange box an arrow shows this contraction 

69
22:24:15,140 --> 22:24:19,140
now compared to the previous slide this collapsing or

70
22:24:19,140 --> 22:24:23,130
contraction of the plastic creates a skeleton of the original draft 

71
22:24:24,400 --> 22:24:27,470
now the algorithm starts again with this reduced graph 

72
22:24:29,480 --> 22:24:32,509
you will find many graph analysis software 

73
22:24:32,509 --> 22:24:36,658
where you can run the louvain method of community detection 

74
22:24:38,747 --> 22:24:40,837
 inaudible 

75
22:24:40,837 --> 22:24:44,749
i took my linked - in network which has me at the center and

76
22:24:44,749 --> 22:24:46,640
all my connections as nodes 

77
22:24:48,630 --> 22:24:51,483
if two of my contacts are also connected in linked - in 

78
22:24:51,483 --> 22:24:53,069
there is an edge between them 

79
22:24:54,110 --> 22:24:58,350
this kind of mean - centric network is often called an ego network 

80
22:24:59,806 --> 22:25:05,700
when i inaudible the community direction algorithm here i found six communities 

81
22:25:05,700 --> 22:25:09,420
with one set of parameters and seven communities with another 

82
22:25:10,500 --> 22:25:13,390
i could clearly see my connections in san diego 

83
22:25:13,390 --> 22:25:17,190
my connections in my professional network my friends in india and so forth 

84
22:25:18,380 --> 22:25:23,090
there always one false community which stands for

85
22:25:23,090 --> 22:25:26,850
others nodes that do not clearly belong to any specific group 

86
22:25:28,730 --> 22:25:33,720
perhaps more interesting and important than the static

87
22:25:33,720 --> 22:25:38,490
analysis of communities is to track communities over a length of time 

88
22:25:38,490 --> 22:25:40,300
and determine how they evolve and why 

89
22:25:42,140 --> 22:25:46,400
there are six large categories of evolution steps

90
22:25:46,400 --> 22:25:47,690
that can happen within a community 

91
22:25:48,810 --> 22:25:51,340
a community like a new facebook group can be born 

92
22:25:52,850 --> 22:25:56,090
a community like a group of people who gathered for an event 

93
22:25:56,090 --> 22:26:00,570
would dissolve because the event and the mutual interest around it has ended 

94
22:26:02,180 --> 22:26:05,986
a community can grow because the members rally around a common cause 

95
22:26:05,986 --> 22:26:11,318
typically new cross - community edges start getting

96
22:26:11,318 --> 22:26:15,977
formed before the communities actually merge 

97
22:26:15,977 --> 22:26:18,930
the communities can shrink like my book club 

98
22:26:18,930 --> 22:26:20,190
where do you see this in real life 

99
22:26:21,430 --> 22:26:22,510
well how about company mergers 

100
22:26:24,520 --> 22:26:27,390
surely enough the inaudible results will happen when a community splits 

101
22:26:28,510 --> 22:26:30,690
going along with the previous example 

102
22:26:30,690 --> 22:26:35,170
a closely working group in a company may at some point create their own product and

103
22:26:35,170 --> 22:26:40,230
form a new company with very little ties to the old company 

104
22:26:40,230 --> 22:26:45,081
one standard symptom of a group splitting is that the nodes in the subgroup

105
22:26:45,081 --> 22:26:49,399
show an increase in the number of edges just amongst themselves 

1
20:45:51,110 --> 20:45:54,130
so far we have seen two versions of the dijkstra algorithm 

2
20:45:55,200 --> 20:45:59,680
both these versions assume that the edge weights provided by the network

3
20:45:59,680 --> 20:46:00,850
must be used as is 

4
20:46:01,890 --> 20:46:04,430
now that can lead to some interesting problems 

5
20:46:04,430 --> 20:46:06,570
we saw one such problem before 

6
20:46:06,570 --> 20:46:09,810
remember we were trying to decide whether we should go from g to d or

7
20:46:09,810 --> 20:46:14,060
from f to e because both options had the same total weight 

8
20:46:14,060 --> 20:46:16,670
now had we chosen to go from g to d 

9
20:46:16,670 --> 20:46:20,070
it would take us a few extra steps to arrive at correct solution 

10
20:46:21,190 --> 20:46:25,110
one way of handling this problem is to use additional knowledge 

11
20:46:25,110 --> 20:46:29,690
so intuitively we want to say that we know that we want to go to b 

12
20:46:29,690 --> 20:46:32,800
so traversing through d is not a good idea 

13
20:46:32,800 --> 20:46:35,530
because it will take us away from b 

14
20:46:35,530 --> 20:46:40,700
in other words we use the knowledge of the destination of b location

15
20:46:40,700 --> 20:46:42,650
to steer the direction of search 

16
20:46:43,930 --> 20:46:47,400
this variant is called goal - directed dijkstra algorithm because

17
20:46:47,400 --> 20:46:52,440
it is using the information about the target known at any point in the search 

18
20:46:52,440 --> 20:46:56,919
the trick to use this information is to change the edge weights as we diverse 

19
20:46:58,000 --> 20:46:59,350
how do we change the weight 

20
20:46:59,350 --> 20:47:02,040
we use a formula where the new weight

21
20:47:02,040 --> 20:47:06,560
is the original weight together with a function called the potential function 

22
20:47:06,560 --> 20:47:08,340
now we will show this in our example 

23
20:47:08,340 --> 20:47:09,950
since our graph is a proxy for

24
20:47:09,950 --> 20:47:14,190
a road network we can assume that we know the coordinates of every node 

25
20:47:14,190 --> 20:47:17,810
therefore we can compute the distance between any two nodes 

26
20:47:17,810 --> 20:47:20,630
in practice we will choose a few nodes so

27
20:47:20,630 --> 20:47:25,210
that we can compute the distance of every other node from these chosen nodes 

28
20:47:25,210 --> 20:47:27,450
these chosen nodes are called landmarks 

29
20:47:27,450 --> 20:47:31,280
let assume b which is our target is a landmark node and

30
20:47:31,280 --> 20:47:34,430
let rewind to the state where we are trying to choose between

31
20:47:34,430 --> 20:47:37,780
the gd expansion or the fe expansion 

32
20:47:37,780 --> 20:47:43,172
so we calculate the distance of f g and e from b 

33
20:47:43,172 --> 20:47:46,730
bf is 20 bg is 80 and be is 15 

34
20:47:46,730 --> 20:47:49,312
now we will apply the formula like this 

35
20:47:49,312 --> 20:47:53,900
for the fg case we subtract the bf distance

36
20:47:53,900 --> 20:47:57,950
from the weight and add the bd distance to the weight 

37
20:47:57,950 --> 20:48:02,180
this gives us 70 because g is far from b 

38
20:48:02,180 --> 20:48:06,210
similarly for the fe case we subtract the bf distance and

39
20:48:06,210 --> 20:48:10,000
add the be distance to the weight and it gives us 15 

40
20:48:10,000 --> 20:48:13,960
now with these modified weights we choose the fe expansion 

41
20:48:13,960 --> 20:48:19,060
in practice this significantly improves the actual performance of the algorithm 

42
20:48:19,060 --> 20:48:23,201
so this technique is used by many online mapping services when they give you

43
20:48:23,201 --> 20:48:23,973
directions 

1
17:34:13,530 --> 17:34:17,710
so now let bring back the constraints that we have ignored so far 

2
17:34:18,820 --> 17:34:23,230
there are two constraints here parts of the graph to include and

3
17:34:23,230 --> 17:34:25,280
parts of the graph to exclude 

4
17:34:25,280 --> 17:34:29,840
in this example we have to go to b but

5
17:34:29,840 --> 17:34:34,020
we should go to j first and then travel from b to j 

6
17:34:35,380 --> 17:34:38,850
also we cannot use any of the paths through e 

7
17:34:39,960 --> 17:34:41,650
this really means two things 

8
17:34:42,930 --> 17:34:48,610
first we split the problem into two independent shortest path problems

9
17:34:48,610 --> 17:34:51,670
that we can solve in parallel if needed 

10
17:34:51,670 --> 17:34:56,440
second when we go from j to b we need to extract

11
17:34:57,630 --> 17:35:01,240
the useful subgraph that we need to consider 

12
17:35:01,240 --> 17:35:04,960
for a large network and a complex exclusion condition 

13
17:35:06,010 --> 17:35:09,160
we will essentially operate over a smaller graph 

14
17:35:09,160 --> 17:35:11,640
thereby reducing the effective size of the problem 

15
17:35:13,130 --> 17:35:17,770
as we will see in module three this kind of subgraph extraction operation

16
17:35:17,770 --> 17:35:22,460
can be done effectively and efficiently with a graph database system 

17
17:35:22,460 --> 17:35:25,738
this concludes our short tour of path analytics 

1
11:09:38,690 --> 11:09:43,620
so in this optional module we will look at the two key player problems again 

2
11:09:45,160 --> 11:09:49,178
the goal of the first problem is to identify a small set of nodes 

3
11:09:49,178 --> 11:09:53,120
whose removal will create maximum disruption 

4
11:09:54,710 --> 11:09:59,540
now in this case a traditional centrality algorithm may not work 

5
11:09:59,540 --> 11:10:02,310
because the optimization goal is to break up the network 

6
11:10:04,040 --> 11:10:06,810
so we need a quantitive measure of the breakage 

7
11:10:08,090 --> 11:10:10,980
if dij is the distance between nodes i and

8
11:10:10,980 --> 11:10:15,920
j then 1 over dij is the closeness of these two nodes 

9
11:10:17,360 --> 11:10:21,980
if we add the closeness of all nodes and normalize it by the number of node pairs 

10
11:10:21,980 --> 11:10:25,150
we will get a measure of cohesiveness as a fraction 

11
11:10:26,420 --> 11:10:32,070
so 1 minus this value is a measure of fragmentation 

12
11:10:33,300 --> 11:10:36,880
our goal is to maximize this fragmentation metric 

13
11:10:38,660 --> 11:10:44,599
in the model terrorist network shown here removing the red nodes a 

14
11:10:44,599 --> 11:10:49,530
b and c will break up the network into seven components 

15
11:10:49,530 --> 11:10:52,669
with f reaching a value of 0 59 

16
11:10:52,669 --> 11:10:58,026
the second key player problem is trying to find a group of s influencers 

17
11:10:58,026 --> 11:11:02,120
which can reach a maximum number of nodes within k steps 

18
11:11:03,920 --> 11:11:07,850
the number of unique nodes reachable from a starting node is called the reach 

19
11:11:07,850 --> 11:11:08,510
of the starting node 

20
11:11:10,240 --> 11:11:15,420
for this we need to adapt the concept of reach to limit it to k steps 

21
11:11:16,820 --> 11:11:21,570
we also need to adapt it to measure the distance of an arbitrary node

22
11:11:21,570 --> 11:11:23,891
from a group of nodes between our influences 

23
11:11:25,530 --> 11:11:31,120
the distance from one node to a group of nodes can be defined as a maximum 

24
11:11:31,120 --> 11:11:36,100
or average or minimum distance of the node from the members of the group 

25
11:11:37,220 --> 11:11:39,530
often the minimum distance is a good choice 

26
11:11:41,560 --> 11:11:43,400
so the distance we could reach 

27
11:11:43,400 --> 11:11:48,280
can then be through of as the proportion of all nodes reached by the group 

28
11:11:48,280 --> 11:11:50,810
where the nodes are weighted by the distance from the set 

29
11:11:52,090 --> 11:11:55,630
and only nodes at distance 1 are given full rate 

30
11:11:56,920 --> 11:12:00,270
hence a distance we could reach that use a maximum value of 1 

31
11:12:00,270 --> 11:12:05,730
where every outside node is adjacent to at least one member of the set of influences 

32
11:12:06,970 --> 11:12:11,340
in the network shown just three nodes a c and d 

33
11:12:11,340 --> 11:12:16,340
are sufficient to reach every other member within just four steps 

34
11:12:18,120 --> 11:12:22,150
now this concludes this module where we looked at several analytic techniques and

35
11:12:22,150 --> 11:12:26,090
measures to extract different kinds of insights from a network 

1
22:22:08,060 --> 22:22:12,180
we saw how a community in a graph can evolve 

2
22:22:13,540 --> 22:22:15,940
to track the nature of evolution 

3
22:22:15,940 --> 22:22:19,180
we need to measure how the community changes over time 

4
22:22:20,430 --> 22:22:22,800
so here are three cases 

5
22:22:22,800 --> 22:22:29,670
one two and three of a community changing between two observation points 

6
22:22:29,670 --> 22:22:35,060
the goal is to figure out whether these are normal fluctuations in the network or

7
22:22:35,060 --> 22:22:36,610
are more drastic changes occurring 

8
22:22:38,040 --> 22:22:38,990
look at them for a second 

9
22:22:40,400 --> 22:22:45,138
just visually the first case seems to show just minor changes 

10
22:22:45,138 --> 22:22:51,770
whereas case two shows a merger and case three shows a split 

11
22:22:54,280 --> 22:22:58,660
now to come up with a quantitative measure of change over time we need to

12
22:22:58,660 --> 22:23:03,210
take two observations from two consecutive time points and fuse the graph 

13
22:23:04,400 --> 22:23:07,910
if you do it for case one you will find one new node 

14
22:23:09,000 --> 22:23:11,870
one living node and the rest will come on over time 

15
22:23:13,320 --> 22:23:18,060
for case two you will see that two previous communities 

16
22:23:18,060 --> 22:23:23,150
colored differently are internally connected the same way as before 

17
22:23:23,150 --> 22:23:27,170
but some members of the two communities have created new crosslinks 

18
22:23:28,470 --> 22:23:31,520
now can you tell me what you observe in case three 

19
22:23:31,520 --> 22:23:35,060
well i see one join node color purple 

20
22:23:36,130 --> 22:23:40,470
apart from it there are just two edges connecting the two groups 

21
22:23:41,830 --> 22:23:46,820
now with these observations we can now compute the autocorrelation

22
22:23:46,820 --> 22:23:50,930
between the graphs across time t and t plus 1 

23
22:23:50,930 --> 22:23:55,770
this is just a measure of the number of common nodes

24
22:23:55,770 --> 22:23:58,090
divided by the number of nodes in the combined graph 

25
22:23:59,560 --> 22:24:03,010
if the community does not change at all this number is 1 

26
22:24:03,010 --> 22:24:06,860
if a community has only a few connection the number is lower 

27
22:24:08,400 --> 22:24:13,360
after computing autocorrelation over every pair of time steps 

28
22:24:13,360 --> 22:24:15,570
we can then compute stationarity 

29
22:24:16,770 --> 22:24:21,040
which measures the overall change in the autocorrelation over a period of time 

30
22:24:22,250 --> 22:24:25,300
so if we measure over 100 time steps 

31
22:24:25,300 --> 22:24:31,990
we will add the 99 correlation values from the steps and then divide it by 99 

32
22:24:31,990 --> 22:24:36,000
this will tell us what fraction of members remain unchanged

33
22:24:36,000 --> 22:24:38,110
on an average over these 100 time steps 

34
22:24:39,590 --> 22:24:44,380
therefore the 1 minus zeta tells us the average ratio of members

35
22:24:44,380 --> 22:24:45,940
that are changed in a time step 

36
22:24:48,642 --> 22:24:50,060
let take three cases 

37
22:24:51,310 --> 22:24:55,010
in the first plot the size of the graph is small and

38
22:24:55,010 --> 22:24:56,890
nothing much is happening here 

39
22:24:56,890 --> 22:25:01,180
a note occasionally joins or leaves keeping the stationarity pretty flat 

40
22:25:03,420 --> 22:25:09,320
in the second case the graph is small but there are a lot of changes especially

41
22:25:09,320 --> 22:25:14,710
at time step seven a whole bunch of purple nodes have joined and then they went away 

42
22:25:15,750 --> 22:25:18,320
the size of the graph clearly reflects this

43
22:25:18,320 --> 22:25:22,440
with a purple spike that you see on the size versus the time graph 

44
22:25:25,123 --> 22:25:28,078
this spike on the time series by the way is called a burst 

45
22:25:28,078 --> 22:25:34,640
the third plot shows a large graph with many nodes joining and leaving 

46
22:25:35,690 --> 22:25:40,683
the stationarity of this graph will be quite low given the abrupt changes we

47
22:25:40,683 --> 22:25:42,039
observe over time 

1
20:47:47,320 --> 20:47:51,140
in this video we will discuss what paths are and

2
20:47:51,140 --> 20:47:54,420
how to find your way as it travels along the nodes and edges of the graph 

3
20:47:56,080 --> 20:47:57,020
let start with an example 

4
20:47:58,480 --> 20:47:59,260
for this example 

5
20:47:59,260 --> 20:48:03,740
we will consider an edge weighted graph with no loops as shown here 

6
20:48:03,740 --> 20:48:07,180
term edge weighted means the edges have weights 

7
20:48:08,520 --> 20:48:12,320
the weight of the edge i to j is 15 

8
20:48:12,320 --> 20:48:16,770
we can think of this graph as a small road network where the nodes are cities and

9
20:48:16,770 --> 20:48:19,190
the edge weights are highway distances between them 

10
20:48:20,630 --> 20:48:24,470
to walk or traverse through this graph we will define what a walk is 

11
20:48:25,940 --> 20:48:29,040
a walk is an arbitrary sequence of nodes and

12
20:48:29,040 --> 20:48:32,860
edges that starts from some node and ends on some node 

13
20:48:32,860 --> 20:48:38,158
here we can go from h to f

14
20:48:38,158 --> 20:48:44,935
to g to c to

15
20:48:44,935 --> 20:48:51,750
f to e to b 

16
20:48:51,750 --> 20:48:55,880
notice that in this walk we went through the node f twice 

17
20:48:57,230 --> 20:49:02,280
in many applications we do not want to consider arbitrary walks but

18
20:49:02,280 --> 20:49:05,640
consider a walk where we do not repeat nodes

19
20:49:05,640 --> 20:49:07,510
unless we need to come back to the starting point 

20
20:49:08,710 --> 20:49:11,640
such a constrained walk is called a path 

21
20:49:11,640 --> 20:49:14,920
the green arrows indicate a path from j to b 

22
20:49:16,330 --> 20:49:20,580
we said on the last slide that a path can start from a node and end on it 

23
20:49:20,580 --> 20:49:25,074
such a path is called a cycle when the path has a three or more nodes 

24
20:49:25,074 --> 20:49:29,694
the j g c 

25
20:49:29,694 --> 20:49:34,260
f j is a four node cycle sometimes called a four cycle 

26
20:49:35,620 --> 20:49:38,010
c f e is a three cycle 

27
20:49:39,650 --> 20:49:45,030
however although there is an edge from f to j and another from j back to f 

28
20:49:45,030 --> 20:49:49,590
this two node path is not a cycle by our definition 

29
20:49:50,980 --> 20:49:54,680
a network with no cycles is called acyclic 

30
20:49:56,440 --> 20:49:59,110
a graph that is both directed and

31
20:49:59,110 --> 20:50:04,600
acyclic is called a directed acyclic graph in short a deck 

32
20:50:05,780 --> 20:50:12,470
a trail is a concept similar to a path it is a walk with no repeating edges 

33
20:50:13,500 --> 20:50:19,211
in the graph shown in the walk h f g c f e 

34
20:50:19,211 --> 20:50:24,500
c f j is not a trail 

35
20:50:24,500 --> 20:50:27,470
because c f is traversed twice 

36
20:50:29,130 --> 20:50:32,050
remember the seven bridges of the konigsberg problem that we described

37
20:50:32,050 --> 20:50:33,580
in module one 

38
20:50:33,580 --> 20:50:34,840
in that problem 

39
20:50:34,840 --> 20:50:38,260
we had the constraint that one cannot cross the same bridge twice 

40
20:50:40,200 --> 20:50:43,090
often we see such constraints in path planning problems 

41
20:50:44,510 --> 20:50:48,190
a common notion in directive graphs is called reachability 

42
20:50:49,710 --> 20:50:54,280
if there is a path from node u to node v v is reachable from u 

43
20:50:55,460 --> 20:50:56,770
reachability is not symmetric 

44
20:50:57,860 --> 20:51:01,800
even if v is reachable from u u may or may not be reachable from v 

45
20:51:02,960 --> 20:51:07,220
as we can see in this graph i is not reachable from a 

46
20:51:09,030 --> 20:51:11,300
look at a more carefully 

47
20:51:11,300 --> 20:51:13,560
there is no outgoing edge from a at all 

48
20:51:14,570 --> 20:51:17,590
so no node in this graph is reachable from a 

49
20:51:18,770 --> 20:51:21,790
a is a terminal a leaf node of the graph 

50
20:51:23,300 --> 20:51:25,480
where do we see this in real life 

51
20:51:25,480 --> 20:51:29,040
think of road network with one way streets and road blocks through the construction 

52
20:51:30,040 --> 20:51:32,610
this can make some areas unreachable by cars 

53
20:51:34,370 --> 20:51:37,810
in biology there are gene regulation networks 

54
20:51:37,810 --> 20:51:39,930
in which the nodes represent genes and

55
20:51:39,930 --> 20:51:44,479
edges represent the fact that gene a regulates gene b and not visa verse 

56
20:51:45,530 --> 20:51:46,940
this makes it a directed graph 

57
20:51:48,090 --> 20:51:52,930
we can easily think of a case where a regulates b and b regulates c but

58
20:51:52,930 --> 20:51:55,270
c does not regulate b or a 

59
20:51:55,270 --> 20:51:58,800
that making a unreachable from c 

60
20:51:58,800 --> 20:52:01,050
the final concept you will explore in this graph 

61
20:52:01,050 --> 20:52:05,220
in this lecture is that of a graph diameter 

62
20:52:05,220 --> 20:52:10,950
the diameter of a graph measures the maximum number of steps also called hops 

63
20:52:10,950 --> 20:52:15,320
want us to traverse to go to the most distant node in the graph 

64
20:52:16,580 --> 20:52:19,630
that means if you go from an arbitrary node

65
20:52:19,630 --> 20:52:25,160
to any other arbitrary node in the graph following only the shortest paths roots 

66
20:52:25,160 --> 20:52:27,790
what is the maximum number of steps you have to take 

67
20:52:29,000 --> 20:52:30,520
let see how this is computed 

68
20:52:31,670 --> 20:52:36,350
for this task we will create a matrix called the shortest hop distance matrix 

69
20:52:37,790 --> 20:52:43,060
just like the adjacency matrix the rows and columns here represent graph nodes 

70
20:52:44,070 --> 20:52:47,930
and each cell contains the distance from the i eighth node 

71
20:52:47,930 --> 20:52:49,780
to the g eighth node via the shortest path 

72
20:52:51,000 --> 20:52:55,700
the distance from any node to itself is zero 

73
20:52:55,700 --> 20:52:58,870
so all diagonal elements of the matrix are zero 

74
20:53:00,570 --> 20:53:04,440
if a node i cannot reach node j through any path 

75
20:53:04,440 --> 20:53:07,200
the distance is marked as infinity 

76
20:53:07,200 --> 20:53:11,600
thus the distance from b to c is infinity 

77
20:53:11,600 --> 20:53:16,203
you can go from e to f in two steps e to c to f 

78
20:53:16,203 --> 20:53:22,670
therefore the wait at the c cell is two 

79
20:53:22,670 --> 20:53:27,830
if you fill this matrix you will notice that the largest reachable value

80
20:53:27,830 --> 20:53:30,610
is 4 in doing the infinite distance 

81
20:53:31,840 --> 20:53:36,463
thus the diameter of this graph is 4 where it represents the part from i

82
20:53:36,463 --> 20:53:41,020
to a through j g and d 

83
20:53:41,020 --> 20:53:45,000
there are a few noteworthy items in this distance matrix 

84
20:53:45,000 --> 20:53:51,310
the row of a has a value of 0 with itself and infinity for every other node 

85
20:53:51,310 --> 20:53:54,230
this happens because a is a leaf node 

86
20:53:55,740 --> 20:53:59,040
similarly except for the 0 distance with itself 

87
20:53:59,040 --> 20:54:03,810
the h column has infinity for all of the nodes 

88
20:54:03,810 --> 20:54:07,984
this happens because h has no incoming edges and

89
20:54:07,984 --> 20:54:11,226
therefore no other node can reach h 

1
17:41:59,450 --> 17:42:03,090
so we have seen how to compute degree histograms of a graph 

2
17:42:04,760 --> 17:42:08,100
while degree histograms are useful to characterize a graph 

3
17:42:08,100 --> 17:42:10,110
it is usually a means to an end 

4
17:42:11,130 --> 17:42:15,400
it a known practice in statistics to compute a mathematical expression for

5
17:42:15,400 --> 17:42:19,390
a statistical distribution using histograms 

6
17:42:20,910 --> 17:42:25,600
for graphs we often look for a function to describe the degree distribution 

7
17:42:26,620 --> 17:42:30,510
but this is expressed as a distribution of the probability

8
17:42:30,510 --> 17:42:35,090
that a random vertex will have exactly k neighbors 

9
17:42:35,090 --> 17:42:37,669
now this problem has been investigated by many 

10
17:42:38,780 --> 17:42:42,300
one popular well known model is a power law 

11
17:42:43,770 --> 17:42:49,910
a graph follows a power law if the best probability is given by k 

12
17:42:49,910 --> 17:42:52,380
erased to a negative exponent called alpha 

13
17:42:53,890 --> 17:42:58,660
the value of alpha for many practical networks is between two and three 

14
17:43:00,240 --> 17:43:01,310
more recently 

15
17:43:01,310 --> 17:43:04,750
people have suggested other distributions that look like the power law graph 

16
17:43:05,880 --> 17:43:09,611
one of them is a log - normal graph 

17
17:43:09,611 --> 17:43:13,148
here the logarithm of k has a gaussian distribution 

18
17:43:13,148 --> 17:43:17,630
so this log - normal distribution seems to have the better fit to

19
17:43:17,630 --> 17:43:20,120
natural graphs that are observed 

20
17:43:22,200 --> 17:43:24,280
so why are power law graphs important 

21
17:43:25,930 --> 17:43:28,290
interestingly many very 

22
17:43:28,290 --> 17:43:31,810
very different real life graphs in the world seem to follow the power law 

23
17:43:33,030 --> 17:43:38,860
if a graph does follow the power law it would have one large connected component

24
17:43:38,860 --> 17:43:46,030
with a very high proportion of notes connected to it in a power law graph 

25
17:43:47,060 --> 17:43:51,036
most nodes have a low degree and some nodes will be disconnected 

26
17:43:51,036 --> 17:43:56,260
the low degree nodes belong to a very dense sub graphs and

27
17:43:56,260 --> 17:43:58,600
those sub graphs are connected to each other through hubs 

28
17:44:00,150 --> 17:44:03,700
in the center of the graph has a high density 

29
17:44:03,700 --> 17:44:06,690
power law graphs can be difficult to compute with 

30
17:44:06,690 --> 17:44:10,907
for example the shortest path algorithm operating in the dense part

31
17:44:10,907 --> 17:44:15,695
will possibly be very inefficient not because of the size of the network but

32
17:44:15,695 --> 17:44:19,928
because there are too many paths to explore inside the denser pieces 

33
17:44:24,090 --> 17:44:28,574
the more interesting reason why people study power - law graphs

34
17:44:28,574 --> 17:44:32,570
is because power - law graphs are supposed to be robust 

35
17:44:33,840 --> 17:44:39,329
so in nature all biological networks show power - law graphs because

36
17:44:39,329 --> 17:44:44,552
it gives you a high rate of redundancy against failure and attacks 

1
11:26:41,760 --> 11:26:46,528
the most primitive path analytics question one can ask is to find the best

2
11:26:46,528 --> 11:26:48,570
path from node one to node two 

3
11:26:49,980 --> 11:26:52,220
what does best mean 

4
11:26:52,220 --> 11:26:54,621
well that depends on the actual needs of the application 

5
11:26:54,621 --> 11:26:57,975
but in general to specify the best path 

6
11:26:57,975 --> 11:27:02,274
we need to define when one path is better than another 

7
11:27:02,274 --> 11:27:09,010
this is usually expressed in terms of an optimization problem 

8
11:27:09,010 --> 11:27:13,790
where we need to minimize and maximize our subfunction subject to constraints 

9
11:27:13,790 --> 11:27:15,420
what kind of constraints 

10
11:27:15,420 --> 11:27:20,280
two common criteria for graphs are inclusion and exclusion conditions 

11
11:27:21,310 --> 11:27:27,860
inclusion criteria may specify which nodes we have to include in the path 

12
11:27:27,860 --> 11:27:31,050
and exclusion criteria specifies which nodes and

13
11:27:31,050 --> 11:27:33,440
edges should be excluded from the path 

14
11:27:33,440 --> 11:27:38,390
in addition one specify a preference criteria

15
11:27:38,390 --> 11:27:42,030
that act as a softer or less strict constraint 

16
11:27:42,030 --> 11:27:45,880
for example we would like to minimize highways on my trip 

17
11:27:45,880 --> 11:27:46,900
or avoid condition 

18
11:27:47,980 --> 11:27:52,179
these are soft because although the users would like to have them

19
11:27:52,179 --> 11:27:56,779
enforced completely it is all right if they are not fully enforced 

20
11:27:56,779 --> 11:28:01,362
a good practical use case occurs when i am trying to drive to work in the morning 

21
11:28:01,362 --> 11:28:06,284
ideally i would like to take a path having the shortest distance from my home 

22
11:28:06,284 --> 11:28:11,620
for example node i to my workplace which is node b in the graph 

23
11:28:12,850 --> 11:28:16,680
but i have to drop off my son at school 

24
11:28:16,680 --> 11:28:20,210
so my path must include his school the j here 

25
11:28:22,130 --> 11:28:26,150
however i would like to avoid roads around the new construction that 

26
11:28:26,150 --> 11:28:30,580
happening about five miles from my workplace like the node e in the graph 

27
11:28:32,140 --> 11:28:36,260
because there is usually a huge traffic delay around that construction site 

28
11:28:37,620 --> 11:28:40,210
i could also add a preference criteria like

29
11:28:40,210 --> 11:28:42,290
i do not prefer to drive on the highway 

30
11:28:42,290 --> 11:28:44,982
but for this discussion we will skip the preference idea 

31
11:28:44,982 --> 11:28:47,059
too complicated 

32
11:28:47,059 --> 11:28:51,900
okay let start with a simpler problem 

33
11:28:51,900 --> 11:28:56,895
to start with let drop the constraints and look at the problem with just

34
11:28:56,895 --> 11:29:01,280
the optimization part of the problem having a single variable 

35
11:29:03,150 --> 11:29:08,115
in our case that variable is the sum of edge weights from the source 

36
11:29:08,115 --> 11:29:12,086
that is the starting node i to the target which is b 

37
11:29:14,095 --> 11:29:19,000
this problem is handled by all mapping and road direction software 

38
11:29:19,000 --> 11:29:22,820
here is a google map screenshot in which i am trying to go from my home

39
11:29:22,820 --> 11:29:26,450
in north san diego to a collision center in a nearby city 

40
11:29:28,030 --> 11:29:31,760
google maps shows three different routes and

41
11:29:31,760 --> 11:29:34,040
highlights one as a preferred solution 

42
11:29:35,876 --> 11:29:41,170
you should readily see that the real shortest path of 26 6 miles

43
11:29:41,170 --> 11:29:45,000
will take the longest time at the time of the day when i was looking at the map 

44
11:29:46,080 --> 11:29:51,980
so this means the weights here are not raw distances but estimated travel time 

45
11:29:51,980 --> 11:29:54,574
you should also notice the blue red and

46
11:29:54,574 --> 11:29:58,482
orange segments in the preferred path are presented by google 

47
11:29:58,482 --> 11:30:03,731
the orange and red street segments clearly represent congestion areas and

48
11:30:03,731 --> 11:30:07,570
therefore have higher weight than the blue segments 

49
11:30:08,840 --> 11:30:13,070
therefore the weights of the street segments are not really static but

50
11:30:13,070 --> 11:30:16,370
change with many other factors like weather or the time of the day 

51
11:30:17,510 --> 11:30:22,360
this is why the least weight path problem is an important problem to solve for

52
11:30:22,360 --> 11:30:23,680
the benefit of the commuter 

53
11:30:24,710 --> 11:30:27,960
a widely applied algorithm that is applied for

54
11:30:27,960 --> 11:30:30,990
shortest path problems is called dijkstra algorithm 

55
11:30:32,360 --> 11:30:37,280
originally dijkstra considered a variant of the problem where the task is

56
11:30:37,280 --> 11:30:41,580
to find the shortest path from a single source node to all other nodes 

57
11:30:43,230 --> 11:30:45,110
we will go through the algorithm here 

58
11:30:45,110 --> 11:30:49,290
however there are many good online resources including tutorials and

59
11:30:49,290 --> 11:30:51,230
youtube videos describing the algorithm 

60
11:30:52,570 --> 11:30:56,284
for our discussion we will confine ourselves to the case where both

61
11:30:56,284 --> 11:30:59,175
the source and the target nodes are known in advance 

1
22:57:40,840 --> 22:57:46,150
as you probably already know cypher is the scripting language used in neo4j and

2
22:57:46,150 --> 22:57:50,120
it what we have been using already in the previous lectures 

3
22:57:50,120 --> 22:57:55,160
in this lecture we are going to go through a series of basic queries using cypher

4
22:57:55,160 --> 22:57:58,900
with the focus on the data sets that we have already been using 

5
22:57:58,900 --> 22:58:02,787
here a listing of the basic queries we will go through step by step 

6
22:58:11,424 --> 22:58:12,341
so let get started 

7
22:58:16,764 --> 22:58:22,167
to keep things simple we will be using a text file containing the basic queries and

8
22:58:22,167 --> 22:58:27,930
we will make this file available for download as a reading in the module 

9
22:58:27,930 --> 22:58:31,130
i will briefly review the queries and

10
22:58:31,130 --> 22:58:36,255
then we will toggle to the browser to view the results of the queries in neo4j 

11
22:58:37,680 --> 22:58:43,120
neo4j has a very nice feature which allows us to retain individual queries

12
22:58:43,120 --> 22:58:49,620
in multiple panels as we are going through the process of exploring each query 

13
22:58:49,620 --> 22:58:53,470
so for the first few queries we will be using the simple road network data set

14
22:58:53,470 --> 22:58:56,250
that we used in previous demonstrations 

15
22:58:56,250 --> 22:58:58,800
and here we have already loaded the data set into neo4j 

16
22:58:59,910 --> 22:59:01,185
so let look at our first query 

17
22:59:01,185 --> 22:59:03,530
our first query is a very a simple one 

18
22:59:03,530 --> 22:59:07,280
in which we are counting the number of nodes in the network 

19
22:59:07,280 --> 22:59:11,770
so the first line of code simply matches all of the nodes with the label mynode and

20
22:59:11,770 --> 22:59:14,200
it returns a count of those nodes 

21
22:59:14,200 --> 22:59:16,120
so let look at the results 

22
22:59:16,120 --> 22:59:20,100
the results are very simple the value of eleven 

23
22:59:20,100 --> 22:59:23,520
and we can visually confirm this by inspecting the graph itself and

24
22:59:23,520 --> 22:59:26,140
see it has 11 nodes and we can see that up here 

25
22:59:29,510 --> 22:59:34,540
the next query is almost as simple we want to count the number of edges 

26
22:59:34,540 --> 22:59:39,290
now first thing to keep in mind is in order to count edges 

27
22:59:39,290 --> 22:59:43,970
we also need to declare nodes that are associated with those edges 

28
22:59:43,970 --> 22:59:47,070
so this first line of codes includes a declaration

29
22:59:47,070 --> 22:59:49,840
of the nodes associated with the edges 

30
22:59:49,840 --> 22:59:52,231
here we are identifying with thevariable r and

31
22:59:52,231 --> 22:59:54,570
then we are returning a count of those edges 

32
22:59:55,580 --> 23:00:00,850
so let take a look in neo4j and the results of this query are a value of 14 

33
23:00:00,850 --> 23:00:04,938
and once again we can confirm this visually by looking at our network and

34
23:00:04,938 --> 23:00:06,720
counting the number of edges 

35
23:00:09,749 --> 23:00:15,290
the next query involves finding all of the leaf nodes in the network 

36
23:00:15,290 --> 23:00:17,535
now as you may remember from a previous lecture 

37
23:00:17,535 --> 23:00:22,810
leaf nodes are defined as those nodes which have no outgoing edges 

38
23:00:24,400 --> 23:00:28,300
notice that we are returning the node represented by the variable m 

39
23:00:28,300 --> 23:00:30,950
which is the target node in this query 

40
23:00:30,950 --> 23:00:35,750
so here we are matching all nodes associated with edges having the label two

41
23:00:36,780 --> 23:00:39,000
and we want to place a constraint on those nodes 

42
23:00:39,000 --> 23:00:42,180
such that they have no outgoing edges 

43
23:00:42,180 --> 23:00:43,930
and then we return all those nodes 

44
23:00:44,950 --> 23:00:47,170
so let take a look at the results 

45
23:00:47,170 --> 23:00:50,380
we are going to see a single node returned the node with the label p 

46
23:00:52,020 --> 23:00:57,490
and if we inspect our graph we can see over here on the right the node p 

47
23:00:57,490 --> 23:01:00,630
and sure enough it has no outgoing edges only one incoming edge 

48
23:01:04,290 --> 23:01:06,170
the next query is also similar 

49
23:01:06,170 --> 23:01:09,390
we are looking for root nodes instead of leaf notes 

50
23:01:09,390 --> 23:01:10,650
and as you may also remember 

51
23:01:10,650 --> 23:01:15,180
root nodes are defined as a node which has no incoming edges 

52
23:01:16,230 --> 23:01:20,550
you may notice that this segment of the first line of code is sort of a mirror

53
23:01:20,550 --> 23:01:26,040
image of the same segment in the first line of code from the previous query 

54
23:01:26,040 --> 23:01:30,540
we also want to place the constraint on the nodes that we want to return

55
23:01:30,540 --> 23:01:34,150
by specifying that they can have no incoming edges 

56
23:01:34,150 --> 23:01:38,340
and then we return all of those nodes when we look at our results in neo4j 

57
23:01:38,340 --> 23:01:40,420
once again 

58
23:01:40,420 --> 23:01:44,270
we see we only get a single node return it the node with the label h 

59
23:01:45,500 --> 23:01:49,640
and if we inspect our graph we see over here the node h and

60
23:01:49,640 --> 23:01:51,480
sure enough it has no incoming edges 

61
23:01:51,480 --> 23:01:55,382
only a single outgoing edge which means it a root node of the network 

62
23:01:58,388 --> 23:02:03,740
the next query can be described as a pattern matching query 

63
23:02:03,740 --> 23:02:06,990
we are looking for a pattern that we are describing as triangles 

64
23:02:06,990 --> 23:02:09,880
this is a pattern that a bit more complex than

65
23:02:09,880 --> 23:02:13,540
the patterns we have been looking for in our previous queries 

66
23:02:13,540 --> 23:02:18,750
a triangle can also be described as a three cycle consisting of three nodes and

67
23:02:18,750 --> 23:02:23,010
three edges where the beginning and end node are the same 

68
23:02:23,010 --> 23:02:27,420
so here we are matching a node a which goes through an edge to node b

69
23:02:27,420 --> 23:02:31,480
which goes through a second edge to a second node c and

70
23:02:31,480 --> 23:02:34,960
through a third edge back to the original node a 

71
23:02:34,960 --> 23:02:37,500
and then we return all of those notes 

72
23:02:37,500 --> 23:02:39,600
let look at our results 

73
23:02:39,600 --> 23:02:43,390
and here we see we have five distinct nodes returned 

74
23:02:43,390 --> 23:02:46,720
and we have two triangles or two three cycles 

75
23:02:46,720 --> 23:02:50,804
from d to e to g and from d to c to b 

76
23:02:53,868 --> 23:02:57,788
finally the last query we are going to execute with this data set 

77
23:02:57,788 --> 23:03:01,290
is going to explore the neighborhood of a particular node 

78
23:03:01,290 --> 23:03:03,220
in this case the node with the label d 

79
23:03:03,220 --> 23:03:07,290
and we are going to be looking for what we are calling second neighbors of d 

80
23:03:07,290 --> 23:03:11,480
this means nodes that are two nodes away from d 

81
23:03:11,480 --> 23:03:16,230
so the first line of code matches all nodes that are two nodes away

82
23:03:16,230 --> 23:03:18,130
from a specific node 

83
23:03:18,130 --> 23:03:23,150
and then the second line of codes specifies the actual node that we

84
23:03:23,150 --> 23:03:27,240
we want to consider by constraining its name to have the label d 

85
23:03:27,240 --> 23:03:30,240
and then we return those nodes 

86
23:03:30,240 --> 23:03:33,040
and here we are using the command distinct

87
23:03:33,040 --> 23:03:36,440
because we want to make sure that we do not return any duplicate nodes 

88
23:03:36,440 --> 23:03:38,650
all of our nodes must be unique 

89
23:03:39,760 --> 23:03:40,930
so let look at the results 

90
23:03:42,130 --> 23:03:47,040
and here we have a network that consists of nine nodes and 11 edges and

91
23:03:47,040 --> 23:03:53,710
we can see that each node is two nodes away from the node d 

92
23:03:53,710 --> 23:03:57,510
some nodes appear to be only one node away from the node d but

93
23:03:57,510 --> 23:04:01,890
we can get to those nodes indirectly through another node which means that

94
23:04:01,890 --> 23:04:04,660
they are not only a first neighbor but they are also a second neighbor 

95
23:04:06,390 --> 23:04:08,930
so we encourage you to play around with these queries 

96
23:04:08,930 --> 23:04:12,320
and make minor changes with them and see what the results might be 

1
22:01:53,190 --> 22:01:58,030
for our next three queries we are going to switch to the terrorist

2
22:01:58,030 --> 22:02:00,820
data set that we had used in our previous demonstration 

3
22:02:02,740 --> 22:02:07,480
our first query involves finding the types of a particular node 

4
22:02:07,480 --> 22:02:11,740
so as you may recall in the terrorist data set there were node types

5
22:02:11,740 --> 22:02:14,010
corresponding to a country 

6
22:02:14,010 --> 22:02:17,240
so in this case we want to match all nodes 

7
22:02:17,240 --> 22:02:20,420
where the name is equal to afghanistan 

8
22:02:20,420 --> 22:02:25,450
and we will return the labels for that particular match 

9
22:02:25,450 --> 22:02:27,990
and the results for this query are relatively simple 

10
22:02:27,990 --> 22:02:33,450
the label for the node named afghanistan is country as you might expect 

11
22:02:36,685 --> 22:02:40,650
next we are going to do something similar but with an edge 

12
22:02:40,650 --> 22:02:43,740
in this case we want to find the label

13
22:02:43,740 --> 22:02:48,690
of a edge associated with a node named afghanistan 

14
22:02:48,690 --> 22:02:53,230
and also if you can recall from that particular network the label for

15
22:02:53,230 --> 22:03:00,038
an edge associated with a country named afghanistan would be an is from label 

16
22:03:03,218 --> 22:03:08,330
and the final query we would like to demonstrate with this terrorist data set 

17
22:03:08,330 --> 22:03:11,500
involves finding all of the properties of a node 

18
22:03:12,590 --> 22:03:16,520
now based on how we defined our data import script 

19
22:03:17,540 --> 22:03:21,370
we defined all of our nodes to be of type actor 

20
22:03:21,370 --> 22:03:25,430
so in this query we are going to search for all nodes of type actor and

21
22:03:25,430 --> 22:03:30,700
then we are going to return all of the properties associated with those nodes 

22
22:03:30,700 --> 22:03:31,990
but since there are thousands of nodes 

23
22:03:31,990 --> 22:03:34,630
we are going to limit the results to the first 20 

24
22:03:34,630 --> 22:03:37,920
so let go ahead and submit our query 

25
22:03:38,970 --> 22:03:40,980
and here are the results 

26
22:03:40,980 --> 22:03:46,750
each node represents a terrorist and if we look at the rows data

27
22:03:46,750 --> 22:03:52,030
behind the scenes we can see the various properties associated with each node 

28
22:03:52,030 --> 22:03:53,920
each node has a name 

29
22:03:53,920 --> 22:03:57,330
each node has aliases one or more aliases 

30
22:03:57,330 --> 22:03:59,960
and they have a type property 

31
22:03:59,960 --> 22:04:03,090
now depending on how we defined our import script in the first place 

32
22:04:03,090 --> 22:04:06,820
these nodes could have different types of properties 

33
22:04:06,820 --> 22:04:09,950
in this case we only defined them to have three different properties 

34
22:04:14,091 --> 22:04:18,460
for our next two queries we are going to use a different data set 

35
22:04:18,460 --> 22:04:23,149
it a biological data set consisting of genetics data representing

36
22:04:23,149 --> 22:04:25,960
interactions between genes 

37
22:04:25,960 --> 22:04:28,690
we will be providing the data for you to download 

38
22:04:28,690 --> 22:04:31,280
in fact there are two separate data sets 

39
22:04:31,280 --> 22:04:36,150
one is a complete data set consisting of 250 000 rows 

40
22:04:36,150 --> 22:04:40,425
and the other data set consists of the first 50 000 rows 

41
22:04:40,425 --> 22:04:43,780
and that what we will be using for this demonstration 

42
22:04:43,780 --> 22:04:47,060
so let take a look at a small sample of the data 

43
22:04:47,060 --> 22:04:52,390
here you see a graph network in which each node is a gene 

44
22:04:52,390 --> 22:04:55,540
and each edge represents the association between genes 

45
22:04:57,200 --> 22:05:02,248
to load this data set this is the smaller

46
22:05:02,248 --> 22:05:07,152
data set required 1446 seconds 

47
22:05:07,152 --> 22:05:11,768
and it consists of 9656 labels and

48
22:05:11,768 --> 22:05:15,388
46621 relationships 

49
22:05:15,388 --> 22:05:20,133
so the first query involves finding loops in the data which represent genes

50
22:05:20,133 --> 22:05:22,980
that have associations with there own types 

51
22:05:24,200 --> 22:05:27,257
so it a very simple query in which the source node and

52
22:05:27,257 --> 22:05:31,335
the target node are the same and we will be returning those along the edges and

53
22:05:31,335 --> 22:05:33,644
we will limit our results to the first ten 

54
22:05:33,644 --> 22:05:35,920
so here are the results of our query 

55
22:05:35,920 --> 22:05:37,840
we can see a few different loops 

56
22:05:38,870 --> 22:05:44,600
if we look at the rows our query returned not only the node but the edge type 

57
22:05:45,610 --> 22:05:49,234
so on the left we see the particular gene and on the right 

58
22:05:49,234 --> 22:05:51,201
we see the type of association 

59
22:05:51,201 --> 22:05:53,616
and there are a range of different types 

60
22:05:57,749 --> 22:06:00,560
so this data set also contains multigraphs 

61
22:06:00,560 --> 22:06:04,520
if you recall from a previous lecture the definition of a multigraph

62
22:06:04,520 --> 22:06:09,370
is any two nodes which have two or more edges between them 

63
22:06:09,370 --> 22:06:14,370
so in this case we will be matching two separate node edge relationships 

64
22:06:14,370 --> 22:06:19,190
and we will apply a constraint in which the edges must be different for

65
22:06:19,190 --> 22:06:20,940
the same pairs of nodes 

66
22:06:20,940 --> 22:06:24,490
and then we will return those nodes and those edges 

67
22:06:24,490 --> 22:06:26,610
we will limit our results to the first ten 

68
22:06:26,610 --> 22:06:28,120
and here the results 

69
22:06:28,120 --> 22:06:33,377
here we see a set of four genes and there are two pairs that have three

70
22:06:33,377 --> 22:06:38,738
edges between them and another pair that has four edges between them 

71
22:06:42,360 --> 22:06:46,189
our final query addresses something that is not necessarily been fully covered

72
22:06:46,189 --> 22:06:49,329
in previous lectures but it is useful enough to address here and

73
22:06:49,329 --> 22:06:52,835
you will get an understanding of it by going through this query example 

74
22:06:54,285 --> 22:06:58,855
we are going to essentially extract a subset of nodes and

75
22:06:58,855 --> 22:07:02,205
edges from the graph that we have been working with 

76
22:07:02,205 --> 22:07:04,445
and this is called an induced subgraph 

77
22:07:05,470 --> 22:07:10,330
in which if we provide a set of nodes we want to return

78
22:07:10,330 --> 22:07:14,970
the network that consists only of those nodes and their associated edges 

79
22:07:14,970 --> 22:07:20,000
so my first line of code is very familiar we are matching nodes and

80
22:07:20,000 --> 22:07:22,910
edges it a very basic node and edge match 

81
22:07:24,570 --> 22:07:28,660
the second line of code is where the constraints are explicitly stated 

82
22:07:28,660 --> 22:07:33,540
a resulting network must consist of edges in which the source node

83
22:07:33,540 --> 22:07:38,520
must be constrained to this subset of node labels 

84
22:07:38,520 --> 22:07:43,180
and the target node must also be constrained to be a part of the subset of

85
22:07:43,180 --> 22:07:45,290
nodes with just these labels 

86
22:07:45,290 --> 22:07:47,410
and then we return the nodes and the edges 

87
22:07:48,890 --> 22:07:51,010
so let see the resulting graph 

88
22:07:51,010 --> 22:07:52,050
and here it is 

89
22:07:52,050 --> 22:07:53,040
so as we would expect 

90
22:07:53,040 --> 22:07:57,650
we are seeing the five nodes that were defined in our query constraints 

91
22:07:57,650 --> 22:08:01,850
and the edges corresponding to them retain the network structure 

92
22:08:01,850 --> 22:08:05,902
so this concludes our review of some of the basic queries you can do

93
22:08:05,902 --> 22:08:08,974
with neo4j using the cipher scripting language 

1
20:10:01,130 --> 20:10:04,808
next we will talk about connectivity analytics with cypher 

2
20:10:04,808 --> 20:10:06,713
if you remember in module two 

3
20:10:06,713 --> 20:10:11,810
we talked about connectivity analytics in terms of network robustness 

4
20:10:11,810 --> 20:10:15,590
in other words a measure of how resistant a graph network is to being disconnected 

5
20:10:16,820 --> 20:10:19,720
specifically we used two kinds of methods 

6
20:10:19,720 --> 20:10:24,790
one computed the eigenvalues and the second computed the degree distribution 

7
20:10:24,790 --> 20:10:29,150
for these examples we are going to use the second one degree distributions 

8
20:10:29,150 --> 20:10:32,710
and we will use the same graph network we have used previously 

9
20:10:32,710 --> 20:10:35,280
a simple graph representing a road network 

10
20:10:36,280 --> 20:10:40,534
and here a listing of the query examples we are going to be applying to our network 

11
20:10:57,639 --> 20:11:03,080
my first query example finds all of the outdegrees of all nodes 

12
20:11:03,080 --> 20:11:07,050
now if you will notice this query consists of two parts because there a specific

13
20:11:07,050 --> 20:11:12,940
type of node a leaf node which does not conform to this particular constraint 

14
20:11:12,940 --> 20:11:16,620
so our first match statement finds all nodes with outgoing edges 

15
20:11:16,620 --> 20:11:19,510
as you can see here this is a directed edge 

16
20:11:19,510 --> 20:11:21,930
and then we returns the names of the nodes and

17
20:11:21,930 --> 20:11:24,870
the count as the variable outdegree 

18
20:11:24,870 --> 20:11:27,610
and for convenience we order by outdegree 

19
20:11:27,610 --> 20:11:32,820
and we need to combine that with a specific query dealing with leaf nodes 

20
20:11:32,820 --> 20:11:36,820
we are familiar with how to do that from past examples 

21
20:11:36,820 --> 20:11:41,220
and so we will match all leaf nodes and return the name and

22
20:11:41,220 --> 20:11:43,950
the value zero for its outdegree 

23
20:11:43,950 --> 20:11:48,460
so when we submit this query we get this listing right here 

24
20:11:48,460 --> 20:11:52,792
the node p has 0 for its outdegree and all of the other nodes are as we might

25
20:11:52,792 --> 20:11:56,286
expect and they are ordered by their value of outdegree 

26
20:12:00,118 --> 20:12:02,695
our next query finds the indegree of all nodes 

27
20:12:02,695 --> 20:12:05,850
which is very similar to our previous example 

28
20:12:05,850 --> 20:12:07,360
but in this case as you might expect 

29
20:12:07,360 --> 20:12:12,240
we are going to take into account root nodes instead of leaf nodes 

30
20:12:12,240 --> 20:12:15,910
and so our match involves incoming edges 

31
20:12:15,910 --> 20:12:19,540
indegree is a measure of all nodes connected to a specific node with

32
20:12:19,540 --> 20:12:21,130
incoming edges 

33
20:12:21,130 --> 20:12:23,730
and we return similar results and

34
20:12:23,730 --> 20:12:29,030
we union that with the specific query commands to find all of the root nodes 

35
20:12:29,030 --> 20:12:33,090
and then we return those names and 0 as the value of indegree 

36
20:12:33,090 --> 20:12:37,820
so when we submit this query here our results as we might expect 

37
20:12:37,820 --> 20:12:40,690
in this case h is our only root node 

38
20:12:40,690 --> 20:12:43,264
so it has a value of 0 for indegree and

39
20:12:43,264 --> 20:12:46,072
all the other nodes are as we might expect 

40
20:12:49,438 --> 20:12:53,040
and our third query example finds the degree of all nodes 

41
20:12:53,040 --> 20:12:56,220
which is a combination of outdegree and indegree 

42
20:12:56,220 --> 20:13:02,010
so in this case we are not including any specific direction in our match statement 

43
20:13:02,010 --> 20:13:06,820
and we are returning the name and the count for all of our edges 

44
20:13:06,820 --> 20:13:09,540
but we are using the distinct statement 

45
20:13:09,540 --> 20:13:13,090
otherwise we would be counting some nodes twice 

46
20:13:13,090 --> 20:13:16,820
and then for convenience we order this by the value of degree 

47
20:13:16,820 --> 20:13:21,628
and when we submit this query we get the results as shown here 

48
20:13:21,628 --> 20:13:26,071
we have 1 column with the name and the other column with the degree and

49
20:13:26,071 --> 20:13:31,048
the values are as we would expect we have a leaf node p with the degree of 1 and

50
20:13:31,048 --> 20:13:33,128
a root node h with a degree of 1 

51
20:13:37,296 --> 20:13:43,262
our next query example generates a degree histogram of the graph since we are able to

52
20:13:43,262 --> 20:13:49,670
calculate the degree of each node we can sort those into actual values of degree 

53
20:13:49,670 --> 20:13:53,606
so if we look at the distribution of degree among our nodes 

54
20:13:53,606 --> 20:13:58,801
we see there 2 nodes with the degree 1 there 3 nodes with degree 2 

55
20:13:58,801 --> 20:14:04,280
there 4 nodes with degree 3 and there 2 nodes with degree 4 

56
20:14:04,280 --> 20:14:07,181
so we are going to group those in the form of a histogram 

57
20:14:07,181 --> 20:14:11,310
so when we submit this query we get this table 

58
20:14:11,310 --> 20:14:16,002
the first column list the degree value in ascending order and

59
20:14:16,002 --> 20:14:22,500
the second column list the counts of the nodes that have that degree value 

60
20:14:22,500 --> 20:14:27,451
so for those of you who are familiar with sql you might recognize this as similar

61
20:14:27,451 --> 20:14:30,989
to the group by command it performs a similar function

62
20:14:34,471 --> 20:14:38,980
our next query example saves the degree of the node as a new node property 

63
20:14:38,980 --> 20:14:44,000
this provides an added convenience so that we do not have to calculate the degree of

64
20:14:44,000 --> 20:14:47,330
a node every time we are performing some sort of analysis 

65
20:14:47,330 --> 20:14:50,640
so we match all nodes with edges and

66
20:14:50,640 --> 20:14:54,950
there no direction in this particular edge definition 

67
20:14:54,950 --> 20:14:59,090
and then we return distinct counts of each node degree and

68
20:14:59,090 --> 20:15:05,460
then we create a new property called deg and assign the value of degree to it 

69
20:15:05,460 --> 20:15:09,090
then we return the names and the degree values and so

70
20:15:09,090 --> 20:15:13,040
when we submit this query we see this distribution right here 

71
20:15:13,040 --> 20:15:17,690
with the names in the left column and the values of degree in the right column 

72
20:15:19,070 --> 20:15:23,740
and we can verify that if we issue a command to return all of the properties of

73
20:15:23,740 --> 20:15:25,250
the specific node 

74
20:15:25,250 --> 20:15:28,970
so in this case i issued a command to match the node named d and

75
20:15:28,970 --> 20:15:30,640
return all of its properties 

76
20:15:30,640 --> 20:15:34,272
and sure enough we see that it has a property name and a property degree 

77
20:15:37,569 --> 20:15:39,476
before we go to the last two examples 

78
20:15:39,476 --> 20:15:43,800
there a philosophical issue that we need to remember with all databases 

79
20:15:43,800 --> 20:15:47,780
every database will allow you some analytical computation and the remainder

80
20:15:47,780 --> 20:15:51,610
of the analytical computations must be done outside of the database 

81
20:15:51,610 --> 20:15:54,830
however it is always a judicious idea to get the database to

82
20:15:54,830 --> 20:15:58,683
achieve an intermediate result formatted in a way that you would need for

83
20:15:58,683 --> 20:15:59,962
the next computation 

84
20:15:59,962 --> 20:16:04,560
and then you use that intermediate result as the input to the next computation 

85
20:16:04,560 --> 20:16:07,290
we have seen that a number of computations in graph analytics

86
20:16:07,290 --> 20:16:10,040
start with the adjacency matrix 

87
20:16:10,040 --> 20:16:14,370
so we should be able to force cypher to produce an adjacency matrix 

88
20:16:14,370 --> 20:16:15,480
and this is what we are doing here 

89
20:16:16,980 --> 20:16:22,010
so think of a matrix as a three column table in which here one column 

90
20:16:22,010 --> 20:16:25,220
here another column and the third column will be the values that we

91
20:16:25,220 --> 20:16:30,140
are calculating when we determine whether two nodes have an edge between them 

92
20:16:30,140 --> 20:16:33,910
and we are introducing a new construct in cypher called case 

93
20:16:33,910 --> 20:16:38,320
this allows us to evaluate conditions and return one result or

94
20:16:38,320 --> 20:16:41,000
a different result depending on the condition 

95
20:16:41,000 --> 20:16:46,200
here we are specifying that when there is an edge between nodes n and

96
20:16:46,200 --> 20:16:50,890
m then we return a value of 1 otherwise return a value of 0 

97
20:16:50,890 --> 20:16:53,068
and we will output those results as a value 

98
20:16:53,068 --> 20:16:57,748
and so when we submit this query we get our three column table in which

99
20:16:57,748 --> 20:17:01,340
the first column is the name of our first node 

100
20:17:01,340 --> 20:17:05,077
the second column is the name of our second node and the value is either a 1 or

101
20:17:05,077 --> 20:17:09,250
a 1 depending on whether the nodes have an edge between them 

102
20:17:09,250 --> 20:17:12,839
so in this case we see node a and c have an edge 

103
20:17:12,839 --> 20:17:16,530
a and l have an edge and so on as we would expect 

104
20:17:20,050 --> 20:17:25,150
so if we can calculate the adjacency matrix then we can calculate any matrix 

105
20:17:25,150 --> 20:17:29,280
you might remember from our module two lecture where we

106
20:17:29,280 --> 20:17:32,760
learned about this complex structure called the normalized laplacian matrix 

107
20:17:33,970 --> 20:17:35,437
so let go ahead and calculate that 

108
20:17:35,437 --> 20:17:41,090
we will perform something very similar to what we did in the previous example 

109
20:17:41,090 --> 20:17:44,320
we will match all nodes for the first column and all nodes for

110
20:17:44,320 --> 20:17:46,080
the second column 

111
20:17:46,080 --> 20:17:51,390
we will return the names of those nodes and then we will use the case structure again

112
20:17:51,390 --> 20:17:55,280
to compare the names of each node and determine whether we have the same node 

113
20:17:56,590 --> 20:18:01,160
if we do have the same node then that is a diagonal of the matrix and

114
20:18:01,160 --> 20:18:01,945
should get a value of 1 

115
20:18:03,030 --> 20:18:06,264
if they are different nodes and contain an edge between them 

116
20:18:06,264 --> 20:18:09,948
then we calculate the normalized laplacian with this equation here 

117
20:18:09,948 --> 20:18:14,638
and you will also want to notice that here we are using the actual degree property

118
20:18:14,638 --> 20:18:18,210
that we assigned to the nodes in a previous example 

119
20:18:18,210 --> 20:18:22,080
this is an example of how that can become a convenient option 

120
20:18:22,080 --> 20:18:25,790
so when the calculation is performed the value would be returned 

121
20:18:25,790 --> 20:18:30,030
if there no edge between the 2 nodes then the value of 0 will be returned 

122
20:18:30,030 --> 20:18:32,194
and these values will end up in the value column 

123
20:18:32,194 --> 20:18:36,780
so when we submit this query here the table that get returned 

124
20:18:36,780 --> 20:18:39,662
this is the first column with the source node 

125
20:18:39,662 --> 20:18:43,370
the second column with the target node and the values 

126
20:18:43,370 --> 20:18:47,070
so in the first row the first node is p and the second node is p so

127
20:18:47,070 --> 20:18:50,600
it identical which means it on the diagonal of the matrix 

128
20:18:50,600 --> 20:18:53,630
likewise for a in this row down here 

129
20:18:53,630 --> 20:18:58,040
and then the first value of the laplacian is calculated between nodes a and

130
20:18:58,040 --> 20:18:59,180
c and so on 

131
20:19:01,310 --> 20:19:06,202
so that concludes our examples of how to perform connectivity analytics

132
20:19:06,202 --> 20:19:07,815
in neo4j with cypher 

1
16:29:07,430 --> 16:29:11,690
hello everyone and welcome to a series of lessons on graph analytics with neo4j 

2
16:29:12,730 --> 16:29:17,520
in this first tutorial we are going to go briefly through the process of downloading

3
16:29:17,520 --> 16:29:22,420
and installing neo4j and then we will go ahead and run neo4j and

4
16:29:23,560 --> 16:29:27,670
provide a brief review of the neo4j graphical interface 

5
16:29:27,670 --> 16:29:28,713
so let get started 

6
16:29:32,417 --> 16:29:37,480
to download neo4j you will want to browse to neo4j com and

7
16:29:37,480 --> 16:29:43,660
once you are at that url you should see a webpage similar to this 

8
16:29:43,660 --> 16:29:47,030
neo4j typically will detect which operating system you are running but

9
16:29:47,030 --> 16:29:50,340
if you want to download other versions of neo4j 

10
16:29:50,340 --> 16:29:53,990
click this link in the upper right - hand corner to download neo4j 

11
16:29:53,990 --> 16:29:58,030
we are going to use the community edition so we will click this link right here 

12
16:29:58,030 --> 16:29:59,950
but if you would like to explore other releases 

13
16:29:59,950 --> 16:30:01,360
you can click the link down here 

14
16:30:02,550 --> 16:30:05,760
when we click the download community edition link 

15
16:30:05,760 --> 16:30:09,260
neo4j should automatically begin the download 

16
16:30:09,260 --> 16:30:14,220
there are versions of neo4j for windows mac and linux 

17
16:30:14,220 --> 16:30:17,300
neo4j is developed with the java programming language so

18
16:30:17,300 --> 16:30:22,240
you will want to be sure you have the minimum system requirements to run neo4j 

19
16:30:22,240 --> 16:30:25,220
we will provide a link to those system requirements in the resources of

20
16:30:25,220 --> 16:30:25,750
this video 

21
16:30:26,820 --> 16:30:31,000
this example is with the mac os x operating system 

22
16:30:31,000 --> 16:30:34,829
and we have downloaded the dmg file and we can click to open that 

23
16:30:38,126 --> 16:30:42,037
and as is the case with the mac applications to install is very

24
16:30:42,037 --> 16:30:43,890
straightforward 

25
16:30:43,890 --> 16:30:47,590
we will just click and drag this icon into the applications folder and

26
16:30:47,590 --> 16:30:50,210
that should automatically install 

27
16:30:50,210 --> 16:30:51,820
once the application is installed 

28
16:30:51,820 --> 16:30:55,260
we should see it listed in the applications folder 

29
16:30:55,260 --> 16:30:57,149
so let go ahead and run neo4j 

30
16:31:01,491 --> 16:31:05,078
to run neo4j you will just double - click on the application icon 

31
16:31:05,078 --> 16:31:09,770
neo4j is browser - based so it launches a web server and

32
16:31:09,770 --> 16:31:14,050
it launches a url specific to neo4j 

33
16:31:14,050 --> 16:31:17,520
so to activate that server we will click the start button 

34
16:31:17,520 --> 16:31:21,940
if it your first time using neo4j then you probably wo not have any graph

35
16:31:21,940 --> 16:31:26,890
databases available to open so neo4j makes it easy for

36
16:31:26,890 --> 16:31:30,650
you by providing a default graph database of movie data 

37
16:31:31,660 --> 16:31:36,534
to run neo4j you click this link and that should launch your default browser 

38
16:31:40,598 --> 16:31:44,510
installation on windows should be just as straightforward as os x 

39
16:31:44,510 --> 16:31:48,380
you will click the same download neo4j button and

40
16:31:48,380 --> 16:31:52,900
when the download page opens you will click download community edition 

41
16:31:52,900 --> 16:31:58,390
neo4j should recognize you are running windows and should download an executable 

42
16:31:58,390 --> 16:32:03,340
once that downloads we can run it and we will step through the same steps

43
16:32:03,340 --> 16:32:06,150
which are typical for a windows operating system 

44
16:32:06,150 --> 16:32:09,491
now i already have neo4j installed in my windows system so

45
16:32:09,491 --> 16:32:12,161
i will be updating my existing installation 

46
16:32:16,538 --> 16:32:19,210
and neo4j downloads a java jar file 

47
16:32:20,340 --> 16:32:24,600
once the application installed it will open the dialog box and

48
16:32:24,600 --> 16:32:26,730
we can start up our web server 

49
16:32:26,730 --> 16:32:30,200
and the same url is available and when we click that our default

50
16:32:30,200 --> 16:32:34,427
browser should be launched and we should see the same interface we see in os x 

51
16:32:35,665 --> 16:32:39,810
you should be able to subsequently run neo4j by accessing the executable

52
16:32:39,810 --> 16:32:41,890
in the windows start menu 

53
16:32:41,890 --> 16:32:44,279
so let take a look at the neo4j interface 

54
16:32:47,797 --> 16:32:52,150
as i mentioned previously neo4j is a browser - based application 

55
16:32:52,150 --> 16:32:56,135
so once the application launches it should open your default browser and

56
16:32:56,135 --> 16:32:59,110
in my case i am using the chrome browser 

57
16:32:59,110 --> 16:33:03,250
and you should see a url like this in the address bar 

58
16:33:03,250 --> 16:33:07,740
most browsers provide a full - screen viewing mode so i am going to go ahead and

59
16:33:07,740 --> 16:33:11,560
activate that in this case what chrome calls presentation mode 

60
16:33:11,560 --> 16:33:15,020
the interface for neo4j consists of three main areas 

61
16:33:15,020 --> 16:33:19,600
there the command line along the top where you can issue commands and you can

62
16:33:19,600 --> 16:33:24,030
paste content into the command line and execute multiple lines at one time 

63
16:33:25,760 --> 16:33:30,330
you can specify these commands to be one of your favorites 

64
16:33:30,330 --> 16:33:32,600
you can clear the command box 

65
16:33:32,600 --> 16:33:35,738
and you can run or execute the commands with the play button 

66
16:33:35,738 --> 16:33:40,640
on the left - hand side is an expandable and collapsible panel that provides

67
16:33:40,640 --> 16:33:43,830
supplementary information about the database you are currently working with 

68
16:33:44,900 --> 16:33:47,550
there an option to list your favorites and

69
16:33:47,550 --> 16:33:52,020
neo4j provides an initial default listing of favorites 

70
16:33:52,020 --> 16:33:54,890
and there supplementary information and links to references and

71
16:33:54,890 --> 16:33:57,920
examples and tutorials and so on 

72
16:33:57,920 --> 16:34:02,990
you can also modify the configuration of your neo4j installation for

73
16:34:02,990 --> 16:34:04,570
your particular user 

74
16:34:04,570 --> 16:34:07,383
and there additional information about the application 

75
16:34:07,383 --> 16:34:11,631
the third area is the main panel in the middle where the results of all of

76
16:34:11,631 --> 16:34:14,760
the commands that you execute are displayed 

77
16:34:14,760 --> 16:34:18,220
in fact this panel itself has been generated by issuing a command 

78
16:34:18,220 --> 16:34:21,770
which is displayed in the very top of the panel 

79
16:34:21,770 --> 16:34:25,180
the command is ;play start 

80
16:34:25,180 --> 16:34:30,160
in fact if i type that command into my command line i should be able to generate

81
16:34:30,160 --> 16:34:33,088
a duplicate of that panel and sure enough there it is 

82
16:34:33,088 --> 16:34:39,310
the upper right - hand corner of these panels i can pin the panel to my webpage 

83
16:34:39,310 --> 16:34:41,590
and i can do this with as many panels as i want and

84
16:34:41,590 --> 16:34:44,820
the window will simply allow me to scroll up and down 

85
16:34:44,820 --> 16:34:47,520
i can view this particular panel full screen 

86
16:34:47,520 --> 16:34:50,430
or it can cancel or close it which is what i am going to do 

87
16:34:50,430 --> 16:34:56,530
now you can see neo4j makes it very easy to learn all about graph analytics 

88
16:34:56,530 --> 16:35:00,460
so let go ahead and get started with learning how to create and

89
16:35:00,460 --> 16:35:02,220
modify our first graph network 

1
09:04:11,654 --> 09:04:16,877
next we are going to get started with neo4j by creating our first graph network 

2
09:04:17,960 --> 09:04:20,940
to do this first we will review a graphical or

3
09:04:20,940 --> 09:04:24,010
a diagrammatic representation of a graph network 

4
09:04:24,010 --> 09:04:28,350
then we will introduce you to an equivalent text representation of that network 

5
09:04:28,350 --> 09:04:33,060
then we will build on those text representations in the form of pseudocode 

6
09:04:33,060 --> 09:04:37,240
with the ultimate goal being to develop an actual script

7
09:04:37,240 --> 09:04:39,010
to create our network in neo4j 

8
09:04:40,160 --> 09:04:44,423
then we will go ahead and run the script to create the network and we will explore it 

9
09:04:44,423 --> 09:04:46,333
and confirm it structure and content 

10
09:04:50,776 --> 09:04:56,800
you are looking at a simple graph network consisting of five nodes and five edges 

11
09:04:56,800 --> 09:05:00,050
each node represents a person an individual 

12
09:05:00,050 --> 09:05:03,450
and the edges represent relationships between those people 

13
09:05:03,450 --> 09:05:07,840
each node has a number associate with it n1 through n5 

14
09:05:07,840 --> 09:05:13,732
and each edge has a corresponding number associated with it e1 through e5 

15
09:05:13,732 --> 09:05:18,286
edges are relationships such as harry is known by tom 

16
09:05:18,286 --> 09:05:20,964
or julian is coworker of harry 

17
09:05:20,964 --> 09:05:25,460
we could have more or less edges but we wanted to keep things relatively simple 

18
09:05:25,460 --> 09:05:29,050
while still maintaining a reasonable degree of complexity 

19
09:05:29,050 --> 09:05:32,000
what we want is a script that we can

20
09:05:32,000 --> 09:05:37,270
process with neo4j in order to create an actual graph network 

21
09:05:37,270 --> 09:05:40,273
so let look at a text representation of this network 

22
09:05:44,494 --> 09:05:49,879
so here we list our five nodes and five edges and we are going to begin

23
09:05:49,879 --> 09:05:55,940
the process of extending the text descriptions of our graph network 

24
09:05:55,940 --> 09:05:59,210
i am going to scroll down briefly and just show you the end result so

25
09:05:59,210 --> 09:06:03,070
you have a better idea of what the final goal is going to be 

26
09:06:03,070 --> 09:06:07,854
this is the actual code that we are going to submit to neo4j in order to create

27
09:06:07,854 --> 09:06:08,745
our network 

28
09:06:08,745 --> 09:06:11,425
but we are going to back up a little bit and

29
09:06:11,425 --> 09:06:14,615
look at some simplified versions of this syntax so

30
09:06:14,615 --> 09:06:19,627
we can better understand how simple the graph network relationships really are 

31
09:06:22,374 --> 09:06:27,930
here we list the five nodes and five edges as you saw just a moment ago 

32
09:06:27,930 --> 09:06:32,340
and down below we have a simple notation structure 

33
09:06:32,340 --> 09:06:35,490
which attempts to describe the five edge relationships 

34
09:06:35,490 --> 09:06:39,600
the first line represents the edge e1 

35
09:06:39,600 --> 09:06:43,820
we can see that the nodes n1 and n2 are included 

36
09:06:43,820 --> 09:06:49,410
because harry is known by tom and tom is node n1 and harry is node n2 

37
09:06:49,410 --> 09:06:53,950
the same goes for the second line the relationship between julian and harry 

38
09:06:53,950 --> 09:06:56,770
julian is co - worker of harry and so on 

39
09:06:57,890 --> 09:07:02,830
so let take this a step further by defining our graph network 

40
09:07:02,830 --> 09:07:06,300
so that each node is a particular type of node 

41
09:07:06,300 --> 09:07:12,410
in this case we are going to define a node type as what we are calling a toynode 

42
09:07:12,410 --> 09:07:17,040
as we introduce each node and its relationship with other nodes 

43
09:07:17,040 --> 09:07:20,248
we will define the node to be of type toynode 

44
09:07:20,248 --> 09:07:25,181
so on this first line n1 goes through e1 to n2 

45
09:07:25,181 --> 09:07:27,822
and both of those are introduced for the first time so

46
09:07:27,822 --> 09:07:30,370
we will define them as type toynode 

47
09:07:30,370 --> 09:07:34,380
but on the next line since we already introduced n2 as type toynode 

48
09:07:34,380 --> 09:07:35,970
we do not need to repeat that statement 

49
09:07:37,000 --> 09:07:42,200
and so we continue in the same manner with the remaining edge relationships 

50
09:07:42,200 --> 09:07:47,860
taking this even further we will apply a similar kind of constraint to our edges 

51
09:07:47,860 --> 09:07:53,880
in this case we will define our network such that each edge is a particular type 

52
09:07:53,880 --> 09:07:56,120
which we are calling toyrelation 

53
09:07:56,120 --> 09:07:59,510
next we are going to add properties to our nodes and edges 

54
09:07:59,510 --> 09:08:03,070
our nodes can have properties such as name or job 

55
09:08:03,070 --> 09:08:08,850
so in this case our first node n1 will have the name tom 

56
09:08:08,850 --> 09:08:13,700
and the appropriate syntax for this includes curly braces surrounding the key

57
09:08:13,700 --> 09:08:18,570
value pairs a colon separating the key value pairs and

58
09:08:18,570 --> 09:08:22,500
the values defined within single quotes 

59
09:08:22,500 --> 09:08:26,738
likewise each edge may have a specific type of relationship 

60
09:08:26,738 --> 09:08:29,722
including co - worker wife and friend 

61
09:08:31,534 --> 09:08:35,499
so finally this brings us to the actual code we are going to use to create our

62
09:08:35,499 --> 09:08:37,130
graph network 

63
09:08:37,130 --> 09:08:40,057
so let go ahead and copy this and paste it into neo4j so

64
09:08:40,057 --> 09:08:41,587
we can take a look at our network 

65
09:08:44,295 --> 09:08:47,410
here we are running neo4j in our browser 

66
09:08:47,410 --> 09:08:52,830
so i am going to paste the code that i just copied from my text file

67
09:08:52,830 --> 09:08:55,360
into the command line in the neo4j interface 

68
09:08:56,450 --> 09:08:59,970
and i will click the play button to execute those commands and

69
09:08:59,970 --> 09:09:04,250
we will see the results returned in this newly displayed panel 

70
09:09:04,250 --> 09:09:09,320
we can see that we have 5 labels added 5 nodes were created 13 properties were

71
09:09:09,320 --> 09:09:15,680
set 5 relationships were created and the entire process required 31 milliseconds 

72
09:09:18,000 --> 09:09:23,550
however we still cannot actually view our graph unless we issue yet another command 

73
09:09:23,550 --> 09:09:28,640
so let shuffle to our text file and take a look at that command 

74
09:09:28,640 --> 09:09:32,910
what this command does is it tries to identify a match

75
09:09:32,910 --> 09:09:38,600
in which a particular node has a relationship with any other node 

76
09:09:38,600 --> 09:09:42,180
and then we will return those nodes and relationships 

77
09:09:42,180 --> 09:09:45,020
so we will go ahead and copy this and

78
09:09:45,020 --> 09:09:49,280
we will paste it in to our command line and we will execute 

79
09:09:49,280 --> 09:09:50,970
and there our graph 

80
09:09:50,970 --> 09:09:56,360
when we mouse over the nodes we can see information displayed on the bottom 

81
09:09:56,360 --> 09:10:00,770
and when we select those nodes that information is displayed permanently along

82
09:10:00,770 --> 09:10:04,810
the bottom likewise with edge relationships 

83
09:10:04,810 --> 09:10:10,536
so we would expect to see things such as michelle is the wife of harry 

84
09:10:10,536 --> 09:10:15,110
julian is a co - worker of harry and so on 

85
09:10:15,110 --> 09:10:18,310
so it looks like our network has been created successfully 

86
09:10:18,310 --> 09:10:21,980
we can also display information in a tabular format

87
09:10:21,980 --> 09:10:24,190
by clicking the rows icon on the left 

88
09:10:24,190 --> 09:10:28,030
and we will see that this information is not constrained by the directed

89
09:10:28,030 --> 09:10:29,320
nature of our graph 

90
09:10:29,320 --> 09:10:32,585
for example we see that harry has a relationship with tom 

91
09:10:32,585 --> 09:10:34,910
in which harry knows tom 

92
09:10:34,910 --> 09:10:38,386
but likewise down here tom has a relationship with harry 

93
09:10:38,386 --> 09:10:39,840
in which tom knows harry 

94
09:10:41,400 --> 09:10:44,620
next we are going to learn how to add to this graph and

95
09:10:44,620 --> 09:10:47,777
modify some of the properties of the nodes and edges 

1
18:14:57,100 --> 18:15:01,230
in this segment we are going to learn how to import data into neo4j 

2
18:15:02,780 --> 18:15:07,440
we will begin by using a fairly simple spreadsheet consisting of only a few

3
18:15:07,440 --> 18:15:11,550
rows and three columns in a format that fairly typical for

4
18:15:11,550 --> 18:15:13,630
importing into a graph database 

5
18:15:13,630 --> 18:15:19,180
we will review the neo4j cypher script used to perform this import 

6
18:15:19,180 --> 18:15:22,340
and then we will run the script and validate the resulting graph 

7
18:15:22,340 --> 18:15:24,790
then we will demonstrate a similar process but

8
18:15:24,790 --> 18:15:28,060
with a more challenging dataset consisting of terrorist network data 

9
18:15:29,210 --> 18:15:31,060
we will review this dataset and

10
18:15:31,060 --> 18:15:34,900
the script commands necessary for performing the import 

11
18:15:34,900 --> 18:15:37,100
and we will run the script and explore the resulting graph 

12
18:15:38,180 --> 18:15:42,953
and finally we will review a third dataset that you will use yourself to

13
18:15:42,953 --> 18:15:45,780
perform similar data inport operations 

14
18:15:50,323 --> 18:15:53,440
first let take a look at our sample dataset 

15
18:15:53,440 --> 18:15:57,890
this spreadsheet consists of just a few rows and three columns 

16
18:15:57,890 --> 18:15:59,480
each column has a heading 

17
18:15:59,480 --> 18:16:04,034
the first column heading is source the second column heading is target and

18
18:16:04,034 --> 18:16:06,324
the third column heading is distance 

19
18:16:06,324 --> 18:16:10,762
you can imagine that this might represent data from a simple road network in which

20
18:16:10,762 --> 18:16:13,570
the source and target values represent towns and

21
18:16:13,570 --> 18:16:17,963
the distance values represent the actual distance in miles between the towns 

22
18:16:21,333 --> 18:16:24,492
so let take a look at the neo4j cypher script and

23
18:16:24,492 --> 18:16:28,110
see what we will need to do to import our spreadsheet data 

24
18:16:28,110 --> 18:16:32,010
the first line of code performs the actual import 

25
18:16:32,010 --> 18:16:36,340
the other three lines of code provide constraints to the formatting

26
18:16:36,340 --> 18:16:37,670
of that data 

27
18:16:37,670 --> 18:16:41,350
since our data file is a comma separated values or

28
18:16:41,350 --> 18:16:45,170
csv file we will need to specify that in our code 

29
18:16:45,170 --> 18:16:47,530
our file also contains headers 

30
18:16:47,530 --> 18:16:49,718
and if you are using a windows operating system 

31
18:16:49,718 --> 18:16:51,758
your file path will look something like this 

32
18:16:51,758 --> 18:16:56,609
since we are running neo4j in a browser the file path needs

33
18:16:56,609 --> 18:17:01,365
to conform to the http address conventions of the word file 

34
18:17:01,365 --> 18:17:06,030
followed by a colon followed by three forward slashes and

35
18:17:06,030 --> 18:17:12,307
then the hard disk letter where the file is located plus the path to that file 

36
18:17:12,307 --> 18:17:16,046
if you are using mac osx your command will be similar but

37
18:17:16,046 --> 18:17:20,490
the file path will probably look something more like this 

38
18:17:20,490 --> 18:17:24,280
the next three lines specify which nodes will be the source nodes and

39
18:17:24,280 --> 18:17:26,810
which nodes will be the target nodes 

40
18:17:26,810 --> 18:17:32,020
and the properties will attach to them as well as defining the relationships and

41
18:17:32,020 --> 18:17:34,860
the properties we will attach to the relationships 

42
18:17:34,860 --> 18:17:39,800
as we read each line of data n we are going to use keyword line 

43
18:17:39,800 --> 18:17:43,330
to specify the individual line we are currently working on 

44
18:17:43,330 --> 18:17:47,120
we use that here at the end of our load command and so

45
18:17:47,120 --> 18:17:50,800
we will have to continue to use it in the subsequent merge commands 

46
18:17:52,000 --> 18:17:54,990
our source node variable is going to be n and

47
18:17:54,990 --> 18:17:59,010
we will use the mynode type which we have made up ourselves 

48
18:17:59,010 --> 18:18:03,150
we are going to add a name property to each of our source nodes 

49
18:18:03,150 --> 18:18:08,880
and we are going to attach the value in the source column to that particular node 

50
18:18:08,880 --> 18:18:13,350
likewise for our target nodes we will use a variable m 

51
18:18:13,350 --> 18:18:15,930
we will define them as the same type mynode 

52
18:18:17,330 --> 18:18:21,820
we will give them a name and we will extract that name from the target column

53
18:18:21,820 --> 18:18:24,300
on the particular line we are working with 

54
18:18:24,300 --> 18:18:27,590
finally we need to define our edge relationships 

55
18:18:27,590 --> 18:18:30,870
we are going to give each edge a label of the word to 

56
18:18:30,870 --> 18:18:36,360
and we are going to add a property called dist which represents the distance 

57
18:18:36,360 --> 18:18:39,150
and we will attach the values in the distance column

58
18:18:39,150 --> 18:18:41,950
from the particular line we are currently working on 

59
18:18:41,950 --> 18:18:48,050
so let go ahead and copy this code and paste it into neo4j and see what happens 

60
18:18:48,050 --> 18:18:50,868
now i am working on a mac osx machine so

61
18:18:50,868 --> 18:18:55,663
i am going to copy this code down here and perform the import operation 

62
18:18:58,380 --> 18:19:03,986
so i have pasted the line of code into neo4j and

63
18:19:03,986 --> 18:19:06,740
i will click execute 

64
18:19:06,740 --> 18:19:08,190
and it takes a few moments to run 

65
18:19:08,190 --> 18:19:12,410
there not too much data so it should not take more than just a few seconds 

66
18:19:12,410 --> 18:19:16,794
and now we get the results in which 11 labels 11 nodes have been added 

67
18:19:16,794 --> 18:19:21,057
25 properties have been set and 14 relationships have been created 

68
18:19:21,057 --> 18:19:23,320
so let take a look at this graph network 

69
18:19:23,320 --> 18:19:27,540
we can see that the nodes are listed all as mynode and there our graph 

70
18:19:28,590 --> 18:19:32,345
the edge relationships should all have different distance values and

71
18:19:32,345 --> 18:19:34,833
each node should be named a different letter 

72
18:19:34,833 --> 18:19:37,887
okay so let try a more difficult dataset 

73
18:19:40,849 --> 18:19:44,135
here is the spreadsheet containing terrorist data 

74
18:19:44,135 --> 18:19:49,175
the spreadsheet consists of seven columns with headings such as country 

75
18:19:49,175 --> 18:19:53,897
actorname actortype affiliationto affiliationstartdate 

76
18:19:53,897 --> 18:20:00,300
affiliationenddate and any aliases associated with that particular terrorist 

77
18:20:00,300 --> 18:20:05,880
this dataset consists of over 100 000 rows of data 

78
18:20:05,880 --> 18:20:10,350
since that could take a very long time to load into neo4j we will be

79
18:20:10,350 --> 18:20:15,980
working with a subset of this dataset consisting of the first 1 000 rows 

80
18:20:15,980 --> 18:20:19,246
that much data will still include three countries 

81
18:20:19,246 --> 18:20:22,296
which should be plenty of data for our purposes 

82
18:20:25,953 --> 18:20:30,911
so here is the script we are going to use to import the subset of terrorist data 

83
18:20:30,911 --> 18:20:35,420
which shares similarities with the script we used previously 

84
18:20:35,420 --> 18:20:38,030
but since there are more columns in our dataset 

85
18:20:38,030 --> 18:20:42,810
we are going to include some additional properties into our graph network 

86
18:20:42,810 --> 18:20:46,160
the first line of code is very similar

87
18:20:46,160 --> 18:20:49,610
to the load command we have used in previous datasets 

88
18:20:49,610 --> 18:20:56,720
the second line of code will use a variable c and a label country for

89
18:20:56,720 --> 18:21:00,155
the particular nodes representing the individual countries in the dataset 

90
18:21:01,300 --> 18:21:05,280
in this particular case we are going to use the keyword row instead of line to

91
18:21:05,280 --> 18:21:07,990
read our data n but it really does not matter 

92
18:21:07,990 --> 18:21:12,397
we could use either word as long we are consistent from command to command 

93
18:21:12,397 --> 18:21:17,136
so we are using the term row and associating the value in the country

94
18:21:17,136 --> 18:21:21,891
column with the node that we are working on in that particular row 

95
18:21:21,891 --> 18:21:26,375
we will do something similar with nodes that are intended to represent

96
18:21:26,375 --> 18:21:29,570
the actors or the actual individual terrorists 

97
18:21:29,570 --> 18:21:35,670
we are going to use a variable a and we will associate a property

98
18:21:35,670 --> 18:21:41,140
called name and associate the actorname with that property 

99
18:21:41,140 --> 18:21:45,410
we will also associate a property named aliases and

100
18:21:45,410 --> 18:21:49,250
associate the value in the aliases column with that property 

101
18:21:50,590 --> 18:21:55,580
and finally we will define a property called type and

102
18:21:55,580 --> 18:21:59,580
associate the values in the actortype column with that property 

103
18:22:00,650 --> 18:22:04,110
we are going to create nodes representing organizations as well and

104
18:22:04,110 --> 18:22:09,700
we will use the variable o and the label organization for those nodes 

105
18:22:09,700 --> 18:22:14,800
we will attach a single property to these nodes called name and

106
18:22:14,800 --> 18:22:19,968
we will assign the values in the affiliationto column to that property 

107
18:22:19,968 --> 18:22:23,862
then we are going to define relationships between the actors and

108
18:22:23,862 --> 18:22:26,700
the organizations they are affiliated with 

109
18:22:28,030 --> 18:22:31,835
the relationship label is going to be affiliated to and

110
18:22:31,835 --> 18:22:35,350
we will define a property called start 

111
18:22:35,350 --> 18:22:39,525
and we will assign the values in the affiliationstartdate with that property 

112
18:22:39,525 --> 18:22:44,210
likewise we will define a property called end and

113
18:22:44,210 --> 18:22:48,800
assign the values in the affiliationenddate with that property 

114
18:22:48,800 --> 18:22:53,500
and finally we are going to create relationships between the countries and

115
18:22:53,500 --> 18:22:55,160
the actors 

116
18:22:55,160 --> 18:23:00,160
in this case we will define relationships with the label is from that will

117
18:23:00,160 --> 18:23:05,890
describe the fact that a particular actor is from a particular country 

118
18:23:05,890 --> 18:23:09,711
so if all this makes sense let go ahead and copy this script and

119
18:23:09,711 --> 18:23:12,250
paste it into neo4j and see the results 

120
18:23:15,270 --> 18:23:18,926
so here we are in neo4j and i am going to go ahead and

121
18:23:18,926 --> 18:23:23,619
paste that script into the command line and we will execute it 

122
18:23:25,951 --> 18:23:33,021
we loaded 1 000 rows of data which consists of 658 labels and 658 nodes 

123
18:23:33,021 --> 18:23:38,401
3 464 properties and 1 403 relationships and

124
18:23:38,401 --> 18:23:41,809
it took about two and a half seconds 

125
18:23:41,809 --> 18:23:46,376
so let look at a small subset of this network 

126
18:23:51,795 --> 18:23:57,090
here we see the equivalent of the first 25 rows of the dataset 

127
18:23:57,090 --> 18:24:01,120
there only enough data such that 1 country 

128
18:24:01,120 --> 18:24:05,067
8 actors and 15 organizations are visible 

129
18:24:05,067 --> 18:24:08,874
let go ahead and change this command to 250 

130
18:24:08,874 --> 18:24:12,550
by clicking on the line of code in the top of our panel 

131
18:24:12,550 --> 18:24:16,653
it automatically gets pasted into the command line above 

132
18:24:16,653 --> 18:24:22,085
so all we need to do is add a zero on the end of our command and execute that again 

133
18:24:22,085 --> 18:24:26,752
now we have a much larger more complex graph 

134
18:24:26,752 --> 18:24:30,262
but we still only have one country 

135
18:24:30,262 --> 18:24:32,918
in order to see more than one country 

136
18:24:32,918 --> 18:24:39,010
we will need to render the entire 1 000 rows of our terrorist data subset 

137
18:24:39,010 --> 18:24:42,710
but in so doing we will have a difficult time viewing the entire graph 

138
18:24:43,720 --> 18:24:47,950
the community edition of neo4j is limited in its ability to

139
18:24:47,950 --> 18:24:49,810
navigate a graph network 

140
18:24:49,810 --> 18:24:52,440
but i am going to show you a little trick by editing

141
18:24:52,440 --> 18:24:56,290
the html behind the scenes to scale the view of our graph 

142
18:24:57,770 --> 18:25:00,862
so let go ahead and render all 1 000 rows 

143
18:25:06,923 --> 18:25:11,473
so most recent versions of the major browsers provide the ability to go behind

144
18:25:11,473 --> 18:25:14,080
the scenes and edit the html 

145
18:25:14,080 --> 18:25:19,024
so the trick is to find a place on the viewing panel that does not have

146
18:25:19,024 --> 18:25:20,096
any objects 

147
18:25:20,096 --> 18:25:23,971
so that when you right - click and inspect the element 

148
18:25:23,971 --> 18:25:26,864
you will be inspecting the viewing area 

149
18:25:26,864 --> 18:25:29,257
neo4j uses svg graphics or

150
18:25:29,257 --> 18:25:34,360
scalable vector graphics to render its graph networks 

151
18:25:34,360 --> 18:25:38,373
and svg uses a g element which can be seen here on the right 

152
18:25:38,373 --> 18:25:44,452
so in order to change the scale of our view we simply need to double - click and

153
18:25:44,452 --> 18:25:49,903
add scale open parentheses and the scale factor that we would like 

154
18:25:49,903 --> 18:25:56,938
we hit return and the graph network now is zoomed out 

155
18:25:56,938 --> 18:26:02,823
i am going to try to position it so we can see at least two countries 

156
18:26:02,823 --> 18:26:05,610
and i will close my html panel 

157
18:26:06,920 --> 18:26:08,371
and so now we can view two countries 

158
18:26:08,371 --> 18:26:14,456
we can see albania in the upper region and afghanistan in the lower right 

159
18:26:14,456 --> 18:26:17,388
and we can see that there are actors and

160
18:26:17,388 --> 18:26:22,970
organizations that have relationships with both countries 

161
18:26:22,970 --> 18:26:27,384
now there are add ons to neo4j that make navigating a graph network a little

162
18:26:27,384 --> 18:26:28,155
easier but

163
18:26:28,155 --> 18:26:33,074
this trick is convenient for those of you who have not added any neo4j extensions 

164
18:26:36,998 --> 18:26:41,355
the last thing that we are going to do is take a look at the sample dataset of

165
18:26:41,355 --> 18:26:46,208
gene - disease associations and give you an idea what is going to be expected of

166
18:26:46,208 --> 18:26:49,930
you in the accompanying assignment for this module 

167
18:26:49,930 --> 18:26:54,060
this data consists of information associating

168
18:26:54,060 --> 18:26:57,200
different genes with different diseases 

169
18:26:57,200 --> 18:27:01,638
the spreadsheet consists of columns with headings for geneid 

170
18:27:01,638 --> 18:27:06,610
genesymbol genename diseaseid 

171
18:27:06,610 --> 18:27:11,870
the diseasename the score that represents the extent to which that gene

172
18:27:11,870 --> 18:27:15,302
is associated with that particular disease 

173
18:27:15,302 --> 18:27:21,554
the numberofpubmed articles containing that information 

174
18:27:21,554 --> 18:27:22,944
the associationtypes 

175
18:27:22,944 --> 18:27:27,584
there are up to three different types of associations between a gene and a disease 

176
18:27:27,584 --> 18:27:30,113
and then the sources of data and

177
18:27:30,113 --> 18:27:35,084
information that confirm this gene disease relationship 

178
18:27:35,084 --> 18:27:40,830
now this dataset contains over 400 000 rows of data 

179
18:27:40,830 --> 18:27:44,322
so if you have difficulty importing this entire dataset 

180
18:27:44,322 --> 18:27:49,810
then you will be better off extracting the first few thousand rows 

181
18:27:49,810 --> 18:27:53,507
so you are goal will be to define the load statement 

182
18:27:53,507 --> 18:27:58,437
which includes a csv with headers that will allow you to import enough

183
18:27:58,437 --> 18:28:03,384
data into neo4j to give you an idea that you have done it successfully 

1
12:43:00,730 --> 12:43:04,250
we have already learned a little about the neo4j interface 

2
12:43:04,250 --> 12:43:07,200
and we have learned how to create a relatively simple graph with it 

3
12:43:08,390 --> 12:43:09,240
in this lecture 

4
12:43:09,240 --> 12:43:13,660
we are going to learn how to add another node and an edge to the graph 

5
12:43:15,210 --> 12:43:18,880
we will also go through the process of adding a node and

6
12:43:18,880 --> 12:43:20,230
edge incorrectly to the graph 

7
12:43:21,360 --> 12:43:23,710
and we will learn how to correct that mistake 

8
12:43:23,710 --> 12:43:27,874
and finally we will learn how to modify an existing node information 

9
12:43:33,748 --> 12:43:38,275
okay so here our network that we created in the previous lecture 

10
12:43:38,275 --> 12:43:40,790
the first modification that we want to make 

11
12:43:40,790 --> 12:43:42,980
is adding a single node to the network 

12
12:43:44,080 --> 12:43:48,440
so let say that julian has a fiance and

13
12:43:48,440 --> 12:43:52,380
her name is joyce and she works as a store clerk 

14
12:43:52,380 --> 12:43:56,090
so let look at the code that we are going to use to make that modification 

15
12:43:57,340 --> 12:44:00,940
so this process involves two steps or two separate commands 

16
12:44:00,940 --> 12:44:05,780
first command requires you to find the node that you want to add the new node to 

17
12:44:05,780 --> 12:44:11,689
so we use the match command and we specify the toynode named julian 

18
12:44:11,689 --> 12:44:16,716
once that command is issued then we will use the merge command and

19
12:44:16,716 --> 12:44:21,370
define the relation between julian and the new node 

20
12:44:21,370 --> 12:44:24,385
and it going to be fiancee 

21
12:44:24,385 --> 12:44:27,920
and then the new node will also be a toynode 

22
12:44:27,920 --> 12:44:31,680
and the name is joyce and her job is store clerk 

23
12:44:31,680 --> 12:44:35,537
so let go ahead and copy both these lines of code 

24
12:44:35,537 --> 12:44:40,430
and we will paste them into our command line and we will run these commands 

25
12:44:40,430 --> 12:44:43,112
and the results that are returned look good 

26
12:44:43,112 --> 12:44:49,057
neo4j says that it has added 1 label created 1 node set 3 properties 

27
12:44:49,057 --> 12:44:55,102
created 1 relationship and it required 55 milliseconds to execute 

28
12:44:55,102 --> 12:44:57,240
so let look at that network 

29
12:44:57,240 --> 12:44:59,110
maybe the easiest way to view an existing network 

30
12:44:59,110 --> 12:45:03,900
if it the only one you are working on and you know the constraints involved 

31
12:45:03,900 --> 12:45:09,290
just toynodes is to expand the panel on the left and just click the toynode node 

32
12:45:09,290 --> 12:45:12,550
and you will easily see the new network 

33
12:45:12,550 --> 12:45:15,600
and we can confirm that joyce has been successfully added and

34
12:45:15,600 --> 12:45:20,220
when we select that we can see that her job is store clerk 

35
12:45:20,220 --> 12:45:25,090
the command that was issued automatically by clicking the toynode button in

36
12:45:25,090 --> 12:45:30,270
the panel is a little different than what we have been using to view our networks 

37
12:45:30,270 --> 12:45:33,310
but they both work essentially the same with this particular network 

38
12:45:34,340 --> 12:45:37,888
so next let see what happens if we do something incorrectly and

39
12:45:37,888 --> 12:45:39,389
how to correct the mistake 

40
12:45:44,521 --> 12:45:45,313
so to do this 

41
12:45:45,313 --> 12:45:49,282
we will need to get back to our original network without the added node 

42
12:45:49,282 --> 12:45:53,198
and maybe the easiest way to do this is to delete everything and

43
12:45:53,198 --> 12:45:55,320
recreate our original network 

44
12:45:56,370 --> 12:45:58,470
so first i am going to copy and

45
12:45:58,470 --> 12:46:02,060
paste command to delete all of the nodes and edges 

46
12:46:02,060 --> 12:46:07,440
you will find this command in the getting started video supplementary resources 

47
12:46:08,600 --> 12:46:12,260
it involves the match command and i am matching all nodes and

48
12:46:12,260 --> 12:46:16,620
all relationships and i am deleting those nodes and relationships 

49
12:46:16,620 --> 12:46:21,173
so let issue that command which should say it deleted 6 nodes and

50
12:46:21,173 --> 12:46:23,532
6 relationships and sure enough 

51
12:46:23,532 --> 12:46:29,109
neo4j command line has a nice feature of maintaining a history of the commands and

52
12:46:29,109 --> 12:46:33,910
so in osx we can use the command + up arrow to cycle through the commands and

53
12:46:33,910 --> 12:46:38,160
find the original create command that created our network 

54
12:46:38,160 --> 12:46:40,950
on windows it a ctrl + up arrow command 

55
12:46:40,950 --> 12:46:45,320
so here i have found that command and i am going to re - execute it 

56
12:46:45,320 --> 12:46:48,100
and we have our original network back again 

57
12:46:48,100 --> 12:46:49,390
let view that network 

58
12:46:51,931 --> 12:46:52,810
and there it is 

59
12:46:54,220 --> 12:46:59,480
so let say for whatever reason i am not quite fully understanding how to add

60
12:46:59,480 --> 12:47:04,380
an additional node to an existing network and i want to use the create command 

61
12:47:06,190 --> 12:47:08,870
for example i might be thinking that

62
12:47:08,870 --> 12:47:12,370
this statement right here will accomplish the same kind of thing 

63
12:47:12,370 --> 12:47:17,930
by using the create command and by specifying a toynode named julian 

64
12:47:19,070 --> 12:47:22,370
well if we go ahead and give that a try let see what happens 

65
12:47:22,370 --> 12:47:28,700
so i will copy this command and i will paste it into my command line and execute it 

66
12:47:30,500 --> 12:47:35,530
and it says it added 2 labels and created 2 nodes and set 4 properties 

67
12:47:36,680 --> 12:47:41,806
and if we look at this we

68
12:47:41,806 --> 12:47:48,240
see we have actually got another node named julian who has a relationship with joyce 

69
12:47:48,240 --> 12:47:53,970
so that much is correct but it not the same julian from the original network 

70
12:47:53,970 --> 12:47:55,110
so how do we undo this 

71
12:47:56,200 --> 12:48:02,200
well we will need to specify the julian that has a relationship with joyce and

72
12:48:02,200 --> 12:48:04,760
delete both the julian and the joyce notes 

73
12:48:07,750 --> 12:48:11,160
so here we will use the match command once again 

74
12:48:11,160 --> 12:48:17,400
and we will identify the node the toynode with the name joyce and any relationship

75
12:48:17,400 --> 12:48:23,540
she has with any other node should be deleted in addition to that other node 

76
12:48:23,540 --> 12:48:25,665
let go ahead and copy this 

77
12:48:25,665 --> 12:48:31,000
and we will paste it into our command line and execute 

78
12:48:31,000 --> 12:48:33,840
and it says it deleted 2 nodes and deleted 1 relationship 

79
12:48:34,920 --> 12:48:36,740
let view our network again 

80
12:48:38,850 --> 12:48:40,660
and it back to normal 

81
12:48:40,660 --> 12:48:45,416
so that one example of how you can intuitively figure out how to correct

82
12:48:45,416 --> 12:48:46,731
certain mistakes 

83
12:48:52,826 --> 12:48:57,400
next we are going to modify information of an existing node 

84
12:48:57,400 --> 12:49:03,790
so if you remember when we first created our network harry did not have a job 

85
12:49:03,790 --> 12:49:07,370
so let go ahead and add a job to harry 

86
12:49:07,370 --> 12:49:07,910
first of all 

87
12:49:07,910 --> 12:49:12,700
we will need to actually select the node by using the match command 

88
12:49:13,810 --> 12:49:15,520
so we will issue that here and

89
12:49:15,520 --> 12:49:19,760
we will specify that that node name must be equal to harry 

90
12:49:21,070 --> 12:49:24,540
and then we are going to use the set command and

91
12:49:24,540 --> 12:49:29,210
specify that job will be equal to drummer 

92
12:49:29,210 --> 12:49:30,430
so let go ahead and copy that 

93
12:49:32,450 --> 12:49:34,958
and we will paste it and execute it 

94
12:49:36,954 --> 12:49:41,410
and the results that are returned says that one property has been set 

95
12:49:41,410 --> 12:49:44,400
but let say that harry does more than just play drums 

96
12:49:44,400 --> 12:49:47,740
let say that he can also play lead guitar 

97
12:49:47,740 --> 12:49:51,810
so in this case we will be adding an additional property

98
12:49:51,810 --> 12:49:56,050
to a property that already exists and we will see how neo4j handles that 

99
12:49:56,050 --> 12:50:01,150
it a relatively simple modification we make in which we are setting

100
12:50:01,150 --> 12:50:05,535
the job key equal to the existing job key

101
12:50:05,535 --> 12:50:11,620
 + an additional value to that key in this case lead guitarist 

102
12:50:11,620 --> 12:50:13,303
so let copy that statement 

103
12:50:15,729 --> 12:50:18,730
and we will paste it and execute it 

104
12:50:18,730 --> 12:50:23,229
and we get a similar result returned and then let look at our network 

105
12:50:26,771 --> 12:50:32,480
and when we select harry sure enough now we see that he has two jobs 

106
12:50:32,480 --> 12:50:33,890
they are separated by a comma 

107
12:50:33,890 --> 12:50:36,660
one is drummer and one is lead guitarist 

108
12:50:36,660 --> 12:50:39,474
there much more you can do with neo4j but

109
12:50:39,474 --> 12:50:43,373
we will want to move on and learn some more advanced capabilities

110
12:50:43,373 --> 12:50:47,212
that work us closer towards managing our big data challenges 

1
01:33:48,250 --> 01:33:52,207
next we will talk about path analytics using cypher 

2
01:33:52,207 --> 01:33:55,100
the query language for neo4j 

3
01:33:55,100 --> 01:33:59,450
here the listing of the various queries we will be demonstrating 

4
01:34:12,288 --> 01:34:16,040
there are some things that cypher is capable of doing very well 

5
01:34:16,040 --> 01:34:19,350
and there are other things that require a little bit of creativity

6
01:34:19,350 --> 01:34:22,230
in order to get the results that you are looking for 

7
01:34:22,230 --> 01:34:24,450
and we will show you some examples of that 

8
01:34:24,450 --> 01:34:29,718
it also important to keep in mind that because we are working with paths 

9
01:34:29,718 --> 01:34:33,285
which are an official structure in graph networks 

10
01:34:33,285 --> 01:34:36,867
each one of these examples includes a new variable 

11
01:34:36,867 --> 01:34:40,221
which in this case we are using the letter p to represent for

12
01:34:40,221 --> 01:34:44,410
the actual path objects that we are going to be returning 

13
01:34:44,410 --> 01:34:49,030
you may also see the complete word path instead of just the single letter

14
01:34:49,030 --> 01:34:51,580
p to represent these objects 

15
01:34:51,580 --> 01:34:55,950
we are going to continue to use the data set of a simple road network that we have 

16
01:34:55,950 --> 01:35:00,970
already been using in previous demonstrations that contains 11 nodes and

17
01:35:00,970 --> 01:35:02,000
14 edges 

18
01:35:04,610 --> 01:35:07,180
so the first query we are going to demonstrate

19
01:35:07,180 --> 01:35:10,040
is finding a path between specific nodes 

20
01:35:10,040 --> 01:35:14,739
so this would be very much like trying to find a root between two different

21
01:35:14,739 --> 01:35:16,829
locations in our road network 

22
01:35:16,829 --> 01:35:21,079
in this case we are going to find a path between the node named h and

23
01:35:21,079 --> 01:35:22,210
the node named p 

24
01:35:23,980 --> 01:35:27,411
to do this we will use the match command and

25
01:35:27,411 --> 01:35:33,162
we will say match p which is a variable we are using to represent our path 

26
01:35:33,162 --> 01:35:36,524
 = node a going through an edge to node c 

27
01:35:36,524 --> 01:35:41,766
there something slightly different about this edge and that is that we are using

28
01:35:41,766 --> 01:35:46,573
a star to represent an arbitrary number of edges in sequence between a and c and

29
01:35:46,573 --> 01:35:51,980
we will be returning all of those edges that are necessary to complete the path 

30
01:35:51,980 --> 01:35:55,530
and in this case we only want to return a single path 

31
01:35:55,530 --> 01:35:59,440
so when we submit this query we see this path 

32
01:35:59,440 --> 01:36:03,240
it consists of eight nodes and seven edges 

33
01:36:03,240 --> 01:36:05,490
and it begins with h and ends with p 

34
01:36:07,600 --> 01:36:11,980
now another common function we will use frequently with paths

35
01:36:11,980 --> 01:36:14,650
is finding the length between two specific nodes 

36
01:36:15,800 --> 01:36:21,010
so we will issue the same two lines of code and then we will use this new command 

37
01:36:21,010 --> 01:36:23,950
length to return an actual value 

38
01:36:23,950 --> 01:36:27,070
we want to be returning an actual path 

39
01:36:27,070 --> 01:36:28,780
and we just want a single value 

40
01:36:28,780 --> 01:36:31,820
and when we submit this query we get the result seven 

41
01:36:31,820 --> 01:36:35,800
and we can see that by visually inspecting the graph or our seven edges 

42
01:36:35,800 --> 01:36:38,310
but because most networks are much more complex than this 

43
01:36:38,310 --> 01:36:42,130
we would need to understand the necessary query to return the length 

44
01:36:44,005 --> 01:36:46,160
and ideally in the case of our road network 

45
01:36:46,160 --> 01:36:50,710
we would want to find the shortest path between those two nodes 

46
01:36:50,710 --> 01:36:52,220
so in this case we are introducing yet

47
01:36:52,220 --> 01:36:56,150
another new command specific to paths called shortestpath 

48
01:36:56,150 --> 01:36:59,149
we will use the same variable key and

49
01:36:59,149 --> 01:37:04,794
the same descriptions in our syntax in connecting node a with node c 

50
01:37:04,794 --> 01:37:09,815
and in this case were going to look for the shortest path between node a and

51
01:37:09,815 --> 01:37:15,090
node p and we are going to return that path as well as the length of that path 

52
01:37:15,090 --> 01:37:16,680
and we are just going to return a single path 

53
01:37:17,760 --> 01:37:22,870
and when we submit this query we get a path that five nodes and

54
01:37:22,870 --> 01:37:27,990
four edges long and if we look at the text results that are returned 

55
01:37:27,990 --> 01:37:32,020
we will see a length displayed in the length column 

56
01:37:32,020 --> 01:37:35,980
and that value is 4 and we can see that by visually inspecting our graph 

57
01:37:38,430 --> 01:37:43,236
the next query we are going to demonstrate is intended to illustrate that

58
01:37:43,236 --> 01:37:46,010
there may be more than one shortest path 

59
01:37:46,010 --> 01:37:50,967
and so we may want to know all of the possible shortest paths in order to make

60
01:37:50,967 --> 01:37:54,300
a choice between which one we prefer 

61
01:37:54,300 --> 01:38:00,290
so we will be using a command that is built into neo4j called allshortestpaths 

62
01:38:00,290 --> 01:38:04,580
we will be issuing a similar query to what we issued previously 

63
01:38:04,580 --> 01:38:10,610
we are going to try to find all of the shortest paths between node a and node p 

64
01:38:10,610 --> 01:38:16,160
and instead of the letters a and c we are using the terms source and destination 

65
01:38:16,160 --> 01:38:20,640
but the results that we are going to return will actually be in the form of an array 

66
01:38:20,640 --> 01:38:25,640
we are using a new term extract which is based on the following 

67
01:38:25,640 --> 01:38:30,370
assuming we have matched our path p we want to identify

68
01:38:30,370 --> 01:38:34,670
all of the nodes in p and extract their names 

69
01:38:34,670 --> 01:38:39,790
and we will return these names as a listing which we will call the variable paths 

70
01:38:39,790 --> 01:38:45,700
if there more than one shortest path we will get multiple listings of node names 

71
01:38:45,700 --> 01:38:50,120
so when we submit this query the results are listed in the rows display and

72
01:38:50,120 --> 01:38:54,130
we see there are actually two shortest paths 

73
01:38:54,130 --> 01:38:57,643
they each have five nodes and four edges 

74
01:38:59,987 --> 01:39:04,060
now we may want to issue a query that finds the shortest path but

75
01:39:04,060 --> 01:39:08,300
with particular constraints or conditions that we place on them 

76
01:39:09,640 --> 01:39:14,160
so in this case we still want to find the shortest path but

77
01:39:14,160 --> 01:39:19,010
in this case we may want to constrain the path length to be greater

78
01:39:19,010 --> 01:39:21,530
than a particular value in this case 5 

79
01:39:21,530 --> 01:39:25,050
and then we want to return essentially the same results that we returned in

80
01:39:25,050 --> 01:39:26,480
the previous query 

81
01:39:26,480 --> 01:39:31,180
but we will also want to return the length of the resulting path just so

82
01:39:31,180 --> 01:39:33,610
we have that information conveniently 

83
01:39:33,610 --> 01:39:35,310
so when we issue this command

84
01:39:36,360 --> 01:39:41,230
we get a path with length six between node a and node p 

85
01:39:41,230 --> 01:39:45,870
so it clearly longer than the shortest path that we had found earlier 

86
01:39:48,710 --> 01:39:52,940
now that we are somewhat familiar with the two shortest path commands 

87
01:39:52,940 --> 01:39:55,750
the shortest path or a single path and

88
01:39:55,750 --> 01:40:00,730
the all shortest paths command or multiple shortest paths we are going to use

89
01:40:00,730 --> 01:40:06,530
that in a little bit of a creative way to return the diameter of the graph 

90
01:40:06,530 --> 01:40:08,750
and if you remember from a previous lecture 

91
01:40:08,750 --> 01:40:11,500
the definition of the diameter of the graph

92
01:40:11,500 --> 01:40:17,170
is actually the longest continuous path between two nodes in the graph 

93
01:40:17,170 --> 01:40:23,440
so by using the shortest path command but returning all possible shortest paths 

94
01:40:23,440 --> 01:40:28,725
we are actually going to get the longest path included in those results returned 

95
01:40:28,725 --> 01:40:30,400
now if we look carefully at this script 

96
01:40:30,400 --> 01:40:33,590
it is a little different than our previous scripts 

97
01:40:33,590 --> 01:40:38,570
in this case our match command is matching all nodes of type mynode 

98
01:40:38,570 --> 01:40:41,272
we will assign those to the variable end 

99
01:40:41,272 --> 01:40:47,001
we are also matching the all nodes of type mynode and assigning that to variable m 

100
01:40:47,001 --> 01:40:48,570
so these matches are the same 

101
01:40:48,570 --> 01:40:53,543
but we want to place a constraint such that the nodes in n are not

102
01:40:53,543 --> 01:40:55,933
the same as the nodes in m and

103
01:40:55,933 --> 01:41:01,981
then we want to find all of the shortest paths between unique nodes in n and m 

104
01:41:01,981 --> 01:41:07,410
and return the names of those nodes as well as the length of that resulting path 

105
01:41:07,410 --> 01:41:10,470
and the trick is to use the command order by 

106
01:41:10,470 --> 01:41:15,360
and so for those of you who are familiar already with sql query language 

107
01:41:15,360 --> 01:41:17,030
you will recognize order by 

108
01:41:17,030 --> 01:41:20,610
you will also recognize the descend command 

109
01:41:20,610 --> 01:41:26,310
so if we order the resulting paths by their length in descending order 

110
01:41:26,310 --> 01:41:31,470
and only return 1 that path should actually be the longest path 

111
01:41:31,470 --> 01:41:33,420
and that equal to the diameter of the graph 

112
01:41:34,500 --> 01:41:38,440
so when we submit this query here the results that we get 

113
01:41:38,440 --> 01:41:43,349
we get a path between node e and node l with length severn 

114
01:41:43,349 --> 01:41:48,809
or maybe it occurs to you that maybe this is not the only diameter of the graph 

115
01:41:48,809 --> 01:41:51,340
the only path with length of seven 

116
01:41:52,580 --> 01:41:59,488
so we can modify our query just a little bit and change the limit from one to five 

117
01:41:59,488 --> 01:42:02,029
and we will see the results 

118
01:42:02,029 --> 01:42:04,240
and sure enough we actually get five paths 

119
01:42:05,730 --> 01:42:08,650
and 3 of those have length 7 

120
01:42:08,650 --> 01:42:12,757
so there are actually three distinct paths which qualify as

121
01:42:12,757 --> 01:42:15,391
a diameter of this particular graph 

1
03:16:09,130 --> 03:16:13,340
so up until now we have been calculating path length

2
03:16:13,340 --> 03:16:17,710
based on the number of hops between our beginning node and our end node 

3
03:16:17,710 --> 03:16:22,590
this is roughly equivalent to counting the number of towns between one town and

4
03:16:22,590 --> 03:16:23,820
another town 

5
03:16:23,820 --> 03:16:26,710
but it does not really get at the value that is usually of

6
03:16:26,710 --> 03:16:28,110
greatest importance to us 

7
03:16:28,110 --> 03:16:33,220
and that is the actual distance between one location and another location 

8
03:16:33,220 --> 03:16:38,220
which is found in the values that we have assigned to the edges between the nodes 

9
03:16:38,220 --> 03:16:42,980
so in this next example we are going to perform that kind of a calculation 

10
03:16:42,980 --> 03:16:46,430
so the first two lines of code we are already fairly familiar with 

11
03:16:46,430 --> 03:16:50,312
we are matching a path between node a and node c 

12
03:16:50,312 --> 03:16:54,396
where the first node is h and the second node is p 

13
03:16:54,396 --> 03:16:57,950
and this third line of code is also fairly familiar 

14
03:16:57,950 --> 03:17:02,940
we are extracting the names of the nodes and the path that been returned and

15
03:17:02,940 --> 03:17:07,140
we are returning a listing of those names as well as a length of the path 

16
03:17:08,470 --> 03:17:11,870
all of that is being returned as the variable pathlength 

17
03:17:11,870 --> 03:17:15,680
we have added a third element to our return statement and

18
03:17:15,680 --> 03:17:17,820
that is using the reduce statement 

19
03:17:17,820 --> 03:17:22,130
so what we are doing here is the purpose of the reduce statement 

20
03:17:22,130 --> 03:17:26,070
takes a set of values and reduces them down to a single value 

21
03:17:27,270 --> 03:17:32,900
so in this line of code we begin by setting a variable s equal to 0 

22
03:17:32,900 --> 03:17:37,200
and then we define a variable e which represents the set of relationships in

23
03:17:37,200 --> 03:17:40,389
a path that returned or in other words the edges 

24
03:17:40,389 --> 03:17:43,842
and we pass that into this variable s and add to it 

25
03:17:43,842 --> 03:17:47,860
the value of the distance that we have assigned to that edge 

26
03:17:49,540 --> 03:17:54,060
so we are performing an aggregate calculation 

27
03:17:54,060 --> 03:17:59,340
and returning the final results to a variable called pathdist 

28
03:17:59,340 --> 03:18:01,193
and we are limiting that results to a single value 

29
03:18:01,193 --> 03:18:06,573
and so when we do this we should get a value that is more indicative

30
03:18:06,573 --> 03:18:12,460
of the actual distance between our source and our destination 

31
03:18:12,460 --> 03:18:13,959
and so here the results 

32
03:18:13,959 --> 03:18:16,977
the path itself as we know begins in h and ends in p 

33
03:18:16,977 --> 03:18:22,376
and it has a pathlength of 7 but it has a pathdist of 39 

34
03:18:22,376 --> 03:18:26,864
so we could interpret this to mean that even though there are 6 towns

35
03:18:26,864 --> 03:18:30,408
between the source town and the destination town or

36
03:18:30,408 --> 03:18:35,310
a pathlength of 7 the actual distance in miles would be a value of 39 

37
03:18:37,446 --> 03:18:39,860
so with that we can apply dijkstra algorithm 

38
03:18:40,950 --> 03:18:47,690
so here i am going to match the node with the name a and the node with the name p 

39
03:18:47,690 --> 03:18:53,461
and we are going to find the shortest path in terms of hops from a to p 

40
03:18:53,461 --> 03:18:57,050
and we will set that equal to the variable path 

41
03:18:57,050 --> 03:19:02,722
then we will perform a reduce command and set the variable dist = 0 

42
03:19:02,722 --> 03:19:04,602
and we will go through and

43
03:19:04,602 --> 03:19:09,960
sum all of the distances of each of the edges in our shortest path 

44
03:19:09,960 --> 03:19:12,480
and return that value as a distance 

45
03:19:12,480 --> 03:19:16,060
and we will also return the path variable 

46
03:19:16,060 --> 03:19:21,870
so remember this is not the path in our network with the least weights 

47
03:19:21,870 --> 03:19:28,260
it is the weight of the shortest path based on numbers of hops 

48
03:19:28,260 --> 03:19:32,940
now that an inherent feature of the shortest path command in cipher 

49
03:19:32,940 --> 03:19:34,470
so here is the path that returned 

50
03:19:34,470 --> 03:19:38,006
it a five node path with four edges and

51
03:19:38,006 --> 03:19:43,730
the total sum of the weights of those edges sums to a value of 22 

52
03:19:45,243 --> 03:19:49,493
so in our previous query we specified that we wanted a match for

53
03:19:49,493 --> 03:19:53,240
the source node and the destination node 

54
03:19:53,240 --> 03:19:58,670
but if i do not specify my destination node i can apply dijkstra single

55
03:19:58,670 --> 03:20:03,730
source shortest path algorithm from node a to any other node 

56
03:20:04,870 --> 03:20:09,637
so when we apply this query the results displayed consist of

57
03:20:09,637 --> 03:20:14,233
the actual original path from a to p with a distance of 22 

58
03:20:14,233 --> 03:20:19,318
and we will see a display of all of the intermediate paths generated in

59
03:20:19,318 --> 03:20:24,240
the process all the way down to a single edge path between a and c 

60
03:20:25,750 --> 03:20:31,830
so just to reiterate what we have calculated is the shortest hop path

61
03:20:31,830 --> 03:20:37,000
with the weights added the sum of the weights of the edges in that path 

62
03:20:37,000 --> 03:20:40,310
this is not the least weight path of the entire network 

63
03:20:43,730 --> 03:20:46,470
okay so let switch gears for a moment 

64
03:20:46,470 --> 03:20:51,320
as we learned in one of our previous lectures we can extract a subset

65
03:20:51,320 --> 03:20:56,070
of nodes and edges from a particular graph for various reasons 

66
03:20:56,070 --> 03:21:00,210
let say for example that we want to avoid a particular town or

67
03:21:00,210 --> 03:21:02,680
a particular area where there might be congestion 

68
03:21:02,680 --> 03:21:06,040
and this would be represented by one of our nodes 

69
03:21:06,040 --> 03:21:09,110
so we are going to perform a similar match as we have done in the past 

70
03:21:09,110 --> 03:21:15,960
where we are going to match any node n with any node m with a two edge between them 

71
03:21:15,960 --> 03:21:20,243
but we want to apply an additional constrained in which none of the n nodes

72
03:21:20,243 --> 03:21:22,150
are going to include the node d 

73
03:21:22,150 --> 03:21:27,920
and none of the m nodes are going to include the node d as well 

74
03:21:27,920 --> 03:21:30,030
and then we will return the resulting graph 

75
03:21:31,290 --> 03:21:35,500
so when we do that we get a graph but looks very much as we might expect 

76
03:21:35,500 --> 03:21:39,958
very similar to our previous graph but it is missing node d so

77
03:21:39,958 --> 03:21:44,183
it only has ten nodes and it now been reduced to ten edges 

78
03:21:47,005 --> 03:21:51,659
so now let say we want to calculate the shortest path over there graph that we

79
03:21:51,659 --> 03:21:54,590
just returned in the previous query 

80
03:21:54,590 --> 03:22:00,690
so in this case we are going to match the shortest path between node a and node p 

81
03:22:00,690 --> 03:22:04,830
but in the second line we want to issue sort of a negative statement

82
03:22:04,830 --> 03:22:09,530
in which the resulting list of node names that we extract using

83
03:22:09,530 --> 03:22:13,780
the extract statement cannot contain the node d 

84
03:22:15,020 --> 03:22:18,710
and then we will return that path and the length of that path 

85
03:22:18,710 --> 03:22:23,190
so when we issue this command here our resulting path 

86
03:22:23,190 --> 03:22:26,730
it a five node path of length four 

87
03:22:26,730 --> 03:22:30,540
as we recall from one of our earlier queries where we were trying to calculate

88
03:22:30,540 --> 03:22:34,500
all of the shortest paths we returned two paths 

89
03:22:34,500 --> 03:22:38,594
one path that contained d and this is the second path which did not contain d 

90
03:22:41,008 --> 03:22:44,538
so we can make this a little complicated 

91
03:22:44,538 --> 03:22:49,728
instead of avoiding a single node in our resulting path we are looking for

92
03:22:49,728 --> 03:22:55,090
a graph that does not contain the immediate neighborhood of a specific node 

93
03:22:55,090 --> 03:22:59,770
this means all of the nearest or the first neighbors of a specific node 

94
03:22:59,770 --> 03:23:04,390
so in this case we are going to match the same node d and

95
03:23:04,390 --> 03:23:08,010
all edges between d and any other node 

96
03:23:08,010 --> 03:23:12,590
and then we are going to issue a collect command to collect all of

97
03:23:12,590 --> 03:23:14,758
the distinct neighbors of d 

98
03:23:14,758 --> 03:23:19,654
and we will apply a constraint to that in which the returned

99
03:23:19,654 --> 03:23:24,960
list of neighbors cannot contain the node with the name d 

100
03:23:24,960 --> 03:23:28,050
likewise the neighbors list for

101
03:23:28,050 --> 03:23:32,070
the target nodes can also not contain the node d 

102
03:23:32,070 --> 03:23:36,800
and when we submit this command we see a network that looks like this 

103
03:23:36,800 --> 03:23:39,010
five nodes and four edges 

104
03:23:39,010 --> 03:23:41,130
and that seems to makes sense 

105
03:23:41,130 --> 03:23:46,170
node d is not in the network nor are its first neighbors 

106
03:23:46,170 --> 03:23:48,590
but if you recall the original network 

107
03:23:48,590 --> 03:23:54,080
there may be a peculiar result that you might find a little bit disconcerting 

108
03:23:54,080 --> 03:23:57,112
so let look at our original network 

109
03:23:57,112 --> 03:24:00,720
now here the node d and here are all it nearest neighbors 

110
03:24:00,720 --> 03:24:03,360
so these are the forbidden neighbors that we want to

111
03:24:03,360 --> 03:24:05,040
remove from our resulting graph 

112
03:24:06,100 --> 03:24:08,360
and we seemed to have done that successfully 

113
03:24:08,360 --> 03:24:12,700
these are the five nodes that are retained in the resulting graph but

114
03:24:12,700 --> 03:24:13,890
there a node out here 

115
03:24:13,890 --> 03:24:20,440
the node p which seems to be neglected or not handled in the results 

116
03:24:20,440 --> 03:24:22,520
it not a first neighbor of d 

117
03:24:23,620 --> 03:24:27,730
so it in some ways arguably should be returned in our results but

118
03:24:27,730 --> 03:24:31,750
it not part of the connected graph that we saw returned 

119
03:24:31,750 --> 03:24:37,580
this is one area in which cipher does not handle these situations by default 

120
03:24:37,580 --> 03:24:42,640
so we will need to supplement our query with an additional query 

121
03:24:42,640 --> 03:24:47,848
in this case the node p was a leaf node so we want to make sure that

122
03:24:47,848 --> 03:24:53,340
not only matching the nodes that conform to these constraints above 

123
03:24:54,400 --> 03:24:59,890
but we also want to include the node or any nodes which are leaf nodes 

124
03:24:59,890 --> 03:25:05,200
which may also be arguably part of the results that you expect to be returned 

125
03:25:05,200 --> 03:25:08,120
now in our network we do have one root node 

126
03:25:08,120 --> 03:25:11,990
but it does not impact the results in this particular query 

127
03:25:11,990 --> 03:25:14,420
in the interests of being complete for

128
03:25:14,420 --> 03:25:19,002
any network most networks being much more complex than the one we are working with 

129
03:25:19,002 --> 03:25:24,450
we would want to take into account not only those leaf nodes that might be left out 

130
03:25:24,450 --> 03:25:26,700
but also any root notes that might be left out 

131
03:25:29,300 --> 03:25:34,400
and finally our last query example extends the previous query

132
03:25:34,400 --> 03:25:38,670
to find the graph which does not contain a selective neighborhood 

133
03:25:38,670 --> 03:25:43,050
in this case the two neighborhood of a particular node 

134
03:25:43,050 --> 03:25:44,571
in this example we are going to use the node f 

135
03:25:44,571 --> 03:25:51,020
and we want to eliminate all of the second neighbors of that node 

136
03:25:51,020 --> 03:25:55,590
so initially we match all of those nodes that are second neighbors of f 

137
03:25:55,590 --> 03:25:57,400
including f itself 

138
03:25:57,400 --> 03:26:01,750
and we will place those in a variable called mylist 

139
03:26:01,750 --> 03:26:04,850
then we go back through the network and match all of the nodes and

140
03:26:04,850 --> 03:26:09,690
edges where the source nodes are not part of the nodes in the mylist and

141
03:26:09,690 --> 03:26:12,740
the target nodes are not contained in mylist 

142
03:26:12,740 --> 03:26:14,940
and then we return those nodes and edges 

143
03:26:16,580 --> 03:26:17,990
and here the resulting graph 

144
03:26:20,020 --> 03:26:25,490
it does not contain f or its first or second neighbors 

145
03:26:25,490 --> 03:26:32,574
if we scroll down to look at the original graph here node f nodes h 

146
03:26:32,574 --> 03:26:38,080
j c a and l are all the first and second neighbors of f 

147
03:26:38,080 --> 03:26:43,070
so those should be eliminated from the graph that gets returned from our results 

148
03:26:43,070 --> 03:26:44,600
and sure enough 

149
03:26:44,600 --> 03:26:50,520
this subset of nodes represents the results that were returned from our query 

150
03:26:50,520 --> 03:26:55,040
it consists of nodes b d e g and p 

151
03:26:55,040 --> 03:26:58,440
and so this concludes our review of some of the more advanced

152
03:26:58,440 --> 03:27:00,490
path analytics queries 

153
03:27:00,490 --> 03:27:02,645
we were using a simple network but

154
03:27:02,645 --> 03:27:06,310
we are providing additional data sets that are much larger and

155
03:27:06,310 --> 03:27:10,770
present a more realistic challenge in applying pathanalythics queries 

1
06:43:11,750 --> 06:43:16,130
hello everyone and welcome to this week module on graph analytics with neo4j

2
06:43:16,130 --> 06:43:18,460
using the cypher query language 

3
06:43:18,460 --> 06:43:21,950
i am jeff sale and i will be your instructor for this series of lessons 

4
06:43:21,950 --> 06:43:25,360
i have been an instructional designer at the san diego supercomputer center for

5
06:43:25,360 --> 06:43:30,090
more than ten years but i have also had a passion for scientific visualization and

6
06:43:30,090 --> 06:43:33,940
visual analytics in one form or another for over two decades 

7
06:43:33,940 --> 06:43:37,010
and i am very excited about this opportunity to introduce you to this

8
06:43:37,010 --> 06:43:40,590
free and very powerful graph analytics tool called neo4j 

9
06:43:41,770 --> 06:43:44,990
first we realized that many of you may not have the systems capable of

10
06:43:44,990 --> 06:43:48,190
pushing the boundaries of neo4j performance limits 

11
06:43:48,190 --> 06:43:51,620
plus the fact that many of you are fitting this course into your already busy

12
06:43:51,620 --> 06:43:56,680
schedules means we will be working with data sets which will load into neo4j and can

13
06:43:56,680 --> 06:44:02,690
be analyze in a reasonable length of time on the order of minutes not hours or days 

14
06:44:02,690 --> 06:44:06,020
however you can be sure that neo4j is capable of processing and

15
06:44:06,020 --> 06:44:10,730
analyzing extremely complex graph networks consisting of millions of nodes and

16
06:44:10,730 --> 06:44:11,360
relationships 

17
06:44:12,580 --> 06:44:16,650
this module consists of a series of hands - on demonstrations with neo4j 

18
06:44:16,650 --> 06:44:19,780
which begin with examples of some basic cypher queries

19
06:44:19,780 --> 06:44:23,590
that soon progress to some of the more advanced cypher queries 

20
06:44:23,590 --> 06:44:27,440
we will begin by using a relatively simple graph representing a road network but

21
06:44:27,440 --> 06:44:30,260
we will also use much larger and more complex data sets 

22
06:44:30,260 --> 06:44:34,280
including sociological data on global terrorist groups and

23
06:44:34,280 --> 06:44:38,340
genetics data on associations and interactions between genes 

24
06:44:38,340 --> 06:44:41,910
these data sets are in fact sub sets of much larger data sets and

25
06:44:41,910 --> 06:44:46,290
we are making both of the sub sets and complete data sets available for download 

26
06:44:46,290 --> 06:44:49,250
once you become comfortable working with the smaller data sets 

27
06:44:49,250 --> 06:44:51,760
we encourage you to explore the larger sets on your own 

28
06:44:52,810 --> 06:44:56,020
finally you will notice that each video is accompanied by a text file

29
06:44:56,020 --> 06:44:59,560
containing all of the code used in the video demonstrations 

30
06:44:59,560 --> 06:45:03,680
these files include dozens of sample scripts written in cypher designed to

31
06:45:03,680 --> 06:45:08,250
make it easy for you to learn not only basic cypher cards but also queries which

32
06:45:08,250 --> 06:45:12,858
focus on more advanced methods such pathenoids and connectivity analytics 

33
06:45:14,040 --> 06:45:17,690
when you are finished with this module you will be able to write cypher scripts to

34
06:45:17,690 --> 06:45:21,900
import and analyze your own data using neo4j 

35
06:45:21,900 --> 06:45:24,682
so thank you for enrolling in this course and let get

36
06:45:24,682 --> 06:45:28,991
started doing some real graph analytics with neo4j and the cypher query language 

1
13:28:39,570 --> 13:28:42,984
a parallel computation is at the heart of big data computing 

2
13:28:42,984 --> 13:28:47,427
however to specify what becomes parallel we usually think of a conceptual model

3
13:28:47,427 --> 13:28:50,760
of parallel computation often called a programming model 

4
13:28:52,630 --> 13:28:55,990
a parallel programming model is a way to specify abstractly 

5
13:28:55,990 --> 13:28:57,280
how a parallel program will run 

6
13:28:58,310 --> 13:29:00,720
naturally for a program to be parallel 

7
13:29:00,720 --> 13:29:05,040
there must be a number of concurrently operating processes 

8
13:29:05,040 --> 13:29:07,540
but how do these processes communicate and exchange data 

9
13:29:08,760 --> 13:29:11,280
how do they decide when to communicate with each other 

10
13:29:12,560 --> 13:29:14,320
further what exactly is done in parallel 

11
13:29:15,550 --> 13:29:17,870
to think of the first question 

12
13:29:17,870 --> 13:29:21,870
two processes communicate data by sharing memory 

13
13:29:23,490 --> 13:29:27,150
indeed there are architectures in which all memory in multiple machines can be

14
13:29:27,150 --> 13:29:30,480
made to virtually look like one large addressable memory space 

15
13:29:31,990 --> 13:29:33,840
however two processes 

16
13:29:33,840 --> 13:29:37,570
we also communicate by passing messages to one another 

17
13:29:37,570 --> 13:29:40,210
either directly from one process to another or

18
13:29:40,210 --> 13:29:43,690
through a common message carrying pipe often called a message bus 

19
13:29:45,750 --> 13:29:48,130
the second question can also have multiple answers 

20
13:29:49,250 --> 13:29:54,050
two of the most common ways of achieving parallelism are pass parallelism and

21
13:29:54,050 --> 13:29:55,790
data parallels 

22
13:29:55,790 --> 13:30:00,860
in task parallelism a large task can be decomposed into multiple sub - tasks 

23
13:30:00,860 --> 13:30:02,380
each of which can be run concurrently 

24
13:30:03,460 --> 13:30:08,690
in data parallelism the data can be partitioned into many smaller fragments

25
13:30:08,690 --> 13:30:11,850
and operation can run on each partition independent of the others 

26
13:30:13,480 --> 13:30:17,040
typically these partial operations have then synchronized and

27
13:30:17,040 --> 13:30:20,500
partially process data combined to produce a full answer 

28
13:30:21,770 --> 13:30:23,870
many parallel data management systems 

29
13:30:23,870 --> 13:30:25,760
operate a partition due to parallel manner 

30
13:30:27,240 --> 13:30:30,490
we need to remember that task parallelism is somewhat independent of

31
13:30:30,490 --> 13:30:31,780
data parallelism 

32
13:30:31,780 --> 13:30:35,440
and it is possible to have both problems of parallelism in a programming model 

33
13:30:36,540 --> 13:30:39,650
it is important to emphasize the issue of a programming model 

34
13:30:39,650 --> 13:30:43,320
should be not be confused with the issue of a programming language 

35
13:30:44,405 --> 13:30:47,295
a programming language is independent of the programming model 

36
13:30:47,295 --> 13:30:49,765
and therefore a programming model can be implemented

37
13:30:49,765 --> 13:30:51,185
in several different languages 

38
13:30:52,425 --> 13:30:56,155
as i mentioned the programming model we are going to consider is bsp 

39
13:30:57,425 --> 13:31:01,350
bsp was not initially created for graph computation 

40
13:31:01,350 --> 13:31:03,550
it was thought of as a parallel computing model 

41
13:31:03,550 --> 13:31:07,740
that will bridge the gap between software models of parallel computation and

42
13:31:07,740 --> 13:31:10,890
hardware capabilities for supporting parallelism 

43
13:31:10,890 --> 13:31:13,100
the basic idea of bsp is as follows 

44
13:31:14,430 --> 13:31:16,880
there are number of processors 

45
13:31:16,880 --> 13:31:20,860
each processor can perform local computation using its own local memory 

46
13:31:22,640 --> 13:31:28,020
there a router which can serve to pass a message from any processor to any other 

47
13:31:29,090 --> 13:31:32,080
when one pair of nodes are exchanging messages 

48
13:31:32,080 --> 13:31:34,420
another third node can still perform computation 

49
13:31:37,000 --> 13:31:40,060
there a facility that can synchronize the state of

50
13:31:40,060 --> 13:31:41,860
all auto - substative processes 

51
13:31:43,010 --> 13:31:45,880
this synchronize may either happen periodically 

52
13:31:45,880 --> 13:31:51,100
at intervals of l time units or there may be another way of specifying 

53
13:31:51,100 --> 13:31:53,490
when this synchronization is going to happen 

54
13:31:53,490 --> 13:31:58,270
but when it does all processors affected by it will come to a consistent state 

55
13:31:59,490 --> 13:32:04,420
when synchronization is performed a fresh round of computation can start 

56
13:32:04,420 --> 13:32:05,905
we call this 

57
13:32:05,905 --> 13:32:07,105
synchronization point 

58
13:32:07,105 --> 13:32:08,815
barrier synchronization 

59
13:32:08,815 --> 13:32:12,575
because all executed processes must reach this barrier point 

60
13:32:12,575 --> 13:32:15,935
before the next step of processing can continue 

61
13:32:15,935 --> 13:32:19,935
a bsp program is broken up into a sequence stop supersteps 

62
13:32:21,325 --> 13:32:25,540
in each superstep each processor will get the data if needed 

63
13:32:25,540 --> 13:32:28,040
performance computation if needed and

64
13:32:28,040 --> 13:32:30,180
then exchange data with the right partner 

65
13:32:31,360 --> 13:32:35,260
once all the nodes are done the system helps to synchronize 

66
13:32:35,260 --> 13:32:36,680
then the next round starts 

67
13:32:37,760 --> 13:32:42,140
each processor can determine if it needs to compute or exchange data 

68
13:32:42,140 --> 13:32:44,300
if not it will make itself inactive 

69
13:32:45,760 --> 13:32:49,540
if required later a processor can be woken up to be active again 

70
13:32:51,390 --> 13:32:54,700
when all processors are inactive the computation stops 

71
13:32:56,320 --> 13:32:59,720
in applying bsp model to graphs we make a few assumptions 

72
13:33:00,830 --> 13:33:03,850
we assume that a processor is synonymous with a vertex 

73
13:33:05,100 --> 13:33:09,130
so for a processor can only send messages to or

74
13:33:09,130 --> 13:33:11,810
receive from its neighboring processes 

75
13:33:13,170 --> 13:33:18,920
we also assume a vertex has an id and possibly a complex value 

76
13:33:18,920 --> 13:33:21,270
and an edge we also have an idea and fact 

77
13:33:22,570 --> 13:33:25,170
each vertex knows which edges it connected to 

78
13:33:27,630 --> 13:33:31,918
we cannot think of a computation as a vertex centered task 

79
13:33:31,918 --> 13:33:34,260
we shall inaudible what this means 

80
13:33:36,640 --> 13:33:38,760
in a now famous paper from google 

81
13:33:38,760 --> 13:33:42,030
this programming model was called think like a vertex 

82
13:33:43,120 --> 13:33:46,620
well to think like a vertex we need to know what a vertex can do 

83
13:33:47,970 --> 13:33:49,950
here a list of actions a vertex can take 

84
13:33:52,030 --> 13:33:53,460
the first one is easy 

85
13:33:53,460 --> 13:33:55,020
a vertex can find its own identifier 

86
13:33:56,680 --> 13:34:01,010
the second operation is to get or set the value of the node 

87
13:34:01,010 --> 13:34:05,450
this operation may be a little involved if the value is a complex data object 

88
13:34:05,450 --> 13:34:08,060
for our purposes we will assume the value is a scalar 

89
13:34:09,490 --> 13:34:11,620
next a node can ask for

90
13:34:11,620 --> 13:34:15,370
its own edges the result of the operation is a set of edge objects 

91
13:34:16,740 --> 13:34:18,720
a node may also count its edges 

92
13:34:19,780 --> 13:34:22,650
since we are referring to outgoing edges throughout 

93
13:34:22,650 --> 13:34:24,160
this is the out degree of the vertex 

94
13:34:25,390 --> 13:34:26,170
recognize that 

95
13:34:26,170 --> 13:34:31,210
this means a vertex does not natively have access to its incident notes 

96
13:34:33,220 --> 13:34:36,830
however it does have control of the outgoing edges 

97
13:34:36,830 --> 13:34:38,990
so it can get inside the edge values 

98
13:34:40,620 --> 13:34:43,470
there may be two different ways of specifying an edge 

99
13:34:43,470 --> 13:34:46,280
the edge we have an id that the node can get 

100
13:34:46,280 --> 13:34:50,460
passively more commonly an edge is identified the vertex of targets 

101
13:34:51,500 --> 13:34:55,520
so in our diagram v1 lasts for the edge targeting v4 

102
13:34:57,950 --> 13:35:03,240
so in the situation like v3 and v5 we are there are multiple edges between v3 and

103
13:35:03,240 --> 13:35:09,370
v5 the source v3 can ask for the values of all edges going to v5 

104
13:35:11,520 --> 13:35:14,990
the operate operation can add or remove an edge of a vertex 

105
13:35:16,270 --> 13:35:22,240
finally since the vertices are processes they can start or stop computing 

106
13:35:22,240 --> 13:35:25,890
typically a node wakes up if it receives a message from another node 

107
13:35:25,890 --> 13:35:30,450
now in comparison to a vertex an edge can do far less 

108
13:35:31,760 --> 13:35:36,220
it can get its own id if the system allows edge id it can set and

109
13:35:36,220 --> 13:35:41,380
retrieve its own values and it can get the id of the node its pointing to 

110
13:35:41,380 --> 13:35:41,880
that it 

111
13:35:43,080 --> 13:35:46,500
now we still have not defined how to think like a vertex 

112
13:35:46,500 --> 13:35:48,470
that what we will do next 

113
13:35:48,470 --> 13:35:52,330
using an example that we have seen several times before 

114
13:35:52,330 --> 13:35:56,700
it dijkstra single source shortest path sssp algorithm 

115
13:35:58,000 --> 13:36:00,120
we have seen this algorithm before 

116
13:36:00,120 --> 13:36:04,710
but now we will show how to compute it in a parallel setting using bsp 

117
13:36:06,140 --> 13:36:09,320
here is the edge value that the weight of the edge 

118
13:36:10,710 --> 13:36:12,830
this is known before the algorithm starts 

119
13:36:14,010 --> 13:36:18,180
each vertex runs the exact same routine concurrently 

120
13:36:19,200 --> 13:36:20,530
each vertex asks 

121
13:36:20,530 --> 13:36:23,500
1 is it super step zero 

122
13:36:23,500 --> 13:36:26,810
2 if yes 

123
13:36:26,810 --> 13:36:31,810
then if this is a source vertex it sets the value to zero 

124
13:36:32,870 --> 13:36:35,930
else it sets the value to infinity which is a large number 

125
13:36:37,310 --> 13:36:38,610
the source vertex 

126
13:36:38,610 --> 13:36:42,450
propagates its edge value to the nodes at the other end of the edges 

127
13:36:42,450 --> 13:36:43,250
just the source vertex 

128
13:36:44,380 --> 13:36:45,650
all other vertices are quiet 

129
13:36:47,300 --> 13:36:50,170
the propagation process works like this 

130
13:36:50,170 --> 13:36:54,710
a vertex gets its own value which for the source vertex is zero 

131
13:36:54,710 --> 13:37:00,990
it gets its edges for each edge it gets the value of the edge 

132
13:37:00,990 --> 13:37:06,020
adds it to its own value and sends the result to the end point of the edge 

133
13:37:07,080 --> 13:37:10,840
the blue numbers indicate that the messages are sent 

134
13:37:10,840 --> 13:37:12,410
all vertices go to halt 

135
13:37:12,410 --> 13:37:16,180
that is they now have hit a synchronization barrier 

136
13:37:17,290 --> 13:37:20,590
notice that the receiving nodes do not look at the messages yet 

137
13:37:21,590 --> 13:37:25,860
it the system job to ensure that the messages are available to these nodes

138
13:37:25,860 --> 13:37:26,810
at the next superstep 

139
13:37:28,690 --> 13:37:33,520
all nodes who have received messages wake up and read the messages 

140
13:37:33,520 --> 13:37:37,530
if a node receives multiple messages it picks the minimum 

141
13:37:37,530 --> 13:37:43,140
in our case the two active nodes have received only one value each 

142
13:37:43,140 --> 13:37:47,720
we have for the sake of convenience colored the processed edges in yellow 

143
13:37:47,720 --> 13:37:50,250
this is just for visualization purposes in this example 

144
13:37:51,470 --> 13:37:56,690
now it compares this band for a minimum value to its own value 

145
13:37:56,690 --> 13:38:03,790
and if its own value is greater it sets its own value with the minimum value 

146
13:38:03,790 --> 13:38:08,480
in our case both notes set their value to that of the incoming message 

147
13:38:09,800 --> 13:38:12,400
the same propagation routine works again 

148
13:38:12,400 --> 13:38:15,320
so each note completes the new distance 

149
13:38:16,680 --> 13:38:20,830
and sends the message along an edge to the other endpoint then halts 

150
13:38:21,990 --> 13:38:25,030
the same step is repeated in the next superstep 

151
13:38:26,700 --> 13:38:30,690
at this point the nodes have updated their values 

152
13:38:30,690 --> 13:38:32,320
the node with the value 6 

153
13:38:32,320 --> 13:38:37,089
has received our message just along one of the three edges on it 

154
13:38:38,120 --> 13:38:38,680
continuing 

155
13:38:40,030 --> 13:38:41,860
at the end of superstep 2 

156
13:38:41,860 --> 13:38:46,690
all nodes are ready to receive messages from all their incident edges 

157
13:38:48,150 --> 13:38:52,210
the node with a value 6 received a value which is lower than its current value 

158
13:38:53,290 --> 13:38:57,020
now the active nodes have no more messages to send 

159
13:38:57,020 --> 13:38:59,010
so each vertex votes to halt 

160
13:39:00,020 --> 13:39:04,300
the vertex id and the value are read out from each vertex and

161
13:39:04,300 --> 13:39:06,600
then the process starts 

162
13:39:06,600 --> 13:39:10,110
if these nodes are in different machines of a cluster 

163
13:39:10,110 --> 13:39:14,320
the system will rely on the underlying platform like yarn 

164
13:39:14,320 --> 13:39:16,960
or sparks underlying infrastructure to ensure 

165
13:39:16,960 --> 13:39:20,770
that the edges going across machines can send and receive messages effectively 

166
13:39:21,880 --> 13:39:25,698
this should give you a sense of the speed of this process for a large scale network 

1
03:08:04,670 --> 03:08:09,260
as we said earlier while the giraph paradigm implements bsb 

2
03:08:09,260 --> 03:08:11,220
it must also be pragmatic 

3
03:08:12,590 --> 03:08:16,590
one such point of pragmatism is the computation of aggregate values 

4
03:08:17,820 --> 03:08:20,550
in the think like a vertex paradigm 

5
03:08:20,550 --> 03:08:24,360
operations local to a vertex can be performed in parallel and

6
03:08:24,360 --> 03:08:28,220
each vertex only has to work with its immediate neighborhood 

7
03:08:28,220 --> 03:08:32,260
this is very useful but it is not sufficient at times 

8
03:08:32,260 --> 03:08:36,450
for example we need to know and use the total number of edges in the graph 

9
03:08:37,470 --> 03:08:41,720
this will be computed by adding edges connected to each vertex but

10
03:08:41,720 --> 03:08:45,140
once aggregated it does not belong to any specific vertex 

11
03:08:46,440 --> 03:08:49,170
so whom does this vertex send the aggregate to 

12
03:08:50,750 --> 03:08:54,500
also suppose a vertex creates some edges as part of its compute step 

13
03:08:55,630 --> 03:08:58,060
when does this information get sent out 

14
03:08:59,260 --> 03:09:01,000
let answer the first question first 

15
03:09:02,240 --> 03:09:07,337
the class in charge of these aggregates is called the defaultmastercompute class 

16
03:09:08,580 --> 03:09:11,250
this class specializes mastercompute 

17
03:09:13,070 --> 03:09:16,460
how does the defaultmastercompute relate to the basic vertex computation 

18
03:09:18,430 --> 03:09:23,150
we have seen that all vertex programs are created by first defining a vertex class 

19
03:09:24,450 --> 03:09:29,314
this class has a compute function that performs like the think like a vertex

20
03:09:29,314 --> 03:09:29,860
logic 

21
03:09:31,000 --> 03:09:36,665
now let add to it a basic vertex function and an aggregate function 

22
03:09:36,665 --> 03:09:37,530
now this function for

23
03:09:37,530 --> 03:09:42,580
our example of all the total number of aggregate edges looks like this 

24
03:09:44,460 --> 03:09:46,070
the name of the function is aggregate 

25
03:09:47,270 --> 03:09:51,990
as you can see in yellow it just gets the number of edges off this vertex 

26
03:09:53,470 --> 03:09:59,003
what is a little strange is that the first argument of this aggregation has

27
03:09:59,003 --> 03:10:03,690
an id of something whose name is total number edges 

28
03:10:05,180 --> 03:10:07,740
what really happens is as follows 

29
03:10:07,740 --> 03:10:12,302
the defaltmastercompute class which is in charge of this global aggregate

30
03:10:12,302 --> 03:10:15,610
operations is specialized by your aggregate class 

31
03:10:16,730 --> 03:10:18,482
this class has an id and

32
03:10:18,482 --> 03:10:24,080
the aggregator gets registered with giraph defaultmastercompute class 

33
03:10:25,630 --> 03:10:32,220
the id we saw before refers to your registered aggregator classes id 

34
03:10:32,220 --> 03:10:36,085
the mastercompute class performs the centralized computation between

35
03:10:36,085 --> 03:10:37,830
supersteps 

36
03:10:37,830 --> 03:10:41,110
this class is initiated on the master node and

37
03:10:41,110 --> 03:10:45,560
will run every superstep before the workers do 

38
03:10:45,560 --> 03:10:49,550
communication with the workers should be performed via aggregators 

39
03:10:51,260 --> 03:10:54,080
the values of the aggregators are broadcast to the workers before

40
03:10:54,080 --> 03:10:56,380
the vertex compute is called 

41
03:10:56,380 --> 03:11:00,670
and collected by the master before the master compute is called 

42
03:11:00,670 --> 03:11:05,550
this means the aggregator values used by the workers are consistent

43
03:11:05,550 --> 03:11:10,000
with the aggregator values from the master from the same superstep 

44
03:11:10,000 --> 03:11:12,750
and the aggregator used by the master are consistent

45
03:11:12,750 --> 03:11:16,330
with the aggregator values from the workers from the previous superstep 

46
03:11:17,630 --> 03:11:20,430
now let go back to the big picture for a second 

47
03:11:20,430 --> 03:11:21,980
giraph is a big data software 

48
03:11:23,090 --> 03:11:28,390
why not implement the giraph system with a set of mapreduce jobs 

49
03:11:28,390 --> 03:11:29,863
too much disk requirement 

50
03:11:29,863 --> 03:11:32,077
no in - memory caching 

51
03:11:32,077 --> 03:11:34,950
every superstep becomes a job 

52
03:11:34,950 --> 03:11:38,680
so all intermediate steps are written to files and

53
03:11:38,680 --> 03:11:42,750
that is not a very scalable solution for iterative operations 

54
03:11:42,750 --> 03:11:46,370
however it will be incorrect to think that giraph works

55
03:11:46,370 --> 03:11:48,041
only in an in memory process 

56
03:11:49,180 --> 03:11:52,220
it can always have less memory than it needs 

57
03:11:52,220 --> 03:11:56,740
there are two broad categories of what is called out - of - core computation 

58
03:11:58,000 --> 03:12:01,650
the first situation occurs when the graph is really large compared to the capacity

59
03:12:01,650 --> 03:12:02,680
of the cluster it running on 

60
03:12:03,980 --> 03:12:08,020
each worker stores the vertices assigned to it inside a set of partitions 

61
03:12:09,300 --> 03:12:13,150
inside a partition are a subset of vertices together with their data 

62
03:12:14,430 --> 03:12:17,700
during a superstep the worker node

63
03:12:17,700 --> 03:12:22,610
processes multiple partitions concurrently one per thread 

64
03:12:22,610 --> 03:12:27,700
if a graph is large then not all partitions are stored in memory 

65
03:12:27,700 --> 03:12:32,120
typically only n partitions are kept in memory at all times and

66
03:12:32,120 --> 03:12:35,180
the rest of the partitions are swapped into disk 

67
03:12:36,980 --> 03:12:41,110
the second situation occurs when the number of messages becomes high 

68
03:12:42,570 --> 03:12:45,530
normally vertices are processed in the order of their ids 

69
03:12:46,710 --> 03:12:49,840
a large number of messages is handled by creating

70
03:12:49,840 --> 03:12:53,950
temporary message stores which are sorted by their destination ids 

71
03:12:55,100 --> 03:12:58,930
all messages going to the same vertex are placed together 

72
03:12:58,930 --> 03:13:03,210
to do this messages are sorted in memory periodically 

73
03:13:03,210 --> 03:13:06,220
the message store files are accessed based on

74
03:13:06,220 --> 03:13:08,170
which vertices are being processed at this moment 

75
03:13:09,240 --> 03:13:13,271
this whole system is managed by giraph out of core message processor 

1
06:21:17,600 --> 06:21:21,830
in this lesson we will talk about two dominant systems developed for

2
06:21:21,830 --> 06:21:23,030
large scale graph processing 

3
06:21:24,270 --> 06:21:27,630
the first one called giraph is from apache and

4
06:21:27,630 --> 06:21:30,340
implements a bsp model on hadoop 

5
06:21:31,510 --> 06:21:36,440
the second system is called graphx is developed on the spark platform 

6
06:21:36,440 --> 06:21:41,450
which as you know emphasizes on interactive in memory computations 

7
06:21:41,450 --> 06:21:44,560
while bsp is a popular graph processing model 

8
06:21:44,560 --> 06:21:49,200
the actual implementation of bsp in an infrastructure needs

9
06:21:49,200 --> 06:21:52,230
additional programmability beyond what we have discussed so far 

10
06:21:53,680 --> 06:21:58,390
in giraph several additional capabilities are added to make it more practical 

11
06:21:59,760 --> 06:22:04,470
a thorough coverage of the giraph platform is beyond the scoop of these lectures 

12
06:22:04,470 --> 06:22:07,464
however we will touch upon a few of these capabilities 

13
06:22:08,862 --> 06:22:14,126
we will first consider graph io that is how graphs can come into a system

14
06:22:14,126 --> 06:22:19,870
represented inside the system and when completed are written out 

15
06:22:19,870 --> 06:22:24,510
next we will describe how giraph interacts with external data sources 

16
06:22:24,510 --> 06:22:27,600
some of these data sources use a different data model 

17
06:22:27,600 --> 06:22:29,070
other sources include databases 

18
06:22:31,310 --> 06:22:33,100
once a graph is imported 

19
06:22:33,100 --> 06:22:35,900
it is important to make sure that the system runs efficiently 

20
06:22:37,490 --> 06:22:42,070
we will look at a method that uses a special kind of global aggregate operation

21
06:22:42,070 --> 06:22:45,400
which saves time by reducing the amount of messaging

22
06:22:45,400 --> 06:22:47,730
to compute aggregate functions like sum and products 

23
06:22:50,200 --> 06:22:54,000
finally we will recognize that even if giraph is designed for

24
06:22:54,000 --> 06:22:57,150
performing iterative in memory computation 

25
06:22:57,150 --> 06:23:01,000
there are times where it is absolutely necessary to store data on disk 

26
06:23:02,340 --> 06:23:06,490
we will briefly touch upon giraph ability to handle out of core graphs and

27
06:23:06,490 --> 06:23:07,820
out of core messages 

28
06:23:08,920 --> 06:23:11,621
a graph can be written in many ways 

29
06:23:11,621 --> 06:23:17,354
for neo4j we saw how graphs can be important to the database from a csv file 

30
06:23:17,354 --> 06:23:23,340
in giraph two of the most common input formats are adjacency list and edge list 

31
06:23:25,010 --> 06:23:29,720
for an adjacency list each line has the node id a node value

32
06:23:29,720 --> 06:23:33,200
which is a single number here and a list of destination weight pairs 

33
06:23:35,050 --> 06:23:39,950
thus in line one a has a value of 10 and 2 neighbors b and

34
06:23:39,950 --> 06:23:42,970
f with edge weights 2 and 5 respectively 

35
06:23:44,300 --> 06:23:47,850
since g has no outgoing edge the adjacency list is empty 

36
06:23:49,350 --> 06:23:52,570
the current way of representing graphs is in terms of triplets 

37
06:23:52,570 --> 06:23:57,880
containing the source and destination nodes followed by an inaudible 

38
06:23:57,880 --> 06:23:59,580
notice the way we have shown it here 

39
06:24:00,620 --> 06:24:02,160
and the node values is not represented 

40
06:24:03,280 --> 06:24:06,170
let us simplify the adjacency list representation of it 

41
06:24:07,930 --> 06:24:11,400
we remove the colons commas braces and

42
06:24:11,400 --> 06:24:15,650
parenthesis and get a space separated set of lines 

43
06:24:15,650 --> 06:24:16,810
one line for each vertex 

44
06:24:17,880 --> 06:24:22,210
we further replace the node ids a b c etc with 1 2 3 etc 

45
06:24:22,210 --> 06:24:25,710
so that these ids are integers 

46
06:24:26,810 --> 06:24:30,700
so what do we need to specify to parse this for giraph 

47
06:24:30,700 --> 06:24:37,136
one the graph is a text subject and not let say a database subject 

48
06:24:37,136 --> 06:24:42,170
two it is a vertex based representation each line is a vertex 

49
06:24:43,500 --> 06:24:44,800
this splitter here is a space 

50
06:24:46,450 --> 06:24:48,830
the idea of the node is a first value for each line 

51
06:24:50,220 --> 06:24:52,250
the value is a second token 

52
06:24:53,740 --> 06:24:57,120
the next pair of items you find an edge with the target and the weight 

53
06:24:57,120 --> 06:24:58,590
respectively 

54
06:24:58,590 --> 06:25:02,420
and lastly there is a list of these pairs until the end of the line 

55
06:25:03,820 --> 06:25:08,124
therefore each line would typically lead to the creation of both notes and

56
06:25:08,124 --> 06:25:09,006
a set of edges 

57
06:25:10,687 --> 06:25:15,020
this shows a typical reader formula decency matrix written in java 

58
06:25:15,020 --> 06:25:18,770
again you do not have to know java to get the elements of this program 

59
06:25:20,280 --> 06:25:23,300
our reader is clearly customized for your specific input 

60
06:25:24,500 --> 06:25:29,020
very often the starting point is a basic reader provided by giraph 

61
06:25:31,140 --> 06:25:35,181
like the reader that knows how to read vertices from each line of a text swipe 

62
06:25:36,330 --> 06:25:41,070
to customize it you extend it and create your own version 

63
06:25:42,350 --> 06:25:44,870
now you need to define how to get the id and

64
06:25:44,870 --> 06:25:48,430
value of the vertex by writing separate message for them 

65
06:25:48,430 --> 06:25:53,455
notice that the id comes from the zeroth item of each line after

66
06:25:53,455 --> 06:25:59,390
the split by white space and the value comes from the next open 

67
06:25:59,390 --> 06:26:02,170
the second term marked by 1 for the 0 base of the light 

68
06:26:03,370 --> 06:26:05,640
the next code element is this block here 

69
06:26:07,000 --> 06:26:11,210
this specifies how to create edges by iterating through every line 

70
06:26:12,390 --> 06:26:16,377
to keep the short we will remove the part that gets the edges here 

71
06:26:16,377 --> 06:26:20,202
as giraph as mature it has included many specialized

72
06:26:20,202 --> 06:26:23,130
to interoperate with compatible resources 

73
06:26:24,190 --> 06:26:27,410
this diagram is from giraph where the show some of these sources 

74
06:26:28,780 --> 06:26:31,690
we can group them into three different categories 

75
06:26:33,210 --> 06:26:37,520
group one interoperates with hive and hbase 

76
06:26:37,520 --> 06:26:41,360
you possibly remember these systems from a prior course 

77
06:26:41,360 --> 06:26:44,290
these systems are designed to give a higher level of data access on

78
06:26:44,290 --> 06:26:45,870
interface on top of mapreduce 

79
06:26:47,170 --> 06:26:51,880
group two accesses relational systems like mysql and cassandra 

80
06:26:52,970 --> 06:26:57,150
but these systems have accessed indirectly through a software module called gora 

81
06:26:58,570 --> 06:27:03,164
gora uses a json schema to map the relation schema of the sql database

82
06:27:03,164 --> 06:27:05,675
to a structure that giraph can read 

83
06:27:05,675 --> 06:27:09,864
group three accesses graph databases like neo4j and dex 

84
06:27:09,864 --> 06:27:12,050
which is now called sparksee 

85
06:27:13,370 --> 06:27:16,190
these systems are all taxes indirectly 

86
06:27:16,190 --> 06:27:18,750
using the inaudible service of tinkerpop 

87
06:27:18,750 --> 06:27:21,440
which is a graph api layer that can use

88
06:27:21,440 --> 06:27:24,800
many different giraph stores including inaudible graph and titan 

89
06:27:26,130 --> 06:27:28,190
consider a relational table stored in hive 

90
06:27:29,270 --> 06:27:32,280
the table shown here is extracted from the bio grid data source that

91
06:27:32,280 --> 06:27:33,290
we mentioned in module two 

92
06:27:34,330 --> 06:27:38,340
each row of the table represents a molecule interaction 

93
06:27:38,340 --> 06:27:42,190
we can create a network from here just by considering the first two columns 

94
06:27:43,250 --> 06:27:47,207
the first column represents the source node of an edge colored red 

95
06:27:47,207 --> 06:27:50,300
and the second column represents the target node of the edge colored blue 

96
06:27:51,640 --> 06:27:54,820
the label on the edge comes from the fifth column of the table

97
06:27:54,820 --> 06:27:56,670
which is a black bold font 

98
06:27:58,100 --> 06:28:00,578
let assume that these predict items 

99
06:28:00,578 --> 06:28:04,560
these are items that we want to pick up from the hive table 

100
06:28:05,730 --> 06:28:08,460
the simplest way to get a record from hive

101
06:28:08,460 --> 06:28:13,195
to giraph is to extend the class called simplehiverowtoedge 

102
06:28:14,970 --> 06:28:19,790
for this class we need to specify the source node the target node and

103
06:28:19,790 --> 06:28:22,580
the edge value using three methods as shown here 

104
06:28:24,160 --> 06:28:27,325
my extension is called myhiverowtoedge 

105
06:28:28,390 --> 06:28:32,440
it shows the implementation of these methods where we just pick up the first 

106
06:28:33,850 --> 06:28:37,430
second and fifth columns as we described before 

107
06:28:39,390 --> 06:28:43,640
now as mentioned before giraph interacts with neo4j through

108
06:28:43,640 --> 06:28:47,080
the gremlin api provided by tinkerpop 

109
06:28:47,080 --> 06:28:51,470
one can think of gremlin as a traversal api which means 

110
06:28:51,470 --> 06:28:56,220
it allows one to start from some node and walk the graph step by step 

111
06:28:56,220 --> 06:29:00,280
to show this consider disease gene graph on the right 

112
06:29:00,280 --> 06:29:02,050
let call this graph g 

113
06:29:03,210 --> 06:29:07,407
so g v represents all the vertices of g 

114
06:29:07,407 --> 06:29:12,524
therefore g v has name mc4r selects the node that

115
06:29:12,524 --> 06:29:17,410
has a property called name whose value is mc4r 

116
06:29:18,900 --> 06:29:23,010
let add to this path the condition out 

117
06:29:23,010 --> 06:29:26,640
which chooses the out edges of the mc4r node and

118
06:29:26,640 --> 06:29:31,050
then traverses the associatedwith edge to the orange node called obesity 

119
06:29:32,150 --> 06:29:34,250
for this call returns the vertex only 

120
06:29:35,940 --> 06:29:42,780
now adding the path to values means gives us obesity 

121
06:29:42,780 --> 06:29:45,170
we can also expand differently from the obesity node 

122
06:29:46,480 --> 06:29:50,600
when we say inv we refer to all nodes that have

123
06:29:50,600 --> 06:29:53,050
incoming edges to the current node 

124
06:29:53,050 --> 06:29:55,400
in this case there is only one 

125
06:29:55,400 --> 06:29:56,000
the lepr node 

126
06:29:57,500 --> 06:30:01,190
to this we add the traversal out beam and

127
06:30:01,190 --> 06:30:05,320
thus we get back the out going edge from the lepr node highlighted in together 

128
06:30:06,550 --> 06:30:09,660
we can also look at the giraph gremlin near project connection

129
06:30:09,660 --> 06:30:10,760
from tinkerpop viewpoint 

130
06:30:12,050 --> 06:30:15,990
tinkerpop is trying to create a standard language for graph reversal 

131
06:30:15,990 --> 06:30:20,750
just like neo4j is trying to create open cypher as a standard query language for

132
06:30:20,750 --> 06:30:21,470
graph databases 

133
06:30:22,960 --> 06:30:27,020
in trying to create the standard tinkerpop recognizes that the actual

134
06:30:27,020 --> 06:30:31,019
storage management for graph databases should be provided by another vendor 

135
06:30:32,050 --> 06:30:34,950
the vendor needs to implement the gremlin api for access 

136
06:30:36,310 --> 06:30:41,560
similarly for graphic processing including expensive analytic operations

137
06:30:41,560 --> 06:30:44,460
should be performed by what they call a graph computer 

138
06:30:45,460 --> 06:30:48,630
this is the role played by giraph as well as spark 

139
06:30:48,630 --> 06:30:50,677
both of which interface with tinkerpop 

1
12:52:08,140 --> 12:52:11,910
next we will count the number of vertices and edges define a min and

2
12:52:11,910 --> 12:52:17,140
max function for spark reduce method computer the min and max degrees 

3
12:52:17,140 --> 12:52:21,390
and compute the histogram data of the degree of connectedness 

4
12:52:23,490 --> 12:52:26,320
in this hands on we will cover the degree distribution of

5
12:52:26,320 --> 12:52:29,810
the metros graph from the first hands on exercise 

6
12:52:29,810 --> 12:52:34,110
this will help you practice finding the degrees of connectedness in a graph 

7
12:52:34,110 --> 12:52:35,310
these numbers will be used for

8
12:52:35,310 --> 12:52:38,780
plotting the visualizations in the next hands on exercises 

9
12:52:41,330 --> 12:52:43,650
the degree of a vertex is the number of edges or

10
12:52:43,650 --> 12:52:46,810
connections the vertex has to other vertices in the graph 

11
12:52:47,880 --> 12:52:51,460
in directed graphs each vertex has an in degree 

12
12:52:51,460 --> 12:52:54,555
the number of edges directed to the vertex 

13
12:52:54,555 --> 12:52:59,797
in and out degree the number of edges directed away from the vertex 

14
12:52:59,797 --> 12:53:02,580
the metros graph is an example of a directed graph 

15
12:53:04,080 --> 12:53:08,650
each metropolis vertex has one outgoing edge to a country vertex 

16
12:53:09,700 --> 12:53:15,460
each country vertex has one or more incoming edges from metropolis vertices 

17
12:53:15,460 --> 12:53:16,725
this will be a quiz question 

18
12:53:23,366 --> 12:53:27,976
starting again where we left off from the previous hands - on exercise first 

19
12:53:27,976 --> 12:53:30,280
ensure your cloudera vm is started and

20
12:53:30,280 --> 12:53:34,430
that you downloaded the dataset examples of analytics 

21
12:53:34,430 --> 12:53:36,110
the link is in the content for this week 

22
12:53:37,460 --> 12:53:41,662
use the numedges attribute to print the number of edges in metrosgraph 

23
12:53:41,662 --> 12:53:44,385
as you can see the result is 65 

24
12:53:44,385 --> 12:53:49,360
which matches the number of lines in metro country csv 

25
12:53:49,360 --> 12:53:53,178
now use the numvertices attribute to print the number

26
12:53:53,178 --> 12:53:55,258
of vertices in metrosgraph 

27
12:53:55,258 --> 12:54:00,344
as you can see the result is 93 which matches the number of

28
12:54:00,344 --> 12:54:06,655
lines in metro csv65 plus the number of lines in country csv 28 

29
12:54:13,666 --> 12:54:18,061
define the max and the min reduce operation to compute the highest and

30
12:54:18,061 --> 12:54:19,586
lowest degree vertex 

31
12:54:22,929 --> 12:54:27,725
let us find the vertex with the most outgoing edges or

32
12:54:27,725 --> 12:54:32,848
the vertex with the largest out degree by passing the max

33
12:54:32,848 --> 12:54:39,065
function to a reduced operation on the out degrees of metrosgraph 

34
12:54:40,190 --> 12:54:45,310
the result in this case is vertex id five with one outgoing edge 

35
12:54:45,310 --> 12:54:49,680
the result could have been any metropolis because every metropolis in this graph

36
12:54:49,680 --> 12:54:52,600
has one outgoing edge to its country 

37
12:54:52,600 --> 12:54:55,890
let us find the vertex with the most incoming edges or

38
12:54:55,890 --> 12:54:58,620
the vertex with the largest indegree 

39
12:54:58,620 --> 12:55:01,620
this is done the same way as the previous example 

40
12:55:01,620 --> 12:55:06,170
except you will run the reduce operation on the indegrees of metrosgraph 

41
12:55:06,170 --> 12:55:11,350
the result is vertexid 108 with 14 incoming edges 

42
12:55:12,390 --> 12:55:18,640
apply a filter to the metrosgraph vertices to find out which vertex is 108 

43
12:55:18,640 --> 12:55:21,280
the answer is the united states 

44
12:55:21,280 --> 12:55:27,860
this means that the united states has 14 metropolises in the metros csvfile 

45
12:55:27,860 --> 12:55:32,500
we can also compute how many vertices have one out going edge by applying a filter of

46
12:55:32,500 --> 12:55:35,530
one to the outgoing degrees and counting the results 

47
12:55:36,550 --> 12:55:43,160
the result is 65 because there are 65 metropolises with one outgoing degree 

48
12:55:43,160 --> 12:55:45,520
none of the countries have any outgoing degrees 

49
12:55:46,790 --> 12:55:49,850
let us ignore whether or not the edge is in or out and

50
12:55:49,850 --> 12:55:52,850
just find which vertex has the most edges 

51
12:55:52,850 --> 12:55:56,460
again we will run the reduce operation with the max function 

52
12:55:56,460 --> 12:55:59,310
but this time we will run it on metrosgraph degrees attribute 

53
12:56:00,340 --> 12:56:03,960
the result is 108 again with 14 connections 

54
12:56:03,960 --> 12:56:08,239
this means that the united states is the most connected vertex in metrosgraph 

55
12:56:15,252 --> 12:56:18,910
finally let us calculate the histogram data of the degrees for

56
12:56:18,910 --> 12:56:20,750
the countries in metrosgraph 

57
12:56:21,880 --> 12:56:25,010
first create a map that only includes countries 

58
12:56:25,010 --> 12:56:29,640
so create a filter to only include the vertices with the vertex id that is

59
12:56:29,640 --> 12:56:31,815
greater than or equal to 100 

60
12:56:31,815 --> 12:56:34,888
then you will group the map by the size of the degree and

61
12:56:34,888 --> 12:56:37,430
sort the map from lowest to highest degree 

62
12:56:39,310 --> 12:56:43,620
the output shows six pairs in an array 

63
12:56:43,620 --> 12:56:46,020
the first number is the number of edges and

64
12:56:46,020 --> 12:56:50,810
the second number is the number of vertices that have that number of edges 

65
12:56:50,810 --> 12:56:55,462
in other words the result of the query shows that there are 18

66
12:56:55,462 --> 12:57:00,461
countries with 1 metropolis 4 countries with 2 metropolises 

67
12:57:00,461 --> 12:57:05,805
2 countries with 3 metropolises 2 countries with 5 metropolises 

68
12:57:05,805 --> 12:57:11,260
1 country with 9 metropolises and 1 country with 14 metropolises 

1
01:49:18,520 --> 01:49:24,640
next we will import the graphx libraries import the vertices import the edges 

2
01:49:24,640 --> 01:49:30,820
create a graph and use spark filter method to return vertices in the graph 

3
01:49:30,820 --> 01:49:33,670
hi this is cristine kirkpatrick division director for

4
01:49:33,670 --> 01:49:35,720
information technology systems and

5
01:49:35,720 --> 01:49:39,320
services at the san diego super computer center 

6
01:49:39,320 --> 01:49:41,810
i will guide you through the hands - on exercises 

7
01:49:41,810 --> 01:49:44,440
i recommend you follow along with the video once 

8
01:49:44,440 --> 01:49:46,370
then either using the video or

9
01:49:46,370 --> 01:49:50,360
the hands - on reading go through the hands - on exercise on your own computer 

10
01:49:51,460 --> 01:49:55,440
this hands - on exercise will show you how to build a graph using graphx 

11
01:49:55,440 --> 01:49:58,720
spark api for graphs and graph - parallel computation 

12
01:49:59,750 --> 01:50:02,720
we will start with importing the data and then build a simple graph 

13
01:50:03,890 --> 01:50:07,050
note that the first four hands on assignments are meant to be completed

14
01:50:07,050 --> 01:50:08,640
sequentially 

15
01:50:08,640 --> 01:50:12,430
that is you will need to have your cloudera vm running continuously 

16
01:50:12,430 --> 01:50:13,840
if you must shut down your vm and

17
01:50:13,840 --> 01:50:18,460
restart you will need to run the commands from this first hands on assignment again 

18
01:50:19,470 --> 01:50:23,930
otherwise you may receive error messages because the data has not been imported 

19
01:50:23,930 --> 01:50:25,680
the data references metro areas 

20
01:50:26,860 --> 01:50:31,190
in an effort to disambiguate or distinguish between metropolitan areas and

21
01:50:31,190 --> 01:50:33,700
an english word also used to refer to a train or

22
01:50:33,700 --> 01:50:37,240
light rail when we mean metro area we will say metropolis 

23
01:50:38,340 --> 01:50:41,070
first ensure your cloudera vm is started and

24
01:50:41,070 --> 01:50:44,780
that you have downloaded the data set examples of analytics 

25
01:50:44,780 --> 01:50:46,600
the link is in the content for this week 

26
01:50:47,720 --> 01:50:50,780
the download process might name the zip file something very long 

27
01:50:51,960 --> 01:50:57,170
copy the examples of analytics zip file to the cloudera home folder 

28
01:50:57,170 --> 01:50:59,300
if the zip file is on the desktop 

29
01:50:59,300 --> 01:51:02,880
simply drag it to the cloudera home folder on the desktop 

30
01:51:02,880 --> 01:51:05,820
if the file is saved elsewhere then you will need to navigate

31
01:51:05,820 --> 01:51:09,680
to the folder where it is saved before dragging it to the cloudera home folder 

32
01:51:10,750 --> 01:51:15,490
open the terminal window by clicking the terminal icon at the top of the screen 

33
01:51:15,490 --> 01:51:19,860
use the unzip command to extract the zip file to the cloudera home directory 

34
01:51:19,860 --> 01:51:22,830
you can copy and paste the name of the zip file 

35
01:51:22,830 --> 01:51:25,750
let it extract with the default name examplesofanalytics 

36
01:51:26,960 --> 01:51:29,690
go into the examplesofanalytics directory and

37
01:51:29,690 --> 01:51:33,260
list the contents of the eoa data directory 

38
01:51:33,260 --> 01:51:36,430
you should see five csv files and one txt file 

39
01:51:37,720 --> 01:51:43,460
in order to access the csv and txt files we have to first copy the files to hdfs 

40
01:51:44,950 --> 01:51:50,500
rally hdfs foot command to copy the eoa data directory to hdfs

41
01:51:50,500 --> 01:51:53,990
then less the contents of the eoa data directory on hdfs 

42
01:51:55,580 --> 01:52:00,140
start the spark shell and include the graph stream and breeze fizz libraries 

43
01:52:00,140 --> 01:52:03,030
do not worry about copying the full command from the video 

44
01:52:03,030 --> 01:52:06,526
a text version will be included within the course reading that contains the full

45
01:52:06,526 --> 01:52:08,048
command for you to copy and paste 

46
01:52:11,310 --> 01:52:15,083
import the log for j classes and suppress the notice in info messages 

47
01:52:21,571 --> 01:52:27,410
import sparks graphx and rdd classes along with scala source class 

48
01:52:27,410 --> 01:52:32,180
the data set we are going to use in this hands on is from three files 

49
01:52:32,180 --> 01:52:36,860
the metro csv file and the country csv file contain the vertices for

50
01:52:36,860 --> 01:52:40,700
the graph and metro underscore country csv 

51
01:52:40,700 --> 01:52:44,740
contains the edges that make up the relationships between the metro areas and

52
01:52:44,740 --> 01:52:45,980
the country they belong too 

53
01:52:47,010 --> 01:52:51,940
before importing any of the csv files we are going to list the first five lines of

54
01:52:51,940 --> 01:52:54,710
each file to verify that they are indeed csv files 

55
01:52:55,880 --> 01:53:02,540
notice that the metric csv contains an id the metropolis name and the population 

56
01:53:02,540 --> 01:53:06,838
the country csv file contains only an id and the country name 

57
01:53:06,838 --> 01:53:10,667
the metro country csv file contains the metro id and

58
01:53:10,667 --> 01:53:14,339
the id of the country that the metropolis belongs to 

59
01:53:20,179 --> 01:53:25,110
create a class called placenode to store the information about the vertices 

60
01:53:25,110 --> 01:53:28,870
then extend the placenode class with case classes specifically for

61
01:53:28,870 --> 01:53:30,920
the metro and the countries vertices 

62
01:53:31,920 --> 01:53:32,990
create attributes for

63
01:53:32,990 --> 01:53:37,900
the metro class to store the name and the population from the csv file 

64
01:53:37,900 --> 01:53:40,660
the country class only needs an attribute to store the name 

65
01:53:41,930 --> 01:53:47,490
to import the metro csv file create a spark resilient distributive data set or

66
01:53:47,490 --> 01:53:53,235
rdd named metros made up of a vertex id in the metro class 

67
01:53:53,235 --> 01:53:56,310
an rdd represents an immutable partition collection

68
01:53:56,310 --> 01:53:58,740
of elements that can be operated on in parallel 

69
01:53:59,800 --> 01:54:02,630
when the contents of the csv files printed 

70
01:54:02,630 --> 01:54:05,900
the line with the column names started with the symbol 

71
01:54:05,900 --> 01:54:09,590
so create a filter to ignore any line that starts with a so

72
01:54:09,590 --> 01:54:12,080
that the column names do not import into the rdd 

73
01:54:13,540 --> 01:54:17,870
the csv files being imported contain values separated by commas 

74
01:54:17,870 --> 01:54:21,770
so you will need to split each line in the metro csv file into rows 

75
01:54:21,770 --> 01:54:24,470
by using a comma as a delimiter 

76
01:54:24,470 --> 01:54:26,970
map the first row to the vertex id 

77
01:54:26,970 --> 01:54:29,610
the second row is the metro name attribute and

78
01:54:29,610 --> 01:54:31,630
the third row is the population attribute 

79
01:54:32,700 --> 01:54:37,162
you are going to import the country csv feed file the same way as the metro csv

80
01:54:37,162 --> 01:54:38,080
feed file 

81
01:54:38,080 --> 01:54:43,120
however this time since the ids in both the metro csv file and the country csv

82
01:54:43,120 --> 01:54:48,350
file start with one you will add 100 to the vertex id of the countries 

83
01:54:48,350 --> 01:54:52,570
so that the vertex ids are unique between both data sets 

84
01:54:52,570 --> 01:54:55,400
if the ids are not unique that will prevent us from

85
01:54:55,400 --> 01:54:57,430
creating an accurate graph 

86
01:54:57,430 --> 01:54:58,011
this will be on the quiz 

87
01:55:04,001 --> 01:55:08,200
import the edges into an rdd named mclinks 

88
01:55:08,200 --> 01:55:11,550
this is done the same way as the previous two examples 

89
01:55:11,550 --> 01:55:14,646
remember to add 100 to the countries vertex id 

90
01:55:20,462 --> 01:55:25,142
concatenate the metros and countries vertices into a single variable and

91
01:55:25,142 --> 01:55:29,143
use graphx graph function to create a graph of the metros and

92
01:55:29,143 --> 01:55:32,030
countries vertices with the mc links edges 

93
01:55:33,140 --> 01:55:35,690
let us take a look at what is in the graph 

94
01:55:35,690 --> 01:55:38,879
use the vertices and edges attributes to print five vertices and

95
01:55:38,879 --> 01:55:40,599
five edges from the metros graph 

96
01:55:46,613 --> 01:55:49,064
query the graph to find how the metropolises and

97
01:55:49,064 --> 01:55:51,340
the countries are related 

98
01:55:51,340 --> 01:55:53,990
let us find which country tokyo is in 

99
01:55:53,990 --> 01:55:56,850
tokyo has a vertex id of 1 

100
01:55:56,850 --> 01:56:00,450
use rdd filter method to filter all of the edges in metro graph

101
01:56:00,450 --> 01:56:05,360
that have a source for text id of one and create a map of destination vertex id 

102
01:56:06,550 --> 01:56:09,420
the result is 101 which is japan 

103
01:56:09,420 --> 01:56:14,530
you can verify this by looking at the metro csv and country csv files 

104
01:56:14,530 --> 01:56:19,470
remember we added 100 to the ids in country csv 

105
01:56:19,470 --> 01:56:20,910
now let us do the opposite and

106
01:56:20,910 --> 01:56:25,320
find all of the metropolises that are in china which has a vertex id of 103 

107
01:56:25,320 --> 01:56:29,880
this time we are going to filter all of the edges in metro graph

108
01:56:29,880 --> 01:56:35,530
that have a vertex id of 103 and create a map of all of the source ids 

109
01:56:35,530 --> 01:56:39,480
the result is 3 4 7 24 and 34 

110
01:56:39,480 --> 01:56:46,353
you can look in metrostats csv and verify that those metropolises are in china 

1
03:46:04,450 --> 03:46:09,380
next we will create a new dataset join two datasets with joinvertices 

2
03:46:09,380 --> 03:46:14,130
join two datasets with outerjoinvertices and create a new return type for

3
03:46:14,130 --> 03:46:15,238
the joined vertices 

4
03:46:15,238 --> 03:46:17,310
in this hands - on exercise 

5
03:46:17,310 --> 03:46:21,370
we will create a new graph made up of five airline flights 

6
03:46:21,370 --> 03:46:23,720
the vertices will represent the airports 

7
03:46:23,720 --> 03:46:27,690
and the edges will represent the departures and the arrivals 

8
03:46:27,690 --> 03:46:31,180
once the graph has been created we will add a second dataset

9
03:46:31,180 --> 03:46:33,670
with additional airport information and

10
03:46:33,670 --> 03:46:39,570
practice using graphx join methods to create new graphs by joining the datasets 

11
03:46:39,570 --> 03:46:42,140
we will start a new spark shell session so

12
03:46:42,140 --> 03:46:44,460
we will be opening a new terminal window 

13
03:46:45,530 --> 03:46:49,000
open the new terminal window by clicking the terminal icon at the top

14
03:46:49,000 --> 03:46:49,570
of the screen 

15
03:46:51,060 --> 03:46:52,152
start the spark shell 

16
03:47:02,295 --> 03:47:06,955
import the log for j classes and suppress the notice and info messages 

17
03:47:08,105 --> 03:47:11,925
import sparks graphics and rdd classes 

18
03:47:11,925 --> 03:47:15,255
now we will create the vertices ourselves from a list 

19
03:47:15,255 --> 03:47:19,375
the list will contain the vertex id and the name of the airport 

20
03:47:19,375 --> 03:47:21,280
now we will create the edges 

21
03:47:21,280 --> 03:47:24,730
the edges will represent a flight departing from one airport and

22
03:47:24,730 --> 03:47:26,150
arriving at another 

23
03:47:26,150 --> 03:47:29,710
we will set the edge property to a fake flight number 

24
03:47:29,710 --> 03:47:32,380
create the flights graph by joining the vertices and

25
03:47:32,380 --> 03:47:34,690
the edges using graph flexes graph functions 

26
03:47:35,720 --> 03:47:38,050
let us explore the graph so far 

27
03:47:38,050 --> 03:47:42,430
look through each triplet in flights graph and print the contents of the graph 

28
03:47:42,430 --> 03:47:45,690
we will specify which airport the flight departs from 

29
03:47:45,690 --> 03:47:48,980
which airport the flight arrives at and the flight number 

30
03:47:48,980 --> 03:47:51,710
you will just put all of the vertices in the graph 

31
03:47:51,710 --> 03:47:55,690
now we are willing to create an additional dataset to store additional information

32
03:47:55,690 --> 03:47:57,330
about each airport 

33
03:47:57,330 --> 03:48:02,390
first we will create a case class called airport information with the city and

34
03:48:02,390 --> 03:48:04,380
airport code properties 

35
03:48:04,380 --> 03:48:08,320
then we are going to create the airport information vertices from a list

36
03:48:08,320 --> 03:48:09,900
as we did earlier 

37
03:48:09,900 --> 03:48:11,830
we will make sure the vertex id for

38
03:48:11,830 --> 03:48:17,930
the airport information matches the vertex id of the airport defined earlier 

39
03:48:17,930 --> 03:48:21,810
note there does not have to be a one to one relationship between the airport

40
03:48:21,810 --> 03:48:24,960
vertices and the airport information vertices 

41
03:48:24,960 --> 03:48:29,640
los angeles international airport is present in the airport vertices but

42
03:48:29,640 --> 03:48:32,990
it is missing from the airport information vertices 

43
03:48:32,990 --> 03:48:37,380
the airport information vertices contains information about airports in london and

44
03:48:37,380 --> 03:48:38,370
hong kong 

45
03:48:38,370 --> 03:48:41,984
but we do not have any flights departing or arriving from those airports 

46
03:48:47,926 --> 03:48:50,540
let us complete our first join 

47
03:48:50,540 --> 03:48:55,050
a mapping function has to be defined in order to join vertices with graphics join

48
03:48:55,050 --> 03:48:56,890
vertices method 

49
03:48:56,890 --> 03:49:00,680
we are going to define a map function that will join the name of the airport from

50
03:49:00,680 --> 03:49:05,580
the airport vertices with the city name from the airport information vertices 

51
03:49:05,580 --> 03:49:09,460
in order to create a new graph with the vertices that has the airport name

52
03:49:09,460 --> 03:49:11,240
colon city name 

53
03:49:11,240 --> 03:49:12,957
after the map function in defined 

54
03:49:12,957 --> 03:49:16,228
then we will use the joined vertices method to create the new graph 

55
03:49:16,228 --> 03:49:21,770
vertices without a matching value in the rdd retain their original value 

56
03:49:21,770 --> 03:49:24,540
now print out the vertices of the new graph 

57
03:49:24,540 --> 03:49:27,814
notice all of the vertices contain the airport and city names 

58
03:49:27,814 --> 03:49:32,089
except los angeles international airport which retained its original value 

59
03:49:37,291 --> 03:49:41,890
now we will join the vertices using graphx outerjoinvertices method 

60
03:49:42,890 --> 03:49:46,060
outerjoinvertices is similar to joinvertices 

61
03:49:46,060 --> 03:49:50,080
except that the user - defined map function is applied to all vertices and

62
03:49:50,080 --> 03:49:51,910
it can change the vertex property type 

63
03:49:53,740 --> 03:49:57,580
here we are using the outerjoinvertices method to create a new graph where

64
03:49:57,580 --> 03:50:01,140
the vertices contain a property with the name of the airport and

65
03:50:01,140 --> 03:50:04,830
a property makeup of the airport information class 

66
03:50:04,830 --> 03:50:07,010
print the vertices once the new graph is made 

67
03:50:08,160 --> 03:50:12,200
notice that the type of the second vertex property is sum 

68
03:50:12,200 --> 03:50:15,250
this is because los angeles international airport

69
03:50:15,250 --> 03:50:18,970
does not have a corresponding vertex in airport information 

70
03:50:18,970 --> 03:50:22,990
therefore graphx assigned all of the properties the sum type so

71
03:50:22,990 --> 03:50:25,700
that all of the vertex property types are the same 

72
03:50:26,960 --> 03:50:28,030
we can use the get or

73
03:50:28,030 --> 03:50:32,400
else method to assign a default value if one does not exist 

74
03:50:32,400 --> 03:50:35,240
now let us rerun the outer join vertices but

75
03:50:35,240 --> 03:50:39,930
this time use the get get or else method to create an airport

76
03:50:39,930 --> 03:50:45,630
information class with na as the values for the city and the airport code 

77
03:50:45,630 --> 03:50:48,140
print the vertices once the new graph is made 

78
03:50:49,270 --> 03:50:54,196
now the type of the second vertex property is airport information in

79
03:50:54,196 --> 03:50:58,957
los angeles international airport airport information class 

80
03:50:58,957 --> 03:51:01,055
has the properties na and na 

81
03:51:06,404 --> 03:51:11,295
finally in this last exercise we will create a new case class called airport

82
03:51:11,295 --> 03:51:16,490
that contains property for the name city and code of the airport 

83
03:51:16,490 --> 03:51:20,080
first we will define the new airport case class with the name city and

84
03:51:20,080 --> 03:51:22,040
code properties 

85
03:51:22,040 --> 03:51:25,580
then use the outer join vertices method to join the data sets and

86
03:51:25,580 --> 03:51:26,350
create the new graph 

87
03:51:27,380 --> 03:51:32,260
create a mapping so if the airport has an airport information vertex the city and

88
03:51:32,260 --> 03:51:36,250
code are taken from the instance of the airport information class 

89
03:51:36,250 --> 03:51:38,770
otherwise only include the airport name 

90
03:51:39,920 --> 03:51:43,370
finally print the vertices of the new graph 

91
03:51:43,370 --> 03:51:48,775
the new graph has only one vertex property which is an instance of the airport class 

92
03:51:48,775 --> 03:51:52,330
graphx has lots of methods for joining graph datasets 

93
03:51:52,330 --> 03:51:56,721
this hands on example only scratched the surface of graphx power 

1
07:38:01,400 --> 07:38:05,190
next we will create a new graph by adding the continents dataset 

2
07:38:05,190 --> 07:38:07,430
import the graphstream library 

3
07:38:07,430 --> 07:38:11,120
import the countriesgraph into a graphstream singlegraph 

4
07:38:11,120 --> 07:38:14,680
visualize the countriesgraph and visualize the facebook graph 

5
07:38:15,688 --> 07:38:19,510
in this hands - on exercise we will use the graphstream library

6
07:38:19,510 --> 07:38:21,510
to visualize countries graph 

7
07:38:21,510 --> 07:38:25,180
the metros graph that we have been working with in the previous exercises 

8
07:38:25,180 --> 07:38:27,990
this time with the continent vertices and edges added 

9
07:38:30,120 --> 07:38:33,391
starting again where we left off from the previous hands - on exercise 

10
07:38:33,391 --> 07:38:36,970
ensure your inaudible has started and that you have imported the data 

11
07:38:43,079 --> 07:38:46,307
first in order to make the graph more interesting 

12
07:38:46,307 --> 07:38:50,754
we are going to import additional vertices from the continent csv and

13
07:38:50,754 --> 07:38:54,342
additional edges from country continent csv to create

14
07:38:54,342 --> 07:38:58,520
a relationship between the country and the continent it belongs to 

15
07:38:59,530 --> 07:39:02,442
we will show the steps for importing the datasets but

16
07:39:02,442 --> 07:39:04,351
we do not cover the steps in detail 

17
07:39:04,351 --> 07:39:07,111
if you are not sure how to import a dataset go back and

18
07:39:07,111 --> 07:39:09,700
view the video called hands - on building a graph 

19
07:39:11,580 --> 07:39:16,250
now we are going to concatenate the metros countries and continents vertices

20
07:39:16,250 --> 07:39:20,800
into a single variable and concatenate the metros to countries edges and

21
07:39:20,800 --> 07:39:25,010
countries to continent edges into another single variable 

22
07:39:25,010 --> 07:39:28,271
finally you will create a new graph called countriesgraph 

23
07:39:36,474 --> 07:39:39,540
now import the graphstream library 

24
07:39:39,540 --> 07:39:43,310
we will not go over in detail how to use the graphstream library 

25
07:39:43,310 --> 07:39:45,246
you can review its documentation and

26
07:39:45,246 --> 07:39:48,314
other online resources on your own if you are interested 

27
07:39:54,742 --> 07:39:58,548
first create a new instance of graphstreams singlegraph class 

28
07:39:58,548 --> 07:40:00,160
using the countriesgraph 

29
07:40:08,574 --> 07:40:12,293
next we will set some attributes for the graph we are going to visualize 

30
07:40:12,293 --> 07:40:14,395
including setting a style for the graph 

31
07:40:14,395 --> 07:40:17,620
graphstream uses cascading style sheets or

32
07:40:17,620 --> 07:40:22,970
css just like ordinary web pages to control the appearance of the graph 

33
07:40:22,970 --> 07:40:27,300
the zip file that you downloaded from coursera contain the css file that we will

34
07:40:27,300 --> 07:40:28,557
use in this hands - on exercise 

35
07:40:30,590 --> 07:40:34,330
now load the countries graph vertices into the visualization

36
07:40:34,330 --> 07:40:37,610
using graphstreams add node method 

37
07:40:37,610 --> 07:40:41,910
notice that we are setting the style ui class of each vertex

38
07:40:41,910 --> 07:40:46,410
depending on if the vertex is an instance of the metro country or continent class 

39
07:40:48,480 --> 07:40:53,470
add the edges of country graph to the visualization using graphstreams

40
07:40:53,470 --> 07:40:54,220
add edge method 

41
07:40:55,960 --> 07:40:59,740
finally call the display method to visualize the graph 

42
07:40:59,740 --> 07:41:01,090
the graph will look similar to this 

43
07:41:02,090 --> 07:41:06,260
the small blue dots are the metropolises the medium sized red dots

44
07:41:06,260 --> 07:41:09,470
are the countries and the large green dots are the continents 

45
07:41:10,790 --> 07:41:13,590
by looking at the clusters you can easily identify which

46
07:41:13,590 --> 07:41:16,790
continent in the visualization is antarctica 

47
07:41:16,790 --> 07:41:19,780
it is the green dot that has no connections or

48
07:41:19,780 --> 07:41:22,500
it is the least connected cluster in the network 

49
07:41:22,500 --> 07:41:24,950
i hope your paying attention because this will be on the quiz 

50
07:41:26,040 --> 07:41:30,205
the large cluster of dots at the bottom is asia because it has the most countries and

51
07:41:30,205 --> 07:41:32,380
metropolises in the graph 

52
07:41:32,380 --> 07:41:36,360
the asia cluster is the most connected cluster in the network 

53
07:41:36,360 --> 07:41:38,980
can you identify which clusters are the other continents

54
07:41:38,980 --> 07:41:40,330
by looking at the visualization 

55
07:41:46,796 --> 07:41:50,572
let look at the visualization of a much larger dataset 

56
07:41:50,572 --> 07:41:53,904
the facebook dataset with 90 000 vertices 

57
07:41:53,904 --> 07:41:57,244
you have seen these techniques before with the metro dataset but

58
07:41:57,244 --> 07:42:00,230
that graph had only around 90 vertices 

59
07:42:00,230 --> 07:42:04,736
because we want you to learn the concepts not be bogged down with repetitive tasks 

60
07:42:04,736 --> 07:42:09,321
we have supplied this cloudera file in order to create the visualization of the graph 

61
07:42:09,321 --> 07:42:12,230
all you need to do is open a new terminal window 

62
07:42:13,290 --> 07:42:16,122
go into the examples of analytic directory and

63
07:42:16,122 --> 07:42:19,546
run the spark shell with the facebook cloudera file 

64
07:42:19,546 --> 07:42:22,610
do not worry about copying the full command from the video 

65
07:42:22,610 --> 07:42:25,340
a text version will be included within the course reading

66
07:42:25,340 --> 07:42:28,230
that contains the full command for you to copy and paste 

67
07:42:33,442 --> 07:42:35,050
what do you see 

68
07:42:35,050 --> 07:42:37,560
my first reaction is that it looks like broccoli 

69
07:42:37,560 --> 07:42:41,560
a green vegetable reviled by many american children unjustly 

70
07:42:41,560 --> 07:42:44,790
social networks are made up of several clusters of communities or

71
07:42:44,790 --> 07:42:47,700
pockets of people who interact densely

72
07:42:47,700 --> 07:42:51,570
that are brought together by people who are members of multiple communities 

73
07:42:51,570 --> 07:42:54,380
this interlinking of clusters gives the social network

74
07:42:54,380 --> 07:42:57,140
a broccoli like shape when visualized 

75
07:42:57,140 --> 07:43:00,218
i hope you heard the last thing i said because that will be on the quiz 

1
15:21:00,570 --> 15:21:04,940
next we will import the breezeviz library define a function to calculate the degree

2
15:21:04,940 --> 15:21:09,810
histogram calculate the probability distribution for the degree histogram and

3
15:21:09,810 --> 15:21:11,500
graph the results 

4
15:21:11,500 --> 15:21:14,580
in this hands on exercise we will plot the degree histogram created

5
15:21:14,580 --> 15:21:18,130
in the previous hands on exercise using the breezeviz library 

6
15:21:24,258 --> 15:21:28,742
starting again where we left off in the previous hands - on exercise 

7
15:21:28,742 --> 15:21:30,880
ensure your is started 

8
15:21:30,880 --> 15:21:33,170
first we will import the breezeviz library 

9
15:21:40,286 --> 15:21:44,958
next we will define a function to calculate the degree histogram of metro 

10
15:21:44,958 --> 15:21:47,480
graph so we can plot it with breezeviz 

11
15:21:47,480 --> 15:21:51,310
the definition of the degree histogram function is nearly identical

12
15:21:51,310 --> 15:21:54,680
to the code from the previous exercise that helped us create 

13
15:21:54,680 --> 15:21:56,050
the histogram data array 

14
15:22:02,003 --> 15:22:06,623
now we will calculate the probability distribution of the vertex degrees over

15
15:22:06,623 --> 15:22:11,453
the whole graph by normalizing the vertex degree by the total number of vertices so

16
15:22:11,453 --> 15:22:14,930
that the degree probabilities add up to 1 

17
15:22:14,930 --> 15:22:20,070
the output of the first command is 28 the number of countries in country csv 

18
15:22:20,070 --> 15:22:25,200
the output of the second command is an array of numbered pairs 

19
15:22:25,200 --> 15:22:27,190
the first number is the vertex degree 

20
15:22:27,190 --> 15:22:31,180
and the second number is the probability distribution 

21
15:22:31,180 --> 15:22:34,590
notice that the sum of all the probability distributions equals one 

22
15:22:40,915 --> 15:22:44,950
now we will plot two graphs to visualize the degree histogram 

23
15:22:44,950 --> 15:22:48,340
the first graph will be a line graph of the degree histogram and

24
15:22:48,340 --> 15:22:51,720
the second will be the degree histogram itself 

25
15:22:51,720 --> 15:22:56,540
we will not go over in detail how to use the breezeviz library to create graphs 

26
15:22:56,540 --> 15:23:00,540
the project is well documented and there are many resources online if you want to

27
15:23:00,540 --> 15:23:03,070
explore the breezeviz library on your own 

28
15:23:04,740 --> 15:23:08,970
for the first graph we will define the x axis as the vertex degree and

29
15:23:08,970 --> 15:23:11,910
the y axis as the degree probability 

30
15:23:11,910 --> 15:23:14,520
for the second graph we will just pass the metro graph

31
15:23:14,520 --> 15:23:17,360
degree attribute to breezeviz histogram function

32
15:23:37,513 --> 15:23:41,200
you should see two graphical representations of the data 

33
15:23:41,200 --> 15:23:43,260
the bottom one is the histogram 

34
15:23:43,260 --> 15:23:46,880
the top shows the degrees distribution plotting the histogram 

35
15:23:46,880 --> 15:23:51,287
as we saw in the previous exercise array most countries have one metropolis 

1
06:44:51,940 --> 06:44:54,250
welcome back to the fourth and final module of the course 

2
06:44:55,470 --> 06:45:00,740
in this module we will cover the underlying principles of large scale graph processing

3
06:45:00,740 --> 06:45:02,800
and the software infrastructure that supports it 

4
06:45:04,380 --> 06:45:07,440
so in this module you will learn a programming model for

5
06:45:07,440 --> 06:45:10,900
graph computation and systems that implement this model 

6
06:45:12,200 --> 06:45:16,530
after this model you will be able to formulate graph analytics computations

7
06:45:16,530 --> 06:45:17,930
in terms of the programming model 

8
06:45:18,970 --> 06:45:21,340
this module will consist of three lessons 

9
06:45:22,360 --> 06:45:26,790
the first lesson will revisit the concept of the programming model and

10
06:45:26,790 --> 06:45:30,720
introduce a programming model called bulk synchronous parallel or

11
06:45:30,720 --> 06:45:35,250
bsp that is designed specifically for graph oriented computation 

12
06:45:36,400 --> 06:45:41,520
i will discuss two well known versions of the central idea pregel from google and

13
06:45:41,520 --> 06:45:43,760
graphlab from carnegie mellon university 

14
06:45:44,760 --> 06:45:48,260
the second lesson we will discuss two software systems 

15
06:45:48,260 --> 06:45:52,390
giraph that operates on hadoop and graphx that operates on spark 

16
06:45:53,560 --> 06:45:56,890
we will compare the basic architecture of these two systems and

17
06:45:56,890 --> 06:45:58,090
point out their differences 

18
06:45:59,200 --> 06:46:04,170
the final lesson will show example computations using graphics platform 

19
06:46:04,170 --> 06:46:06,210
since graphics come with spark 

20
06:46:06,210 --> 06:46:10,320
we have created these examples on the virtual machine which you have access to 

21
06:46:10,320 --> 06:46:12,181
the code and some data sets are available for

22
06:46:12,181 --> 06:46:13,849
those who wish to play with the system 

1
13:31:04,830 --> 13:31:06,950
now we are going to introduce you to graphx 

2
13:31:08,080 --> 13:31:10,967
which we will primarily cover as a hands - on presentation 

3
13:31:12,490 --> 13:31:16,440
graphx was developed by the amp lab at uc berkeley and

4
13:31:16,440 --> 13:31:18,020
has now become an apache product 

5
13:31:19,280 --> 13:31:23,610
in this brief introduction we are using a few slides from amp lab presentations 

6
13:31:26,450 --> 13:31:30,540
like inaudible graphic uses a property graph model 

7
13:31:31,580 --> 13:31:35,690
that means both nodes and edges can have attributes and values 

8
13:31:37,212 --> 13:31:41,700
in graphx the node properties are stored in a vertex table 

9
13:31:41,700 --> 13:31:43,940
and edge properties are stored in an edge table 

10
13:31:45,140 --> 13:31:49,680
the connectivity information that is which edge connects to which nodes 

11
13:31:49,680 --> 13:31:53,570
is stored separately from the nodes and edge properties 

12
13:31:53,570 --> 13:31:55,700
since graphx is based on spark 

13
13:31:55,700 --> 13:32:01,179
whose central information representation is based on resilient data sets or rdds 

14
13:32:02,230 --> 13:32:06,520
you may recall that rdds are typically in memory information objects 

15
13:32:07,700 --> 13:32:11,320
these objects can be used to perform an action which returns a value 

16
13:32:12,320 --> 13:32:16,420
or they can perform a transformation which can produce another rdd 

17
13:32:17,865 --> 13:32:21,330
graphx is built on special rdds for vertices and edges 

18
13:32:22,820 --> 13:32:26,810
note that vertexids are defined to be unique by design 

19
13:32:26,810 --> 13:32:32,680
vertexrdd a represents a set of vertices all of which have an attribute called a 

20
13:32:34,400 --> 13:32:39,400
the edge class is an object with a source vertex a destination vertex and

21
13:32:39,400 --> 13:32:39,930
edge attribute 

22
13:32:41,280 --> 13:32:46,230
the edgerdd extends this basic edge by storing

23
13:32:46,230 --> 13:32:51,546
edges in a columnar format on each partition of performance 

24
13:32:51,546 --> 13:32:56,140
very often it easier to operate on a data structure that has both

25
13:32:56,140 --> 13:32:58,120
node properties and edge properties 

26
13:32:59,380 --> 13:33:03,620
if you are familiar with this schema you will readily see that this is a three - way

27
13:33:03,620 --> 13:33:07,990
join operation between the node set the edge set and the second node set 

28
13:33:09,010 --> 13:33:12,600
finally graphx implements it own version of bsp 

29
13:33:13,820 --> 13:33:16,820
this implementation not surprisingly is called pregel 

30
13:33:17,950 --> 13:33:23,610
it allows a user to write a vertex program a send message routine 

31
13:33:23,610 --> 13:33:25,990
and a message combiner routine just like bsp 

32
13:33:27,160 --> 13:33:30,840
however this implementation also performs

33
13:33:30,840 --> 13:33:32,970
vertex partitioning like we saw in graph lab 

34
13:33:34,210 --> 13:33:36,170
also similarly to graph lab 

35
13:33:36,170 --> 13:33:40,770
it enables the message sending computation to reach the attributes of both vertices 

36
13:33:41,960 --> 13:33:44,570
the code limit shown here is written in scala 

37
13:33:46,230 --> 13:33:50,495
the hands on code after this you will see are all certain in scatter 

38
13:33:50,495 --> 13:33:52,860
will have a notice here in no scatter 

39
13:33:52,860 --> 13:33:57,640
we would like to show you here an example which uses

40
13:33:57,640 --> 13:34:02,600
the pregel runtime object in graphx to implement the simple bsp desk 

41
13:34:03,870 --> 13:34:08,280
we would like to show here how pregel runtime objects in graphx works 

42
13:34:09,890 --> 13:34:12,760
we point out all the functions that are defined by users

43
13:34:12,760 --> 13:34:14,160
in running the pregel operation 

44
13:34:15,420 --> 13:34:20,301
the vprogf function which takes a vertex and a message and

45
13:34:20,301 --> 13:34:24,220
returns a new attribute value for that vertex 

46
13:34:24,220 --> 13:34:29,660
the sendmsgr function which computes the new message along each edge 

47
13:34:30,880 --> 13:34:35,850
the option m construct says that it is optional for a vertex to send the message 

48
13:34:35,850 --> 13:34:38,520
that is the system should not show an error

49
13:34:38,520 --> 13:34:41,650
if your message is not sent for the vertex 

50
13:34:41,650 --> 13:34:44,230
the combinef function is a message combiner that we

51
13:34:44,230 --> 13:34:46,210
mention in passive for graph 

52
13:34:46,210 --> 13:34:50,913
finally the user can also specify the number of reiterations that

53
13:34:50,913 --> 13:34:53,483
the graphx bsp process will run for 

1
03:05:58,040 --> 03:05:58,700
in 2010 google

2
03:06:00,550 --> 03:06:03,650
published a paper outlining a system that they had been working on 

3
03:06:06,130 --> 03:06:10,740
this publication about a system called pregel has been one of the most

4
03:06:10,740 --> 03:06:15,370
influential publications on large scale graph computing 

5
03:06:15,370 --> 03:06:19,160
the pregel system essentially implemented the bsb model that we covered in

6
03:06:19,160 --> 03:06:19,850
the last lecture 

7
03:06:21,230 --> 03:06:24,180
to show how the pregel system is programmed 

8
03:06:24,180 --> 03:06:29,810
we present the published code of google most famous algorithm the pagerank 

9
03:06:31,300 --> 03:06:35,080
recall that pagerank task is to compute note centrality 

10
03:06:36,240 --> 03:06:40,160
the basic philosophy of pagerank is that a note that is connected to an more

11
03:06:40,160 --> 03:06:42,350
important note gains more importance 

12
03:06:43,900 --> 03:06:48,400
in the filler on the right b is a very important node with a high page rank

13
03:06:48,400 --> 03:06:52,650
because a lot of other nodes directly or indirectly point to it 

14
03:06:52,650 --> 03:06:57,130
so c being his direct neighbor that receives an edge from b 

15
03:06:57,130 --> 03:06:59,050
also has a high page rank 

16
03:06:59,050 --> 03:07:02,190
all the c itself there not have too many incident edges 

17
03:07:03,820 --> 03:07:07,500
on the left side we have google published c + + code

18
03:07:07,500 --> 03:07:10,130
that implements pagerank method on the pregel infrastructure 

19
03:07:11,450 --> 03:07:14,730
you do not have to know c + + to understand the basic essence of the code 

20
03:07:15,840 --> 03:07:19,860
we will explain the basic elements of the vertex program in the next few slides 

21
03:07:21,540 --> 03:07:24,690
remember from the last lecture that the vsp technique considers

22
03:07:24,690 --> 03:07:25,980
a vertex to be a process 

23
03:07:27,360 --> 03:07:31,100
every vertex implements a method called compute 

24
03:07:33,070 --> 03:07:37,760
this method implements the logic of what a vertex should do during the super steps 

25
03:07:39,490 --> 03:07:44,350
the program starts by creating a special kind of vertex called the pagerank vertex

26
03:07:44,350 --> 03:07:46,880
for which this compute method is specified 

27
03:07:48,100 --> 03:07:52,500
you will notice that the compute method starts by saying what happens

28
03:07:52,500 --> 03:07:54,810
when the super step is one or more 

29
03:07:56,060 --> 03:07:57,590
but what happens in super step zero 

30
03:07:58,900 --> 03:08:02,691
usually superstep 0 is used for initialization 

31
03:08:02,691 --> 03:08:07,940
every vertex initialization pagerank value to a same number

32
03:08:07,940 --> 03:08:11,870
which is one divided by the total number of vertices in the graph 

33
03:08:13,090 --> 03:08:18,840
computationally a pagerank of a vertex is a number calculated by adding two terms 

34
03:08:20,380 --> 03:08:24,790
the first term depends on the total number of vertices in the graph and

35
03:08:24,790 --> 03:08:26,120
is therefore the same every time 

36
03:08:26,120 --> 03:08:31,990
and the second term depends on the page rank of the neighbors of the vertex 

37
03:08:33,420 --> 03:08:35,200
how does the vertex compute the second term 

38
03:08:36,270 --> 03:08:39,590
it gets the pagerank values of the neighbors in its messages 

39
03:08:39,590 --> 03:08:40,170
and adds them up 

40
03:08:41,390 --> 03:08:45,950
as we saw for sssp after a vertex computes its value 

41
03:08:45,950 --> 03:08:51,170
it goes to the propagate step and sends a message to its outgoing edges 

42
03:08:51,170 --> 03:08:54,210
for pagerank the message it sends out

43
03:08:54,210 --> 03:08:57,680
is just computed value divided by the number of outgoing edges 

44
03:08:58,890 --> 03:09:03,800
when this is done the node halts for the next superstep and waits for

45
03:09:03,800 --> 03:09:05,190
some other node to wake it up 

46
03:09:06,650 --> 03:09:11,590
at this point we have seen two examples of graph analytic operations

47
03:09:11,590 --> 03:09:15,380
executed on the bsb programming model 

48
03:09:15,380 --> 03:09:19,370
now we will look at the same problem from a slightly different viewpoint 

49
03:09:20,710 --> 03:09:25,220
graphlab originally a project from carnegie mellon university now turned

50
03:09:25,220 --> 03:09:29,450
into a company called dato took a similar yet different approach to the problem 

51
03:09:30,970 --> 03:09:34,540
in graphlab any kind of data can be associated with a vertex or an edge 

52
03:09:35,710 --> 03:09:38,310
this information is stored in what is called a data graph 

53
03:09:39,610 --> 03:09:42,050
let look at the same page like i already did 

54
03:09:42,050 --> 03:09:45,780
the syntax is a bit different but the logical blocks are identical 

55
03:09:47,080 --> 03:09:49,500
these are the same blocks that we highlighted before 

56
03:09:50,820 --> 03:09:55,450
graphlab breaks up these blocks into three different user specified functions

57
03:09:56,910 --> 03:10:00,780
called gather apply scatter or gas for short 

58
03:10:02,540 --> 03:10:04,050
okay so what is different 

59
03:10:05,110 --> 03:10:08,523
let mention a few important differences 

60
03:10:08,523 --> 03:10:10,300
first let consider gather 

61
03:10:11,830 --> 03:10:16,234
rather than adopting a message passing or data flow model like pregel 

62
03:10:16,234 --> 03:10:21,230
graphlab allows the user defined update function complete freedom to read and

63
03:10:21,230 --> 03:10:25,032
modify any of the data on adjacent vertices edges 

64
03:10:26,822 --> 03:10:32,180
in graphlab a receiving vertex has access to the data on adjacent vertices

65
03:10:32,180 --> 03:10:37,060
even if the adjacent vertices did not schedule the current update 

66
03:10:38,450 --> 03:10:43,340
in contrast for pregel the control is with descending nodes 

67
03:10:43,340 --> 03:10:46,420
an update can happen only when a node sends out messages 

68
03:10:47,530 --> 03:10:51,710
for some graph analytics operations like a dynamic version of pagerank 

69
03:10:51,710 --> 03:10:52,620
this is very important 

70
03:10:53,830 --> 03:10:59,100
further if vertex can update its value asynchronously 

71
03:10:59,100 --> 03:11:03,260
that is as soon as it receives an update without having to wait for

72
03:11:03,260 --> 03:11:05,250
all nodes like we do in vsp 

73
03:11:06,630 --> 03:11:11,520
this often helps some intuitive algorithms like page rank converge faster 

74
03:11:12,610 --> 03:11:15,620
when the graph is large and must be split across machines 

75
03:11:16,700 --> 03:11:20,310
bsp cuts the graph along edges as we see on this slide 

76
03:11:21,480 --> 03:11:25,770
so every cut edge results in a machine to machine communication 

77
03:11:26,850 --> 03:11:29,710
if we increase the number of across machine edges 

78
03:11:29,710 --> 03:11:32,250
we increase communication cost 

79
03:11:32,250 --> 03:11:34,330
this has an interesting practical consequence 

80
03:11:36,110 --> 03:11:40,650
as we have mentioned many graph applications have communities 

81
03:11:40,650 --> 03:11:42,290
and central nodes that have high degree 

82
03:11:43,290 --> 03:11:46,150
many young people today have over 500 facebook friends 

83
03:11:47,410 --> 03:11:50,005
so when an analytical operation like page rank 

84
03:11:50,005 --> 03:11:53,850
that goes through multiple iterations until convergence for

85
03:11:53,850 --> 03:11:57,070
these graphs the communication cost can become very high 

86
03:11:58,270 --> 03:12:02,540
in the cluster graph shown here every color represents a different machine 

87
03:12:03,780 --> 03:12:05,917
let look at that red vertex 

88
03:12:05,917 --> 03:12:08,239
what would happen if we split it like this instead 

89
03:12:10,515 --> 03:12:13,250
the red node gets split 

90
03:12:13,250 --> 03:12:18,260
and different vertices of different machines work with their own copy

91
03:12:18,260 --> 03:12:18,960
of the red vertex 

92
03:12:20,480 --> 03:12:24,490
in the diagram the primary red vertex is marked zero and

93
03:12:24,490 --> 03:12:26,330
copies are marked one through five 

94
03:12:27,970 --> 03:12:30,400
now the gather phase happens for each copy 

95
03:12:33,010 --> 03:12:36,590
followed by a second operation from the copies to the primary red vertex 

96
03:12:38,100 --> 03:12:42,000
and this is followed by new user defined operation called merge

97
03:12:42,000 --> 03:12:45,488
that combines the partial results from the copies to the primary vertex 

98
03:12:45,488 --> 03:12:50,290
so for pagerank the merge operation boils down to computing the total

99
03:12:50,290 --> 03:12:54,760
summation of the partial summation of the edges computed at the copies 

100
03:12:54,760 --> 03:12:59,230
thus in this lesson we have seen two of the most influential

101
03:12:59,230 --> 03:13:03,470
paradigms of large scale graph computation when the number of nodes and

102
03:13:03,470 --> 03:13:06,400
edges run into tens of millions and more 

